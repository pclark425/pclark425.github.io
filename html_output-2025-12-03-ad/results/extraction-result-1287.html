<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1287 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1287</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1287</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-2ef9ee354cfedebe497b4614926b733d10a23693</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2ef9ee354cfedebe497b4614926b733d10a23693" target="_blank">In-Context Freeze-Thaw Bayesian Optimization for Hyperparameter Optimization</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> FT-PFN is a prior-data fitted network (PFN) that leverages the transformers' in-context learning ability to efficiently and reliably do Bayesian learning curve extrapolation in a single forward pass, and yields new state-of-the-art performance in the same three families of deep learning HPO benchmarks considered in prior work.</p>
                <p><strong>Paper Abstract:</strong> With the increasing computational costs associated with deep learning, automated hyperparameter optimization methods, strongly relying on black-box Bayesian optimization (BO), face limitations. Freeze-thaw BO offers a promising grey-box alternative, strategically allocating scarce resources incrementally to different configurations. However, the frequent surrogate model updates inherent to this approach pose challenges for existing methods, requiring retraining or fine-tuning their neural network surrogates online, introducing overhead, instability, and hyper-hyperparameters. In this work, we propose FT-PFN, a novel surrogate for Freeze-thaw style BO. FT-PFN is a prior-data fitted network (PFN) that leverages the transformers' in-context learning ability to efficiently and reliably do Bayesian learning curve extrapolation in a single forward pass. Our empirical analysis across three benchmark suites shows that the predictions made by FT-PFN are more accurate and 10-100 times faster than those of the deep Gaussian process and deep ensemble surrogates used in previous work. Furthermore, we show that, when combined with our novel acquisition mechanism (MFPI-random), the resulting in-context freeze-thaw BO method (ifBO), yields new state-of-the-art performance in the same three families of deep learning HPO benchmarks considered in prior work.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1287.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1287.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ifBO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>in-context Freeze-Thaw Bayesian Optimization (ifBO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A freeze-thaw Bayesian optimization agent that combines an in-context surrogate (FT-PFN) with a randomized multi-fidelity Probability-of-Improvement acquisition (MFPI-random) to adaptively allocate single-step training budget across hyperparameter configurations using only partial learning-curve observations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ifBO</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An HPO agent implementing freeze-thaw Bayesian optimization where (1) the surrogate is FT-PFN, a transformer-based prior-data fitted network performing in-context Bayesian prediction without online refitting, and (2) the acquisition is MFPI-random which samples a random extrapolation horizon h and random threshold T and selects the configuration maximizing the surrogate's Probability-of-Improvement for that (h,T) draw.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian optimization (freeze-thaw) with an acquisition portfolio (randomized multi-fidelity Probability of Improvement)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each unit step the agent (i) inputs the history H (partial learning curves) to the FT-PFN surrogate which returns posterior predictive distributions for candidate configurations at future budgets, (ii) samples h^{rand} ~ U(1,b_max) and tau^{rand} (log-uniform) to form a threshold T^{rand}, (iii) computes MFPI(λ; h^{rand},T^{rand}) = P_{surrogate}(f(λ, b_λ + h^{rand}) > T^{rand}) for each candidate, and (iv) continues (thaws) the argmax configuration for one step. The surrogate requires no online retraining; decisions use predicted distributions (uncertainty) over continuations.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Hyperparameter optimization benchmarks (LCBench, Taskset, PD1) — framed as freeze-thaw HPO environments with partial learning curves</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable (only partial learning curves observed per configuration), stochastic/noisy observations (Gaussian noise in curve prior), preemptible training (can freeze/thaw), discrete action choices (which configuration to advance by one training step), bounded performance metric normalized to [0,1]; heterogeneous curve lengths and possible divergence/stagnation behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Benchmarks use finite tabular configuration spaces of size up to 1000 per task (LCBench and Taskset 1000 configurations), hyperparameter dimensionalities from 4 up to 8 (Taskset variants) or 7 (LCBench) and up to 10 supported by FT-PFN; learning-curve lengths vary by task (5 to 1414 steps reported), HPO budget used in experiments B = 1000 unit steps (≈20 full trainings for 52-epoch tasks). Action space: choose one configuration to thaw among up to ~1000 candidates each step.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>ifBO yields state-of-the-art HPO performance on the three benchmark families: outperforms DPL and DyHPO on LCBench and Taskset and competes closely on PD1; has the best average rank across benchmarks. FT-PFN+MFPI-random achieves superior anytime behavior and dominates pairwise after ≈150 steps. (Quantitative HPO metrics are reported as normalized regret and average rank across tasks; specific numeric regrets per task are in Appendix F.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Designed for low-budget regime; demonstrated strong performance with total budgets B=1000 freeze-thaw steps (≈20 full trainings for LCBench/Taskset). Empirically dominates baselines in this few-evaluations regime and shows improved performance after ~150 steps.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Managed by MFPI-random which randomizes extrapolation horizon h and improvement threshold T each iteration, effectively sampling from a portfolio of PI acquisitions; this hedges between short-term (small h) exploitation and long-term (large h) exploration, and between low-risk (small τ) and high-risk (large τ) thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against DyHPO (deep-kernel GP freeze-thaw), DPL (deep-ensemble power-law freeze-thaw), GP-based freeze-thaw (Matérn GP), HyperBand, ASHA, uniform random search; also ablated with EI/PI one-step and EI/PI max acquisitions and variants.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Combining an in-context PFN surrogate with a randomized MFPI acquisition yields: (1) substantially better posterior predictive quality (higher log-likelihood and lower MSE) than DyHPO and DPL across benchmarks; (2) major inference speedups (FT-PFN inference 10–100x faster than DyHPO/DPL depending on context size); (3) improved HPO results in the low-budget regime (ifBO achieves SOTA or competitive results on LCBench, Taskset, PD1); (4) MFPI-random is more robust across benchmarks than fixed-horizon EI/PI acquisitions because it hedges across horizons and thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires performance metric and hyperparameter values normalized to [0,1] and supports up to 10 hyperparameters; FT-PFN trained for budgets B ≤ 1000 (scaling beyond this is untested); combinations of FT-PFN with EI-based acquisitions can fail (heavy-tailed predictive posteriors cause EI to over-explore and catastrophically under-exploit in very low-budget settings); not yet demonstrated at scale for modern large-scale DL pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'In-Context Freeze-Thaw Bayesian Optimization for Hyperparameter Optimization', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1287.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1287.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FT-PFN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Freeze-Thaw Prior-Data Fitted Network (FT-PFN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based prior-data fitted network trained on synthetic learning-curve priors to produce in-context posterior predictive distributions for learning-curve extrapolation without any online retraining, used as the dynamic surrogate in freeze-thaw BO.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>FT-PFN (surrogate)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A sequence Transformer (6 layers, embed 512, 4 heads, hidden 1024, ~14.69M params) that ingests tokens of the form (hyperparameters, normalized budget t, observed performance) as context and outputs a discretized posterior predictive density (1000 bins) for target (hyperparameter, budget) pairs; trained on 2.0M synthetic datasets sampled from a generative curve prior that models realistic learning-curve shapes including saturation and divergence.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Used as the probabilistic model component within an adaptive Bayesian optimization loop (freeze-thaw BO); itself performs in-context Bayesian prediction (metalearned Bayesian inference).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>FT-PFN does not adapt its weights online. Instead, it adapts predictions to the observed partial data by taking H (set of observed (λ,b,y) tuples) as input in-context and producing posterior predictive distributions for candidate continuations; these predictions are then used by the acquisition function to choose the next action.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Partial learning-curve observation setting within HPO benchmarks (LCBench, PD1, Taskset)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partial/limited observations (subsets of per-configuration learning curves), noisy/bounded metrics (Gaussian noise clipped to [0,1]), heterogeneous budgets and variable curve shapes including possible divergence or stagnation, discrete finite configuration spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Context sizes tested up to ~1800 samples; meta-training limited to N=1000 tokens per dataset; supports up to M=10 hyperparameters; typical benchmarks have up to 1000 configurations and learning-curve lengths up to 1414.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Predictive performance (example at context size ~1000): log-likelihood median across tasks: LCBench=2.118, PD1=1.133, Taskset=3.016; MSE: LCBench=0.004, PD1=0.024, Taskset=0.004. Inference runtime (single-CPU evaluation reported): ~0.719s for 1000-sample contexts. Achieves 10–100x speedups over DPL/DyHPO depending on context size.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Maintains high predictive quality as context size grows; trained on synthetic data so requires no online samples to adapt weights, only needs observed partial curves at test time for in-context prediction; demonstrated effective with contexts from a few hundred to ~1800 samples.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>FT-PFN provides calibrated uncertainty estimates which the acquisition uses to trade off exploration/exploitation; heavy-tailed posterior predictions can increase exploration if used with EI, motivating MFPI-random's threshold/horizon randomization to stabilize behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against DyHPO's deep Gaussian process surrogate and DPL's deep ensemble of power laws; ablated against a variant of FT-PFN with no-hyperparameter inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>FT-PFN produces superior posterior predictive quality (higher log-likelihoods, lower MSE) and much faster inference than DyHPO and DPL across three benchmarks, despite being trained only on synthetic curve priors; its uncertainty estimates are more calibrated, which improves acquisition performance when paired with an appropriate AF (MFPI-random).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires normalization of performance and hyperparameters to [0,1]; trained/meta-trained for budgets up to 1000 and at most 10 hyperparameters; predictive heavy tails can harm performance with EI acquisitions (leads to over-exploration).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'In-Context Freeze-Thaw Bayesian Optimization for Hyperparameter Optimization', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1287.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1287.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DyHPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DyHPO (Dynamic and efficient gray-box hyperparameter optimization for deep learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior freeze-thaw BO implementation that uses a learned deep kernel Gaussian process surrogate and a multi-fidelity Expected Improvement (one-step EI) acquisition to decide which runs to continue.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dynamic and efficient gray-box hyperparameter optimization for deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DyHPO</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Freeze-thaw BO using a deep-kernel Gaussian Process surrogate that predicts one-step-ahead performance and uses a one-step Expected Improvement acquisition (greedy) to select the next configuration to continue.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian optimization (freeze-thaw) with deep-kernel GP surrogate and one-step Expected Improvement acquisition</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>DyHPO refits its surrogate online using the history of observed partial learning curves and uses one-step EI to select the configuration with the highest expected improvement one step ahead, favoring short-horizon greedy gains.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same HPO benchmarks (LCBench, Taskset, PD1) in freeze-thaw setting</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable (partial learning curves), noisy observations, preemptible training, discrete finite configuration spaces; DyHPO targets one-step ahead extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Benchmarks with up to 1000 configurations, learning-curve lengths vary per task; surrogate retraining incurs computational overhead (reported runtimes higher than FT-PFN; see Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Predictive quality per Table 1 (example context=1000): log-likelihood medians: LCBench = -0.368, PD1 = -0.457, Taskset = -0.381; MSE example at context 1000: LCBench=0.012, PD1=0.071, Taskset=0.008. Runtime (context 1000): ~59.95s (single-CPU reported). In HPO experiments, DyHPO is a strong baseline but is generally outperformed by ifBO; DyHPO performs well when paired with its one-step greedy acquisition in some settings (e.g., PD1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Requires online surrogate retraining after steps (incurs computational cost); performance depends on fidelity of one-step predictions and retraining frequency.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Relies on one-step EI (greedy, short horizon) which emphasizes near-term exploitation; limited horizon can reduce exploratory continuation of promising but slow-improving runs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared in paper against DPL, FT-PFN/ifBO, GP-based FT-BO, HyperBand, ASHA, random search; ablation includes using MFPI-random acquisition with DyHPO surrogate.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>DyHPO's deep-kernel GP is competent but provides lower log-likelihood and slower inference than FT-PFN; the one-step EI acquisition is effective for DyHPO's one-step surrogate but less robust across benchmarks when combined with FT-PFN's calibrated uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Struggles to extrapolate beyond a single step effectively (limits multi-step planning); surrogate retraining online is computationally expensive; lower log-likelihoods relative to FT-PFN indicate poorer uncertainty calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'In-Context Freeze-Thaw Bayesian Optimization for Hyperparameter Optimization', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1287.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1287.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DPL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DPL (Deep Power Laws for Hyperparameter Optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A freeze-thaw BO agent that fits a deep ensemble of parametric power-law learning-curve models per configuration and uses Expected Improvement at max budget (EI-max) as acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep power laws for hyperparameter optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DPL</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Surrogate is a deep ensemble of power-law curve fitters (power-law basis functions) producing predictive distributions; acquisition uses EI evaluated at maximum budget (greedy toward eventual final performance). The ensemble is trained/updated online during the freeze-thaw run.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian optimization (freeze-thaw) with a deep-ensemble parametric learning-curve surrogate and EI at maximum budget acquisition</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>DPL fits/fine-tunes deep ensembles of power-law curve models to the observed partial learning curves (online) and uses the surrogate to predict final performance (b_max) and select configurations via EI at max budget.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>HPO freeze-thaw benchmarks (LCBench, Taskset, PD1)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observed learning curves, assumed to follow power-law-like parametric trends (strong modeling assumption), stochastic observations, discrete configuration spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Benchmarks with up to 1000 configurations and varied learning-curve lengths; ensemble training and online updates incur runtime overhead (see Table 1 runtimes).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Predictive quality per Table 1 (context=1000): log-likelihood medians: LCBench = -11.983, PD1 = -11.017, Taskset = -20.350; MSEs comparable on Taskset but worse log-likelihoods (overconfident/poor uncertainty). Runtime (context 1000): ~41.96s (single-CPU reported). In HPO experiments DPL is a competitive baseline and in some situations (e.g., early Taskset) can perform well but is overall outperformed by ifBO in aggregated measures.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Requires online fitting and ensemble inference which is computationally heavier; ensemble size used in referenced work was small (n=5) which contributed to poor uncertainty calibration in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Uses EI at maximum budget which focuses on predicted eventual performance (long-horizon exploitation); may under-explore in some settings, and its strong power-law assumptions can lead to overly confident incorrect predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to DyHPO, FT-PFN/ifBO, GP-based FT-BO, HyperBand, ASHA, random search; ablations include using MFPI-random acquisition with DPL surrogate.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>DPL's parametric power-law ensemble can achieve good point predictions in some benchmarks (Taskset) but shows very poor log-likelihoods (overconfident) and is significantly slower in inference than FT-PFN; its strong curve-shape assumptions can be limiting when real curves violate power-law forms.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Strong power-law assumptions can produce overly confident incorrect predictions; small ensemble sizes worsen uncertainty estimates; online retraining introduces computational overhead; performs worse than ifBO on aggregated HPO metrics though it can be strong on some specific benchmarks/early budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'In-Context Freeze-Thaw Bayesian Optimization for Hyperparameter Optimization', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Freeze-thaw Bayesian optimization <em>(Rating: 2)</em></li>
                <li>Dynamic and efficient gray-box hyperparameter optimization for deep learning <em>(Rating: 2)</em></li>
                <li>Deep power laws for hyperparameter optimization <em>(Rating: 2)</em></li>
                <li>Transformers can do Bayesian inference <em>(Rating: 2)</em></li>
                <li>Efficient bayesian learning curve extrapolation using prior-data fitted networks <em>(Rating: 2)</em></li>
                <li>Speeding up automatic Hyperparameter Optimization of deep neural networks by extrapolation of learning curves <em>(Rating: 1)</em></li>
                <li>Learning curve prediction with Bayesian neural networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1287",
    "paper_id": "paper-2ef9ee354cfedebe497b4614926b733d10a23693",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "ifBO",
            "name_full": "in-context Freeze-Thaw Bayesian Optimization (ifBO)",
            "brief_description": "A freeze-thaw Bayesian optimization agent that combines an in-context surrogate (FT-PFN) with a randomized multi-fidelity Probability-of-Improvement acquisition (MFPI-random) to adaptively allocate single-step training budget across hyperparameter configurations using only partial learning-curve observations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ifBO",
            "agent_description": "An HPO agent implementing freeze-thaw Bayesian optimization where (1) the surrogate is FT-PFN, a transformer-based prior-data fitted network performing in-context Bayesian prediction without online refitting, and (2) the acquisition is MFPI-random which samples a random extrapolation horizon h and random threshold T and selects the configuration maximizing the surrogate's Probability-of-Improvement for that (h,T) draw.",
            "adaptive_design_method": "Bayesian optimization (freeze-thaw) with an acquisition portfolio (randomized multi-fidelity Probability of Improvement)",
            "adaptation_strategy_description": "At each unit step the agent (i) inputs the history H (partial learning curves) to the FT-PFN surrogate which returns posterior predictive distributions for candidate configurations at future budgets, (ii) samples h^{rand} ~ U(1,b_max) and tau^{rand} (log-uniform) to form a threshold T^{rand}, (iii) computes MFPI(λ; h^{rand},T^{rand}) = P_{surrogate}(f(λ, b_λ + h^{rand}) &gt; T^{rand}) for each candidate, and (iv) continues (thaws) the argmax configuration for one step. The surrogate requires no online retraining; decisions use predicted distributions (uncertainty) over continuations.",
            "environment_name": "Hyperparameter optimization benchmarks (LCBench, Taskset, PD1) — framed as freeze-thaw HPO environments with partial learning curves",
            "environment_characteristics": "Partially observable (only partial learning curves observed per configuration), stochastic/noisy observations (Gaussian noise in curve prior), preemptible training (can freeze/thaw), discrete action choices (which configuration to advance by one training step), bounded performance metric normalized to [0,1]; heterogeneous curve lengths and possible divergence/stagnation behaviors.",
            "environment_complexity": "Benchmarks use finite tabular configuration spaces of size up to 1000 per task (LCBench and Taskset 1000 configurations), hyperparameter dimensionalities from 4 up to 8 (Taskset variants) or 7 (LCBench) and up to 10 supported by FT-PFN; learning-curve lengths vary by task (5 to 1414 steps reported), HPO budget used in experiments B = 1000 unit steps (≈20 full trainings for 52-epoch tasks). Action space: choose one configuration to thaw among up to ~1000 candidates each step.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "ifBO yields state-of-the-art HPO performance on the three benchmark families: outperforms DPL and DyHPO on LCBench and Taskset and competes closely on PD1; has the best average rank across benchmarks. FT-PFN+MFPI-random achieves superior anytime behavior and dominates pairwise after ≈150 steps. (Quantitative HPO metrics are reported as normalized regret and average rank across tasks; specific numeric regrets per task are in Appendix F.)",
            "performance_without_adaptation": null,
            "sample_efficiency": "Designed for low-budget regime; demonstrated strong performance with total budgets B=1000 freeze-thaw steps (≈20 full trainings for LCBench/Taskset). Empirically dominates baselines in this few-evaluations regime and shows improved performance after ~150 steps.",
            "exploration_exploitation_tradeoff": "Managed by MFPI-random which randomizes extrapolation horizon h and improvement threshold T each iteration, effectively sampling from a portfolio of PI acquisitions; this hedges between short-term (small h) exploitation and long-term (large h) exploration, and between low-risk (small τ) and high-risk (large τ) thresholds.",
            "comparison_methods": "Compared against DyHPO (deep-kernel GP freeze-thaw), DPL (deep-ensemble power-law freeze-thaw), GP-based freeze-thaw (Matérn GP), HyperBand, ASHA, uniform random search; also ablated with EI/PI one-step and EI/PI max acquisitions and variants.",
            "key_results": "Combining an in-context PFN surrogate with a randomized MFPI acquisition yields: (1) substantially better posterior predictive quality (higher log-likelihood and lower MSE) than DyHPO and DPL across benchmarks; (2) major inference speedups (FT-PFN inference 10–100x faster than DyHPO/DPL depending on context size); (3) improved HPO results in the low-budget regime (ifBO achieves SOTA or competitive results on LCBench, Taskset, PD1); (4) MFPI-random is more robust across benchmarks than fixed-horizon EI/PI acquisitions because it hedges across horizons and thresholds.",
            "limitations_or_failures": "Requires performance metric and hyperparameter values normalized to [0,1] and supports up to 10 hyperparameters; FT-PFN trained for budgets B ≤ 1000 (scaling beyond this is untested); combinations of FT-PFN with EI-based acquisitions can fail (heavy-tailed predictive posteriors cause EI to over-explore and catastrophically under-exploit in very low-budget settings); not yet demonstrated at scale for modern large-scale DL pretraining.",
            "uuid": "e1287.0",
            "source_info": {
                "paper_title": "In-Context Freeze-Thaw Bayesian Optimization for Hyperparameter Optimization",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "FT-PFN",
            "name_full": "Freeze-Thaw Prior-Data Fitted Network (FT-PFN)",
            "brief_description": "A transformer-based prior-data fitted network trained on synthetic learning-curve priors to produce in-context posterior predictive distributions for learning-curve extrapolation without any online retraining, used as the dynamic surrogate in freeze-thaw BO.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "FT-PFN (surrogate)",
            "agent_description": "A sequence Transformer (6 layers, embed 512, 4 heads, hidden 1024, ~14.69M params) that ingests tokens of the form (hyperparameters, normalized budget t, observed performance) as context and outputs a discretized posterior predictive density (1000 bins) for target (hyperparameter, budget) pairs; trained on 2.0M synthetic datasets sampled from a generative curve prior that models realistic learning-curve shapes including saturation and divergence.",
            "adaptive_design_method": "Used as the probabilistic model component within an adaptive Bayesian optimization loop (freeze-thaw BO); itself performs in-context Bayesian prediction (metalearned Bayesian inference).",
            "adaptation_strategy_description": "FT-PFN does not adapt its weights online. Instead, it adapts predictions to the observed partial data by taking H (set of observed (λ,b,y) tuples) as input in-context and producing posterior predictive distributions for candidate continuations; these predictions are then used by the acquisition function to choose the next action.",
            "environment_name": "Partial learning-curve observation setting within HPO benchmarks (LCBench, PD1, Taskset)",
            "environment_characteristics": "Partial/limited observations (subsets of per-configuration learning curves), noisy/bounded metrics (Gaussian noise clipped to [0,1]), heterogeneous budgets and variable curve shapes including possible divergence or stagnation, discrete finite configuration spaces.",
            "environment_complexity": "Context sizes tested up to ~1800 samples; meta-training limited to N=1000 tokens per dataset; supports up to M=10 hyperparameters; typical benchmarks have up to 1000 configurations and learning-curve lengths up to 1414.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Predictive performance (example at context size ~1000): log-likelihood median across tasks: LCBench=2.118, PD1=1.133, Taskset=3.016; MSE: LCBench=0.004, PD1=0.024, Taskset=0.004. Inference runtime (single-CPU evaluation reported): ~0.719s for 1000-sample contexts. Achieves 10–100x speedups over DPL/DyHPO depending on context size.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Maintains high predictive quality as context size grows; trained on synthetic data so requires no online samples to adapt weights, only needs observed partial curves at test time for in-context prediction; demonstrated effective with contexts from a few hundred to ~1800 samples.",
            "exploration_exploitation_tradeoff": "FT-PFN provides calibrated uncertainty estimates which the acquisition uses to trade off exploration/exploitation; heavy-tailed posterior predictions can increase exploration if used with EI, motivating MFPI-random's threshold/horizon randomization to stabilize behavior.",
            "comparison_methods": "Compared against DyHPO's deep Gaussian process surrogate and DPL's deep ensemble of power laws; ablated against a variant of FT-PFN with no-hyperparameter inputs.",
            "key_results": "FT-PFN produces superior posterior predictive quality (higher log-likelihoods, lower MSE) and much faster inference than DyHPO and DPL across three benchmarks, despite being trained only on synthetic curve priors; its uncertainty estimates are more calibrated, which improves acquisition performance when paired with an appropriate AF (MFPI-random).",
            "limitations_or_failures": "Requires normalization of performance and hyperparameters to [0,1]; trained/meta-trained for budgets up to 1000 and at most 10 hyperparameters; predictive heavy tails can harm performance with EI acquisitions (leads to over-exploration).",
            "uuid": "e1287.1",
            "source_info": {
                "paper_title": "In-Context Freeze-Thaw Bayesian Optimization for Hyperparameter Optimization",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "DyHPO",
            "name_full": "DyHPO (Dynamic and efficient gray-box hyperparameter optimization for deep learning)",
            "brief_description": "A prior freeze-thaw BO implementation that uses a learned deep kernel Gaussian process surrogate and a multi-fidelity Expected Improvement (one-step EI) acquisition to decide which runs to continue.",
            "citation_title": "Dynamic and efficient gray-box hyperparameter optimization for deep learning",
            "mention_or_use": "use",
            "agent_name": "DyHPO",
            "agent_description": "Freeze-thaw BO using a deep-kernel Gaussian Process surrogate that predicts one-step-ahead performance and uses a one-step Expected Improvement acquisition (greedy) to select the next configuration to continue.",
            "adaptive_design_method": "Bayesian optimization (freeze-thaw) with deep-kernel GP surrogate and one-step Expected Improvement acquisition",
            "adaptation_strategy_description": "DyHPO refits its surrogate online using the history of observed partial learning curves and uses one-step EI to select the configuration with the highest expected improvement one step ahead, favoring short-horizon greedy gains.",
            "environment_name": "Same HPO benchmarks (LCBench, Taskset, PD1) in freeze-thaw setting",
            "environment_characteristics": "Partially observable (partial learning curves), noisy observations, preemptible training, discrete finite configuration spaces; DyHPO targets one-step ahead extrapolation.",
            "environment_complexity": "Benchmarks with up to 1000 configurations, learning-curve lengths vary per task; surrogate retraining incurs computational overhead (reported runtimes higher than FT-PFN; see Table 1).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Predictive quality per Table 1 (example context=1000): log-likelihood medians: LCBench = -0.368, PD1 = -0.457, Taskset = -0.381; MSE example at context 1000: LCBench=0.012, PD1=0.071, Taskset=0.008. Runtime (context 1000): ~59.95s (single-CPU reported). In HPO experiments, DyHPO is a strong baseline but is generally outperformed by ifBO; DyHPO performs well when paired with its one-step greedy acquisition in some settings (e.g., PD1).",
            "performance_without_adaptation": null,
            "sample_efficiency": "Requires online surrogate retraining after steps (incurs computational cost); performance depends on fidelity of one-step predictions and retraining frequency.",
            "exploration_exploitation_tradeoff": "Relies on one-step EI (greedy, short horizon) which emphasizes near-term exploitation; limited horizon can reduce exploratory continuation of promising but slow-improving runs.",
            "comparison_methods": "Compared in paper against DPL, FT-PFN/ifBO, GP-based FT-BO, HyperBand, ASHA, random search; ablation includes using MFPI-random acquisition with DyHPO surrogate.",
            "key_results": "DyHPO's deep-kernel GP is competent but provides lower log-likelihood and slower inference than FT-PFN; the one-step EI acquisition is effective for DyHPO's one-step surrogate but less robust across benchmarks when combined with FT-PFN's calibrated uncertainty.",
            "limitations_or_failures": "Struggles to extrapolate beyond a single step effectively (limits multi-step planning); surrogate retraining online is computationally expensive; lower log-likelihoods relative to FT-PFN indicate poorer uncertainty calibration.",
            "uuid": "e1287.2",
            "source_info": {
                "paper_title": "In-Context Freeze-Thaw Bayesian Optimization for Hyperparameter Optimization",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "DPL",
            "name_full": "DPL (Deep Power Laws for Hyperparameter Optimization)",
            "brief_description": "A freeze-thaw BO agent that fits a deep ensemble of parametric power-law learning-curve models per configuration and uses Expected Improvement at max budget (EI-max) as acquisition.",
            "citation_title": "Deep power laws for hyperparameter optimization",
            "mention_or_use": "use",
            "agent_name": "DPL",
            "agent_description": "Surrogate is a deep ensemble of power-law curve fitters (power-law basis functions) producing predictive distributions; acquisition uses EI evaluated at maximum budget (greedy toward eventual final performance). The ensemble is trained/updated online during the freeze-thaw run.",
            "adaptive_design_method": "Bayesian optimization (freeze-thaw) with a deep-ensemble parametric learning-curve surrogate and EI at maximum budget acquisition",
            "adaptation_strategy_description": "DPL fits/fine-tunes deep ensembles of power-law curve models to the observed partial learning curves (online) and uses the surrogate to predict final performance (b_max) and select configurations via EI at max budget.",
            "environment_name": "HPO freeze-thaw benchmarks (LCBench, Taskset, PD1)",
            "environment_characteristics": "Partially observed learning curves, assumed to follow power-law-like parametric trends (strong modeling assumption), stochastic observations, discrete configuration spaces.",
            "environment_complexity": "Benchmarks with up to 1000 configurations and varied learning-curve lengths; ensemble training and online updates incur runtime overhead (see Table 1 runtimes).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Predictive quality per Table 1 (context=1000): log-likelihood medians: LCBench = -11.983, PD1 = -11.017, Taskset = -20.350; MSEs comparable on Taskset but worse log-likelihoods (overconfident/poor uncertainty). Runtime (context 1000): ~41.96s (single-CPU reported). In HPO experiments DPL is a competitive baseline and in some situations (e.g., early Taskset) can perform well but is overall outperformed by ifBO in aggregated measures.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Requires online fitting and ensemble inference which is computationally heavier; ensemble size used in referenced work was small (n=5) which contributed to poor uncertainty calibration in experiments.",
            "exploration_exploitation_tradeoff": "Uses EI at maximum budget which focuses on predicted eventual performance (long-horizon exploitation); may under-explore in some settings, and its strong power-law assumptions can lead to overly confident incorrect predictions.",
            "comparison_methods": "Compared to DyHPO, FT-PFN/ifBO, GP-based FT-BO, HyperBand, ASHA, random search; ablations include using MFPI-random acquisition with DPL surrogate.",
            "key_results": "DPL's parametric power-law ensemble can achieve good point predictions in some benchmarks (Taskset) but shows very poor log-likelihoods (overconfident) and is significantly slower in inference than FT-PFN; its strong curve-shape assumptions can be limiting when real curves violate power-law forms.",
            "limitations_or_failures": "Strong power-law assumptions can produce overly confident incorrect predictions; small ensemble sizes worsen uncertainty estimates; online retraining introduces computational overhead; performs worse than ifBO on aggregated HPO metrics though it can be strong on some specific benchmarks/early budgets.",
            "uuid": "e1287.3",
            "source_info": {
                "paper_title": "In-Context Freeze-Thaw Bayesian Optimization for Hyperparameter Optimization",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Freeze-thaw Bayesian optimization",
            "rating": 2
        },
        {
            "paper_title": "Dynamic and efficient gray-box hyperparameter optimization for deep learning",
            "rating": 2
        },
        {
            "paper_title": "Deep power laws for hyperparameter optimization",
            "rating": 2
        },
        {
            "paper_title": "Transformers can do Bayesian inference",
            "rating": 2
        },
        {
            "paper_title": "Efficient bayesian learning curve extrapolation using prior-data fitted networks",
            "rating": 2
        },
        {
            "paper_title": "Speeding up automatic Hyperparameter Optimization of deep neural networks by extrapolation of learning curves",
            "rating": 1
        },
        {
            "paper_title": "Learning curve prediction with Bayesian neural networks",
            "rating": 1
        }
    ],
    "cost": 0.016521,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>In-Context Freeze-Thaw Bayesian Optimization for Hyperparameter Optimization</h1>
<p>Herilalaina Rakotoarison<em> ${ }^{</em> 1}$ Steven Adriaensen ${ }^{<em> 1}$ Neeratyoy Mallik ${ }^{</em> 1}$<br>Samir Garibov ${ }^{1}$ Edward Bergman ${ }^{1}$ Frank Hutter ${ }^{12}$</p>
<h4>Abstract</h4>
<p>With the increasing computational costs associated with deep learning, automated hyperparameter optimization methods, strongly relying on black-box Bayesian optimization (BO), face limitations. Freeze-thaw BO offers a promising grey-box alternative, strategically allocating scarce resources incrementally to different configurations. However, the frequent surrogate model updates inherent to this approach pose challenges for existing methods, requiring retraining or fine-tuning their neural network surrogates online, introducing overhead, instability, and hyper-hyperparameters. In this work, we propose FT-PFN, a novel surrogate for Freezethaw style BO. FT-PFN is a prior-data fitted network (PFN) that leverages the transformers' in-context learning ability to efficiently and reliably do Bayesian learning curve extrapolation in a single forward pass. Our empirical analysis across three benchmark suites shows that the predictions made by FT-PFN are more accurate and 10-100 times faster than those of the deep Gaussian process and deep ensemble surrogates used in previous work. Furthermore, we show that, when combined with our novel acquisition mechanism (MFP I-random), the resulting in-context freeze-thaw BO method (ifBO), yields new state-of-the-art performance in the same three families of deep learning HPO benchmarks considered in prior work.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>1. Introduction</h2>
<p>Hyperparameters are essential in deep learning (DL) to achieve strong model performance. However, due to the increasing complexity of these models, it is becoming more challenging to find promising hyperparameter settings, even with the help of hyperparameter optimization (HPO, Section 3.1) tools (Feurer \&amp; Hutter, 2019; Bischl et al., 2023). Traditional HPO techniques using Bayesian Optimization (BO) are unsuitable for modern DL because they treat the problem as a black box, making it computationally expensive, requiring a full model training for each evaluation.</p>
<p>Recent research in HPO has shifted towards multi-fidelity methods (Li et al., 2017; 2020a; Falkner et al., 2018; Klein et al., 2020; Li et al., 2020b; Awad et al., 2021), utilizing lower fidelity proxies (e.g., training for fewer steps, using less data, smaller models) and only evaluating the most promising hyperparameter settings at the full fidelity. While these methods have potential, they often use coarse-grained fidelity spaces and rely on the rank correlation of performances across fidelities. Moreover, they do not always fully utilize the anytime nature of algorithms such as checkpointing, continuation, and extrapolation. As a result, these methods struggle to allocate computational resources efficiently, leading to suboptimal performance in many scenarios.</p>
<p>The freeze-thaw BO method (Section 3.2), originally proposed by Swersky et al. (2014), is a promising approach to efficiently allocate computational resources by pausing and resuming the evaluation of different hyperparameter configurations. This method is an improvement over traditional grey-box methods as it dynamically manages resources, and recent implementations (Wistuba et al., 2022; Kadra et al., 2023) hold the state-of-the-art in the low-budget regime ( $\sim$ 20 full function evaluations). However, these contemporary implementations have limitations. In particular, they rely on online learning to update the surrogate model at each step which can lead high computational overhead, instability, and complexity in managing additional hyper-hyperparameters. Moreover, they also suffer from strong assumptions about learning curves, which may not be applicable in all scenarios, resulting in overly confident incorrect predictions.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Comparison of freeze-thaw surrogate model predictions, given the same set of hyperparameters (HPs) and their partial learning curves. The Ground truth curves show the real learning curves with dots ( $\cdot$ ) indicating the points observed as training set or context for all the surrogates. ifBO uses FT-PFN as its surrogate, which requires no refitting but instead uses the training dots as context for inferring the posterior predictive distribution of the model performance obtained at step $b$ using any set of given HPs. Surrogates used in prior art, using Deep Power Laws Ensembles (DPL) and Deep Kernel Gaussian Process (DyHPO) respectively, are trained on the training set till convergence and then used to extrapolate the given partial curves. The bottom row shows for each surrogate, the probabilistic performance predictions made at step 50 (last step in top row), with the stars ( $\star$ ) indicating the true value of the curve.</p>
<p>In this work, we leverage prior-data fitted networks (Müller et al., 2022, PFNs) (Section 3.3), a Transformer-based metalearning approach to Bayesian inference, to enhance freezethaw BO through in-context learning. Our PFN model (FT-PFN) infers the task-specific relationship between hyperparameter settings and their learning curves in a single forward pass, eliminating the need for online training during the search. Figure 1 compares learning curves extrapolation, including uncertainty, by our model (FT-PFN) and two baselines. Beyond demonstrating superior extrapolation quality, our model directly addresses key challenges in traditional HPO methods, lowering computational overhead and stabilizing the optimization process. The contributions of this paper are as follows:</p>
<ul>
<li>We propose FT-PFN (Section 4.1), a new surrogate model for freeze-thaw BO, replacing online learning with in-context learning using PFNs. We train a single instance of FT-PFN model exclusively on synthetic data, generated from a curve prior designed to mimic realistic HPO learning curves.</li>
<li>We empirically show that FT-PFN outperforms existing surrogates, at point prediction and posterior distribution approximation, while being over an order of magnitude faster, and despite never having been trained on real HPO data (Section 5.1).</li>
<li>We combine FT-PFN with a novel acquisition function (MFPI-random, Section 4.2) and find that the resulting in-context freeze-thaw method (ifBO) yields a new state-of-the-art performance on three benchmark suites for HPO for deep learning (LCBench, Taskset, PD1).</li>
</ul>
<p>The code for the surrogate PFN training and reproducing experiments from this paper, is available at: https:// github.com/automl/ifBO.</p>
<h2>2. Related Work</h2>
<p>Multi-fidelity hyperparameter optimization uses lowcost approximations of the objective function, for example, by evaluating only a few epochs of model training. A notable approach in this category is Hyperband (Li et al., 2017), which iteratively allocates a similar budget to various candidate hyperparameter settings and retains only the most promising ones, to be evaluated at a higher budget. Hyperband was extended for efficient parallelization (Li et al., 2020a), Bayesian optimization (Falkner et al., 2018) and evolutionary search (Awad et al., 2021) and to incorporate user priors (Mallik et al., 2023a). However, a key limitation of these multi-fidelity approaches is that they do not allow the continuation of previously discarded runs in the light of new evidence, resulting in a waste of computational resources. Additionally, their effectiveness heavily depends on a good manual choice of fidelity levels and a strong correlation between the ranks of hyperparameter settings at low and high fidelities (e.g., no crossings of learning curves).</p>
<p>A promising research direction to mitigate these limitations involves predicting performance at higher fideli-</p>
<p>ties. Domhan et al. (2015) addressed this by proposing a Bayesian learning curve extrapolation (LCE) method. Then, Klein et al. (2017) extended the latter approach to jointly model learning curves and hyperparameter values with Bayesian Neural Networks. Non-Bayesian versions of LCE have also been explored by Chandrashekaran \&amp; Lane (2017) and Gargiani et al. (2019). Despite the robust extrapolation capabilities of these LCE methods, fully leveraging them in guiding HPO remains a challenge.</p>
<p>Freeze-Thaw Bayesian Optimization offers a promising solution to address the limitations of standard multi-fidelity and LCE-based HPO. Its ability to pause and resume optimization runs enables the efficient management of computational resources and ensures solid anytime performance. As a BO method, it incorporates a surrogate model to predict performance at higher fidelities and relies on the acquisition function to guide the search. The freeze-thaw concept was introduced by Swersky et al. (2014), who used a Gaussian process with exponential decay kernel for modeling learning curves and an entropy search acquisition function. Wistuba et al. (2022) recently proposed DyHPO, an improved version that uses a learned deep kernel combined with a multi-fidelity-based Expected Improvement (EI) acquisition. Most recently, Kadra et al. (2023) introduced DPL, another surrogate model that utilizes a deep ensemble of power laws and EI at the maximum budget as acquisition function. These Freeze-Thaw BO methods substantially improve upon standard multi-fidelity approaches, showing particularly strong performance in low-budget settings. Nevertheless, these methods share a common challenge: The necessity for online training of a surrogate model during the search, which can be computationally intensive and may cause optimization instabilities.</p>
<p>In-context Learning (ICL) is an exciting new learning paradigm that offers a promising alternative to online learning methods. A key advantage of ICL is that it does not require retraining/fine-tuning the model with new data, but instead, the data is fed to the model as a contextual prompt. ICL first gained a lot of interest with the rise of Transformerbased models like large language models (Radford et al., 2019, LLMs) and has since also been explored as an end-toend approach to black box HPO (Chen et al., 2022).</p>
<p>Prior data fitted networks, proposed by Müller et al. (2022), are transformer-based models that are trained to do in-context Bayesian prediction. They have been successfully used as an in-context classifier for tabular data (Hollmann et al., 2023), an in-context surrogate model for black-box HPO (Müller et al., 2023), an in-context model for Bayesian learning curve extrapolation (Adriaensen et al., 2023), and an in-context time-series forecaster (Dooley et al., 2023). Our approach draws on and expands these prior works to
create an efficient in-context surrogate model for freezethaw BO.</p>
<h2>3. Preliminaries</h2>
<p>We now discuss some preliminaries that we build on more formally, introducing our notation along the way.</p>
<h3>3.1. Hyperparameter Optimization (HPO)</h3>
<p>Consider an iterative machine learning training pipeline with configurable hyperparameters $\lambda \in \Lambda$. E.g., when training a neural network using gradient descent we could configure the learning rate, weight decay, dropout rate, etc. Let $\mathrm{f}\left(\lambda, b_{\lambda}\right)$ be some measure of downstream performance (e.g., validation accuracy) of the model obtained, using hyperparameter settings $\lambda$, after $b_{\lambda}$ iterations of training, that we would like to maximize. HPO aims to find a hyperparameter setting producing the best model, i.e., $\lambda^{<em>}:(\lambda^{</em>}, \cdot) \in \operatorname{argmax}<em _lambda="\lambda">{\lambda \in \Lambda, 1 \leq b \leq B} \mathrm{f}(\lambda, b)$, within the limited total optimization budget of $B$ iterations. Note that in modern deep learning the budget available for HPO often does not allow us to execute more than a few full training runs. In this setting, the crux of HPO lies in allocating these limited training resources to the most promising hyperparameter settings, i.e., to find an allocation $\left{b</em>\right}<em _lambda="\lambda">{\lambda \in \Lambda}$ with $b</em> \leq B$ that maximizes $\max } \geq 0$ and $\sum_{\lambda \in \Lambda} b_{\lambda<em _lambda="\lambda">{\lambda \in \Lambda, 1 \leq b \leq b</em>(\lambda, b)$.}} \mathrm{f</p>
<h3>3.2. Freeze-Thaw Bayesian Optimization</h3>
<p>As discussed above, Swersky et al. (2014) proposed to address this challenge by following a fine-grained dynamic scheduling approach, allocating resources to configurations (and observing their performance) "one step at a time". Here, one "step" corresponds to $\mathbb{k}<em _max="{max" _text="\text">{\mathrm{b}} \geq 1$ iterations of model training (e.g., one epoch). Further, assume the maximum number of steps allocated to any single configuration $\lambda$ to be limited to $\mathrm{b}</em>}}$ (i.e., training runs are limited to $\mathrm{b<em _mathrm_b="\mathrm{b">{\text {max }} \cdot \mathbb{k}</em>$ trading off exploration and exploitation (Algorithm 1, line 7). The crucial difference with traditional black box Bayesian Optimization lies in that resources in the freeze-thaw framework are allocated one step, rather than one full training run, at the time. Note, this framework assumes training to be preemptive, i.e., that we can stop (freeze) a model training, and continue (thaw) it later; this assumption is reasonable in modern DL where checkpointing is common practice. Also}}$ iterations $\approx$ training compute per configuration). Algorithm 1 shows the freeze-thaw Bayesian optimization framework that uses its history $H$, i.e., the various partial learning curves observed thus far in the current partial allocation, to fit a dynamic Bayesian surrogate model $\mathcal{M}$ that probabilistically extrapolates the partially seen performance of configuration $\lambda$ beyond $b_{\lambda}$ (Algorithm 1, line 6). Following the BO framework, it decides which of these to continue (or start if $b_{\lambda}=0$ ) using a dynamic acquisition policy $\mathcal{A</p>
<p>Algorithm 1 Freeze-thaw Bayesian Optimization. Blue comments detail ifBO specifics.
Input: $\Lambda$ : configuration space,
f: measure of model performance to be maximized, $\mathbb{1}<em _max="{max" _text="\text">{\mathrm{b}}$ : iterations of model training per freeze-thaw step, $\mathrm{b}</em>$ : maximal steps for any configuration $\lambda \in \Lambda$, $B$ : total HPO budget in iterations of model training.
Components:
$\mathcal{M}$ : the dynamic surrogate model (FT-PFN),
$\mathcal{A}$ : the dynamic acquisition policy (MFPI-random, Alg. 2)
Output: $\lambda^{*} \in \Lambda$, obtaining the best observed performance
Procedure: $\operatorname{HPO}\left(\Lambda, \mathrm{f}, \mathbb{1}}<em _max="\max">{\mathrm{b}}, \mathrm{b}</em>, B\right)$ :</p>
<div class="codehilite"><pre><span></span><code><span class="err">\</span><span class="p">(</span><span class="nx">b_</span><span class="p">{</span><span class="err">\</span><span class="nx">lambda</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="k">forall</span><span class="w"> </span><span class="err">\</span><span class="nx">lambda</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="err">\</span><span class="nx">Lambda</span><span class="err">\</span><span class="p">)</span>
<span class="err">\</span><span class="p">(</span><span class="mi">2</span><span class="p">:</span><span class="w"> </span><span class="err">\</span><span class="nx">lambda</span><span class="w"> </span><span class="err">\</span><span class="nx">sim</span><span class="w"> </span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">U</span><span class="p">}(</span><span class="err">\</span><span class="nx">Lambda</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">quad</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">initial</span><span class="w"> </span><span class="nx">random</span><span class="w"> </span><span class="nx">sample</span>
<span class="err">\</span><span class="p">(</span><span class="nx">b_</span><span class="p">{</span><span class="err">\</span><span class="nx">lambda</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbb</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">b</span><span class="p">}}</span><span class="err">\</span><span class="p">)</span>
<span class="err">\</span><span class="p">(</span><span class="nx">H</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">lambda</span><span class="p">,</span><span class="w"> </span><span class="nx">b_</span><span class="p">{</span><span class="err">\</span><span class="nx">lambda</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">f</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">lambda</span><span class="p">,</span><span class="w"> </span><span class="nx">b_</span><span class="p">{</span><span class="err">\</span><span class="nx">lambda</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">quad</span><span class="w"> </span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nx">evaluate</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">f</span><span class="p">}(</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nx">train</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">lambda</span><span class="w"> </span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">first</span><span class="w"> </span><span class="nx">step</span><span class="w"> </span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="k">while</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="o">|</span><span class="nx">H</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="nx">lvert</span><span class="err">\</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbb</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">b</span><span class="p">}&lt;</span><span class="nx">B</span><span class="err">\</span><span class="nx">right</span><span class="p">.</span><span class="err">\</span><span class="nx">right</span><span class="p">.</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">    </span><span class="nx">Train</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">M</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">H</span><span class="w"> </span><span class="err">\</span><span class="nx">quad</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">FT</span><span class="o">-</span><span class="nx">PFN</span><span class="w"> </span><span class="nx">requires</span><span class="w"> </span><span class="nx">no</span><span class="w"> </span><span class="nx">model</span><span class="w"> </span><span class="nx">fitting</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">lambda</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">A</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">Lambda</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">M</span><span class="p">},</span><span class="w"> </span><span class="nx">H</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="o">~</span><span class="nx">b</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="err">\</span><span class="nx">max</span><span class="w"> </span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">quad</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">select</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">lambda</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">thaw</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">b_</span><span class="p">{</span><span class="err">\</span><span class="nx">lambda</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">b_</span><span class="p">{</span><span class="err">\</span><span class="nx">lambda</span><span class="p">}</span><span class="o">+</span><span class="err">\</span><span class="nx">mathbb</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">b</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">quad</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">thaw</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">lambda</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">one</span><span class="w"> </span><span class="nx">step</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">y</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">f</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">lambda</span><span class="p">,</span><span class="w"> </span><span class="nx">b_</span><span class="p">{</span><span class="err">\</span><span class="nx">lambda</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">quad</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">evaluate</span><span class="w"> </span><span class="nx">f</span>
<span class="err">\</span><span class="p">(</span><span class="nx">H</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">H</span><span class="w"> </span><span class="err">\</span><span class="nx">cup</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">lambda</span><span class="p">,</span><span class="w"> </span><span class="nx">b_</span><span class="p">{</span><span class="err">\</span><span class="nx">lambda</span><span class="p">},</span><span class="w"> </span><span class="nx">y</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="nx">end</span><span class="w"> </span><span class="k">while</span>
</code></pre></div>

<p>12: return $\lambda^{<em>}:\left(\lambda^{</em>}, \cdot\right) \in \operatorname{argmax}<em _lambda="\lambda">{\lambda \in \Lambda, 1 \leq b \leq b</em>(\lambda, b)$
note, that black box BO can be recovered as a special case, by setting $\mathrm{b}}} \mathrm{f<em _mathrm_b="\mathrm{b">{\text {max }}$, the maximum number of steps allocated to any single configuration, to one, and $\mathbb{1}</em>$ to the maximum budget available for any single training run.}</p>
<p>For simplicity, in the remainder, we assume the budget step $\mathbb{1}_{\mathrm{b}}$ is set to 1 , and the output of f to be bounded in $[0,1]$.</p>
<h3>3.3. Prior-data Fitted Networks (PFNs)</h3>
<p>As briefly discussed above, PFNs (Müller et al., 2022) are neural networks $q_{\theta}$ that are trained to do Bayesian prediction for supervised learning in a single forward pass. More specifically, let $D=D_{\text {train }} \cup\left{\left(x_{\text {test }}, y_{\text {test }}\right)\right}$ be a dataset used for training; the PFN's parameters $\theta$ are optimized to take $D_{\text {train }}$ and $x_{\text {test }}$ as inputs and make predictions that approximate the posterior predictive distribution (PPD) of the output label $y_{\text {test }}$ :</p>
<p>$$
q_{\theta}\left(x_{\text {test }}, D_{\text {train }}\right) \approx \mathbb{P}\left(y_{\text {test }} \mid x_{\text {test }}, D_{\text {train }}\right)
$$</p>
<p>in expectation over datasets $D$ sampled from a prior $p(\mathcal{D})$ over datasets. At test time, the PFN does not update its parameters given a training dataset $D_{\text {train }}$, but rather takes $D_{\text {train }}$ as a contextual input, predicting the labels of unseen examples through in-context learning. The PFN is pretrained once for a specific prior $p(\mathcal{D})$ and used in downstream Bayesian prediction tasks without further fine-tuning. More specifically, it is trained to minimize the cross-entropy for predicting the hold-out example's label $y_{\text {test }}$, given $x_{\text {test }}$
and $D_{\text {train }}$ :</p>
<p>$$
\begin{gathered}
\ell_{\theta}=\mathbb{E}\left[-\log q_{\theta}\left(y_{\text {test }} \mid x_{\text {test }}, D_{\text {train }}\right)\right] \
\text { with } \quad\left{\left(x_{\text {test }}, y_{\text {test }}\right)\right} \cup D_{\text {train }} \sim p(\mathcal{D})
\end{gathered}
$$</p>
<p>Müller et al. (2022) proved that this training procedure coincides with minimizing the KL divergence between the PFN's predictions and the true PPD.</p>
<h2>4. In-Context Freeze-Thaw BO (ifBO)</h2>
<p>In this section, we describe ifBO, the in-context learning variant of the freeze-thaw framework that we propose as an alternative for the existing online learning implementations (Wistuba et al., 2022; Kadra et al., 2023). As can be seen in Algorithm 1 (line 6), the critical difference lies in the fact that we do not need to refit our surrogate model after every allocation step. By skipping the online refitting stage, we reduce computational overhead, code complexity, and hyper-hyperparameters. Note that within the freeze-thaw BO framework described in Section 3.2, our method is fully characterized by our choice of surrogate model (Section 4.1) and dynamic acquisition policy (Section 4.2).</p>
<h3>4.1. Dynamic Surrogate Model (FT-PFN)</h3>
<p>We propose FT-PFN a prior-data fitted network (Müller et al., 2022, PFNs) trained to be used as an in-context dynamic surrogate model in the freeze-thaw framework. As described in Section 3.3, PFNs represent a general metalearned approach to Bayesian prediction, characterized by the data used for meta-training. Following previous works using PFNs (Müller et al., 2022; Hollmann et al., 2023; Müller et al., 2023; Adriaensen et al., 2023; Dooley et al., 2023), we train the PFN only on synthetically generated data, allowing us to generate virtually unlimited data and giving us full control over any biases therein. We aim to train FT-PFN on artificial data $(D)$ that resembles the real performance data we observe in the context of freeze-thaw Bayesian optimization, i.e., a collection of learning curves of training runs for the same task, but using different hyperparameter settings, i.e.,</p>
<p>$$
\bigcup_{\lambda \in \Lambda}\left{((\lambda, 1), \mathrm{f}(\lambda, 1)), \ldots,\left(\left(\lambda, \mathrm{b}<em _max="\max">{\max }\right), \mathrm{f}\left(\lambda, \mathrm{~b}</em>\right)\right)\right}
$$</p>
<p>Prior desiderata: From a Bayesian perspective, we want to generate data from a prior data model that captures our beliefs on the relationship between hyperparameters $\lambda$, training budget $b$, and model performance $\mathrm{f}(\lambda, b)$. While one could design such prior for a specific HPO scenario, our goal here is to construct a generic prior, resulting in an FT-PFN surrogate for general HPO. To this end, we leverage general beliefs, e.g., we expect learning curves to be noisy, but to exhibit an improving, convex, and converging trend; curves</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Diagram for the prior data model described in Section 4.1 that was used to generate data for meta-training FT-PFN. On the left, we have the randomly initialized neural network $\pi_{\text {config }}$ that models the relationship between a hyperparameter setting $\lambda$ and its learning curve (shown in pink), whose output parameterizes a curve model $\pi_{\text {curve }}$ that is a linear combination of $K$ ( $\approx 2$ in this illustration) basis functions (shown in red and blue) with added $\lambda$-specific Gaussian noise with variance $\sigma^{2}$.
on the same task are expected to have similar start, saturation, and convergence points; and training runs using similar hyperparameter settings to produce similar learning curves.</p>
<p>Prior data model: Following Klein et al. (2017) and Kadra et al. (2023), we model the performance curve of a hyperparameter $\lambda$ using a parametric curve model $\pi_{\text {curve }}$, whose parameters are sampled from an another prior model $\pi_{\text {config }}$ taking the hyperparameter $\lambda$ as input (see Figure 2). Following Domhan et al. (2015) and Adriaensen et al. (2023), we define $\pi_{\text {curve }}$ as a weighted combination of $K$ basis functions $f_{k}$, with additive Gaussian noise. Thus, the parameters of $\pi_{\text {curve }}$ include $\mathcal{E}$ (the set of parameters and weight of all basis functions), along with $\sigma^{2}$ (noise). As for $\pi_{\text {config }}$, we adopt a neural network with weights $\theta$. Unlike previous works, we do not train the weights $\theta$ of this neural network. Instead, we randomly initialize the network, to represent a task-specific relationship between hyperparameters and their learning curves, which we then use to generate data for training FT-PFN. This can be viewed as generating samples from a Bayesian Neural Network (BNN) prior, meta-training FT-PFN to emulate LCNet-like (Klein et al., 2017) BNN inference through in-context learning.</p>
<p>Formally, we define the performance of a hyperparameter $\lambda$ at a training time $t$ as follows:
$\pi_{\text {curve }}(\lambda, t) \sim \mathcal{N}\left(f_{\text {comb }}(t ; \mathcal{E}), \sigma^{2}\right)$
with $\quad f_{\text {comb }}(t ; \mathcal{E})=y_{0}+\left(y_{\infty}-y_{0}\right) \cdot \sum_{k=1}^{K} w_{k} f_{k}\left(t ; \Psi_{k}\right)$
and $\quad\left(\sigma^{2},\left(\underbrace{\left(y_{\infty}, w_{1}, \ldots, w_{K}, \Psi_{1}, \ldots, \Psi_{K}\right)}<em _config="{config" _text="\text">{\mathcal{E}}\right) \sim \pi</em>(\lambda ; \theta)\right.$,
where $y_{0}$ is the initial model performance ${ }^{1}$, and $y_{\infty}$ that at convergence. $w_{k}$ is the weight of basis curve $f_{k}$, and $\Psi_{k}$ its basis function specific parameters. In this work, we adopt four different basis functions $(K=4)$, each having four parameters, resulting in a total of $22(=|\mathcal{E}|+1)$ parameters depending on $\lambda$ through $\pi_{\text {config }}$. Our four basis functions subsume the power law model used by Kadra et al. (2023), all three basis functions used by Adriaensen et al. (2023), and 9 of the 11 basis functions originally proposed by Domhan et al. (2015). ${ }^{2}$ Furthermore, unlike those considered in previous works, our basis functions can have a breaking point (Caballero et al., 2023) at which convergence stagnates or performance diverges, resulting in a more heterogeneous and realistic model.}</p>
<p>To sample a collection of curves on the same task from our prior data model, we (i) sample their hyperparameters from a unit hypercube; (ii) initialize $\pi_{\text {config }}$ as in (Müller et al., 2023); then (iii) apply Equation 2 to obtain the performance of each hyperparameter $\lambda$ at a given training time $t$. To reduce the entropy of this prior (and PFN training time), we assume the training time $t$ to be normalized in $[0,1]$. In practice, we also sample a maximum training time $\mathrm{b}<em b="b">{\text {max }}$ per task and define $t</em>}=\frac{b}{\mathrm{~b<em _max="{max" _text="\text">{\text {max }}}$ for $b \in\left[1, \mathrm{~b}</em>\right]$. Further details about meta-training FT-PFN can be found in Appendix A, including the basis curves, their parameters, and illustrations of samples from our learning curve prior (Appendix A.1); a detailed description of the procedure we use for generating our meta-training data (Appendix A.2); and the architecture and hyperparameters used (Appendix A.3).}</p>
<h3>4.2. Dynamic Acquisition Policy (MFPI-random)</h3>
<p>Following the Freeze-Thaw Bayesian Optimization framework (Section 3.2), we continue training the configuration that maximizes some acquisition function (AF), i.e., $\operatorname{argmax}_{\lambda \in \Lambda} \operatorname{AF}(\lambda)$. Conceptually, we would like to continue the training that is most likely to quickly produce a model performing substantially better than the best model obtained thus far. This notion can be formalized as an acquisition function,</p>
<p>$$
\operatorname{MFPI}(\lambda ; h, T)=\mathbb{P}\left(\mathcal{M}\left(\lambda, \min \left(b_{\lambda}+h, \mathrm{~b}_{\max }\right)\right)&gt;T\right)
$$</p>
<p>which evaluates to the predicted likelihood that a candidate configuration $\lambda \in \Lambda$ after $h$ more steps of training obtains a model exceeding the $T$ performance threshold. Here, $\mathcal{M}$ is the trained surrogate model, and the extrapolation horizon $1 \leq h \leq \mathrm{b}<em _best="{best" _text="\text">{\text {max }}$ is a parameter controlling the tradeoff between immediate and long-term gains, i.e. what is quick enough, and the threshold $f</em>$ and}} \leq T&lt;1$ a parameter controlling the trade-off between high and low risk/gain, i.e., what improvement is substantial. Note that for $h=\mathrm{b}_{\text {max }</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>$T=f_{\text {best }}$, we recover the Probability of Improvement (PI) acquisition function (Mockus et al., 1978). The values we choose for these hyper-hyperparameters will affect which configuration gets continued (see Figure 6, Appendix A.4). Their optimal settings depend on the desired freeze-thaw behavior and are not straightforward to determine. It might even be beneficial to adjust them dynamically during the run. Instead of using a fixed performance threshold $T$ or a fixed extrapolation horizon $h$ (Wistuba et al., 2022; Kadra et al., 2023), we explore a range of possible thresholds and horizons by randomizing these. Such random sampling procedure is undertaken every freeze-thaw BO iteration and is akin to an AF selection from a portfolio of different MFPIs. We posit that this hedging with a portfolio of AFs (portfolio of Equation 3 with different $h, T$ ) in each iteration benefits freeze-thaw setups where queries are more granular than standard BO. The result is a simple, parameter-free AF with a balanced exploration-exploitation trade-off,</p>
<p>$$
\begin{array}{r}
\operatorname{MFPI} \text {-random }(\lambda)=\operatorname{MFPI}\left(\lambda ; h^{\text {rand }}, T^{\text {rand }}\right) \
\text { with } h^{\text {rand }} \sim \mathcal{U}\left(1, \mathrm{~b}<em _best="{best" _text="\text">{\max }\right) \text { and } \
T^{\text {rand }}=f</em>\right) \
\text { with } \log _{10}\left(\tau^{\text {rand }}\right) \sim \mathcal{U}(-4,-1)
\end{array}
$$}}+\tau^{\text {rand }} \cdot\left(1-f_{\text {best }</p>
<p>Further details, as well as pseudo code (Algorithm 2) can be found in Appendix A.4.</p>
<h2>5. Experiments</h2>
<p>In this section, we compare i fBO to state-of-the-art multifidelity freeze-thaw Bayesian optimization methods. To this end, we first assess FT-PFN in terms of the quality and cost of its prediction (Section 5.1). Then, we assess our approach, which combines FT-PFN with our MFPI-random acquisition function, on HPO tasks (Section 5.2). Finally, we conduct an ablation study on acquisition function used in i fBO (Section 5.3).</p>
<p>We conduct our experiments on three benchmarks: LCBench (Zimmer et al., 2021), PD1 (Wang et al., 2021), and Taskset (Metz et al., 2020). These benchmarks, covering different architectures (Transformers, CNNs, MLPs) and tasks (NLP, vision, tabular data), are commonly used in the HPO literature. A detailed overview of the tasks included in each benchmark is presented in Appendix B.</p>
<p>Our main baselines are both other recent freeze-thaw approaches: DyHPO (Wistuba et al., 2022) and DPL (Kadra et al., 2023). We reimplement the above two baselines in order to allow ablation of the online learning surrogates with different acquisition functions. Refer to Appendix C for more details.</p>
<h3>5.1. Cost and Quality of Predictions</h3>
<p>In this section, we compare the predictive capabilities of FT-PFN to that of existing surrogate models, including the deep Gaussian process of DyHPO (Wistuba et al., 2022) and the deep ensemble of power laws model of DPL (Kadra et al., 2023). We also consider a variant of FT-PFN trained on the same prior, but not taking the hyperparameters as input (referred to as "no HPs"). This variant bases its predictions solely on a set of partially observed learning curves.</p>
<p>Evaluation procedure: From a given benchmark, we sample both a set of partial curves, where each curve has its own set of target epochs. The selection process is strategically designed to encompass a wide range of scenarios, varying from depth-first approaches, which involve a smaller number of long curves, to breadth-first approaches, where multiple shorter curves are explored. Additional details on the sampling strategy can be found in Appendix A.2. To assess the quality of the predictions, we utilize two metrics: log-likelihood (log-score, the higher the better), measuring the approximation of the posterior distribution ( $\sim$ uncertainty calibration), and mean squared error (the lower the better), measuring the accuracy of point predictions. We also report the runtime, accounting for fitting and inference of each surrogate. The evaluation was run on a single Intel Xeon 6242 CPU.</p>
<p>Results discussion: Table 1 presents the log-likelihood and MSE (Mean Squared Error) for each approach relative to the context sample size. As expected, we observe an increase in log-likelihood and a decrease in MSE as the context size get larger. Notably, FT-PFN and its No HPs variant significantly outperform DPL and DyHPO in terms of log-likelihood. DPL in particular has low log-likelihood values, corresponding to a poor uncertainty estimate such as being overly confident in incorrect predictions. This may be due to the very low ensemble size $(=5)$ adopted by (Kadra et al., 2023) compounded by their strong power law assumption. On the other hand, DyHPO struggles with low log-likelihood due to its inability to extrapolate beyond a single step effectively. Regarding MSE, FT-PFN generally surpasses the baselines in LCBench and PD1, performing comparably to DPL on Taskset.</p>
<p>Beyond the impressive log-likelihood and MSE results, our approach also yield significant speed advantages over the baseline methods. Importantly, FT-PFN maintains superiority in quality and speed for inferences with more than 1000 samples as a context without not being trained in this regime. Depending on the context sample size, our method achieves speedups ranging from $10 \times$ to $100 \times$ faster than DPL and DyHPO.</p>
<p>Table 1. Comparison of FT-PFN, a variant of FT-PFN that excludes hyperparameters, DyHPO and DPL across three benchmarks. Values represent the median over tasks of the log-likelihood and mean squared error (MSE) as well as the runtime of predictions.</p>
<table>
<thead>
<tr>
<th># samples</th>
<th>Method</th>
<th>LCBench</th>
<th></th>
<th>PD1</th>
<th></th>
<th>Taskset</th>
<th></th>
<th>Runtime (s)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Log-likelihood</td>
<td>MSE</td>
<td>Log-likelihood</td>
<td>MSE</td>
<td>Log-likelihood</td>
<td>MSE</td>
<td></td>
</tr>
<tr>
<td>400</td>
<td>DPL</td>
<td>$-14.577$</td>
<td>0.007</td>
<td>$-13.384$</td>
<td>0.043</td>
<td>$-26.011$</td>
<td>0.005</td>
<td>17.686</td>
</tr>
<tr>
<td></td>
<td>DyHPO</td>
<td>$-0.481$</td>
<td>0.042</td>
<td>$-0.573$</td>
<td>0.104</td>
<td>$-0.465$</td>
<td>0.009</td>
<td>16.860</td>
</tr>
<tr>
<td></td>
<td>FT-PFN (no HPs)</td>
<td>1.649</td>
<td>0.008</td>
<td>0.983</td>
<td>0.028</td>
<td>2.860</td>
<td>0.005</td>
<td>0.215</td>
</tr>
<tr>
<td></td>
<td>FT-PFN</td>
<td>1.876</td>
<td>0.005</td>
<td>0.925</td>
<td>0.030</td>
<td>2.934</td>
<td>0.004</td>
<td>0.225</td>
</tr>
<tr>
<td>800</td>
<td>DPL</td>
<td>$-13.291$</td>
<td>0.007</td>
<td>$-11.721$</td>
<td>0.037</td>
<td>$-21.779$</td>
<td>0.005</td>
<td>33.480</td>
</tr>
<tr>
<td></td>
<td>DyHPO</td>
<td>$-0.426$</td>
<td>0.031</td>
<td>$-0.510$</td>
<td>0.088</td>
<td>$-0.419$</td>
<td>0.008</td>
<td>64.809</td>
</tr>
<tr>
<td></td>
<td>FT-PFN (no HPs)</td>
<td>1.701</td>
<td>0.007</td>
<td>1.103</td>
<td>0.024</td>
<td>2.835</td>
<td>0.005</td>
<td>0.527</td>
</tr>
<tr>
<td></td>
<td>FT-PFN</td>
<td>2.044</td>
<td>0.004</td>
<td>1.072</td>
<td>0.025</td>
<td>2.975</td>
<td>0.004</td>
<td>0.541</td>
</tr>
<tr>
<td>1000</td>
<td>DPL</td>
<td>$-11.983$</td>
<td>0.007</td>
<td>$-11.017$</td>
<td>0.035</td>
<td>$-20.350$</td>
<td>0.004</td>
<td>41.956</td>
</tr>
<tr>
<td></td>
<td>DyHPO</td>
<td>$-0.368$</td>
<td>0.012</td>
<td>$-0.457$</td>
<td>0.071</td>
<td>$-0.381$</td>
<td>0.008</td>
<td>59.949</td>
</tr>
<tr>
<td></td>
<td>FT-PFN (no HPs)</td>
<td>1.763</td>
<td>0.007</td>
<td>1.120</td>
<td>0.024</td>
<td>2.877</td>
<td>0.005</td>
<td>0.687</td>
</tr>
<tr>
<td></td>
<td>FT-PFN</td>
<td>2.118</td>
<td>0.004</td>
<td>1.133</td>
<td>0.024</td>
<td>3.016</td>
<td>0.004</td>
<td>0.719</td>
</tr>
<tr>
<td>1400</td>
<td>DPL</td>
<td>$-11.333$</td>
<td>0.007</td>
<td>$-10.353$</td>
<td>0.033</td>
<td>$-17.760$</td>
<td>0.004</td>
<td>56.576</td>
</tr>
<tr>
<td></td>
<td>DyHPO</td>
<td>$-0.361$</td>
<td>0.011</td>
<td>$-0.438$</td>
<td>0.061</td>
<td>$-0.374$</td>
<td>0.008</td>
<td>112.168</td>
</tr>
<tr>
<td></td>
<td>FT-PFN (no HPs)</td>
<td>1.733</td>
<td>0.007</td>
<td>1.225</td>
<td>0.021</td>
<td>2.874</td>
<td>0.005</td>
<td>1.084</td>
</tr>
<tr>
<td></td>
<td>FT-PFN</td>
<td>2.137</td>
<td>0.003</td>
<td>1.201</td>
<td>0.022</td>
<td>3.042</td>
<td>0.004</td>
<td>1.130</td>
</tr>
<tr>
<td>1800</td>
<td>DPL</td>
<td>$-9.182$</td>
<td>0.007</td>
<td>$-9.263$</td>
<td>0.035</td>
<td>$-13.712$</td>
<td>0.004</td>
<td>73.435</td>
</tr>
<tr>
<td></td>
<td>DyHPO</td>
<td>$-0.365$</td>
<td>0.009</td>
<td>$-0.437$</td>
<td>0.058</td>
<td>$-0.381$</td>
<td>0.008</td>
<td>166.491</td>
</tr>
<tr>
<td></td>
<td>FT-PFN (no HPs)</td>
<td>1.753</td>
<td>0.006</td>
<td>1.251</td>
<td>0.019</td>
<td>2.858</td>
<td>0.005</td>
<td>1.635</td>
</tr>
<tr>
<td></td>
<td>FT-PFN</td>
<td>2.199</td>
<td>0.003</td>
<td>1.192</td>
<td>0.022</td>
<td>3.057</td>
<td>0.004</td>
<td>1.715</td>
</tr>
</tbody>
</table>
<h3>5.2 Assessment of HPO performance</h3>
<p>In this section, we present an extensive empirical comparison, showing the merits of our method on HPO tasks across a variety of tabular benchmarks (see, Appendix B) in the low budget regime. Additionally to DPL and DyHPO, we include Hyperband, ASHA, Gaussian process-based FT-BO (using DyHPO's one-step EI acquisition function) and uniform random search, as baselines (see, Appendix C). Each algorithm is allocated a total budget of 1000 steps for every task, which corresponds to 20 full trainings on LCBench and Taskset with each task being repeated 10 times with different seeds, each time starting from a different random configuration (see Algorithm 1). For PD1 tasks have varying maximum steps allowed ( $\mathrm{b}_{\text {max }}$ ), but for consistency and fair aggregation across benchmarks, we applied the same HPO budget of 1000 here too. We report two complementary metrics: The normalized regret, capturing performance differences, and the average rank of each method, capturing the relative order. Formally, the normalized regret corresponds to a $[0,1]$ normalization of the observed error w.r.t to the best (lowest) and worst (highest) errors recorded by all algorithms on the task.</p>
<p>Results discussion: Figure 3 presents the comparative results per benchmark family. The results validate the superiority of freeze-thaw approaches (ifBO, DyHPO, and DPL) compared to standard approaches (Hyperband, ASHA, and
random search) for low-budget settings. Most notably, these results establish the promise of ifBO, which either outperforms (on LCBench and Taskset) or competes closely (on PD1) with DPL and DyHPO. ifBO is also consistently the best on average rank across all benchmarks (see Appendix D.2, Figure 8). Appendix F (Figures 12-17) offers a closer look at the raw error metrics for each algorithm per task. These detailed results collectively confirm the robustness of ifBO for HPO tasks, showing its ability to compete if not outperform the most competitive baselines.</p>
<h3>5.3 Ablation on Acquisition</h3>
<p>In this section, we evaluate how ifBO performs in combination with other acquisition functions and aim to assess to what extent our novel acquisition function described in Section 4.2 contributes to its HPO success. To this end, we compare against ifBO variants combining FT-PFN with the EI-based acquisitions used in prior-art (Wistuba et al., 2022; Kadra et al., 2023), i.e., EI (one step) predicting one step in the future (DyHPO), and EI (max) predicting at the highest budget $b_{\max }$ (DPL). We also include their PI counterparts PI (one step) and PI (max), as well as variants of MFPI-random that only vary the prediction horizon, PI (random horizon) with $T=0$, or only vary the threshold, PI (max, random-T) with $h=b_{\text {max }}$. Apart from the methods compared, the experimental setup is identical to that in Section 5.2.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Comparison of our method against state-of-the-art baselines on all 3 benchmarks. First row shows normalized regret aggregated across multiple tasks in each benchmark (See Appendix B for benchmark details, and the results per task can be found in Appendix F). Second row shows the average ranks of each method.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Results of an ablation study of the acquisition function in ifBO on each benchmark family. First row shows normalized regret aggregated across multiple tasks in each benchmark (Appendix B). Second row shows the average ranks of each method.</p>
<p>Results discussion: Figure 4 shows the comprehensive results for our ifBO variants for each of the benchmarks, in terms of average ranks and average normalized regrets, aggregated across all tasks. Generally, we find that performance varies strongly between acquisitions, suggesting this choice is at least as important for HPO success as our surrogate's superior predictive quality. In particular, we find that combinations with the EI-based acquisitions EI (one step) and EI (max) proposed in prior- art, are amongst the worst-performing variants, both in terms of rank and regret. For example, EI (max) fails on LCBench and PD1, while EI (one step) fails on PD1 and Taskset. Curiously, these trends do not seem to extend to our baselines, e.g., DPL using EI (max) performs strongly on LCBench and DyHPO using EI (one step) performs strongly on PD1. We conjecture that this failure is related to the (justified) lack of confidence FT-PFN has about its predictions, as is evident by the superior log-scores</p>
<p>in Table 1. As a result, the predicted posterior will be heavy-tailed, resulting in the high EI values for those configurations our predictions are least confident for. While this drives exploration, in the very low budget regime, it can easily lead to a catastrophic failure to exploit. Overall, we find that while some variants are successful at specific tasks in early stages of the optimization, none exhibit the same robustness in performance across benchmarks, making MFPI-random the clear winner.</p>
<p>We perform comparable ablation studies for DPL and DyHPO, as detailed in Appendix D.3, to demonstrate the benefits of randomizing the horizon and the threshold.</p>
<h2>6. Conclusion</h2>
<p>In this paper, we proposed FT-PFN a novel surrogate for freeze-thaw Bayesian optimization. We showed that the point and uncertainty estimates produced by FT-PFN through in-context learning are superior to those obtained by fitting/training recently proposed deep Gaussian process (Wistuba et al., 2022) and deep power law ensemble (Kadra et al., 2023) models, while being over an order of magnitude faster. We presented the first-ever empirical comparison of different freeze-thaw implementations. Our results confirm the superiority of these HPO methods, in the low-budget regime, and show that our in-context learning approach is competitive with the state-of-the-art.</p>
<p>Despite our promising results, we admit that attaining the sample efficiency required to scale up to modern deep learning (e.g., LLM pretraining) remains a challenging endeavor. Future work should attempt to extend our approach to take advantage of additional sources of prior information, e.g., to do in-context meta-learning, leveraging learning curves on related tasks (Ruhkopf et al., 2022); to incorporate user priors (Müller et al., 2023; Mallik et al., 2023a); and additional information about the training process (e.g., gradient statistics). An alternative to scaling up is scaling in parallel. Here, we expect our in-context learning approach to reduce the overhead further, as the online learning/refitting stage occurs on the critical path, while in-context learning during prediction can easily be parallelized. Finally, the current FT-PFN model has some limitations. First, it requires the performance metric and each hyperparameter value to be normalized in $[0,1]$; and supports up to 10 hyperparameters. While we believe that this is reasonable, future work building systems should push these limits, train larger models, on more data, and explore ways to scale to larger context sizes. In summary, there is a lot left unexplored, and we hope that the relative simplicity, efficiency, and public availability of our method, lowers the threshold for future research in this direction.</p>
<h2>Impact Statement</h2>
<p>This paper presents work whose goal is to advance the field of hyperparameter optimization (HPO) in machine learning. There are many potential societal consequences of machine learning, none which we feel must be specifically highlighted here. We would, however, like to highlight that our work makes HPO more robust and efficient, and will thus help make machine learning more reliable and sustainable.</p>
<h2>Acknowledgements</h2>
<p>We thank Johannes Hog for his feedback on an earlier version of the paper. Frank Hutter is a Hector Endowed Fellow at the ELLIS Institute Tübingen. All authors acknowledge funding by the state of Baden-Württemberg through bwHPC, the German Research Foundation (DFG) through grant numbers INST 39/963-1 FUGG and 417962828, and the European Union (via ERC Consolidator Grant Deep Learning 2.0, grant no. 101045765), TAILOR, a project funded by EU Horizon 2020 research and innovation programme under GA No 952215. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them.</p>
<h2>Funded by <br> the European Union</h2>
<h2>References</h2>
<p>Adriaensen, S., Rakotoarison, H., Müller, S., and Hutter, F. Efficient bayesian learning curve extrapolation using prior-data fitted networks. In Proceedings of the 37th International Conference on Advances in Neural Information Processing Systems (NeurIPS'23), 2023.</p>
<p>Awad, N., Mallik, N., and Hutter, F. DEHB: Evolutionary hyberband for scalable, robust and efficient Hyperparameter Optimization. In Zhou, Z. (ed.), Proceedings of the 30th International Joint Conference on Artificial Intelligence (IJCAI'21), pp. 2147-2153, 2021.</p>
<p>Bischl, B., Binder, M., Lang, M., Pielok, T., Richter, J., Coors, S., Thomas, J., Ullmann, T., Becker, M., Boulesteix, A., Deng, D., and Lindauer, M. Hyperparameter optimization: Foundations, algorithms, best practices, and open challenges. pp. e1484, 2023.</p>
<p>Bojar, O., Chatterjee, R., Federmann, C., Haddow, B., Huck, M., Hokamp, C., Koehn, P., Logacheva, V., Monz, C., Negri, M., Post, M., Scarton, C., Specia, L., and Turchi, M. Findings of the 2015 workshop on statistical machine translation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pp. 1-46, Lisbon, Portugal, September 2015. Association for Computational Linguistics. URL http://aclweb.org/anthology/ W15-3001.</p>
<p>Caballero, E., Gupta, K., Rish, I., and Krueger, D. Broken neural scaling laws. In International Conference on Learning Representations (ICLR'23), 2023. Published online: iclr.cc.</p>
<p>Chandrashekaran, A. and Lane, I. R. Speeding up hyperparameter optimization by extrapolation of learning curves using previous builds. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 477-492. Springer, 2017.</p>
<p>Chelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., and Koehn, P. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005, 2013.</p>
<p>Chen, Y., Song, X., Lee, C., Wang, Z., Zhang, Q., Dohan, D., Kawakami, K., Kochanski, G., Doucet, A., Ranzato, M., Perel, S., and de Freitas, N. Towards learning universal hyperparameter optimizers with transformers. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Proceedings of the 36th International Conference on Advances in Neural Information Processing Systems (NeurIPS'22), 2022.</p>
<p>Deng, L. The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6):141-142, 2012.</p>
<p>Domhan, T., Springenberg, J., and Hutter, F. Speeding up automatic Hyperparameter Optimization of deep neural networks by extrapolation of learning curves. In Yang, Q. and Wooldridge, M. (eds.), Proceedings of the 24th International Joint Conference on Artificial Intelligence (IJCAI'15), pp. 3460-3468, 2015.</p>
<p>Dooley, S., Khurana, G. S., Mohapatra, C., Naidu, S., and White, C. ForecastPFN: Synthetically-trained zero-shot forecasting. In Proceedings of the 37th International Conference on Advances in Neural Information Processing Systems (NeurIPS'23), 2023.</p>
<p>Falkner, S., Klein, A., and Hutter, F. BOHB: Robust and efficient Hyperparameter Optimization at scale. In Dy, J. and Krause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning (ICML'18), volume 80, pp. 1437-1446. Proceedings of Machine Learning Research, 2018.</p>
<p>Feurer, M. and Hutter, F. Hyperparameter Optimization. In Hutter, F., Kotthoff, L., and Vanschoren, J. (eds.), Automated Machine Learning: Methods, Systems, Challenges, chapter 1, pp. 3 - 38. Springer, 2019. Available for free at http://automl.org/book.</p>
<p>Gargiani, M., Klein, A., Falkner, S., and Hutter, F. Probabilistic rollouts for learning curve extrapolation across hyperparameter settings. In 6th ICML Workshop on Automated Machine Learning, 2019.</p>
<p>Gijsbers, P., LeDell, E., Poirier, S., Thomas, J., Bischl, B., and Vanschoren, J. An open source automl benchmark. In Eggensperger, K., Feurer, M., Hutter, F., and Vanschoren, J. (eds.), ICML workshop on Automated Machine Learning (AutoML workshop 2019), 2019.</p>
<p>Gilmer, J. M., Dahl, G. E., and Nado, Z. init2winit: a jax codebase for initialization, optimization, and tuning research, 2021. URL http://github.com/google/ init2winit.</p>
<p>He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the International Conference on Computer Vision and Pattern Recognition (CVPR'16), pp. 770-778, 2016.</p>
<p>Hollmann, N., Müller, S., Eggensperger, K., and Hutter, F. TabPFN: A transformer that solves small tabular classification problems in a second. In International Conference on Learning Representations (ICLR'23), 2023. Published online: iclr.cc.</p>
<p>Kadra, A., Janowski, M., Wistuba, M., and Grabocka, J. Deep power laws for hyperparameter optimization. In Proceedings of the 37th International Conference on Advances in Neural Information Processing Systems (NeurIPS'23), 2023.</p>
<p>Kingma, D., Salimans, T., and Welling, M. Variational dropout and the local reparameterization trick. In Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., and Garnett, R. (eds.), Proceedings of the 29th International Conference on Advances in Neural Information Processing Systems (NeurIPS'15), 2015.</p>
<p>Klein, A., Falkner, S., Springenberg, J., and Hutter, F. Learning curve prediction with Bayesian neural networks. In Proceedings of the International Conference on Learning Representations (ICLR'17), 2017. Published online: iclr.cc.</p>
<p>Klein, A., Tiao, L., Lienart, T., Archambeau, C., and Seeger, M. Model-based Asynchronous Hyperparameter and Neural Architecture Search. arXiv:2003.10865 [cs.LG], 2020.</p>
<p>Krizhevsky, A. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.</p>
<p>Lefaudeux, B., Massa, B., Liskovich, D., Xiong, W., Caggiano, W., Naren, S., Xu, M., Hu, J., Tintore, M., Zhang, S., Labatut, P., and Haziza, D. xformers: A modular and hackable transformer modelling library, 2022.</p>
<p>Li, J., Liu, Y., Liu, J., and Wang, W. Neural architecture optimization with graph VAE. arXiv:2006.10310 [cs.LG], 2020a.</p>
<p>Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., and Talwalkar, A. Hyperband: Bandit-based configuration evaluation for Hyperparameter Optimization. In Proceedings of the International Conference on Learning Representations (ICLR'17), 2017. Published online: iclr.cc.</p>
<p>Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., and Talwalkar, A. Hyperband: A novel bandit-based approach to Hyperparameter Optimization. 18(185):1-52, 2018.</p>
<p>Li, L., Jamieson, K., Rostamizadeh, A., Gonina, E., Bentzur, J., Hardt, M., Recht, B., and Talwalkar, A. A system for massively parallel hyperparameter tuning. In Dhillon, I., Papailiopoulos, D., and Sze, V. (eds.), Proceedings of Machine Learning and Systems 2, volume 2, 2020b.</p>
<p>Liao, Z. and Carneiro, G. Competitive multi-scale convolution. arXiv preprint arXiv:1511.05635, 2022.</p>
<p>Loshchilov, I. and Hutter, F. SGDR: Stochastic gradient descent with warm restarts. In Proceedings of the International Conference on Learning Representations (ICLR'17), 2017. Published online: iclr.cc.</p>
<p>Mallik, N., Bergman, E., Hvarfner, C., Stoll, D., Janowski, M., Lindauer, M., Nardi, L., and Hutter, F. Priorband:</p>
<p>Practical hyperparameter optimization in the age of deep learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a. URL https: //openreview.net/forum?id=uoiwugtpCH.</p>
<p>Mallik, N., Hvarfner, C., Bergman, E., Stoll, D., Janowski, M., Lindauer, M., Nardi, L., and Hutter, F. PriorBand: Practical hyperparameter optimization in the age of deep learning. In Proceedings of the 37th International Conference on Advances in Neural Information Processing Systems (NeurIPS'23), 2023b.</p>
<p>Metz, L., Maheswaranathan, N., Sun, R., Freeman, C. D., Poole, B., and Sohl-Dickstein, J. Using a thousand optimization tasks to learn hyperparameter search strategies. arXiv:2002.11887 [cs.LG], abs/, 2020.</p>
<p>Mockus, J., Tiesis, V., and Zilinskas, A. The application of Bayesian methods for seeking the extremum. Towards Global Optimization, 2(117-129), 1978.</p>
<p>Müller, S., Hollmann, N., Arango, S., Grabocka, J., and Hutter, F. Transformers can do Bayesian inference. In Proceedings of the International Conference on Learning Representations (ICLR'22), 2022. Published online: iclr.cc.</p>
<p>Müller, S., Feurer, M., Hollmann, N., and Hutter, F. Pfns4bo: In-context learning for bayesian optimization. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), Proceedings of the 40th International Conference on Machine Learning (ICML'23), volume 202 of Proceedings of Machine Learning Research. PMLR, 2023.</p>
<p>Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.</p>
<p>Ruhkopf, T., Mohan, A., Deng, D., Tornede, A., Hutter, F., and Lindauer, M. Masif: Meta-learned algorithm selection using implicit fidelity information. Transactions on Machine Learning Research, 2022.</p>
<p>Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A., and Fei-Fei, L. Imagenet large scale visual recognition challenge. 115(3):211-252, 2015.</p>
<p>Swersky, K., Snoek, J., and Adams, R. Freeze-thaw Bayesian optimization. arXiv:1406.3896 [stats.ML], 2014.</p>
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, L., and Polosukhin, I. Attention is all you need. In Guyon, I., von Luxburg, U., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Proceedings of the 31st International Conference</p>
<p>on Advances in Neural Information Processing Systems (NeurIPS'17). Curran Associates, Inc., 2017.</p>
<p>Wang, Z., Dahl, G., Swersky, K., Lee, C., Mariet, Z., Nado, Z., Gilmer, J., Snoek, J., and Ghahramani, Z. Pretrained Gaussian processes for Bayesian optimization. arXiv:2207.03084v4 [cs.LG], 2021.</p>
<p>Wistuba, M., Kadra, A., and Grabocka, J. Dynamic and efficient gray-box hyperparameter optimization for deep learning. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Proceedings of the 36th International Conference on Advances in Neural Information Processing Systems (NeurIPS'22), 2022.</p>
<p>Xiao, H., Rasul, K., and Vollgraf, R. Fashion-MNIST: a novel image dataset for benchmarking Machine Learning algorithms. arXiv:1708.07747v2 [cs.LG], 2017.</p>
<p>Zagoruyko, S. and Komodakis, N. Wide residual networks. In Richard C. Wilson, E. R. H. and Smith, W. A. P. (eds.), Proceedings of the 27th British Machine Vision Conference (BMVC), pp. 87.1-87.12. BMVA Press, 2016. ISBN 1-901725-59-6. doi: 10.5244/C.30.87. URL https://dx.doi.org/10.5244/C.30.87.</p>
<p>Zimmer, L., Lindauer, M., and Hutter, F. Auto-Pytorch: Multi-fidelity metalearning for efficient and robust AutoDL. 43:3079-3090, 2021.</p>
<h1>A. Further Details about ifBO</h1>
<h2>A.1. The Learning Curve Prior</h2>
<p>In this section, we continue the discussion of our learning curve prior defined by Equation 2:</p>
<p>$$
\begin{array}{rlrl}
\pi_{\text {curve }}(\lambda, t) \sim \mathcal{N}\left(f_{\text {comb }}(t ; \mathcal{E}), \sigma^{2}\right) &amp; &amp; \text { with } \quad f_{\text {comb }}(t ; \mathcal{E})=y_{0}+\left(y_{\infty}-y_{0}\right) \cdot \sum_{k=1}^{K} w_{k} f_{k}\left(t ; \Psi_{k}\right) \
&amp; \text { and } \quad\left(\sigma^{2},(\underbrace{\left(y_{\infty}, w_{1}, \ldots, w_{K}, \Psi_{1}, \ldots, \Psi_{K}\right.}<em _config="{config" _text="\text">{\mathcal{E}})\right) \sim \pi</em>(\lambda ; \theta)
\end{array}
$$}</p>
<p>Note that $\sigma^{2}$ and $\mathcal{E}$ are all outputs of the same neural network $\pi_{\text {config }}$. Due to the symmetry of this network, when marginalizing over $\lambda$ and $\theta$, all these parameters would have the same distribution. This is undesirable. To impose parameter-specific marginal distributions, we (i) estimate the empirical CDF of marginal output distribution; (ii) apply it to each output to obtain a new output with $\mathcal{U}(0,1)$ marginal distribution; (iii) apply the icdf of the parameter-specific target marginal distribution. Specifically, let $u_{1}, u_{2}, u_{3} \sim \mathcal{U}(0,1)$ be three i.i.d. uniform random variables that are hyperparameter independent and like $\theta$ are sampled once per task, then the non-basis curve specific parameters of our curve model are (marginally) distributed as follows:</p>
<p>$$
\begin{array}{ll}
y_{\infty} \sim \mathcal{U}\left(y_{0}, y_{\max }\right) \quad \text { with } \quad y_{0}=\min \left(u_{1}, u_{2}\right) \quad \text { and } \quad y_{\max }= \begin{cases}\max \left(u_{1}, u_{2}\right) &amp; \text { if } u_{3} \leq 0.25 \
1.0 &amp; \text { if } u_{3}&gt;0.25\end{cases} \
\log (\sigma) \sim \mathcal{N}(-5,1) \quad w_{k}=\frac{W_{k}}{W} \quad \text { with } \quad W_{k} \sim \operatorname{Gamma}(1,1) \quad \text { and } \quad W=\sum_{k=1}^{K} W_{k}
\end{array}
$$</p>
<p>Each of the basis curves takes the form</p>
<p>$$
f_{k}\left(t ; \Psi_{k}\right)=f_{k}^{\prime}\left(x_{t} ; \Psi_{k}\right) \quad \text { with } \quad x_{t}= \begin{cases}t &amp; \text { if } t \leq x_{\mathrm{sat}, k}^{\lambda} \ r_{\mathrm{sat}, k}^{\lambda}\left(t-x_{\mathrm{sat}, k}\right)+x_{\mathrm{sat}, k} &amp; \text { if } t&gt;x_{\mathrm{sat}, k}^{\lambda}\end{cases}
$$</p>
<p>where each $f_{k}$ has the following four parameters $\Psi_{k}$ :
$\alpha_{k}$ The skew of the curve, determining the convergence rate change over time.
$x_{\text {sat }, k}, y_{\text {sat }, k}$ The point at which model performance saturates and the convergence rate is suddenly reduced.
$r_{\text {sat }, k}$ The reduced convergence rate after saturation, which can be negative, modeling divergence.
and $f_{k}^{\prime}\left(x_{t}, \theta_{k}\right)$ is a [0,1] bounded monotonic growth function. The formulas for these growth functions, alongside the target distributions of their parameters, are listed in Table 2.</p>
<p>Finally, note that given these choices, we have $f_{\text {comb }}(t, \mathcal{E}) \in[0,1]$ and we clip the Gaussian noise in $\pi_{\text {curve }}(\lambda, t)$ in the same range. As a consequence, if performance does not naturally fall in this range, it must be normalized before passing it to FT-PFN. Examples of collections of curves generated using this prior can be found in Figure 5.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Reference name</th>
<th style="text-align: left;">Formula $f_{k}^{\prime}\left(x_{t} ; \Psi_{k}\right)$</th>
<th style="text-align: left;">Prior $p\left(\Psi_{k}\right)$</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">pow $_{4}$</td>
<td style="text-align: left;">$1-\left(\left(\epsilon_{1}^{\frac{\alpha_{1}}{\alpha_{1}}}-1\right) * \frac{x_{t}}{x_{\text {sat }, 1}}+1\right)^{-\alpha_{1}}$</td>
<td style="text-align: left;">$\ln \left(\alpha_{1}\right) \sim \mathcal{N}(1,1)$</td>
<td style="text-align: left;">$\log <em _sat="{sat" _text="\text">{10}\left(x</em>(0,1), \forall k$}, k}\right) \sim \mathcal{N</td>
</tr>
<tr>
<td style="text-align: left;">$\exp _{4}$</td>
<td style="text-align: left;">$1-\left(\epsilon_{2}\right)^{\left(\frac{\alpha_{1}}{\pi_{\text {sat }, 2}}\right)^{\alpha_{2}}}$</td>
<td style="text-align: left;">$\ln \left(\alpha_{2}\right) \sim \mathcal{N}(0,1)$</td>
<td style="text-align: left;">$\log _{1} 0(\epsilon) \sim \mathcal{U}(-3,0), \forall k$</td>
</tr>
<tr>
<td style="text-align: left;">$\operatorname{ilog}_{4}$</td>
<td style="text-align: left;">$1-\frac{\ln \left(\alpha_{3}\right)}{\ln \left(\left(\alpha_{3}^{\frac{1}{\alpha_{3}}}-\alpha_{3}\right) \frac{x_{t}}{\pi_{\text {sat }, 3}}+\alpha_{3}\right)}$</td>
<td style="text-align: left;">$\ln \left(\alpha_{3}-1\right) \sim \mathcal{N}(-4,1)$</td>
<td style="text-align: left;">$y_{\text {sat }, k}=y_{\infty}-\epsilon \cdot\left(y_{\infty}-y_{0}\right), \forall k$</td>
</tr>
<tr>
<td style="text-align: left;">hill $_{4}$</td>
<td style="text-align: left;">$1-\frac{1}{\left(\frac{\alpha_{1}}{\pi_{\text {sat }, 1}}\right)^{\alpha_{4}}\left(\frac{1}{\epsilon_{4}}-1\right)+1}$</td>
<td style="text-align: left;">$\ln \left(\alpha_{4}\right) \sim \mathcal{N}(0.5,0.25)$</td>
<td style="text-align: left;">$1-r_{\text {sat }, k} \sim \operatorname{Exp}(1), \forall k$</td>
</tr>
</tbody>
</table>
<p>Table 2. The formulas for each of the four basis functions in our curve prior. Note that each of them are normalized to start at 0 , converge to 1 , and pass through the saturation point.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Twenty-one i.i.d. samples of the FT-PFN prior, i.e., synthetically generated collections of learning curves for the same task using different hyperparameter configurations. In these examples, we consider 3 hyperparameters that are mapped onto the color of the curves, such that runs using similar hyperparameters, have similarly colored curves. We observe correlations, in varying degrees, between curves on the same task, especially with similar hyperparameter configurations.</p>
<h1>A.2. Meta-training Data Generating Procedure</h1>
<p>A single meta-training example in our setting corresponds to a training set $D_{\text {train }}$ and test set $D_{\text {test }}$, where $D_{\text {train }}=\bigcup_{\lambda \in \Lambda}\left{\left(\left(\lambda, \frac{b}{\mathrm{~b}<em _curve="{curve" _text="\text">{\text {test }}}\right), \pi</em>}}\left(\lambda, \frac{b}{\mathrm{~b<em b="1">{\text {test }}}\right)\right)\right}</em>}^{b_{\lambda}}$ corresponds to the (synthetic) partial learning curves observed thus far (i.e., the analog of $H$ at test time) and $D_{\text {test }} \subseteq \bigcup_{\lambda \in \Lambda}\left{\left(\left(\lambda, \frac{b}{\mathrm{~b<em _curve="{curve" _text="\text">{\text {test }}}\right), \pi</em>}}\left(\lambda, \frac{b}{\mathrm{~b<em b="b_{\lambda">{\text {test }}}\right)\right)\right}</em>}}^{\mathrm{b<em _text="\text" _train="{train">{\text {test }}}$ the extrapolation targets we want FT-PFN to predict. To keep the input size of FT-PFN fixed we choose $\left|D</em>}}\right|+\left|D_{\text {test }}\right|=N=1,000$ and vary the size of $\left|D_{\text {train }}\right| \sim \mathcal{U}(0, N-1)$. As $\mathrm{b<em _max="\max">{\text {max }}$ varies in practice, we sample it log-uniformly in $[1, N]$. Note that in the special case $b</em>\right}}=1$, we train FT-PFN for black box BO. $\Lambda=\left{\lambda_{i<em i="i">{i=1}^{N}$ is our synthetic configuration space with $\lambda</em>\right}} \sim \mathcal{U}(0,1)^{m}$, with $\left|\lambda_{i}\right|=m \sim \mathcal{U}(0, M)$ the dimensionality of our configuration space. We determine $b_{\lambda}$ by sampling a bag of $\left|D_{\text {train }}\right|$ elements from $\Lambda$ proportionally to weights $\left{w_{\lambda<em 10="10">{\lambda \in \Lambda}$ that follow a Dirichlet distribution with $\log </em>\right)$ using Equation 2 .}(\alpha) \sim \mathcal{U}(-4,-1)$ resulting in heterogeneous budget allocations that vary from breadth-first to depth-first. ${ }^{3}$ We use the same weights to sample another bag of $\left|D_{\text {test }}\right|$ determining the number of extrapolation targets for each $\lambda$, where each target $b$ is chosen $\mathcal{U}\left(b_{\lambda}, b_{\max }\right)$. Finally, to generate the corresponding performance observation/target, we first instantiate the random variables that are task-specific but do not depend on $\lambda$, i.e., $y_{0}, y_{\max }$ and the architecture and weights $\theta$ of the neural network $\pi_{\text {config }}$; and subsequently obtain $\pi_{\text {curve }}\left(\lambda, \frac{b}{\mathrm{~b}_{\text {test }}</p>
<p>Limitations: With these modeling choices come some limitations. First, FT-PFN is trained for HPO budgets $B \leq N=$ 1,000 ; requires the performance metric $f$ and each hyperparameter value to be normalized in [0,1]; and supports up to $M=10$ hyperparameters.</p>
<h2>A.3. Architecture and Hyperparameters</h2>
<p>Following Müller et al. (2022), we use a sequence Transformer (Vaswani et al., 2017) for FT-PFN and treat each tuple $\left(\lambda, t, \pi_{\text {curve }}(\lambda, t)\right)$ (for train) and $(\lambda, t)$ (for test) as a separate position/token. We do not use positional encoding such that we are permutation invariant. FT-PFN outputs a discretized approximation of the PPD, each output corresponding to the probability density of one of the equal-sized bins. We set the number of bins/outputs to 1,000 . For the transformer, we use 6 layers, an embedding size of 512 , four heads, and a hidden size of 1,024 , resulting in a total of 14.69 M parameters. We use a standard training procedure for all experiments, minimizing the cross-entropy loss from Equation 1 on 2.0M synthethic datasets generated as described in Section A.2, using the Adam optimizer (Kingma et al., 2015) (learning rate 0.0001 , batch size 25) with cosine annealing (Loshchilov \&amp; Hutter, 2017) with a linear warmup over the first $25 \%$ epochs of the training. Training took roughly 8 GPU hours on an RTX2080 GPU and the same FT-PFN is used in all experiments described in Section 5, without any retraining/fine-tuning.</p>
<h2>A.4. Acquisition function</h2>
<p>Algorithm 2 describes the acquisition procedure MFPI-random, used in ifBO. In each iteration of ifBO (L5-L11 in Algorithm 1), Algorithm 2 is invoked once taking as input the configuration space $\Lambda$, the surrogate model $\mathcal{M}$, the observed history $H$, and the maximal training steps $\mathrm{b}<em _lambda="\lambda">{\text {max }}$ of a configuration. First, the random horizon $h^{\text {rand }}$ and the scaled factor of improvement $\tau^{\text {rand }}$ (and thereby $T^{\text {rand }}$ ) are sampled once in every execution of the algorithm (L2-L3). This process can be seen as instantiating an acquisition function from a portfolio of multi-fidelity PIs. The choice of PI, the multi-fidelity component of extrapolating hyperparameters, and the random selection of an acquisition behaviour lends the naming of this acquisition function, MFPI - random. Then, for each candidate hyperparameter $\lambda \in \Lambda$, the performance of the hyperparameter at a total step of $b</em>$, with FT-PFN as a surrogate.}+h^{\text {rand }}$ is inferred, using the surrogate $\mathcal{M}$. Finally, the candidate with the highest obtained PI score is returned as the candidate solution to query next in the main Algorithm 1 loop. Figure 6 illustrates the behavior of MFPI - random w.r.t some values of $h^{\text {rand }}$ and $T^{\text {rand }</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Illustration of the MFPI acquisition (Equation 3). (Left) The figure shows a collection of partial learning curves and their corresponding continuations predicted by our FT-PFN model. Here again, we consider 3 hyperparameters whose values are mapped onto the color of the curves. (Right)The figure shows the color of the curve continued (i.e., maximizing MFPI) for different values of the horizon and threshold parameters. Note that the ranges shown (and scale used), match those sampled uniformly by MFPI-random (Equation 4) and consequently, the likelihood of continuing a specific curve is proportional to the surface area covered in this image by its corresponding color. Finally, note that the bright red color corresponds to starting a new curve.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 2 MFPI-random
Input: configuration space \(\Lambda\),
    probabilistic surrogate \(\mathcal{M}\)
    history of observations \(H\),
    maximal steps \(\mathrm{b}_{\text {max }}\)
</code></pre></div>

<p>Output: $\lambda \in \Lambda$, hyperparameter to evaluate next
Procedure MFPI - random $\left(\Lambda, \mathcal{M}, H, b_{\max }\right)$ :</p>
<div class="codehilite"><pre><span></span><code><span class="err">\</span><span class="o">(</span><span class="nt">f_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{best</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">max</span><span class="w"> </span><span class="err">\</span><span class="p">{</span><span class="err">y\</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">(\cdot,</span><span class="w"> </span><span class="err">\cdot,</span><span class="w"> </span><span class="err">y)</span><span class="w"> </span><span class="err">\in</span><span class="w"> </span><span class="err">H</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="nt">2</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="p">.</span><span class="nc">h</span><span class="o">^</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{rand</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">sim</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">U</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">1</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">mathrm</span><span class="p">{</span><span class="err">~b</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{max</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="nt">right</span><span class="o">.</span><span class="err">\</span><span class="o">)</span>
<span class="nt">3</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">T</span><span class="o">^</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{rand</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">=</span><span class="nt">f_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{best</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">+</span><span class="nt">10</span><span class="o">^</span><span class="p">{</span><span class="err">\tau^{\text</span><span class="w"> </span><span class="err">{rand</span><span class="w"> </span><span class="p">}</span><span class="err">}}</span><span class="w"> </span><span class="err">\</span><span class="nt">cdot</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">1-f_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{best</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">with</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">tau</span><span class="o">^</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{rand</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">sim</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">U</span><span class="p">}</span><span class="o">(</span><span class="nt">-4</span><span class="o">,</span><span class="nt">-1</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="nt">4</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">return</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">underset</span><span class="p">{</span><span class="err">\mathrm{b</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">in</span><span class="w"> </span><span class="err">\</span><span class="nt">Lambda</span><span class="err">}</span><span class="p">{</span><span class="err">\operatorname{argmax</span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">mathbb</span><span class="p">{</span><span class="err">P</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">M</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">lambda</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">min</span><span class="w"> </span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">b_</span><span class="p">{</span><span class="err">\lambda</span><span class="p">}</span><span class="o">+</span><span class="nt">h</span><span class="o">^</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{rand</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">mathrm</span><span class="p">{</span><span class="err">b</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{max</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="nt">H</span><span class="err">\</span><span class="nt">right</span><span class="o">)&gt;</span><span class="nt">T</span><span class="o">^</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{rand</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
</code></pre></div>

<p>best score seen in $H$
random horizon
random threshold scaling
to perform in-context learning we pass $H$ as input to FT-PFN</p>
<h1>B. Benchmarks</h1>
<p>Below, we enumerate the set of benchmarks we have considered. These benchmark cover a variety of optimization scenarios, including the model being optimized, the task for which it's being trained on, and the training metric with which to optimize hyperparameters with respect to. Notably, each of these benchmarks are tabular, meaning that the set of possible configurations to sample from is finite.</p>
<p>This choice of benchmarks is largely dictated by following the existing benchmarks used in prior work, especially the two primary baselines with which we compare to, DyHPO and DPL. These benchmarks were provided using mf-prior-bench ${ }^{4}$.</p>
<ul>
<li>LCBench (Zimmer et al., 2021) [DyHPO, DPL] - We use all 35 tasks available which represent the 7 integer and float hyperparameters of deep learning models from AutoPyTorch. Each task represents the 1000 possible configurations, trained for 52 epochs on a dataset taken from the AutoML Benchmark (Gijsbers et al., 2019). We drop the first epoch as suggested by the original authors.</li>
</ul>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 3. The 7 hyperparameters for all LCBenchtasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">name</th>
<th style="text-align: left;">type</th>
<th style="text-align: left;">values</th>
<th style="text-align: center;">info</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">batch_size</td>
<td style="text-align: left;">integer</td>
<td style="text-align: left;">$[16,512]$</td>
<td style="text-align: center;">log</td>
</tr>
<tr>
<td style="text-align: left;">learning_rate</td>
<td style="text-align: left;">continuous</td>
<td style="text-align: left;">$[0.0001,0.1]$</td>
<td style="text-align: center;">log</td>
</tr>
<tr>
<td style="text-align: left;">max_dropout</td>
<td style="text-align: left;">continuous</td>
<td style="text-align: left;">$[0.0,1.0]$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">max_units</td>
<td style="text-align: left;">integer</td>
<td style="text-align: left;">$[64,1024]$</td>
<td style="text-align: center;">log</td>
</tr>
<tr>
<td style="text-align: left;">momentum</td>
<td style="text-align: left;">continuous</td>
<td style="text-align: left;">$[0.1,0.99]$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">num_layers</td>
<td style="text-align: left;">integer</td>
<td style="text-align: left;">$[1,5]$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">weight_decay</td>
<td style="text-align: left;">continuous</td>
<td style="text-align: left;">$[1 e-05,0.1]$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<ul>
<li>Taskset (Metz et al., 2020) [DyHPO, DPL] This set benchmark provides 1000 diverse task on a variety of deep learning models on a variety of datasets and tasks. We choose the same 12 tasks as used in the DyHPO experimentation which consists of NLP tasks with purely numerical hyperparameters, mostly existing on a log scale. We additionally choose a 4 hyperparameter variant and an 8 hyperparameter variant, where the 4 hyperparameter variant is a super set of the former. This results in 24 total tasks that we use for the Taskset benchmark.
One exception that needs to be considred with this set of benchmarks is that the optimizers must optimize for is the model's log-loss. This metric has no upper bound, which contrasts to all other benchmarks, where the bounds of the metric are known a-priori. We note that in the DyHPO evaluation setup, they removed diverging curves as a benchmark preprocessing step, essentially side-stepping the issue that the response function for a given configuration mays return nans or out-of-distribution values. As our method requires bounded metrics, we make the assumption that a practitioner can provide a reasonable upper bound for the log loss that will be observed. By clampling to this upper bound, this effectively shrinks the range of values that our method will observe. As we are in a simulated benchmark setup, we must simulate this a-priori knowledge. We take the median value of at epoch 0 , corresponding to the median log loss of randomly initialized configurations that have not yet taken a gradient step. Any observed value that is nan or greater will then be clamped to this upper bound before being fed to the optimizer.</li>
</ul>
<p>Table 4. The 4 hyperparameter search space for Taskset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">name</th>
<th style="text-align: left;">type</th>
<th style="text-align: left;">values</th>
<th style="text-align: center;">info</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">beta1</td>
<td style="text-align: left;">continuous</td>
<td style="text-align: left;">$[0.0001,1.0]$</td>
<td style="text-align: center;">log</td>
</tr>
<tr>
<td style="text-align: left;">beta2</td>
<td style="text-align: left;">continuous</td>
<td style="text-align: left;">$[0.001,1.0]$</td>
<td style="text-align: center;">log</td>
</tr>
<tr>
<td style="text-align: left;">epsilon</td>
<td style="text-align: left;">continuous</td>
<td style="text-align: left;">$[1 e-12,1000.0]$</td>
<td style="text-align: center;">log</td>
</tr>
<tr>
<td style="text-align: left;">learning_rate</td>
<td style="text-align: left;">continuous</td>
<td style="text-align: left;">$[1 e-09,10.0]$</td>
<td style="text-align: center;">log</td>
</tr>
</tbody>
</table>
<p>Table 5. The 8 hyperparameter search space for Taskset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">name</th>
<th style="text-align: left;">type</th>
<th style="text-align: left;">values</th>
<th style="text-align: center;">info</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">beta1</td>
<td style="text-align: left;">continuous</td>
<td style="text-align: left;">$[0.0001,1.0]$</td>
<td style="text-align: center;">log</td>
</tr>
<tr>
<td style="text-align: left;">beta2</td>
<td style="text-align: left;">continuous</td>
<td style="text-align: left;">$[0.001,1.0]$</td>
<td style="text-align: center;">log</td>
</tr>
<tr>
<td style="text-align: left;">epsilon</td>
<td style="text-align: left;">continuous</td>
<td style="text-align: left;">$[1 e-12,1000.0]$</td>
<td style="text-align: center;">log</td>
</tr>
<tr>
<td style="text-align: left;">learning_rate</td>
<td style="text-align: left;">continuous</td>
<td style="text-align: left;">$[1 e-09,10.0]$</td>
<td style="text-align: center;">log</td>
</tr>
<tr>
<td style="text-align: left;">exponential_decay</td>
<td style="text-align: left;">continuous</td>
<td style="text-align: left;">$[9 e-07,0.0001]$</td>
<td style="text-align: center;">log</td>
</tr>
<tr>
<td style="text-align: left;">l1</td>
<td style="text-align: left;">continuous</td>
<td style="text-align: left;">$[1 e-09,10.0]$</td>
<td style="text-align: center;">log</td>
</tr>
<tr>
<td style="text-align: left;">l2</td>
<td style="text-align: left;">continuous</td>
<td style="text-align: left;">$[1 e-09,10.0]$</td>
<td style="text-align: center;">log</td>
</tr>
<tr>
<td style="text-align: left;">linear_decay</td>
<td style="text-align: left;">continuous</td>
<td style="text-align: left;">$[1 e-08,0.0001]$</td>
<td style="text-align: center;">log</td>
</tr>
</tbody>
</table>
<ul>
<li>PD1 (Wang et al., 2021) [DPL] These benchmarks were obtained from the output generated by HyperBO (Wang et al., 2021) using the dataset and training setup of (Gilmer et al., 2021). We choose a variety of tasks including the tuning of</li>
</ul>
<p>large vision ResNet (Zagoruyko \&amp; Komodakis, 2016) models on datasets such as CIFAR-10, CIFAR-100 (Krizhevsky, 2009) and SVHN (Liao \&amp; Carneiro, 2022) image classification datasets, along with training a ResNet (He et al., 2016) on the ImageNet (Russakovsky et al., 2015) image classification dataset. We also include some natural language processing tasks, notable transformers train on the LM1B (Chelba et al., 2013) statistical language modelling dataset, the XFormer (Lefaudeux et al., 2022) trained on the WMT15 German-English (Bojar et al., 2015) translation dataset and also a transformer trained to sequence prediction for protein modelling on the uniref50 dataset. Lastly, we also include a simple CNN trained on the MNIST (Deng, 2012) and Fashion-MNIST (Xiao et al., 2017) datasets.
Notably, all of these benchmarks share the same 4 deep learning hyperparameters given in table 6.</p>
<p>Table 6. The 4 hyperparameters for all PD1tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">name</th>
<th style="text-align: left;">type</th>
<th style="text-align: left;">values</th>
<th style="text-align: center;">info</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">lr_decay_factor</td>
<td style="text-align: left;">continuous</td>
<td style="text-align: left;">$[0.01,0.99]$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">lr_initial</td>
<td style="text-align: left;">continuous</td>
<td style="text-align: left;">$[1 e-05,10.0]$</td>
<td style="text-align: center;">log</td>
</tr>
<tr>
<td style="text-align: left;">lr_power</td>
<td style="text-align: left;">continuous</td>
<td style="text-align: left;">$[0.1,2.0]$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">opt_momentum</td>
<td style="text-align: left;">continuous</td>
<td style="text-align: left;">$[1 e-05,1.0]$</td>
<td style="text-align: center;">log</td>
</tr>
</tbody>
</table>
<p>Each benchmark ranges in the size of their learning curves, depending on the task, ranging from 5 to 1414. For each task, there are different variant based on a pair of dataset and batchsize. In total we evaluate our method on the 16 PD1 tasks below.</p>
<ul>
<li>WideResnet - Tuned on the CIFAR10, CIFAR100 datasets, each with a constant batch size of 256 and 2048. Also included is the SVHN dataset with a constant batch size 256 and 1024.</li>
<li>Resnet - Tuned on ImageNet with three constant batch sizes, 256, 512, and 1024.</li>
<li>XFormer - Tuned with a batch size of 2048 on the LM1B statistical language modelling dataset.</li>
<li>Transfomer Language Modelling - Tuned on the WMT15 German-English dataset with a batch size of 64.</li>
<li>Transformer Protein Modelling - Tuned on the uniref50 dataset with a batch size of 128.</li>
<li>Simple CNN - Tuned on MNIST and Fashion-MNIST with constant batch sizes of 256 and 2048 for each of them.</li>
</ul>
<h1>C. Baselines</h1>
<p>To use ifBO in practice for an HPO task, please refer to NePS $^{5}$. All our baselines were developed into the NePS framework that we forked and copied into our setup. Below, we describe the basic configuration of these baselines that were included in our experiments.</p>
<p>All baseline implementations can be found under neps in our experiment code available at: https://github.com/ automl/ifBO/tree/icml-2024.</p>
<h2>C.1. General baselines</h2>
<p>We chose random search based algorithms as baselines for the different benchmarks. This additionally also shows the utility of the different fidelity scheduling algorithms in HyperBand and ASHA which traverses the fidelity space in progressive geometric intervals, relying on strong performance correlation at these fidelity checkpoints. For these baselines, we chose the existing implementations in NePS, benchmarked in previously published work (Mallik et al., 2023b).</p>
<p>Random Search Simply searches uniformly random in the hyperparameter space. The fidelity is set to the $b_{\max }$ as specified by each benchmark instance (see, Appendix B). Therefore, as an example, a budget of 1000 freeze-thaw steps, will be equivalent to 20 full random search evaluations for LCBench and Taskset tasks.</p>
<p>HyperBand The NePS implementation follows the algorithm described in Li et al. (2018) and uses the early stopping hyper-hyperparameter, as $\eta=3$. The $b_{\min }$ is either 1 or as specified by the benchmark instances. Similarly for $b_{\max }$.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>ASHA The NePS implementation follows the algorithm described in Li et al. (2020b) and uses the early stopping hyper-hyperparameter, as $\eta=3$. The $b_{\min }$ is either 1 or as specified by the benchmark instances. Similarly for $b_{\max }$.</p>
<h1>C.2. Freeze-thaw baselines</h1>
<p>Here we describe the set of freeze-thaw BO algorithms. We note that due to experimental framework (optimizer-benchmark interfacing and analysis) related differences, performing ablation studies on the original implementations of DyHPO and DPL were not straightforward. For consistency and reducing confounding factors, all experiments were performed with implementations in the same experimental framework. Each of the algorithms were implemented in our custom NePS framework.</p>
<p>Freeze-Thaw with GPs This algorithm is designed to take one unit step per configuration in the fidelity space. The first 3 samples are selected uniformly random, as influenced by the seed. Subsequently, a Gaussian Process (GP) is fit on the joint hyperparameter and fidelity space to predict the loss, as a surrogate model. This baseline uses the greedy MF-EI acquisition function from Wistuba et al. (2022). The GP here uses a standard 5/2-Matérn kernel with a lengthscale of 1.0.</p>
<p>DyHPO This implementation follows the exact details given in Wistuba et al. (2022) and their publicly available code ${ }^{6}$. For a quick hyper-hyperparameter glance, refer here: https://github.com/automl/ifBO/blob/icml-2024/ src/pfns_hpo/pfns_hpo/configs/algorithm/dyhpo-neps-v2.yaml</p>
<p>DPL This implementation follows the exact details given in Kadra et al. (2023) and their publicly available code ${ }^{7}$. For a quick hyper-hyperparameter glance, refer here: https://github.com/automl/ifBO/blob/icml-2024/src/ pfns_hpo/pfns_hpo/configs/algorithm/dpl-neps-max.yaml</p>
<h2>D. Further Ablations</h2>
<h2>D.1. Effectiveness of modeling curve divergence</h2>
<p>As detailed in Section A.1, our curve prior is capable to model learning curve with diverging behavior. This capability is novel compared to the related works (Adriaensen et al., 2023; Klein et al., 2017; Kadra et al., 2023; Domhan et al., 2015), which are restricted to monotonic curves only. In Figure 7, we empirically show that modeling diverging curves yields a better surrogate model in terms of both extrapolation and HPO.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Comparison of the relative ranks of the performance gained by modeling divergences in ICL-FT-PFN. The plots, showing the average ranks across all the benchmarks (LCBench, PD1, and TaskSet), confirm the merits of capturing diverging curves both in terms of the quality of the predictions (log-likelihood, left) and HPO performances (regret, right).</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>D.2. Pairwise comparison of freeze-thaw approaches</h1>
<p>For a fine-grained assessment of the performance of ifBO, we present a pairwise comparison with the main freeze-thaw approaches including DPL and DyHPO. This is to visualize the relative gain of performance compared to each baseline, which may have been hidden from Figure 5.2. As shown in Figure 8, our approach dominates consistently DPL and DyHPO after $\approx 150$ steps of HPO run.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Comparison of relative ranks when aggregated over all benchmark families, showing strong anytime performance in both pairwise comparisons and also overall among freeze-thaw algorithms.</p>
<h2>D.3. Acquisition function ablation of the baselines</h2>
<p>In this section, our objective is to explore the impact of incorporating randomization into the acquisition function on the baseline methods (DPL and DyHPO). For this purpose, we assess each baseline across four distinct acquisition functions (Figure 9). The variants include: (ours), where both the horizon and the threshold for improvement are randomly selected, similar to the approach in ifBO; (one-step), where the horizon and threshold for improvement are chosen as in DyHPO; (at max), where the selection criteria for the horizon and threshold follow the methodology in DPL; and (random horizon), where the horizon is randomly determined, and the threshold is set to the best value observed.</p>
<p>The results presented in Figure 9 confirm that the randomization technique markedly enhances the performance of methods capable of extending learning curves over many steps, such as ifBO and DPL. Furthermore, please note that only the greedy one-step acquisition function is effective for DyHPO, given that it is specifically designed for one-step ahead predictions.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. Relative ablation over the horizon and threshold parameters of a multi-fidelity AF. For each algorithm, we take the AF designed into the original algorithm and ablate over the two variables: extrapolation horizon and the best performance threshold.</p>
<h2>D.4. Comparison of freeze-thaw approaches with MFPI-random</h2>
<p>In Figure 10, we present a comparison of freeze-thaw approaches-including ours, DPL, and DyHPO-when employing our acquisition function (MFPI-random). Despite all models utilizing the same acquisition function, our model significantly outperforms those of DPL and DyHPO. This clearly indicates the crucial role our prior in achieving the final performance.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10. Comparison of freeze-thaw approaches (ifBO, DPL, and DyHPO) when using our acquisition (MFPI-random) with their specific surrogate models. The plots represent the average ranks over all benchmarks (LCBench, PD1, and Taskset). This ablation confirms that our novel surrogate (and not only our novel acquisition function) contributes significantly to the HPO performance of ifBO.</p>
<h1>E. Aggregate plots over time</h1>
<p>Figure 11 plots Figure 3(bottom) but with the $x$-axis as cumulative wallclock time from the evaluation costs returned by the benchmark for each hyperparameter for every unit step. The overall conclusions remain over our HPO budget of 1000 steps. ifBO is on average anytime better ranked than the freeze-thaw HPO baselines.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11. Comparing relative rank over wallclock time (in $s$ ) over different benchmark families and the aggregated result overall. ifBO is on average better than the baselines, DyHPO and DPL, except for the TaskSet benchmark family where DPL starts the best but ifBO improves with more budget.</p>
<h2>F. Per-task HPO Plots</h2>
<p>In Section 3.1, we presented HPO results on each of these three benchmarks in a comprehensive form, averaging rank and normalized regrets across every task in the suite. These averages may hide / be susceptible to outliers. For completeness, Figures 12-17 provide regret plots for every task in the benchmark, averaged across the 10 seeds. We find that our method consistently performs on par, or better than the best previous best HPO method, especially in later stages of the search, without notable outliers.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ https://github.com/releaunifreiburg/DyHPO/tree/main
${ }^{7}$ https://github.com/releaunifreiburg/DPL/tree/main&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
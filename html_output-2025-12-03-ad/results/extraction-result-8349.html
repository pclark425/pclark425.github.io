<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8349 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8349</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8349</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-2a803d28a3b1db4a2590ef93435d3979ecd1e4f9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2a803d28a3b1db4a2590ef93435d3979ecd1e4f9" target="_blank">Harmonics of Learning: Universal Fourier Features Emerge in Invariant Networks</a></p>
                <p><strong>Paper Venue:</strong> Annual Conference Computational Learning Theory</p>
                <p><strong>Paper TL;DR:</strong> It is formally proved that, under certain conditions, if a neural network is invariant to a finite group then its weights recover the Fourier transform on that group, providing a mathematical explanation for the emergence of Fourier features.</p>
                <p><strong>Paper Abstract:</strong> In this work, we formally prove that, under certain conditions, if a neural network is invariant to a finite group then its weights recover the Fourier transform on that group. This provides a mathematical explanation for the emergence of Fourier features -- a ubiquitous phenomenon in both biological and artificial learning systems. The results hold even for non-commutative groups, in which case the Fourier transform encodes all the irreducible unitary group representations. Our findings have consequences for the problem of symmetry discovery. Specifically, we demonstrate that the algebraic structure of an unknown group can be recovered from the weights of a network that is at least approximately invariant within certain bounds. Overall, this work contributes to a foundation for an algebraic learning theory of invariant neural network representations.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8349",
    "paper_id": "paper-2a803d28a3b1db4a2590ef93435d3979ecd1e4f9",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0055734999999999995,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Harmonics of Learning: <br> Universal Fourier Features Emerge in Invariant Networks</h1>
<p>Giovanni Luca Marchetti<br>GLMA @ KTH.SE<br>KTH Royal Institute of Technology<br>Christopher J Hillar<br>HILLARMATH@ GMAIL.COM<br>Redwood Center for Theoretical Neuroscience<br>Danica Kragic<br>DANI@ KTH.SE<br>KTH Royal Institute of Technology<br>Sophia Sanborn<br>SOPHIAS@ SCIENCE.XYZ<br>Science Corporation</p>
<p>Editors: Shipra Agrawal and Aaron Roth</p>
<h4>Abstract</h4>
<p>In this work, we formally prove that, under certain conditions, if a neural network is invariant to a finite group then its weights recover the Fourier transform on that group. This provides a mathematical explanation for the emergence of Fourier features - a ubiquitous phenomenon in both biological and artificial learning systems. The results hold even for non-commutative groups, in which case the Fourier transform encodes all the irreducible unitary group representations. Our findings have consequences for the problem of symmetry discovery. Specifically, we demonstrate that the algebraic structure of an unknown group can be recovered from the weights of a network that is at least approximately invariant within certain bounds. Overall, this work contributes to a foundation for an algebraic learning theory of invariant neural network representations.</p>
<p>Keywords: Invariant neural networks, harmonic analysis, group representations
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Weights learned by a neural network trained for invariance to planar rotations resemble circular harmonics. Data from Sanborn et al. (2023).</p>
<h1>1. Introduction and Related Work</h1>
<p>Artificial neural networks trained on natural data exhibit a striking phenomenon: regardless of exact initialization, dataset, or training objective, models trained on the same data domain frequently converge to similar learned representations (Li et al., 2015). For example, the early layer weights of diverse image models tend to converge to Gabor filters and color-contrast detectors (Olah et al., 2020). Remarkably, many of these same features are observed in the visual cortex (Hubel and Wiesel, 1959; Hass and Horwitz, 2013; Willeke et al., 2023), suggesting a form of representational universality that transcends biological and artificial substrates. While such findings are empirically well-established (Räuker et al., 2023), the field lacks theoretical explanations.</p>
<p>Spatially localized versions of canonical 2D Fourier basis functions, such as Gabor filters or wavelets, are perhaps the most frequently observed universal features. They commonly arise in the early layers of vision models - trained with efficient coding (Olshausen and Field, 1997; Bell and Sejnowski, 1997), classification (Olah et al., 2020), temporal coherence (Hurri and Hyvärinen, 2003), and next-step prediction (Fiquet and Simoncelli, 2023) objectives - as well as in the primary visual cortices of diverse mammals - including cats (Hubel and Wiesel, 1962), monkeys (Hubel and Wiesel, 1968), and mice (Dräger, 1975). Non-localized Fourier features have been observed in networks trained to solve tasks that permit cyclic wraparound - for example, modular arithmetic (Nanda et al., 2023), more general group compositions (Chughtai et al., 2023), or invariance to the group of cyclic translations (Sanborn et al., 2023). In the domain of spatial navigation, the so-called grid cells of the entorhinal cortex (Moser et al., 2008) display periodic firing patterns at different spatial frequencies as they build a map of space. Their response properties are naturally modeled with the harmonics of the twisted torus (Guanella et al., 2007; Orchard et al., 2013; Gardner et al., 2022). Similar features also emerge in artificial neural networks trained to solve spatial navigation tasks (Banino et al., 2018; Cueva and Wei, 2018; Sorscher et al., 2019; Dorrell et al., 2022; Sorscher et al., 2023; Schaeffer et al., 2023). The ubiquity of these features across diverse learning systems is both striking and unexplained.</p>
<p>In this work, we provide a mathematical explanation for the emergence of Fourier features in learning systems such as neural networks. We argue that the mechanism responsible for this emergence is the downstream invariance of the learner to the action of a group of symmetries (e.g., planar translations or rotations). Since natural data typically possess symmetries, invariance is a fundamental bias that is injected both implicitly and sometimes explicitly into learning systems (Bronstein et al., 2021; Cohen and Welling, 2016; Kondor and Trivedi, 2018). Motivated by this, we derive theoretical guarantees for the presence of Fourier features in invariant learners that apply to a broad class of machine learning models.</p>
<p>Our results rely on the inextricable link between harmonic analysis and group theory (Folland, 2016). The standard discrete Fourier transform is a special case of more general Fourier transforms on groups, which can be defined by replacing the standard basis of harmonics by irreducible unitary group representations. The latter are equivalent to the familiar definition for cyclic or, more generally, commutative groups, but are more involved for non-commutative ones. In order to accommodate both scenarios, we develop a general theory that applies, in principle, to arbitrary finite groups.</p>
<p>This work represents an attempt to provide mathematical grounding for a general algebraic theory of representation learning, while addressing the universality hypothesis for neural networks (Olah et al., 2020; Moschella et al., 2022). A suite of earlier theoretical works (Isely et al., 2010; Hillar and Sommer, 2015; Garfinkle and Hillar, 2019) established such universality for sparse coding</p>
<p>models (Olshausen and Field, 1997), deriving the conditions under which a network will recover the original bases that generate data through sparse linear combinations. In this case, the statistics of the data determine the uniqueness of the representation. Our findings, on the other hand, are purely algebraic, since they rely exclusively on the invariance properties of the learner. Given the centrality of invariance to many machine learning tasks, our theory encompasses a broad class of scenarios and neural network architectures, while providing a new perspective on classical neuroscience (Pitts and McCulloch, 1947; Hubel and Wiesel, 1962). As such, it sets a foundation for a learning theory of representations in artificial and biological neural systems, grounded in the mathematics of symmetry.</p>
<h1>1.1. Overview of Results</h1>
<p>In this section, we provide a non-technical overview of the theoretical results presented in this work. Our main result can be summarized as follows.</p>
<p>Informal Theorem 1 (Theorem 10 and Corollary 11) If $\varphi(W, x)$ is a parametric function of a certain kind that is invariant in the input variable $x$ to the action of a finite group $G$, then each component of its weights $W$ coincides with a harmonic of $G$ up to a linear transformation. In particular, when the weights are orthonormal, $W$ coincides with the Fourier transform of $G$ up to linear transformations.</p>
<p>Here, the term "harmonic" refers to an irreducible unitary representation of $G$. In particular, onedimensional unitary representations correspond to homomorphisms with the unit circle $\mathrm{U}(1) \subseteq \mathbb{C}$, which is reminiscent of the classical definition via the imaginary exponential. However, noncommutative groups can have higher-dimensional irreducible representations, intuitively meaning that harmonics are valued in unitary matrices. In this case, the components of $W$ can be interpreted as capsules in the sense of Hinton et al. (2018); i.e., neural units processing matrix-valued signals.</p>
<p>Since harmonic analysis is naturally formalized over the complex numbers, we consider models with complex weights $W$, which fits into the broader program of complex-valued machine learning (Bassey et al., 2021; Trabelsi et al., 2017; Löwe et al., 2022). We show that the hypothesis on $\varphi$ in Theorem 1 is satisfied by several machine learning models from the literature. In particular, the theorem applies to the recently-introduced (Bi)Spectral Networks (Sanborn et al., 2023), to single fully-connected layers of McCulloch-Pitts neurons, and, to an extent, to traditional deep networks. As an additional contribution, we generalize Spectral Networks to non-commutative groups.</p>
<p>The group-theoretical Fourier transform encodes the entire group structure of $G$. Therefore, as a consequence of Theorem 1, the multiplication table of $G$ can be recovered from the weights $W$ of an invariant parametric function $\varphi$ - a fact empirically demonstrated by Sanborn et al. (2023). This addresses the question of symmetry discovery - an established machine learning problem aiming to recover the unknown group of symmetries of data with minimal supervision and prior knowledge (Rao and Ruderman, 1998; Sohl-Dickstein et al., 2010; Desai et al., 2022). Since the multiplication table is a discrete object, it is expected that the invariance constraint on $\varphi$ can be loosened while still recovering the group correctly. To this end, we prove the following.</p>
<p>Informal Theorem 2 (Theorem 17) If $\varphi(W, x)$ is "almost invariant" to $G$ according to certain functional bounds and the weights are "almost orthonormal", then the multiplication table of $G$ can be recovered from $W$.</p>
<p>Lastly, we implement a model satisfying the requirements of our theory and demonstrate its symmetry discovery capabilities. To this end, we train it via contrastive learning on an objective encouraging invariance and extract the multiplication table of $G$ from its weights. Our Python implementation is available at a public repository ${ }^{1}$.</p>
<h1>2. Mathematical Background</h1>
<p>We begin by introducing the fundamental concepts from harmonic analysis and group theory used in this paper. For a complete treatment, we refer the reader to Steinberg (2012).</p>
<h3>2.1. Groups and Actions</h3>
<p>A group is an algebraic object whose elements represent abstract symmetries, which can be composed and inverted.</p>
<p>Definition 3 A group is a set $G$ equipped with a multiplication map $G \times G \rightarrow G$ denoted by $(g, h) \mapsto g h$, an inversion map $G \rightarrow G$ denoted by $g \mapsto g^{-1}$, and a distinguished identity element $1 \in G$ such that for all $g, h, k \in G$ :</p>
<p>$$
\begin{array}{ccc}
\text { Associativity } &amp; \text { Inversion } &amp; \text { Identity } \
g(h k)=(g h) k &amp; g^{-1} g=g g^{-1}=1 &amp; g 1=1 g=g
\end{array}
$$</p>
<p>A map $\rho: G \rightarrow G^{\prime}$ between groups is called a homomorphism if $\rho(g h)=\rho(g) \rho(h)$ for all $g, h \in G$.
Examples of groups include the permutations of a set and the general linear group $\mathrm{GL}(V)$ of invertible operators over a vector space $V$, both equipped with the usual composition and inversion of functions. A further example that will be relevant in this work is the unitary group $\mathrm{U}(V) \subseteq \mathrm{GL}(V)$ associated to a Hilbert space $V$, consisting of operators $U$ satisfying $U U^{\dagger}=I$, where $I$ is the identity matrix. ${ }^{2}$ Groups satisfying $g h=h g$ for all $g, h \in G$ are deemed commutative. The idea of a space $\mathcal{X}$ having $G$ as a group of symmetries is abstracted by the notion of group action.</p>
<p>Definition 4 An action by a group $G$ on a set $\mathcal{X}$ is a map $G \times \mathcal{X} \rightarrow \mathcal{X}$ denoted by $(g, x) \mapsto g \cdot x$, satisfying for all $g, h \in G, x \in \mathcal{X}$ :</p>
<p>$$
\begin{aligned}
&amp; \text { Associativity Identify } \
&amp; g \cdot(h \cdot x)=(g h) \cdot x \quad 1 \cdot x=x
\end{aligned}
$$</p>
<p>A map $\varphi: \mathcal{X} \rightarrow \mathcal{Z}$ between sets acted upon by $G$ is called equivariant if $\varphi(g \cdot x)=g \cdot \varphi(x)$ for all $g \in G, x \in \mathcal{X}$. It is called invariant if moreover $G$ acts trivially on $\mathcal{Z}$ or, explicitly, if $\varphi(g \cdot x)=\varphi(x)$.</p>
<p>In general, the following actions can be defined for arbitrary groups: $G$ acts on any set trivially by $g \cdot x=x$, and $G$ acts on itself seen as a set via (left) multiplication by $g \cdot h=g h$. Further examples are $\mathrm{GL}(V)$ and $\mathrm{U}(V)$ acting on $V$ by evaluating operators.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>2.2. Harmonic Analysis on Groups</h1>
<p>Harmonic analysis on groups (Folland, 2016) generalizes standard harmonic analysis. We focus here on finite groups for simplicity, which are sufficient for practical applications. This avoids technicalities such as integrability conditions and continuity issues arising for infinite groups. We start by considering commutative groups and cover non-commutative ones in Section 2.3.</p>
<p>Let $G$ be a finite commutative group of order $|G|$. Denote by $\langle G\rangle=\mathbb{C}^{G}$ the free complex vector space generated by $G$. Intuitively, an element $x=\left(x_{g}\right)_{g \in G} \in\langle G\rangle$ represents a complex-valued signal over $G$. The space $\langle G\rangle$ is endowed with the convolution product:</p>
<p>$$
(x \star y)<em G="G" _in="\in" h="h">{g}=\sum</em>
$$} x_{h} y_{h^{-1} g</p>
<p>and is acted upon by $G$ via $g \cdot x=\delta_{g} \star x=\left(x_{g^{-1} h}\right)<em g="g">{h \in G}$, where $\delta</em>$ is the canonical basis vector.
Definition 5 The dual $G^{\vee}$ of $G$ is the set of homomorphisms $\rho: G \rightarrow \mathrm{U}(\mathbb{C})$, where $\mathrm{U}(\mathbb{C}) \subseteq \mathbb{C}$ is the group of unitary complex numbers equipped with multiplication. It is itself a group when equipped with pointwise composition $(\rho \mu)(g)=\rho(g) \mu(g)$.</p>
<p>A homomorphism $\rho \in G^{\vee}$ intuitively represents a harmonic over $G$, generalizing the familiar notion from signal processing. If we endow $\langle G\rangle$ with the canonical scalar product $\langle x, y\rangle=\sum_{g \in G} \overline{x_{g}} y_{g}$, then $G^{\vee} \subseteq\langle G\rangle$ forms an orthogonal basis with all the norms equal to $\sqrt{|G|}$. The linear base-change is, by definition, the Fourier transform over $\langle G\rangle$ :</p>
<p>Definition 6 The Fourier transform is the map $\langle G\rangle \rightarrow\left\langle G^{\vee}\right\rangle, x \mapsto \hat{x}$, defined for $\rho \in G^{\vee}$ as $\hat{x}_{\rho}=\langle\rho, x\rangle$.</p>
<p>The Fourier transform is a linear isometry or, equivalently, a unitary operator, up to a multiplicative constant of $|G|$. Moreover, it exchanges the convolution product $\star$ over $\langle G\rangle$ with the Hadamard product $\odot$ over $\left\langle G^{\vee}\right\rangle$. Definition 6 generalizes the usual discrete Fourier transform in the following sense. For an integer $d&gt;0$, consider the cyclc group $C_{d}$ with $d$ elements. Concretely, $C_{d}=\mathbb{Z} / d \mathbb{Z}$ is the group of integers modulo $d$ equipped with addition as composition. The dual $G^{\vee}$ consists of homomorphisms of the form $\mathbb{Z} / d \mathbb{Z} \ni g \mapsto e^{2 \pi \sqrt{-1} g k / d}$ for $k \in{0, \cdots, d-1}$. Definition 6 specializes then to the familiar Fourier transform.</p>
<h3>2.3. Non-Commutative Harmonic Analysis</h3>
<p>So far, we have assumed that $G$ is commutative. In this section we briefly discuss the extension of Fourier theory to non-commutative groups. This however requires more elaborate theoretical tools, which we now introduce. To begin, in order to perform harmonic analysis on general groups it is necessary to discuss unitary representations. The latter will play the role of matrix-valued harmonics.</p>
<p>Definition 7 A unitary representation of $G$ is an action by $G$ on a finite-dimensional Hilbert space $V$ via unitary operators or, in other words, a homomorphism $\rho_{V}: G \rightarrow \mathrm{U}(V)$. A unitary representation is said to be irreducible if $V$ does not contain any non-trivial ${ }^{3}$ sub-representations.</p>
<p>We denote by $\operatorname{Irr}(G)$ the set of all irreducible representations of $G$ up to isomorphism. Moreover, for a vector space $V$ we denote by $\operatorname{End}(V)$ the space of its linear operators.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Definition 8 The Fourier transform is the map $\langle G\rangle \rightarrow \bigoplus_{\rho_{V} \in \operatorname{Irr}(G)} \operatorname{End}(V), x \mapsto \hat{x}$, defined for $\rho_{V} \in \operatorname{Irr}(G)$ as:</p>
<p>$$
\hat{x}<em V="V">{\rho</em>(V)
$$}}=\sum_{g \in G} \rho_{V}(g)^{\dagger} x_{g} \in \operatorname{End</p>
<p>This generalizes Definition 6 since for a commutative group, $\rho_{V}$ is irreducible if, and only if, $\operatorname{dim}(V)=1$. Analogously to the commutative setting, the Fourier transform exchanges the convolution product $\star$ with the point-wise operator composition, which we still denote by $\odot$. Moreover, the Fourier transform is a unitary operator up to a multiplicative constant of $|G|$ with respect to the normalized Hilbert-Schmidt scalar product on $\operatorname{End}(V)$, given by $\langle A, B\rangle=\operatorname{dim}(V) \operatorname{tr}\left(A^{\dagger} B\right)$. The norm associated to the Hilbert-Schmidt scalar product is the Frobenius norm. The relations between irreducible unitary representations coming from the unitarity of the Fourier transform are known as Schur orthogonality relations.</p>
<h1>3. Theoretical Results</h1>
<p>We now present the primary theoretical contributions of this work. Concretely, we demonstrate that if certain parametric functions are invariant to a group then their weights must almost coincide with harmonics, i.e. irreducible unitary group representations. We start by introducing general algebraic notions and principles, and then proceed to specialize them to machine learning scenarios.</p>
<p>Let $G$ be a finite group, $\mathcal{H}$ be a set, and $V_{1}, \ldots, V_{k}$ be complex finite-dimensional Hilbert spaces. In what follows, we will consider the space:</p>
<p>$$
\mathcal{W}=\langle G\rangle \otimes \bigoplus_{i} \operatorname{End}\left(V_{i}\right) \simeq \bigoplus_{i} \operatorname{End}\left(V_{i}\right)^{\oplus G}
$$</p>
<p>$\mathcal{W}$ is a Hilbert space when endowed with the scalar product given by the product of the canonical scalar product over $\langle G\rangle$ and the normalized Hilbert-Schmidt scalar products over $\operatorname{End}\left(V_{i}\right)$. For $W \in \mathcal{W}$, we will denote each of its components as $W_{i}=\left(W_{i}(g)\right)<em i="i">{g} \in \operatorname{End}\left(V</em>(g)\right)}\right)^{\oplus G}$. Moreover, we will often interpret elements $W \in \mathcal{W}$ as linear maps $\langle G\rangle \rightarrow \bigoplus_{i} \operatorname{End}\left(V_{i}\right)$ via $W(x)=\sum_{g \in G} W(g) x_{g}$ for $x \in\langle G\rangle$, where $W(g)=\left(W_{i<em i="i">{i}$. Note that $G$ acts on the left tensor factor of $\mathcal{W}$ while for every $i, \mathrm{U}\left(V</em>\right)$ by composition of operators.}\right)$ acts on the right tensor factor of $\langle G\rangle \otimes \operatorname{End}\left(V_{i</p>
<p>Example 1 Concretely, given coordinates on $V_{i} \simeq \mathbb{C}^{d_{i}}$ for $i=1, \ldots, k$, an element of $\mathcal{W}$ consists of a $k$-tuple of tensors $W_{i} \in \mathbb{C}^{d_{i} \times d_{i} \times|G|} \simeq \operatorname{End}\left(V_{i}\right)^{\oplus G}$. For example, the dihedral group $D_{n}$ of isometries of a regular n-gon has $2 n$ elements. For $n$ odd, it has 2 irreducible unitary representations of dimension 1, and $(n-1) / 2$ of dimension 2, implying that $\mathcal{W}$ consists of a direct sum of copies of $\mathbb{C}^{2 n}$ and $\mathbb{C}^{2 \times 2 \times 2 n}$.</p>
<p>Definition 9 We say that a map $\varphi: \mathcal{W} \rightarrow \mathcal{H}$ has unitary symmetries if for all $W, W^{\prime} \in \mathcal{W}$ such hat $\left|W_{i}\right|=\left|W_{i}^{\prime}\right|$ for all $i$ and $\varphi(W)=\varphi\left(W^{\prime}\right)$, we have that for every $i$ there exists a unitary operator $U_{i} \in \mathrm{U}\left(V_{i}\right)$ such that $W_{i}=U_{i} \cdot W_{i}^{\prime}$.</p>
<p>In the context of machine learning, $\mathcal{H}$ will represent the hypothesis space, consisting of functions the model can learn. On the other hand, $\varphi$ will represent the parametrization of such hypotheses, with its domain $\mathcal{W}$ being the space of weights. Each component $\operatorname{End}\left(V_{i}\right)$ of $\mathcal{W}$ will be responsible for parametrizing a computational unit, i.e. a complex-valued neuron in the language of neural networks.</p>
<p>For commutative groups, we simply have $V_{i}=\mathbb{C} \simeq \operatorname{End}\left(V_{i}\right)$. In general, $\operatorname{End}\left(V_{i}\right)$ can be thought of as parametrizing matrix-valued signals, which, as mentioned in Section 1.1, are computed by neural units sometimes referred to as capsules (Hinton et al., 2018; Sabour et al., 2017). Lastly, the components of $\langle G\rangle$ will represent the input space. The fact that the latter consists of scalar signals over $G$ is a simplification of several practical scenarios. However, the results of this section can be generalized, to an extent, to signals over a set acted upon by $G$ - see Section 3.1. The following is an algebraic principle at the core of this work.</p>
<p>Theorem 10 Suppose that $\varphi: \mathcal{W} \rightarrow \mathcal{H}$ has unitary symmetries and that for some $W \in \mathcal{W}$ the following holds:</p>
<ul>
<li>$\varphi(g \cdot W)=\varphi(W)$ for all $g \in G$.</li>
<li>$W_{i}$, seen as a linear map $\langle G\rangle \rightarrow \operatorname{End}\left(V_{i}\right)$, is surjective for all $i$.</li>
</ul>
<p>Then for every $i$ there exist $W_{i}^{\prime} \in \operatorname{End}\left(V_{i}\right)$ and an irreducible unitary representation $\rho_{i}: G \rightarrow \mathrm{U}\left(V_{i}\right)$ such that for all $g \in G$,</p>
<p>$$
W_{i}(g)=W_{i}^{\prime} \rho_{i}(g)^{\dagger}
$$</p>
<p>We refer to the Appendix for a proof. Note that the surjectivity assumption implies the constraint $\operatorname{dim}\left(V_{i}\right)^{2} \leq|G|$ for all $i$. As a consequence of the result above, the full Fourier transform arises with an additional orthogonality assumption.</p>
<p>Corollary 11 Suppose that $\varphi: \mathcal{W} \rightarrow \mathcal{H}$ has unitary symmetries and that for some $W \in \mathcal{W}$ the following holds:</p>
<ul>
<li>$\varphi(g \cdot W)=\varphi(W)$ for all $g \in G$.</li>
<li>$W$ is unitary up to a multiplicative constant, i.e. $W^{\dagger} W=|G| I$.</li>
</ul>
<p>Then $W$ is the Fourier transform up to composing each of the components $W_{i}$ by an operator with Frobenius norm equal to 1.</p>
<p>We refer to the Appendix for a proof. Again, the orthogonality assumption implies that $V_{1}, \ldots, V_{k}$ are the ambient Hilbert spaces of all the irreducible unitary representations of $G$ up to isomorphism, and in particular $\sum_{i} \operatorname{dim}\left(V_{i}\right)^{2}=|G|$.</p>
<p>We now wish to discuss the other crucial assumption of Theorem 10 requiring that $\varphi(g \cdot W)=$ $\varphi(W)$ for all $g \in G$, which is reminiscent of invariance. However, when $\mathcal{H}$ is a space of functions, we are typically interested in models that are invariant in the input variable rather than the weight variable. Therefore, we introduce the following condition, aimed at reconciling inputs and weights. To this end, suppose that $\mathcal{H}$ is a set of functions $\mathcal{X} \rightarrow \mathcal{Y}$, where $\mathcal{X}$ is a set acted upon by $G$ and $\mathcal{Y}$ is a set. We adhere to the notation $\varphi(W, x)=\varphi(W)(x), x \in\langle G\rangle, W \in \mathcal{W}$, for simplicity.</p>
<p>Definition 12 We say that $\varphi$ satisfies the adjunction property if $\varphi(W, g \cdot x)=\varphi\left(g^{-1} \cdot W, x\right)$ for all $x \in \mathcal{X}, g \in G$.</p>
<p>The adjunction property implies that if $\varphi(W, x)$ is invariant in $x$, then $\varphi(g \cdot W, x)=\varphi(W, x)$ for all $x, g$, recovering the assumption of Theorem 10 .</p>
<h1>3.1. Extensions</h1>
<p>As explained above, the tensor component $\langle G\rangle$ of $\mathcal{W}$ typically represents the input space of a given machine learning model. However, it is often the case that data does not consist of signals over $G$. This happens, for example, in the case of image data acted upon by the cyclic group via rotations (see Figure 1), since the pixel plane is composed of several copies of $G$.</p>
<p>One can consider the more general scenario when data consist of complex signals over a finite set $\mathcal{S}$ acted upon by $G$, therefore replacing $\langle G\rangle=\mathbb{C}^{G}$ by $\mathbb{C}^{\mathcal{S}}$. Assuming the action over $\mathcal{S}$ is free, meaning that $g \cdot s=s$ implies $g=1, \mathcal{S}$ can be decomposed into copies of $G$ deemed orbits. Specifically, there is an equivariant isomorphism $\mathcal{S} \simeq G \sqcup \cdots \sqcup G$, which in turn induces a linear isomorphism $\mathbb{C}^{\mathcal{S}} \simeq\langle G\rangle^{\oplus p}$, where $p$ is the number of orbits. The results from this section can be extended to this scenario by applying all the arguments to the copies of $\langle G\rangle$ separately, each of which will serve as a domain for its set of irreducible unitary representations.</p>
<p>When the action is not free, it is necessary to take into account stabilizers, i.e. $g \in G$ such that $g \cdot s=s$ for some $s \in \mathcal{S}$. Roughly speaking, we expect that the results from this section can be adapted to an extent, obtaining unitary representations "up to stabilizers". However, the precise meaning of the latter has yet to be clarified, and goes beyond the scope of this work.</p>
<h3>3.2. Examples</h3>
<p>In this section, we provide examples of machine learning models with unitary symmetries. As anticipated, in the context of machine learning $\mathcal{H}$ and $\mathcal{W}$ represent the hypothesis space and the parameter space, respectively. Indeed, in what follows $\mathcal{H}$ will consist of functions of the form $\langle G\rangle \rightarrow \mathcal{Y}$ for some codomain $\mathcal{Y}$, and we will adhere to the notation from Definition 12 accordingly. All the models considered in this section satisfy the adjunction property.</p>
<h3>3.2.1. Spectral Networks</h3>
<p>We start by considering Spectral Networks - a class of polynomial machine learning models that inspired this work - and to which our theory applies naturally. These models were introduced by Sanborn et al. (2023) in cubic form and for commutative groups. Here, we generalize them to arbitrary degree and to the non-commutative setting. Spectral Networks are grounded in the algebraic invariant theory of $\langle G\rangle$. We review the latter in the Appendix (Section B), including simple proofs for finite groups of classical results. The overall idea behind Spectral Networks is to approximate the $n$-order polynomial invariants - deemed spectral invariants - of $\langle G\rangle$ for an unknown group $G$. Specifically, suppose that $V_{1}, \ldots, V_{k}$ are the ambient Hilbert spaces of the irreducible unitary representations of $G$. Given a multi-index $\underline{i}=\left(i_{1}, \ldots, i_{n}\right) \in{1, \ldots, k}^{n}$, the Spectral Network of order $n$ is defined as the collection of parametric maps $\varphi_{\underline{i}}(W, \cdot):\langle G\rangle \rightarrow \operatorname{End}\left(V_{i_{1}} \otimes \cdots \otimes V_{i_{n}}\right)$ :</p>
<p>$$
\varphi_{\underline{i}}(W, x)=W_{i_{1}}(x) \otimes \cdots \otimes W_{i_{n}}(x)\left(W_{i_{1}}^{\dagger} \odot \cdots \odot W_{i_{n}}^{\dagger}\right)(\bar{x})
$$</p>
<p>where $W=\oplus_{i} W_{i} \in \mathcal{W}=\langle G\rangle \otimes \oplus_{i} \operatorname{End}\left(V_{i}\right), \odot$ denotes the $G$-wise tensor product of operators, and $\bar{x}$ denotes the component-wise conjugate of $x$. For a commutative $G$, since $V_{i}=\mathbb{C}$ for all $i$, the above expression reduces to $\varphi_{\underline{i}}(W, x)=W_{i_{1}} x \cdots W_{i_{n}} x \overline{W_{i_{1}} \odot \cdots \odot W_{i_{n}} x}$. For $n=1$ Spectral Networks are called Power-Spectral Networks, and for a commutative $G$ they take the form $\varphi_{i}(W, x)=\left|W_{i} x\right|^{2}$. The latter can be simply interpreted as a linear model followed by a function</p>
<p>given by the squared absolute value. Even though the latter is uncommon in machine learning, it has appeared in models of biological neural networks (Adelson and Bergen, 1985).</p>
<p>For simplicity, and without loss of generality, we will consider only the Spectral Networks involving a single unitary representation; that is, we will focus on constant multi-indices $\underline{i}=(i, \ldots, i)$ in Equation 5. To this end, let $V$ be a finite-dimensional Hilbert space and $\mathcal{H}$ be the set of functions $\langle G\rangle \rightarrow \operatorname{End}\left(V^{\otimes n}\right)$ for some $n \in \mathbb{N}$. We also set $\mathcal{W}=\langle G\rangle \otimes \operatorname{End}(V)$. The following is proved in the Appendix.</p>
<p>Proposition 13 Consider the Spectral Network given by $\varphi(W, x)=W(x)^{\otimes n} W^{\dagger \odot n}(\bar{x})$. Then $\varphi$ has unitary symmetries.</p>
<h1>3.2.2. McCulloch-Pitts Neurons and Deep Networks</h1>
<p>While Spectral Networks provide the most direct application of our theory, in this section we discuss the most common and fundamental neural network primitives in deep learning: the fully-connected McCulloch-Pitts neuron (McCulloch and Pitts, 1943) and the deep neural network. We consider models with complex coefficients and focus on commutative groups, i.e. all the Hilbert spaces $V_{i}$ from Definition 9 will be equal to $\mathbb{C}$. A McCulloch-Pitts neuron has the form $\varphi(W, x)=\sigma(W x)$, where $\sigma: \mathbb{C} \rightarrow \mathcal{Y}$ is a map playing the role of an activation function and $W \in \mathcal{W}=\langle G\rangle$ is the weight vector. For $\sigma(z)=|z|^{2}$, the McCulloch-Pitts neuron reduces to a commutative Power-Spectral Network, i.e. a Spectral Network with $n=1$. The hypothesis space $\mathcal{H}$ consists of functions $\langle G\rangle \rightarrow \mathcal{Y}$.</p>
<p>Proposition 14 Consider a map $\sigma: \mathbb{C} \rightarrow \mathcal{Y}$ and let $\varphi(W, x)=\sigma(W x)$. Suppose that $0 \in \mathbb{C}$ is isolated in its fiber of $\sigma$, i.e. there exists an open subspace $O \subseteq \mathbb{C}$ such that $\sigma^{-1}(\sigma(0)) \cap O={0}$. Then $\varphi$ has unitary symmetries.</p>
<p>We refer to the Appendix for a proof. The above assumption on $\sigma$ is satisfied by popular activations functions from neuroscience and machine learning, such as the sigmoid and the leaky Rectified Linear Unit (ReLU), applied after taking complex absolute value. Moreover, any non-constant holomorphic map $\sigma: \mathbb{C} \rightarrow \mathbb{C}$ satisfies the assumption, since its fibers are discrete by the Analytic Continuation Theorem.</p>
<p>Example 2 Consider the logistic regressor with complex weights, corresponding to a McCullochPitts neuron with a sigmoid activation function, i.e., $\sigma(z)=\frac{1}{1+e^{-|z|}}+b$, where $b \in \mathbb{R}$ is the bias term. Since $\sigma^{-1}(\sigma(0))={0}$, Proposition 14 applies. In particular, if the logistic regressor $\varphi(W, x)=\sigma(W x)$ is invariant in $x$ to the cyclic group $C_{d}=\mathbb{Z} / d \mathbb{Z}$, from Theorem 10 it follows that, if $W \neq 0$, then $W(g)=u e^{-2 \pi \sqrt{-1} g / d}$ for all $g=0, \ldots, d-1$ and for some nonzero $u \in \mathbb{C}$.</p>
<p>Next, we discuss the case of classical deep neural networks. We model the latter as $\varphi(W, x)=$ $\chi(\sigma(W(x)))$, where $W \in\langle G\rangle \otimes \mathbb{C}^{k}$ represents the weights of the first layer containing $k$ neurons, $\sigma: \mathbb{C} \rightarrow \mathbb{R}$ represents the activation function, computed coordinate-wise on $\mathbb{C}^{k}$, and $\chi: \mathbb{R}^{k} \rightarrow \mathbb{R}$ encompasses all the layers after the first one. Therefore, the first layer is assumed to be complexvalued, as required by our theory, while the subsequent layers are allowed to take real values, as typical in practice. Since the weights $W$ only account for the first layer, our main results will guarantee that invariant deep neural networks recover the Fourier transform in that layer, which is consistent with empirical observations (Olah et al., 2020).</p>
<p>Differently from Section 3, for the next result we will restrict $\mathcal{W}$ to the subspace of $\langle G\rangle \otimes \mathbb{C}^{k}$ consisting of $W$ such that the components $W_{i}$ are orthonormal. This implies, in particular, the constraint $k \leq|G|$. Note that $\mathcal{W}$ is closed by the actions of $G$ and $\mathrm{U}(\mathbb{C})$. The orthonormality condition is anyway necessary in order to recover the full Fourier transform - see Corollary 11.</p>
<p>Proposition 15 Let $\mathcal{W}=\left{W \in\langle G\rangle \otimes \mathbb{C}^{k} \mid \forall i, j\left\langle W_{i}, W_{j}\right\rangle=0\right}$, consider maps $\sigma: \mathbb{C} \rightarrow \mathbb{R}$, $\chi: \mathbb{R}^{k} \rightarrow \mathbb{R}$, and let $\varphi(W, x)=\chi(\sigma(W(x)))$. Suppose that:</p>
<ul>
<li>There exists an open subspace $O \subseteq \mathbb{R}^{k}$ containing $\sigma(0)$ where $\chi$ is affine with distinct non-vanishing coefficients i.e., $\chi(z)=\sum_{i} a_{i} z_{i}+b$ for $z \in O$ with $0 \neq a_{i} \neq a_{j}$ for $i \neq j$.</li>
<li>$\sigma \in C^{2}(\mathbb{C})$ and $\partial_{z} \partial_{\bar{z}} \sigma(0) \neq 0$.</li>
</ul>
<p>Then $\varphi$ has unitary symmetries.
We refer to the Appendix for a proof. The hypothesis on $\chi$ is motivated by the fact that typical (real-valued) deep networks have piece-wise linear activation functions, such as (leaky) ReLU. As such, they define piece-wise affine maps and therefore are affine when restricted to appropriate open subspaces. Moreover, the hypothesis on the coefficients $a_{i}$ in Proposition 15 is generic, meaning that it defines an open dense subset of $\left(a_{1}, \ldots, a_{k}\right) \in \mathbb{R}^{k}$.</p>
<h1>3.3. Group Recovery</h1>
<p>Corollary 11 allows us to recover the group structure of $G$ up to isomorphism from the weights of a map with unitary symmetries. In other words, this enables the recovery of an unknown group in a data-driven manner from the weights of an invariant machine learning model, addressing the problem of symmetry discovery discussed in Section 1.1. The procedure was originally suggested and validated empirically by Sanborn et al. (2023). To this end, assume that $\varphi_{i}$ satisfies the requirements of Corollary 11. Moreover, we introduce the additional assumption that $W_{i}(1)=I \in \operatorname{End}\left(V_{i}\right)$ for all $i$. If that is the case, $W$ coincides exactly with the Fourier transform by Corollary 11. This implies that the multiplication table of $G$ can be recovered by:</p>
<p>$$
g h=\underset{l \in G}{\operatorname{argmin}} | W(g) \odot W(h)-W(l) |
$$</p>
<p>where $\odot$ denotes the Hadamard product, i.e. component-wise operator composition. Note that this notation has a different meaning here than in Section 3.2.1. Since the $W(g)$ 's are orthogonal, the only possible values for the norms over which the minimum is performed are 0 and $\sqrt{2|G|}$.</p>
<p>The multiplication table of $G$ is a discrete object, while the weights $W \in \mathcal{W}$ can vary continuously. Therefore, it is natural to expect that the invariance condition ( $g \cdot W=W$ for all $g \in G$ ) can be relaxed, while still recovering the multiplication table correctly. In what follows, we analyze relaxations of invariance and give bounds in which the group recovery algorithm holds. We start by introducing a quantity measuring how close a map is to having unitary symmetries. To this end, we assume that $\mathcal{H}$ is a metric space with distance function $\Delta: \mathcal{H} \times \mathcal{H} \rightarrow \mathbb{R}_{\geq 0}$.</p>
<p>Definition 16 Given a map $\varphi: \mathcal{W} \rightarrow \mathcal{H}$, its unitarity defect is defined for $\delta \in \mathbb{R}_{&gt;0}$ as:</p>
<p>$$
\omega_{\varphi}(\delta)=\sup <em i="i">{W, W^{\prime}} \max </em> \inf <em i="i">{U \in \mathrm{U}\left(V</em>\right|
$$}\right)}\left|W_{i}-U \cdot W_{i}^{\prime</p>
<p>where the supremum runs over all the $W, W^{\prime} \in\langle G\rangle \otimes \operatorname{End}(V)$ of the same norm such that $\Delta\left(\varphi(W), \varphi\left(W^{\prime}\right)\right) \leq \delta$.</p>
<p>Note that $\varphi$ has unitary symmetries if, and only if, $\omega_{\varphi}(0)=0$. We now state our main relaxation result.</p>
<p>Theorem 17 Suppose that $\varphi: \mathcal{W} \rightarrow \mathcal{H}$ is a map and fix $W \in \mathcal{W}$. Denote $L=\left|W^{\dagger} W-|G| I\right|<em _infty="\infty">{\infty}$, where $|A|</em>=\max <em g_="g," h="h">{g, h}\left|A</em>\right|$ is the uniform Frobenius norm for $G \times G$ matrices. Suppose that the following holds:</p>
<ul>
<li>For all $g \in G, \omega_{\varphi}(\Delta(\varphi(g \cdot W), \varphi(W)))&lt;\sqrt{\frac{1}{2}-\frac{L}{|G|}} /(\sqrt{|G|+L}+1)$.</li>
<li>$L \leq \frac{|G|}{2}$.</li>
<li>$W_{i}(1)=I \in \operatorname{End}\left(V_{i}\right)$ for all $i$.</li>
</ul>
<p>Then Equation 6 holds, i.e. the group recovery algorithm is correct.
We refer to the Appendix for a proof. The assumption on $L$ in the above result is a relaxation of the unitarity assumption in Corollary 11 since $L=0$ if, and only if, $W$ is unitary up to a multiplicative constant.</p>
<p>We provide an explicit bound for the unitarity defect of the McCulloch-Pitts neurons discussed in Section 3.2. To this end, let $\mathcal{H}$ be the space of continuous functions defined on the unit sphere in $\langle G\rangle$ equipped with the uniform metric (i.e., the $L_{\infty}$ distance) as $\Delta$.
Proposition 18 Let $W \in\langle G\rangle$ and consider $\varphi(W, x)=\sigma(W x)$. Suppose that the activation function $\sigma: \mathbb{C} \rightarrow \mathbb{C}$ is continuous and satisfies the following coercivity condition: there exist constants $C \in \mathbb{R}_{&gt;0}, n \in \mathbb{N}$ such that for every $x \in \mathbb{C},|\sigma(0)-\sigma(x)| \geq C|x|^{n}$. Then the unitarity defect of $\varphi$ satisfies for $\delta&lt;C$ :</p>
<p>$$
\omega_{\varphi}(\delta) \leq \sqrt{2\left(1-\sqrt{1-\left(\frac{\delta}{C}\right)^{\frac{2}{n}}}\right)}
$$</p>
<p>We refer to the Appendix for a proof. Note that the coercivity condition from above plays the role of the assumption on $\sigma$ from Proposition 14.</p>
<h1>3.3.1. IMPLEMENTATION</h1>
<p>We now empirically explore the theory developed in this paper and demonstrate that Spectral Networks are able to recover the group structure in practice. We implement a non-commutative Power-Spectral Network $\varphi_{i}(W, x)=W_{i} x W_{i}^{\dagger} \bar{x}$ with weights $W \in \mathbb{C}^{d_{i} \times d_{i} \times d}$, where $d=|G|$ is the cardinality of the given group and $d_{1}, \ldots, d_{k}$ are the dimensions of its irreducible unitary representations. As discussed at the beginning of Section 3.3, we force $W_{i}(1)=I \in \mathbb{C}^{d_{i} \times d_{i}}$, where the index 1 is arbitrarily chosen.</p>
<p>Following Sanborn et al. (2023), we train the model via contrastive learning (Jaiswal et al., 2020). Namely, given a finite dataset $\mathcal{D}$ of pairs $(x, y)$, where $x, y \in\langle G\rangle \simeq \mathbb{C}^{d}$ and $x=g \cdot y$ for an unknown $g \in G$, the objective optimized by the model is:</p>
<p>$$
\mathcal{L}(W)=\sum_{(x, y) \in \mathcal{D}} \sum_{i}\left|\varphi_{i}(W, x)-\varphi_{i}(W, y)\right|^{2}+\eta\left|d I-W W^{\dagger}\right|^{2}
$$</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Multiplication tables inferred by Power-Spectral Networks for various groups. Group elements are labeled with integers. The table is symmetric if, and only if, the group is commutative.
where $\eta&gt;0$ is a hyper-parameter and $|\cdot|$ is the Frobenius norm. The first term in Equation 9 encourages invariance with respect to $G$ while the second one encourages $W$ to be unitary. The model is trained via the Adam optimizer (Kingma and Ba, 2014), which interprets the complex weights as real tensors of doubled dimensionality. The dataset $\mathcal{D} \subseteq\langle G\rangle \oplus\langle G\rangle$ is generated procedurally by first sampling $x$ from a standard Gaussian distribution over $\langle G\rangle \simeq \mathbb{R}^{2 d}$, then sampling $g \in G$ uniformly, and finally producing the datapoint $(x, y=g \cdot x) \in \mathcal{D}$. We provide a Python implementation of both the model and the experiments at a public repository (see Section 1.1). The code is available in both the PyTorch (Paszke et al., 2019) and the JAX (Bradbury et al., 2018) frameworks.</p>
<p>Once trained, we evaluate the model by checking whether the multiplication table $M \in$ ${1, \ldots, d}^{d \times d}$ obtained via the group recovery algorithm described by Equation 6 coincides with the one of $G$. Since there is no canonical ordering on $G$, the table is recovered up to a permutation $\pi$ of ${1, \ldots, d}$ acting as $(\pi \cdot M)<em _pi_-1="\pi^{-1">{i, j}=\pi\left(M</em>$.}(i), \pi^{-1}(j)}\right)$. Therefore, we check whether $\pi \cdot M$ coincides with the table of $G$ for all the permutations $\pi$. Figure 2 reports the (correct) multiplication tables obtained at convergence for both commutative and non-commutative groups. Specifically, we consider the cyclic group $C_{6}$, the product of cyclic groups $C_{2} \times C_{2} \times C_{2}$, and the dihedral group $D_{3}$, which is isomorphic to the symmetric group $S_{3</p>
<p>In order to validate empirically the robustness of the group recovery procedure, we additionally train the model on data corrupted by white noise, i.e. $\mathcal{D}$ consists of pairs $(x, y=g \cdot x+\epsilon)$, where $\epsilon$ is sampled from an isotropic Gaussian distribution. We vary the standard deviation of the latter and report in Figure 3 the number of times the multiplication table is recovered correctly across 20 training runs - a metric referred to as "table accuracy". As can be seen, the group structure is recovered most of the times even with large amounts of noise - up to $\sim 0.5$ standard deviation. The performance quickly degrades as the noise reaches a critical threshold. This shows empirically that the group recovery procedure is robust to noise, which is in line with the bounds from Theorem 17.</p>
<h1>4. Conclusions, Limitations, and Future Work</h1>
<p>In this work, we proved that if a machine learning model of a certain kind is invariant to a finite group, then its weights are closely related to the Fourier transform on that group. We discussed how, as a consequence, the algebraic structure of an unknown group can be recovered from a model that is invariant. We established these results for both commutative and non-commutative groups, and</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Accuracy for the recovery of the group multiplication tables across 20 training runs as the amount of noise injected into data increases. Since the transition from 1.0 to 0.0 accuracy is sharp, we visualize only the noise regions where the values are non-trivial.
discussed relaxed conditions under which the group recovery procedure holds. Our results represent a first step towards a mathematical explanation of universal features inferred by both biological and artificial neural networks. As such, this work contributes to the line of mathematical results stemming from biology (Sturmfels, 2005).</p>
<p>Due to its open-ended nature, this work is subject to a number of limitations and leaves directions open for future investigation. First, our theory encompasses models with complex-valued weights, which are non-canonical in machine learning. Thus, exploring analogues of the theory over real numbers is an interesting direction that would fit more directly with current practices in the field.</p>
<p>In addition, our theoretical framework encompasses learning models with unitary symmetries. The latter is a technical property satisfied by Spectral Networks and, to an extent, by traditional deep networks. However, it is not clear what other models fit into our framework, and whether the notion is general enough to accommodate other computational primitives fundamental to machine learning, such as attention mechanisms. This is an open question that is worthy of investigation.</p>
<p>Lastly, in this work we focused on groups and their associated harmonics. However, the representations within neural networks or biological systems often resemble imperfect, or localized, versions of harmonics, i.e. wavelets, such as Gabors. Since wavelets do not describe group homomorphisms, our theory would need to be extended to accommodate this kind of locality. We suspect that this may be achieved by generalizing the framework to groupoids - an algebraic group-like structure that formalizes a locally-defined composition. This, however, goes beyond the scope of our work, and we leave it for future research.</p>
<h1>Acknowledgments</h1>
<p>This work was supported by the Swedish Research Council, the Knut and Alice Wallenberg Foundation and the European Research Council (ERC-BIRD-884807).</p>
<h1>References</h1>
<p>Edward H Adelson and James R Bergen. Spatiotemporal energy models for the perception of motion. Josa a, 2(2):284-299, 1985.</p>
<p>Andrea Banino, Caswell Barry, Benigno Uria, Charles Blundell, Timothy Lillicrap, Piotr Mirowski, Alexander Pritzel, Martin J Chadwick, Thomas Degris, Joseph Modayil, et al. Vector-based navigation using grid-like representations in artificial agents. Nature, 557(7705):429-433, 2018.</p>
<p>Joshua Bassey, Lijun Qian, and Xianfang Li. A survey of complex-valued neural networks. arXiv preprint arXiv:2101.12249, 2021.</p>
<p>Anthony J Bell and Terrence J Sejnowski. The "independent components" of natural scenes are edge filters. Vision research, 37(23):3327-3338, 1997.</p>
<p>James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs. In GitHub, 2018.</p>
<p>Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Veličković. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478, 2021.</p>
<p>Bilal Chughtai, Lawrence Chan, and Neel Nanda. A toy model of universality: Reverse engineering how networks learn group operations. arXiv preprint arXiv:2302.03025, 2023.</p>
<p>Taco Cohen and Max Welling. Group equivariant convolutional networks. In Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 2990-2999, 2016.</p>
<p>Christopher J Cueva and Xue-Xin Wei. Emergence of grid-like representations by training recurrent neural networks to perform spatial localization. arXiv preprint arXiv:1803.07770, 2018.</p>
<p>Krish Desai, Benjamin Nachman, and Jesse Thaler. Symmetry discovery with deep learning. Physical Review D, 105(9):096031, 2022.</p>
<p>William Dorrell, Peter E Latham, Timothy EJ Behrens, and James CR Whittington. Actionable neural representations: Grid cells from minimal constraints. arXiv preprint arXiv:2209.15563, 2022.</p>
<p>Ursula C Dräger. Receptive fields of single cells and topography in mouse visual cortex. Journal of Comparative Neurology, 160(3):269-289, 1975.</p>
<p>Pierre-Étienne H Fiquet and Eero P Simoncelli. Polar prediction of natural videos. arXiv preprint arXiv:2303.03432, 2023.</p>
<p>Gerald B Folland. A course in abstract harmonic analysis, volume 29. CRC press, 2016.
Richard J Gardner, Erik Hermansen, Marius Pachitariu, Yoram Burak, Nils A Baas, Benjamin A Dunn, May-Britt Moser, and Edvard I Moser. Toroidal topology of population activity in grid cells. Nature, 602(7895):123-128, 2022.</p>
<p>Charles J Garfinkle and Christopher J Hillar. On the uniqueness and stability of dictionaries for sparse representation of noisy signals. IEEE Transactions on Signal Processing, 67(23):5884-5892, 2019 .</p>
<p>Alexis Guanella, Daniel Kiper, and Paul Verschure. A model of grid cells based on a twisted torus topology. International journal of neural systems, 17(04):231-240, 2007.</p>
<p>Charles A Hass and Gregory D Horwitz. V1 mechanisms underlying chromatic contrast detection. Journal of Neurophysiology, 109(10):2483-2494, 2013.</p>
<p>Christopher J Hillar and Friedrich T Sommer. When can dictionary learning uniquely recover sparse data from subsamples? IEEE Transactions on Information Theory, 61(11):6290-6297, 2015.</p>
<p>Geoffrey E Hinton, Sara Sabour, and Nicholas Frosst. Matrix capsules with em routing. In International conference on learning representations, 2018.</p>
<p>Roger A Horn and Charles R Johnson. Matrix analysis. Cambridge university press, 2012.
Roger A Horn, Roger A Horn, and Charles R Johnson. Topics in matrix analysis. Cambridge university press, 1994.</p>
<p>David H Hubel and Torsten N Wiesel. Receptive fields of single neurones in the cat's striate cortex. The Journal of physiology, 148(3):574, 1959.</p>
<p>David H Hubel and Torsten N Wiesel. Receptive fields, binocular interaction and functional architecture in the cat's visual cortex. The Journal of physiology, 160(1):106, 1962.</p>
<p>David H Hubel and Torsten N Wiesel. Receptive fields and functional architecture of monkey striate cortex. The Journal of physiology, 195(1):215-243, 1968.</p>
<p>Jarmo Hurri and Aapo Hyvärinen. Simple-cell-like receptive fields maximize temporal coherence in natural video. Neural Computation, 15(3):663-691, 2003.</p>
<p>Guy Isely, Christopher Hillar, and Fritz Sommer. Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication. Advances in neural information processing systems, 23, 2010.</p>
<p>Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A survey on contrastive self-supervised learning. Technologies, 9(1):2, 2020.</p>
<p>Ramakrishna Kakarala. Completeness of bispectrum on compact groups. arXiv preprint arXiv:0902.0196, 1, 2009.</p>
<p>Ramakrishna Kakarala. The bispectrum as a source of phase-sensitive invariants for Fourier descriptors: a group-theoretic approach. Journal of Mathematical Imaging and Vision, 44:341-353, 2012.</p>
<p>Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>Risi Kondor and Shubhendu Trivedi. On the generalization of equivariance and convolution in neural networks to the action of compact groups. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 2747-2755, 2018.</p>
<p>Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John Hopcroft. Convergent learning: Do different neural networks learn the same representations? arXiv preprint arXiv:1511.07543, 2015.</p>
<p>Sindy Löwe, Phillip Lippe, Maja Rudolph, and Max Welling. Complex-valued autoencoders for object discovery. arXiv preprint arXiv:2204.02075, 2022.</p>
<p>Warren S McCulloch and Walter Pitts. A logical calculus of the ideas immanent in nervous activity. The bulletin of mathematical biophysics, 5:115-133, 1943.</p>
<p>Luca Moschella, Valentino Maiorca, Marco Fumero, Antonio Norelli, Francesco Locatello, and Emanuele Rodola. Relative representations enable zero-shot latent space communication. arXiv preprint arXiv:2209.15430, 2022.</p>
<p>Edvard I Moser, Emilio Kropff, and May-Britt Moser. Place cells, grid cells, and the brain's spatial representation system. Annu. Rev. Neurosci., 31:69-89, 2008.</p>
<p>Neel Nanda, Lawrence Chan, Tom Liberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking via mechanistic interpretability. arXiv preprint arXiv:2301.05217, 2023.</p>
<p>Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. An overview of early vision in inceptionV1. Distill, 5(4):e00024-002, 2020.</p>
<p>Bruno A Olshausen and David J Field. Sparse coding with an overcomplete basis set: A strategy employed by V1? Vision research, 37(23):3311-3325, 1997.</p>
<p>Jeff Orchard, Hao Yang, and Xiang Ji. Does the entorhinal cortex use the Fourier transform? Frontiers in computational neuroscience, 7:179, 2013.</p>
<p>Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32, pages 8024-8035. Curran Associates, Inc., 2019.</p>
<p>Walter Pitts and Warren S McCulloch. How we know universals: The perception of auditory and visual forms. The Bulletin of mathematical biophysics, 9:127-147, 1947.</p>
<p>Rajesh Rao and Daniel Ruderman. Learning Lie groups for invariant visual perception. Advances in neural information processing systems, 11, 1998.</p>
<p>Tilman Räuker, Anson Ho, Stephen Casper, and Dylan Hadfield-Menell. Toward transparent ai: A survey on interpreting the inner structures of deep neural networks. In 2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML), pages 464-483. IEEE, 2023.</p>
<p>Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between capsules. Advances in neural information processing systems, 30, 2017.</p>
<p>Sophia Sanborn, Christian Shewmake, Bruno Olshausen, and Christopher Hillar. Bispectral neural networks. International Conference on Learning Representations (ICLR), 2023.</p>
<p>Rylan Schaeffer, Mikail Khona, Tzuhsuan Ma, Cristóbal Eyzaguirre, Sanmi Koyejo, and Ila Rani Fiete. Self-supervised learning of representations for space generates multi-modular grid cells. arXiv preprint arXiv:2311.02316, 2023.</p>
<p>Fethi Smach, Cedric Lemaître, Jean-Paul Gauthier, Johel Miteran, and Mohamed Atri. Generalized Fourier descriptors with applications to objects recognition in SVM context. Journal of mathematical imaging and Vision, 30:43-71, 2008.</p>
<p>Jascha Sohl-Dickstein, Ching Ming Wang, and Bruno A Olshausen. An unsupervised algorithm for learning Lie group transformations. arXiv preprint arXiv:1001.1027, 2010.</p>
<p>Ben Sorscher, Gabriel Mel, Surya Ganguli, and Samuel Ocko. A unified theory for the origin of grid cells through the lens of pattern formation. Advances in neural information processing systems, 32, 2019 .</p>
<p>Ben Sorscher, Gabriel C Mel, Samuel A Ocko, Lisa M Giocomo, and Surya Ganguli. A unified theory for the computational and mechanistic origins of grid cells. Neuron, 111(1):121-137, 2023.</p>
<p>Benjamin Steinberg. Representation theory of finite groups: an introductory approach. Springer, 2012.</p>
<p>Bernd Sturmfels. Can biology lead to new theorems. Annual report of the Clay Mathematics Institute, 1468:13-26, 2005.</p>
<p>Bernd Sturmfels. Algorithms in invariant theory. Springer Science \&amp; Business Media, 2008.
Chiheb Trabelsi, Olexa Bilaniuk, Ying Zhang, Dmitriy Serdyuk, Sandeep Subramanian, João Felipe Santos, Soroush Mehri, Negar Rostamzadeh, Yoshua Bengio, and Christopher J Pal. Deep complex networks (2017). arXiv preprint arXiv:1705.09792, 2017.</p>
<p>Konstantin F Willeke, Kelli Restivo, Katrin Franke, Arne F Nix, Santiago A Cadena, Tori Shinn, Cate Nealley, Gabby Rodriguez, Saumil Patel, Alexander S Ecker, et al. Deep learning-driven characterization of single cell tuning in primate visual area V4 unveils topological organization. bioRxiv, pages 2023-05, 2023.</p>
<h1>Appendix A. Proofs of Theoretical Results</h1>
<h2>A.1. Proof of Theorem 10</h2>
<p>Proof Since $\varphi$ has unitary symmetries and $\left|g \cdot W_{i}\right|=\left|W_{i}\right|$ for all $i$, it follows that for every $g \in G$ and every $i$ there exists $\rho_{i}(g) \in \mathrm{U}\left(V_{i}\right)$ such that:</p>
<p>$$
g \cdot W_{i}=\rho_{i}(g) \cdot W_{i}
$$</p>
<p>In particular, by considering the component with index $1 \in G$ on both sides of Equation 10, we see that $W_{i}\left(g^{-1}\right)=W_{i}(1) \rho_{i}(g)$. We wish to show that $\rho_{i}$ is a homomorphism. To this end, for all $g, h \in G$ it holds that:</p>
<p>$$
\rho_{i}(g h) \cdot W_{i}=(g h) \cdot W_{i}=g \cdot\left(\rho_{i}(h) \cdot W_{i}\right)=\rho_{i}(h) \cdot\left(g \cdot W_{i}\right)=(\rho(g) \rho(h)) \cdot W_{i}
$$</p>
<p>By the surjectivity hypothesis, the set $\left{W_{i}(g)\right}<em i="i">{g \in G}$ generates $\operatorname{End}\left(V</em>$.}\right)$ as a vector space. Therefore, Equation 11 implies that $\rho_{i}(g h)=\rho_{i}(g) \rho_{i}(h)$, as desired. Lastly, note that $\rho_{i}$ is irreducible by the sujectivity assumption. Indeed, a non-trivial linear subspace of $V_{i}$ fixed by $\rho_{i}(g)$ for all $g$ would be sent by $W_{i}(g)$ into a fixed proper subspace due to Equation 10. This contradicts the surjectivity of $W_{i</p>
<h2>A.2. Proof of Corollary 11</h2>
<p>Proof Firstly, note that the unitarity assumption implies the surjectivity assumption from Theorem 10. Therefore, it follows that for every $i$, there exists an irreducible unitary representation $\rho_{i}: G \rightarrow \mathrm{U}\left(V_{i}\right)$ such that $W_{i}(g)=W_{i}(1) \rho_{i}(g)^{\dagger}$. We wish to show that if $i \neq j$ then $\rho_{i}$ and $\rho_{j}$ are non-isomorphic representations. If not, the orthogonality assumption implies that:</p>
<p>$$
\begin{aligned}
0=\sum_{g \in G} \overline{W_{i}(g)} \otimes W_{j}(g) &amp; =\sum_{g \in G}\left(\overline{W_{i}(1)} \otimes W_{j}(1)\right)\left(\rho_{i}(g)^{\top} \otimes \rho_{j}(g)^{\dagger}\right)= \
&amp; =|G| \overline{W_{i}(1)} \otimes W_{j}(1)
\end{aligned}
$$</p>
<p>where $\top$ denotes the transpose and where the last identity follows from the Schur orthogonality relations. But then $W_{i}(1)$ or $W_{j}(1)$ is vanishing, which contraddicts the unitarity assumption.</p>
<p>Lastly, in order to compute the Frobenius norm of $W_{i}(1)$, note that:</p>
<p>$$
|G|\left|W_{i}(1)\right|^{2}=\sum_{g \in G}\left|W_{i}(g) \rho_{i}(g)\right|^{2}=\sum_{g \in G}\left|W_{i}(g)\right|^{2}=|G|
$$</p>
<p>from which $\left|W_{i}(1)\right|=1$ follows.</p>
<h2>A.3. Proof of Proposition 13</h2>
<p>In order to prove this proposition, we will need some technical facts from matrix algebra. We start by showing the following uniqueness result.</p>
<p>Lemma 19 Let $d^{\prime} \geq d$ and $A, B \in \mathbb{C}^{d^{\prime} \times d}$. If $A A^{\dagger}=B B^{\dagger}$, then there exists a unitary matrix $U \in \mathbb{C}^{d \times d}$ such that $A=B U$.</p>
<p>Proof From the polar decomposition for matrices (Horn et al., 1994, Theorem 3.1.9), we know that:</p>
<p>$$
A=P V, B=Q W
$$</p>
<p>where $P, Q \in \mathbb{C}^{d^{\prime} \times d^{\prime}}$ are Hermitian positive semidefinite and $V, W \in \mathbb{C}^{d^{\prime} \times d}$ have orthonormal rows. Also, $P^{2}=A A^{\dagger}=B B^{\dagger}=Q^{2}$ from which it follows that $P=Q$ by uniqueness of square roots of positive semidefinite Hermitian matrices (Horn and Johnson, 2012, Theorem 7.2.6). In particular, we have:</p>
<p>$$
A=P V=Q V=Q W W^{\dagger} V=B U
$$</p>
<p>with $U=W^{\dagger} V$ unitary.</p>
<p>Remark 20 We note that if $A$ and $B$ are real matrices, then the conclusion holds with $U$ being a real orthogonal matrix.</p>
<p>Next, we show that positive semidefinite Hermitian matrices possess unique tensor roots.
Lemma 21 Let $A, B \in \mathbb{C}^{d \times d}$ be Hermitian and positive semidefinite. If $A^{\otimes n}=B^{\otimes n}$ for some $n&gt;0$, then $A=B$.</p>
<p>Proof From the Spectral Theorem we know that:</p>
<p>$$
A=U D U^{\dagger}
$$</p>
<p>where $U$ is unitary and $D$ is diagonal. Since $A^{\otimes n}=B^{\otimes n}$, it follows that $D^{\otimes n}=C^{\otimes n}$, where $C=U^{\dagger} B U$. Note that the (point-wise) Hadamard product of matrices is a submatrix of the tensor (Kronecker) product. In particular, the off-diagonal entries of $C$ must vanish. On the diagonal we have $\left(D_{i, i}\right)^{n}=\left(C_{i, i}\right)^{n}$ for every $i$, and therefore $D_{i, i}=C_{i, i}$ since they are non-negative. This shows that $C=D$, implying that $B=U D U^{\dagger}=A$.</p>
<p>Putting together the above lemmas, we obtain the following.
Lemma 22 Let $A_{1}, \ldots, A_{k}, B_{1}, \ldots, B_{k} \in \mathbb{C}^{d \times d}$. Suppose that for some $n&gt;0$, for all $x \in \mathbb{C}^{k}$ :</p>
<p>$$
\left(\sum_{i} x_{i} A_{i}\right)^{\otimes n}\left(\sum_{i} \bar{x}<em i="i">{i} A</em>}^{\dagger \otimes n}\right)=\left(\sum_{i} x_{i} B_{i}\right)^{\otimes n}\left(\sum_{i} \bar{x<em i="i">{i} B</em>\right)
$$}^{\dagger \otimes n</p>
<p>Then there exists a unitary matrix $U \in \mathbb{C}^{d \times d}$ such that $A_{i}=B_{i} U$ for every $i$.
Proof By multilinearity of the tensor product we see that for all $x \in \mathbb{C}^{k}$ :</p>
<p>$$
\begin{aligned}
&amp; \sum_{i_{1}, \ldots, i_{n+1}} x_{i_{1}} \cdots x_{i_{n}} \bar{x}<em n_1="n+1">{i</em>= \
= &amp; \sum_{i_{1}, \ldots, i_{n+1}} x_{i_{1}} \cdots x_{i_{n}} \bar{x}}}\left(A_{i_{1}} \otimes \cdots \otimes A_{i_{n}}\right) A_{i_{n+1}}^{\dagger \otimes n<em n_1="n+1">{i</em>\right)= \
= &amp; \sum_{i_{1}, \ldots, i_{n+1}} x_{i_{1}} \cdots x_{1_{n}} \bar{x}}}\left(A_{i_{1}} A_{i_{n+1}}^{\dagger}\right) \otimes \cdots \otimes\left(A_{i_{n}} A_{i_{n+1}}^{\dagger<em n_1="n+1">{i</em>\right)
\end{aligned}
$$}}\left(B_{i_{1}} B_{i_{n+1}}^{\dagger}\right) \otimes \cdots \otimes\left(B_{i_{n}} B_{i_{n+1}}^{\dagger</p>
<p>Since a polynomial vanishes as function if and only if it is the zero polynomial, it follows that $\left(A_{i_{1}} A_{i_{n+1}}^{\dagger}\right) \otimes \cdots \otimes\left(A_{i_{n}} A_{i_{n+1}}^{\dagger}\right)=\left(B_{i_{1}} B_{i_{n+1}}^{\dagger}\right) \otimes \cdots \otimes\left(B_{i_{n}} B_{i_{n+1}}^{\dagger}\right)$ for all $i_{1}, \ldots, i_{n+1}$, and in particular $\left(A_{i} A_{j}^{\dagger}\right)^{\otimes n}=\left(B_{i} B_{j}^{\dagger}\right)^{\otimes n}$ for all $i, j$. From Lemma 21 we conclude that $A_{i} A_{j}=B_{i} B_{j}$ for all $i, j$, which can be rephrased as $A A^{\dagger}=B B^{\dagger}$, where $A, B$ are the $(d k) \times d$ matrices obtained by concatenating the $A_{i}$ 's and $B_{i}$ 's, respectively. From Lemma 19 we conclude that $A=B U$ for a unitary $d \times d$ matrix $U$, as desired.</p>
<p>Proposition 13 now follows immediately from Lemma 22 by setting $A_{i}=W\left(g_{i}\right)$ and $B_{i}=$ $W^{\prime}\left(g_{i}\right)$ for $g_{i} \in G$ and $W, W^{\prime} \in\langle G\rangle \otimes \operatorname{End}(V)$ of the same norm.</p>
<h1>A.4. Proof of Proposition 14</h1>
<p>Proof Pick $W, W^{\prime} \in\langle G\rangle$ of the same norm such that $\varphi(W, x)=\varphi\left(W^{\prime}, x\right)$ for all $x \in\langle G\rangle$. Given the open set $O \subseteq \mathbb{C}$ from the hypothesis on $\sigma$, consider $O^{\prime}=\left{x \in\langle G\rangle \mid W x, W^{\prime} x \in O\right}$, which is open and non-empty since $0 \in O^{\prime}$. For $x \in O^{\prime}, W x=0$ implies $\varphi(W, x)=\varphi\left(W^{\prime}, x\right)=0$, from which it follows that $W^{\prime} x=0$ by definition of $O$. Therefore, $W$ and $W^{\prime}$ share the same orthogonal complement, implying that $W^{\prime}=\rho W$ for some $\rho \in \mathbb{C}$. Since $W$ and $W^{\prime}$ have the same norm, we conclude that $\rho \in \mathrm{U}(\mathbb{C})$.</p>
<h2>A.5. Proof of Proposition 15</h2>
<p>Proof Pick $W, W^{\prime} \in \mathcal{W}$ such that $\left|W_{i}\right|=\left|W_{i}^{\prime}\right|$ for all $i$ and $\varphi(W, x)=\varphi\left(W^{\prime}, x\right)$ for all $x \in\langle G\rangle$. Given the open set $O \subseteq \mathbb{R}^{k}$ from the hypothesis on $\chi$, consider the set $O^{\prime}={x \in$ $\langle G\rangle \mid \sigma(W(x)), \sigma\left(W^{\prime}(x)\right) \in O}$, which is open and non-empty since $0 \in O^{\prime}$. For $x \in O^{\prime}$, an immediate calculation shows that:</p>
<p>$$
\partial_{x} \partial_{\bar{x}} \varphi(W, x)=\sum_{i} a_{i} \partial_{z} \partial_{\bar{z}} \sigma\left(W_{i} x\right) W_{i} \otimes \overline{W_{i}}
$$</p>
<p>with the $a_{i}$ 's being distinct, and similarly for $\varphi\left(W^{\prime}, x\right)$. By specializing to $x=0$, we deduce that:</p>
<p>$$
\partial_{z} \partial_{\bar{z}} \sigma(\underline{0}) \sum_{i} a_{i} W_{i} \otimes \overline{W_{i}}=\partial_{z} \partial_{\bar{z}} \sigma(\underline{0}) \sum_{i} a_{i} W_{i}^{\prime} \otimes \overline{W_{i}^{\prime}}
$$</p>
<p>Since both the sets $\left{W_{i}\right}<em i="i">{i}$ and $\left{W</em>\right}}^{\prime<em i="i">{i}$ are orthonormal by hypothesis on $\mathcal{W}$, both sides of Equation 23 define a spectral decomposition, i.e. a decomposition into projections over orthonormal vectors. From the hypothesis on $\chi$ it follows that the eigenvalues $a</em>)$, as desired.} \partial_{z} \partial_{\bar{z}} \sigma\left(W_{i} x\right)$ of the operators in Equation 23 are distinct and non-vanishing, implying that the eigenvectors coincide up to a multiplicative scalar, i.e. $W_{i}=\rho_{i} W_{i}^{\prime}$ for some $\rho_{i} \in \mathrm{U}(\mathbb{C</p>
<h2>A.6. Proof of Theorem 17</h2>
<p>Proof Firstly, the definition of $L$ implies the inequalities $|G|-L \leq|W(g)|^{2} \leq|G|+L$ and $|\langle W(g), W(h)\rangle| \leq L$ for all $g, h \in G$. In particular, we have:</p>
<p>$$
|W(g)-W(h)|^{2}=|W(g)|^{2}+|W(h)|^{2}-2 \operatorname{Re}(\langle W(g), W(g)\rangle) \geq 2|G|-4 L
$$</p>
<p>By hypothesis, for every $i$ and $g \in G$ there exists $\rho_{i}(g) \in \mathrm{U}\left(V_{i}\right)$ such that:</p>
<p>$$
\left|g^{-1} \cdot W_{i}-\rho_{i}(g) \cdot W_{i}\right|&lt;\frac{\sqrt{\frac{1}{2}-\frac{L}{|G|}}}{\sqrt{|G|+L}+1}
$$</p>
<p>In particular, $\left|W_{i}(g h)-\rho_{i}(g) W_{i}(h)\right|$ is bounded by the same quantity for all $g, h \in G$. Therefore, via the triangle inequality we see that:</p>
<p>$$
\begin{aligned}
&amp; \left|W_{i}(g) W_{i}(h)-W_{i}(g h)\right| \leq \
\leq &amp; \left|W_{i}(h) W_{i}(g)-\rho_{i}(g) W_{i}(h)\right|+\left|W_{i}(g h)-\rho_{i}(g) W_{i}(h)\right|= \
= &amp; \underbrace{\left|W_{i}(h)\right|}<em i="i">{\leq \sqrt{|G|+L}}\left|W</em>(h)\right|&lt; \
&lt; &amp; \sqrt{\frac{1}{2}-\frac{L}{|G|}}
\end{aligned}
$$}(g)-\rho_{i}(g) W_{i}(1)\right|+\left|W_{i}(g h)-\rho_{i}(g) W_{i</p>
<p>Equation 26 implies that:</p>
<p>$$
|W(g) \odot W(h)-W(g h)|&lt;\sqrt{|G|} \sqrt{\frac{1}{2}-\frac{L}{|G|}}=\frac{\sqrt{2|G|-4 L}}{2}
$$</p>
<p>Since $|W(p)-W(q)| \geq \sqrt{2|G|-4 L}$ for all $p \neq q \in G, W(g) \odot W(h)$ is closer to $W(g h)$ than to any other $W(q)$ for $q \in G$, which immediately implies the desired result.</p>
<h1>A.7. Proof of Proposition 18</h1>
<p>In order to prove this proposition, we will need the following technical fact from linear algebra.
Lemma 23 Let $H$ be a finite-dimensional complex Hilbert space, $v, w \in H$ normal and $\varepsilon \in \mathbb{R}$ such that $0&lt;\varepsilon&lt;1$. Suppose that for every normal $x$ orthogonal to $w$, it holds that $|\langle x, v\rangle| \leq \varepsilon$. Then there exists $\rho \in \mathrm{U}(\mathbb{C})$ such that:</p>
<p>$$
|v-\rho w| \leq \sqrt{2\left(1-\sqrt{1-\varepsilon^{2}}\right)}
$$</p>
<p>Proof Consider an orthogonal decomposition $v=\left\langle w_{1}, v\right\rangle w_{1}+\left\langle w_{2}, v\right\rangle w_{2}$, where $w_{1} \in w^{\perp}$ is normal and $w_{2}=\rho w$ for some $\rho \in \mathrm{U}(\mathbb{C})$ such that $\left\langle w_{2}, v\right\rangle \in \mathbb{R}_{\geq 0}$. It follows that:</p>
<p>$$
1=|v|^{2}=\left|\left\langle w_{1}, v\right\rangle\right|^{2}+\left\langle w_{2}, v\right\rangle^{2}
$$</p>
<p>The hypothesis implies then that $\left\langle w_{2}, v\right\rangle \geq \sqrt{1-\varepsilon^{2}}$. Therefore, we have:</p>
<p>$$
\left|v-w_{2}\right|^{2}=2-2\left\langle w_{2}, v\right\rangle \leq 2\left(1-\sqrt{1-\varepsilon^{2}}\right)
$$</p>
<p>as desired.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ol>
<li>The trivial sub-representations of $V$ are 0 and $V$.</li>
</ol>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
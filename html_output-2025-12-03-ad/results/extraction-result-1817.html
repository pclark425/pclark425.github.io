<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1817 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1817</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1817</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-6ca16c1c2c60ceda87242c8f8e522d12cc4a13bc</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6ca16c1c2c60ceda87242c8f8e522d12cc4a13bc" target="_blank">Eureka: Human-Level Reward Design via Coding Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> Eureka is presented, a human-level reward design algorithm powered by LLMs that exploits the remarkable zero-shot generation, code-writing, and in-context improvement capabilities of state-of-the-art LLMs to perform evolutionary optimization over reward code.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have excelled as high-level semantic planners for sequential decision-making tasks. However, harnessing them to learn complex low-level manipulation tasks, such as dexterous pen spinning, remains an open problem. We bridge this fundamental gap and present Eureka, a human-level reward design algorithm powered by LLMs. Eureka exploits the remarkable zero-shot generation, code-writing, and in-context improvement capabilities of state-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization over reward code. The resulting rewards can then be used to acquire complex skills via reinforcement learning. Without any task-specific prompting or pre-defined reward templates, Eureka generates reward functions that outperform expert human-engineered rewards. In a diverse suite of 29 open-source RL environments that include 10 distinct robot morphologies, Eureka outperforms human experts on 83% of the tasks, leading to an average normalized improvement of 52%. The generality of Eureka also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF), readily incorporating human inputs to improve the quality and the safety of the generated rewards without model updating. Finally, using Eureka rewards in a curriculum learning setting, we demonstrate for the first time, a simulated Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a pen in circles at rapid speed.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1817.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1817.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (as used by EUREKA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (Generative Pre-trained Transformer 4) used as backbone coding LLM in EUREKA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large pre-trained language-and-code model (GPT-4) used as a code-writing, zero-shot and in-context engine to generate executable reward functions that are then used to train 3D embodied policies via reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>GPT-4 (used as coding LLM in EUREKA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Large transformer-based language model pretrained on large-scale language and code corpora and used here as a coding LLM to emit executable Python reward programs (TorchScript-compatible). GPT-4 is used only to generate and iteratively refine reward code; it does not directly output low-level motor actions or policies.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Language and code (large-scale web text and code corpora)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>IsaacGym + Dexterity suite (29 tasks across 10 robot morphologies); pen-spinning curriculum on Shadow Hand</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>A diverse suite of 3D simulated robotics tasks implemented in IsaacGym: 9 IsaacGym tasks (quadruped, bipedal, quadrotor, cobot arm, dexterous hands, etc.) and 20 Dexterity benchmark (bi-manual ShadowHand) tasks. Objectives include locomotion, object reorientation, handover, dexterous manipulation, and a custom pen-spinning curriculum on a Shadow Hand (goal: rapidly rotate a pen through spinning configurations for many cycles). Policies are trained with PPO in GPU-accelerated IsaacGym.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>N/A for pretraining (GPT-4 is pretrained on text/code, not on a text-action environment); text pretraining involves tokens and code generation, not explicit action commands.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Continuous low-level motor controls / joint torques or joint-position targets used by PPO policies in IsaacGym; high-dimensional continuous action spaces for dexterous hands (e.g., joint commands), discrete/continuous controls for other robots.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Indirect: GPT-4 generates executable reward functions (code) from environment source code + task text; the generated reward is plugged into an RL loop (PPO) which learns low-level continuous control policies. There is no direct mapping from GPT-4 tokens to motor commands — mapping is mediated by RL using the generated reward signal.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>State-based observations (proprioceptive and object state vectors exposed by environment code). The evaluated environments use numeric state observations (positions, orientations, velocities, object poses); raw RGB/vision input was not the primary modality in these benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Using GPT-4 within EUREKA to generate reward code led to policies that: outperformed expert human-engineered rewards on 83% of tasks and achieved an average normalized improvement of 52% over human rewards (aggregate IsaacGym/Dexterity results reported in the paper). EUREKA (GPT-4 backbone) exceeded or matched human-level on all Isaac tasks and 15/20 Dexterity tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>GPT-4's code-generation ability + environment-as-context (feeding environment source code into LLM) enabled zero-shot executable reward synthesis; iterative in-context evolutionary search (sampling and mutation via the LLM) plus GPU-accelerated RL evaluation enabled effective reward-to-policy transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Not discussed in detail: potential mismatches include that GPT-4 does not directly act on motor-level controls and success depends on the RL algorithm's ability to optimize the generated reward; initial single-shot generations can be buggy or suboptimal without evolutionary refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A coding LLM pretrained on language and code (GPT-4) can be repurposed (without fine-tuning) to generate executable reward functions that, when used to train RL agents in 3D simulated environments, produce strong policy-level performance — outperforming hand-designed rewards on many high-dimensional embodied tasks. Transfer occurs indirectly via reward synthesis rather than direct text→action mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Eureka: Human-Level Reward Design via Coding Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1817.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1817.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rt-2: Vision-language-action models transfer web knowledge to robotic control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced as an example of vision–language–action models that transfer web-scale knowledge to robotic control tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rt-2: Vision-language-action models transfer web knowledge to robotic control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>RT-2 (vision-language-action model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A vision-language-action model (cited) that incorporates web knowledge into robotic control; referenced in related work as an example of models that ground language/web knowledge for robotics.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Web-scale textual knowledge / web data (as implied by title)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Robotic control tasks (general; cited as transferring to robotic control)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Not detailed in this paper; referenced generically as robot control use-cases.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Not specified in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Described in citation as vision-language-action models (no further detail provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Implied vision + language grounding (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Eureka: Human-Level Reward Design via Coding Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1817.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1817.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited as an example of an open-ended embodied agent that integrates large language models to operate in embodied environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>An embodied agent design that leverages a large language model to drive open-ended behavior in an embodied setting (cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large language model pretraining / internet-scale text (implied by title)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Open-ended embodied interaction tasks (general)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Not specified in this paper; cited as related work integrating LLMs into embodied agents.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Eureka: Human-Level Reward Design via Coding Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1817.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1817.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Code-as-Policies</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Code as policies: Language model programs for embodied control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced work that uses coding LLMs to produce structured programmatic outputs (policies) for embodied control problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Code as policies: Language model programs for embodied control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Code-as-Policies (LLM-generated programs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Approach using code-capable LLMs to synthesize programmatic policies or plan fragments for embodied control; cited as related work for programmatic agent generation.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Code and language corpora (implied)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Embodied control tasks (general; cited)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Generates programmatic routines/policies via code-generation (cited), rather than directly mapping natural-language actions to low-level motor commands within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Eureka: Human-Level Reward Design via Coding Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1817.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1817.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MineDojo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Minedojo: Building open-ended embodied agents with internet-scale knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited as prior work that builds open-ended embodied agents using internet-scale textual knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Minedojo: Building open-ended embodied agents with internet-scale knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>MineDojo-style agents (LLM-powered open-ended agents)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Frameworks that leverage internet-scale knowledge and language models to support open-ended embodied agent behavior (referenced as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Internet-scale textual knowledge (language)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Open-ended embodied agent tasks (general)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Eureka: Human-Level Reward Design via Coding Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1817.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1817.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>L2R (Language-to-Rewards)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language to Rewards (L2R)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An existing two-stage LLM-based method that generates templated rewards by first producing a motion description from text and then converting it to reward API calls; used in this paper as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language to rewards for robotic skill synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>L2R (two-stage LLM pipeline for reward synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Two-stage approach: (1) an LLM produces a motion description from a natural-language task description, (2) a second LLM maps the motion description into code that calls a predefined set of reward API primitives. Produces templated, componentized reward programs.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Pretrained language models (text); implementation uses LLM(s) to process natural language instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Same IsaacGym and Dexterity tasks used in this paper (implemented as a baseline comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>L2R was evaluated as a baseline on the same suite of simulated 3D robotics tasks (IsaacGym and Dexterity). The paper's L2R implementation used templates and reward API primitives resembling human reward components.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>High-level motion description language (templated natural-language statements describing motion/relations)</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Low-level continuous motor control via RL policies trained on generated reward functions (same as other methods in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Maps text-derived motion descriptions to calls into engineered reward-API primitives; the reward sum is then optimized by RL to produce continuous motor policies. This is a templated conversion from language -> reward-API calls (manually specified APIs).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Same state-based observations as other methods in the benchmark (environment-specific state vectors exposed to reward and policy).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>L2R performed comparably to EUREKA on low-dimensional tasks (e.g., CartPole, BallBalance) but underperformed on high-dimensional dexterity tasks; it lagged significantly behind EUREKA on complex tasks despite being given access to components resembling human rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Availability of engineered reward primitives and motion templates; L2R benefits when tasks are expressible in its templated language and when reward primitives match task needs.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Limited expressivity of templated reward primitives and reliance on manual templates hindered performance on high-dimensional dexterous tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>In this paper L2R is a weaker baseline: templated, API-driven reward generation struggles on complex, high-dimensional embodied tasks compared to free-form coding-LLM-based reward synthesis (EUREKA with GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Eureka: Human-Level Reward Design via Coding Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1817.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1817.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VIP / LIV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VIP / LIV (Language-image representations and rewards for robotic control / Value-implicit pre-training work by Ma et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited prior works exploring language-image or value-implicit pretraining for robotic rewards and representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>VIP / LIV (language-image / value-implicit pretraining approaches)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Prior work by the same authors exploring language-image representations and reward design (cited in related work). These methods leverage multimodal pretraining to produce representations/rewards for control.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Language-image (multimodal) pretraining (implied); value-implicit pretraining methods (implied)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Robotic control / reward learning (general references in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Eureka: Human-Level Reward Design via Coding Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Rt-2: Vision-language-action models transfer web knowledge to robotic control <em>(Rating: 2)</em></li>
                <li>Voyager: An open-ended embodied agent with large language models <em>(Rating: 2)</em></li>
                <li>Code as policies: Language model programs for embodied control <em>(Rating: 2)</em></li>
                <li>Minedojo: Building open-ended embodied agents with internet-scale knowledge <em>(Rating: 2)</em></li>
                <li>Language to rewards for robotic skill synthesis <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1817",
    "paper_id": "paper-6ca16c1c2c60ceda87242c8f8e522d12cc4a13bc",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "GPT-4 (as used by EUREKA)",
            "name_full": "GPT-4 (Generative Pre-trained Transformer 4) used as backbone coding LLM in EUREKA",
            "brief_description": "A large pre-trained language-and-code model (GPT-4) used as a code-writing, zero-shot and in-context engine to generate executable reward functions that are then used to train 3D embodied policies via reinforcement learning.",
            "citation_title": "Gpt-4 technical report",
            "mention_or_use": "use",
            "model_agent_name": "GPT-4 (used as coding LLM in EUREKA)",
            "model_agent_description": "Large transformer-based language model pretrained on large-scale language and code corpora and used here as a coding LLM to emit executable Python reward programs (TorchScript-compatible). GPT-4 is used only to generate and iteratively refine reward code; it does not directly output low-level motor actions or policies.",
            "pretraining_data_type": "Language and code (large-scale web text and code corpora)",
            "pretraining_data_details": null,
            "embodied_task_name": "IsaacGym + Dexterity suite (29 tasks across 10 robot morphologies); pen-spinning curriculum on Shadow Hand",
            "embodied_task_description": "A diverse suite of 3D simulated robotics tasks implemented in IsaacGym: 9 IsaacGym tasks (quadruped, bipedal, quadrotor, cobot arm, dexterous hands, etc.) and 20 Dexterity benchmark (bi-manual ShadowHand) tasks. Objectives include locomotion, object reorientation, handover, dexterous manipulation, and a custom pen-spinning curriculum on a Shadow Hand (goal: rapidly rotate a pen through spinning configurations for many cycles). Policies are trained with PPO in GPU-accelerated IsaacGym.",
            "action_space_text": "N/A for pretraining (GPT-4 is pretrained on text/code, not on a text-action environment); text pretraining involves tokens and code generation, not explicit action commands.",
            "action_space_embodied": "Continuous low-level motor controls / joint torques or joint-position targets used by PPO policies in IsaacGym; high-dimensional continuous action spaces for dexterous hands (e.g., joint commands), discrete/continuous controls for other robots.",
            "action_mapping_method": "Indirect: GPT-4 generates executable reward functions (code) from environment source code + task text; the generated reward is plugged into an RL loop (PPO) which learns low-level continuous control policies. There is no direct mapping from GPT-4 tokens to motor commands — mapping is mediated by RL using the generated reward signal.",
            "perception_requirements": "State-based observations (proprioceptive and object state vectors exposed by environment code). The evaluated environments use numeric state observations (positions, orientations, velocities, object poses); raw RGB/vision input was not the primary modality in these benchmarks.",
            "transfer_successful": true,
            "performance_with_pretraining": "Using GPT-4 within EUREKA to generate reward code led to policies that: outperformed expert human-engineered rewards on 83% of tasks and achieved an average normalized improvement of 52% over human rewards (aggregate IsaacGym/Dexterity results reported in the paper). EUREKA (GPT-4 backbone) exceeded or matched human-level on all Isaac tasks and 15/20 Dexterity tasks.",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "GPT-4's code-generation ability + environment-as-context (feeding environment source code into LLM) enabled zero-shot executable reward synthesis; iterative in-context evolutionary search (sampling and mutation via the LLM) plus GPU-accelerated RL evaluation enabled effective reward-to-policy transfer.",
            "transfer_failure_factors": "Not discussed in detail: potential mismatches include that GPT-4 does not directly act on motor-level controls and success depends on the RL algorithm's ability to optimize the generated reward; initial single-shot generations can be buggy or suboptimal without evolutionary refinement.",
            "key_findings": "A coding LLM pretrained on language and code (GPT-4) can be repurposed (without fine-tuning) to generate executable reward functions that, when used to train RL agents in 3D simulated environments, produce strong policy-level performance — outperforming hand-designed rewards on many high-dimensional embodied tasks. Transfer occurs indirectly via reward synthesis rather than direct text→action mapping.",
            "uuid": "e1817.0",
            "source_info": {
                "paper_title": "Eureka: Human-Level Reward Design via Coding Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "RT-2",
            "name_full": "Rt-2: Vision-language-action models transfer web knowledge to robotic control",
            "brief_description": "Referenced as an example of vision–language–action models that transfer web-scale knowledge to robotic control tasks.",
            "citation_title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control",
            "mention_or_use": "mention",
            "model_agent_name": "RT-2 (vision-language-action model)",
            "model_agent_description": "A vision-language-action model (cited) that incorporates web knowledge into robotic control; referenced in related work as an example of models that ground language/web knowledge for robotics.",
            "pretraining_data_type": "Web-scale textual knowledge / web data (as implied by title)",
            "pretraining_data_details": null,
            "embodied_task_name": "Robotic control tasks (general; cited as transferring to robotic control)",
            "embodied_task_description": "Not detailed in this paper; referenced generically as robot control use-cases.",
            "action_space_text": "Not specified in this paper",
            "action_space_embodied": "Not specified in this paper",
            "action_mapping_method": "Described in citation as vision-language-action models (no further detail provided in this paper).",
            "perception_requirements": "Implied vision + language grounding (not detailed here).",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": null,
            "transfer_failure_factors": null,
            "key_findings": null,
            "uuid": "e1817.1",
            "source_info": {
                "paper_title": "Eureka: Human-Level Reward Design via Coding Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Voyager",
            "name_full": "Voyager: An open-ended embodied agent with large language models",
            "brief_description": "Cited as an example of an open-ended embodied agent that integrates large language models to operate in embodied environments.",
            "citation_title": "Voyager: An open-ended embodied agent with large language models",
            "mention_or_use": "mention",
            "model_agent_name": "Voyager",
            "model_agent_description": "An embodied agent design that leverages a large language model to drive open-ended behavior in an embodied setting (cited in related work).",
            "pretraining_data_type": "Large language model pretraining / internet-scale text (implied by title)",
            "pretraining_data_details": null,
            "embodied_task_name": "Open-ended embodied interaction tasks (general)",
            "embodied_task_description": "Not specified in this paper; cited as related work integrating LLMs into embodied agents.",
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": null,
            "transfer_failure_factors": null,
            "key_findings": null,
            "uuid": "e1817.2",
            "source_info": {
                "paper_title": "Eureka: Human-Level Reward Design via Coding Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Code-as-Policies",
            "name_full": "Code as policies: Language model programs for embodied control",
            "brief_description": "Referenced work that uses coding LLMs to produce structured programmatic outputs (policies) for embodied control problems.",
            "citation_title": "Code as policies: Language model programs for embodied control",
            "mention_or_use": "mention",
            "model_agent_name": "Code-as-Policies (LLM-generated programs)",
            "model_agent_description": "Approach using code-capable LLMs to synthesize programmatic policies or plan fragments for embodied control; cited as related work for programmatic agent generation.",
            "pretraining_data_type": "Code and language corpora (implied)",
            "pretraining_data_details": null,
            "embodied_task_name": "Embodied control tasks (general; cited)",
            "embodied_task_description": "Not detailed in this paper.",
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": "Generates programmatic routines/policies via code-generation (cited), rather than directly mapping natural-language actions to low-level motor commands within this paper.",
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": null,
            "transfer_failure_factors": null,
            "key_findings": null,
            "uuid": "e1817.3",
            "source_info": {
                "paper_title": "Eureka: Human-Level Reward Design via Coding Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "MineDojo",
            "name_full": "Minedojo: Building open-ended embodied agents with internet-scale knowledge",
            "brief_description": "Cited as prior work that builds open-ended embodied agents using internet-scale textual knowledge.",
            "citation_title": "Minedojo: Building open-ended embodied agents with internet-scale knowledge",
            "mention_or_use": "mention",
            "model_agent_name": "MineDojo-style agents (LLM-powered open-ended agents)",
            "model_agent_description": "Frameworks that leverage internet-scale knowledge and language models to support open-ended embodied agent behavior (referenced as related work).",
            "pretraining_data_type": "Internet-scale textual knowledge (language)",
            "pretraining_data_details": null,
            "embodied_task_name": "Open-ended embodied agent tasks (general)",
            "embodied_task_description": "Not described in this paper.",
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": null,
            "transfer_failure_factors": null,
            "key_findings": null,
            "uuid": "e1817.4",
            "source_info": {
                "paper_title": "Eureka: Human-Level Reward Design via Coding Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "L2R (Language-to-Rewards)",
            "name_full": "Language to Rewards (L2R)",
            "brief_description": "An existing two-stage LLM-based method that generates templated rewards by first producing a motion description from text and then converting it to reward API calls; used in this paper as a baseline.",
            "citation_title": "Language to rewards for robotic skill synthesis",
            "mention_or_use": "use",
            "model_agent_name": "L2R (two-stage LLM pipeline for reward synthesis)",
            "model_agent_description": "Two-stage approach: (1) an LLM produces a motion description from a natural-language task description, (2) a second LLM maps the motion description into code that calls a predefined set of reward API primitives. Produces templated, componentized reward programs.",
            "pretraining_data_type": "Pretrained language models (text); implementation uses LLM(s) to process natural language instructions.",
            "pretraining_data_details": null,
            "embodied_task_name": "Same IsaacGym and Dexterity tasks used in this paper (implemented as a baseline comparison)",
            "embodied_task_description": "L2R was evaluated as a baseline on the same suite of simulated 3D robotics tasks (IsaacGym and Dexterity). The paper's L2R implementation used templates and reward API primitives resembling human reward components.",
            "action_space_text": "High-level motion description language (templated natural-language statements describing motion/relations)",
            "action_space_embodied": "Low-level continuous motor control via RL policies trained on generated reward functions (same as other methods in paper)",
            "action_mapping_method": "Maps text-derived motion descriptions to calls into engineered reward-API primitives; the reward sum is then optimized by RL to produce continuous motor policies. This is a templated conversion from language -&gt; reward-API calls (manually specified APIs).",
            "perception_requirements": "Same state-based observations as other methods in the benchmark (environment-specific state vectors exposed to reward and policy).",
            "transfer_successful": null,
            "performance_with_pretraining": "L2R performed comparably to EUREKA on low-dimensional tasks (e.g., CartPole, BallBalance) but underperformed on high-dimensional dexterity tasks; it lagged significantly behind EUREKA on complex tasks despite being given access to components resembling human rewards.",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Availability of engineered reward primitives and motion templates; L2R benefits when tasks are expressible in its templated language and when reward primitives match task needs.",
            "transfer_failure_factors": "Limited expressivity of templated reward primitives and reliance on manual templates hindered performance on high-dimensional dexterous tasks.",
            "key_findings": "In this paper L2R is a weaker baseline: templated, API-driven reward generation struggles on complex, high-dimensional embodied tasks compared to free-form coding-LLM-based reward synthesis (EUREKA with GPT-4).",
            "uuid": "e1817.5",
            "source_info": {
                "paper_title": "Eureka: Human-Level Reward Design via Coding Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "VIP / LIV",
            "name_full": "VIP / LIV (Language-image representations and rewards for robotic control / Value-implicit pre-training work by Ma et al.)",
            "brief_description": "Cited prior works exploring language-image or value-implicit pretraining for robotic rewards and representations.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_agent_name": "VIP / LIV (language-image / value-implicit pretraining approaches)",
            "model_agent_description": "Prior work by the same authors exploring language-image representations and reward design (cited in related work). These methods leverage multimodal pretraining to produce representations/rewards for control.",
            "pretraining_data_type": "Language-image (multimodal) pretraining (implied); value-implicit pretraining methods (implied)",
            "pretraining_data_details": null,
            "embodied_task_name": "Robotic control / reward learning (general references in related work)",
            "embodied_task_description": "Not specified in this paper.",
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": null,
            "perception_requirements": null,
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": null,
            "transfer_failure_factors": null,
            "key_findings": null,
            "uuid": "e1817.6",
            "source_info": {
                "paper_title": "Eureka: Human-Level Reward Design via Coding Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control",
            "rating": 2
        },
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models",
            "rating": 2
        },
        {
            "paper_title": "Code as policies: Language model programs for embodied control",
            "rating": 2
        },
        {
            "paper_title": "Minedojo: Building open-ended embodied agents with internet-scale knowledge",
            "rating": 2
        },
        {
            "paper_title": "Language to rewards for robotic skill synthesis",
            "rating": 2
        }
    ],
    "cost": 0.018256249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Eureka: Human-level Reward Design via Coding Large Language Models</h1>
<p>Yecheng Jason Ma ${ }^{12 \boxtimes}$, William Liang ${ }^{2}$, Guanzhi Wang ${ }^{13}$, De-An Huang ${ }^{1}$, Osbert Bastani ${ }^{2}$, Dinesh Jayaraman ${ }^{2}$, Yuke Zhu ${ }^{14}$, Linxi "Jim" Fan ${ }^{1 \boxtimes \dagger}$, Anima Anandkumar ${ }^{13 \dagger}$<br>${ }^{1}$ NVIDIA, ${ }^{2}$ UPenn, ${ }^{3}$ Caltech, ${ }^{4}$ UT Austin; ${ }^{\dagger}$ Equal advising<br>https://eureka-research.github.io</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) have excelled as high-level semantic planners for sequential decision-making tasks. However, harnessing them to learn complex lowlevel manipulation tasks, such as dexterous pen spinning, remains an open problem. We bridge this fundamental gap and present EUREKA, a human-level reward design algorithm powered by LLMs. EUREKA exploits the remarkable zero-shot generation, code-writing, and in-context improvement capabilities of state-of-theart LLMs, such as GPT-4, to perform evolutionary optimization over reward code. The resulting rewards can then be used to acquire complex skills via reinforcement learning. Without any task-specific prompting or pre-defined reward templates, EUREKA generates reward functions that outperform expert human-engineered rewards. In a diverse suite of 29 open-source RL environments that include 10 distinct robot morphologies, EUREKA outperforms human experts on $\mathbf{8 3 \%}$ of the tasks, leading to an average normalized improvement of $\mathbf{5 2 \%}$. The generality of EUREKA also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF), readily incorporating human inputs to improve the quality and the safety of the generated rewards without model updating. Finally, using EUREKA rewards in a curriculum learning setting, we demonstrate for the first time, a simulated Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a pen in circles at rapid speed.</p>
<h2>1 INTRODUCTION</h2>
<p>Large Language Models (LLMs) have excelled as high-level semantic planners for robotics tasks (Ahn et al., 2022; Singh et al., 2023), but whether they can be used to learn complex low-level manipulation tasks, such as dexterous pen spinning, remains an open problem. Existing attempts require substantial
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: EUREKA generates human-level reward functions across diverse robots and tasks. Combined with curriculum learning, EUREKA for the first time, unlocks rapid pen-spinning capabilities on an anthropomorphic five-finger hand.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: EUREKA takes unmodified environment source code and language task description as context to zero-shot generate executable reward functions from a coding LLM. Then, it iterates between reward sampling, GPU-accelerated reward evaluation, and reward reflection to progressively improve its reward outputs.
domain expertise to construct task prompts or learn only simple skills, leaving a substantial gap in achieving human-level dexterity (Yu et al., 2023; Brohan et al., 2023).</p>
<p>On the other hand, reinforcement learning (RL) has achieved impressive results in dexterity (Andrychowicz et al., 2020; Handa et al., 2023) as well as many other domains-if the human designers can carefully construct reward functions that accurately codify and provide learning signals for the desired behavior; likewise, many real-world RL tasks admit sparse rewards that are difficult for learning, necessitating reward shaping that provides incremental learning signals. Despite their fundamental importance, reward functions are known to be notoriously difficult to design in practice (Russell \&amp; Norvig, 1995; Sutton \&amp; Barto, 2018); a recent survey conducted finds $92 \%$ of polled reinforcement learning researchers and practitioners report manual trial-and-error reward design and $89 \%$ indicate that their designed rewards are sub-optimal (Booth et al., 2023) and lead to unintended behavior (Hadfield-Menell et al., 2017).</p>
<p>Given the paramount importance of reward design, we ask whether it is possible to develop a universal reward programming algorithm using state-of-the-art coding LLMs, such as GPT-4. Their remarkable abilities in code writing, zero-shot generation, and in-context learning have previously enabled effective programmatic agents (Shinn et al., 2023; Wang et al., 2023a). Ideally, this reward design algorithm should achieve human-level reward generation capabilities that scale to a broad spectrum of tasks, including dexterity, automate the tedious trial-and-error procedure without human supervision, and yet be compatible with human oversight to assure safety and alignment.</p>
<p>We introduce Evolution-driven Universal REward Kit for Agent (EUREKA), a novel reward design algorithm powered by coding LLMs with the following contributions:</p>
<ol>
<li>Achieves human-level performance on reward design across a diverse suite of 29 open-sourced RL environments that include 10 distinct robot morphologies, including quadruped, quadcopter, biped, manipulator, as well as several dexterous hands; see Fig. 1. Without any task-specific prompting or reward templates, EUREKA autonomously generates rewards that outperform expert human rewards on $\mathbf{8 3 \%}$ of the tasks and realizes an average normalized improvement of $\mathbf{5 2 \%}$.</li>
<li>Solves dexterous manipulation tasks that were previously not feasible by manual reward engineering. We consider pen spinning, in which a five-finger hand needs to rapidly rotate a pen in pre-defined spinning configurations for as many cycles as possible. Combining EUREKA with curriculum learning, we demonstrate for the first time rapid pen spinning maneuvers on a simulated anthropomorphic Shadow Hand (see Figure 1 bottom).</li>
<li>Enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF) that can generate more performant and human-aligned reward functions</li>
</ol>
<p>based on various forms of human inputs without model updating. We demonstrate that EUREKA can readily benefit from and improve upon existing human reward functions. Likewise, we showcase EUREKA's capability in using purely textual feedback to generate progressively more human-aligned reward functions.</p>
<p>Unlike prior work L2R on using LLMs to aid reward design (Yu et al., 2023), EUREKA is completely free of task-specific prompts, reward templates, as well as few-shot examples. In our experiments, EUREKA significantly outperforms L2R due to its ability to generate free-form, expressive reward programs. EUREKA's generality is made possible through three key algorithmic design choices: environment as context, evolutionary search, and reward reflection. First, by taking the environment source code as context, EUREKA can zero-shot generate executable reward functions from the backbone coding LLM (GPT-4). Then, EUREKA substantially improves the quality of its rewards by performing evolutionary search, iteratively proposing batches of reward candidates and refining the most promising ones within the LLM context window. This in-context improvement is made effective via reward reflection, a textual summary of the reward quality based on policy training statistics that enables automated and targeted reward editing; see Fig. 3 for an example of EUREKA zero-shot reward as well as various improvements accumulated during its optimization. To ensure that EUREKA can scale up its reward search to maximum potential, EUREKA evaluates intermediate rewards using GPU-accelerated distributed reinforcement learning on IsaacGym (Makoviychuk et al., 2021), which offers up to three orders of magnitude in policy learning speed, making EUREKA an extensive algorithm that scales naturally with more compute. See Fig. 2 for an overview. We are committed to open-sourcing all prompts, environments, and generated reward functions to promote further research on LLM-based reward design.</p>
<h1>2 Problem Setting and Definitions</h1>
<p>The goal of reward design is to return a shaped reward function for a ground-truth reward function that may be difficult to optimize directly (e.g., sparse rewards); this ground-truth reward function may only be accessed via queries by the designer. We first introduce the formal definition from Singh et al. (2010), which we then adapt to the program synthesis setting, which we call reward generation.
Definition 2.1. (Reward Design Problem (Singh et al., 2010)) A reward design problem (RDP) is a tuple $P=\left\langle M, \mathcal{R}, \pi_{M}, F\right\rangle$, where $M=(S, A, T)$ is the world model with state space $S$, action space $A$, and transition function $T . \mathcal{R}$ is the space of reward functions; $\mathcal{A}<em M="M">{M}(\cdot): \mathcal{R} \rightarrow \Pi$ is a learning algorithm that outputs a policy $\pi: S \rightarrow \Delta(A)$ that optimizes reward $R \in \mathcal{R}$ in the resulting Markov Decision Process (MDP), $(M, R) ; F: \Pi \rightarrow \mathbb{R}$ is the fitness function that produces a scalar evaluation of any policy, which may only be accessed via policy queries (i.e., evaluate the policy using the ground truth reward function). In an RDP, the goal is to output a reward function $R \in \mathcal{R}$ such that the policy $\pi:=\mathcal{A}</em>(R)$ that optimizes $R$ achieves the highest fitness score $F(\pi)$.</p>
<p>Reward Generation Problem. In our problem setting, every component within a RDP is specified via code. Then, given a string $l$ that specifies the task, the objective of the reward generation problem is to output a reward function code $R$ such that $F\left(\mathcal{A}_{M}(R)\right)$ is maximized.</p>
<h2>3 Method</h2>
<p>EUREKA consists of three algorithmic components: 1) environment as context that enables zero-shot generation of executable rewards, 2) evolutionary search that iteratively proposes and refines reward candidates, and 3) reward reflection that enables fine-grained reward improvement. See Alg. 1 for pseudocode; all prompts are included in App. A.</p>
<h3>3.1 Environment as Context</h3>
<p>Reward design requires the environment specification to be provided to the LLM. We propose directly feeding the raw environment source code (without the reward code, if exists) as context. Given that any reward function is a function over the environment's state and action variables, the only requirement in the source code is that it exposes these environment variables, which is easy to satisfy. In cases where the source code is not available, relevant state information can also be supplied via</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: EUREKA can zero-shot generate executable rewards and then flexibly improve them with many distinct types of free-form modification, such as (1) changing the hyperparameter of existing reward components, (2) changing the functional form of existing reward components, and (3) introducing new reward components.
an API, for example. In practice, to ensure that the environment code fits within the LLM’s context window and does not leak simulation internals (so that we can expect the same prompt to generalize to new simulators), we have an automatic script to extract just the environment code snippets that expose and fully specify the environment state and action variables. see App. D for details.</p>
<p>Given environment as context, EUREKA instructs the coding LLM to directly return executable Python code with only generic reward design and formatting tips, such as exposing individual components in the reward as a dictionary output (for reasons that will be apparent in Sec. 3.3); see Prompt 1 and 3 in App. A. Remarkably, with only these minimal instructions, EUREKA can already zero-shot generate plausibly-looking rewards in diverse environments in its first attempts. An example EUREKA output is shown in Fig. 3. As seen, EUREKA adeptly composes over existing observation variables (e.g.,</p>
<div class="codehilite"><pre><span></span><code>fingertip_pos) in the provided
</code></pre></div>

<p>environment code and produces a competent reward code - all without any environment-specific prompt engineering or reward templating. On the first try, however, the generated reward may not always be executable, and even if it is, it can be quite sub-optimal with respect to the task fitness metric $F$. While we can improve the prompt with task-specific formatting and reward design hints, doing so does not scale to new tasks and hinders the overall generality of our system. How can we effectively overcome the sub-optimality of single-sample reward generation?</p>
<h1>3.2 Evolutionary Search</h1>
<p>In this section, we will demonstrate how evolutionary search presents a natural solution that addresses the aforementioned execution error and sub-optimality challenges. In each iteration, EUREKA samples several independent outputs from the LLM (Line 5 in Alg. 1). Since the generations are i.i.d, the probability that all reward functions from an iteration are buggy exponentially decreases as the number of samples increases. We find that for all environments we consider, sampling just a modest number of samples (16) contains at least one executable reward code in the first iteration.</p>
<p>Given executable reward functions from an earlier iteration, EUREKA performs in-context reward mutation, proposing new improved reward functions from the best one in the previous iteration. Concretely, a new EUREKA iteration will take the best-performing reward from the previous iteration, its reward reflection (Sec. 3.3), and the mutation prompt (Prompt 2 in App. A) as context and generate $K$ more i.i.d reward outputs from the LLM; several illustrative reward modifications are visualized in Fig. 3. This iterative optimization continues until a specified number of iterations has been reached. Finally, we perform multiple random restarts to find better maxima; this is a standard strategy in global optimization. In all our experiments, EUREKA conducts 5 independent runs per environment, and for each run, searches for 5 iterations with $K=16$ samples per iteration.</p>
<h3>3.3 Reward Reflection</h3>
<p>In order to ground the in-context reward mutation, we must be able to put into words the quality of the generated rewards. We propose reward reflection, an automated feedback that summarizes the policy training dynamics in texts. Specifically, given that EUREKA reward functions are asked to expose their individual components in the reward program (e.g., reward_components in Fig. 3), reward reflection tracks the scalar values of all reward components and the task fitness function at intermediate policy checkpoints throughout training. For instance, consider the illustrative example in Fig. 2, where the snapshot values of av_penalty are provided as a list in the reward feedback. See App. G. 1 for full examples.</p>
<p>This reward reflection procedure, though simple to construct, is important due to two reasons: (1) the lack of fine-grained reward improvement signal in the task fitness function, and (2) the algorithmdependent nature of reward optimization (Booth et al., 2023). First, as we can query the task fitness function $F$ on the resulting policies, a simple strategy is to just provide this numerical score as the reward evaluation. While serving as the holistic ground-truth metric, the task fitness function itself lacks in credit assignment, providing no useful information on why a reward function works or not. Second, whether a reward function is effective is influenced by the particular choice of RL algorithm, and the same reward may perform very differently even under the same optimizer given hyperparameter differences (Henderson et al., 2018; Agarwal et al., 2021). By providing detailed accounts on how well the RL algorithm optimizes individual reward components, reward reflection enables EUREKA to produce more intricate and targeted reward editing.</p>
<h2>4 EXPERIMENTS</h2>
<p>We thoroughly evaluate EUREKA on a diverse suite of robot embodiments and tasks, testing its ability to generate reward functions, solve new tasks, and incorporate various forms of human input. We use GPT-4 (OpenAI, 2023), in particular the gpt-4-0314 variant, as the backbone LLM for all LLM-based reward-design algorithms unless specified otherwise.</p>
<p>Environments. Our environments consist of 10 distinct robots and 29 tasks implemented using the IsaacGym simulator (Makoviychuk et al., 2021). First, we include 9 original environments from IsaacGym (Isaac), covering a diverse set of robot morphologies from quadruped, bipedal, quadrotor, cobot arm, to dexterous hands. In addition to coverage over robot form factors, we ensure depth in our evaluation by including all 20 tasks from the Bidexterous Manipulation (Dexterity) benchmark (Chen et al., 2022). Dexterity contains 20 complex bi-manual tasks that require a pair of Shadow Hands to solve a wide range of complex manipulation skills, ranging from object handover to rotating a cup by 180 degrees. For the task description input to EUREKA, we use the official description provided in the environment repository when possible. See App. B for details on all environments. It is worth noting that both benchmarks are publicly released concurrently, or after the GPT-4 knowledge cut-off date (September 2021), so GPT-4 is unlikely to have accumulated extensive internet knowledge</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: EUREKA outperforms Human and L2R across all tasks. In particular, EUREKA realizes much greater gains on high-dimensional dexterity environments.
about these tasks, making them ideal testbeds for assessing EUREKA's reward generation capability compared to measurable human-engineered reward functions.</p>
<h1>4.1 BASELINES</h1>
<p>L2R (Yu et al., 2023) proposes a two-stage LLM-prompting solution to generate templated rewards. For an environment and task specified in natural language, a first LLM is asked to fill in a natural language template describing the agent's motion; then, a second LLM is asked to convert this "motion description" into code that calls a manually defined set of reward API primitives to write a reward program that sets their parameters. To make L2R competitive for our tasks, we define the motion description template to mimic the original L2R templates, and we construct the API reward primitives using the individual components of the original human rewards when possible. Note that this gives L2R an advantage as it has access to the original reward functions. Consistent with EUREKA, we conduct 5 independent L2R runs per environment, and for each run, we generate 16 reward samples. See App. C for more details.</p>
<p>Human. These are the original shaped reward functions provided in our benchmark tasks. As these reward functions are written by active reinforcement learning researchers who designed the tasks, these reward functions represent the outcomes of expert-level human reward engineering.</p>
<p>Sparse. These are identical to the fitness functions $F$ that we use to evaluate the quality of the generated rewards. Like Human, these are also provided by the benchmark. On the dexterity tasks, they are uniformly binary indicator functions that measure task success; on Isaac tasks, they vary in functional forms depending on the nature of the task. See App. B for a description of the ground-truth scoring metric for all tasks.</p>
<h3>4.2 TRAINING DETAILS</h3>
<p>Policy Learning. For each task, all final reward functions are optimized using the same RL algorithm with the same set of hyperparameters. Isaac and Dexterity share a well-tuned PPO implementation (Schulman et al., 2017; Makoviichuk \&amp; Makoviychuk, 2021), and we use this implementation and the task-specific PPO hyperparameters without any modification. Note that these task hyperparameters are tuned to make the official human-engineered rewards work well. For each final reward function obtained from each method, we run 5 independent PPO training runs and report the average of the maximum task metric values achieved from 10 policy checkpoints sampled at fixed intervals. In particular, the maximum is taken over the same number of checkpoints for each approach.</p>
<p>Reward Evaluation Metrics. For Isaac tasks, since the task metric $F$ for each task varies in semantic meaning and scale, we report the human normalized score for EUREKA and L2R, $\frac{\text { Method-Sparse }}{[\text { Human-Sparse] }}$. This metric provides a holistic measure of how EUREKA rewards fare against human-expert rewards with respect to the ground-truth task metric. For Dexterity, since all tasks are evaluated using the binary success function, we directly report success rates.</p>
<h3>4.3 ReSULtS</h3>
<p>EUREKA outperforms human rewards. In Figure 4, we report the aggregate results on Dexterity and Isaac. Notably, EUREKA exceeds or performs on par to human level on all Isaac tasks and 15 out of 20 tasks on Dexterity (see App. F for a per-task breakdown). In contrast, L2R, while comparable</p>
<p>on low-dimensional tasks (e.g., CartPole, BallBalance), lags significantly behind on high-dimensional tasks. Despite being provided access to some of the same reward components as Human, L2R still underperforms EUREKA after its initial iteration, when both methods have had the same number of reward queries. As expected, L2R’s lack of expressivity severely limits its performance. In contrast, EUREKA generates free-form rewards from scratch without any domain-specific knowledge and performs substantially better. In App. F, we present results on additional evaluation metrics such as interquantile mean (IQM), probability of improvement (Agarwal et al., 2021), and the aggregate RL training curves; on all evaluations, we observe the consistent trend that EUREKA generates the most capable reward functions. Furthermore, we ablate GPT-4 with GPT-3.5 and find EUREKA degrades in performance but still matches or exceeds human-level on most Isaac tasks, indicating that its general principles can be readily applied to coding LLMs of varying qualities.</p>
<p>EUREKA consistently improves over time. In Fig. 5, we visualize the average performance of the cumulative best EUREKA rewards after each evolution iteration. Moreover, we study an ablation, EUREKA w.o. Evolution (32 Samples), which performs only the initial reward generation step, sampling the same number of reward functions as two iterations in the original EUREKA. This ablation helps study, given a fixed number of reward function budget, whether it is more advantageous to perform the
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: EUREKA progressively produces better rewards via incontext evolutionary reward search.</p>
<p>EUREKA evolution or simply sample more first-attempt rewards without iterative improvement. As seen, on both benchmarks, EUREKA rewards steadily improve and eventually surpass human rewards in performance despite sub-par initial performances. This consistent improvement also cannot be replaced by just sampling more in the first iteration as the ablation’s performances are lower than EUREKA after 2 iterations on both benchmarks. Together, these results demonstrate that EUREKA’s novel evolutionary optimization is indispensable for its final performance.</p>
<p>EUREKA generates novel rewards. We assess the novelty of EUREKA rewards by computing the <em>correlations</em> between EUREKA and human rewards on all the Isaac tasks; see App. B for details on this procedure. Then, we plot the correlations against the human normalized scores on a scatter-plot in Figure 6, where each point represents a single EUREKA reward on a single task. As shown, EUREKA mostly generates weakly correlated reward functions that outperform the human ones. In addition, by examining the average correlation by task (App. F), we observe that <em>the harder the task is, the less correlated the EUREKA rewards</em>. We hypothesize that human rewards are less likely to be near optimal for difficult tasks, leaving more room for EUREKA rewards to be different and better. In a few cases, EUREKA rewards are even <em>negatively</em> correlated with human rewards but perform significantly better, demonstrating that EUREKA can <em>discover</em> novel reward design principles that may run counter to human intuition; we illustrate these EUREKA rewards in App. G.2.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Eureka generates novel rewards.</p>
<p>Reward reflection enables targeted improvement. To assess the importance of constructing reward reflection in the reward feedback, we evaluate an ablation, EUREKA (No Reward Reflection), which reduces the reward feedback prompt to include only snapshot values of the task metric $F$. Averaged over all Isaac tasks, EUREKA without reward reflection reduces the average normalized score by 28.6%; in App. F, we provide detailed per-task breakdown and observe much greater performance deterioration on higher dimensional tasks. To provide qualitative analysis, in App. G.1, we include several examples in which EUREKA utilizes the reward reflection to perform targeted reward editing.</p>
<p>EUREKA with curriculum learning enables dexterous pen spinning. Finally, we investigate whether EUREKA can be used to solve a truly novel and challenging dexterous task. To this end, we propose pen spinning as a test bed. This task is highly dynamic and requires a Shadow Hand to continuously rotate a pen to achieve some pre-defined spinning patterns for as many cycles as possible; we implement this task on top of the original Shadow Hand environment in Isaac Gym without changes to any physics parameter, ensuring physical realism. We consider a curriculum learning (Bengio et al., 2009) approach to break down the task into manageable components that can be independently solved by EUREKA. Specifically, we first use EUREKA to generate a reward for the task of re-orienting the pen to random target configurations and train a policy using the final EUREKA reward. Then, using this pre-trained policy (Pre-Trained), we fine-tune it using the same EUREKA reward to reach the sequence of pen-spinning configurations (Fine-Tuned). To demonstrate the importance of curriculum learning, we also directly train a policy from scratch on the target task using EUREKA reward without the first-stage pre-training (Scratch). The RL training curves are shown in Figure 7. Eureka fine-tuning quickly adapts the policy to successfully spin the pen for many cycles in a row; see project website for videos. In contrast, neither pre-trained or learning-from-scratch policies can complete even a single cycle of pen spinning. In addition, using this EUREKA fine-tuning approach, we have also trained pen spinning policies for a variety of different spinning configurations; all pen spinning videos can be viewed on our project website, and experimental details are in App. D.1. These results demonstrate EUREKA’s applicability to advanced policy learning approaches, which are often necessary for learning very complex skills</p>
<h3>4.4 EUREKA From Human Feedback</h3>
<p>In addition to automated reward design, EUREKA enables a new gradient-free in-context learning approach to RL from Human Feedback (RLHF) that can readily incorporate various types of human inputs to generate more performant and human-aligned reward functions.</p>
<p>EUREKA can improve and benefit from human reward functions. We study whether starting with a human reward function initialization, a common scenario in real-world RL applications, is advantageous for EUREKA. Importantly, incorporating human initialization requires no modification to EUREKA – we can simply substitute the raw human reward function as the output of the first EUREKA iteration. To investigate this, we select several tasks from Dexterity that differ in the relative performances between the original EUREKA and human rewards. The full results are shown in Figure 8.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: EUREKA can be flexibly combined with curriculum learning to acquire complex dexterous skills.</p>
<p>As shown, regardless of the quality of the human rewards, EUREKA improves and benefits from human rewards as EUREKA (Human Init.) is uniformly better than both EUREKA and Human on all tasks. This suggests that EUREKA’s in-context reward improvement capability is largely independent of the quality of the base reward. Furthermore, the fact that EUREKA can significantly improve over human rewards even when they are highly sub-optimal hints towards an interesting hypothesis: human designers are generally knowledgeable about relevant state variables but are less proficient at designing rewards using them. This makes intuitive sense as identifying relevant state variables that should be included in the reward function involves mostly common sense reasoning, but reward design requires specialized knowledge and experience in RL. Together, these results demonstrate EUREKA’s reward assistant capability, perfectly complementing human designers’ knowledge about useful state variables and making up for their less proficiency on how to design rewards using them. In App. G.3, we provide several examples of EUREKA (Human Init.) steps.</p>
<p>Reward reflection via human feedback induces aligned behavior. So far, all EUREKA rewards are optimized against a fixed, black-box task fitness function $F$. This task metric, however, may not fully align with human intent. Moreover, in many open-ended tasks, $F$ may not be available in the first place (Fan et al., 2022). In these challenging scenarios, we propose to augment EUREKA by having humans step in and put into words the reward reflection in terms of the desired behavior and correction. We investigate this capability in EUREKA by teaching a Humanoid agent how to run purely from textual reward reflection; in App. G.4, we show the exact sequence of human feedback and EUREKA rewards. Then, we conduct a user study asking 20 unfamiliar users to indicate their preferences between two policy rollout videos shown in random order, one trained with human reward reflection (EUREKA-HF) and the other one trained with the original best EUREKA reward; the details are in App. D.3. As shown in Fig. 9, despite running a bit slower, the EUREKA-HF agent is preferred by a large majority of our users. Qualitative, we indeed see that the EUREKA-HF agent acquires safer and more stable gait, as instructed by the human. See the project website for a comparison.</p>
<h1>5 Related Work</h1>
<p>Reward Design. Reward engineering is a long-standing challenge in reinforcement learning (Singh et al., 2010; Sutton \&amp; Barto, 2018). The most common reward design method is manual trial-and-error (Knox et al., 2023; Booth et al., 2023). Inverse reinforcement learning (IRL) infers reward functions from demonstrations (Abbeel \&amp; Ng, 2004; Ziebart et al., 2008; Ho \&amp; Ermon, 2016), but it requires expensive expert data collection, which may not be available, and outputs non-interpretable black-box reward functions. Several prior works have studied automated reward search through evolutionary algorithms (Niekum et al., 2010; Chiang et al., 2019; Faust et al., 2019). These early attempts
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: EUREKA can incorporate human reward reflection to modify rewards that induce safer and more human-aligned behavior.
are limited to task-specific implementations of evolutionary algorithms that search over only parameters within provided reward templates. Recent works have also proposed using pretrained foundation models that can produce reward functions for new tasks (Ma et al., 2022; 2023; Fan et al., 2022; Du et al., 2023a; Karamcheti et al., 2023; Du et al., 2023b; Kwon et al., 2023). Most of these approaches output scalar rewards that lack interpretability and do not naturally admit the capability to improve or adapt rewards on-the-fly. In contrast, EUREKA adeptly generates free-form, white-box reward code and effectively in-context improves.</p>
<p>Code Large Language Models for Decision Making. Recent works have considered using coding LLMs (Austin et al., 2021; Chen et al., 2021; Rozière et al., 2023) to generate grounded and structured programmatic output for decision making and robotics problems (Liang et al., 2023; Singh et al., 2023; Wang et al., 2023b; Huang et al., 2023; Wang et al., 2023a; Liu et al., 2023a; Silver et al., 2023; Ding et al., 2023; Lin et al., 2023; Xie et al., 2023). However, most of these works rely on known motion primitives to carry out robot actions and do not apply to robot tasks that require low-level skill learning, such as dexterous manipulation. The closest to our work is a recent work (Yu et al., 2023) that also explores using LLMs to aid reward design. Their approach, however, requires domain-specific task descriptions and reward templates.</p>
<h2>6 CONCLUSION</h2>
<p>We have presented EUREKA, a universal reward design algorithm powered by coding large language models and in-context evolutionary search. Without any task-specific prompt engineering or human intervention, EUREKA achieves human-level reward generation on a wide range of robots and tasks. EUREKA's particular strength in learning dexterity solves dexterous pen spinning for the first time with a curriculum learning approach. Finally, EUREKA enables a gradient-free approach to reinforcement learning from human feedback that readily incorporates human reward initialization and textual feedback to better steer its reward generation. The versatility and substantial performance gains of EUREKA suggest that the simple principle of combining large language models with evolutionary algorithms are a general and scalable approach to reward design, an insight that may be generally applicable to difficult, open-ended search problems.</p>
<h1>ACKNOWLEDGEMENT</h1>
<p>We are grateful to colleagues and friends at NVIDIA and UPenn for their helpful feedback and insightful discussions. We thank Viktor Makoviychuk, Yashraj Narang, Iretiayo Akinola, Erwin Coumans for their assistance on Isaac Gym experiment and rendering. This work is done during Yecheng Jason Ma's internship at NVIDIA. We acknowledge funding support from NSF CAREER Award 2239301, ONR award N00014-22-1-2677, NSF Award CCF-1917852, and ARO Award W911NF-20-1-0080.</p>
<h1>REFERENCES</h1>
<p>Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-first international conference on Machine learning, pp. 1, 2004.</p>
<p>Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare. Deep reinforcement learning at the edge of the statistical precipice. Advances in neural information processing systems, 34:29304-29320, 2021.</p>
<p>Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.</p>
<p>Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving rubik's cube with a robot hand. arXiv preprint arXiv:1910.07113, 2019.</p>
<p>OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):3-20, 2020.</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.</p>
<p>Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pp. 41-48, 2009.</p>
<p>Serena Booth, W Bradley Knox, Julie Shah, Scott Niekum, Peter Stone, and Alessandro Allievi. The perils of trial-and-error reward design: misdesign through overfitting and invalid task specifications. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 5920-5929, 2023.</p>
<p>Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.</p>
<p>Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.</p>
<p>Yuanpei Chen, Tianhao Wu, Shengjie Wang, Xidong Feng, Jiechuan Jiang, Zongqing Lu, Stephen McAleer, Hao Dong, Song-Chun Zhu, and Yaodong Yang. Towards human-level bimanual dexterous manipulation with reinforcement learning. Advances in Neural Information Processing Systems, 35:5150-5163, 2022.</p>
<p>Hao-Tien Lewis Chiang, Aleksandra Faust, Marek Fiser, and Anthony Francis. Learning navigation behaviors end-to-end with autorl. IEEE Robotics and Automation Letters, 4(2):2007-2014, 2019.</p>
<p>Yan Ding, Xiaohan Zhang, Chris Paxton, and Shiqi Zhang. Task and motion planning with large language models for object rearrangement. arXiv preprint arXiv:2303.06247, 2023.</p>
<p>Yuqing Du, Ksenia Konyushkova, Misha Denil, Akhil Raju, Jessica Landon, Felix Hill, Nando de Freitas, and Serkan Cabi. Vision-language models as success detectors. arXiv preprint arXiv:2303.07280, 2023a.</p>
<p>Yuqing Du, Olivia Watkins, Zihan Wang, Cédric Colas, Trevor Darrell, Pieter Abbeel, Abhishek Gupta, and Jacob Andreas. Guiding pretraining in reinforcement learning with large language models. arXiv preprint arXiv:2302.06692, 2023b.</p>
<p>Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. Advances in Neural Information Processing Systems, 35: $18343-18362,2022$.</p>
<p>Aleksandra Faust, Anthony Francis, and Dar Mehta. Evolving rewards to automate reinforcement learning. arXiv preprint arXiv:1905.07628, 2019.</p>
<p>Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. In 2017 IEEE international conference on robotics and automation (ICRA), pp. 3389-3396. IEEE, 2017.</p>
<p>Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart J Russell, and Anca Dragan. Inverse reward design. Advances in neural information processing systems, 30, 2017.</p>
<p>Ankur Handa, Arthur Allshire, Viktor Makoviychuk, Aleksei Petrenko, Ritvik Singh, Jingzhou Liu, Denys Makoviichuk, Karl Van Wyk, Alexander Zhurkevich, Balakumar Sundaralingam, et al. Dextreme: Transfer of agile in-hand manipulation from simulation to reality. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pp. 5977-5984. IEEE, 2023.</p>
<p>Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.</p>
<p>Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in neural information processing systems, 29, 2016.</p>
<p>Taylor Howell, Nimrod Gileadi, Saran Tunyasuvunakool, Kevin Zakka, Tom Erez, and Yuval Tassa. Predictive sampling: Real-time behaviour synthesis with mujoco. arXiv preprint arXiv:2212.00541, 2022.</p>
<p>Siyuan Huang, Zhengkai Jiang, Hao Dong, Yu Qiao, Peng Gao, and Hongsheng Li. Instruct2act: Mapping multi-modality instructions to robotic actions with large language model. arXiv preprint arXiv:2305.11176, 2023.</p>
<p>Siddharth Karamcheti, Suraj Nair, Annie S Chen, Thomas Kollar, Chelsea Finn, Dorsa Sadigh, and Percy Liang. Language-driven representation learning for robotics. arXiv preprint arXiv:2302.12766, 2023.</p>
<p>W Bradley Knox, Alessandro Allievi, Holger Banzhaf, Felix Schmitt, and Peter Stone. Reward (mis) design for autonomous driving. Artificial Intelligence, 316:103829, 2023.</p>
<p>Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra Malik. Rma: Rapid motor adaptation for legged robots. arXiv preprint arXiv:2107.04034, 2021.</p>
<p>Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. Reward design with language models. arXiv preprint arXiv:2303.00001, 2023.</p>
<p>Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pp. 9493-9500. IEEE, 2023.</p>
<p>Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jeannette Bohg. Text2motion: From natural language instructions to feasible plans. arXiv preprint arXiv:2303.12153, 2023.</p>
<p>Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. Llm+ p: Empowering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477, 2023a.</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023b.</p>
<p>Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and Amy Zhang. Vip: Towards universal visual reward and representation via value-implicit pre-training. arXiv preprint arXiv:2210.00030, 2022.</p>
<p>Yecheng Jason Ma, William Liang, Vaidehi Som, Vikash Kumar, Amy Zhang, Osbert Bastani, and Dinesh Jayaraman. Liv: Language-image representations and rewards for robotic control. arXiv preprint arXiv:2306.00958, 2023.</p>
<p>Denys Makoviichuk and Viktor Makoviychuk. rl-games: A high-performance framework for reinforcement learning. https://github.com/Denys88/rl_games, May 2021.</p>
<p>Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, et al. Isaac gym: High performance gpu-based physics simulation for robot learning. arXiv preprint arXiv:2108.10470, 2021.</p>
<p>Gabriel B Margolis, Ge Yang, Kartik Paigwar, Tao Chen, and Pulkit Agrawal. Rapid locomotion via reinforcement learning. arXiv preprint arXiv:2205.02824, 2022.</p>
<p>Scott Niekum, Andrew G Barto, and Lee Spector. Genetic programming for reward function search. IEEE Transactions on Autonomous Mental Development, 2(2):83-90, 2010.</p>
<p>OpenAI. Gpt-4 technical report, 2023.
Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.</p>
<p>Stuart J Russell and Peter Norvig. Artificial Intelligence: A Modern Approach. Prentice Hall, Englewood Cliffs, NJ, USA, 1st edition, 1995.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.</p>
<p>Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.</p>
<p>Tom Silver, Soham Dan, Kavitha Srinivas, Joshua B Tenenbaum, Leslie Pack Kaelbling, and Michael Katz. Generalized planning in pddl domains with pretrained large language models. arXiv preprint arXiv:2305.11014, 2023.</p>
<p>Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task plans using large language models. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pp. 11523-11530. IEEE, 2023.</p>
<p>Satinder Singh, Richard L. Lewis, , and Andrew G. Barto. Where do rewards come from? In Proceedings of the International Symposium on AI Inspired Biology - A Symposium at the AISB 2010 Convention, pp. 111-116, 2010. ISBN 1902956923.</p>
<p>Laura Smith, Ilya Kostrikov, and Sergey Levine. A walk in the park: Learning to walk in 20 minutes with model-free reinforcement learning. arXiv preprint arXiv:2208.07860, 2022.</p>
<p>Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, USA, 2nd edition, 2018.</p>
<p>Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ international conference on intelligent robots and systems, pp. 5026-5033. IEEE, 2012.</p>
<p>Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023a.</p>
<p>Huaxiaoyue Wang, Gonzalo Gonzalez-Pumariega, Yash Sharma, and Sanjiban Choudhury. Demo2code: From summarizing demonstrations to synthesizing code via extended chain-ofthought. arXiv preprint arXiv:2305.16744, 2023b.</p>
<p>Grady Williams, Andrew Aldrich, and Evangelos Theodorou. Model predictive path integral control using covariance variable importance sampling. arXiv preprint arXiv:1509.01149, 2015.</p>
<p>Yaqi Xie, Chen Yu, Tongyao Zhu, Jinbin Bai, Ze Gong, and Harold Soh. Translating natural language to planning goals with large-language models. arXiv preprint arXiv:2302.05128, 2023.</p>
<p>Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 9, 2023.</p>
<p>Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonzalez Arenas, Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasenclever, Jan Humplik, et al. Language to rewards for robotic skill synthesis. arXiv preprint arXiv:2306.08647, 2023.</p>
<p>Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.</p>
<p>Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entropy inverse reinforcement learning. In Aaai, volume 8, pp. 1433-1438. Chicago, IL, USA, 2008.</p>
<h1>Appendix</h1>
<h2>Table of Contents</h2>
<p>A Full Prompts ..... 16
B Environment Details ..... 16
C Baseline Details ..... 20
C. 1 L2R Reward Examples ..... 21
D Eureka Details ..... 23
D. 1 Pen Spinning Tasks ..... 24
D. 2 EUREKA from Human Initialization ..... 24
D. 3 EUREKA from Human Feedback ..... 24
D. 4 Computation Resources ..... 25
E EUREKA on Mujoco Environments ..... 25
F Additional Results ..... 28
G Eureka Reward Examples ..... 31
G. 1 Reward Reflection Examples. ..... 31
G. 2 Negatively Correlated EUREKA Reward Examples ..... 34
G. 3 EUREKA from Human Initialization Examples ..... 35
G. 4 EUREKA from Human Reward Reflection ..... 39
G. 5 EUREKA and Human Reward Comparison ..... 43
H Limitations and Discussion ..... 45</p>
<h1>A Full Prompts</h1>
<p>In this section, we provide all EUREKA prompts.</p>
<h2>Prompt 1: Initial system prompt</h2>
<p>You are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible.
Your goal is to write a reward function for the environment that will help the agent learn the task described in text.
Your reward function should use useful variables from the environment as inputs. As an example
the reward function signature can be:
@torch.jit.script
def compute_reward(object_pos: torch.Tensor, goal_pos: torch.Tensor) -&gt; Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
...
return reward, ( )
Since the reward function will be decorated with @torch.jit.script,
please make sure that the code is compatible with TorchScript (e.g., use torch tensor instead of numpy array).
Make sure any new tensor or variable you introduce is on the same device as the input tensors.</p>
<h2>Prompt 2: Reward reflection and feedback</h2>
<div class="codehilite"><pre><span></span><code>We trained a RL policy using the provided reward function code and tracked the values of the
    individual components in the reward function as well as global policy metrics such as
    success rates and episode lengths after every (epoch_freq) epochs and the maximum, mean,
    minimum values encountered:
&lt;REWARD REFLECTION HERE&gt;
Please carefully analyze the policy feedback and provide a new, improved reward function that
    can better solve the task. Some helpful tips for analyzing the policy feedback:
    (1) If the success rates are always near zero, then you must rewrite the entire reward
        function
        (2) If the values for a certain reward component are near identical throughout, then this
        means RL is not able to optimize this component as it is written. You may consider
            (a) Changing its scale or the value of its temperature parameter
            (b) Re-writing the reward component
            (c) Discarding the reward component
    (3) If some reward components&#39; magnitude is significantly larger, then you must re-scale
    its value to a proper range
Please analyze each existing reward component in the suggested manner above first, and then
        write the reward function code.
</code></pre></div>

<p>Prompt 3: Code formatting tip
The output of the reward function should consist of two items:
(1) the total reward,
(2) a dictionary of each individual reward component.</p>
<p>The code output should be formatted as a python code string: " ' ' python ... ' ' ".
Some helpful tips for writing the reward function code:
(1) You may find it helpful to normalize the reward to a fixed range by applying transformations like torch.exp to the overall reward or its components
(2) If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable
(3) Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor
(4) Most importantly, the reward code's input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables.</p>
<h2>B ENVIRONMENT DETAILS</h2>
<p>In this section, we provide environment details. For each environment, we list its observation and action dimensions, the verbatim task description, and the task fitness function $F . F$ is evaluated per-environment, and our policy feedback uses the mean across environment instances.
For the functions below, || denotes the $L_{2}$ norm, and 1 [ ] denotes the indicator function.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">IsaacGym Environments</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Environment (obs dim, action dim)</td>
</tr>
<tr>
<td style="text-align: left;">Task description</td>
</tr>
<tr>
<td style="text-align: left;">Task fitness function $F$</td>
</tr>
<tr>
<td style="text-align: left;">Cartpole $(4,1)$</td>
</tr>
<tr>
<td style="text-align: left;">To balance a pole on a cart so that the pole stays upright</td>
</tr>
<tr>
<td style="text-align: left;">duration</td>
</tr>
<tr>
<td style="text-align: left;">Quadcopter $(21,12)$</td>
</tr>
<tr>
<td style="text-align: left;">To make the quadcopter reach and hover near a fixed position</td>
</tr>
<tr>
<td style="text-align: left;">-cur_dist</td>
</tr>
<tr>
<td style="text-align: left;">FrankaCabinet $(23,9)$</td>
</tr>
<tr>
<td style="text-align: left;">To open the cabinet door</td>
</tr>
<tr>
<td style="text-align: left;">1[cabinet_pos &gt; 0.39]</td>
</tr>
<tr>
<td style="text-align: left;">Anymal $(48,12)$</td>
</tr>
<tr>
<td style="text-align: left;">To make the quadruped follow randomly chosen $\mathrm{x}, \mathrm{y}$, and yaw target velocities</td>
</tr>
<tr>
<td style="text-align: left;">-(linvel_error + angvel_error)</td>
</tr>
<tr>
<td style="text-align: left;">BallBalance $(48,12)$</td>
</tr>
<tr>
<td style="text-align: left;">To keep the ball on the table top without falling</td>
</tr>
<tr>
<td style="text-align: left;">duration</td>
</tr>
<tr>
<td style="text-align: left;">Ant $(60,8)$</td>
</tr>
<tr>
<td style="text-align: left;">To make the ant run forward as fast as possible</td>
</tr>
<tr>
<td style="text-align: left;">cur_dist - prev_dist</td>
</tr>
<tr>
<td style="text-align: left;">AllegroHand $(88,16)$</td>
</tr>
<tr>
<td style="text-align: left;">To make the hand spin the object to a target orientation</td>
</tr>
<tr>
<td style="text-align: left;">number of consecutive successes where</td>
</tr>
<tr>
<td style="text-align: left;">current success is 1[rot_dist &lt; 0.1]</td>
</tr>
<tr>
<td style="text-align: left;">Humanoid $(108,21)$</td>
</tr>
<tr>
<td style="text-align: left;">To make the humanoid run as fast as possible</td>
</tr>
<tr>
<td style="text-align: left;">cur_dist - prev_dist</td>
</tr>
<tr>
<td style="text-align: left;">ShadowHand $(211,20)$</td>
</tr>
<tr>
<td style="text-align: left;">To make the shadow hand spin the object to a target orientation</td>
</tr>
<tr>
<td style="text-align: left;">number of consecutive successes where</td>
</tr>
<tr>
<td style="text-align: left;">current success is 1[rot_dist &lt; 0.1]</td>
</tr>
</tbody>
</table>
<h1>Dexterity Environments</h1>
<p>Environment (obs dim, action dim)
Task description
Task fitness function $F$
Over $(398,40)$
This class corresponds to the HandOver task. This environment consists of two shadow hands with palms facing up, opposite each other, and an object that needs to be passed. In the beginning, the object will fall randomly in the area of the shadow hand on the right side. Then the hand holds the object and passes the object to the other hand. Note that the base of the hand is fixed. More importantly, the hand which holds the object initially can not directly touch the target, nor can it directly roll the object to the other hand, so the object must be thrown up and stays in the air in the process
1 [dist &lt; 0.03]</p>
<h1>DoorCloseInward $(417,52)$</h1>
<p>This class corresponds to the DoorCloseInward task. This environment require a closed door to be opened and the door can only be pushed outward or initially open inward. Both these two environments only need to do the push behavior, so it is relatively simple
1 [door_handle_dist &lt; 0.5]
DoorCloseOutward $(417,52)$
This class corresponds to the DoorCloseOutward task. This environment also require a closed door to be opened and the door can only be pushed inward or initially open outward, but because they can't complete the task by simply pushing, which need to catch the handle by hand and then open or close it, so it is relatively difficult
1 [door_handle_dist &lt; 0.5]
DoorOpenInward $(417,52)$
This class corresponds to the DoorOpenInward task. This environment also require a opened door to be closed and the door can only be pushed inward or initially open outward, but because they can't complete the task by simply pushing, which need to catch the handle by hand and then open or close it, so it is relatively difficult
1 [door_handle_dist &gt; 0.5]
DoorOpenOutward $(417,52)$
This class corresponds to the DoorOpenOutward task. This environment require a opened door to be closed and the door can only be pushed outward or initially open inward. Both these two environments only need to do the push behavior, so it is relatively simple
1 [door_handle_dist &lt; 0.5]
Scissors $(417,52)$
This class corresponds to the Scissors task. This environment involves two hands and scissors, we need to use two hands to open the scissors
1 [dof_pos &gt; -0.3]
SwingCup $(417,52)$
This class corresponds to the SwingCup task. This environment involves two hands and a dual handle cup, we need to use two hands to hold and swing the cup together
1 [rot_dist &lt; 0.785]
Switch $(417,52)$
This class corresponds to the Switch task. This environment involves dual hands and a bottle, we need to use dual hand fingers to press the desired button
1 [1.4 - (left_switch_z + right_switch_z) &gt; 0.05]
Kettle $(417,52)$
This class corresponds to the PourWater task. This environment involves two hands, a kettle, and a bucket, we need to hold the kettle with one hand and the bucket with the other hand, and pour the water from the kettle into the bucket. In the practice task in Isaac Gym, we use many small balls to simulate the water
1 [|bucket - kettle_spout| &lt; 0.05]
LiftUnderarm $(417,52)$
This class corresponds to the LiftUnderarm task. This environment requires grasping the pot handle with two hands and lifting the pot to the designated position. This environment is designed to simulate the scene of lift in daily life and is a practical skill
1 [dist &lt; 0.05]
Pen $(417,52)$
This class corresponds to the Open Pen Cap task. This environment involves two hands and a pen, we need to use two hands to open the pen cap
1[5 * |pen_cap - pen_body| &gt; 1.5]
BottleCap $(420,52)$</p>
<p>This class corresponds to the Bottle Cap task. This environment involves two hands and a bottle, we need to hold the bottle with one hand and open the bottle cap with the other hand. This skill requires the cooperation of two hands to ensure that the cap does not fall
1 [dist &gt; 0.03]
CatchAbreast $(422,52)$
This class corresponds to the Catch Abreast task. This environment consists of two shadow hands placed side by side in the same direction and an object that needs to be passed. Compared with the previous environment which is more like passing objects between the hands of two people, this environment is designed to simulate the two hands of the same person passing objects, so different catch techniques are also required and require more hand translation and rotation techniques
1 [dist] &lt; 0.03
CatchOver2Underarm $(422,52)$
This class corresponds to the Over2Underarm task. This environment is similar to Catch Underarm, but with an object in each hand and the corresponding goal on the other hand. Therefore, the environment requires two objects to be thrown into the other hand at the same time, which requires a higher manipulation technique than the environment of a single object
1 [dist &lt; 0.03]
CatchUnderarm $(422,52)$
This class corresponds to the Catch Underarm task. In this task, two shadow hands with palms facing upwards are controlled to pass an object from one palm to the other. What makes it more difficult than the Hand over task is that the hands' translation and rotation degrees of freedom are no longer frozen but are added into the action space
1 [dist &lt; 0.03]
ReOrientation $(422,40)$
This class corresponds to the ReOrientation task. This environment involves two hands and two objects. Each hand holds an object and we need to reorient the object to the target orientation
1 [rot_dist &lt; 0.1]
GraspAndPlace $(425,52)$
This class corresponds to the GraspAndPlace task. This environment consists of dual-hands, an object and a bucket that requires us to pick up the object and put it into the bucket
1 [|block - bucket| &lt; 0.2]
BlockStack $(428,52)$
This class corresponds to the Block Stack task. This environment involves dual hands and two blocks, and we need to stack the block as a tower
1[goal_dist_1 &lt; 0.07 and goal_dist_2 &lt; 0.07 and
$50 *(0.05-$ z_dist_1) $&gt;1]$
PushBlock $(428,52)$
This class corresponds to the PushBlock task. This environment involves two hands and two blocks, we need to use both hands to reach and push the block to the desired goal separately. This is a relatively simple task
1 [left_dist &lt;= 0.1 and right_dist &lt;= 0.1] +
$0.5 * 1[$ left_dist $&lt;=0.1$ and right_dist $&gt;0.1]$
TwoCatchUnderarm $(446,52)$
This class corresponds to the TwoCatchUnderarm task. This environment is similar to Catch Underarm, but with an object in each hand and the corresponding goal on the other hand. Therefore, the environment requires two objects to be thrown into the other hand at the same time, which requires a higher manipulation technique than the environment of a single object
1 [goal_dist_1 + goal_dist_2 &lt; 0.06]</p>
<h1>C BASELINE DETAILS</h1>
<p>Language-to-Rewards (L2R) uses an LLM to generate a motion description from a natural language instruction and a set of reward API calls from the motion description. The reward is computed as the sum of outputs from the reward API calls. While the LLM automates the process of breaking down the task into basic low-level instructions, manual effort is still required to specify the motion description template, low-level reward API, and the API's function implementations.</p>
<p>All three parts require significant design considerations and can drastically affect L2R's performance and capabilities. Unfortunately, this makes comparison difficult since L2R requires manual engineering whereas Eureka is fully automatic-ambiguity thus arises from how much human-tuning should be done with L2R's components. Nonetheless, we seek to provide a fair comparison and base our implementation off two factors:</p>
<ul>
<li>To create our motion description template, we reference L2R's quadruped and dexterous manipulator templates. Specifically, our templates consist of statements that set parameters to quantitative values and statements that relate two parameters. We also aim to mimic the style of L2R's template statements in general.</li>
<li>The reward API is designed so that each template statement can be faithfully written in terms of an API function. The functions are implemented to resemble their respective human reward terms from their environment; thus, L2R is given an advantage in that its components resemble the manually-tuned human reward. In a few exceptions where the human reward differs significantly from the L2R template style, we base our API implementation on the formulas provided in the L2R appendix.</li>
</ul>
<p>L2R was designed to allow for an agent in a single environment to perform multiple tasks. Thus, each environment has its own motion description template and reward API. Since our experiments range over many agents and environments, we have one template and API for each Isaac task, and we generalize all Dexterity tasks into one environment with all necessary objects.</p>
<p>For illustration, our descriptor and coder prompts for the Dexterity experiments are below.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<div class="codehilite"><pre><span></span><code><span class="nx">We</span><span class="w"> </span><span class="nx">have</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">plan</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">robot</span><span class="w"> </span><span class="nx">arm</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">palm</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">manipulate</span><span class="w"> </span><span class="nx">objects</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">we</span><span class="w"> </span><span class="nx">want</span><span class="w"> </span><span class="nx">you</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">turn</span><span class="w"> </span><span class="nx">that</span>
<span class="w">    </span><span class="nx">into</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">corresponding</span><span class="w"> </span><span class="nx">program</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">following</span><span class="w"> </span><span class="nx">functions</span><span class="p">:</span>
<span class="o">...</span>
<span class="nx">def</span><span class="w"> </span><span class="nx">set_min_l2_distance_reward</span><span class="p">(</span><span class="nx">name_obj_A</span><span class="p">,</span><span class="w"> </span><span class="nx">name_obj_B</span><span class="p">)</span>
<span class="o">***</span>
<span class="nx">This</span><span class="w"> </span><span class="nx">term</span><span class="w"> </span><span class="nx">sets</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">reward</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">minimizing</span><span class="w"> </span><span class="mi">12</span><span class="w"> </span><span class="nx">distance</span><span class="w"> </span><span class="nx">between</span><span class="w"> </span><span class="nx">name_obj_A</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">name_obj_B</span><span class="w"> </span><span class="nx">so</span><span class="w"> </span><span class="nx">they</span>
<span class="w">    </span><span class="nx">get</span><span class="w"> </span><span class="nx">closer</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">each</span><span class="w"> </span><span class="nx">other</span><span class="p">.</span>
<span class="nx">name_obj_A</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">name_obj_B</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">selected</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="p">[&lt;</span><span class="nx">INSERT</span><span class="w"> </span><span class="nx">FIELDS</span><span class="w"> </span><span class="nx">HERE</span><span class="p">&gt;].</span>
<span class="o">...</span>
<span class="nx">def</span><span class="w"> </span><span class="nx">set_max_l2_distance_reward</span><span class="p">(</span><span class="nx">name_obj_A</span><span class="p">,</span><span class="w"> </span><span class="nx">name_obj_B</span><span class="p">)</span>
<span class="o">***</span>
<span class="nx">This</span><span class="w"> </span><span class="nx">term</span><span class="w"> </span><span class="nx">sets</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">reward</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">maximizing</span><span class="w"> </span><span class="mi">12</span><span class="w"> </span><span class="nx">distance</span><span class="w"> </span><span class="nx">between</span><span class="w"> </span><span class="nx">name_obj_A</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">name_obj_B</span><span class="w"> </span><span class="nx">so</span><span class="w"> </span><span class="nx">they</span>
<span class="w">    </span><span class="nx">get</span><span class="w"> </span><span class="nx">closer</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">each</span><span class="w"> </span><span class="nx">other</span><span class="p">.</span>
<span class="nx">name_obj_A</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">name_obj_B</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">selected</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="p">[&lt;</span><span class="nx">INSERT</span><span class="w"> </span><span class="nx">FIELDS</span><span class="w"> </span><span class="nx">HERE</span><span class="p">&gt;].</span>
<span class="o">...</span>
<span class="nx">def</span><span class="w"> </span><span class="nx">set_obj_orientation_reward</span><span class="p">(</span><span class="nx">name_obj_A</span><span class="p">,</span><span class="w"> </span><span class="nx">name_obj_B</span><span class="p">)</span>
<span class="o">***</span>
<span class="nx">This</span><span class="w"> </span><span class="nx">term</span><span class="w"> </span><span class="nx">encourages</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">orientation</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">name_obj_A</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">be</span><span class="w"> </span><span class="nx">close</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">orientation</span><span class="w"> </span><span class="nx">of</span>
<span class="w">    </span><span class="nx">name_obj_B</span><span class="p">.</span><span class="w"> </span><span class="nx">name_obj_A</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">name_obj_B</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">selected</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="p">[&lt;</span><span class="nx">INSERT</span><span class="w"> </span><span class="nx">ORIENTATION</span><span class="w"> </span><span class="nx">FIELDS</span><span class="w"> </span><span class="nx">HERE</span>
<span class="w">    </span><span class="p">&gt;].</span>
<span class="nx">Example</span><span class="w"> </span><span class="nx">plan</span><span class="p">:</span>
<span class="nx">object1</span><span class="p">=</span><span class="nx">object1</span><span class="w"> </span><span class="nx">should</span><span class="w"> </span><span class="nx">be</span><span class="w"> </span><span class="nx">close</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">object2</span><span class="p">=</span><span class="nx">object1_goal</span><span class="p">.</span>
<span class="nx">object1</span><span class="w"> </span><span class="nx">needs</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">have</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">rotation</span><span class="w"> </span><span class="nx">orientation</span><span class="w"> </span><span class="nx">similar</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">object2</span><span class="p">.</span>
<span class="nx">To</span><span class="w"> </span><span class="nx">perform</span><span class="w"> </span><span class="nx">this</span><span class="w"> </span><span class="nx">task</span><span class="p">,</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">left</span><span class="w"> </span><span class="nx">manipulator</span><span class="err">&#39;</span><span class="nx">s</span><span class="w"> </span><span class="nx">palm</span><span class="w"> </span><span class="nx">should</span><span class="w"> </span><span class="nx">move</span><span class="w"> </span><span class="nx">close</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">object1</span><span class="p">.</span>
<span class="nx">Example</span><span class="w"> </span><span class="nx">answer</span><span class="w"> </span><span class="nx">code</span><span class="p">:</span>
<span class="o">***</span>
<span class="nx">set_min_l2_distance_reward</span><span class="p">(</span><span class="s">&quot;object1&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;object1_goal&quot;</span><span class="p">)</span>
<span class="nx">set_min_l2_distance_reward</span><span class="p">(</span><span class="s">&quot;object1&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;left_palm&quot;</span><span class="p">)</span>
<span class="nx">set_obj_orientation_reward</span><span class="p">(</span><span class="s">&quot;object1&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;object1_goal&quot;</span><span class="p">)</span>
<span class="o">***</span>
<span class="nx">Remember</span><span class="p">:</span>
<span class="mi">1</span><span class="p">.</span><span class="w"> </span><span class="nx">Always</span><span class="w"> </span><span class="nx">format</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">code</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">code</span><span class="w"> </span><span class="nx">blocks</span><span class="p">.</span>
<span class="mi">2</span><span class="p">.</span><span class="w"> </span><span class="nx">Do</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="nx">wrap</span><span class="w"> </span><span class="nx">your</span><span class="w"> </span><span class="nx">code</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">function</span><span class="p">.</span><span class="w"> </span><span class="nx">Your</span><span class="w"> </span><span class="nx">output</span><span class="w"> </span><span class="nx">should</span><span class="w"> </span><span class="nx">only</span><span class="w"> </span><span class="nx">consist</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">function</span><span class="w"> </span><span class="nx">calls</span><span class="w"> </span><span class="k">like</span>
<span class="nx">the</span><span class="w"> </span><span class="nx">example</span><span class="w"> </span><span class="nx">above</span><span class="p">.</span>
<span class="mi">3</span><span class="p">.</span><span class="w"> </span><span class="nx">Do</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="nx">invent</span><span class="w"> </span><span class="nx">new</span><span class="w"> </span><span class="nx">functions</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">classes</span><span class="p">.</span><span class="w"> </span><span class="nx">The</span><span class="w"> </span><span class="nx">only</span><span class="w"> </span><span class="nx">allowed</span><span class="w"> </span><span class="nx">functions</span><span class="w"> </span><span class="nx">you</span><span class="w"> </span><span class="nx">can</span><span class="w"> </span><span class="nx">call</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">the</span>
<span class="nx">ones</span><span class="w"> </span><span class="nx">listed</span><span class="w"> </span><span class="nx">above</span><span class="p">,</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">do</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="nx">implement</span><span class="w"> </span><span class="nx">them</span><span class="p">.</span><span class="w"> </span><span class="nx">Do</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="nx">leave</span><span class="w"> </span><span class="nx">unimplemented</span><span class="w"> </span><span class="nx">code</span><span class="w"> </span><span class="nx">blocks</span><span class="w"> </span><span class="k">in</span>
<span class="nx">your</span><span class="w"> </span><span class="nx">response</span><span class="p">.</span>
<span class="mi">4</span><span class="p">.</span><span class="w"> </span><span class="nx">The</span><span class="w"> </span><span class="nx">only</span><span class="w"> </span><span class="nx">allowed</span><span class="w"> </span><span class="kn">library</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">numpy</span><span class="p">.</span><span class="w"> </span><span class="nx">Do</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="kn">import</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">use</span><span class="w"> </span><span class="nx">any</span><span class="w"> </span><span class="nx">other</span><span class="w"> </span><span class="kn">library</span><span class="p">.</span>
<span class="mi">5</span><span class="p">.</span><span class="w"> </span><span class="nx">If</span><span class="w"> </span><span class="nx">you</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="nx">sure</span><span class="w"> </span><span class="nx">what</span><span class="w"> </span><span class="nx">value</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">use</span><span class="p">,</span><span class="w"> </span><span class="nx">just</span><span class="w"> </span><span class="nx">use</span><span class="w"> </span><span class="nx">your</span><span class="w"> </span><span class="nx">best</span><span class="w"> </span><span class="nx">judge</span><span class="p">.</span><span class="w"> </span><span class="nx">Do</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="nx">use</span><span class="w"> </span><span class="nx">None</span><span class="w"> </span><span class="k">for</span>
<span class="nx">anything</span><span class="p">.</span>
<span class="mi">6</span><span class="p">.</span><span class="w"> </span><span class="nx">Do</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="nx">calculate</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">position</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">direction</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">any</span><span class="w"> </span><span class="nx">object</span><span class="w"> </span><span class="p">(</span><span class="nx">except</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">ones</span><span class="w"> </span><span class="nx">provided</span>
<span class="nx">above</span><span class="p">).</span><span class="w"> </span><span class="nx">Just</span><span class="w"> </span><span class="nx">use</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">number</span><span class="w"> </span><span class="nx">directly</span><span class="w"> </span><span class="nx">based</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="nx">your</span><span class="w"> </span><span class="nx">best</span><span class="w"> </span><span class="nx">guess</span><span class="p">.</span>
<span class="mi">7</span><span class="p">.</span><span class="w"> </span><span class="nx">You</span><span class="w"> </span><span class="nx">do</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="nx">need</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">make</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">robot</span><span class="w"> </span><span class="nx">do</span><span class="w"> </span><span class="nx">extra</span><span class="w"> </span><span class="nx">things</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="nx">mentioned</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">plan</span><span class="w"> </span><span class="nx">such</span><span class="w"> </span><span class="k">as</span>
<span class="nx">stopping</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">robot</span><span class="p">.</span>
</code></pre></div>

<p>For the sections surrounded by angle brackets $&lt;&gt;$, we specify a list of valid objects for each Dexterity task. For example, ShadowHandPen's list of objects is defined as follows:</p>
<div class="codehilite"><pre><span></span><code>&quot;shadow_hand_pen&quot;: [&quot;left_palm&quot;, &quot;right_palm&quot;, &quot;left_forefinger&quot;, &quot;left_middlefinger&quot;, &quot;
    left_ringfinger&quot;, &quot;left_littlefinger&quot;, &quot;left_thumb&quot;, &quot;right_forefinger&quot;, &quot;
    right_middlefinger&quot;, &quot;right_ringfinger&quot;, &quot;right_littlefinger&quot;, &quot;right_thumb&quot;, &quot;pen_cap&quot;,
    &quot;pen&quot;]
</code></pre></div>

<p>A summary of terms and their implementations for each experiment is in Table 3. Note that many environments automatically randomize their target parameters during training after a reset or success criteria is met, which L2R cannot account for during the reward generation stage. Thus, while L2R's experiments define targets in terms of quantitative values, it's incompatible with our environments, and we define targets instead as relations between two parameters (usually the object and the object's target).</p>
<h1>C. 1 L2R ReWard EXAMPLES</h1>
<p>Example 1: L2R reward function on Humanoid, Human Normalized Score: 0.0</p>
<div class="codehilite"><pre><span></span><code>set_torso_height_reward(1.1)
set_torso_velocity_reward(3.6)
set_angle_to_target_reward(0.0)
</code></pre></div>            </div>
        </div>

    </div>
</body>
</html>
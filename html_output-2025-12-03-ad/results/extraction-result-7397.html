<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7397 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7397</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7397</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-273993243</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.08561v5.pdf" target="_blank">LogLLM: Log-based Anomaly Detection Using Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Software systems often record important runtime information in logs to help with troubleshooting. Log-based anomaly detection has become a key research area that aims to identify system issues through log data, ultimately enhancing the reliability of software systems. Traditional deep learning methods often struggle to capture the semantic information embedded in log data, which is typically organized in natural language. In this paper, we propose LogLLM, a log-based anomaly detection framework that leverages large language models (LLMs). LogLLM employs BERT for extracting semantic vectors from log messages, while utilizing Llama, a transformer decoder-based model, for classifying log sequences. Additionally, we introduce a projector to align the vector representation spaces of BERT and Llama, ensuring a cohesive understanding of log semantics. Unlike conventional methods that require log parsers to extract templates, LogLLM preprocesses log messages with regular expressions, streamlining the entire process. Our framework is trained through a novel three-stage procedure designed to enhance performance and adaptability. Experimental results across four public datasets demonstrate that LogLLM outperforms state-of-the-art methods. Even when handling unstable logs, it effectively captures the semantic meaning of log messages and detects anomalies accurately.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7397.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7397.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogLLM: Log-based Anomaly Detection Using Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised log-sequence anomaly detection framework that extracts per-message semantic embeddings with BERT, projects them into Llama's token embedding space, and fine-tunes Llama (decoder) to classify sequences as normal or anomalous using a three-stage training procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT + Llama-3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Hybrid: BERT (transformer encoder) as message-level embedder; a linear projector to map BERT embeddings to Llama token-embedding space; Llama-3 (transformer decoder) for sequence classification via prompt tuning/fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>BERT-base (encoder), Llama-3-8B (decoder, 8B)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Supervised fine-tuning (classification) using per-message embedding + projector + Llama classifier; three-stage training (1: fine-tune Llama to answer template, 2: train BERT+projector, 3: fine-tune entire model).</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td>"Below is a sequence of system log messages:" + [projected token embeddings for each message] + ". Is this sequence normal or anomalous?" (expects answers: 'The sequence is normal.' / 'The sequence is anomalous.')</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Supervised labeled log sequences from four public datasets (HDFS, BGL, Liberty, Thunderbird); 8:2 train-test split (chronological split for BGL/Liberty/Thunderbird, session split for HDFS). Minority-class oversampling is applied (hyperparameter β, default 30%).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Log entries (ordered lists of textual log messages / sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>HDFS, BGL, Liberty, Thunderbird</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision, Recall, F1-score</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported F1-scores (Table II): HDFS F1=0.997, BGL F1=0.916, Liberty F1=0.958, Thunderbird F1=0.966; average F1 = 0.959.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms compared methods (selected F1s in same table): NeuralLog avg F1=0.893, RAPID avg F1=0.602, FastLogAD avg F1=0.341, LogBERT avg F1=0.456; LogLLM improves average F1 by ~6.6% over best existing (NeuralLog).</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Fully supervised (but stage-1 fine-tuning of Llama answer template uses a small number of samples).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High computational cost and memory demand (large Llama); sensitive to extreme class imbalance (very low anomaly rates can cause model to predict all-normal); directly concatenating raw messages into decoder can cause OOM and poor boundary clarity — mitigated by per-message BERT embedder and projector.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Training time (all datasets avg): 1,065.15 minutes; testing time avg: 64.48 minutes (Table III). GPU memory and training times vary by Llama size: with Llama-3-8B + embedder: GPU mem ~16.6–38.2 GB depending on dataset and config; training times reported per-dataset in Table V (e.g., Emb. & L.-8B training times: HDFS 2168.2 min, BGL 396.2 min, Liberty 412.1 min, Thunderbird 1284.2 min).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogLLM: Log-based Anomaly Detection Using Large Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7397.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7397.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NeuralLog</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NeuralLog (BERT+Transformer-based classifier for logs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A log-anomaly detection method that uses BERT to extract semantic vectors from raw log messages and applies a transformer-based classifier to detect anomalous sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Log-based anomaly detection without log parsing.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (encoder) + transformer classifier</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer encoder (BERT) used as embedder; smaller classification model (transformer-based) trained on embeddings to classify sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Supervised fine-tuning using semantic embeddings and a downstream transformer classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Same public log datasets used in comparisons (HDFS, BGL, Liberty, Thunderbird) as reported in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Log entries (lists of messages / sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>HDFS, BGL, Liberty, Thunderbird</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision, Recall, F1-score</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported in Table II: HDFS F1=0.979 (Prec0.971, Rec0.988); BGL F1=0.835 (0.792/0.884); Liberty F1=0.900 (0.875/0.926); Thunderbird F1=0.857 (0.794/0.931); avg F1=0.893.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared directly in Table II against methods like LogLLM (avg F1 0.959), RAPID, FastLogAD, LogBERT, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Fully supervised</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Uses smaller models than large decoder LLMs; paper notes its performance is lower than LogLLM which uses a larger decoder LLM (Llama).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Training time reported in Table III: 267.46 minutes; testing time 21.44 minutes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogLLM: Log-based Anomaly Detection Using Large Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7397.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7397.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAPID</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RAPID (Retrieval-based anomaly detection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-based method that uses transformer encoder-derived semantic vectors to retrieve the nearest normal document log sequence and classifies anomalies by distance/comparison rather than training a parametric classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Training-free retrieval-based log anomaly detection with pre-trained language model considering token-level information.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer encoder (pretrained LM used for embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Retrieval-based approach using pretrained transformer encoder embeddings and nearest-neighbor / distance-based comparison for anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Embedding similarity / retrieval (non-parametric) comparing query sequences to stored normal examples.</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Requires a store of normal log samples for retrieval; evaluated on the four public log datasets in paper comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Log entries (sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>HDFS, BGL, Liberty, Thunderbird</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision, Recall, F1-score</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported in Table II: HDFS F1=0.924 (Prec1.000, Rec0.859); BGL F1=0.548 (0.874/0.399); Liberty F1=0.732 (0.911/0.611); Thunderbird F1=0.203 (0.200/0.207); avg F1=0.602.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against supervised and semi-supervised methods in Table II; shows high precision but low recall on some datasets (e.g., BGL).</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Training-free retrieval (uses stored normal samples rather than supervised classifier training)</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not sensitive enough in some datasets (low recall); retrieval and vector extraction are time-consuming despite no training.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Reported in Table III: training time 63.98 minutes (indexing/embedding), testing time 38.43 minutes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogLLM: Log-based Anomaly Detection Using Large Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7397.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7397.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FastLogAD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FastLogAD: Mask-guided pseudo anomaly generation and discrimination</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that leverages BERT-style models for mask-guided pseudo-anomaly generation and trains a discriminator for log anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fastlogad: Log anomaly detection with mask-guided pseudo anomaly generation and discrimination.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (encoder-based LLM used)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses transformer-encoder LLM (BERT) for pseudo-anomaly generation and discrimination; details are cited from the referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Fine-tuning with pseudo-anomaly generation + discriminator (supervised / semi-supervised components).</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Evaluated on the same log datasets in this paper's comparison experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Log sequences</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>HDFS, BGL, Liberty, Thunderbird</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision, Recall, F1-score</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported in Table II: HDFS F1=0.798 (0.721/0.893); BGL F1=0.287 (0.167/1.000); Liberty F1=0.263 (0.151/0.999); Thunderbird F1=0.017 (0.008/0.931); avg F1=0.341.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Included in Table II comparisons; shows high recall but often very low precision on some datasets (many false positives).</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not zero-shot; involves training (semi-supervised / pseudo-label strategies).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High false positive rates on some datasets (e.g., BGL recall=1 but precision=0.167); inconsistent performance across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Reported in Table III: training time 254.17 minutes; testing time 0.29 minutes (fast inference using discriminator).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogLLM: Log-based Anomaly Detection Using Large Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7397.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7397.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogBERT: Log anomaly detection via BERT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method using BERT (transformer encoder) to reconstruct sequences of log template IDs and detect anomalies via reconstruction error, relying on parsed template IDs rather than raw semantic content.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Logbert: Log anomaly detection via bert.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only LLM (BERT) fine-tuned for sequence reconstruction of log template IDs (masked language / reconstruction objective).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>BERT-base (not explicitly sized in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Reconstruction-based anomaly detection on sequences of template IDs (semi-supervised).</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Trains on template-ID sequences obtained via log parsers (Drain) as in referenced work; evaluated on public log datasets in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Sequences of log template IDs (categorical list)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>HDFS, BGL, Liberty, Thunderbird</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision, Recall, F1-score</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported in Table II: HDFS F1=0.758 (0.989/0.614); BGL F1=0.283 (0.165/0.989); Liberty F1=0.744 (0.902/0.633); Thunderbird F1=0.039 (0.022/0.172); avg F1=0.456.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared in Table II; generally lower average performance than semantic-embedding methods like NeuralLog and LogLLM.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Semi-supervised reconstruction (trained on normal data)</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Ignores semantic information in raw messages by using template IDs; performance degrades when parser fails or templates change (unstable logs).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Reported in Table III: training time 429.04 minutes; testing time 443.77 minutes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogLLM: Log-based Anomaly Detection Using Large Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7397.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7397.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT / decoder LLM used with prompt engineering for log anomaly detection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompt-engineering based approach that leverages ChatGPT's zero-/few-shot capabilities by feeding log sequences via prompts and asking whether sequences are anomalous; cited work reports exploratory use of ChatGPT for log anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Loggpt: Exploring chatgpt for log-based anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (decoder-only instruction-tuned LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large decoder-only instruction-tuned LLM (ChatGPT family) used via prompts for zero/few-shot classification.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Prompt engineering (zero-shot / few-shot classification using direct prompts containing the log sequence).</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td>Prompt templates that integrate the log sequence directly (example: include the log messages and ask the model whether the sequence is normal or anomalous); specific templates cited in referenced works.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Log sequences (lists of messages)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Zero-shot / few-shot prompting (as reported in citation)</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Impractical for large window sizes due to token limits and cost; performance is dataset-agnostic and thus can be suboptimal for dataset-specific patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogLLM: Log-based Anomaly Detection Using Large Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7397.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7397.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Egersdoerfer et al.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Summary-based memory for ChatGPT log detection (Egersdoerfer et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that uses ChatGPT with a summary-based memory to avoid inputting full long log histories, summarizing previous messages to reduce prompt size for anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Early exploration of using chatgpt for log-based anomaly detection on parallel file systems logs.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (or similar decoder LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only LLM augmented by an external memory summarization technique to reduce token input size.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Prompt engineering with summary-augmented context (zero/few-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td>Summarize previous log messages into a memory, then include summary + current window in prompt asking if current sequence is anomalous.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Log sequences (time-ordered lists)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Zero-shot / few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reduces need to input entire sequence but still depends on LLM internal knowledge; no dataset-specific fine-tuning reported in the cited work (as summarized).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogLLM: Log-based Anomaly Detection Using Large Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7397.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7397.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAGLog (Raglog)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RAGLog: Log anomaly detection using Retrieval Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Uses a retrieval-augmented generative framework to analyze log entries by querying a store of normal log samples and designing prompts for an LLM to determine normal vs abnormal entries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Raglog: Log anomaly detection using retrieval augmented generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Retrieval-augmented LLM (RAG pipeline; decoder LLM + retriever)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RAG: retrieval module retrieves similar normal log samples, then a decoder LLM uses retrieved context + prompt to classify entries.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Retrieval-augmented prompting (uses retrieved normal examples and prompt templates to decide normality).</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td>Prompt templates designed to provide retrieved normal examples and ask the LLM whether the queried log entry is normal or abnormal.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Individual log entries / sequences</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Few-shot / retrieval-augmented (relies on retrieved exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Relies on quality and coverage of retrieved normal examples; no quantitative results reported in this paper (citation only).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogLLM: Log-based Anomaly Detection Using Large Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7397.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7397.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hadadi et al.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anomaly detection on unstable logs with GPT models (Hadadi et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A study that directly inputs parsed template sequences into GPT models and fine-tunes them, highlighting challenges such as unclear template boundaries and tokenization-induced OOM issues for long sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Anomaly detection on unstable logs with gpt models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-family models (decoder LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only GPT models fine-tuned on sequences of log templates (tokenized templates may span many tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>Fine-tuning on template sequences (supervised)</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Template sequences parsed from logs (as described in citation); details not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Sequences of log templates (categorical tokens / list of templates)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Fine-tuned (supervised)</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Tokenization and sequence-length issues: template boundaries unclear and multiple tokens per template cause long token sequences that can exceed LLM token/memory limits, causing OOM and limiting sequence length that can be processed.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Mentioned as problematic due to tokenization causing increased memory consumption and OOM; no exact costs provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogLLM: Log-based Anomaly Detection Using Large Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7397.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7397.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dabl (business processes)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dabl: Detecting semantic anomalies in business processes using large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced work that applies LLMs to detect semantic anomalies in business process data (sequence/list-like data) — cited as related work demonstrating LLMs applied to anomaly detection beyond system logs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Detecting semantic anomalies in business processes using large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large language models (unspecified in this paper's citation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM-based approach for semantic anomaly detection in business-process traces (sequence/list data).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_approach</strong></td>
                            <td>LLM-based semantic anomaly detection (likely prompt or fine-tune based; specifics in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>prompt_template</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Business process traces / sequences (list-like/tabular process logs)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogLLM: Log-based Anomaly Detection Using Large Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Loggpt: Exploring chatgpt for log-based anomaly detection. <em>(Rating: 2)</em></li>
                <li>Anomaly detection on unstable logs with gpt models. <em>(Rating: 2)</em></li>
                <li>Raglog: Log anomaly detection using retrieval augmented generation. <em>(Rating: 2)</em></li>
                <li>Training-free retrieval-based log anomaly detection with pre-trained language model considering token-level information. <em>(Rating: 2)</em></li>
                <li>Fastlogad: Log anomaly detection with mask-guided pseudo anomaly generation and discrimination. <em>(Rating: 2)</em></li>
                <li>Logbert: Log anomaly detection via bert. <em>(Rating: 2)</em></li>
                <li>Log-based anomaly detection without log parsing. <em>(Rating: 2)</em></li>
                <li>Detecting semantic anomalies in business processes using large language models. <em>(Rating: 1)</em></li>
                <li>Early exploration of using chatgpt for log-based anomaly detection on parallel file systems logs. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7397",
    "paper_id": "paper-273993243",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [
        {
            "name_short": "LogLLM",
            "name_full": "LogLLM: Log-based Anomaly Detection Using Large Language Models",
            "brief_description": "A supervised log-sequence anomaly detection framework that extracts per-message semantic embeddings with BERT, projects them into Llama's token embedding space, and fine-tunes Llama (decoder) to classify sequences as normal or anomalous using a three-stage training procedure.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT + Llama-3-8B",
            "model_description": "Hybrid: BERT (transformer encoder) as message-level embedder; a linear projector to map BERT embeddings to Llama token-embedding space; Llama-3 (transformer decoder) for sequence classification via prompt tuning/fine-tuning.",
            "model_size": "BERT-base (encoder), Llama-3-8B (decoder, 8B)",
            "anomaly_detection_approach": "Supervised fine-tuning (classification) using per-message embedding + projector + Llama classifier; three-stage training (1: fine-tune Llama to answer template, 2: train BERT+projector, 3: fine-tune entire model).",
            "prompt_template": "\"Below is a sequence of system log messages:\" + [projected token embeddings for each message] + \". Is this sequence normal or anomalous?\" (expects answers: 'The sequence is normal.' / 'The sequence is anomalous.')",
            "training_data": "Supervised labeled log sequences from four public datasets (HDFS, BGL, Liberty, Thunderbird); 8:2 train-test split (chronological split for BGL/Liberty/Thunderbird, session split for HDFS). Minority-class oversampling is applied (hyperparameter β, default 30%).",
            "data_type": "Log entries (ordered lists of textual log messages / sequences)",
            "dataset_name": "HDFS, BGL, Liberty, Thunderbird",
            "evaluation_metric": "Precision, Recall, F1-score",
            "performance": "Reported F1-scores (Table II): HDFS F1=0.997, BGL F1=0.916, Liberty F1=0.958, Thunderbird F1=0.966; average F1 = 0.959.",
            "baseline_comparison": "Outperforms compared methods (selected F1s in same table): NeuralLog avg F1=0.893, RAPID avg F1=0.602, FastLogAD avg F1=0.341, LogBERT avg F1=0.456; LogLLM improves average F1 by ~6.6% over best existing (NeuralLog).",
            "zero_shot_or_few_shot": "Fully supervised (but stage-1 fine-tuning of Llama answer template uses a small number of samples).",
            "limitations_or_failure_cases": "High computational cost and memory demand (large Llama); sensitive to extreme class imbalance (very low anomaly rates can cause model to predict all-normal); directly concatenating raw messages into decoder can cause OOM and poor boundary clarity — mitigated by per-message BERT embedder and projector.",
            "computational_cost": "Training time (all datasets avg): 1,065.15 minutes; testing time avg: 64.48 minutes (Table III). GPU memory and training times vary by Llama size: with Llama-3-8B + embedder: GPU mem ~16.6–38.2 GB depending on dataset and config; training times reported per-dataset in Table V (e.g., Emb. & L.-8B training times: HDFS 2168.2 min, BGL 396.2 min, Liberty 412.1 min, Thunderbird 1284.2 min).",
            "uuid": "e7397.0",
            "source_info": {
                "paper_title": "LogLLM: Log-based Anomaly Detection Using Large Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "NeuralLog",
            "name_full": "NeuralLog (BERT+Transformer-based classifier for logs)",
            "brief_description": "A log-anomaly detection method that uses BERT to extract semantic vectors from raw log messages and applies a transformer-based classifier to detect anomalous sequences.",
            "citation_title": "Log-based anomaly detection without log parsing.",
            "mention_or_use": "use",
            "model_name": "BERT (encoder) + transformer classifier",
            "model_description": "Transformer encoder (BERT) used as embedder; smaller classification model (transformer-based) trained on embeddings to classify sequences.",
            "model_size": null,
            "anomaly_detection_approach": "Supervised fine-tuning using semantic embeddings and a downstream transformer classifier.",
            "prompt_template": null,
            "training_data": "Same public log datasets used in comparisons (HDFS, BGL, Liberty, Thunderbird) as reported in this paper's experiments.",
            "data_type": "Log entries (lists of messages / sequences)",
            "dataset_name": "HDFS, BGL, Liberty, Thunderbird",
            "evaluation_metric": "Precision, Recall, F1-score",
            "performance": "Reported in Table II: HDFS F1=0.979 (Prec0.971, Rec0.988); BGL F1=0.835 (0.792/0.884); Liberty F1=0.900 (0.875/0.926); Thunderbird F1=0.857 (0.794/0.931); avg F1=0.893.",
            "baseline_comparison": "Compared directly in Table II against methods like LogLLM (avg F1 0.959), RAPID, FastLogAD, LogBERT, etc.",
            "zero_shot_or_few_shot": "Fully supervised",
            "limitations_or_failure_cases": "Uses smaller models than large decoder LLMs; paper notes its performance is lower than LogLLM which uses a larger decoder LLM (Llama).",
            "computational_cost": "Training time reported in Table III: 267.46 minutes; testing time 21.44 minutes.",
            "uuid": "e7397.1",
            "source_info": {
                "paper_title": "LogLLM: Log-based Anomaly Detection Using Large Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "RAPID",
            "name_full": "RAPID (Retrieval-based anomaly detection)",
            "brief_description": "A retrieval-based method that uses transformer encoder-derived semantic vectors to retrieve the nearest normal document log sequence and classifies anomalies by distance/comparison rather than training a parametric classifier.",
            "citation_title": "Training-free retrieval-based log anomaly detection with pre-trained language model considering token-level information.",
            "mention_or_use": "use",
            "model_name": "Transformer encoder (pretrained LM used for embeddings)",
            "model_description": "Retrieval-based approach using pretrained transformer encoder embeddings and nearest-neighbor / distance-based comparison for anomaly detection.",
            "model_size": null,
            "anomaly_detection_approach": "Embedding similarity / retrieval (non-parametric) comparing query sequences to stored normal examples.",
            "prompt_template": null,
            "training_data": "Requires a store of normal log samples for retrieval; evaluated on the four public log datasets in paper comparisons.",
            "data_type": "Log entries (sequences)",
            "dataset_name": "HDFS, BGL, Liberty, Thunderbird",
            "evaluation_metric": "Precision, Recall, F1-score",
            "performance": "Reported in Table II: HDFS F1=0.924 (Prec1.000, Rec0.859); BGL F1=0.548 (0.874/0.399); Liberty F1=0.732 (0.911/0.611); Thunderbird F1=0.203 (0.200/0.207); avg F1=0.602.",
            "baseline_comparison": "Compared against supervised and semi-supervised methods in Table II; shows high precision but low recall on some datasets (e.g., BGL).",
            "zero_shot_or_few_shot": "Training-free retrieval (uses stored normal samples rather than supervised classifier training)",
            "limitations_or_failure_cases": "Not sensitive enough in some datasets (low recall); retrieval and vector extraction are time-consuming despite no training.",
            "computational_cost": "Reported in Table III: training time 63.98 minutes (indexing/embedding), testing time 38.43 minutes.",
            "uuid": "e7397.2",
            "source_info": {
                "paper_title": "LogLLM: Log-based Anomaly Detection Using Large Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "FastLogAD",
            "name_full": "FastLogAD: Mask-guided pseudo anomaly generation and discrimination",
            "brief_description": "A method that leverages BERT-style models for mask-guided pseudo-anomaly generation and trains a discriminator for log anomaly detection.",
            "citation_title": "Fastlogad: Log anomaly detection with mask-guided pseudo anomaly generation and discrimination.",
            "mention_or_use": "use",
            "model_name": "BERT (encoder-based LLM used)",
            "model_description": "Uses transformer-encoder LLM (BERT) for pseudo-anomaly generation and discrimination; details are cited from the referenced work.",
            "model_size": null,
            "anomaly_detection_approach": "Fine-tuning with pseudo-anomaly generation + discriminator (supervised / semi-supervised components).",
            "prompt_template": null,
            "training_data": "Evaluated on the same log datasets in this paper's comparison experiments.",
            "data_type": "Log sequences",
            "dataset_name": "HDFS, BGL, Liberty, Thunderbird",
            "evaluation_metric": "Precision, Recall, F1-score",
            "performance": "Reported in Table II: HDFS F1=0.798 (0.721/0.893); BGL F1=0.287 (0.167/1.000); Liberty F1=0.263 (0.151/0.999); Thunderbird F1=0.017 (0.008/0.931); avg F1=0.341.",
            "baseline_comparison": "Included in Table II comparisons; shows high recall but often very low precision on some datasets (many false positives).",
            "zero_shot_or_few_shot": "Not zero-shot; involves training (semi-supervised / pseudo-label strategies).",
            "limitations_or_failure_cases": "High false positive rates on some datasets (e.g., BGL recall=1 but precision=0.167); inconsistent performance across datasets.",
            "computational_cost": "Reported in Table III: training time 254.17 minutes; testing time 0.29 minutes (fast inference using discriminator).",
            "uuid": "e7397.3",
            "source_info": {
                "paper_title": "LogLLM: Log-based Anomaly Detection Using Large Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "LogBERT",
            "name_full": "LogBERT: Log anomaly detection via BERT",
            "brief_description": "A method using BERT (transformer encoder) to reconstruct sequences of log template IDs and detect anomalies via reconstruction error, relying on parsed template IDs rather than raw semantic content.",
            "citation_title": "Logbert: Log anomaly detection via bert.",
            "mention_or_use": "use",
            "model_name": "BERT (encoder)",
            "model_description": "Encoder-only LLM (BERT) fine-tuned for sequence reconstruction of log template IDs (masked language / reconstruction objective).",
            "model_size": "BERT-base (not explicitly sized in this paper)",
            "anomaly_detection_approach": "Reconstruction-based anomaly detection on sequences of template IDs (semi-supervised).",
            "prompt_template": null,
            "training_data": "Trains on template-ID sequences obtained via log parsers (Drain) as in referenced work; evaluated on public log datasets in comparisons.",
            "data_type": "Sequences of log template IDs (categorical list)",
            "dataset_name": "HDFS, BGL, Liberty, Thunderbird",
            "evaluation_metric": "Precision, Recall, F1-score",
            "performance": "Reported in Table II: HDFS F1=0.758 (0.989/0.614); BGL F1=0.283 (0.165/0.989); Liberty F1=0.744 (0.902/0.633); Thunderbird F1=0.039 (0.022/0.172); avg F1=0.456.",
            "baseline_comparison": "Compared in Table II; generally lower average performance than semantic-embedding methods like NeuralLog and LogLLM.",
            "zero_shot_or_few_shot": "Semi-supervised reconstruction (trained on normal data)",
            "limitations_or_failure_cases": "Ignores semantic information in raw messages by using template IDs; performance degrades when parser fails or templates change (unstable logs).",
            "computational_cost": "Reported in Table III: training time 429.04 minutes; testing time 443.77 minutes.",
            "uuid": "e7397.4",
            "source_info": {
                "paper_title": "LogLLM: Log-based Anomaly Detection Using Large Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "ChatGPT (prompting)",
            "name_full": "ChatGPT / decoder LLM used with prompt engineering for log anomaly detection",
            "brief_description": "Prompt-engineering based approach that leverages ChatGPT's zero-/few-shot capabilities by feeding log sequences via prompts and asking whether sequences are anomalous; cited work reports exploratory use of ChatGPT for log anomaly detection.",
            "citation_title": "Loggpt: Exploring chatgpt for log-based anomaly detection.",
            "mention_or_use": "mention",
            "model_name": "ChatGPT (decoder-only instruction-tuned LLM)",
            "model_description": "Large decoder-only instruction-tuned LLM (ChatGPT family) used via prompts for zero/few-shot classification.",
            "model_size": null,
            "anomaly_detection_approach": "Prompt engineering (zero-shot / few-shot classification using direct prompts containing the log sequence).",
            "prompt_template": "Prompt templates that integrate the log sequence directly (example: include the log messages and ask the model whether the sequence is normal or anomalous); specific templates cited in referenced works.",
            "training_data": null,
            "data_type": "Log sequences (lists of messages)",
            "dataset_name": null,
            "evaluation_metric": null,
            "performance": null,
            "baseline_comparison": null,
            "zero_shot_or_few_shot": "Zero-shot / few-shot prompting (as reported in citation)",
            "limitations_or_failure_cases": "Impractical for large window sizes due to token limits and cost; performance is dataset-agnostic and thus can be suboptimal for dataset-specific patterns.",
            "computational_cost": null,
            "uuid": "e7397.5",
            "source_info": {
                "paper_title": "LogLLM: Log-based Anomaly Detection Using Large Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Egersdoerfer et al.",
            "name_full": "Summary-based memory for ChatGPT log detection (Egersdoerfer et al.)",
            "brief_description": "An approach that uses ChatGPT with a summary-based memory to avoid inputting full long log histories, summarizing previous messages to reduce prompt size for anomaly detection.",
            "citation_title": "Early exploration of using chatgpt for log-based anomaly detection on parallel file systems logs.",
            "mention_or_use": "mention",
            "model_name": "ChatGPT (or similar decoder LLM)",
            "model_description": "Decoder-only LLM augmented by an external memory summarization technique to reduce token input size.",
            "model_size": null,
            "anomaly_detection_approach": "Prompt engineering with summary-augmented context (zero/few-shot).",
            "prompt_template": "Summarize previous log messages into a memory, then include summary + current window in prompt asking if current sequence is anomalous.",
            "training_data": null,
            "data_type": "Log sequences (time-ordered lists)",
            "dataset_name": null,
            "evaluation_metric": null,
            "performance": null,
            "baseline_comparison": null,
            "zero_shot_or_few_shot": "Zero-shot / few-shot",
            "limitations_or_failure_cases": "Reduces need to input entire sequence but still depends on LLM internal knowledge; no dataset-specific fine-tuning reported in the cited work (as summarized).",
            "computational_cost": null,
            "uuid": "e7397.6",
            "source_info": {
                "paper_title": "LogLLM: Log-based Anomaly Detection Using Large Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "RAGLog (Raglog)",
            "name_full": "RAGLog: Log anomaly detection using Retrieval Augmented Generation",
            "brief_description": "Uses a retrieval-augmented generative framework to analyze log entries by querying a store of normal log samples and designing prompts for an LLM to determine normal vs abnormal entries.",
            "citation_title": "Raglog: Log anomaly detection using retrieval augmented generation.",
            "mention_or_use": "mention",
            "model_name": "Retrieval-augmented LLM (RAG pipeline; decoder LLM + retriever)",
            "model_description": "RAG: retrieval module retrieves similar normal log samples, then a decoder LLM uses retrieved context + prompt to classify entries.",
            "model_size": null,
            "anomaly_detection_approach": "Retrieval-augmented prompting (uses retrieved normal examples and prompt templates to decide normality).",
            "prompt_template": "Prompt templates designed to provide retrieved normal examples and ask the LLM whether the queried log entry is normal or abnormal.",
            "training_data": null,
            "data_type": "Individual log entries / sequences",
            "dataset_name": null,
            "evaluation_metric": null,
            "performance": null,
            "baseline_comparison": null,
            "zero_shot_or_few_shot": "Few-shot / retrieval-augmented (relies on retrieved exemplars)",
            "limitations_or_failure_cases": "Relies on quality and coverage of retrieved normal examples; no quantitative results reported in this paper (citation only).",
            "computational_cost": null,
            "uuid": "e7397.7",
            "source_info": {
                "paper_title": "LogLLM: Log-based Anomaly Detection Using Large Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Hadadi et al.",
            "name_full": "Anomaly detection on unstable logs with GPT models (Hadadi et al.)",
            "brief_description": "A study that directly inputs parsed template sequences into GPT models and fine-tunes them, highlighting challenges such as unclear template boundaries and tokenization-induced OOM issues for long sequences.",
            "citation_title": "Anomaly detection on unstable logs with gpt models.",
            "mention_or_use": "mention",
            "model_name": "GPT-family models (decoder LLMs)",
            "model_description": "Decoder-only GPT models fine-tuned on sequences of log templates (tokenized templates may span many tokens).",
            "model_size": null,
            "anomaly_detection_approach": "Fine-tuning on template sequences (supervised)",
            "prompt_template": null,
            "training_data": "Template sequences parsed from logs (as described in citation); details not provided in this paper.",
            "data_type": "Sequences of log templates (categorical tokens / list of templates)",
            "dataset_name": null,
            "evaluation_metric": null,
            "performance": null,
            "baseline_comparison": null,
            "zero_shot_or_few_shot": "Fine-tuned (supervised)",
            "limitations_or_failure_cases": "Tokenization and sequence-length issues: template boundaries unclear and multiple tokens per template cause long token sequences that can exceed LLM token/memory limits, causing OOM and limiting sequence length that can be processed.",
            "computational_cost": "Mentioned as problematic due to tokenization causing increased memory consumption and OOM; no exact costs provided in this paper.",
            "uuid": "e7397.8",
            "source_info": {
                "paper_title": "LogLLM: Log-based Anomaly Detection Using Large Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Dabl (business processes)",
            "name_full": "Dabl: Detecting semantic anomalies in business processes using large language models",
            "brief_description": "A referenced work that applies LLMs to detect semantic anomalies in business process data (sequence/list-like data) — cited as related work demonstrating LLMs applied to anomaly detection beyond system logs.",
            "citation_title": "Detecting semantic anomalies in business processes using large language models.",
            "mention_or_use": "mention",
            "model_name": "Large language models (unspecified in this paper's citation)",
            "model_description": "LLM-based approach for semantic anomaly detection in business-process traces (sequence/list data).",
            "model_size": null,
            "anomaly_detection_approach": "LLM-based semantic anomaly detection (likely prompt or fine-tune based; specifics in cited work).",
            "prompt_template": null,
            "training_data": null,
            "data_type": "Business process traces / sequences (list-like/tabular process logs)",
            "dataset_name": null,
            "evaluation_metric": null,
            "performance": null,
            "baseline_comparison": null,
            "zero_shot_or_few_shot": null,
            "limitations_or_failure_cases": null,
            "computational_cost": null,
            "uuid": "e7397.9",
            "source_info": {
                "paper_title": "LogLLM: Log-based Anomaly Detection Using Large Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Loggpt: Exploring chatgpt for log-based anomaly detection.",
            "rating": 2,
            "sanitized_title": "loggpt_exploring_chatgpt_for_logbased_anomaly_detection"
        },
        {
            "paper_title": "Anomaly detection on unstable logs with gpt models.",
            "rating": 2,
            "sanitized_title": "anomaly_detection_on_unstable_logs_with_gpt_models"
        },
        {
            "paper_title": "Raglog: Log anomaly detection using retrieval augmented generation.",
            "rating": 2,
            "sanitized_title": "raglog_log_anomaly_detection_using_retrieval_augmented_generation"
        },
        {
            "paper_title": "Training-free retrieval-based log anomaly detection with pre-trained language model considering token-level information.",
            "rating": 2,
            "sanitized_title": "trainingfree_retrievalbased_log_anomaly_detection_with_pretrained_language_model_considering_tokenlevel_information"
        },
        {
            "paper_title": "Fastlogad: Log anomaly detection with mask-guided pseudo anomaly generation and discrimination.",
            "rating": 2,
            "sanitized_title": "fastlogad_log_anomaly_detection_with_maskguided_pseudo_anomaly_generation_and_discrimination"
        },
        {
            "paper_title": "Logbert: Log anomaly detection via bert.",
            "rating": 2,
            "sanitized_title": "logbert_log_anomaly_detection_via_bert"
        },
        {
            "paper_title": "Log-based anomaly detection without log parsing.",
            "rating": 2,
            "sanitized_title": "logbased_anomaly_detection_without_log_parsing"
        },
        {
            "paper_title": "Detecting semantic anomalies in business processes using large language models.",
            "rating": 1,
            "sanitized_title": "detecting_semantic_anomalies_in_business_processes_using_large_language_models"
        },
        {
            "paper_title": "Early exploration of using chatgpt for log-based anomaly detection on parallel file systems logs.",
            "rating": 1,
            "sanitized_title": "early_exploration_of_using_chatgpt_for_logbased_anomaly_detection_on_parallel_file_systems_logs"
        }
    ],
    "cost": 0.01846125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LogLLM: Log-based Anomaly Detection Using Large Language Models
14 Apr 2025</p>
<p>Wei Guan guan-wei@sjtu.edu.cn 
Department of Computer Science and Engineering
SJTU
ShanghaiChina</p>
<p>Jian Cao cao-jian@sjtu.edu.cn 
Department of Computer Science and Engineering
SJTU
ShanghaiChina</p>
<p>Shiyou Qian 
Department of Computer Science and Engineering
SJTU
ShanghaiChina</p>
<p>Jianqi Gao 
Department of Computer Science and Engineering
SJTU
ShanghaiChina</p>
<p>Chun Ouyang c.ouyang@qut.edu.au 
The School of Information Systems
QUT
BrisbaneAustralia</p>
<p>LogLLM: Log-based Anomaly Detection Using Large Language Models
14 Apr 2025FB83E6DB44533E5CD1A46BC66200A957arXiv:2411.08561v5[cs.SE]System loganomaly detectionlarge language modeldeep learninglog analysis
Software systems often record important runtime information in logs to help with troubleshooting.Log-based anomaly detection has become a key research area that aims to identify system issues through log data, ultimately enhancing the reliability of software systems.Traditional deep learning methods often struggle to capture the semantic information embedded in log data, which is typically organized in natural language.In this paper, we propose LogLLM, a log-based anomaly detection framework that leverages large language models (LLMs).LogLLM employs BERT for extracting semantic vectors from log messages, while utilizing Llama, a transformer decoder-based model, for classifying log sequences.Additionally, we introduce a projector to align the vector representation spaces of BERT and Llama, ensuring a cohesive understanding of log semantics.Unlike conventional methods that require log parsers to extract templates, LogLLM preprocesses log messages with regular expressions, streamlining the entire process.Our framework is trained through a novel three-stage procedure designed to enhance performance and adaptability.Experimental results across four public datasets demonstrate that LogLLM outperforms state-of-the-art methods.Even when handling unstable logs, it effectively captures the semantic meaning of log messages and detects anomalies accurately.</p>
<p>I. INTRODUCTION</p>
<p>Ensuring high availability and reliability is crucial for largescale software-intensive systems [1], [2].As these systems become more complex and expansive, the occurrence of anomalies becomes unavoidable [3], [4].Even a minor issue can lead to performance degradation, data integrity problems, and substantial losses in both customers and revenue.Therefore, anomaly detection is vital for maintaining the health and stability of complex software-intensive systems [5].</p>
<p>Software-intensive systems typically produce console logs that record system states and critical runtime events [6].Engineers can utilize this log data to evaluate system health, identify anomalies, and trace the root causes of issues.However, due to the potentially vast volume of logs, manually analyzing them for anomalies can be both labor-intensive and prone to mistakes [7].Consequently, log-based anomaly detection has emerged as a key area in automated log analysis, focusing on the automatic identification of system anomalies through log data.</p>
<p>Numerous deep learning-based methods [8]- [22] for logbased anomaly detection have been proposed.These methods typically employ sequential deep learning models such as LSTM [23] and transformers [24].These methods can be further divided into reconstruction-based methods [8]- [15] and binary classification-based methods [16]- [22].Reconstructionbased methods involve designing and training a deep neural network to reconstruct input log sequences, with anomalies detected based on reconstruction errors.The underlying principle is that anomalous samples cannot be accurately reconstructed.Binary classification-based methods, on the other hand, involve designing a binary classifier to classify samples as either normal or anomalous.These methods often require labeled anomalies for training purposes.It is recognized that system logs are documented in natural language and contain a significant amount of semantic information.Nevertheless, traditional deep learning-based methods struggle to effectively capture this information.</p>
<p>In recent years, significant advancements have been achieved in LLMs, such as GPT-4 [25], Llama 3 [26], and ChatGLM [27].These models are characterized by their vast parameter sizes and are pretrained on substantially larger datasets, ranging from several gigabytes to terabytes in size.This extensive pretraining equips them with remarkable language comprehension abilities, enabling superior performance in tasks such as summarization, paraphrasing, and instruction following even in zero-shot scenarios [28].Existing methods that utilize LLMs for log-based anomaly detection can be categorized into prompt engineering-based [7], [29]- [31] and fine-tuning-based [3], [32]- [40] approaches.Prompt engineering-based methods leverage the zero/few-shot capabilities of LLMs to detect anomalies based solely on the models' internal knowledge.However, these methods often struggle to customize solutions for specific datasets, leading to suboptimal detection performance.Fine-tuning-based methods integrate LLMs into deep neural networks and tailor them to user-specific datasets.Nevertheless, these methods encounter challenges such as limited semantic understanding, suboptimal LLM utilization (relying solely on LLMs for semantic information extraction), and insufficient consideration of input data format, which can lead to memory overflow.</p>
<p>To tackle the aforementioned challenges, we propose LogLLM, a novel log-based anomaly detection framework that harnesses LLMs.Unlike traditional methods that rely on log parsers for template extraction, LogLLM preprocesses log messages using regular expressions, thereby streamlining the entire process.LogLLM, a fine-tuning-based method, utilizes BERT, a transformer encoder-based model, to extract semantic vectors from log messages.Additionally, it employs Llama, a transformer decoder-based model, to classify log sequences.To ensure coherence in log semantics, we introduce a projector that aligns the vector representation spaces of BERT and Llama.Our framework is trained using a novel threestage procedure designed to enhance both performance and adaptability.</p>
<p>As illustrated in Section V-G, LLMs frequently face out-ofmemory challenges due to their extensive parameter sizes [41].Directly inputting the entire log sequence (by concatenating log messages into a long string) into Llama can lead to outof-memory issues and potentially confuse the LLM, making it difficult to focus on key points for distinguishing anomalies.By adopting BERT to summarize each log message, LogLLM effectively mitigates these problems.We conduct experiments across four public datasets, and the results demonstrate that LogLLM outperforms state-of-the-art methods.Even when handling unstable logs, where new log templates frequently emerge due to software evolution, it effectively captures the semantic meaning of log messages and detects anomalies accurately.The ablation study confirms the effectiveness of the three-stage training procedure.</p>
<p>The main contributions of our work are as follows:</p>
<p>II. RELATED WORK</p>
<p>In this section, we explore related work in the field of log-based anomaly detection, with a particular focus on deep learning-based methods.We give special attention to approaches that utilize pretrained LLMs.</p>
<p>A. Traditional Deep Learning for Log-based Anomaly Detection</p>
<p>Many traditional deep learning-based methods for log-based anomaly detection have been proposed.These works can be grouped into two types based on the training paradigm: reconstruction-based methods and binary classification-based methods.</p>
<p>Reconstruction-based methods [8]- [15] involve designing and training a deep neural network to reconstruct input log sequences.Anomalies are detected based on reconstruction errors.Normal log sequences can be reconstructed with minimal errors, while anomalous log sequences cannot be effectively reconstructed, resulting in significantly higher reconstruction errors.These methods consistently train the deep model on normal data that is free of anomalies, which means they are semi-supervised.</p>
<p>DeepLog [8] adopts LSTM to predict the next log template ID based on past log sequences.Similarly, LogAnomaly [9] predicts the next log template ID based on both sequential and quantitative patterns.Autoencoders (AEs) [10]- [13] and generative adversarial networks (GANs) [14], [15] are widely used in reconstruction-based methods.For example, LogAttn [10] adopts an AE that incorporates a temporal convolutional network (TCN) to capture temporal semantic correlations and a deep neural network (DNN) to capture statistical correlations.Duan et al. [14] use a GAN, where an encoder-decoder framework based on LSTM serves as the generator.Convolutional neural networks (CNNs) are used as the discriminator.The reconstruction error is calculated based on the difference between the input and the output from the generator.</p>
<p>Binary classification-based methods [16]- [22] often employ deep neural networks that output either one or two values.Typically, a single value represents the probability that a sample belongs to the anomalous class, and anomalies are detected by applying a threshold to convert this probability into a binary classification.When two values are output, they represent the probabilities of the sample belonging to the normal and anomalous classes, respectively.</p>
<p>Most methods [16]- [20] typically train deep models in a supervised manner.For example, Zhang et al. [16] propose LayerLog, which integrates word, log, and logseq layers to extract semantic features from log sequences.CNNs are utilized in [17], [18] to develop a binary classifier.LogRobust [19] integrates a pre-trained Word2Vec model, specifically FastText [42], and combines it with TF-IDF weights to learn representation vectors of log templates.These vectors are then fed into an attention-based Bi-LSTM model for anomaly detection.LogGD [20] transforms log sequences into graphs and utilizes a graph transformer neural network that combines graph structure and node semantics for log-based anomaly detection.</p>
<p>Some work [21], [22] involves training binary classifiers in a semi-supervised manner.For example, Trine [21] uses a transformer encoder [24] to encode normal log sequences into vector representations and a generator to produce random fake vector representations.The discriminator, which is composed of a transformer and a multi-layer perceptron (MLP), is trained to distinguish whether the given vector representations are normal log sequences and it is subsequently used to detect anomalies.PLELog [22] tackles the challenge of insufficient labeling by employing probabilistic label estimation and develops an attention-based GRU neural network for anomaly detection.</p>
<p>It is acknowledged that system logs are recorded in natural Fig. 1: An example of a system log.</p>
<p>language and contain a substantial amount of semantic information.However, traditional deep learning-based methods face challenges in capturing this information.</p>
<p>B. LLMs for Log-based Anomaly Detection</p>
<p>Existing LLMs can be categorized into transformer encoderbased models, such as BERT [43], RoBERTa [44], and Span-BERT [45], and transformer decoder-based models, including GPT-4 [25], Llama 3 [26], and ChatGLM [27].Two prevalent strategies for utilizing LLMs are prompt engineering and finetuning.</p>
<p>Prompt engineering-based methods [7], [29]- [31] detect anomalies solely by relying on the internal knowledge of LLMs.These methods typically employ transformer decoderbased models.For instance, Qi et al. [7] employ ChatGPT for zero-shot and few-shot log-based anomaly detection, utilizing prompt templates that integrate the log sequence directly.However, this approach becomes impractical when using a large window size for grouping log messages.Egersdoerfer et al. [30] address this issue by maintaining a summary-based memory, which summarizes the previous log messages, eliminating the need to input the entire log sequence for anomaly detection.RAGLog [31] uses a retrieval augmented generative (RAG) framework [46] to analyze log entries by querying its store of samples of normal log entries.They design prompt templates for LLMs to determine whether a queried log entry is normal or abnormal.Prompt engineering-based methods often struggle to customize solutions for specific datasets, which can lead to suboptimal detection performance in particular datasets.</p>
<p>Fine-tuning-based methods [3], [32]- [40] incorporate LLMs into deep neural networks and customize them to the user's own dataset.Some methods [32]- [35], although adopting transformer encoder-based LLMs for anomaly detection, do not capture the semantic information within log sequences.For example, LogBERT [32] and LAnoBERT [33] utilize BERT to reconstruct the input sequence of log template IDs (IDs of log string templates) and detect anomalies based on reconstruction errors, disregarding the semantic information.Other methods [3], [36]- [39] use transformer encoder-based LLMs solely for extracting semantic information from log messages and then employ either smaller models [3], [36]- [38] or distance-based comparison [39] for classification.For instance, NeuralLog [3] leverages BERT to extract semantic vectors from raw log messages, which are subsequently used to detect anomalies via a transformer-based classification model.Similarly, RAPID [39] utilizes transformer encoder-based models to extract semantic vectors and performs anomaly detection by comparing each query log sequence with its nearest document log sequence.Hadadi et al. [40] directly input template sequences parsed from log sequences, into GPT models and fine-tune it to accurately predict sequence labels.However, this approach faces two key challenges.First, the boundaries between templates within the sequences are unclear, making it difficult for the model to learn the sequential dependencies.Second, each template may be tokenized into multiple tokens by the LLM's tokenizer, and a single sequence can contain numerous log templates.As a result, an excessive number of tokens may be generated, often exceeding the token (memory) limits of LLMs [41], thereby restricting the length of sequences that can be processed.These two challenges are further demonstrated in Section V-G.</p>
<p>LogLLM is a fine-tuning-based method that utilizes BERT for extracting semantic vectors from log messages and Llama, a transformer decoder-based model, for log sequence classification.This method aligns the vector representation spaces of BERT and Llama using a projector.The use of BERT ensures clear boundaries between log messages, as each message is represented by a distinct embedding vector, thereby enhancing classification performance.Moreover, when memory and parameter size of Llama are held constant, this approach can handle longer sequences compared to directly tokenizing the entire log sequence using Llama's tokenizer.</p>
<p>III. PRELIMINARIES</p>
<p>To establish the groundwork for subsequent sections, we introduce the system log, which records the system's events and internal states during runtime.A system log contains a list of log messages in chronological order.</p>
<p>Fig. 1 presents a snippet of a raw system log generated by the BGL (the BlueGene/L supercomputer system), with each log message ordered according to the recorded time.These raw log messages are semi-structured texts consisting of a header and content.The header, determined by the logging framework, includes information such as timestamp, verbosity level (e.g., WARN/INFO), and component [47].The log content comprises a constant part (keywords that reveal the log template) and a variable part (parameters that carry dynamic runtime information).In this paper, we focus solely on the content of each log message.</p>
<p>The log messages can be grouped into log sequences (i.e., series of log messages that record specific execution flows) based on session or fixed/sliding windows [48].Session window partitioning groups log messages according to their session IDs, thereby generating sequences that include the log  messages within each session.For example, Fig. 2a illustrates the HDFS [49] logs undergoing the session window grouping process, where the block_id serves as the session ID.In contrast, fixed/sliding window partitioning groups log messages based on a fixed size (window size), which can be defined by either the time span or the number of log messages.This method creates sequences that capture snapshots of system log messages over time.For example, Fig. 2b illustrates the BGL [50] logs undergoing the sliding window grouping process, with a window size of 2 messages and a step size of 2 messages.</p>
<p>The objective of log-based anomaly detection is to identify anomalous log sequences, facilitating the recognition of potential issues within the system's operational behavior.</p>
<p>IV. METHODOLOGY</p>
<p>In this section, we present our innovative anomaly detection framework, LogLLM.As illustrated in Fig. 3, the log sequence undergoes preprocessing using regular expressions before being fed into a deep neural network that integrates BERT [43], a projector, and Llama [26] for log sequence classification.In the following sections, we will provide detailed insights into log sequence preprocessing, the architecture of the deep model, and the model training procedure.</p>
<p>A. Preprocessing</p>
<p>Considering that the log message content includes variable parameters carrying dynamic runtime information, which is always irrelevant to the anomalies and complicates deep model training, as demonstrated in Section V-F, a technique is needed to identify these parameters and replace them with a constant token.Log parsers, such as Drain [51] and Spell [52], are widely adopted in log-based anomaly detection methods and appear to be a useful technique.However, as noted by Le et al. [3], existing log parsers do not always perform correctly on all log datasets and struggle to handle out-of-vocabulary (OOV) words in new log messages, resulting in a loss of semantic information.When logs are unstable, these parsers become increasingly ineffective over time, making it difficult to support subsequent anomaly detection.</p>
<p>Thanks to the structured log generation process, the textual format of parameters representing specific objects can be easily identified using regular expressions [53].Consequently, we replace each variable parameter, such as account, directory path, and IP address, with '&lt;*&gt;'.Despite its simplicity, this technique offers significant performance advantages.Compared with log parsers, this preprocessing technique is more effective and does not require training.</p>
<p>B. Model Architecture</p>
<p>As shown in Fig. 3, our deep model consists of three main components: BERT, a projector, and Llama.Both BERT and Llama are pretrained LLMs.BERT is utilized to extract vector representations of log messages, while Llama is employed to classify the log sequences.The projector serves as a bridge, aligning the vector representation spaces of BERT and Llama.It is important to note that our model incorporates only one instance of BERT and one projector.</p>
<p>1) BERT: BERT generates a semantic vector by processing the semantic vector of the classification token ([CLS]) through a linear layer followed by a tanh activation function.Each log message, once preprocessed, is encoded into a semantic vector using the BERT tokenizer and BERT model.For a preprocessed log sequence, the output of BERT is a sequence of semantic vectors C = (c 1 , c 2 , . . ., c N ) ∈ R N ×d BERT , where N represents the length of the log sequence (i.e., the number of log messages) and d BERT is the dimension of each semantic vector (i.e., hidden size).</p>
<p>2) Projector: The projector is a linear layer that maps the semantic vectors C ∈ R N ×d BERT to the token embedding vectors accepted by Llama, represented as E = (e 1 , e 2 , . . ., e N ) ∈ R N ×d Llama , where d Llama is the hidden size of Llama.The projector is designed to align the vector representation spaces of BERT and Llama.</p>
<p>3) Llama: To conduct prompt tuning on Llama, the transformer decoder-based LLM, we generate corresponding textual queries based on embedded log sequences.Specifically, each query consists of three components.</p>
<p>The first component introduces the log sequence, such as "Below is a sequence of system log messages:".The second component comprises the token embeddings E output by the projector.The third component queries whether the sequence is anomalous, asking, for instance, ".Is this sequence normal or anomalous?".The first and third components are fed into the Llama tokenizer and Llama embedding layer sequentially, producing E 1 ∈ R A×d Llama and E 3 ∈ R Q×d Llama , where A and Q are the number of tokens produced by tokenizing the first and third components, respectively.Then, the token embeddings of the three components are concatenated, rep-</p>
<p>C. Training</p>
<p>1) Minority Class Oversampling: LogLLM is a supervised anomaly detection method, which means it needs labeled normal and anomalous samples for training.However, supervised anomaly detection methods often face the challenge of data imbalance, which can lead to biased model training.In an anomaly detection task, there are only two classes: normal and anomalous, and the number of instances in each class is uncertain.To cope with data imbalance, we oversample the class with fewer samples, ensuring that the proportion of the minority class is no less than β.Formally, let the proportion of the minority class be α and α &lt; β, and the total number of samples be Sample_num.To achieve a proportion of β for the minority class, it will be oversampled to the following quantity:
β(1 − α) 1 − β × Sample_num(1)
This adjustment will make the proportion of the minority class equal to β.</p>
<p>2) Training Objective: Our objective is to train the deep model to predict whether a given log sequence is normal or anomalous.We fine-tune the model to respond appropriately: if the sequence is anomalous, it outputs 'The sequence is anomalous.';if normal, it outputs 'The sequence is normal.'.We utilize cross-entropy loss [54] as our loss function.</p>
<p>3) Training Procedure: To train our deep model, we follow three main stages.</p>
<p>Stage 1. Fine-tuning Llama to capture the answer template: The first stage involves fine-tuning Llama to capture the answer template.Specifically, we train Llama to respond to the prompt 'Is this sequence normal or anomalous?' with 'The sequence is anomalous/normal.'.This stage requires only a few data samples.</p>
<p>Stage 2. Training the embedder of log messages:</p>
<p>The second stage involves training the embedder of log messages, specifically BERT and the projector.This stage aims to project each log message to the embedding of the most suitable token in Llama, enabling Llama to discern whether the given log sequence is normal or anomalous.</p>
<p>Stage 3. Fine-tuning the entire model: Finally, we fine-tune the entire model to ensure cohesive and accurate performance across all components.</p>
<p>4) Efficient Fine-Tuning on LLMs: To reduce the costs involved in fine-tuning LLMs (BERT and Llama) with a substantial number of parameters, we utilize QLoRA [55] to minimize memory usage.QLoRA accomplishes this by backpropagating gradients into a frozen 4-bit quantized model, while maintaining the performance levels achieved during the full 16-bit fine-tuning process.</p>
<p>V. EXPERIMENTS</p>
<p>In this section, we conduct extensive experiments on four real-life logs to investigate the following research questions (RQs): LogLLM is coded in Python, and the source code is available at https://github.com/guanwei49/LogLLM.</p>
<p>A. Benchmark Methods</p>
<p>To verify the superiority of the proposed method, we compare LogLLM with five state-of-the-art semi-supervised methods: DeepLog [8], LogAnomaly [9], PLELog [22], Fast-LogAD [34], and LogBERT [32].We also compare it with three supervised methods: LogRobust [19], CNN [18] and NeuralLog [3], and one method that does not require training a deep model but needs some normal samples for retrieval: RAPID [39].</p>
<p>Notably, FastLogAD, LogBERT, NeuralLog, and RAPID adopt LLMs for anomaly detection.</p>
<p>B. Experimental Settings</p>
<p>We conduct all experiments on a server equipped with an Intel Xeon Gold 6330 CPU (38 cores), 256GB of memory, and an NVIDIA A40 GPU with 48 GB of memory.</p>
<p>In our experiment, we utilize the BERT-base model1 and Llama-3-8B model2 as backbones.The hyperparameter β, which is described in Section IV-C1, is set to 30%.We use the AdamW optimizer [56] to train the model with a mini-batch size of 16.Unless otherwise specified, the training procedure is configured as follows: In the first stage, only 1,000 samples are involved with a learning rate of 5e-4.The second and third stages each consist of two epochs with a learning rate of 5e-5.</p>
<p>For a fair comparison, we configure the hyperparameters for all compared methods according to the values provided in their original articles.</p>
<p>C. Metrics</p>
<p>We evaluate the performance of these methods using the widely adopted P recision, Recall and F 1 − score.These metrics are calculated as follows:</p>
<p>P recision = T P T P + F P</p>
<p>(2)
Recall = T P T P + F N(3)F 1 −score = 2 * P recision * Recall P recision + Recall(4)
, where T P , F N , F P represent true positives, false negatives and false positives respectively.Precision refers to the percentage of correctly detected anomalies among all anomalies identified by the model, while recall represents the percentage of anomalies that are correctly identified from all real anomalies.The F 1 -score combines these two metrics into a single measure, providing a balanced assessment of the model's performance in detecting anomalies.</p>
<p>D. Dataset</p>
<p>To evaluate our method for log-based anomaly detection, we selected four public datasets [57]: HDFS, BGL, Liberty, and Thunderbird.The details for each dataset are provided below:</p>
<p>HDFS (Hadoop Distributed File System) dataset [49] is generated by running Hadoop-based mapreduce jobs on over 200 Amazon EC2 nodes and contains a total of 11,175,629 log messages.These log messages are grouped into different log windows based on their block_id, which reflect program executions in the HDFS, resulting in 575,061 blocks.Among these, 16,838 blocks (2.93%) indicate system anomalies.</p>
<p>BGL (Blue Gene/L) dataset [50] is a supercomputing system log dataset collected from a BlueGene/L supercomputer system at lawrence livermore national labs (LLNL).The dataset contains 4,747,963 log messages, each of which has been manually labeled as either normal or anomalous.There are 348,460 log messages (7.34%) that are labeled as anomalous.</p>
<p>Thunderbird dataset [50] is a publicly accessible collection of log data sourced from the Thunderbird supercomputer at sandia national laboratories (SNL).This dataset consists of both normal and anomalous messages, each of which has been manually categorized.Although the dataset contains over 200 million log messages, we focus on a subset of 10 million continuous log messages for computational efficiency.This subset includes 4,937 anomalous log messages, representing approximately 0.049% of the total.</p>
<p>Liberty dataset [50] comprises system logs from the Liberty supercomputer at sandia national labs (SNL) in Albuquerque.This supercomputer features 512 processors and 944 GB of memory, and the dataset contains over 200 million log messages.For computational efficiency, we sample 5 million consecutive log messages, among which 1,600,525 are identified as anomalous, constituting approximately 32.01% of the total sampled messages.</p>
<p>In the context of HDFS, we adopt a session window strategy, which involves grouping log messages into sequences based on the block_id present in each log message.Each session is labeled using ground truth.For other datasets, including BGL, Thunderbird, and Liberty, we utilize a sliding window strategy to group log messages, with a window size of 100 messages and a step size of 100 messages.A log sequence is deemed anomalous if it contains at least one anomalous log message according to the ground truth.</p>
<p>Similar to existing work [8], [9], [19], [22], [34], [39], we split each dataset into a training set and a testing set with a ratio of 8:2 to evaluate the performance of a log-based anomaly detection approach.For the HDFS dataset, we randomly split the log sequences into training and testing data.In contrast, for the BGL, Thunderbird, and Liberty datasets, we adhere to a chronological split [6].This strategy ensures that all log sequences in the training set precede those in the testing set, reflecting real-world conditions and mitigating potential data leakage from unstable log data.</p>
<p>Table I summarizes the statistics of the datasets used in the experiments.</p>
<p>E. Performance Evaluation (RQ1)</p>
<p>Table II presents the experimental results of various logbased anomaly detection methods on the HDFS, BGL, Liberty, and Thunderbird datasets.The best results are highlighted in bold.We have the following observations:  The proposed LogLLM achieves the highest F 1 -score across all datasets.On average, LogLLM's F 1 -scores are 6.6% better than the best existing method, NeuralLog, demonstrating its effectiveness in log-based anomaly detection.Despite the adoption of LLMs in FastLogAD, LogBERT, NeuralLog, and RAPID for anomaly detection, their performance remains unsatisfactory.FastLogAD and LogBERT utilize BERT, a transformer encoder-based model, for detecting anomalies based on log sequence reconstruction errors.Their inputs consist of sequences of log template IDs (IDs of log string templates) extracted from log messages via log parsers, lacking semantic information.In contrast, NeuralLog and RAPID utilize transformer encoder-based models to extract semantic vectors from log messages.However, NeuralLog utilizes smaller models, whereas RAPID relies on distance-based comparison for anomaly sequence classification.LogLLM, on the other hand, leverages both BERT for extracting semantic vectors and Llama, a transformer decoder-based LLM, for anomaly detection.The representation spaces of BERT and Llama are aligned via a projector, fully harnessing the potential of LLMs for log-based anomaly detection.</p>
<p>Moreover, LogLLM achieves a balance between precision and recall, indicating that it maintains low false alarm rates and minimizes missed reports.In contrast, methods like Fast-LogAD are excessively sensitive to anomalies, often resulting in numerous false alarms.For example, on the BGL dataset, despite FastLogAD having a recall of 1, it only achieves a precision of 0.167, making it impractical for real-world use.Similarly, methods such as DeepLog, LogAnomaly and LogBERT exhibit similar issues.On the other hand, RAPID is not sensitive enough to anomalies, leading to many undetected anomalies.For instance, on the BGL dataset, RAPID achieves a precision of 0.874 but a recall of only 0.399.</p>
<p>Effect of labeled anomalies: As illustrated in Table II, in contrast to methods such as DeepLog, LogAnomaly, FastLo-gAD, LogBERT, and RAPID, which require clean datasets devoid of anomalies to build anomaly detection models, methods like PLELog, LogRobust, CNN, NeuralLog, and LogLLM demonstrate superior performance.These models are trained using not only normal samples but also labeled anomalies.For instance, these five methods achieve an average F 1 -score above 0.771 across four datasets, whereas others that do not utilize labeled anomalies perform poorly, with an average F 1score below 0.602 across four datasets.This demonstrates that incorporating labeled anomalies can provide a significant advantage to anomaly detection methods.</p>
<p>Computational cost: The time consumption of each method is presented in Table III.These results have been averaged across all the datasets.</p>
<p>Although RAPID does not require training a deep model, the extraction and retrieval of vector representations remain time-consuming.In comparison to other methods, FastLogAD requires relatively high training time, but it has the shortest testing time because it uses only the discriminator of the model during testing.As anticipated, while our proposed LogLLM demonstrates the best performance, it also incurs the highest  computational cost due to its large number of parameters.However, the testing time of LogLLM remains acceptable when compared to other methods that utilize LLMs, such as LogBERT, NeuralLog, and RAPID.</p>
<p>F. Different Preprocessing Techniques (RQ2)</p>
<p>We evaluate the effectiveness of the different preprocessing techniques.The results are shown in Table IV.In this table, 'Raw' indicates that the content of log messages is not preprocessed and is directly input into the proposed deep model.'Template' indicates that sequences of log templates produced by Drain [51], a log parser, are used as input for the proposed deep model.'Template ID' signifies that the IDs of log templates, obtained by Drain, are simply encoded into numeric vectors using an embedding layer instead of BERT.The preprocessing technique 'Template ID' renders the model unable to capture the semantic information within log messages.Notably, the parser Drain is applied to the entire dataset, rather than only the training dataset, to avoid performance degradation due to the OOV problem.'RE' indicates that regular expressions, as introduced in Section IV-A, are used for preprocessing log messages.</p>
<p>As anticipated, the preprocessing technique 'RE' yields the highest F 1 -score across all datasets.Conversely, the preprocessing technique 'Template ID' consistently results in the lowest F 1 -score across all datasets, averaging 36.8% lower than that of 'RE'.This can be attributed to the fact that 'Template ID' hinders the model's ability to capture the semantic information within log messages, thereby impairing its capability to detect anomalies from a natural language perspective.The preprocessing techniques 'Raw' and 'Template' result in relatively good performance, but their F 1 -scores are still 6.4% and 5.7% lower than that of 'RE', respectively.For the preprocessing technique 'Raw', the variable parts (parameters that carry dynamic runtime information) within the content of each log message have little influence on anomaly detection.However, due to their high randomness, they can confuse the model, making it difficult to discern anomalies.For the preprocessing technique 'Template', the parser is not always reliable, sometimes incorrectly removing the constant parts or retaining the variable parts, which can lead to information loss or confusion for the model, making it difficult to discern anomalies.</p>
<p>G. Effect of the Embedder (RQ3)</p>
<p>We investigate whether the embedder (BERT and adapter) is necessary for LogLLM.The results are presented in Table V. 'L.-1B' refers to directly inputting the log sequence (by concatenating log messages with semicolons (;) as separators into a long string) into the 'Llama-3.2-1B'model3 .'Emb.&amp; L.-1B' represents LogLLM based on 'Llama-3.2-1B'.</p>
<p>As expected, with the assistance of the embedder, the model requires less GPU memory, thereby avoiding out-of-memory (OOM) errors.Additionally, it enhances model performance by clarifying the boundaries between messages within a sequence.This improved representation enables the LLM to capture sequential dependencies better.</p>
<p>H. Effect of the Llama Model Size (RQ4)</p>
<p>As shown in Table V, larger LLaMA model sizes lead to better performance, at the cost of increased GPU memory usage and longer training times.</p>
<p>On average, compared to using Llama-3.2-1B,adopting Llama-3-8B improves the F 1 -score by 4.3%, but increases GPU memory usage by 7.7 GB and extends training time by 443.2 minutes.</p>
<p>I. Ablation Study of the Training Procedure (RQ5)</p>
<p>We investigate the effect of each training procedure through an ablation study.The results are presented in Table VI, where 'W/O' denotes 'without'.We have the following observations:</p>
<p>Skipping any training stage results in a decrease in the F 1score across all datasets, demonstrating the effectiveness of our This stage allows the embedder to generate better semantic vectors of log messages for Llama to discern anomalies.In summary, our proposed three-stage training procedure is well-suited for our deep model in log-based anomaly detection.</p>
<p>J. Impact of Minority Class Oversampling (RQ6)</p>
<p>Note that normal and anomalous samples in the training dataset are imbalanced, as shown in Table I.For the HDFS, BGL, and Thunderbird datasets, normal samples outnumber anomalous samples.Conversely, in the Liberty dataset, anomalous samples exceed normal samples.As described in Section IV-C1, the hyper-parameter β controls the proportion of the minority class by oversampling to address the data imbalance problem.In this section, we investigate the impact of β by varying its value.Fig. 4 illustrates the performance of LogLLM on the four datasets under different magnitudes of β.When β = 0, the samples are not oversampled; instead, the original datasets are utilized directly for training.</p>
<p>As illustrated in Fig. 4b, for the HDFS, BGL, and Thunderbird datasets, the recall always increases, while for the Liberty dataset, recall decreases as β increases.This can be attributed to the fact that for the HDFS, BGL, and Thunderbird datasets, when β increases, anomalies are oversampled, making the model more prone to identifying samples as anomalies.In As illustrated in Fig. 4c, the trend of the F 1 -score is basically the same across all datasets.The F 1 -score increases and then decreases as β increases.However, the LogLLM seems not to be sensitive to β; when β is between 10% and 80%, the variation in the F 1 -score is no more than 0.07.Thanks to the substantial semantic knowledge embedded in LLMs, a trained model can effectively learn anomalous patterns and detect anomalies, even when the minority class constitutes only 10% of the dataset.However, LogLLM appears unable to effectively handle extremely imbalanced scenarios.For instance, in the Thunderbird dataset, anomalies constitute only 1.05% of the samples, causing the trained model to be biased and classify all samples as normal.As a result, precision, recall, and F 1score are all equal to 0.</p>
<p>Compared to the BGL and Thunderbird datasets, the precision, recall and F 1 -score for the HDFS and Liberty datasets exhibit minimal variation with respect to β.This consistency arises from the more distinct patterns between abnormal and normal samples in the HDFS and Liberty datasets, allowing LogLLM to easily differentiate them, regardless of the ratio of normal and abnormal samples.</p>
<p>As anticipated, as β increases, the training time also increases, as shown in Fig. 4d.This relationship arises because a higher β leads to more oversampled data samples, as indicated by equation ( 1), thereby enlarging the training dataset.</p>
<p>To summarize, minority class oversampling is essential; however, the value of the hyperparameter β does not significantly impact the performance of LogLLM, making careful selection unnecessary.Moreover, excessively large values of β are undesirable, as they result in prolonged training times.Values between 30% and 50% are deemed acceptable.</p>
<p>VI. CONCLUSION</p>
<p>In this paper, we propose LogLLM, a novel log-based anomaly detection framework that leverages LLMs.LogLLM employs both transformer encoder-based and decoder-based LLMs, specifically BERT and Llama, for log-based anomaly detection.BERT is utilized to extract semantic vectors from log messages, while Llama is used to classify log sequences.To ensure coherence in log semantics, we introduce a projector that aligns the vector representation spaces of BERT and Llama.LogLLM is trained using an innovative three-stage procedure designed to enhance both performance and adaptability.Extensive experiments conducted on four public realworld datasets demonstrate that LogLLM achieves remarkable performance.Subsequent ablation studies further confirm the effectiveness of our three-stage training procedure.</p>
<p>Log Sequence # 2 1 2 CELogFig. 2 :
222
Fig. 2: Illustrative examples of log message partitioning.</p>
<p>Fig. 3 :
3
Fig. 3: The framework of LogLLM.Notably, the model includes a single instance of BERT and the projector.</p>
<p>•:</p>
<p>RQ1How effective is LogLLM in log-based anomaly detection?• RQ2: How do different preprocessing techniques impact the performance of LogLLM?• RQ3: How effective is the embedder for Llama?• RQ4: How does the size of the Llama model affect the performance of LogLLM?• RQ5: How does each stage of the three-stage training process influence the performance of LogLLM?• RQ6: How do different levels of minority class oversampling, determined by the hyperparameter β, affect the performance of LogLLM?</p>
<p>Fig. 4 :
4
Fig. 4: Impact of minority class oversampling.</p>
<p>TABLE I :
I
The statistics of datasets used in the experiments.</p>
<h1>Log messages # Log sequencesTraining DataTesting Data# Log sequences # Anomalies Anomaly ratio # Log sequences # Anomalies Anomaly ratioHDFS11,175,629575,061460,04813,4972.93%115,0133,3412.90%BGL4,747,96347,13537,7084,00910.63%9,4278178.67%Liberty5,000,00050,00040,00034,14485.36%10,0006516.51%Thunderbird10,000,00099,99779,9978371.05%20,000290.15%</h1>
<p>TABLE II :
II
Experimental results on HDFS, BGL, Liberty, and Thunderbird datasets.The best results are highlighted in bold.
MethodsDatasets Log parserPrec.HDFS Rec.F 1Prec.BGL Rec.F 1Prec.Liberty Rec.F 1Prec.Thunderbird Rec.F 1Avg. F 1DeepLog0.835 0.994 0.908 0.166 0.988 0.285 0.751 0.855 0.800 0.017 0.966 0.0330.506LogAnomaly0.886 0.893 0.966 0.176 0.985 0.299 0.684 0.876 0.768 0.025 0.966 0.0500.521PLELog0.893 0.979 0.934 0.595 0.880 0.710 0.795 0.874 0.832 0.808 0.724 0.7640.810FastLogAD0.721 0.893 0.798 0.167 1.000 0.287 0.151 0.999 0.263 0.008 0.931 0.0170.341LogBERT0.989 0.614 0.758 0.165 0.989 0.283 0.902 0.633 0.744 0.022 0.172 0.0390.456LogRobust0.961 1.000 0.980 0.696 0.968 0.810 0.695 0.979 0.813 0.318 1.000 0.4820.771CNN0.966 1.000 0.982 0.698 0.965 0.810 0.580 0.914 0.709 0.870 0.690 0.7690.818NeuralLog0.971 0.988 0.979 0.792 0.884 0.835 0.875 0.926 0.900 0.794 0.931 0.8570.893RAPID1.000 0.859 0.924 0.874 0.399 0.548 0.911 0.611 0.732 0.200 0.207 0.2030.602LogLLM0.994 1.000 0.997 0.861 0.979 0.916 0.992 0.926 0.958 0.966 0.966 0.9660.959</p>
<p>TABLE III :
III
Computational cost.
Training time (Minutes)Testing time (Minutes)DeepLog72.173.42LogAnomaly156.167.25PLELog315.4733.59LogRobust108.422.48CNN98.162.16FastLogAD254.170.29LogBERT429.0443.77NeuralLog267.4621.44RAPID63.9838.43LogLLM1,065.1564.48</p>
<p>TABLE IV :
IV
Effects of different preprocessing techniques on HDFS, BGL, Liberty, and Thunderbird datasets.The best results are highlighted in bold.
HDFSBGLLibertyThunderbirdAvg. F 1Prec.Rec.F 1Prec.Rec.F 1Prec.Rec.F 1Prec.Rec.F 1Raw0.9940.9910.9930.9430.7670.8460.9110.9080.9090.8060.8620.8330.895Template ID0.9950.9450.9690.7750.2860.4180.9940.2700.4251.0000.3790.5500.591Template0.9911.0000.9950.8610.9190.8890.9680.9310.9490.9500.6550.7760.902RE (LogLLM)0.9941.0000.9970.8610.9790.9160.9920.9260.9580.9660.9660.9660.959</p>
<p>TABLE V :
V
Effects of the embedder (BERT &amp; adapter) and LLaMA model size, where 'Mem.' indicates GPU memory usage (GB), and 'Tim.' indicates training time (Minutes).'-' indicates an out-of-memory (OOM) error.
HDFSBGLLibertyThunderbirdPrec. Rec.F 1 Mem. Tim. Prec. Rec.F 1 Mem. Tim. Prec. Rec.F 1 Mem. Tim. Prec. Rec.F 1 Mem. Tim.L.-1B0.986 0.995 0.991 16.5 1022.1-----0.960 0.699 0.809 42.6 443.2 1.000 0.724 0.840 44.5 1732.1Emb. &amp; L.-1B 0.996 0.996 0.996 8.0 1412.2 0.734 0.944 0.825 32.4 187.1 0.950 0.905 0.927 29.3 173.2 0.875 0.966 0.918 32.4 715.1L.-8B0.988 0.997 0.992 43.0 4712.1---------------Emb. &amp; L.-8B 0.994 1.000 0.997 16.6 2168.2 0.861 0.979 0.916 38.0 396.2 0.992 0.926 0.958 36.1 412.1 0.966 0.966 0.966 38.2 1284.2</p>
<p>TABLE VI :
VI
Ablation study of the training procedure on HDFS, BGL, Liberty, and Thunderbird datasets.The best results are highlighted in bold.(i.e., directly training the embedder), the embedder may be misdirected, resulting in incorrect semantic capture of log messages and model failure.Training without stage 3 yields relatively poor performance, with an average F 1score decrease of 10.5%.This indicates that sequentially finetuning Llama and training the embedder alone is insufficient for the model to capture anomalous patterns; cohesive finetuning of the entire model is essential.Training without stages 2 and 1&amp;2 also results in a performance decrease, with average F 1 -score reductions of 2.5% and 3.2%, respectively.This demonstrates that individually training the embedder before fine-tuning the entire model can also enhance performance.
HDFSBGLLibertyThunderbirdAvg. F 1Prec.Rec.F 1Prec.Rec.F 1Prec.Rec.F 1Prec.Rec.F 1W/O Stage 10.9911.0000.9950.5780.9710.7250.6850.2900.4080.3810.8280.5220.662W/O Stage 20.9941.0000.9970.8580.9200.8880.9950.9060.9490.8480.9660.9030.934W/O Stage 1&amp;20.9921.0000.9960.8530.8820.8680.9950.9060.9490.8970.8970.8970.927W/O Stage 30.9930.9990.9960.7040.7760.7381.0000.6840.8120.9580.7930.8680.854LogLLM0.9941.0000.9970.8610.9790.9160.9920.9260.9580.9660.9660.9660.959three-stage training procedure. It is noteworthy that trainingwithout stage 1 leads to the worst performance, with the F 1 -score averaged across all datasets decreasing by as much as29.7%. However, training without stages 1&amp;2 (only adoptingtraining stage 3: fine-tuning the entire model) yields acceptableperformance, with only a 3.2% decrease in the average F 1 -score. This demonstrates that fine-tuning Llama to capturethe answer template (Stage 1) is essential before training theembedder (BERT and projector) of log messages (Stage 2).Without stage 1
https://huggingface.co/google-bert/bert-base-uncased
https://huggingface.co/meta-llama/Meta-Llama-3-8B
https://huggingface.co/meta-llama/Llama-3.2-1B</p>
<p>Reliable and highly available distributed publish/subscribe service. R S Kazemzadeh, H.-A Jacobsen, 2009 28th IEEE International Symposium on Reliable Distributed Systems. IEEE2009</p>
<p>Reliability and availability of cloud computing. E Bauer, R Adams, 2012John Wiley &amp; Sons</p>
<p>Log-based anomaly detection without log parsing. V.-H Le, H Zhang, 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE2021</p>
<p>Survey and benchmark of anomaly detection in business processes. W Guan, J Cao, H Zhao, Y Gu, S Qian, IEEE Transactions on Knowledge and Data Engineering. 2024</p>
<p>End-to-end automl for unsupervised log anomaly detection. S Zhang, Y Ji, J Luan, X Nie, Z Chen, M Ma, Y Sun, D Pei, 2024Automated Software Engineering (ASE'24</p>
<p>Log-based anomaly detection with deep learning: How far are we?. V.-H Le, H Zhang, Proceedings of the 44th international conference on software engineering. the 44th international conference on software engineering2022</p>
<p>Loggpt: Exploring chatgpt for log-based anomaly detection. J Qi, S Huang, Z Luan, S Yang, C Fung, H Yang, D Qian, J Shang, Z Xiao, Z Wu, 2023 IEEE International Conference on High Performance Computing &amp; Communications, Data Science &amp; Systems, Smart City &amp; Dependability in Sensor, Cloud &amp; Big Data Systems &amp; Application. HPCC/DSS/SmartCity/DependSys. IEEE2023</p>
<p>Deeplog: Anomaly detection and diagnosis from system logs through deep learning. M Du, F Li, G Zheng, V Srikumar, Proceedings of the 2017 ACM SIGSAC conference on computer and communications security. the 2017 ACM SIGSAC conference on computer and communications security2017</p>
<p>Loganomaly: Unsupervised detection of sequential and quantitative anomalies in unstructured logs. W Meng, Y Liu, Y Zhu, S Zhang, D Pei, Y Liu, Y Chen, R Zhang, S Tao, P Sun, IJCAI. 1972019</p>
<p>Logattn: Unsupervised log anomaly detection with an autoencoder based attention mechanism. L Zhang, W Li, Z Zhang, Q Lu, C Hou, P Hu, T Gui, S Lu, International conference on knowledge science, engineering and management. Springer2021</p>
<p>Autolog: Anomaly detection by deep autoencoding of system logs. M Catillo, A Pecchia, U Villano, Expert Systems with Applications. 1911162632022</p>
<p>Log anomaly detection by adversarial autoencoders with graph feature fusion. Y Xie, K Yang, IEEE Transactions on Reliability. 2023</p>
<p>Anomaly detection model for log based on lstm network and variational autoencoder. X Zhang, X Chai, M Yu, D Qiu, 2023 4th International Conference on Information Science, Parallel and Distributed Systems (ISPDS). IEEE2023</p>
<p>A generative adversarial networks for log anomaly detection. X Duan, S Ying, W Yuan, H Cheng, X Yin, Comput. Syst. Sci. Eng. 3712021</p>
<p>Graph-based log anomaly detection via adversarial training. Z He, Y Tang, K Zhao, J Liu, W Chen, International Symposium on Dependable Software Engineering: Theories, Tools, and Applications. Springer2023</p>
<p>Layerlog: Log sequence anomaly detection based on hierarchical semantics. C Zhang, X Wang, H Zhang, J Zhang, H Zhang, C Liu, P Han, Applied Soft Computing. 1321098602023</p>
<p>Onelog: towards end-to-end software log anomaly detection. S Hashemi, M Mäntylä, Automated Software Engineering. 312372024</p>
<p>Detecting anomaly in big data system logs using convolutional neural network. S Lu, X Wei, Y Li, L Wang, 2018 IEEE 16th Intl Conf on Dependable, Autonomic and Secure Computing, 16th Intl Conf on Pervasive Intelligence and Computing, 4th Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress. </p>
<p>Robust log-based anomaly detection on unstable log data. X Zhang, Y Xu, Q Lin, B Qiao, H Zhang, Y Dang, C Xie, X Yang, Q Cheng, Z Li, Proceedings of the 2019 27th ACM joint meeting on European software engineering conference and symposium on the foundations of software engineering. the 2019 27th ACM joint meeting on European software engineering conference and symposium on the foundations of software engineering2019</p>
<p>Loggd: Detecting anomalies from system logs with graph neural networks. Y Xie, H Zhang, M A Babar, 2022 IEEE 22nd International conference on software quality, reliability and security (QRS). IEEE2022</p>
<p>Trine: Syslog anomaly detection with three transformer encoders in one generative adversarial network. Z Zhao, W Niu, X Zhang, R Zhang, Z Yu, C Huang, Applied Intelligence. 5282022</p>
<p>Semi-supervised log-based anomaly detection via probabilistic label estimation. L Yang, J Chen, Z Wang, W Wang, J Jiang, X Dong, W Zhang, 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE2021</p>
<p>Long short-term memory. S Hochreiter, 1997Neural Computation MIT-Press</p>
<p>Attention is all you need. A Vaswani, Advances in Neural Information Processing Systems. 2017</p>
<p>J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>The llama 3 herd of models. A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Yang, A Fan, arXiv:2407.217832024arXiv preprint</p>
<p>T Glm, A Zeng, B Xu, B Wang, C Zhang, D Yin, D Rojas, G Feng, H Zhao, H Lai, arXiv:2406.12793Chatglm: A family of large language models from glm-130b to glm-4 all tools. 2024arXiv preprint</p>
<p>Dabl: Detecting semantic anomalies in business processes using large language models. W Guan, J Cao, J Gao, H Zhao, S Qian, arXiv:2406.157812024arXiv preprint</p>
<p>Logprompt: Prompt engineering towards zero-shot and interpretable log analysis. Y Liu, S Tao, W Meng, F Yao, X Zhao, H Yang, Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings. the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings2024</p>
<p>Early exploration of using chatgpt for log-based anomaly detection on parallel file systems logs. C Egersdoerfer, D Zhang, D Dai, Proceedings of the 32nd International Symposium on High-Performance Parallel and Distributed Computing. the 32nd International Symposium on High-Performance Parallel and Distributed Computing2023</p>
<p>Raglog: Log anomaly detection using retrieval augmented generation. J Pan, W S Liang, Y Yidi, 2024 IEEE World Forum on Public Safety Technology (WFPST). IEEE2024</p>
<p>Logbert: Log anomaly detection via bert. H Guo, S Yuan, X Wu, 2021 international joint conference on neural networks (IJCNN). IEEE2021</p>
<p>Lanobert: System log anomaly detection based on bert masked language model. Y Lee, J Kim, P Kang, Applied Soft Computing. 1461106892023</p>
<p>Fastlogad: Log anomaly detection with mask-guided pseudo anomaly generation and discrimination. Y Lin, H Deng, X Li, arXiv:2404.087502024arXiv preprint</p>
<p>Logfit: Log anomaly detection using fine-tuned language models. C Almodovar, F Sabrina, S Karimi, S Azad, IEEE Transactions on Network and Service Management. 2024</p>
<p>Bert-log: Anomaly detection for system logs based on pre-trained language model. S Chen, H Liao, Applied Artificial Intelligence. 36121456422022</p>
<p>Sarlog: Semantic-aware robust log anomaly detection via bert-augmented contrastive learning. J L Adeba, D.-H Kim, J Kwak, IEEE Internet of Things Journal. 2024</p>
<p>Mlog: Mogrifier lstm-based log anomaly detection approach using semantic representation. Y Fu, K Liang, J Xu, IEEE Transactions on Services Computing. 1652023</p>
<p>Training-free retrieval-based log anomaly detection with pre-trained language model considering tokenlevel information. G No, Y Lee, H Kang, P Kang, Engineering Applications of Artificial Intelligence. 1331086132024</p>
<p>Anomaly detection on unstable logs with gpt models. F Hadadi, Q Xu, D Bianculli, L Briand, arXiv:2406.074672024arXiv preprint</p>
<p>The working limitations of large language models. M Burtsev, M Reeves, A Job, MIT Sloan Management Review. 6522024</p>
<p>Fasttext. zip: Compressing text classification models. A Joulin, arXiv:1612.036512016arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, arXiv:1810.048052018arXiv preprint</p>
<p>Roberta: A robustly optimized bert pretraining approach. Y Liu, arXiv:1907.116922019arXiv preprint</p>
<p>Spanbert: Improving pre-training by representing and predicting spans. M Joshi, D Chen, Y Liu, D S Weld, L Zettlemoyer, O Levy, Transactions of the association for computational linguistics. 82020</p>
<p>Retrievalaugmented generation for knowledge-intensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W -T. Yih, T Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Tools and benchmarks for automated log parsing. J Zhu, S He, J Liu, P He, Q Xie, Z Zheng, M R Lyu, 2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP). IEEE2019</p>
<p>An evaluation study on log parsing and its use in log mining. P He, J Zhu, S He, J Li, M R Lyu, 2016 46th annual IEEE/IFIP international conference on dependable systems and networks (DSN). </p>
<p>Online system problem detection by mining patterns of console logs. W Xu, L Huang, A Fox, D Patterson, M Jordan, 2009 ninth IEEE international conference on data mining. IEEE2009</p>
<p>What supercomputers say: A study of five system logs. A Oliner, J Stearley, 37th annual IEEE/IFIP international conference on dependable systems and networks (DSN'07. IEEE2007</p>
<p>Drain: An online log parsing approach with fixed depth tree. P He, J Zhu, Z Zheng, M R Lyu, 2017 IEEE international conference on web services (ICWS). IEEE2017</p>
<p>Spell: Streaming parsing of system event logs. M Du, F Li, 2016 IEEE 16th International Conference on Data Mining (ICDM). </p>
<p>Log parsing with prompt-based few-shot learning. V.-H Le, H Zhang, 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE2023</p>
<p>Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. J S Bridle, Neurocomputing: Algorithms, architectures and applications. Springer1990</p>
<p>Qlora: Efficient finetuning of quantized llms. T Dettmers, A Pagnoni, A Holtzman, L Zettlemoyer, Advances in Neural Information Processing Systems. 202436</p>
<p>Decoupled weight decay regularization. I Loshchilov, arXiv:1711.051012017arXiv preprint</p>
<p>Loghub: A large collection of system log datasets for ai-driven log analytics. J Zhu, S He, P He, J Liu, M R Lyu, 2023 IEEE 34th International Symposium on Software Reliability Engineering (ISSRE). </p>            </div>
        </div>

    </div>
</body>
</html>
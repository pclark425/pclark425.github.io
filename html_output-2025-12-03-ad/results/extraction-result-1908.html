<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1908 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1908</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1908</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-40.html">extraction-schema-40</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <p><strong>Paper ID:</strong> paper-282102744</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2510.13778v1.pdf" target="_blank">InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy</a></p>
                <p><strong>Paper Abstract:</strong> We introduce InternVLA-M1, a unified framework for spatial grounding and robot control that advances instruction-following robots toward scalable, general-purpose intelligence. Its core idea is spatially guided vision-language-action training, where spatial grounding serves as the critical link between instructions and robot actions. InternVLA-M1 employs a two-stage pipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning data to determine ``where to act''by aligning instructions with visual, embodiment-agnostic positions, and (ii) spatially guided action post-training to decide ``how to act''by generating embodiment-aware actions through plug-and-play spatial prompting. This spatially guided training recipe yields consistent gains: InternVLA-M1 outperforms its variant without spatial guidance by +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO Franka, while demonstrating stronger spatial reasoning capability in box, point, and trace prediction. To further scale instruction following, we built a simulation engine to collect 244K generalizable pick-and-place episodes, enabling a 6.2% average improvement across 200 tasks and 3K+ objects. In real-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with synthetic co-training, achieved +20.6% on unseen objects and novel configurations. Moreover, in long-horizon reasoning-intensive scenarios, it surpassed existing works by over 10%. These results highlight spatially guided training as a unifying principle for scalable and resilient generalist robots. Code and models are available at https://github.com/InternRobotics/InternVLA-M1.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1908.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1908.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InternVLA-M1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InternVLA-M1: A Spatially Guided Vision-Language-Action Framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dual-system VLA model that separates an embodiment-agnostic VLM planner (System 2) trained on large-scale spatial grounding data from an embodiment-specific Action Expert (System 1), and connects them via latent spatial prompting and a lightweight querying transformer to improve spatial grounding and downstream robot control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InternVLA-M1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dual-system end-to-end VLA: a VLM Planner (language head) is pre-trained on >2.3M spatial grounding samples (points, boxes, traces) to learn embodiment-agnostic spatial priors; an Action Expert (DiT Actor / diffusion policy) models embodiment-specific motor commands. During post-training the Action Expert conditions on latent planning embeddings emitted by the VLM Planner via a lightweight querying transformer (cross-attention with learnable query tokens) and explicit spatial prompting. A gradient-decay factor attenuates gradients from the Action Expert back into the VLM to preserve semantic reasoning. Training alternates robot trajectory updates (both modules) and spatial grounding updates (VLM only).</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>DINOv2 visual encoder (used in Action Expert) and Qwen2.5-style multimodal encoder for the VLM planner (Qwen2.5-VL variants referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>DINOv2 (self-supervised visual pretraining; paper references DINOv2 technical literature) for the Action Expert; VLM backbone based on Qwen2.5-VL family (Qwen2.5-VL-3B-Instruct referenced). The paper does not provide exact dataset sizes for these pretrainings beyond referencing the corresponding technical reports.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Explicit spatial grounding via VLM outputs of spatial tokens/embeddings (points, boxes, traces) learned in Stage 1; at inference these embeddings are activated by spatial prompting and fed to the Action Expert via a querying transformer implementing cross-attention between learnable query tokens and intermediate VLM layers. Co-training alternates VLM-only grounding updates and joint action updates so the action head conditions on grounded spatial embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Multi-level: region/box-level and point-level spatial outputs and 2D end-effector traces (explicit box/point/trace supervision) plus learned latent planning embeddings used for conditioning actions.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Explicit spatial representations: 2D bounding boxes, 2D points, and end-effector traces; latent spatial planning tokens (embeddings) produced by the VLM planner; supervisory spatial QA-style labels in pre-training.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Instruction following for object manipulation / pick-and-place, long-horizon manipulation, drawer manipulation, sandwich assembly, affordance prediction and trajectory prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>SimplerEnv (Google Robot, WidowX), LIBERO (Franka), in-house Isaac-Sim large-scale pick-and-place benchmark (200 tasks, 3k+ objects), real-world clustered pick-and-place and long-horizon benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Photorealistic simulation (Isaac-Sim, GenManip pipeline) and real-world robot RGB camera inputs (wrist-mounted and third-person RealSense D435 views), with synthetic rendering variations for domain randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success Rate (SR) for manipulation tasks; IoU@0.5 for box grounding (RefCOCO-style), trajectory MAE for trajectory tasks, Point accuracy for point prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Multiple reported results: e.g., SimplerEnv (Google Robot average) InternVLA-M1 reported avg ~80.7% (table rows show 95.3 / 90.0 / 75.5 / 62.0 / 80.7 breakdowns); compared to variant without spatial guidance, InternVLA-M1 outperforms by +14.6% on SimplerEnv Google Robot, +17.0% on WidowX, and +4.3% on LIBERO; In-house large-scale pick-and-place: +6.2% average improvement across 200 tasks over GR00T N1.5 when mid-trained; real-world clustered pick-and-place: +7.3% improvement, and with synthetic co-training achieved +20.6% on unseen objects and novel configurations. Long-horizon tasks: >10% improvement over baselines on reasoning-intensive benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Ablation results: variant without spatial guidance is lower by the reported deltas: -14.6% SR on SimplerEnv Google Robot, -17.0% SR on WidowX, and -4.3% SR on LIBERO (paper explicitly states these deltas). Figure 5 ablation shows IoU and SR degrade when omitting spatial prompting/co-training.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Reported improvements attributable to spatial grounding/spatial prompting: +14.6% (SimplerEnv Google Robot), +17.0% (WidowX), +4.3% (LIBERO); +6.2% average in large-scale simulation pick-and-place vs GR00T N1.5 (with mid-train); +20.6% on unseen objects in real clustered pick-and-place when synthetic co-training used.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>No explicit controlled comparison between different visual encoders or pretraining datasets is provided. The paper cites use of DINOv2 for the Action Expert and Qwen2.5-VL-family for the VLM, but does not present a head-to-head encoder ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Authors identify a grounding/perception bottleneck: vanilla VLA models 'overfit fine-grained motor behaviors while under-generalizing to high-level linguistic instructions involving absolute or relational positions.' They report low gradient-subspace alignment (Projection-space Similarity PSS=0.25) in vanilla co-training, which they link to misalignment between spatial grounding and action objectives; spatially guided training increases PSS to 0.42, improving optimization consistency. They also report rapid degradation of spatial grounding when spatial data and spatial prompting are omitted.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Qualitative and quantitative failure analyses: (1) Overfitting to motor behaviors and under-generalization to spatial language (leading to failures on absolute/relational instructions); (2) Omission of spatial data/prompting leads to degraded box/point/trace prediction (Figure 5); (3) Baselines suffer larger performance drops under perturbations (physical interference, task replanning) while InternVLA-M1 maintains robustness; (4) Domain shifts (new backgrounds / unseen objects) cause variance but mid-training on synthetic InternData M1 reduces failures. The paper does not provide precise per-failure-type percentages except the reported delta improvements and PSS numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Multiple techniques: large-scale spatial grounding pre-training on mixed web/robot/synthetic data; synthetic mid-training (InternData M1, 244K pick-and-place episodes) with domain randomization (lighting, textures); calibration of simulated cameras to real camera intrinsics/extrinsics; co-training (alternating spatial and trajectory batches). These reduce performance degradation under new backgrounds and unseen objects (reported improvements: mid-train variant surpasses baselines and reduces variance). No exact absolute drop numbers for sim->real are provided, but real-world unseen-object improvement is reported as +20.6% with synthetic co-training.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Reported: in in-house simulation and real-world tests, InternVLA-M1 w/ synthetic co-training improves performance on unseen objects by +20.6% SR in clustered pick-and-place; in the 200-task simulation benchmark the model obtained an average +6.2% over GR00T N1.5 (indicating improved generalization to novel objects and instructions).</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Paper emphasizes scale: spatial grounding pre-training uses >2.3M spatial reasoning samples and total multimodal training >3M samples; authors attribute improved spatial perception and downstream action transfer to this scale. There is no controlled ablation sweeping dataset scale to quantify marginal gains per data factor, so claims are qualitative/empirical rather than a formal scaling curve.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Cross-attention via an 8.7 MB querying transformer: learnable fixed query tokens perform cross-attention over intermediate VLM layers to map variable-length latent planning tokens into a fixed conditioning vector for the Action Expert; spatial prompting (text appended to instruction) is used to activate spatial perception. Losses are summed during joint training (robot trajectory loss + VLM next-token loss on spatial grounding batches).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Fine-tuning uses as few as five trajectories per task in in-house experiments (w/o mid-train). Mid-training on InternData M1 (244K synthetic samples) before fine-tuning yields consistent gains: an average +6.2% improvement over baseline GR00T N1.5 across 200 tasks, indicating improved sample efficiency for downstream fine-tuning; exact sample-efficiency multipliers (e.g., 2x, 5x) are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Explicit, large-scale spatial grounding pre-training (points/boxes/traces) combined with spatial prompting and a latent-planning interface (querying transformer with gradient attenuation) substantially improves alignment between perception and action objectives (PSS 0.25 -> 0.42), accelerates convergence in manipulation training, increases robustness to domain shifts (backgrounds, unseen objects), and yields sizable SR gains over baselines (e.g., +14.6% Google Robot, +17% WidowX, +20.6% unseen real-world objects with synthetic co-training). Removing spatial guidance or prompting degrades grounding IoU and downstream manipulation SR, showing grounding is a critical bottleneck and lever for embodied VLA performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Roboground: Robotic manipulation with grounded vision-language priors <em>(Rating: 2)</em></li>
                <li>Robopoint: A vision-language model for spatial affordance prediction for robotics <em>(Rating: 2)</em></li>
                <li>GenManip: LLM-driven simulation for generalizable instruction-following manipulation <em>(Rating: 2)</em></li>
                <li>Knowledge insulating vision-language-action models: Train fast, run fast, generalize better <em>(Rating: 2)</em></li>
                <li>GR00T N1: An open foundation model for generalist humanoid robots <em>(Rating: 2)</em></li>
                <li>Rt-affordance: Affordances are versatile intermediate representations for robot manipulation <em>(Rating: 2)</em></li>
                <li>Diffusion policy: Visuomotor policy learning via action diffusion <em>(Rating: 1)</em></li>
                <li>Qwen2.5-vl technical report <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1908",
    "paper_id": "paper-282102744",
    "extraction_schema_id": "extraction-schema-40",
    "extracted_data": [
        {
            "name_short": "InternVLA-M1",
            "name_full": "InternVLA-M1: A Spatially Guided Vision-Language-Action Framework",
            "brief_description": "A dual-system VLA model that separates an embodiment-agnostic VLM planner (System 2) trained on large-scale spatial grounding data from an embodiment-specific Action Expert (System 1), and connects them via latent spatial prompting and a lightweight querying transformer to improve spatial grounding and downstream robot control.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InternVLA-M1",
            "model_description": "Dual-system end-to-end VLA: a VLM Planner (language head) is pre-trained on &gt;2.3M spatial grounding samples (points, boxes, traces) to learn embodiment-agnostic spatial priors; an Action Expert (DiT Actor / diffusion policy) models embodiment-specific motor commands. During post-training the Action Expert conditions on latent planning embeddings emitted by the VLM Planner via a lightweight querying transformer (cross-attention with learnable query tokens) and explicit spatial prompting. A gradient-decay factor attenuates gradients from the Action Expert back into the VLM to preserve semantic reasoning. Training alternates robot trajectory updates (both modules) and spatial grounding updates (VLM only).",
            "visual_encoder_type": "DINOv2 visual encoder (used in Action Expert) and Qwen2.5-style multimodal encoder for the VLM planner (Qwen2.5-VL variants referenced)",
            "visual_encoder_pretraining": "DINOv2 (self-supervised visual pretraining; paper references DINOv2 technical literature) for the Action Expert; VLM backbone based on Qwen2.5-VL family (Qwen2.5-VL-3B-Instruct referenced). The paper does not provide exact dataset sizes for these pretrainings beyond referencing the corresponding technical reports.",
            "grounding_mechanism": "Explicit spatial grounding via VLM outputs of spatial tokens/embeddings (points, boxes, traces) learned in Stage 1; at inference these embeddings are activated by spatial prompting and fed to the Action Expert via a querying transformer implementing cross-attention between learnable query tokens and intermediate VLM layers. Co-training alternates VLM-only grounding updates and joint action updates so the action head conditions on grounded spatial embeddings.",
            "representation_level": "Multi-level: region/box-level and point-level spatial outputs and 2D end-effector traces (explicit box/point/trace supervision) plus learned latent planning embeddings used for conditioning actions.",
            "spatial_representation": "Explicit spatial representations: 2D bounding boxes, 2D points, and end-effector traces; latent spatial planning tokens (embeddings) produced by the VLM planner; supervisory spatial QA-style labels in pre-training.",
            "embodied_task_type": "Instruction following for object manipulation / pick-and-place, long-horizon manipulation, drawer manipulation, sandwich assembly, affordance prediction and trajectory prediction.",
            "embodied_task_name": "SimplerEnv (Google Robot, WidowX), LIBERO (Franka), in-house Isaac-Sim large-scale pick-and-place benchmark (200 tasks, 3k+ objects), real-world clustered pick-and-place and long-horizon benchmarks",
            "visual_domain": "Photorealistic simulation (Isaac-Sim, GenManip pipeline) and real-world robot RGB camera inputs (wrist-mounted and third-person RealSense D435 views), with synthetic rendering variations for domain randomization.",
            "performance_metric": "Success Rate (SR) for manipulation tasks; IoU@0.5 for box grounding (RefCOCO-style), trajectory MAE for trajectory tasks, Point accuracy for point prediction.",
            "performance_value": "Multiple reported results: e.g., SimplerEnv (Google Robot average) InternVLA-M1 reported avg ~80.7% (table rows show 95.3 / 90.0 / 75.5 / 62.0 / 80.7 breakdowns); compared to variant without spatial guidance, InternVLA-M1 outperforms by +14.6% on SimplerEnv Google Robot, +17.0% on WidowX, and +4.3% on LIBERO; In-house large-scale pick-and-place: +6.2% average improvement across 200 tasks over GR00T N1.5 when mid-trained; real-world clustered pick-and-place: +7.3% improvement, and with synthetic co-training achieved +20.6% on unseen objects and novel configurations. Long-horizon tasks: &gt;10% improvement over baselines on reasoning-intensive benchmarks.",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Ablation results: variant without spatial guidance is lower by the reported deltas: -14.6% SR on SimplerEnv Google Robot, -17.0% SR on WidowX, and -4.3% SR on LIBERO (paper explicitly states these deltas). Figure 5 ablation shows IoU and SR degrade when omitting spatial prompting/co-training.",
            "grounding_improvement": "Reported improvements attributable to spatial grounding/spatial prompting: +14.6% (SimplerEnv Google Robot), +17.0% (WidowX), +4.3% (LIBERO); +6.2% average in large-scale simulation pick-and-place vs GR00T N1.5 (with mid-train); +20.6% on unseen objects in real clustered pick-and-place when synthetic co-training used.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": "No explicit controlled comparison between different visual encoders or pretraining datasets is provided. The paper cites use of DINOv2 for the Action Expert and Qwen2.5-VL-family for the VLM, but does not present a head-to-head encoder ablation.",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Authors identify a grounding/perception bottleneck: vanilla VLA models 'overfit fine-grained motor behaviors while under-generalizing to high-level linguistic instructions involving absolute or relational positions.' They report low gradient-subspace alignment (Projection-space Similarity PSS=0.25) in vanilla co-training, which they link to misalignment between spatial grounding and action objectives; spatially guided training increases PSS to 0.42, improving optimization consistency. They also report rapid degradation of spatial grounding when spatial data and spatial prompting are omitted.",
            "failure_mode_analysis": "Qualitative and quantitative failure analyses: (1) Overfitting to motor behaviors and under-generalization to spatial language (leading to failures on absolute/relational instructions); (2) Omission of spatial data/prompting leads to degraded box/point/trace prediction (Figure 5); (3) Baselines suffer larger performance drops under perturbations (physical interference, task replanning) while InternVLA-M1 maintains robustness; (4) Domain shifts (new backgrounds / unseen objects) cause variance but mid-training on synthetic InternData M1 reduces failures. The paper does not provide precise per-failure-type percentages except the reported delta improvements and PSS numbers.",
            "domain_shift_handling": "Multiple techniques: large-scale spatial grounding pre-training on mixed web/robot/synthetic data; synthetic mid-training (InternData M1, 244K pick-and-place episodes) with domain randomization (lighting, textures); calibration of simulated cameras to real camera intrinsics/extrinsics; co-training (alternating spatial and trajectory batches). These reduce performance degradation under new backgrounds and unseen objects (reported improvements: mid-train variant surpasses baselines and reduces variance). No exact absolute drop numbers for sim-&gt;real are provided, but real-world unseen-object improvement is reported as +20.6% with synthetic co-training.",
            "novel_object_performance": "Reported: in in-house simulation and real-world tests, InternVLA-M1 w/ synthetic co-training improves performance on unseen objects by +20.6% SR in clustered pick-and-place; in the 200-task simulation benchmark the model obtained an average +6.2% over GR00T N1.5 (indicating improved generalization to novel objects and instructions).",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": "Paper emphasizes scale: spatial grounding pre-training uses &gt;2.3M spatial reasoning samples and total multimodal training &gt;3M samples; authors attribute improved spatial perception and downstream action transfer to this scale. There is no controlled ablation sweeping dataset scale to quantify marginal gains per data factor, so claims are qualitative/empirical rather than a formal scaling curve.",
            "fusion_mechanism": "Cross-attention via an 8.7 MB querying transformer: learnable fixed query tokens perform cross-attention over intermediate VLM layers to map variable-length latent planning tokens into a fixed conditioning vector for the Action Expert; spatial prompting (text appended to instruction) is used to activate spatial perception. Losses are summed during joint training (robot trajectory loss + VLM next-token loss on spatial grounding batches).",
            "sample_efficiency": "Fine-tuning uses as few as five trajectories per task in in-house experiments (w/o mid-train). Mid-training on InternData M1 (244K synthetic samples) before fine-tuning yields consistent gains: an average +6.2% improvement over baseline GR00T N1.5 across 200 tasks, indicating improved sample efficiency for downstream fine-tuning; exact sample-efficiency multipliers (e.g., 2x, 5x) are not provided.",
            "key_findings_grounding": "Explicit, large-scale spatial grounding pre-training (points/boxes/traces) combined with spatial prompting and a latent-planning interface (querying transformer with gradient attenuation) substantially improves alignment between perception and action objectives (PSS 0.25 -&gt; 0.42), accelerates convergence in manipulation training, increases robustness to domain shifts (backgrounds, unseen objects), and yields sizable SR gains over baselines (e.g., +14.6% Google Robot, +17% WidowX, +20.6% unseen real-world objects with synthetic co-training). Removing spatial guidance or prompting degrades grounding IoU and downstream manipulation SR, showing grounding is a critical bottleneck and lever for embodied VLA performance.",
            "uuid": "e1908.0"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Roboground: Robotic manipulation with grounded vision-language priors",
            "rating": 2
        },
        {
            "paper_title": "Robopoint: A vision-language model for spatial affordance prediction for robotics",
            "rating": 2
        },
        {
            "paper_title": "GenManip: LLM-driven simulation for generalizable instruction-following manipulation",
            "rating": 2
        },
        {
            "paper_title": "Knowledge insulating vision-language-action models: Train fast, run fast, generalize better",
            "rating": 2
        },
        {
            "paper_title": "GR00T N1: An open foundation model for generalist humanoid robots",
            "rating": 2
        },
        {
            "paper_title": "Rt-affordance: Affordances are versatile intermediate representations for robot manipulation",
            "rating": 2
        },
        {
            "paper_title": "Diffusion policy: Visuomotor policy learning via action diffusion",
            "rating": 1
        },
        {
            "paper_title": "Qwen2.5-vl technical report",
            "rating": 1
        }
    ],
    "cost": 0.013713,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy
15 Oct 2025</p>
<p>Intern Robotics 
Shanghai Ai Laboratory 
InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy
15 Oct 202545CF65B8225EEB55D354F1E2C7F233A2arXiv:2510.13778v1[cs.RO]A: [List of 2D Trajectory Points] A: [(320280)(358280)(396290)]
We introduce InternVLA-M1, a unified framework for spatial grounding and robot control that advances instructionfollowing robots toward scalable, general-purpose intelligence.Its core idea is spatially guided vision-languageaction training, where spatial grounding serves as the critical link between instructions and robot actions.InternVLA-M1 employs a two-stage pipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning data to determine "where to act" by aligning instructions with visual, embodiment-agnostic positions, and (ii) spatially guided action post-training to decide "how to act" by generating embodiment-aware actions through plug-and-play spatial prompting.This spatially guided training recipe yields consistent gains: InternVLA-M1 outperforms its variant without spatial guidance by +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO Franka, while demonstrating stronger spatial reasoning capability in box, point, and trace prediction.To further scale instruction following, we built a simulation engine to collect 244K generalizable pick-and-place episodes, enabling a 6.2% average improvement across 200 tasks and 3K+ objects.In real-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with synthetic co-training, achieved +20.6% on unseen objects and novel configurations.Moreover, in long-horizon reasoning-intensive scenarios, it surpassed existing works by over 10%.These results highlight spatially guided training as a unifying principle for scalable and resilient generalist robots.</p>
<p>Introduction</p>
<p>Large multimodal foundation models Bai et al. (2025b); Chen et al. (2024); Li et al. (2024b); Radford et al. (2021); Zhai et al. (2023) have demonstrated strong generalization by leveraging web-scale vision-language alignment and instruction-following corpora.To extend these capabilities into the physical domain, robots must not only understand what an instruction means but also determine where and how to act in the 3D world.This gap is fundamental.Textual abstractions capture spatial cues only indirectly, whereas real-world actions demand continuous, embodied interactions that are scarcely represented in the training data of vision-language models (VLMs).Teleoperated datasets Bu et al. (2025a); Collaboration et al. (2023); Khazatsky et al. (2024); Wu et al. (2024) provide valuable supervision; yet, their scale and diversity remain modest compared to large instruction-following corpora.In this context, an embodiment-agnostic spatial prior, which functions as a bridge between textual instructions and embodiment-specific motor commands, offers a promising foundation for scalable robot learning.</p>
<p>Prior work has approached this challenge through hierarchical robotic systems Cao et al. (2025); Huang et al. (2024aHuang et al. ( , 2023Huang et al. ( , 2024b)); Liu et al. (2024); Qi et al. (2025); Yuan et al. (2024), which explicitly encode spatial priors using foundation models Fang et al. (2023); Kirillov et al. (2023); Oquab et al. (2023) but often rely on rule-based task decomposition and manually designed planning heuristics.This rigid separation between symbolic task structures and low-level motor control makes such systems difficult to scale automatically to more complex and diverse tasks, particularly hindering end-to-end policy learning.In contrast, recent data-driven VLAs AI (2024); Black et al. (2024); Brohan et al. (2023); Kim et al. (2024); Lee et al. (2025); Shi et al. (2025) leverage pretrained vision-language models and large-scale teleoperation datasets Bu et al. (2025a); Collaboration et al.</p>
<p>Figure 1.InternVLA-M1 integrates spatial grounding into the vision-language-action training pipeline.Given a task instruction, the VLM planner produces latent plans through explicit spatial prompting, which then effectively guides the action expert to generate control signals.</p>
<p>(2023); Khazatsky et al. (2024); Wu et al. (2024) to directly learn robot control.However, these models tend to overfit fine-grained motor behaviors while under-generalizing to high-level linguistic instructions that involve absolute or relational positions, thereby failing to fully incorporate spatial priors into execution.Core spatial priors such as object recognition, affordance grounding, visual trajectory reasoning, relative localization, and scaling provide transferable knowledge across robotic platforms.Once these priors are established, embodiment-specific learning can focus on concrete control strategies (e.g., manipulator joints, end-effector trajectories, humanoid locomotion, or mobile navigation).Such a division clarifies the role of spatial priors as general-purpose foundations while leaving embodiment-specific details to downstream adaptation, thereby bridging the gap between abstract instruction following and grounded physical execution.</p>
<p>Building on the separation between spatial priors and embodiment-specific control, we introduce InternVLA-M1, a dual-system vision-language-action framework that unifies high-level reasoning with grounded execution.InternVLA-M1 consists of a spatial prior VLM planner that interprets linguistic instructions and reasons about spatial relations, and an action expert that translates these grounded representations into executable motor commands.To achieve this, we construct over 3M multimodal training samples, including 2.3M spatial grounding data and 0.7M multimodal understanding data collected from web, real-world, and simulated sources.To leverage the above pre-training data, we propose spatially guided two-stage training recipes: (i) spatial grounding pre-training for the VLM, which establishes transferable spatial understanding through large-scale multimodal supervision on points, boxes, and traces; and (ii) spatially guided action post-training, which specializes these priors for embodiment-specific control under joint supervision.This design bridges abstract goal reasoning with concrete physical execution, enabling robust instruction following across diverse and complex environments.</p>
<p>To validate these capabilities, we conduct a comprehensive evaluation across multiple benchmarks in both simulated environments and real-world settings.InternVLA-M1 demonstrates strong generalization and robust performance across diverse scenarios:</p>
<p>• On SimplerEnv (Google Robot and WidowX), InternVLA-M1 achieves a new state of the art, surpassing its variant by improving the average success rate by up to +5.9% and +9.8%, respectively.It also demonstrates strong spatial reasoning capabilities across box, point, and trace prediction tasks.Further analysis shows that spatially guided action post-training effectively transfers spatial reasoning ability to motor control.• For the generalizable pick-and-place 200 tabletop scenarios, our model exhibits strong generalization to unseen objects and instructions under few-demonstration fine-tuning, achieving an average improvement of 6.2% over prior works.• In real-world settings, InternVLA-M1 demonstrates strong instruction-following capability, achieving a +20.6% success rate on unseen objects and novel setups in clustered pick-and-place tasks.It also maintains robust long-horizon performance under perturbations (e.g., physical interference, task replanning), outperforming baselines such as GR00T and  0 by large margins.</p>
<p>InternVLA-M1</p>
<p>We propose InternVLA-M1, a dual-system, end-to-end vision-language-action (VLA) framework.It integrates both a language head and an action head within a single model (Section 2.1).The language head establishes instruction-to-visual grounding through spatial pretraining and co-training, while the action head conditions on these learned spatial priors to generate embodiment-specific motor commands(Section 2.2).This joint design bridges abstract linguistic goals with grounded execution, enabling robust instruction following across diverse and complex scenes.</p>
<p>Model Architecture</p>
<p>Dual-System.InternVLA-M1 is a dual-system, end-to-end VLA framework pre-trained on large-scale spatial grounding data collected from diverse sources.InternVLA-M1 employs the Qwen2.</p>
<p>Sub-Task Planning</p>
<p>Collect snacks.</p>
<p>[point]</p>
<p>Give the box coordinates according to the instruction… Your answer should be formatted as a list of tuples …</p>
<p>Based on the task description, predict the trajectory that the end effector should take… robot demonstration data, enabling it to specialize these priors into embodiment-specific motor commands.This dual-supervision strategy establishes a cohesive link between high-level semantic perception and low-level motion control, which is essential for robust instruction following in both simulation and real-world settings.</p>
<p>Latent planning via spatial prompting.To connect the VLM Planner with the action expert, we adopt a lightweight querying transformer (8.7 MB) conditioned on the latent planning embeddings produced by the VLM Planner.The querying transformer stabilizes expert learning and inference by mapping variable-length input tokens into a fixed set of learnable query tokens.It is implemented as a -layer cross-attention module, where the query tokens selectively attend to  intermediate layers of the VLM (e.g.,  = 1 attends only to the final layer).</p>
<p>To explicitly activate the spatial perception capability learned during spatial grounding pre-training, we employ spatial prompting.For instance, in general object manipulation tasks, we append simple prompts such as "Figure out how to execute it, then locate the key object needed."after the task instruction.The extracted feature embeddings provide the planner with explicit spatial cues that facilitate more reliable grounding.Motivated by prior studies Bjorck et al. (2025); Driess et al. (2025); Zhou et al. (2025b) showing that direct gradient flow between action and VLM modules may distort multimodal knowledge, we introduce a gradient decay factor within the querying transformer.This attenuates the gradients propagated from the Action Expert back to the VLM (e.g., by a factor of 0.5), thereby preserving the Planner's semantic reasoning ability while still enabling effective joint optimization.</p>
<p>Training Recipe</p>
<p>To leverage spatial priors for stronger embodiment-specific control in instruction following, InternVLA-M1 adopts a spatially guided two-stage training pipeline:</p>
<p>Stage 1: Spatial grounding pre-training.As shown in Figure 2, the first stage optimizes only the VLM.The objective is not generic vision-language pre-training, but stronger spatial reasoning and planning ability essential for robotics.We combine internet-scale multimodal corpora with robotspecific datasets such as RefCOCO, RoboRef It Lu et al. (2023b), A0 Xu et al. (2025b), MolmoAct Lee et al. (2025), and Pixmo-Points Deitke et al. (2024).All robot datasets are reformatted into a unified QA-style structure covering bounding-box detection, trajectory prediction, affordance recognition, and chain-of-thought reasoning.Aligning them with web-scale data enables training under the same supervised fine-tuning framework as conventional VLMs.</p>
<p>Stage 2: Spatially guided action post-training.In this stage, both the VLM and Action Expert are jointly optimized on demonstration data, ensuring semantic understanding and motion generation remain tightly integrated.Two strategies are employed:</p>
<p>• Spatial prompting.Before predicting actions, we prepend a spatial cue to the task instruction to elicit structured reasoning about object relationships and task constraints.For example, the instruction "store all toys into the toy box" can be augmented with: "Identify all relevant toys and their spatial relationships to the container."Although the VLM does not explicitly output a response to this auxiliary cue, its inclusion improves spatial awareness and generalization in manipulation tasks.• Co-training with spatial grounding data.Training alternates between robot trajectory data and grounding data.For trajectory data, both the VLM backbone and the action Expert are optimized with an L2 loss between predicted and ground-truth noise.For spatial grounding data, only the VLM backbone is updated via next-token prediction.This co-training scheme reinforces spatial reasoning while supporting efficient end-to-end optimization.</p>
<p>Data</p>
<p>This section introduces the datasets used in InternVLA-M1, covering pre-training, mid-training, and post-training stages.For VLM pre-training, we construct large-scale spatial grounding datasets with point, box, and trajectory annotations to enhance spatial perception and vision-language alignment.Mid-training employs synthetic manipulation data to bridge pre-training knowledge and robotic execution.Post-training uses both simulated and real-world instruction-following data, including large-scale tabletop tasks and real-robot demonstrations for long-horizon manipulation.</p>
<p>Spatial Grounding Data for Pre-training</p>
<p>The multimodal training dataset for our model comprises over 3M data, categorized into four distinct types: General QA, Box QA, Trajectory QA, and Point QA, as shown in Figure 3. Notably, more than 2.3M of these data are dedicated to spatial reasoning datasets.These categories ensure robust multimodal understanding while supporting adaptation to embodied tasks in tabletop robotic scenarios.Below, we describe each category:</p>
<p>• General QA.</p>
<p>Synthetic Data For Action Post-Pre-training</p>
<p>To bridge the gap between VLM and VLA, we introduce a Post-Pre-Training phase, where large-scale simulated data is used to pre-train the VLA after VLM pre-training.This stage initializes the action head and facilitates the learning of action representations.Post-Pre-Training requires maintaining diversity both at the instruction and object levels.Consistent with the InternVLA-M1-Interface Data, we leverage GenManip as our data synthesis pipeline to construct a large-scale pick-and-place dataset, the InternData M1 dataset, which comprises 244K closed-loop samples.Specifically, we adopt the same object set and positional distributions as in InternVLA-M1-Interface Data, and process them through our scalable data pipeline.Each synthesized sample is rigorously validated to ensure correctness and consistency.To further enhance visual diversity, we introduce controlled randomization in lighting conditions and texture mappings.The day is late, help me light the lantern now.</p>
<p>{"response": "Sure, lighting the lantern now.","subtask": "Ignite the lantern <box> [[398, 150, 426, 240]] </box>"} Question: Answer:</p>
<p>random camera</p>
<p>Scalable Synthetic Data Engine for Instruction-Following</p>
<p>To support large-scale end-to-end data generation for VLM pre-training, we build a highly scalable, flexible, and fully automated simulation pipeline on top of GenManip Gao et al. (2025) and Isaac Sim Makoviychuk et al. (2021).</p>
<p>Automatic task synthesis for generalizable pick-and-place.We develop a scalable simulation pipeline (shown in Figure 4) that generates diverse manipulation trajectories from randomized object layouts and lighting conditions.By leveraging privileged simulation signals including object poses, object meshes, and robot arm state, the system rapidly generates scene layouts via a scene graph solver and computes candidate grasps based on object meshes Liang et al. (2019).Each candidate trajectory is then executed once in physics for closed-loop verification, after which a scene-graph validator checks whether the task goals are achieved.Only trajectories that both execute successfully and pass validation are accepted, ensuring that all collected data are physically feasible and task-complete.</p>
<p>Synthesis of VLM data and VLA data for spatial grounding.For higher efficiency, robot planning and rendering are fully decoupled in our framework.The planner records structured scene and trajectory data, including joint states, object positions, and action information, which are later replayed by the renderer under randomized lighting, materials, and viewpoints.To align the simulation with real world, we calibrate all cameras using ArUco markers, ensuring that their intrinsic and extrinsic parameters match those of real-world cameras, thus maintaining consistent viewpoint geometry.In addition to high-resolution images, the renderer produces rich intermediate outputs, such as object bounding boxes and 2D end-effector trajectories.These signals provide dense supervision for action learning and facilitate the creation of auxiliary datasets for tasks such as spatial grounding, affordance reasoning, and trajectory prediction.Our asset library includes 14K annotated objects, 211 tables, 1.6K textures, and 87 dome lights, offering data with high visual and physical diversity-critical for developing generalizable models.</p>
<p>Experiments</p>
<p>We conducted extensive experiments to evaluate the performance of InternVLA-M1 in both simulation and real-world settings.First, we assess the performance on public simulated benchmarks (Section 4.1).Next, we fully evaluate the instruction-following of InternVLA-M1 for generalizable pickand-place using Isaac-Sim (Section 4.2).Finally, we examine real-robot performance on long-horizon manipulation tasks to study instruction-following in real-world deployment (Section 4.2.2).</p>
<p>Experiments on Public Benchmarks</p>
<p>We use two established simulation suites:</p>
<p>• SimplerEnv is designed to probe robustness to visual appearance shifts.It includes both WidowX and Google Robot platforms, short-horizon atomic tasks, and controlled changes in lighting, color, surface texture, and camera pose.We report results on three task sets: Google Robot-VM (visual matching under viewpoint and lighting changes), Google Robot-VA (visual aggregation with varying textures and colors), and WidowX-VM (cross-robot generalization).• LIBERO is a language-conditioned manipulation suite built on a Franka arm with diverse scenes and expert demonstrations.We evaluate four task sets: LIBERO-Spatial (same objects, different spatial layouts), LIBERO-Object (fixed layout, different objects), LIBERO-Goal (fixed objects and layout, different goals), and LIBERO-Long (also known as LIBERO-10; longer tasks that span multiple objects, layouts, and operations).The results demonstrate that omitting spatial data and spatially guided prompting during training leads to rapid degradation of spatial grounding capabilities and slower convergence in manipulation tasks.In contrast, spatially guided action post-training accelerates convergence, substantially improves manipulation success rates, and enhances spatial grounding accuracy as shown in Figure 5.</p>
<p>To further analyze the relationship between the spatial grounding objective and the action manipulation objective, we compute the Projection-space Similarity (PSS) Raghu et al. (2017) using Singular Value Decomposition (SVD).As shown in Figure 5(c), vanilla co-training of action data with spatial data yields a PSS of only 0.25, indicating significant misalignment between the gradient subspaces.In contrast, our spatially guided training approach increases the PSS to 0.42, demonstrating substantially improved optimization consistency.This enhanced alignment correlates with better preservation of spatial perception capabilities and faster convergence in manipulation tasks.We conduct a comprehensive study of VLA training strategies and their effects across three distinct task categories: multi-modal understanding, spatial grounding, and robot manipulation performance, which is a type of generalist VLA.Specifically, we evaluate: As shown in Table 3, our InternVLA-M1 achieves superior robotic manipulation performance while simultaneously preserving stronger multimodal understanding and spatial grounding capabilities compared to vanilla fine-tuning from VLM to VLA (Vanilla VLA) and direct co-training with spatial grounding data (vanilla co-train).Result analysis.The primary experimental results on the LIBERO benchmark are presented in Table 4.</p>
<p>Compared to previous strong baselines, such as GR00T N1 and  0 , the InternVLA-M1 framework achieves notable improvements, particularly on the spatial and long-horizon tracks, with success rates of 98.0% and 92.6%, respectively.These results demonstrate the efficacy of our proposed method in managing complex, multi-step manipulation tasks.Specifically, for object placement, InternVLA-M1 attains a 99.0%SR, which highlights its robust object grounding capability.</p>
<p>Experiments on Instruction-Following in In-house Environment</p>
<p>Evaluation in Simulated Large-scale Pick-and-place</p>
<p>Existing benchmarks such as SimplerEnv and LIBERO are limited in scale, which restricts the comprehensive evaluation of instruction-following manipulation in diverse and cluttered settings.To more rigorously assess generalization capabilities, we conduct an experimental study on a large-scale simulation evaluation with enhanced object diversity and layout variation.</p>
<p>Experimental setups.We constructed 200 pick-and-place tasks based on Isaac-Sim Gao et al. (2025), where the manipulated objects in each task are mutually distinct.Including background objects, the benchmark covers over 3K items and containers in total.Each task was executed once through the data generation pipeline to ensure its executability.Furthermore, for each of the 200 tasks, we additionally collected 5 trajectories with identical object sets but randomized layouts, which were used for post-training.The observation space comprises two RGB images: one captured from a fixed third-person viewpoint and the other from a first-person camera mounted on the Franka end-effector.Both images are resized to 224 × 224 before being fed into the model.We fine-tune the model on each suite independently using 16 A100 GPUs, with a total batch size of 256 and an action chunk size of 16.Training is conducted for 20K steps.Both our model and all baseline models are trained using delta joint space control.</p>
<p>Result analysis.As shown in Figure 6, we evaluate InternVLA-M1 under four generalization settings:</p>
<p>In-distribution, Unseen Object, New Background, and Unseen Instruction.For each setting, we report two variants of the model: w/o mid-train, which is fine-tuned using only five trajectories per task, and w/ mid-train, which is additionally mid-trained on InternData M1 prior to fine-tuning.The results, summarized in Figure 7, show that across all settings, both variants outperform the baseline  0 , while InternVLA-M1 w/ mid-train consistently surpasses GR00T N1.5.Although InternVLA-M1 w/o mid-train exhibits slight variance in certain settings, the mid-trained variant achieves a consistent advantage, with an average gain of +6.2% over GR00T N1.5.</p>
<p>The performance on unseen objects highlights the benefit of simulation-enhanced visual generalization, enabling the model to handle novel instances beyond the training distribution.When evaluated under new backgrounds with randomized textures and layouts, both variants maintain strong performance, and the improvements from mid-training indicate increased robustness to scenelevel shifts.Furthermore, under paraphrased instructions involving attribute-level or commonsense rewrites, InternVLA-M1 w/ mid-train demonstrates reliable instruction grounding, reflecting strong language generalization beyond templated expressions.</p>
<p>New Background Unseen Instruction</p>
<p>Transfer the item with the red lid on the barrel.</p>
<p>Drop the green object into the middle of shallow metal bowl.</p>
<p>Move the bottle to the top of the first aid kit.</p>
<p>Move the blue bottle to the top of the wooden barrel.</p>
<p>Unseen Object</p>
<p>Move the flower to the top of the bowl.</p>
<p>Move the microphone to the top of the microwave oven.</p>
<p>In-distribution</p>
<p>Move the yellow bottle to the top of the board.</p>
<p>Move the flashlight to the top of the speaker.</p>
<p>Evaluation in Real-world Cluttered-scene Pick-and-Place</p>
<p>Experimental setup.To evaluate our model's instruction-following capability in real-world scenarios, we employ a Franka Research 3 robotic arm equipped with a Robotiq 2F-85 gripper.The setup includes two Intel RealSense D435 cameras for RGB visual input-one mounted on the end-effector and another positioned at a rear, third-person perspective.We assess the model across a variety of manipulation tasks, including short-range pick-and-place, long-horizon object sorting, drawer opening/closing, and sandwich assembly.For quantitative evaluation, we design a real-world objectsorting benchmark consisting of single-horizon pick-and-place tasks within a 60 × 90 cm tabletop workspace.The benchmark features 23 seen objects and 5 seen containers (listed in Figure 8).In each episode, three containers are fixed at designated positions, while diverse objects are scattered randomly among them.The model must follow natural language instructions to pick specific objects and place them into the correct containers.To support post-training, we collect 6 hours of teleoperated demonstrations using only objects and containers from the predefined "seen" set.We compare two variants of InternVLA-M1, w/o co-train and w/ co-train, against GR00T N1.Evaluation settings.To evaluate generalization, we further partition all available object and container assets into disjoint seen and unseen sets, as illustrated in Figure 8.Only the seen set is included in the training data, while both seen and unseen sets are evaluated during testing to measure the model's ability to generalize to novel objects.As shown in Figure 9, we evaluate instructionfollowing capabilities of various models on real-world pick-and-place tasks under the below settings:</p>
<p>In-Distribution, Unseen Objects, Unseen Object Position, Unseen Object Orientation, and Unseen Instructions.We report success rate, defined as the fraction of trials in which the specified object is placed into the designated container.Higher SR indicates better performance.For each model, we conducted a total of 300 rollout evaluations.Each trial corresponds to one or more testing settings, and we ensured that each individual setting was evaluated at least 50 times.To ensure fair comparisons across models, we fixed the positions of the objects and containers for each task during testing.</p>
<p>Result analysis.</p>
<p>As shown in Figure 10, both variants of InternVLA-M1 demonstrate superior performance under the in-distribution setting, consistently outperforming GR00T N1.5 and  0 when evaluated on objects and containers seen during training.This indicates strong instruction-following capabilities within familiar contexts.Beyond this, the inclusion of Interndata-M1 during co-training significantly enhances the model's visual generalization, enabling improved performance on novel objects not encountered during training.This suggests that synthetic data serves as an effective complement to limited real-world demonstrations.Additionally, because real-world data collection cannot exhaustively cover the spatial workspace, simulation data enriches the distribution of object positions and orientations.This leads to substantially better generalization to unseen configurations in terms of both object placement and pose.Finally, InternVLA-M1 maintains robust performance when given novel instructions, highlighting its ability to generalize across diverse linguistic expressions</p>
<p>Evaluation in Long-horizon and Reasoning Manipulation</p>
<p>A key strength of our dual-system framework is its ability to leverage a high-level planner (System 2) to decompose long-horizon, reasoning-heavy tasks into a sequence of atomic actions, which are then robustly executed by a low-level action model (System 1).To evaluate this capability, we design a series of tasks that require not only multi-step planning but also the ability to reason about object attributes, monitor progress, and adapt to changes.As illustrated in Figure 11, these include:</p>
<p>• Desktop Sorting.The Franka robot is tasked with sorting objects into containers based on highlevel semantic categories, aiming to ensure that all items on the desktop are eventually placed into the correct containers.Both objects and containers are scattered within a 60×90 cm region in front of the robot base.The setup includes five seen containers and five object categories: fruits, toys, vegetables, bottles, and snacks.Each evaluation instance involves sorting objects from one to three categories into their respective containers.Each trial consists of three pick-and-place actions, and we report success rates consistent with the metric used for pick-and-place under clustered environments.• Sorting Items into Drawers.The Franka robot is required to (i) open a designated drawer (either lower or upper), (ii) place the target objects into it, and (iii) close the drawer.This task demands precise temporal reasoning and articulated manipulation.The objects are placed within a 35×35 cm area located to the front-right of the robot base.We report stepwise execution success, where a step is considered valid only if all preceding steps have succeeded.• Making Sandwiches.The Franka robot is instructed to assemble sandwiches following a predefined meal recipe.Ingredients and plates are placed within a 50×70 cm region in front of the robot base.We define five types of sandwich recipes as the seen set: [ bread-lettuce-bread ], [ bread-lettucemeat-bread ], [ bread-meat-lettuce-meat-bread ], [ bread-meat-meat-bread ], and [ bread-meatbread ].We report success rates on both the seen set and an unseen set involving real-time environment interaction, using the same success definition as in the drawer sorting task.• Math Calculation.The Franka robot is prompted to solve a math problem and press the color-coded button (red, yellow, or blue) that corresponds to the correct answer based on arithmetic reasoning.The buttons are randomly placed within a 40×40 cm area in front of the robot base.• Goods Purchase.The ARX LIFT2 dual-arm robot is tasked with identifying and placing into a basket the object bearing the correct price tag, given a numerical cue ranging from 1 to 9. We report the success rate of correctly placing the item corresponding to the queried price into the basket.</p>
<p>Experimental setup.To support fine-grained training for these long-horizon tasks, we collect a total of 22 hours of high-quality long-horizon and reasoning teleoperated demonstrations, amounting to approximately 500 demonstrations per task.Each collected trajectory is segmented into subtasks and annotated with corresponding atomic actions.For example, a "make a classic sandwich" task is decomposed into four subtasks: (1) "Put a piece of bun on the plate."→ (2) "Put a piece of meat on the plate."→ (3) "Put a piece of lettuce on the plate."→ (4) "Put a piece of bun on the plate."Put a piece of lettuce on the plate.</p>
<p>Put a piece of bun on the plate.</p>
<p>Stop.Put a piece of bun on the plate.</p>
<p>Long-horizon sorting: Sort snacks, toys, and fruits into respective bins.(6-14 steps)</p>
<p>Put all the snacks into the white basket.</p>
<p>Put all the fruits into the wooden basket.</p>
<p>Start.</p>
<p>Stop.</p>
<p>Put all the toys into the brown basket.</p>
<p>Put all the toys into the brown basket.Each sub-instruction is paired with a specific segment of the demonstration.To enable subtask-level transition, we introduce zero-action vectors padding after each subtask segment.This allows the model to stop upon subtask completion and then be prompted to predict the transition to the next subtask.In addition, to improve temporal consistency and ensure smooth inference, we remove frames in which the robot arm exhibits clear pauses or idle behavior.In contrast to prior VLA models that depend on an additional VLM to serve as a task planner for long-horizon or reasoning-intensive tasks, our unified model architecture is trained jointly on multimodal inputs encompassing task decomposition, subtask identification, numerical reasoning, and action supervision.This joint training paradigm enables a single model to seamlessly integrate task planning, reasoning, and action prediction in an   Evaluation settings.We evaluate model performance under three distinct settings: In-distribution, Physical Interference, and Task Replanning:</p>
<p>• Physical interference.External disturbances are introduced during task execution.For example, during the sorting items into drawers task, the drawer is manually closed after the robot opens it, or the target object is displaced during grasping.This evaluates the model's ability to perceive environmental changes and adapt accordingly.• Task replanning.New instructions are issued mid-execution.For instance, after placing an object in the drawer but before closing it, the robot is told: "Also put the cow toy into the top drawer."This tests the model's ability to incorporate new subgoals and dynamically adjust its plan.</p>
<p>Results analysis.</p>
<p>As shown in Figure 12, across long-horizon tasks, InternVLA-M1 consistently outperforms the baselines, enabled by its unified subtask planning mechanism.In the in-distribution setting, it achieves more reliable execution than GR00T N1.5 and  0 , showing stronger grounding of high-level goals into actionable steps.Under physical interference, the model demonstrates robust adaptability: for example, in desktop sorting when containers are unexpectedly moved, InternVLA-M1 can track the new container locations and complete the placement.Moreover, when task replanning is required, such as when additional instructions are introduced during execution, InternVLA-M1 is able to revise its subtask sequence on the fly and continue with correct actions.This adaptability leads to minimal performance degradation under stress conditions, while the baselines exhibit much larger declines, underscoring the model's resilience to dynamic environments and shifting instructions.2025) unify perception and planning by outputting not only plans but also affordance predictions as bounding boxes.For tasks requiring higher precision, specialized models such as RoboRefer Zhou et al. (2025a) employ dedicated architectures and reinforcement learning to predict exact 3D coordinates from complex spatial language.In contrast, our method provides a unified latent modeling framework that integrates spatial guidance into downstream action training, enabling end-to-end optimization with direct feedback from real-world deployment.</p>
<p>Related work</p>
<p>Embodied reasoning and planning in VLA.Chain-of-Thought prompting has proven effective for improving reasoning in large language models Wei et al. (2022), and its success has inspired extensions to embodied AI.In Vision-Language-Action (VLA) models, generating intermediate reasoning steps before acting enables agents to handle complex, long-horizon tasks.Early approaches emphasized linguistic reasoning: ECOT Zawalski et al. (2024) 2025) exemplify this forward-predictive approach to decision-making.Our model adopts a typical dual-system approach, building upon the VLA with unified architectures, then introducing additional planning design, thereby achieving better adaptability to real-world environments.</p>
<p>Discussion and conclusion</p>
<p>In this work, we presented InternVLA-M1, a unified vision-language-action framework that leverages spatial grounding priors to bridge high-level multimodal reasoning with low-level robotic execution.By combining large-scale multimodal pre-training with spatially guided post-training, our model effectively transfers spatially grounded understanding into embodied control, achieving strong generalization to unseen objects, instructions, and environments.Extensive evaluations across simulation and real-world settings demonstrate that InternVLA-M1 surpasses existing VLA models and specialized systems in instruction following, long-horizon manipulation, and multimodal grounding, highlighting spatial reasoning as a unifying substrate for scalable and reliable generalist robots.</p>
<p>Figure 2 .
2
Figure 2. Overview of InternVLA-M1.InternVLA-M1 adopts a spatially guided two-stage training pipeline.Stage 1 (spatial grounding pre-training): the VLM is trained on large-scale multisource multimodal spatial grounding data to learn embodiment-agnostic spatial priors.Stage 2 (spatially guided action post-training): the VLM Planner, functioning as a slow but reliable System 2 reasoner, generates latent planning tokens via spatial prompting as the condition to the action expert (instantiated as a DiT Actor) to execute as a fast System 1 controller.</p>
<p>Figure 3 .
3
Figure 3. Overview of the pre-training data for the vision-language model.The data comprises two main parts: general VQA data to maintain the model's general multimodal capabilities, and spatial VQA data focusing on robotic-related grounding and spatial perception in a VQA format.</p>
<p>": "Sure thing.","subtask": "Pick up the white bottle <box> [8, 176, 106, 294]] </box> and open it."}Simulation for generalizable pick-place: Put <Obj1> to the <Relation> of <Obj2></p>
<p>Figure 4 .
4
Figure 4. Simulation data synthesis pipeline.The pipeline generates diverse robotic manipulation data from a large asset library, converts intermediate representations into VQA data, and separates physics from rendering to reduce wasted failures and improve efficiency.</p>
<p>Figure 5 .
5
Figure 5. Ablation study on the effect of auxiliary spatial prompting for co-training robot manipulation with spatial grounding.From left to right: (a) spatial grounding performance (IoU@0.5 on RefCOCOg); (b) manipulation performance (SimplerEnv-WidowX SR); (c) shows the gradient similarity of the spatial grounding and manipulation objectives.</p>
<p>•</p>
<p>Multi-modal understanding: MME Zhang et al. (2021), MMVet Yu et al. (2023), TextVQA Singh et al. (2019), POPE Li et al. (2023), COCO Caption Chen et al. (2015) • Spatial grounding: RefCOCO-g Mao et al. (2016) (Box IoU0.5),Refit-testB Lu et al. (2023a) (Box IoU0.5),Where2Place Yuan et al. (2024) (evaluated by accuracy of predicted points with respect to ground-truth free space), and A0-maniskill Gu et al. (2023b) (evaluated using trajectory MAE, measuring mean absolute error between predicted and reference waypoints).• Robot manipulation: Google-Robot Visual Matching (VM), Variant Aggregations (VA), and WindowX Visual Matching (VM).</p>
<p>Figure 6 .Figure 7 .
67
Figure 6.Evaluation settings for generalizable pick-and-place in large-scale simulation.</p>
<p>Figure 8 .
8
Figure 8. Overview of objects and containers used in instruction-following pick-and-place.</p>
<p>Figure 9 .
9
Figure 9. Evaluation settings showcase for real-world instruction-following manipulations.</p>
<p>Figure 10 .
10
Figure 10.Result comparison in real-world instruction-following pick-and-place.</p>
<p>customized tasks: Make a classic sandwich and add another meat.(6-12 steps)</p>
<p>-</p>
<p>9 x 7 =• -72 • -63 • -54 Press the blue botton.Task replanning: Sort the brush into a closed drawer, then sort another hippo toy on sudden human request.(2-8 steps)Collect the brush to the upper drawer.Open the upper drawer.Close the upper drawer.Clear the hippo toy to the upper drawer., also clear the hippo toy into the drawer.</p>
<p>Figure 11 .
11
Figure 11.Showcase for long-horizon instruction-following manipulation.</p>
<p>Figure 12 .
12
Figure 12.Result comparison in real-world long-horizon task planning for manipulation.</p>
<p>elicits explicit text-based plans and sub-tasks to enhance performance and interpretability; RT-HBelkhale et al. (2024) introduces a fine-grained "action language" for hierarchical policies and human intervention; InstructVLAYang et al. (2025b) jointly optimizes reasoning and action through VLA-IT, improving generalization; OneTwoVLALin et al. (2025) adaptively alternates between "thinking" and execution;RAD Clark et al. (2025)  leverages action-free human videos to derive reasoning guides; and  0.5Intelligence et al. (2025) trains on heterogeneous data before fine-tuning for subtask prediction.Later work has also explored visual and spatial modalities, such as graph-based representations for spatial reasoningHuang et al. (2025a).Despite their differences, these approaches all generate intermediate steps such as textual, visual, or spatial representations during inference.While effective, this adds computational overhead.In contrast, we propose a post-training phase that directly unlocks the VLM's intrinsic reasoning capacity, removing the need for explicit generative reasoning.Generalistrobot policy.Recent research in general-purpose robotics has seen the emergence of several mainstream technical paradigms.Monolithic VLA models utilize a single end-to-end network to directly map multimodal inputs to tokenized low-level actions, as demonstrated by systems Brohan et al. (2023); Kim et al. (2024); Lee et al. (2025); Yang et al. (2025a).In contrast, unified architectures decouple high-level cognition from low-level action, allowing for greater modularity and interpretability.This category has seen extensive exploration Black et al. (2024); Li et al. (2025a, 2024c) leveraging specialized generative models for action synthesis.Other notable approaches in this vein Cheang et al. (2025); Intelligence et al. (2025); Shukor et al. (2025); Song et al. (2025); Yang et al. (2025b);Zhou et al. (2025b), which uses an LLM to break down high-level language commands into intermediate action plans.A third paradigm is based on world models, which learn a predictive model of the environment's dynamics to enable planning and control.These models allow for simulating future outcomes, often facilitating planning via search in a learned latent space or by conditioning a separate policy.While powerful, this approach can be computationally intensive.Representative works Bjorck et al. (2025); Bu et al. (2025b); Cen et al. (2025); Li et al. (2025b); Liao et al. (2025); Lv et al. (2025); Tian et al. (2024); Wang et al. (2025); Ye et al. (</p>
<p>Spatially Guided Action Post-training VLM -Planner Stage 1 | Spatial Grounding Pre-training Multimodal understanding robot observation</p>
<p>During inference, the system runs on a single RTX 4090 GPU with around 12 GB of memory usage.With FlashAttention, the VLM component achieves inference speeds of approximately 10 FPS.Action execution can be further accelerated via chunking and KV caching.
Spatial Grounding(box / point / trace)Dual-Supervision. The dual-system architecture supports both multimodal supervision and actionsupervision during training. In each training step, batches from both data types are jointly pro-cessed, and the model computes losses from the two supervision signals. The resulting gradients areaggregated and applied in a single optimization update, ensuring that perception and control are
Chi et al. (2023))i et al. (2025a)as the multimodal encoder for System 2, which is to capture spatial priors.It adopts the diffusion policyChi et al. (2023)(86 M) as the Action Expert (System 1, the fast executor), which effectively models embodiment-specific control.This expert is built on the DINOv2 visual encoder Oquab et al. (2023) (21 M) and a lightweight state encoder (0.4 M), forming a compact vision-action model.In total, InternVLA-M1 comprises approximately 4.1B parameters.co-adapted rather than learned in isolation.Specifically, the VLM planner is aligned with a broad range of spatial grounding data, both real and synthetic, covering tasks such as object detection, affordance recognition, and visual trajectory planning.In parallel, the Action Expert is trained on Stage 2 |</p>
<p>multi-modal web data [box] [trace] VLM real &amp; sim robot data Organize the table. Noisy Actions Actions DiT -Actor Conditioned State (opt)</p>
<p>Your task is to {instruction}. Figure</p>
<p>out how to execute it, then locate the key object needed.</p>
<p>Table 2 .
2
Li et al. (2024c)4)-of-the-art open VLA systems, including  0Black et al. (2024),GR00T Bjorck et al. (2025), OpenVLAKim et al. (2024),CogACT Li et al. (2024c), and etc.We also include a Vanilla VLA built on QwenVL-2.5-3B-InstructwithaDiTactionhead.When available, we use official reported numbers; otherwise, we reimplement and mark such entries with * .We keep training data, observation spaces, and action type aligned with the most popular setupsLi et al. (2024c)to ensure a fair comparison.As described in Section 2.2, we post-train InternVLA-M1 on a subset of Open-X Embodiment (OXE) (including fractal_rt_1 and bridge_v1), with co-training on spatial grounding data (Section 3.1).The VLM takes the primary observation image, task instruction, and an auxiliary spatial prompt as input, while the action expert predicts actions with an action chunk size of 16.For multimodal data, the model follows an SFT-style question-answering format.Training is performed on 16 NVIDIA A100 GPUs for 50k steps (around 2.5 epochs), with total batch sizes of 256 for robot data and 64 for multimodal data, optimized with a summed loss over both data types.All evaluations are conducted within SimplerEnv using its official evaluation protocol.Result comparisons of robotic manipulation on SimplerEnv (WidowX) benchmark.The underlined scores indicate the best results excluding InternVLA-M1.The main experimental results are presented in Table1 and Table 2. Compared with prior state-of-the-art models, it attains a 5.9% gain in Google Robot Visual Matching, a 5.3% gain in Visual Aggregation, and a 9.8% gain on the WidowX benchmark.These results highlight the strong competitiveness of InternVLA-M1 within the community.Compared to the Vanilla VLA based on Qwen2.5-VL-3B-Instruct,InternVLA-M1 achieves substantial improvements: a 14.6% increase in Google Robot Visual Matching and a 12.4% increase in Visual Aggregation, along with a 17.0% improvement on the WidowX benchmark.These results demonstrate the effectiveness of our spatially guided pre-training and action post-training strategies.
Google RobotModelsCo-TrainPick Coke CanMove NearOpen/Close DrawerOpen Top Drawer and Place AppleAvgRT-1 Brohan et al. (2022)✗85.744.273.06.552.4RT-1-X Collaboration et al. (2023)✗56.731.759.721.342.4RT-2-X Brohan et al. (2023)✓78.777.925.03.746.3OpenVLA Kim et al. (2024)✗18.056.363.00.034.3VisualCogACT Li et al. (2024c)✗91.385.071.850.974.8MatchingSpatialVLA Qu et al. (2025)✗86.077.957.4-75.1𝜋 0 Black et al. (2024)✗72.765.338.3-58.8𝜋 0 -FAST Pertsch et al. (2025)✗75.367.542.9-61.9GR00T N1.5  *  Bjorck et al. (2025)✗51.754.027.87.435.2Magma Yang et al. (2025a)✓83.765.456.06.452.9Vanilla VLA✗90.069.852.552.266.1InternVLA-M1✓95.390.075.562.080.7Δ+5.3+20.2+23.0+9.8+14.6RT-1 Brohan et al. (2022)✗89.850.032.32.643.7RT-1-X Collaboration et al. (2023)✗49.032.329.410.130.2RT-2-X Brohan et al. (2023)✓82.379.235.320.654.4OpenVLA Kim et al. (2024)✗60.867.728.80.039.3VariantCogACT Li et al. (2024c)✗89.680.828.346.661.3AggregationSpatialVLA Qu et al. (2025)✗88.082.541.8-70.7𝜋 0 Black et al. (2024)✗75.263.725.6-54.8𝜋 0 -FAST Pertsch et al. (2025)✗77.668.231.3-59.0GR00T N1.5 Bjorck et al. (2025)✗69.368.735.84.044.5Magma Yang et al. (2025a)✓68.865.753.418.551.6Vanilla VLA✗92.380.350.131.463.5InternVLA-M1✓86.182.072.064.076.0Δ-6.2+1.7+21.9+32.6+12.5
Table 1.Result comparisons of robotic manipulation on SimplerEnv (Google-Robot) benchmark.The underlined scores indicate the best results excluding InternVLA-M1.Numbers are officially reported; otherwise, we reimplement and mark such entries with * .Baselines.</p>
<p>Table 3 .
3
Kim et al. (2025) VLA training strategies across multi-modal understanding, spatial grounding, and robotic manipulation tasks.FollowingKim et al. (2025), we filter out failed demonstrations and pause frames.During training, the policy takes as input both wrist-mounted and third-person camera views.We fine-tune the model on each suite independently using 8 A100 GPUs with a batch size of 128 and an action chunk size of 8. Training runs for roughly 30K steps, lasting about 20 hours.Each suite is evaluated with 500 trials.
Multi-modal UnderstandingSpatial GroundingRobotic ManipulationModelsMME MMVet TextVQA POPECOCO Caption BLEU/ROUGERefCOCO-g Box IoU 0.5 ↑Refit-testB Box IoU 0.5 ↑Where2place Point Acc↑A0-maniskill Traj. MAE ↓Google Robot VM/VAWindowX VMVanilla VLA---------66.1/63.554.7Vanilla co-train 110619.220.578.010.4/15.147.166.721.46.470.2/66.561.1InternVLA-M1141123.328.686.213.0/13.471.274.325.55.180.7/76.071.7</p>
<p>Table 4 .
4
Result comparisons of robotic manipulation on LIBERO (Franka) benchmark.</p>
<p>Table 5, despite GPT-5's strong reasoning capability, additional post-training of the unified model notably improves performance on long-horizon and reasoning-intensive tasks, underscoring the necessity of post-training for effective high-level task planning.
Long-horizon tasksReasoning tasksModelsSort intoMakeDesktopMathGoodsDrawersSandwichesSortingcalculationPurchaseGemini-2.5 Pro5762835361GPT-57567627982GPT-4o3757353941Qwen2.5-VL-72B3171343329Qwen2.5-VL-3B3049524138Ours-3B9091919392</p>
<p>Table 5 .
5
Task scheduling performance of VLM planner in long-horizon and reasoning scenarios.</p>
<p>Luo et al. (2025)5b)24)m.A key challenge in embodied AI is bridging high-level instructions with low-level actions, often addressed by generating intermediate representations (IRs) that range from formal symbolic structures to learned embeddingsXie et al. (2019).Inspired by Chain-of-Thought (CoT) reasoning, many approaches train vision-language-action (VLA) models to generate explicit textual plans before acting, which improves both interpretability and performance on complex tasksZawalski et al. (2024).Beyond textual plans, research has explored more structured or physically grounded IRs.Historically, systems relied on direct perceptual outputs, such as bounding boxes from object detectors for manipulationGriffin (2023), specific 3D points for grasp planning from point clouds TenPas and Platt (2017), or dense correspondence fields derived from self-supervised features learned for control, such as DINO featuresLaskin et al. (2020);Nair et al. (2022).Some methods construct persistent 3D scene graphs as comprehensive world models that LLMs can query to ground long-horizon plansRana et al. (2023).Others emphasize action-centric IRs, for example by conditioning policies on visual affordances that specify the robot's end-effector pose at key moments in a taskNasiriany et al. (2024).A growing trend involves generating explicit spatial localizers directly consumable by robot controllersGu et al. (2023a);Huang et al. (2025b);Li et al. (2025c).Large-scale foundation modelsLuo et al. (2025);Team et al. (</p>
<p>A. Author contributionsAll contributors are listed in alphabetical order by their last names.A.1. Core ContributorsYilun Chen, Ning Gao, Jiangmiao Pang, Bolun Wang, Fangjing Wang, Jinhui Ye, Junqiu Yu, Jinyu Zhang, Yangkun ZhuA.2. ContributorsXinyi Chen, Yanwei Fu, Jiaya Jia, Weiyang Jin, Hao Li, Yao Mu, Yu Qiao,Yang Tian, Bin Wang, Hanqing Wang, Tai Wang, Ziqin Wang, Xueyuan Wei, Chao Wu, Shuai Yang, Jia Zeng, Jingjing Zhang, Shi Zhang, Feng Zheng, Bowen Zhou
. F Ai Helix, 2024</p>
<p>S Bai, K Chen, X Liu, J Wang, W Ge, S Song, K Dang, P Wang, S Wang, J Tang, H Zhong, Y Zhu, M Yang, Z Li, J Wan, P Wang, W Ding, Z Fu, Y Xu, J Ye, X Zhang, T Xie, Z Cheng, H Zhang, Z Yang, H Xu, J Lin, . , arXiv:2502.13923Qwen2.5-vl technical report. 2025aarXiv preprint</p>
<p>S Bai, K Chen, X Liu, J Wang, W Ge, S Song, K Dang, P Wang, S Wang, J Tang, arXiv:2502.139235-vl technical report. 2025barXiv preprint</p>
<p>S Belkhale, T Ding, T Xiao, P Sermanet, Q Vuong, J Tompson, Y Chebotar, D Dwibedi, D Sadigh, arXiv:2403.01823Rt-h: Action hierarchies using language. 2024arXiv preprint</p>
<p>J Bjorck, F Castañeda, N Cherniadev, X Da, R Ding, L Fan, Y Fang, D Fox, F Hu, S Huang, arXiv:2503.14734Gr00t n1: An open foundation model for generalist humanoid robots. 2025arXiv preprint</p>
<p>\𝑝𝑖 0 : A vision-language-action flow model for general robot control. K Black, N Brown, D Driess, A Esmail, M Equi, C Finn, N Fusai, L Groom, K Hausman, B Ichter, arXiv:2410.241642024arXiv preprint</p>
<p>Rt-1: Robotics transformer for real-world control at scale. A Brohan, N Brown, J Carbajal, Y Chebotar, J Dabis, C Finn, K Gopalakrishnan, K Hausman, A Herzog, J Hsu, arXiv:2212.068172022arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. A Brohan, N Brown, J Carbajal, Y Chebotar, X Chen, K Choromanski, T Ding, D Driess, A Dubey, C Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Agibot world colosseo: A large-scale manipulation platform for scalable and intelligent embodied systems. Q Bu, J Cai, L Chen, X Cui, Y Ding, S Feng, S Gao, X He, X Hu, X Huang, arXiv:2503.066692025aarXiv preprint</p>
<p>Q Bu, Y Yang, J Cai, S Gao, G Ren, M Yao, P Luo, H Li, arXiv:2505.06111Univla: Learning to act anywhere with task-centric latent actions. 2025barXiv preprint</p>
<p>Robobrain 2.0 technical report. M Cao, B R Team, arXiv:2507.02029Beijing Academy of Artificial Intelligence. 2025arXiv preprint</p>
<p>J Cen, C Yu, H Yuan, Y Jiang, S Huang, J Guo, X Li, Y Song, H Luo, F Wang, arXiv:2506.21539Towards autoregressive action world model. 2025arXiv preprint</p>
<p>C Cheang, S Chen, Z Cui, Y Hu, L Huang, T Kong, H Li, Y Li, Y Liu, X Ma, arXiv:2507.15493Gr-3 technical report. 2025arXiv preprint</p>
<p>Microsoft coco captions: Data collection and evaluation server. X Chen, H Fang, T.-Y Lin, 2015</p>
<p>Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. Z Chen, J Wu, W Wang, W Su, G Chen, S Xing, M Zhong, Q Zhang, X Zhu, L Lu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Diffusion policy: Visuomotor policy learning via action diffusion. C Chi, RSSS Feng, RSSY Du, RSSZ Xu, RSSE Cousineau, RSSB Burchfiel, RSSS Song, RSSProceedings of Robotics: Science and Systems. Robotics: Science and Systems2023</p>
<p>Action-free reasoning for policy generalization. J Clark, S Mirchandani, D Sadigh, S Belkhale, arXiv:2502.037292025arXiv preprint</p>
<p>O X -E. Collaboration, A O'neill, A Rehman, A Gupta, A Maddukuri, A Gupta, A Padalkar, A Lee, A Pooley, A Gupta, A Mandlekar, A Jain, A Tung, A Bewley, A Herzog, A Irpan, A Khazatsky, A Rai, A Gupta, A Wang, A Kolobov, A Singh, A Garg, A Kembhavi, A Xie, A Brohan, A Raffin, A Sharma, A Yavary, A Jain, A Balakrishna, A Wahid, B Burgess-Limerick, B Kim, B Schölkopf, B Wulfe, B Ichter, C Lu, C Xu, C Le, C Finn, C Wang, C Xu, C Chi, C Huang, C Chan, C Agia, C Pan, C Fu, C Devin, D Xu, D Morton, D Driess, D Chen, D Pathak, D Shah, D Büchler, D Jayaraman, D Kalashnikov, D Sadigh, E Johns, E Foster, F Liu, F Ceola, F Xia, F Zhao, F V Frujeri, F Stulp, G Zhou, G S Sukhatme, G Salhotra, G Yan, G Feng, G Schiavi, G Berseth, G Kahn, G Yang, G Wang, H Su, H.-S Fang, H Shi, H Bao, H B Amor, H I Christensen, H Furuta, H Bharadhwaj, H Walke, H Fang, H Ha, I Mordatch, I Radosavovic, I Leal, J Liang, J Abou-Chakra, J Kim, J Drake, J Peters, J Schneider, J Hsu, J Vakil, J Bohg, J Bingham, J Wu, J Gao, J Hu, J Wu, J Wu, J Sun, J Luo, J Gu, J Tan, J Oh, J Wu, J Lu, J Yang, J Malik, J Silvério, J Hejna, J Booher, J Tompson, J Yang, J Salvador, J J Lim, J Han, K Wang, K Rao, K Pertsch, K Hausman, K Go, K Gopalakrishnan, K Goldberg, K Byrne, K Oslund, K Kawaharazuka, K Black, K Lin, K Zhang, K Ehsani, K Lekkala, K Ellis, K Rana, K Srinivasan, K Fang, K P Singh, K.-H Zeng, K Hatch, K Hsu, L Itti, L Y Chen, L Pinto, L Fei-Fei, L Tan, L J Fan, L Ott, L Lee, L Weihs, M Chen, M Lepert, M Memmel, M Tomizuka, M Itkina, M G Castro, M Spero, M Du, M Ahn, M C Yip, M Zhang, M Ding, M Heo, M K Srirama, M Sharma, M J Kim, N Kanazawa, N Hansen, N Heess, N J Joshi, N Suenderhauf, N Liu, N D Palo, N M M Shafiullah, O Mees, O Kroemer, O Bastani, P R Sanketi, P T Miller, P Yin, P Wohlhart, P Xu, P D Fagan, P Mitrano, P Sermanet, P Abbeel, P Sundaresan, Q Chen, Q Vuong, R Rafailov, R Tian, R Doshi, R Mart'in-Mart'in, R Baijal, R Scalise, R Hendrix, R Lin, R Qian, R Zhang, R Mendonca, R Shah, R Hoque, R Julian, S Bustamante, S Kirmani, S Levine, S Lin, S Moore, S Bahl, S Dass, S Sonawani, S Tulsiani, S Song, S Xu, S Haldar, S Karamcheti, S Adebola, S Guist, S Nasiriany, S Schaal, S Welker, S Tian, S Ramamoorthy, S Dasari, S Belkhale, S Park, S Nair, S Mirchandani, T Osa, T Gupta, T Harada, T Matsushima, T Xiao, T Kollar, T Yu, T Ding, T Davchev, T Z Zhao, T Armstrong, T Darrell, T Chung, V Jain, V Kumar, V Vanhoucke, W Zhan, W Zhou, W Burgard, X Chen, X Chen, X Wang, X Zhu, X Geng, X Liu, X Liangwei, X Li, Y Pang, Y Lu, Y J Ma, Y Kim, Y Chebotar, Y Zhou, Y Zhu, Y Wu, Y Xu, Y Wang, Y Bisk, Y Dou, Y Cho, Y Lee, Y Cui, Y Cao, Y.-H Wu, Y Tang, Y Zhu, Y Zhang, Y Jiang, Y Li, Y Li, Y Iwasawa, Y Matsuo, Z Ma, Z Xu, Robotic learning datasets and RT-X models. 2023</p>
<p>M Deitke, C Clark, S Lee, R Tripathi, Y Yang, J S Park, M Salehi, N Muennighoff, K Lo, L Soldaini, J Lu, T Anderson, E Bransom, K Ehsani, H Ngo, Y Chen, A Patel, M Yatskar, C Callison-Burch, A Head, R Hendrix, F Bastani, E Vanderbilt, N Lambert, Y Chou, A Chheda, J Sparks, S Skjonsberg, M Schmitz, A Sarnat, B Bischoff, P Walsh, C Newell, P Wolters, T Gupta, K.-H Zeng, J Borchardt, D Groeneveld, J Dumas, C Nam, S Lebrecht, C Wittlif, C Schoenick, O Michel, R Krishna, L Weihs, N A Smith, H Hajishirzi, R Girshick, A Farhadi, A Kembhavi, arXiv:2409.17146Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. 2024arXiv preprint</p>
<p>Knowledge insulating vision-language-action models: Train fast, run fast, generalize better. D Driess, J T Springenberg, B Ichter, L Yu, A Li-Bell, K Pertsch, A Z Ren, H Walke, Q Vuong, L X Shi, arXiv:2505.237052025arXiv preprint</p>
<p>Anygrasp: Robust and efficient grasp perception in spatial and temporal domains. H.-S Fang, C Wang, H Fang, M Gou, J Liu, H Yan, W Liu, Y Xie, C Lu, IEEE Transactions on Robotics. 2023</p>
<p>Genmanip: Llm-driven simulation for generalizable instruction-following manipulation. N Gao, Y Chen, S Yang, X Chen, Y Tian, H Li, H Huang, H Wang, T Wang, J Pang, CVPR. 2025</p>
<p>Mobile robot manipulation using pure object detection. B Griffin, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision2023</p>
<p>Rt-trajectory: Robotic task generalization via hindsight trajectory sketches. J Gu, S Kirmani, P Wohlhart, Y Lu, M G Arenas, K Rao, W Yu, C Fu, K Gopalakrishnan, Z Xu, arXiv:2311.019772023aarXiv preprint</p>
<p>J Gu, F Xiang, X Li, Z Ling, X Liu, T Mu, Y Tang, S Tao, X Wei, Y Yao, arXiv:2302.04659A unified benchmark for generalizable manipulation skills. 2023barXiv preprint</p>
<p>H Huang, F Lin, Y Hu, S Wang, Y Gao, Copa, arXiv:2403.08248General robotic manipulation through spatial constraints of parts with foundation models. 2024aarXiv preprint</p>
<p>Graphcot-vla: A 3d spatial-aware reasoning vision-language-action model for robotic manipulation with ambiguous instructions. H Huang, M Cen, K Tan, X Quan, G Huang, H Zhang, arXiv:2508.076502025aarXiv preprint</p>
<p>Roboground: Robotic manipulation with grounded vision-language priors. H Huang, X Chen, Y Chen, H Li, X Han, Z Wang, T Wang, J Pang, Z Zhao, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference2025b</p>
<p>W Huang, C Wang, R Zhang, Y Li, J Wu, L Fei-Fei, arXiv:2307.05973Voxposer: Composable 3d value maps for robotic manipulation with language models. 2023arXiv preprint</p>
<p>W Huang, C Wang, Y Li, R Zhang, L Fei-Fei, arXiv:2409.01652Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. 2024barXiv preprint</p>
<p>𝑝𝑖 0.5 : a vision-language-action model with open-world generalization. P Intelligence, K Black, N Brown, J Darpinian, K Dhabalia, D Driess, A Esmail, M Equi, C Finn, N Fusai, arXiv:2504.160542025arXiv preprint</p>
<p>A Khazatsky, K Pertsch, S Nair, A Balakrishna, S Dasari, S Karamcheti, S Nasiriany, M K Srirama, L Y Chen, K Ellis, arXiv:2403.12945A large-scale in-the-wild robot manipulation dataset. 2024arXiv preprint</p>
<p>M J Kim, K Pertsch, S Karamcheti, T Xiao, A Balakrishna, S Nair, R Rafailov, E Foster, G Lam, P Sanketi, arXiv:2406.09246An open-source vision-language-action model. 2024arXiv preprint</p>
<p>Fine-tuning vision-language-action models: Optimizing speed and success. M J Kim, C Finn, P Liang, arXiv:2502.196452025arXiv preprint</p>
<p>A Kirillov, E Mintun, N Ravi, H Mao, C Rolland, L Gustafson, T Xiao, S Whitehead, A C Berg, W.-Y Lo, P Dollár, R Girshick, arXiv:2304.02643Segment anything. 2023</p>
<p>Curl: Contrastive unsupervised representations for reinforcement learning. M Laskin, A Srinivas, P Abbeel, International conference on machine learning. PMLR2020</p>
<p>J Lee, J Duan, H Fang, Y Deng, S Liu, B Li, B Fang, J Zhang, Y R Wang, S Lee, arXiv:2508.07917Action reasoning models that can reason in space. 2025arXiv preprint</p>
<p>B Li, Y Zhang, D Guo, R Zhang, F Li, H Zhang, K Zhang, P Zhang, Y Li, Z Liu, arXiv:2408.03326Llava-onevision: Easy visual task transfer. 2024aarXiv preprint</p>
<p>B Li, Y Zhang, D Guo, R Zhang, F Li, H Zhang, K Zhang, P Zhang, Y Li, Z Liu, arXiv:2408.03326Llava-onevision: Easy visual task transfer. 2024barXiv preprint</p>
<p>H Li, S Yang, Y Chen, Y Tian, X Yang, X Chen, H Wang, T Wang, F Zhao, D Lin, arXiv:2506.19816Transferring latent motion across time for multi-frame prediction in manipulation. 2025aarXiv preprint</p>
<p>Cogact: A foundational vision-language-action model for synergizing cognition and action in robotic manipulation. Q Li, Y Liang, Z Wang, L Luo, X Chen, M Liao, F Wei, Y Deng, S Xu, Y Zhang, arXiv:2411.196502024carXiv preprint</p>
<p>Unified video action model. S Li, Y Gao, D Sadigh, S Song, arXiv:2503.002002025barXiv preprint</p>
<p>Y Li, Y Du, K Zhou, J Wang, W X Zhao, J.-R Wen, arXiv:2305.10355Evaluating object hallucination in large vision-language models. 2023arXiv preprint</p>
<p>Y Li, Y Deng, J Zhang, J Jang, M Memmel, R Yu, C R Garrett, F Ramos, D Fox, A Li, arXiv:2502.05485Hierarchical action models for open-world robot manipulation. 2025carXiv preprint</p>
<p>Pointnetgpd: Detecting grasp configurations from point sets. H Liang, X Ma, S Li, M Görner, S Tang, B Fang, F Sun, J Zhang, 2019 International Conference on Robotics and Automation (ICRA). IEEE2019</p>
<p>Genie envisioner: A unified world foundation platform for robotic manipulation. Y Liao, P Zhou, S Huang, D Yang, S Chen, Y Jiang, Y Hu, J Cai, S Liu, J Luo, arXiv:2508.056352025arXiv preprint</p>
<p>F Lin, R Nai, Y Hu, J You, J Zhao, Y Gao, arXiv:2505.11917Onetwovla: A unified vision-language-action model with adaptive reasoning. 2025arXiv preprint</p>
<p>Moka: Open-vocabulary robotic manipulation through mark-based visual prompting. F Liu, K Fang, P Abbeel, S Levine, First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024. 2024</p>
<p>Vl-grasp: a 6-dof interactive grasp policy for language-oriented objects in cluttered indoor scenes. Y Lu, Y Fan, B Deng, F Liu, Y Li, S Wang, 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2023a</p>
<p>Vl-grasp: a 6-dof interactive grasp policy for language-oriented objects in cluttered indoor scenes. Y Lu, Y Fan, B Deng, F Liu, Y Li, S Wang, 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2023b</p>
<p>Visual embodied brain: Let multimodal large language models see, think, and control in spaces. G Luo, G Yang, Z Gong, G Chen, H Duan, E Cui, R Tong, Z Hou, T Zhang, Z Chen, arXiv:2506.001232025arXiv preprint</p>
<p>F1: A vision-language-action model bridging understanding and generation to actions. Q Lv, W Kong, H Li, J Zeng, Z Qiu, D Qu, H Song, Q Chen, X Deng, M Y Wang, L Nie, J Pang, 2025</p>
<p>V Makoviychuk, L Wawrzyniak, Y Guo, M Lu, K Storey, M Macklin, D Hoeller, N Rudin, A Allshire, A Handa, arXiv:2108.10470Isaac gym: High performance gpu-based physics simulation for robot learning. 2021arXiv preprint</p>
<p>Generation and comprehension of unambiguous object descriptions. J Mao, J Huang, A Toshev, O Camburu, A L Yuille, K Murphy, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016</p>
<p>R3m: A universal visual representation for robot manipulation. S Nair, A Rajeswaran, V Kumar, C Finn, A Gupta, arXiv:2203.126012022arXiv preprint</p>
<p>S Nasiriany, S Kirmani, T Ding, L Smith, Y Zhu, D Driess, D Sadigh, T Xiao, arXiv:2411.02704Rt-affordance: Affordances are versatile intermediate representations for robot manipulation. 2024arXiv preprint</p>
<p>Octo: An open-source generalist robot policy. Octo Model Team, D Ghosh, H Walke, K Pertsch, K Black, O Mees, S Dasari, J Hejna, C Xu, J Luo, T Kreiman, Y Tan, P Sanketi, Q Vuong, T Xiao, D Sadigh, C Finn, S Levine, Proceedings of Robotics: Science and Systems. Robotics: Science and SystemsDelft, Netherlands2024</p>
<p>M Oquab, T Darcet, T Moutakanni, H Vo, M Szafraniec, V Khalidov, P Fernandez, D Haziza, F Massa, A El-Nouby, arXiv:2304.07193Learning robust visual features without supervision. 2023arXiv preprint</p>
<p>Fast: Efficient action tokenization for vision-language-action models. K Pertsch, K Stachowicz, B Ichter, D Driess, S Nair, Q Vuong, O Mees, C Finn, S Levine, arXiv:2501.097472025arXiv preprint</p>
<p>Sofar: Language-grounded orientation bridges spatial reasoning and object manipulation. Z Qi, W Zhang, Y Ding, R Dong, X Yu, J Li, L Xu, B Li, X He, G Fan, J Zhang, J He, J Gu, X Jin, K Ma, Z Zhang, H Wang, L Yi, 10.48550/arXiv.2502.131432025</p>
<p>D Qu, H Song, Q Chen, Y Yao, X Ye, Y Ding, Z Wang, J Gu, B Zhao, D Wang, arXiv:2501.15830Exploring spatial representations for visual-language-action model. 2025arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International conference on machine learning. PMLR2021</p>
<p>Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability. Advances in neural information processing systems. M Raghu, J Gilmer, J Yosinski, J Sohl-Dickstein, 201730</p>
<p>Sayplan: Grounding large language models using 3d scene graphs for scalable robot task planning. K Rana, J Haviland, S Garg, J Abou-Chakra, I Reid, N Suenderhauf, arXiv:2307.061352023arXiv preprint</p>
<p>Hi robot: Open-ended instruction following with hierarchical vision-language-action models. L X Shi, B Ichter, M Equi, L Ke, K Pertsch, Q Vuong, J Tanner, A Walling, H Wang, N Fusai, arXiv:2502.194172025arXiv preprint</p>
<p>M Shukor, D Aubakirova, F Capuano, P Kooijmans, S Palma, A Zouitine, M Aractingi, C Pascal, M Russi, A Marafioti, arXiv:2506.01844A vision-language-action model for affordable and efficient robotics. 2025arXiv preprint</p>
<p>Towards vqa models that can read. A Singh, V Natarjan, M Shah, Y Jiang, X Chen, D Parikh, M Rohrbach, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2019</p>
<p>Benchmarking object detectors with coco: A new path forward. S Singh, A Yadav, J Jain, H Shi, J Johnson, K Desai, European Conference on Computer Vision. Springer2024</p>
<p>H Song, D Qu, Y Yao, Q Chen, Q Lv, Y Tang, M Shi, G Ren, M Yao, B Zhao, arXiv:2505.21432Introducing system-2 thinking in visual-language-action model. 2025arXiv preprint</p>
<p>R Team, M Cao, H Tan, Y Ji, M Lin, Z Li, Z Cao, P Wang, E Zhou, Y Han, arXiv:2507.02029Robobrain 2.0 technical report. 2025arXiv preprint</p>
<p>Using geometry to detect grasp poses in 3d point clouds. A Ten Pas, R Platt, Robotics Research. Springer20171</p>
<p>Y Tian, S Yang, J Zeng, P Wang, D Lin, H Dong, J Pang, arXiv:2412.15109Predictive inverse dynamics models are scalable learners for robotic manipulation. 2024arXiv preprint</p>
<p>The all-seeing project v2: Towards general relation comprehension of the open world. W Wang, Y Ren, H Luo, T Li, C Yan, Z Chen, W Wang, Q Li, L Lu, X Zhu, European Conference on Computer Vision. Springer2024</p>
<p>Unified vision-language-action model. Y Wang, X Li, W Wang, J Zhang, Y Li, Y Chen, X Wang, Z Zhang, arXiv:2506.198502025arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 202235</p>
<p>K Wu, C Hou, J Liu, Z Che, X Ju, Z Yang, M Li, Y Zhao, Z Xu, G Yang, arXiv:2412.13877Benchmark on multi-embodiment intelligence normative data for robot manipulation. 2024arXiv preprint</p>
<p>Embedding symbolic knowledge into deep networks. Y Xie, Z Xu, M S Kankanhalli, K S Meel, H Soh, Advances in neural information processing systems. 201932</p>
<p>A0: An affordance-aware hierarchical model for general robotic manipulation. R Xu, J Zhang, M Guo, Y Wen, H Yang, M Lin, J Huang, Z Li, K Zhang, L Wang, Y Kuang, M Cao, F Zheng, X Liang, 2025a</p>
<p>A0: An affordance-aware hierarchical model for general robotic manipulation. R Xu, J Zhang, M Guo, Y Wen, H Yang, M Lin, J Huang, Z Li, K Zhang, L Wang, arXiv:2504.126362025barXiv preprint</p>
<p>J Yang, R Tan, Q Wu, R Zheng, B Peng, Y Liang, Y Gu, M Cai, S Ye, J Jang, arXiv:2502.13130A foundation model for multimodal ai agents. 2025aarXiv preprint</p>
<p>Instructvla: Vision-language-action instruction tuning from understanding to manipulation. S Yang, H Li, Y Chen, B Wang, Y Tian, T Wang, H Wang, F Zhao, Y Liao, J Pang, arXiv:2507.175202025barXiv preprint</p>
<p>Latent action pretraining from videos. S Ye, J Jang, B Jeon, S Joo, J Yang, B Peng, A Mandlekar, R Tan, Y.-W Chao, B Y Lin, L Liden, K Lee, J Gao, L Zettlemoyer, D Fox, M Seo, The Thirteenth International Conference on Learning Representations (ICLR). 2025</p>
<p>Modeling context in referring expressions. L Yu, P Poirson, S Yang, A C Berg, T L Berg, European conference on computer vision. Springer2016</p>
<p>W Yu, Z Yang, L Li, J Wang, K Lin, Z Liu, X Wang, L Wang, arXiv:2308.02490Mm-vet: Evaluating large multimodal models for integrated capabilities. 2023arXiv preprint</p>
<p>Robopoint: A vision-language model for spatial affordance prediction for robotics. W Yuan, J Duan, V Blukis, W Pumacay, R Krishna, A Murali, A Mousavian, D Fox, arXiv:2406.107212024arXiv preprint</p>
<p>Robotic control via embodied chain-of-thought reasoning. M Zawalski, W Chen, K Pertsch, O Mees, C Finn, S Levine, arXiv:2407.086932024arXiv preprint</p>
<p>Sigmoid loss for language image pre-training. X Zhai, B Mustafa, A Kolesnikov, L Beyer, arXiv:2306.13394Mme: A comprehensive evaluation benchmark for multimodal large language models. Y S Y Q M Zhang, X L J Y X Zheng, K L X S Y Wu, R J C Fu, P Chen, 2023. 202118arXiv preprintProceedings of the IEEE/CVF International Conference on Computer Vision</p>
<p>Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. Q Zhao, Y Lu, M J Kim, Z Fu, Z Zhang, Y Wu, Z Li, Q Ma, S Han, C Finn, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference2025</p>
<p>E Zhou, J An, C Chi, Y Han, S Rong, C Zhang, P Wang, Z Wang, T Huang, L Sheng, arXiv:2506.04308Towards spatial referring with reasoning in vision-language models for robotics. 2025aarXiv preprint</p>
<p>Vision-language-action model with open-world embodied reasoning from pretrained knowledge. Z Zhou, Y Zhu, J Wen, C Shen, Y Xu, arXiv:2505.219062025barXiv preprint</p>
<p>J Zhu, W Wang, Z Chen, Z Liu, S Ye, L Gu, H Tian, Y Duan, W Su, J Shao, Z Gao, E Cui, X Wang, Y Cao, Y Liu, X Wei, H Zhang, H Wang, W Xu, H Li, J Wang, N Deng, S Li, Y He, T Jiang, J Luo, Y Wang, C He, B Shi, X Zhang, W Shao, J He, Y Xiong, W Qu, P Sun, P Jiao, H Lv, L Wu, K Zhang, H Deng, J Ge, K Chen, L Wang, M Dou, L Lu, X Zhu, T Lu, D Lin, Y Qiao, J Dai, W Wang, Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. 2025</p>            </div>
        </div>

    </div>
</body>
</html>
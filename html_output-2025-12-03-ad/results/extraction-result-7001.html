<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7001 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7001</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7001</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-220546439</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclanthology.org/2021.nlp4convai-1.20.pdf" target="_blank">Investigating Pretrained Language Models for Graph-to-Text Generation</a></p>
                <p><strong>Paper Abstract:</strong> Graph-to-text generation aims to generate fluent texts from graph-based data. In this paper, we investigate two recent pretrained language models (PLMs) and analyze the impact of different task-adaptive pretraining strategies for PLMs in graph-to-text generation. We present a study across three graph domains: meaning representations, Wikipedia knowledge graphs (KGs) and scientific KGs. We show that approaches based on PLMs BART and T5 achieve new state-of-the-art results and that task-adaptive pretraining strategies improve their performance even further. We report new state-of-the-art BLEU scores of 49.72 on AMR-LDC2017T10, 59.70 on WebNLG, and 25.66 on AGENDA datasets - a relative improvement of 31.8%, 4.5%, and 42.4%, respectively, with our models generating significantly more fluent texts than human references. In an extensive analysis, we identify possible reasons for the PLMs’ success on graph-to-text tasks. Our findings suggest that the PLMs benefit from similar facts seen during pretraining or fine-tuning, such that they perform well even when the input graph is reduced to a simple bag of node and edge labels.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7001.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7001.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMR PENMAN linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PENMAN linearization of Abstract Meaning Representation (AMR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential textual serialization of AMR graphs using the PENMAN notation (nested parentheses and role labels) that encodes nodes (concepts) and edges (semantic relations) as a linear sequence; used as the input string for encoder–decoder PLMs in AMR-to-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>PENMAN AMR linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encodes an AMR graph as a nested parenthesized sequence (PENMAN format) where nodes (concepts) and relation roles (e.g., :ARG0, :ARG1, :time) are written in a depth‑first nested form with parentheses marking graph structure and role labels indicating edge types; example: (feel :ARG0 (we) :ARG1 (terrible :degree (very)) :time (now) ...).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential (hierarchical/nested parentheses)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>PENMAN linearization (depth‑first style traversal / parenthesized serialization as commonly used for AMR)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AMR (LDC2017T10)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text generation (graph-to-text)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART, T5 (small/base/large)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained encoder–decoder Transformers: BART (base ~140M, large ~400M) and T5 (small ~60M, base ~220M, large ~770M) fine‑tuned on linearized AMR sequences to generate target sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, chrF++, MoverScore, BERTScore, BLEURT, human evaluation (fluency / meaning similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>T5 large (fine‑tuned) BLEU 45.80 (no task adaptation); T5 large + supervised task adaptation (STA) BLEU 49.72 (new state of the art reported in paper); BART large also shows substantial gains over prior state of the art (numbers reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Supervised task adaptation (STA) using additional graph–text pairs speeds convergence (example: T5 large converged after ~2 fine‑tuning epochs with STA vs ~5 epochs without) and increases final BLEU; language model adaptation (LMA) also yields smaller gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>When AMR structure cues are removed (e.g., parentheses removed or input shuffled), performance drops substantially — indicating that for AMR the hierarchical ordering matters; PLMs can still partially rely on memorized language facts, risking decreased faithfulness to input graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Despite being a purely sequential representation, PENMAN linearization used with strong PLMs (BART/T5) outperforms many specialized graph‑encoding (GNN) models; however, prior work that explicitly encodes graph structure (GNNs, graph transformers) can be beneficial, and the paper notes that PLMs sometimes ignore graph structure and rely on language model knowledge.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7001.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7001.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG triple sequence (H/R/T)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-graph triple sequence with H / R / T markers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text sequence that represents a KG as an ordered list of triples, each triple expressed as tokenized head, relation and tail with explicit special tokens inserted before each element (H, R, T) so PLMs receive a flat triple list as input for KG-to-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Triple sequence with H / R / T tokens</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each RDF/DBpedia triple (head, relation, tail) is linearized into a contiguous text fragment with a special token prepended to each element: <H> head_entity <R> relation_label <T> tail_entity; the full input is the concatenation (sequence) of such triple fragments. For AGENDA the title and a sequence of entities/triples are concatenated similarly.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential (token‑based edge list)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Edge‑list / triple ordering (linear triple sequence); simple concatenation of triples in some dataset order with H/R/T markers; no explicit graph adjacency encoding beyond triple membership.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG (DBPedia triples), AGENDA (scientific KGs)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>KG-to-text generation / knowledge graph verbalization</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART, T5 (various sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BART and T5 encoder–decoder PLMs fine‑tuned on triple‑sequence inputs; vocabulary extended to include H, R, T tokens and dataset relation labels where needed.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU (seen / unseen / all partitions for WebNLG), chrF++, METEOR, MoverScore, BERTScore, BLEURT, human evaluation (semantic adequacy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>WebNLG (T5 approach reported in paper): BLEU 59.70 (all), 65.05 (seen), 54.69 (unseen) — reported as new state of the art; AGENDA: BART large + STA BLEU 25.66 (new state of the art in that dataset according to paper).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Simple triple‑sequence linearization works very well with PLMs; supervised task adaptation (STA) and language model adaptation (LMA) provide additional gains; in some settings PLMs trained on triple sequences converge faster and require less fine‑tuning data (T5 shown to be data‑efficient).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Shuffling triples (removing ordering/connectivity cues) has limited effect on KG‑to‑text performance for WebNLG/AGENDA, indicating that the representation (when used with PLMs) may not force the model to learn graph structure and can encourage reliance on memorized facts; unseen relation categories still cause drops in performance (models benefit from seeing relation–text pairs during training).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Sequence‑based triple encoding combined with PLMs outperforms many prior graph‑based architectures on KG‑to‑text benchmarks; however, models that explicitly incorporate graph structure (GNNs, graph transformers) still perform strongly among non‑PLM approaches and may be preferable when faithfulness to graph connectivity is critical.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7001.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7001.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neutral‑separator / shuffled linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parentheses‑removed / neutral‑separator linearization and shuffled triple (bag‑of‑labels) representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant / ablation representation where structural markers (parentheses or H/R/T tags) are replaced by a neutral separator token or where the triple sequence is randomly shuffled to produce a bag of node and edge labels, used to test the degree to which PLMs rely on graph order/structure vs. memorized language facts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Neutral‑separator linearization and shuffled triple (bag‑of‑labels)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Two related variants: (1) remove parentheses from AMR or replace H/R/T with a neutral separator token (•) so the sequence only preserves order but not explicit structure markers; (2) randomly shuffle the sequence of triple fragments so the input becomes an unordered bag of head/relation/tail labels (connectivity information obscured).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential (token‑based), lossy (when shuffled)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Order‑preserving neutralized tokenization (replace structural tokens with a neutral separator) or random permutation of triple fragments (shuffle) to create bag‑of‑labels input.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AMR (ablation), WebNLG (ablation), AGENDA (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Ablation studies for graph-to-text generation (tests of structure sensitivity / faithfulness)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (order vs shuf variants; e.g., T5 small order and T5 small shuf), BART (ablation experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Variants of T5 fine‑tuned on either correctly ordered linearizations ('order') or randomly shuffled inputs ('shuf') to probe reliance on structure.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU (primary), qualitative human analysis, example-based checks (faithfulness to corrupted facts)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>For KG datasets (WebNLG/AGENDA) shuffling causes only small degradation (KG-to-text performance 'not much lower'); for AMR shuffling causes a substantial drop in BLEU (large relative decrease reported). Neutral separators (removing explicit tags) cause only marginal drops (~2–4%) compared to using H/R/T tags or parentheses.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Shows that PLMs can still produce high‑quality KG verbalizations even when structural cues are reduced or removed, implying strong reliance on language modeling and memorized facts rather than learned graph connectivity; exposes risk that models may not learn to use graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Representation becomes lossy when shuffled (graph connectivity lost), harming tasks (notably AMR) that depend on structure; enables models to 'correct' or ignore corrupted facts in some cases (silent correction), and raises concerns about faithfulness and parroting of memorized training sentences rather than faithful rendering of input graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>The ablation demonstrates that PLM performance advantage over graph‑aware models is partly due to powerful language modeling and memorization rather than an ability to encode graph structure — in contrast, graph‑aware models (GNNs, graph transformers) explicitly model connectivity and can better ensure structural fidelity.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural AMR: Sequence-to-sequence models for parsing and generation <em>(Rating: 2)</em></li>
                <li>Text Generation from Knowledge Graphs with Graph Transformers <em>(Rating: 2)</em></li>
                <li>The WebNLG challenge: Generating text from RDF data <em>(Rating: 2)</em></li>
                <li>A graph-to-sequence model for AMR-to-text generation <em>(Rating: 2)</em></li>
                <li>Bridging the structural gap between encoding and decoding for data-to-text generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7001",
    "paper_id": "paper-220546439",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "AMR PENMAN linearization",
            "name_full": "PENMAN linearization of Abstract Meaning Representation (AMR)",
            "brief_description": "A sequential textual serialization of AMR graphs using the PENMAN notation (nested parentheses and role labels) that encodes nodes (concepts) and edges (semantic relations) as a linear sequence; used as the input string for encoder–decoder PLMs in AMR-to-text generation.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "PENMAN AMR linearization",
            "representation_description": "Encodes an AMR graph as a nested parenthesized sequence (PENMAN format) where nodes (concepts) and relation roles (e.g., :ARG0, :ARG1, :time) are written in a depth‑first nested form with parentheses marking graph structure and role labels indicating edge types; example: (feel :ARG0 (we) :ARG1 (terrible :degree (very)) :time (now) ...).",
            "representation_type": "sequential (hierarchical/nested parentheses)",
            "encoding_method": "PENMAN linearization (depth‑first style traversal / parenthesized serialization as commonly used for AMR)",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "AMR (LDC2017T10)",
            "task_name": "AMR-to-text generation (graph-to-text)",
            "model_name": "BART, T5 (small/base/large)",
            "model_description": "Pretrained encoder–decoder Transformers: BART (base ~140M, large ~400M) and T5 (small ~60M, base ~220M, large ~770M) fine‑tuned on linearized AMR sequences to generate target sentences.",
            "performance_metric": "BLEU, METEOR, chrF++, MoverScore, BERTScore, BLEURT, human evaluation (fluency / meaning similarity)",
            "performance_value": "T5 large (fine‑tuned) BLEU 45.80 (no task adaptation); T5 large + supervised task adaptation (STA) BLEU 49.72 (new state of the art reported in paper); BART large also shows substantial gains over prior state of the art (numbers reported in paper).",
            "impact_on_training": "Supervised task adaptation (STA) using additional graph–text pairs speeds convergence (example: T5 large converged after ~2 fine‑tuning epochs with STA vs ~5 epochs without) and increases final BLEU; language model adaptation (LMA) also yields smaller gains.",
            "limitations": "When AMR structure cues are removed (e.g., parentheses removed or input shuffled), performance drops substantially — indicating that for AMR the hierarchical ordering matters; PLMs can still partially rely on memorized language facts, risking decreased faithfulness to input graph structure.",
            "comparison_with_other": "Despite being a purely sequential representation, PENMAN linearization used with strong PLMs (BART/T5) outperforms many specialized graph‑encoding (GNN) models; however, prior work that explicitly encodes graph structure (GNNs, graph transformers) can be beneficial, and the paper notes that PLMs sometimes ignore graph structure and rely on language model knowledge.",
            "uuid": "e7001.0"
        },
        {
            "name_short": "KG triple sequence (H/R/T)",
            "name_full": "Knowledge-graph triple sequence with H / R / T markers",
            "brief_description": "A text sequence that represents a KG as an ordered list of triples, each triple expressed as tokenized head, relation and tail with explicit special tokens inserted before each element (H, R, T) so PLMs receive a flat triple list as input for KG-to-text generation.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Triple sequence with H / R / T tokens",
            "representation_description": "Each RDF/DBpedia triple (head, relation, tail) is linearized into a contiguous text fragment with a special token prepended to each element: &lt;H&gt; head_entity &lt;R&gt; relation_label &lt;T&gt; tail_entity; the full input is the concatenation (sequence) of such triple fragments. For AGENDA the title and a sequence of entities/triples are concatenated similarly.",
            "representation_type": "sequential (token‑based edge list)",
            "encoding_method": "Edge‑list / triple ordering (linear triple sequence); simple concatenation of triples in some dataset order with H/R/T markers; no explicit graph adjacency encoding beyond triple membership.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WebNLG (DBPedia triples), AGENDA (scientific KGs)",
            "task_name": "KG-to-text generation / knowledge graph verbalization",
            "model_name": "BART, T5 (various sizes)",
            "model_description": "BART and T5 encoder–decoder PLMs fine‑tuned on triple‑sequence inputs; vocabulary extended to include H, R, T tokens and dataset relation labels where needed.",
            "performance_metric": "BLEU (seen / unseen / all partitions for WebNLG), chrF++, METEOR, MoverScore, BERTScore, BLEURT, human evaluation (semantic adequacy)",
            "performance_value": "WebNLG (T5 approach reported in paper): BLEU 59.70 (all), 65.05 (seen), 54.69 (unseen) — reported as new state of the art; AGENDA: BART large + STA BLEU 25.66 (new state of the art in that dataset according to paper).",
            "impact_on_training": "Simple triple‑sequence linearization works very well with PLMs; supervised task adaptation (STA) and language model adaptation (LMA) provide additional gains; in some settings PLMs trained on triple sequences converge faster and require less fine‑tuning data (T5 shown to be data‑efficient).",
            "limitations": "Shuffling triples (removing ordering/connectivity cues) has limited effect on KG‑to‑text performance for WebNLG/AGENDA, indicating that the representation (when used with PLMs) may not force the model to learn graph structure and can encourage reliance on memorized facts; unseen relation categories still cause drops in performance (models benefit from seeing relation–text pairs during training).",
            "comparison_with_other": "Sequence‑based triple encoding combined with PLMs outperforms many prior graph‑based architectures on KG‑to‑text benchmarks; however, models that explicitly incorporate graph structure (GNNs, graph transformers) still perform strongly among non‑PLM approaches and may be preferable when faithfulness to graph connectivity is critical.",
            "uuid": "e7001.1"
        },
        {
            "name_short": "Neutral‑separator / shuffled linearization",
            "name_full": "Parentheses‑removed / neutral‑separator linearization and shuffled triple (bag‑of‑labels) representation",
            "brief_description": "A variant / ablation representation where structural markers (parentheses or H/R/T tags) are replaced by a neutral separator token or where the triple sequence is randomly shuffled to produce a bag of node and edge labels, used to test the degree to which PLMs rely on graph order/structure vs. memorized language facts.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Neutral‑separator linearization and shuffled triple (bag‑of‑labels)",
            "representation_description": "Two related variants: (1) remove parentheses from AMR or replace H/R/T with a neutral separator token (•) so the sequence only preserves order but not explicit structure markers; (2) randomly shuffle the sequence of triple fragments so the input becomes an unordered bag of head/relation/tail labels (connectivity information obscured).",
            "representation_type": "sequential (token‑based), lossy (when shuffled)",
            "encoding_method": "Order‑preserving neutralized tokenization (replace structural tokens with a neutral separator) or random permutation of triple fragments (shuffle) to create bag‑of‑labels input.",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "AMR (ablation), WebNLG (ablation), AGENDA (ablation)",
            "task_name": "Ablation studies for graph-to-text generation (tests of structure sensitivity / faithfulness)",
            "model_name": "T5 (order vs shuf variants; e.g., T5 small order and T5 small shuf), BART (ablation experiments)",
            "model_description": "Variants of T5 fine‑tuned on either correctly ordered linearizations ('order') or randomly shuffled inputs ('shuf') to probe reliance on structure.",
            "performance_metric": "BLEU (primary), qualitative human analysis, example-based checks (faithfulness to corrupted facts)",
            "performance_value": "For KG datasets (WebNLG/AGENDA) shuffling causes only small degradation (KG-to-text performance 'not much lower'); for AMR shuffling causes a substantial drop in BLEU (large relative decrease reported). Neutral separators (removing explicit tags) cause only marginal drops (~2–4%) compared to using H/R/T tags or parentheses.",
            "impact_on_training": "Shows that PLMs can still produce high‑quality KG verbalizations even when structural cues are reduced or removed, implying strong reliance on language modeling and memorized facts rather than learned graph connectivity; exposes risk that models may not learn to use graph structure.",
            "limitations": "Representation becomes lossy when shuffled (graph connectivity lost), harming tasks (notably AMR) that depend on structure; enables models to 'correct' or ignore corrupted facts in some cases (silent correction), and raises concerns about faithfulness and parroting of memorized training sentences rather than faithful rendering of input graphs.",
            "comparison_with_other": "The ablation demonstrates that PLM performance advantage over graph‑aware models is partly due to powerful language modeling and memorization rather than an ability to encode graph structure — in contrast, graph‑aware models (GNNs, graph transformers) explicitly model connectivity and can better ensure structural fidelity.",
            "uuid": "e7001.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural AMR: Sequence-to-sequence models for parsing and generation",
            "rating": 2,
            "sanitized_title": "neural_amr_sequencetosequence_models_for_parsing_and_generation"
        },
        {
            "paper_title": "Text Generation from Knowledge Graphs with Graph Transformers",
            "rating": 2,
            "sanitized_title": "text_generation_from_knowledge_graphs_with_graph_transformers"
        },
        {
            "paper_title": "The WebNLG challenge: Generating text from RDF data",
            "rating": 2,
            "sanitized_title": "the_webnlg_challenge_generating_text_from_rdf_data"
        },
        {
            "paper_title": "A graph-to-sequence model for AMR-to-text generation",
            "rating": 2,
            "sanitized_title": "a_graphtosequence_model_for_amrtotext_generation"
        },
        {
            "paper_title": "Bridging the structural gap between encoding and decoding for data-to-text generation",
            "rating": 1,
            "sanitized_title": "bridging_the_structural_gap_between_encoding_and_decoding_for_datatotext_generation"
        }
    ],
    "cost": 0.0163525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Investigating Pretrained Language Models for Graph-to-Text Generation
November 10, 2021</p>
<p>Leonardo F R Ribeiro 
Center for Information and Language Processing (CIS)
Research Training Group AIPHES and UKP Lab
Technical University of Darmstadt
LMU Munich</p>
<p>Martin Schmitt 
Center for Information and Language Processing (CIS)
Research Training Group AIPHES and UKP Lab
Technical University of Darmstadt
LMU Munich</p>
<p>Hinrich Schütze 
Center for Information and Language Processing (CIS)
Research Training Group AIPHES and UKP Lab
Technical University of Darmstadt
LMU Munich</p>
<p>Iryna Gurevych 
Center for Information and Language Processing (CIS)
Research Training Group AIPHES and UKP Lab
Technical University of Darmstadt
LMU Munich</p>
<p>Investigating Pretrained Language Models for Graph-to-Text Generation</p>
<p>Proceedings of the Third Workshop on Natural Language Processing for Conversational AI
the Third Workshop on Natural Language Processing for Conversational AINovember 10, 2021211
Graph-to-text generation aims to generate fluent texts from graph-based data. In this paper, we investigate two recent pretrained language models (PLMs) and analyze the impact of different task-adaptive pretraining strategies for PLMs in graph-to-text generation. We present a study across three graph domains: meaning representations, Wikipedia knowledge graphs (KGs) and scientific KGs. We show that approaches based on PLMs BART and T5 achieve new state-of-the-art results and that task-adaptive pretraining strategies improve their performance even further. We report new state-of-the-art BLEU scores of 49.72 on AMR-LDC2017T10, 59.70 on WebNLG, and 25.66 on AGENDA datasets -a relative improvement of 31.8%, 4.5%, and 42.4%, respectively, with our models generating significantly more fluent texts than human references. In an extensive analysis, we identify possible reasons for the PLMs' success on graph-totext tasks. Our findings suggest that the PLMs benefit from similar facts seen during pretraining or fine-tuning, such that they perform well even when the input graph is reduced to a simple bag of node and edge labels. 1Zhao et al., 2020a); and investigate the possible reasons for such a good performance.</p>
<p>Introduction</p>
<p>Graphs are important data structures in NLP as they represent complex relations within a set of objects. For example, semantic and syntactic structures of sentences can be represented using different graph representations (e.g., AMRs, Banarescu et al., 2013;semantic-role labeling, Surdeanu et al., 2008;syntactic and semantic graphs, Belz et al., 2011) and knowledge graphs (KGs) are used to describe factual knowledge in the form of relations between entities (Gardent et al., 2017;Vougiouklis et al., 2018;Koncel-Kedziorski et al., 2019).</p>
<p>Graph-to-text generation, a subtask of data-totext generation (Gatt and Krahmer, 2018), aims to create fluent natural language text to describe an input graph (see Figure 1). This task is important for NLP applications such as dialogue generation (Moon et al., 2019) and question answering (Duan et al., 2017). Recently, it has been shown that structured meaning representation, such as AMR or KG, can store the internal state of a dialog system, providing core semantic knowledge (Bonial et al., 2020;Bai et al., 2021) or can be the result of a database query for conversational QA (Yu et al., 2019). Moreover, dialog states can be represented as KGs to encode compositionality and can be shared across different domains, slot types and dialog participators (Cheng et al., 2020).</p>
<p>Transfer learning has become ubiquitous in NLP and pretrained Transformer-based architectures (Vaswani et al., 2017) have considerably outperformed prior state of the art in various downstream tasks (Devlin et al., 2019;Yang et al., 2019a;Radford et al., 2019).</p>
<p>In this paper, we analyze the applicability of two recent text-to-text pretrained language models (PLMs), BART  and T5 (Raffel et al., 2019), for graph-to-text generation. We choose these models because of their encoderdecoder architecture, which makes them particularly suitable for conditional text generation. Our study comprises three graph domains (meaning representations, Wikipedia KGs, and scientific KGs). We further introduce task-adaptive graph-to-text pretraining approaches for PLMs and demonstrate that such strategies improve the state of the art by a substantial margin.</p>
<p>While recent works have shown the benefit of explicitly encoding the graph structure in graph-totext generation (Song et al., 2018;Ribeiro et al., 2019Ribeiro et al., , 2020Schmitt et al., 2020;Zhao et al., 2020a, to name a few), our approaches based on PLMs consistently outperform these models, even though PLMs -as sequence models -do not exhibit any Linearized representation: <H> Apollo 12 <R> backup pilot <T> Alfred Worden <H> Alan Bean <R> was a crew member of <T> Apollo 12 <H> Apollo 12 <R> operator <T> NASA <H> Alan Bean <R> occupation <T> Test pilot <H> Apollo 12 <R> commander <T> David Scott <H> Alan Bean <R> was selected by NASA <T> 1963 <H> Alan Bean <R> alma Mater <T> UT Austin B.S. 1955  Text: Alan Bean graduated from UT Austin in 1955 with a Bachelor of Science degree. He was hired by NASA in 1963 and served as a test pilot. Apollo 12's backup pilot was Alfred Worden and was commanded by David Scott.</p>
<p>Text: As his children, we feel very terrible now.  Figure 1: Examples of (a) AMR and (b) WebNLG graphs, the input for the models and the reference texts.</p>
<p>graph-specific structural bias. 2 Simply representing the graph as a linear traversal (see Figure 1) leads to remarkable generation performance in the presence of a strong language model. In our analysis we investigate to what extent fine-tuned PLMs make use of the graph structure represented in the graph linearization. We notably observe that PLMs achieve high performance on two popular KG-totext benchmarks even when the KG is reduced to a mere bag of node and edge labels.</p>
<p>Our contributions are the following:</p>
<p>• We investigate and compare two PLMs, BART and T5, for graph-to-text generation, exploring language model adaptation (LMA) and supervised task adaptation (STA) pretraining, employing additional task-specific data. • Our approaches consistently outperform the state of the art by significant margins, ranging from 2.6 to 12.0 BLEU points, on three established graph-to-text benchmarks from different domains, exceeding specialized graph architectures (e.g., Graph Neural Networks, GNNs, Kipf and Welling, 2017). • In a crowdsourcing experiment, we demonstrate that our methods generate texts with significantly better fluency than existing works and the human references. • We discover that PLMs perform well even when trained on a shuffled linearized graph representation without any information about connectivity (bag of node and edge labels), which is surprising since prior studies showed that explicitly encoding the graph structure improves models trained from scratch (e.g.,</p>
<p>Related Work</p>
<p>Graph-to-text Learning. Various neural models have been proposed to generate sentences from graphs from different domains. Konstas et al. (2017) propose the first neural approach for AMRto-text generation that uses a linearized input graph. Prior approaches for KG-to-text generation train text-to-text neural models using sequences of KG triples as input (Trisedya et al., 2018;Moryossef et al., 2019;Castro Ferreira et al., 2019;Ribeiro et al., 2021a). Recent approaches (Marcheggiani and Perez Beltrachini, 2018;Song et al., 2018;Beck et al., 2018;Damonte and Cohen, 2019;Ribeiro et al., 2019;Zhao et al., 2020a;Schmitt et al., 2021;Ribeiro et al., 2021b) propose architectures based on GNNs to directly encode the graph structure, whereas other efforts (Ribeiro et al., 2020;Schmitt et al., 2020;Yao et al., 2020; inject the graph structure information into Transformer-based architectures. The success of those approaches suggests that imposing a strong relational inductive bias into the graph-to-text model can assist the generation.</p>
<p>Pretrained Language Models. Pretrained</p>
<p>Transformer-based models, such as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019b), or RoBERTa , have established a qualitatively new level of baseline performance for many widely used natural language understanding (NLU) benchmarks.</p>
<p>Generative pretrained Transformer-based methods, such as GPT-2 (Radford et al., 2019), BART T5 (Raffel et al., 2019), are employed in many natural language generation (NLG) tasks. Mager et al. (2020) were the first to employ GPT-2, a decoder-only PLM, for AMR-to-text generation and use cycle consistency to improve the adequacy. In contrast, we are the first to investigate BART and T5 models, which have both a Transformer-based encoder and decoder, in AMRto-text generation. Recently, Harkous et al. (2020) and Kale (2020) demonstrate state-of-the-art results in different data-to-text datasets, employing GPT-2 and T5 models respectively. Radev et al. (2020) propose DART, a new data-to-text dataset, and train a BART model gradually augmenting the WebNLG training data with DART data.</p>
<p>Hoyle et al. (2021) explore scaffolding objectives in PLMs and show gains in low-resource graph-to-text settings. Different from the above works, we focus on a general transfer learning strategies for graph-to-text generation, investigating task-adaptive pretraining approaches, employing additional collected task-specific data for different PLMs (BART and T5) and benchmarks. In addition, we provide a detailed analysis aimed at explaining the good performance of PLMs on KGto-text tasks.</p>
<p>Recently, Gururangan et al. (2020) explored taskadaptive pretraining strategies for text classification. While our LMA (see §3) is related to their DAPT as both use a self-supervised objective on a domainspecific corpus, they notably differ in that DAPT operates on the model input while LMA models the output. We are the first to show the benefits of additional task-specific pretraining in PLMs for graph-to-text tasks.</p>
<p>3 PLMs for Graph-to-Text Generation</p>
<p>Models in this Study</p>
<p>We investigate BART  and T5 (Raffel et al., 2019), two PLMs based on the Transformer encoder-decoder architecture (Vaswani et al., 2017), for graph-to-text generation. They mainly differ in how they are pretrained and the input corpora used for pretraining. We experiment with different T5 (small -60M parameters, base -220M, and large -770M) and BART (base -140M and large -400M) capacity models.</p>
<p>We fine-tune both PLMs for a few epochs on the supervised downstream graph-to-text datasets. For T5, in the supervised setup, we add a prefix "translate from Graph to Text:" before the graph input. We add this prefix to imitate the T5 setup, when translating between different languages.</p>
<p>Task-specific Adaptation</p>
<p>Inspired by previous work (Konstas et al., 2017;Gururangan et al., 2020), we investigate whether leveraging additional task-specific data can improve the PLMs' performance on graph-to-text generation. Task-specific data refers to a pretraining corpus that is more task-relevant and usually smaller than the text corpora used for taskindependent pretraining. In order to leverage the task-specific data, we add an intermediate adaptive pretraining step between the original pretraining and fine-tuning phases for graph-to-text generation.</p>
<p>More precisely, we first continue pretraining BART and T5 using language model adaptation (LMA) or supervised task adaptation (STA) training. In the supervised approach, we use pairs of graphs and corresponding texts collected from the same or similar domain as the target task. In the LMA approach, we follow BART and T5 pretraining strategies for language modeling, using the reference texts that describe the graphs. Note that we do not use the graphs in the LMA pretraining, but only the target text of our task-specific data collections. The goal is to adapt the decoder to the domain of the final task (Gururangan et al., 2020). In particular, we randomly mask text spans, replacing 15% of the tokens. 3 Before evaluation, we finally fine-tune the models using the original training set as usual.</p>
<p>Datasets</p>
<p>We evaluate the text-to-text PLMs on three graph-to-text benchmarks: AMR (LDC2017T10), WebNLG (Gardent et al., 2017), and AGENDA (Koncel-Kedziorski et al., 2019). We chose those datasets because they comprise different domains and are widely used in prior work. Table 10 in Appendix shows statistics for each dataset.</p>
<p>AMR. Abstract meaning representation (AMR) is a semantic formalism that represents the meaning of a sentence as a rooted directed graph expressing "who is doing what to whom" (Banarescu et al., 2013). In an AMR graph, nodes represent concepts and edges represent semantic relations. An instance in LDC2017T10 consists of a sentence annotated with its corresponding AMR graph. Following Mager et al. (2020), we linearize the AMR graphs using the PENMAN notation (see Figure 1a). 4 WebNLG. Each instance of WebNLG contains a KG from DBPedia (Auer et al., 2007) and a target text with one or multiple sentences that describe the graph. The test set is divided into two partitions: seen, which contains only DBPedia categories present in the training set, and unseen, which covers categories never seen during training. Their union is called all. Following previous work (Harkous et al., 2020), we prepend H , R , and T tokens before the head entity, the relation and tail entity of a triple (see Figure 1b).</p>
<p>AGENDA. In this dataset, KGs are paired with scientific abstracts extracted from proceedings of AI conferences. Each sample contains the paper title, a KG, and the corresponding abstract. The KG contains entities corresponding to scientific terms and the edges represent relations between these entities. This dataset has loose alignments between the graph and the corresponding text as the graphs were automatically generated. The input for the models is a text containing the title, a sequence of all KG entities, and the triples. The target text is the paper abstract. We add special tokens into the triples in the same way as for WebNLG.</p>
<p>Additional Task-specific Data</p>
<p>In order to evaluate the proposed task-adaptive pretraining strategies for graph-to-text generation, we collect task-specific data for two graph domains: meaning representations (like AMR) and scientific data (like AGENDA). We did not attempt collecting additional data like WebNLG because the texts in this benchmark do not stem from a corpus but were specifically written by annotators.</p>
<p>AMR Silver Data. In order to generate additional data for AMR, we sample two sentence collections of size 200K and 2M from the Gigaword 5 corpus and use a state-of-the-art AMR parser (Cai and Lam, 2020a) to parse them into AMR graphs. 6 For supervised pretraining, we condition a model on the AMR silver graphs to generate the corresponding sentences before fine-tuning it on gold AMR graphs. For self-supervised pretraining, we only use the sentences. 7 Semantic Scholar AI Data. We collect titles and abstracts of around 190K scientific papers from the Semantic Scholar (Ammar et al., 2018) taken from the proceedings of 36 top Computer Science/AI conferences. We construct KGs from the paper abstracts employing DyGIE++ (Wadden et al., 2019), an information extraction system for scientific texts. Note that the AGENDA dataset was constructed using the older SciIE system (Luan et al., 2018), which also extracts KGs from AI scientific papers. A second difference is that in our new dataset, the domain is broader as we collected data from 36 conferences compared to 12 from AGENDA. Furthermore, to prevent data leakage, all AGENDA samples used for performance evaluation are removed from our dataset. We will call the new dataset KGAIA (KGs from AI Abstracts). 8 Table 11 in Appendix shows relevant dataset statistics.</p>
<p>Model</p>
<p>Experiments</p>
<p>We modify the BART and T5 implementations released by Hugging Face (Wolf et al., 2019) in order to adapt them to graph-to-text generation. For the KG datasets, we add the H , R , and T tokens to the models' vocabulary. We add all edge labels seen in the training set to the vocabulary of the  with an initial learning rate of 3 · 10 −5 . We employ a linearly decreasing learning rate schedule without warm-up. The batch and beam search sizes are chosen from {2,4,8} and {1,3,5}, respectively, based on the respective development set. Dev BLEU is used for model selection.</p>
<p>Following previous works, we evaluate the results with BLEU (Papineni et al., 2002), ME-TEOR (Denkowski and Lavie, 2014), and chrF++ (Popović, 2015) metrics. We also use Mover-Score (Zhao et al., 2019), BERTScore (Zhang et al., 2020), and BLEURT (Sellam et al., 2020) metrics, as they employ contextual and semantic knowledge and thus depend less on the surface symbols. Additionally, we perform a human evaluation (cf. §5.4) quantifying the fluency, semantic adequacy and meaning similarity of the generated texts. Table 1 shows our results for the setting without additional pretraining, with additional self-supervised task-adaptive pretraining solely using the collected Gigaword sentences (LMA), and with additional supervised task adaptation (STA), before fine-tuning. We also report several recent results on the AMR test set. Mager et al. (2020) and Harkous et al.</p>
<p>Results on AMR-to-Text</p>
<p>(2020) employ GPT-2 in their approaches. Note that GPT-2 only consists of a Transformer-based decoder.</p>
<p>Only considering approaches without task adaptation, BART large already achieves a considerable improvement of 5.77 BLEU and 3.98 METEOR scores over the previous state of the art. With a BLEU score of 45.80, T5 large performs best. The other metrics follow similar trends. See Table 13 in Appendix for evaluation with more metrics. The strong performance of both BART and T5 in the AMR dataset suggests that PLMs can infer the AMR structure by a simple linear sequence of the graph, in contrast to GNN-based models that explicitly consider the graph structure using messagepassing between adjacent nodes (Beck et al., 2018).</p>
<p>Task-specific Adaptation. LMA already brings some gains with T5 benefitting more than BART in most metrics. It still helps less than STA even though we only have automatically generated annotations. This suggests that the performance increases with STA do not only come from additional exposure to task-specific target texts and that the models learn how to handle graphs and the graphtext correspondence even with automatically generated AMRs. After STA, T5 achieves 49.72 BLEU points, the new state of the art for AMR-to-text generation. Interestingly, gains from STA with 2M over 200K are larger in BART than in T5, suggesting that large amounts of silver data may not be required for a good performance with T5.</p>
<p>In general, models pretrained on the STA setup converge faster than without task-specific adaptation. For example, T5 large without additional pretraining converges after 5 epochs of fine-tuning whereas T5 large with STA already converges after 2 epochs.  hand, fully end-to-end models (Ribeiro et al., 2020;Schmitt et al., 2020) have strong performance on the seen dataset and usually perform poorly in unseen data. Models that explicitly encode the graph structure (Ribeiro et al., 2020;Zhao et al., 2020a) achieve the best performance among approaches that do not employ PLMs. Note that T5 is also used in Kale (2020). Differences in our T5 setup include a modified model vocabulary, the use of beam search, the learning rate schedule and the prefix before the input graph. Our T5 approach achieves 59.70, 65.05 and 54.69 BLEU points on all, seen and unseen sets, the new state of the art. We conjecture that the performance gap between seen and unseen sets stems from the advantage obtained by a model seeing examples of relation-text pairs during fine-tuning. For example, the relation party (political party) was never seen during training and the model is required to generate a text that verbalizes the tuple: Abdul Taib Mahmud, party, Parti Bumiputera Sarawak . Interestingly, BART performs much worse than T5 on this benchmark, especially in the unseen partition with 9.7 BLEU points lower compared to T5.</p>
<p>Results on WebNLG</p>
<p>For lack of a suitable data source (cf. §4), we did not explore our LMA or STA approaches for WebNLG. However, we additionally discuss crossdomain STA in Appendix B.  dataset. We believe that their capacity to generate fluent text helps when generating paper abstracts, even though they were not pretrained in the scientific domain. BART large shows an impressive performance with a BLEU score of 23.65, which is 5.6 points higher than the previous state of the art.</p>
<p>Results on AGENDA</p>
<p>Task-specific Adaptation. On AGENDA, BART benefits more from our task-adaptive pretraining, achieving the new state of the art of 25.66 BLEU points, a further gain of 2 BLEU points compared to its performance without task adaptation. The improvements from task-adaptive pretraining are not as large as for AMR. We hypothesize that this is due to the fact that the graphs do not completely cover the target text (Koncel-Kedziorski et al., 2019), making this dataset more challenging. See Table 12 in Appendix for more automatic metrics.</p>
<p>Human Evaluation</p>
<p>To further assess the quality of the generated text, we conduct a human evaluation on AMR and WebNLG via crowd sourcing on Amazon Mechanical Turk. Arrabbiata sauce can be found in Italy where Sergio Mattarella is the leader and the capital city is Rome. Italians are the people who live there and the language spoken is Italian.</p>
<p>Italians live in Italy where the capital is Rome and the language is Italian. Sergio Mattarella is the leader of the country and arrabbiata sauce can be found there.</p>
<p>T5</p>
<p>Reference: Arrabbiata sauce is from Italy where the capital is Rome, Italian is the language spoken and Sergio Mattarella is a leader. close in meaning is the generated text to the reference sentence?) for AMR; (ii) Semantic Adequacy (i.e., does the text clearly express the data?) for WebNLG. We randomly select 100 generated texts of each model, which the annotators then rate on a 1-7 Likert scale. For each text, we collect scores from 3 annotators and average them. 10 Table 4 shows the results. Our approaches improve the fluency, meaning similarity, and semantic adequacy on both datasets compared to other stateof-the-art approaches with statistically significant margins (p&lt;0.05). Interestingly, the highest fluency improvement (+0.97) is on AMR, where our approach also has the largest BLEU improvement (+8.10) over Harkous et al. (2020). Finally, our models score higher than the references in fluency with statistically significant margins, highlighting their strong language generation abilities. 11</p>
<p>Limiting the Training Data</p>
<p>In Figure 3, we investigate the PLMs' performance, measured with BLEU score, while varying (from 1% to 100%) the amount of training data used for 10 Inter-annotator agreement for the three criteria ranged from 0.40 to 0.79, with an average Krippendorff's α of 0.56. 11 Examples of fluent generations can be found in the Tables 15 and 16 in Appendix.  fine-tuning. We find that, when fine-tuned with only 40% of the data, both BART and T5 already greatly improve the performance compared to using the entire training data in all three benchmarks. For example, BART fine-tuned on 40% of AMR training data achieves 91% of the BLEU score when fine-tuned on full data. Note that in a low-resource scenario in AMR and WebNLG, T5 considerably outperforms BART. In particular, with only 1% of training examples, the difference between T5 and BART is 7.51 and 5.64 BLEU points for AMR and WebNLG, respectively. This suggests that T5 is more data efficient when adapting to the new task, likewise our findings in AMR-STA (cf. §5.1).</p>
<p>Model AMR WebNLG AGENDA</p>
<p>Influence of the Graph Structure</p>
<p>We conduct further experiments to examine how much the PLMs consider the graph structure. To this end, we remove parentheses in AMRs and replace H , R , and T tokens with neutral separator tokens, denoted •, for KGs, such that the graph structure is only defined by the order of node and edge labels. If we shuffle such a sequence, the graph structure is thus completely obscured and the input effectively becomes a bag of node and edge labels. See Figure 2 for an example of both a correctly ordered and a shuffled triple sequence. Antwerp International Airport serves the city of Antwerp in Belgium where the German language is spoken and Charles Michel is the leader.</p>
<p>Quantitative Analysis</p>
<p>(
2) T • California • is Part Of • US • California • capital • Sacramento
California is part of the United States and its capital is Sacramento.</p>
<p>California is part of the United States and its capital is Sacramento.
(3) F • US • is Part Of • California • California • capital • Sacramento
California's capital is Sacramento and the United States is part of California.</p>
<p>California is part of the United States and its capital is Sacramento.
(4) T • Amarillo, Texas • is Part Of • United States
Amarillo, Texas is part of the United States.</p>
<p>Amarillo, Texas is part of the United States.
(5) F • United States • is Part Of • Amarillo, Texas
Amarillo, Texas is part of the United States.</p>
<p>Amarillo, Texas is part of the United States. Table 6: Example generations from shuffled (S), true (T), and corrupted (F) triple facts by T5 small , fine-tuned on correctly ordered triples (order) and randomly shuffled input (shuf ).</p>
<p>vs. shuffled ones (T5 shuf ) for both training and evaluation. We first observe that T5 order only has marginally lower performance (around 2-4%) with the neutral separators than with the H / R / T tags or parentheses. 12 We see that as evidence that the graph structure is similarly well captured by T5 order . Without the graph structure (T5 shuf ), AMR-to-text performance drops significantly. Possible explanations of this drop are: (i) the relative ordering of the AMR graph is known to correlate with the target sentence order (Konstas et al., 2017); (ii) in contrast to WebNLG that contains common knowledge, the AMR dataset contains very specific sentences with higher surprisal; 13 (iii) AMRs are much more complex graph structures than the KGs from WebNLG and AGENDA. 14 On the other hand, KG-to-text performance is not much lower, indicating that most of the PLMs' success in this task stems from their language modeling rather than their graph encoding capabilities. We hypothesize that a PLM can match the entities in a shuffled input with sentences mentioning these entities from the pretraining or fine-tuning phase. It has recently been argued that large PLMs can recall certain common knowledge facts from pretraining (Petroni et al., 2019;Bosselut et al., 2019).</p>
<p>Qualitative Analysis</p>
<p>The example in Figure 2 confirms our impression. T5 shuf produces a text with the same content as 12 See a more fine-grained comparison in Appendix C. 13 Perplexities estimated on the dev sets of AMR and WebNLG datasets, with GPT-2 fine-tuned on the corresponding training set, are 20.9 and 7.8, respectively. 14 In Appendix D, we present the graph properties of the datasets and discuss the differences. T5 order but does not need the correct triple structure to do so. Example (1) in Table 6 shows the output of both models with shuffled input. Interestingly, even T5 order produces a reasonable and truthful text. This suggests that previously seen facts serve as a strong guide during text generation, even for models that were fine-tuned with a clearly marked graph structure, suggesting that T5 order also relies more on language modeling than the graph structure. It does have more difficulties covering the whole input graph though. The fact that Antwerp is located in Belgium is missing from its output.</p>
<p>To further test our hypothesis that PLMs make use of previously seen facts during KG-to-text generation, we generate example true facts, corrupt them in a controlled setting, and feed them to both T5 order and T5 shuf to observe their output (examples (2)-(5) in Table 6). The model trained on correctly ordered input has learned a bit more to rely on the input graph structure. The false fact in example (3) with two triples is reliably transferred to the text by T5 order but not by T5 shuf , which silently corrects it. Also note that, in example (5), both models refuse to generate an incorrect fact. More examples can be found in Table 14 in the Appendix.</p>
<p>Our qualitative analysis illustrates that state-ofthe-art PLMs, despite their fluency capacities (cf. §5.4), bear the risk of parroting back training sentences while ignoring the input structure. This issue can limit the practical usage of those models as, in many cases, it is important for a generation model to stay true to its input (Wiseman et al., 2017;Falke et al., 2019).</p>
<p>Conclusion</p>
<p>We investigated two pretrained language models (PLMs) for graph-to-text generation and show that the pretraining strategies, language model adaptation (LMA) and supervised task adaptation (STA), can lead to notable improvements. Our approaches outperform the state of the art by a substantial margin on three graph-to-text benchmarks. Moreover, in a human evaluation our generated texts are perceived significantly more fluent than human references. Examining the influence of the graph structure on the text generation process, we find that PLMs may not always follow the graph structure and instead use memorized facts to guide the generation. A promising direction for future work is to explore ways of injecting a stronger graphstructural bias into PLMs, thus possibly leveraging their strong language modeling capabilities and keeping the output faithful to the input graph. </p>
<p>Appendices</p>
<p>In this supplementary material, we provide: (i) additional information about the data used in the experiments, and (ii) results that we could not fit into the main body of the paper.</p>
<p>A AMR Input Representation</p>
<p>We test three variants for the representation of the input AMR graph. Following previous work (Konstas et al., 2017;Mager et al., 2020) In this experiment we employ T5 small . Table 7 shows the results on the AMR development set. The PENMAN representation leads to best results. Therefore, this representation is used in the rest of the experiments.  </p>
<p>B Cross-domain Adaptation</p>
<p>For a given task, it is not always possible to collect closely related data -as we saw, e.g., for WebNLG. We therefore report STA in a cross-domain setting for the different KG-to-text benchmarks. Ta  C Input Graph Size Figure 4 visualizes T5 small 's performance with respect to the number of input graph triples in WebNLG dataset. We observe that T5 order and T5 shuf perform similarly for inputs with only one triple but that the gap between the models increases with larger graphs. While it is obviously more difficult to reconstruct a larger graph than a smaller one, this also suggests that the graph structure is more taken into account for graphs with more than 2 triples. For the unseen setting, the performance gap for these graphs is even larger, suggesting that the PLM can make more use of the graph structure when it has to.  </p>
<p>D Graph Statistics</p>
<p>In Table 9, we present the graph properties of the three datasets. All statistics are calculated using      The capital city is London, the currency is the Pound sterling and the leader is Elizabeth II. Boris Johnson is also a leader in the UK.</p>
<p>The capital of the United Kingdom is London, the currency is the Pound sterling and the country is lead by Elizabeth II and Boris Johnson.</p>
<p>Model Examples</p>
<p>Reference in this paper, a new array signal processing technique by using particle swarm optimization is proposed to identify multipath channel parameters. the proposed array signal processing technique provides estimates to the channel parameters by finding a global minimum of an optimization problem. since the optimization problem is formulated in the cross-ambiguity function (caf) domain of the transmitted signal and the received array outputs, the proposed array signal processing technique is called as particle swarm optimization. the performance of the particle swarm optimization is compared with the space alternating generalized expectation maximization technique and with another recently proposed pso based technique for various snr values. simulation results indicate the superior performance of the pso based technique over mentioned techniques for all snr values.</p>
<p>T5 this paper, we propose an array signal processing technique based on particle swarm optimization to estimate the multipath channel parameters from the received and the transmitted signal in a cross-ambiguity domain. the proposed array signal processing technique is formulated as an optimization problem and particle swarm optimization is compared with the generalized expectation maximization technique. simulation results show that the proposed particle swarm optimization outperforms the pso based technique for a wide range of snr values. in addition, the proposed particle swarm optimization is computationally more efficient than the gem based technique for small snr values.</p>
<p>BART in this paper, a new array signal processing technique based on particle swarm optimization is proposed. the proposed array signal processing technique is used to estimate the multipath channel parameters from the transmitted signal. the proposed array signal processing technique is formulated as an optimization problem in the cross-ambiguity domain. the particle swarm optimization is compared with the generalized expectation maximization technique. simulation results show that the proposed particle swarm optimization outperforms the pso based technique for all snr values. furthermore, the proposed particle swarm optimization is able to estimate the channel parameters more accurately than the generalized expectation maximization technique. Ribeiro et al. (2020) in this paper, a novel array signal processing technique based on particle swarm optimization is proposed to estimate the multipath channel parameters from the transmitted signal. the proposed array signal processing technique uses particle swarm optimization to estimate the multipath channel parameters. the proposed array signal processing technique is formulated as an optimization problem. simulation results show that the proposed array signal processing technique outperforms the conventional generalized expectation maximization technique and the pso based technique is robust to the snr values. </p>
<p>representation: ( feel :ARG0 ( we ) :ARG1 ( terrible :degree ( very ) ) :time ( now ) :ARG1-of ( cause :ARG0 ( have-rel-role :ARG0 we :ARG1 ( he ) :ARG2 ( child ) ) ) )</p>
<p>Figure 2 :Figure 3 :
23Example graph with 5 triples, from WebNLG dev linearized with the neutral separator tag, denoted •, (top left), its shuffled version (top right), texts generated with two fine-tuned versions of T5 small and a gold reference (bottom). Note that T5 can produce a reasonable text even when the input triples are shuffled randomly. Performance of BART base and T5 base in the dev set when experimenting with different amounts of training data.</p>
<p>seen T5 shuf seen T5 order unseen T5 shuf unseen</p>
<p>Figure 4 :
4chrF++ scores with respect to the number of triples for WebNLG seen and unseen test sets.</p>
<p>Table 2 :
2Results on WebNLG. A, S and U stand for all, seen, and unseen partitions of the test set, respectively.models for AMR. Following Wolf et al. (2019), we 
use the Adam optimizer (Kingma and Ba, 2015) </p>
<p>Table 2
2shows the results for the WebNLG test set. Neural pipeline models(Moryossef et al., 2019; Castro Ferreira et al., 2019)  achieve strong performance in the unseen dataset. On the otherModel 
BLEU 
M 
BT </p>
<p>Koncel et al. 2019 
14.30 18.80 
-
An (2019) 
15.10 19.50 
-
Schmitt et al. (2020) 17.33 21.43 
-
Ribeiro et al. (2020) 18.01 22.23 
-</p>
<p>BART base 
22.01 23.54 -13.02 
BART large 
23.65 25.19 -10.93 
T5 small 
20.22 21.62 -24.10 
T5 base 
20.73 21.88 -21.03 
T5 large 
22.15 23.73 -13.96 </p>
<p>with task-adaptive pretraining </p>
<p>BART large + LMA 
25.30 25.54 -08.79 
T5 large + LMA 
22.92 24.40 -10.39 </p>
<p>BART large + STA 
25.66 25.74 -08.97 
T5 large + STA 
23.69 24.92 -08.94 </p>
<p>Table 3 :
3Results on AGENDA test set. Bold (Italic) indicates best scores without (with) task-adaptive pretraining.</p>
<p>Table 3
3lists the results for the AGENDA test set. The models also show strong performance on thisModel 
AMR </p>
<p>F 
MS </p>
<p>Mager et al. (2020) 
5.69 A 
5.08 A 
Harkous et al. (2020) 
5.78 A 
5.47 AB 
T5 large 
6.55 B 
6.44 C 
BART large 
6.70 B 
5.72 BC 
Reference 
5.91 A 
-</p>
<p>Model 
WebNLG </p>
<p>F 
SA </p>
<p>Castro Ferreira et al. (2019) 5.52 A 
4.77 A 
Harkous et al. (2020) 
5.74 AB 6.21 B 
T5 large 
6.71 C 
6.63 B 
BART large 
6.53 C 
6.50 B 
Reference 
5.89 B 
6.47 B </p>
<p>Table 4 :
4Fluency (F), Meaning Similarity (MS) and Se-
mantic Adequacy (SA) obtained in the human evalua-
tion. Differences between models which have a letter in 
common are not statistically significant and were deter-
mined by pairwise Mann-Whitney tests with p &lt; 0.05. </p>
<p>Table 5 :
5Impact (measured with BLEU) of using a bag of entities and relations (shuf ) as input for T5 small .</p>
<p>Table 5
5shows the effect on T5's performance when its input contains correctly ordered triples (T5 order ) German language• Antwerp • Antwerp • Antwerp International Airport • Belgium • Belgium • Charles Michel • city Served • leader Name • Belgium • language • countryT/F 
Input Fact 
T5 order 
T5 shuf </p>
<p>(1) S • Antwerp International Airport serves 
the city of Antwerp. German is the 
language spoken in Belgium where 
Charles Michel is the leader. </p>
<p>Diarmuid Ó Séaghdha, and Anders Johannsen. 2020. Conversational semantic parsing for dialog state tracking. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8107-8117, Online. Association for Computational Linguistics.Sören Auer, Christian Bizer, Georgi Kobilarov, Jens 
Lehmann, Richard Cyganiak, and Zachary Ives. 
2007. Dbpedia: A nucleus for a web of open data. 
In Proceedings of the 6th International The Seman-
tic Web and 2nd Asian Conference on Asian Se-
mantic Web Conference, ISWC'07/ASWC'07, page 
722-735, Berlin, Heidelberg. Springer-Verlag. </p>
<p>Xuefeng Bai, Yulong Chen, Linfeng Song, and Yue 
Zhang. 2021. Semantic representation for dialogue 
modeling. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics 
and the 11th International Joint Conference on Nat-
ural Language Processing (Volume 1: Long Papers), 
pages 4430-4445, Online. Association for Computa-
tional Linguistics. </p>
<p>Laura Banarescu, Claire Bonial, Shu Cai, Madalina 
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin 
Knight, Philipp Koehn, Martha Palmer, and Nathan 
Schneider. 2013. Abstract Meaning Representation 
for sembanking. In Proceedings of the 7th Linguis-
tic Annotation Workshop and Interoperability with 
Discourse, pages 178-186, Sofia, Bulgaria. Associa-
tion for Computational Linguistics. </p>
<p>Daniel Beck, Gholamreza Haffari, and Trevor Cohn. 
2018. 
Graph-to-sequence learning using gated 
graph neural networks. In Proceedings of the 
56th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers), 
pages 273-283, Melbourne, Australia. Association 
for Computational Linguistics. </p>
<p>Anja Belz, Michael White, Dominic Espinosa, Eric 
Kow, Deirdre Hogan, and Amanda Stent. 2011. 
The first surface realisation shared task: Overview 
and evaluation results. In Proceedings of the 13th 
European Workshop on Natural Language Genera-
tion, pages 217-226, Nancy, France. Association for 
Computational Linguistics. </p>
<p>Claire Bonial, Lucia Donatelli, Mitchell Abrams, 
Stephanie M. Lukin, Stephen Tratz, Matthew Marge, 
Ron Artstein, David Traum, and Clare Voss. 2020. 
Dialogue-AMR: Abstract Meaning Representation 
for dialogue. In Proceedings of the 12th Lan-
guage Resources and Evaluation Conference, pages 
684-695, Marseille, France. European Language Re-
sources Association. </p>
<p>Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-
tanya Malaviya, Asli Celikyilmaz, and Yejin Choi. 
2019. COMET: Commonsense transformers for au-
tomatic knowledge graph construction. In Proceed-
ings of the 57th Annual Meeting of the Association 
for Computational Linguistics, pages 4762-4779, 
Florence, Italy. Association for Computational Lin-
guistics. </p>
<p>Deng Cai and Wai Lam. 2020a. AMR parsing via 
graph-sequence iterative inference. In Proceedings 
of the 58th Annual Meeting of the Association for 
Computational Linguistics, pages 1290-1301, On-
line. Association for Computational Linguistics. 
Deng Cai and Wai Lam. 2020b. Graph transformer for 
graph-to-sequence learning. In The Thirty-Fourth 
AAAI Conference on Artificial Intelligence, AAAI 
2020, The Thirty-Second Innovative Applications of 
Artificial Intelligence Conference, IAAI 2020, The 
Tenth AAAI Symposium on Educational Advances 
in Artificial Intelligence, EAAI 2020, New York, NY, 
USA, February 7-12, 2020, pages 7464-7471. AAAI 
Press. </p>
<p>Thiago Castro Ferreira, Chris van der Lee, Emiel 
van Miltenburg, and Emiel Krahmer. 2019. Neu-
ral data-to-text generation: A comparison between 
pipeline and end-to-end architectures. In Proceed-
ings of the 2019 Conference on Empirical Methods 
in Natural Language Processing and the 9th Interna-
tional Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP), pages 552-562, Hong 
Kong, China. Association for Computational Lin-
guistics. </p>
<p>Jianpeng Cheng, 
Devang Agrawal, 
Héctor 
Martínez Alonso, Shruti Bhargava, Joris Driesen, 
Federico Flego, Dain Kaplan, Dimitri Kartsaklis, 
Lin Li, Dhivya Piraviperumal, Jason D. Williams, 
Hong Yu, Marco Damonte and Shay B. Cohen. 2019. Structural 
neural encoders for AMR-to-text generation. In Pro-
ceedings of the 2019 Conference of the North Amer-
ican Chapter of the Association for Computational 
Linguistics: Human Language Technologies, Vol-
ume 1 (Long and Short Papers), pages 3649-3658, 
Minneapolis, Minnesota. Association for Computa-
tional Linguistics. </p>
<p>Michael Denkowski and Alon Lavie. 2014. Meteor uni-
versal: Language specific translation evaluation for 
any target language. In Proceedings of the Ninth 
Workshop on Statistical Machine Translation, pages 
376-380, Baltimore, Maryland, USA. Association 
for Computational Linguistics. </p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and 
Kristina Toutanova. 2019. BERT: Pre-training of 
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference 
of the North American Chapter of the Association 
for Computational Linguistics: Human Language 
Technologies, Volume 1 (Long and Short Papers), 
pages 4171-4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics. </p>
<p>Nan Duan, Duyu Tang, Peng Chen, and Ming Zhou. 
2017. Question generation for question answering. 
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages 
866-874, Copenhagen, Denmark. Association for 
Computational Linguistics. </p>
<p>Tobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie 
Utama, Ido Dagan, and Iryna Gurevych. 2019. 
Ranking generated summaries by correctness: An in-
teresting but challenging application for natural lan-
guage inference. In Proceedings of the 57th Annual 
Meeting of the Association for Computational Lin-
guistics, pages 2214-2220, Florence, Italy. Associa-
tion for Computational Linguistics. </p>
<p>Claire Gardent, Anastasia Shimorina, Shashi Narayan, 
and Laura Perez-Beltrachini. 2017. The WebNLG 
challenge: Generating text from RDF data. In Pro-
ceedings of the 10th International Conference on 
Natural Language Generation, pages 124-133, San-
tiago de Compostela, Spain. Association for Compu-
tational Linguistics. </p>
<p>Albert Gatt and Emiel Krahmer. 2018. Survey of the 
state of the art in natural language generation: Core 
tasks, applications and evaluation. Journal of Artifi-
cial Intelligence Research, 61(1):65-170. </p>
<p>Zhijiang Guo, Yan Zhang, Zhiyang Teng, and Wei 
Lu. 2019. Densely connected graph convolutional 
networks for graph-to-sequence learning. Transac-
tions of the Association for Computational Linguis-
tics, 7:297-312. </p>
<p>Suchin Gururangan, Ana Marasović, Swabha 
Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, 
and Noah A. Smith. 2020. Don't stop pretraining: 
Adapt language models to domains and tasks. In 
Proceedings of the 58th Annual Meeting of the 
Association for Computational Linguistics, pages 
8342-8360, Online. Association for Computational 
Linguistics. </p>
<p>Hamza Harkous, Isabel Groves, and Amir Saffari. 2020. 
Have your text and use it too! end-to-end neural 
data-to-text generation with semantic fidelity. In 
Proceedings of the 28th International Conference 
on Computational Linguistics, pages 2410-2424, 
Barcelona, Spain (Online). International Committee 
on Computational Linguistics. </p>
<p>Alexander Miserlis Hoyle, Ana Marasović, and 
Noah A. Smith. 2021. Promoting graph awareness 
in linearized graph-to-text generation. In Findings 
of the Association for Computational Linguistics: 
ACL-IJCNLP 2021, pages 944-956, Online. Asso-
ciation for Computational Linguistics. </p>
<p>Mihir Kale. 2020. Text-to-text pre-training for data-to-
text tasks. arXiv e-prints. </p>
<p>Diederik P. Kingma and Jimmy Ba. 2015. Adam: A 
method for stochastic optimization. In 3rd Inter-
national Conference on Learning Representations, 
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, 
Conference Track Proceedings. </p>
<p>Thomas N. Kipf and Max Welling. 2017. Semi-
Supervised Classification with Graph Convolutional 
Networks. In Proceedings of the 5th International 
Conference on Learning Representations, ICLR 
2017. </p>
<p>Table 7 :
7Results on the AMR dev set using T5 small for different AMR linearizations.</p>
<p>ble 8 shows the results using BART base and T5 base . While the texts in KGAIA and AGENDA share the domain of scientific abstracts, texts in WebNLG are more general. Also note that WebNLG graphs do not share any relations with the other KGs. For BART base , STA increases the performance in the cross-domain setting in most of the cases. For T5 base , STA in KGAIA improves the performance on WebNLG.In general, we find that exploring additional adaptive pretraining for graph-to-text generation can improve the performance even if the data do not come from the same domain.STA on 
Fine-tuned &amp; Evaluated on </p>
<p>WebNLG-Seen AGENDA 
BART base 
None 
58.71 
22.01 
KGAIA 
63.20 
23.48 
WebNLG 
-
21.98 
AGENDA 
61.25 
-
T5 base 
None 
62.93 
20.73 
KGAIA 
63.19 
22.44 
WebNLG 
-
20.27 
AGENDA 
62.75 
-</p>
<p>Table 8 :
8Effect (measured with BLEU score) of crossdomain STA.</p>
<p>Table 9 :
9Graph statistics of AMR, WebNLG and AGENDA datasets. The values are calculated using the training data. Note that AMR graphs contain a more complex structure than WebNLG and AGENDA graphs.the Levi transformation(Beck et al., 2018)  of the undirected version of the graphs, where edges are also considered nodes in the graph. WebNLG and AGENDA datasets contain disconnected graphs, and we use the largest subgraph to calculate the diameter. Note that AMR graphs have a much more complex structure: (i) they have more nodes and edges than WebNLG and AGENDA graphs; (ii) the average graph diameter and the average shortest path between nodes in AMRs are at least three times larger than in WebNLG and AGENDA graphs; (iii) nodes in AMRs have larger degrees than nodes in WebNLG and AGENDA graphs.AMR17 WebNLG AGENDA </p>
<h1>Train</h1>
<p>36,521 
18,102 
38,720 </p>
<h1>Dev</h1>
<p>1,368 
872 
1,000 </p>
<h1>Test</h1>
<p>1,371 
1,862 
1,000 </p>
<h1>Relations</h1>
<p>155 
373 
7 
Avg #Tokens 
16.1 
31.5 
157.9 </p>
<p>Table 10 :
10Statistics for the graph-to-text benchmarks.Title 
Abstract 
KG </p>
<p>Vocab 
48K 
173K 
113K 
Tokens 
2.1M 
31.7M 
9.6M 
Entities 
-
-
3.7M 
Avg Length 
11.1 
167.1 
-
Avg #Nodes 
-
-
19.9 
Avg #Edges 
-
-
9.4 </p>
<p>Table 11 :
11Statistics for the KGAIA dataset.Model 
chrF++ BS (F1) MS 
Schmitt et al. (2020) 44.53 
-
-
Ribeiro et al. (2020) 46.37 
-
-
BART base 
48.02 
89.36 
34.33 
BART large 
50.44 
88.74 
32.24 
T5 small 
44.91 
88.56 
30.25 
T5 base 
48.14 
88.81 
31.33 
T5 large 
48.14 
89.60 
35.23 </p>
<p>with task-adaptive pretraining </p>
<p>BART large + LMA 
51.33 
89.12 
33.42 
T5 large + LMA 
49.37 
89.75 
36.13 </p>
<p>BART large + STA 
51.63 
89.27 
34.28 
T5 large + STA 
50.27 
89.93 
36.86 </p>
<p>Table 12 :
12Results of the chrF++, BertScore (BS) and MoverScore (MS) scores for AGENDA test set. Bold (Italic) indicates best scores without (with) taskadaptive pretraining.Model 
chrF++ BS (F1) MS 
Guo et al. (2019) 
57.30 
-
-
Zhu et al. (2019) 
64.05 
-
-
Cai and Lam (2020b) 
59.40 
-
-
Wang et al. (2020) 
65.80 
-
-
Yao et al. (2020) 
65.60 
-
-</p>
<p>based on PLMs </p>
<p>Mager et al. (2020) 
63.89 
-
-
BART base 
66.65 95.22 60.78 
BART large 
71.06 96.08 65.74 
T5 small 
68.78 95.62 63.70 
T5 base 
70.81 95.99 65.63 
T5 large 
72.57 96.27 67.37 </p>
<p>with task-adaptive pretraining </p>
<p>BART large + LMA 
71.14 95.94 64.75 
T5 large + LMA 
72.83 96.32 67.44 </p>
<p>BART large + STA (200K) 72.26 96.21 66.75 
BART large + STA (2M) 
73.58 96.43 68.14 
T5 large + STA (200K) 
74.09 96.51 68.86 
T5 large + STA (2M) 
74.79 96.59 69.53 </p>
<p>Table 13 :
13Results of the chrF++, BertScore (BS) and MoverScore (MS) scores for the LDC2017T10 test set. Bold (Italic) indicates the best score without (with) task-adaptive pretraining.(1) S • capital • leader Name • London • Pound sterling • United Kingdom • leader Name • United Kingdom • Elizabeth II • United Kingdom • Boris Johnson • London • currencyT/F 
Input Facts 
T5 order 
T5 shuf </p>
<p>Table 16 :
16Examples of text generated by the different models trained on the AGENDA dataset.
Our code is available at https://github.com/UKPLab/plms-graph2text.
The model architecture does not explicitly encode the graph structure, i.e., which entities are connected to each other, but has to retrieve it from a sequence that tries to encode this information.
Please, refer to andRaffel et al. (2019)   for details about the self-supervised pretraining strategies.
Details of the preprocessing procedure of AMRs are provided in Appendix A. 5 https://catalog.ldc.upenn.edu/LDC2003T056 We filter out sentences that do not yield well-formed AMR graphs.7 Gigaword and AMR datasets share similar data sources.
We will release the collected additional task-specific data.
We exclude AGENDA because its texts are scientific in nature and annotators are not necessarily AI experts.
Table 15: Examples of text generated by the different models. D refers to the dataset.
AcknowledgmentsWe thank our anonymous reviewers for their thoughtful feedback. Leonardo F. R. Ribeiro is supported by the German Research Foundation (DFG) as part of the Research Training Group "Adaptive Preparation of Information form Heterogeneous Sources" (AIPHES, GRK 1994/1) and as part of the DFG funded project UKP-SQuARE with the number GU 798/29-1. Martin Schmitt is supported by the BMBF as part of the project MLWin (01IS18050) and by the German Academic Scholarship Foundation (Studienstiftung des deutschen Volkes).Andrews County Airport is located in Texas where Austin is the capital.Table 14: Example generations from shuffled (S), true (T), and corrupted (F) triple facts by T5 small , fine-tuned on correctly ordered triples (order) and randomly shuffled input (shuf ).D Model ExamplesAMRReference I had to deal with verbal abuse from my dad for a long 8 years before I came to uni and honestly, the only reason why I'm here is because it was the only way out. T5 I had to deal with 8 years of verbal abuse from my dad before coming to university and honestly the only reason I'm here is because it's the only way out. BART I had to deal with my dad's verbal abuse for 8 years long before coming to uni and honestly the only reason I'm here is because it's the only way out.Mager et al. (2020)i've had to deal with verbal abuse from my dad for 8 years (before i came to uni i was honestly the only reason i was here) and it's only because of the way it is.WebNLGReference Aaron Turner is an electric guitar player who has played with the black metal band Twilight and with Old Man Gloom. Death metal is a musical fusion of black metal. T5Aaron Turner plays the electric guitar and is associated with the band Twilight. He is also a member of the Old Man Gloom band. Black metal and death metal are both genres of music. BARTThe black metal genre is a variation of the death metal genre. It is also where the band, Twilight, and the alternative rock band, Old Man Gloom, are from. One of the members of the band is Aaron Turner, who plays the electric guitar. Harkous et al. (2020) Aaron Turner, a.k.a. Black Metal, is a member of theTwilight (band)and Old Man Gloom bands. He also plays electric guitar and has a strong connection with the Death Metal genre.
Construction of the literature graph in semantic scholar. Waleed Ammar, Dirk Groeneveld, Chandra Bhagavatula, Iz Beltagy, Miles Crawford, Doug Downey, Jason Dunkelberger, Ahmed Elgohary, Sergey Feldman, Vu Ha, Rodney Kinney, Sebastian Kohlmeier, Kyle Lo, Tyler Murray, Hsu-Han, Matthew Ooi, Joanna Peters, Sam Power, Lucy Skjonsberg, Chris Wang, Zheng Wilhelm, Madeleine Yuan, Oren Van Zuylen, Etzioni, 10.18653/v1/N18-3011Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans -Louisiana3Association for Computational LinguisticsWaleed Ammar, Dirk Groeneveld, Chandra Bhagavat- ula, Iz Beltagy, Miles Crawford, Doug Downey, Ja- son Dunkelberger, Ahmed Elgohary, Sergey Feld- man, Vu Ha, Rodney Kinney, Sebastian Kohlmeier, Kyle Lo, Tyler Murray, Hsu-Han Ooi, Matthew Pe- ters, Joanna Power, Sam Skjonsberg, Lucy Wang, Chris Wilhelm, Zheng Yuan, Madeleine van Zuylen, and Oren Etzioni. 2018. Construction of the litera- ture graph in semantic scholar. In Proceedings of the 2018 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers), pages 84-91, New Orleans -Louisiana. As- sociation for Computational Linguistics.</p>
<p>Repulsive bayesian sampling for diversified attention modeling. 4th workshop on Bayesian Deep Learning. Bang AnBang An. 2019. Repulsive bayesian sampling for di- versified attention modeling. In 4th workshop on Bayesian Deep Learning (NeurIPS 2019).</p>
<p>Text Generation from Knowledge Graphs with Graph Transformers. Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, Hannaneh Hajishirzi, 10.18653/v1/N19-1238Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, and Hannaneh Hajishirzi. 2019. Text Generation from Knowledge Graphs with Graph Transformers. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2284-2293, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Neural amr: Sequence-to-sequence models for parsing and generation. Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, Luke Zettlemoyer, 10.18653/v1/P17-1014Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational LinguisticsLong Papers)Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, and Luke Zettlemoyer. 2017. Neural amr: Sequence-to-sequence models for parsing and gener- ation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 146-157, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal ; Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational LinguisticsMike Lewis, Yinhan Liu, Naman Goyal, Mar- jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehension. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, Roberta: A robustly optimized bert pretraining approach. arXiv e-printsYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Roberta: A robustly optimized bert pretraining ap- proach. arXiv e-prints.</p>
<p>Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. Yi Luan, Luheng He, Mari Ostendorf, Hannaneh Hajishirzi, 10.18653/v1/D18-1360Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsYi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018. Multi-task identification of enti- ties, relations, and coreference for scientific knowl- edge graph construction. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan- guage Processing, pages 3219-3232, Brussels, Bel- gium. Association for Computational Linguistics.</p>
<p>GPT-too: A language-model-first approach for AMR-to-text generation. Manuel Mager, Ramón Fernandez Astudillo, Tahira Naseem, Arafat Md, Young-Suk Sultan, Radu Lee, Salim Florian, Roukos, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsManuel Mager, Ramón Fernandez Astudillo, Tahira Naseem, Md Arafat Sultan, Young-Suk Lee, Radu Florian, and Salim Roukos. 2020. GPT-too: A language-model-first approach for AMR-to-text gen- eration. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics, pages 1846-1852, Online. Association for Computa- tional Linguistics.</p>
<p>Deep graph convolutional encoders for structured data to text generation. Diego Marcheggiani, Laura Perez Beltrachini, Proceedings of the 11th International Conference on Natural Language Generation. the 11th International Conference on Natural Language GenerationTilburg UniversityAssociation for Computational LinguisticsThe NetherlandsDiego Marcheggiani and Laura Perez Beltrachini. 2018. Deep graph convolutional encoders for structured data to text generation. In Proceedings of the 11th International Conference on Natural Language Gen- eration, pages 1-9, Tilburg University, The Nether- lands. Association for Computational Linguistics.</p>
<p>OpenDialKG: Explainable conversational reasoning with attention-based walks over knowledge graphs. Seungwhan Moon, Pararth Shah, Anuj Kumar, Rajen Subba, 10.18653/v1/P19-1081Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsSeungwhan Moon, Pararth Shah, Anuj Kumar, and Ra- jen Subba. 2019. OpenDialKG: Explainable conver- sational reasoning with attention-based walks over knowledge graphs. In Proceedings of the 57th An- nual Meeting of the Association for Computational Linguistics, pages 845-854, Florence, Italy. Associ- ation for Computational Linguistics.</p>
<p>Step-by-step: Separating planning from realization in neural data-to-text generation. Amit Moryossef, Yoav Goldberg, Ido Dagan, 10.18653/v1/N19-1236Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaLong and Short Papers1Association for Computational LinguisticsAmit Moryossef, Yoav Goldberg, and Ido Dagan. 2019. Step-by-step: Separating planning from realization in neural data-to-text generation. In Proceedings of the 2019 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2267-2277, Minneapolis, Minnesota. Association for Computational Linguis- tics.</p>
<p>Bleu: A method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL '02. the 40th Annual Meeting on Association for Computational Linguistics, ACL '02Stroudsburg, PA, USAAssociation for Computational LinguisticsKishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: A method for automatic eval- uation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Com- putational Linguistics, ACL '02, pages 311-318, Stroudsburg, PA, USA. Association for Computa- tional Linguistics.</p>
<p>Association for Computational Linguistics. Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, 10.18653/v1/D19-1250Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaLanguage models as knowledge bases?Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowl- edge bases? In Proceedings of the 2019 Confer- ence on Empirical Methods in Natural Language Processing and the 9th International Joint Confer- ence on Natural Language Processing (EMNLP- IJCNLP), pages 2463-2473, Hong Kong, China. As- sociation for Computational Linguistics.</p>
<p>chrF: character n-gram F-score for automatic MT evaluation. Maja Popović, 10.18653/v1/W15-3049Proceedings of the Tenth Workshop on Statistical Machine Translation. the Tenth Workshop on Statistical Machine TranslationLisbon, PortugalAssociation for Computational LinguisticsMaja Popović. 2015. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392-395, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Smelting gold and silver for improved multilingual amr-to-text generation. F R Leonardo, Jonas Ribeiro, Yue Pfeiffer, Iryna Zhang, Gurevych, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingPunta Cana2021Leonardo F. R. Ribeiro, Jonas Pfeiffer, Yue Zhang, and Iryna Gurevych. 2021a. Smelting gold and silver for improved multilingual amr-to-text generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Punta Cana, November 7-11, 2021.</p>
<p>Claire Gardent, and Iryna Gurevych. 2020. Modeling global and local node contexts for text generation from knowledge graphs. F R Leonardo, Yue Ribeiro, Zhang, 10.1162/tacl_a_00332Transactions of the Association for Computational Linguistics. 8Leonardo F. R. Ribeiro, Yue Zhang, Claire Gardent, and Iryna Gurevych. 2020. Modeling global and local node contexts for text generation from knowl- edge graphs. Transactions of the Association for Computational Linguistics, 8:589-604.</p>
<p>Structural adapters in pretrained language models for amr-to-text generation. F R Leonardo, Yue Ribeiro, Iryna Zhang, Gurevych, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingPunta Cana2021Leonardo F. R. Ribeiro, Yue Zhang, and Iryna Gurevych. 2021b. Structural adapters in pretrained language models for amr-to-text generation. In Pro- ceedings of the 2021 Conference on Empirical Meth- ods in Natural Language Processing, EMNLP 2021, Punta Cana, November 7-11, 2021.</p>
<p>Modeling graph structure via relative position for text generation from knowledge graphs. Martin Schmitt, Leonardo F R Ribeiro, Philipp Dufter, Iryna Gurevych, Hinrich Schütze, Proceedings of the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-15). the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-15)Mexico City, MexicoAssociation for Computational LinguisticsMartin Schmitt, Leonardo F. R. Ribeiro, Philipp Dufter, Iryna Gurevych, and Hinrich Schütze. 2021. Mod- eling graph structure via relative position for text generation from knowledge graphs. In Proceedings of the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-15), pages 10-21, Mexico City, Mexico. Association for Computational Linguistics.</p>
<p>Martin Schmitt, Leonardo F R Ribeiro, Philipp Dufter, Iryna Gurevych, and Hinrich Schütze. 2020. Modeling graph structure via relative position for better text generation from knowledge graphs. arXiv eprintsMartin Schmitt, Leonardo F. R. Ribeiro, Philipp Dufter, Iryna Gurevych, and Hinrich Schütze. 2020. Mod- eling graph structure via relative position for better text generation from knowledge graphs. arXiv e- prints.</p>
<p>BLEURT: Learning robust metrics for text generation. Thibault Sellam, Dipanjan Das, Ankur Parikh, 10.18653/v1/2020.acl-main.704Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational LinguisticsThibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics, pages 7881-7892, Online. Association for Computa- tional Linguistics.</p>
<p>A graph-to-sequence model for AMRto-text generation. Linfeng Song, Yue Zhang, Zhiguo Wang, Daniel Gildea, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaLong Papers1Association for Computational LinguisticsLinfeng Song, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2018. A graph-to-sequence model for AMR- to-text generation. In Proceedings of the 56th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1616- 1626, Melbourne, Australia. Association for Compu- tational Linguistics.</p>
<p>The CoNLL 2008 shared task on joint parsing of syntactic and semantic dependencies. Mihai Surdeanu, Richard Johansson, Adam Meyers, Lluís Màrquez, Joakim Nivre, CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning. Manchester, England. ColingMihai Surdeanu, Richard Johansson, Adam Meyers, Lluís Màrquez, and Joakim Nivre. 2008. The CoNLL 2008 shared task on joint parsing of syn- tactic and semantic dependencies. In CoNLL 2008: Proceedings of the Twelfth Conference on Computa- tional Natural Language Learning, pages 159-177, Manchester, England. Coling 2008 Organizing Com- mittee.</p>
<p>GTR-LSTM: A triple encoder for sentence generation from RDF data. Jianzhong Bayu Distiawan Trisedya, Rui Qi, Wei Zhang, Wang, 10.18653/v1/P18-1151Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, Australia1Association for Computational LinguisticsBayu Distiawan Trisedya, Jianzhong Qi, Rui Zhang, and Wei Wang. 2018. GTR-LSTM: A triple encoder for sentence generation from RDF data. In Proceed- ings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa- pers), pages 1627-1637, Melbourne, Australia. As- sociation for Computational Linguistics.</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Illia Kaiser, Polosukhin, Advances in Neural Information Processing Systems. I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. GarnettCurran Associates, Inc30Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar- nett, editors, Advances in Neural Information Pro- cessing Systems 30, pages 5998-6008. Curran Asso- ciates, Inc.</p>
<p>Neural wikipedian: Generating textual summaries from knowledge base triples. Pavlos Vougiouklis, Hady Elsahar, Lucie-Aimée Kaffee, Christophe Gravier, Frédérique Laforest, Jonathon Hare, Elena Simperl, 10.1016/j.websem.2018.07.002Journal of Web Semantics. Pavlos Vougiouklis, Hady Elsahar, Lucie-Aimée Kaffee, Christophe Gravier, Frédérique Laforest, Jonathon Hare, and Elena Simperl. 2018. Neu- ral wikipedian: Generating textual summaries from knowledge base triples. Journal of Web Semantics, 52-53:1 -15.</p>
<p>Entity, relation, and event extraction with contextualized span representations. David Wadden, Ulme Wennberg, Yi Luan, Hannaneh Hajishirzi, 10.18653/v1/D19-1585Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsDavid Wadden, Ulme Wennberg, Yi Luan, and Han- naneh Hajishirzi. 2019. Entity, relation, and event extraction with contextualized span representations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan- guage Processing (EMNLP-IJCNLP), pages 5784- 5789, Hong Kong, China. Association for Computa- tional Linguistics.</p>
<p>Amr-to-text generation with graph transformer. Tianming Wang, Xiaojun Wan, Hanqi Jin, 10.1162/tacl_a_00297Transactions of the Association for Computational Linguistics. 8Tianming Wang, Xiaojun Wan, and Hanqi Jin. 2020. Amr-to-text generation with graph transformer. Transactions of the Association for Computational Linguistics, 8:19-33.</p>
<p>Challenges in data-to-document generation. Sam Wiseman, Stuart Shieber, Alexander Rush, 10.18653/v1/D17-1239Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational LinguisticsSam Wiseman, Stuart Shieber, and Alexander Rush. 2017. Challenges in data-to-document generation. In Proceedings of the 2017 Conference on Empiri- cal Methods in Natural Language Processing, pages 2253-2263, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Huggingface's transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie BrewThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, Rémi Louf, Morgan Funtow- icz, and Jamie Brew. 2019. Huggingface's trans- formers: State-of-the-art natural language process- ing.</p>
<p>Xlnet: Generalized autoregressive pretraining for language understanding. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, R Russ, Quoc V Salakhutdinov, Le, Advances in Neural Information Processing Systems. Curran Associates, Inc32Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car- bonell, Russ R Salakhutdinov, and Quoc V Le. 2019a. Xlnet: Generalized autoregressive pretrain- ing for language understanding. In Advances in Neural Information Processing Systems, volume 32, pages 5753-5763. Curran Associates, Inc.</p>
<p>Xlnet: Generalized autoregressive pretraining for language understanding. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, R Russ, Quoc V Salakhutdinov, Le, Advances in Neural Information Processing Systems. H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. GarnettCurran Associates, Inc32Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car- bonell, Russ R Salakhutdinov, and Quoc V Le. 2019b. Xlnet: Generalized autoregressive pre- training for language understanding. In H. Wal- lach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neu- ral Information Processing Systems 32, pages 5753- 5763. Curran Associates, Inc.</p>
<p>Heterogeneous graph transformer for graphto-sequence learning. Shaowei Yao, Tianming Wang, Xiaojun Wan, 10.18653/v1/2020.acl-main.640Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational LinguisticsShaowei Yao, Tianming Wang, and Xiaojun Wan. 2020. Heterogeneous graph transformer for graph- to-sequence learning. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics, pages 7145-7154, Online. Association for Computational Linguistics.</p>
<p>CoSQL: A conversational text-to-SQL challenge towards crossdomain natural language interfaces to databases. Tao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue, Bo Pang, Victoria Xi, Yi Lin, Tianze Chern Tan, Zihan Shi, Youxuan Li, Michihiro Jiang, Sungrok Yasunaga, Tao Shim, Alexander Chen, Zifan Fabbri, Luyao Li, Yuwen Chen, Shreya Zhang, Vincent Dixit, Caiming Zhang, Richard Xiong, Socher, 10.18653/v1/D19-1204Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaWalter Lasecki, and Dragomir Radev. Association for Computational LinguisticsTao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue, Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze Shi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga, Sungrok Shim, Tao Chen, Alexander Fabbri, Zifan Li, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vin- cent Zhang, Caiming Xiong, Richard Socher, Wal- ter Lasecki, and Dragomir Radev. 2019. CoSQL: A conversational text-to-SQL challenge towards cross- domain natural language interfaces to databases. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan- guage Processing (EMNLP-IJCNLP), pages 1962- 1979, Hong Kong, China. Association for Computa- tional Linguistics.</p>
<p>Bertscore: Evaluating text generation with bert. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, International Conference on Learning Representations. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Eval- uating text generation with bert. In International Conference on Learning Representations.</p>
<p>Bridging the structural gap between encoding and decoding for data-to-text generation. Chao Zhao, Marilyn Walker, Snigdha Chaturvedi, 10.18653/v1/2020.acl-main.224Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsChao Zhao, Marilyn Walker, and Snigdha Chaturvedi. 2020a. Bridging the structural gap between encod- ing and decoding for data-to-text generation. In Pro- ceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 2481- 2491, Online. Association for Computational Lin- guistics.</p>
<p>MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance. Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, Steffen Eger, 10.18653/v1/D19-1053Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris- tian M. Meyer, and Steffen Eger. 2019. MoverScore: Text generation evaluating with contextualized em- beddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna- tional Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), pages 563-578, Hong Kong, China. Association for Computational Lin- guistics.</p>
<p>Line graph enhanced AMR-to-text generation with mix-order graph attention networks. Yanbin Zhao, Lu Chen, Zhi Chen, Ruisheng Cao, Su Zhu, Kai Yu, 10.18653/v1/2020.acl-main.67Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational LinguisticsYanbin Zhao, Lu Chen, Zhi Chen, Ruisheng Cao, Su Zhu, and Kai Yu. 2020b. Line graph enhanced AMR-to-text generation with mix-order graph at- tention networks. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics, pages 732-741, Online. Association for Computational Linguistics.</p>
<p>Modeling graph structure in transformer for better AMR-to-text generation. Jie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min Zhang, Guodong Zhou, 10.18653/v1/D19-1548Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsJie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min Zhang, and Guodong Zhou. 2019. Modeling graph structure in transformer for better AMR-to-text gen- eration. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP), pages 5459-5468, Hong Kong, China. Association for Computational Linguistics.</p>
<p>. T • Germany, • capital • Berlin Berlin is the capital of Germany. Berlin is the capital of Germany. T • Germany • capital • Berlin Berlin is the capital of Germany. Berlin is the capital of Germany.</p>
<p>• capital • Germany Berlin's capital is Germany. Berlin is the capital of Germany. F • Berlin, F • Berlin • capital • Germany Berlin's capital is Germany. Berlin is the capital of Germany.</p>
<p>. F • Leinster, • is Part Of • Dublin Leinster is part of Dublin. Leinster is part of Dublin. F • Leinster • is Part Of • Dublin Leinster is part of Dublin. Leinster is part of Dublin.</p>
<p>• capital • Italy Rome's capital is Italy. Rome is the capital of Italy. F • Rome, F • Rome • capital • Italy Rome's capital is Italy. Rome is the capital of Italy.</p>
<p>• capital • Rome Italy's capital is Rome. Rome is the capital of Italy. T • Italy, T • Italy • capital • Rome Italy's capital is Rome. Rome is the capital of Italy.</p>
<p>Andrews County Airport • location • Texas Austin is the capital of Texas where Andrews County Airport is located. T Austin, T • Texas • capital • Austin • Andrews County Airport • location • Texas Austin is the capital of Texas where Andrews County Airport is located.</p>
<p>Austin is the capital of Texas where Andrews County Airport is located. Austin is the capital of Texas where Andrews County Airport is located.</p>
<p>• capital • Texas • Andrews County Airport • location • Texas The capital of Austin is Texas and Andrews County Airport is. F • Austin, TexasF • Austin • capital • Texas • Andrews County Airport • location • Texas The capital of Austin is Texas and Andrews County Airport is located in Texas.</p>            </div>
        </div>

    </div>
</body>
</html>
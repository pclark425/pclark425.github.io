<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9674 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9674</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9674</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-957afdde0cf5f812d4cd1534f066414eaf088f61</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/957afdde0cf5f812d4cd1534f066414eaf088f61" target="_blank">FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> FreeEval is introduced, a modular framework not only for conducting trustworthy and efficient automatic evaluations of LLMs but also serving as a platform to develop and validate new evaluation methodologies.</p>
                <p><strong>Paper Abstract:</strong> The rapid growth of evaluation methodologies and datasets for large language models (LLMs) has created a pressing need for their unified integration. Meanwhile, concerns about data contamination and bias compromise the trustworthiness of evaluation findings, while the efficiency of evaluation processes remains a bottleneck due to the significant computational costs associated with LLM inference.In response to these challenges, we introduce FreeEval, a modular framework not only for conducting trustworthy and efficient automatic evaluations of LLMs but also serving as a platform to develop and validate new evaluation methodologies. FreeEval addresses key challenges through: (1) unified abstractions that simplify the integration of diverse evaluation methods, including dynamic evaluations requiring complex LLM interactions; (2) built-in meta-evaluation techniques such as data contamination detection and human evaluation to enhance result fairness; (3) a high-performance infrastructure with distributed computation and caching strategies for efficient large-scale evaluations; and (4) an interactive Visualizer for result analysis and interpretation to support innovation of evaluation techniques. We open-source all our code at https://github.com/WisdomShell/FreeEval and our demostration video, live demo, installation guides are available at: https://freeeval.zhuohao.me/.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9674.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9674.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Min-K% Prob</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Min-K Percent Probability (Min-K% Prob)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contamination-detection statistic that examines the minimum top-K token probability or related scoring over dataset instances to identify examples likely seen during model pretraining, used to flag potential test-set leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Detecting pretraining data from large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>LLaMA-2 (examples in paper used llama-2-7b-chat and llama-2-70b)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Variants of Meta's LLaMA-2 family; the paper reports experiments with llama-2-7b-chat for accuracy comparisons and shows configs for llama-2-70b in example pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General LLM evaluation (natural language tasks); not specific to a single scientific discipline</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automatically compute Min-K% Prob per example as a meta-evaluation step to detect pretraining/test contamination prior to reporting downstream evaluation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Higher Min-K% Prob signals that an example may have been memorized or present in pretraining; used as a binary/score cue to question the validity of downstream performance numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Integrated as a step applicable to any dataset (example config shows ARC-Challenge used in pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>FreeEval implements Min-K% Prob as a built-in contamination detection module and demonstrates it as a configurable pipeline step (example shown in Figure 2/config).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Min-K% Prob is a heuristic and requires calibration per model and dataset; detection is suggestive rather than definitive and may produce false positives/negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Automated contamination detection complements but does not replace human inspection; it is presented as a meta-evaluation tool to increase trustworthiness of automated benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Run contamination detection (e.g., Min-K% Prob) as a standard pre-processing meta-evaluation step in any benchmark evaluation pipeline to avoid inflated performance claims.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9674.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9674.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Average Loss (contamination)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Average Loss over Dataset (used for contamination detection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contamination detection approach that computes models' average loss (or perplexity) on test items to identify unusually low losses that may indicate exposure during pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>LLaMA-2 (paper examples: llama-2-7b-chat, llama-2-70b)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>See LLaMA-2 entry above; used as evaluated models in FreeEval pipelines and examples.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General LLM evaluation (NLP tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute average loss/perplexity of the evaluated model on the target dataset as a meta-evaluation signal to detect memorization/contamination.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Low average loss or abnormally low perplexity on test instances relative to expectations suggests possible contamination.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applicable to any dataset (paper mentions using GSM8K example and many standard benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>FreeEval includes average loss as a contamination-detection module; users can run it as a step in evaluation pipelines to flag suspect examples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Average loss can be influenced by model capacity, domain mismatch, and formatting; it requires baselines and care to interpret.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Automated loss-based checks provide scalable detection compared to manual inspection, but should be combined with other meta-evaluation signals and human review.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use average-loss checks alongside other contamination detectors (e.g., Min-K% Prob) and follow up flagged cases with manual analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9674.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9674.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Evaluation (pairwise/direct)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human Preference Evaluation (Pairwise Comparison and Direct Scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human annotations capturing preferences over model outputs via pairwise comparisons or direct scores; treated as the gold standard for subjective quality and meta-evaluation of automated judges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A survey on evaluation of large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Not an LLM; human annotators are used to judge LLM outputs (FreeEval supports collecting human judgments for any model outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General evaluation of LLM outputs (human-likeness, preference, quality) across NLP tasks</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Set up human annotation sessions via FreeEval Visualizer to collect pairwise comparisons or direct scores on outputs; use these annotations to calibrate or validate automatic evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Human preference, perceived quality, correctness, fluency, relevance â€” captured either as pairwise wins or scalar ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>FreeEval ships with meta-evaluation datasets from PandaLM, MT-Bench, LLMBar, AlpacaEval and allows creating new preference datasets via the interface.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>FreeEval includes a human-evaluation interface (creation, management, annotation) and integrates existing human-preference datasets as meta-evaluation resources.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Human annotation is costly, subjective, and can be inconsistent without careful protocol design; subjectivity can introduce bias into meta-evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Human evaluation is explicitly stated as the gold standard and is used to validate automated LLM-based evaluators and metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use standardized human annotation protocols (pairwise or direct scoring), provide clear instructions, and combine human judgments with automated meta-evaluation checks to ensure fairness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9674.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9674.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-Based Evaluators (PandaLM, MT-Bench, AlpacaEval, GPTScore, PRD, KIEval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automated evaluators that use strong LLMs (e.g., GPT-4) or curated evaluation prompts to judge outputs of other models, capturing subjective qualities beyond reference-based metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4 and other strong LLMs (referenced generically); FreeEval integrates multiple LLM-judge designs and can call proprietary models as judges.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Judge LLMs like GPT-4 are high-capacity instruction-following models used to score or compare outputs; FreeEval supports both open-source and proprietary judge LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General LLM output evaluation across multiple NLP tasks</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Deploy a judge LLM with carefully designed prompts (or multi-agent debate protocols) to rank, score, or compare candidate outputs; integrate as an evaluation step.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Subjective judgments such as overall quality, correctness, helpfulness; specific rubric depends on the judge prompt and dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>FreeEval integrates existing LLM-evaluator datasets/frameworks: PandaLM, MT-Bench, AlpacaEval, KIEval, and can run these judge protocols on target datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>FreeEval unifies and provides modular implementations of LLM-based evaluators and supports meta-evaluation (bias checks, human validation) for these judges.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Evaluator LLMs introduce evaluator bias (position bias, length bias) and their judgments depend on prompts; they require meta-evaluation and human calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>LLM-based evaluators can approximate human judgments and scale better, but the paper emphasizes validating them against human preference datasets and using meta-evaluation to detect bias.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Always meta-evaluate LLM-based judges (e.g., with human annotations and bias checks), document prompting and settings, and include these judges as configurable steps in evaluation pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9674.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9674.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reference-based metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Classic Reference-Based Metrics (BLEU, ROUGE, BERTScore, BARTScore, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automated metrics that compare model-generated text against human references to score overlap (BLEU/ROUGE) or semantic similarity using pretrained encoders (BERTScore/BARTScore).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural language generation evaluation across tasks like summarization, translation, and open-ended generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute standard overlap or embedding-based similarity metrics between generated outputs and reference texts as an automated quality signal.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>n-gram overlap (BLEU/ROUGE), learned semantic similarity (BERTScore), and other reference-based scores reflect closeness to references but may miss open-ended qualities.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to classic reference-based tasks; FreeEval supports ROUGE and BERTScore as integrated evaluation steps.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>FreeEval incorporates classic reference-based metrics (ROUGE, BERTScore, etc.) as part of the modular evaluation toolkit to provide baseline automated assessments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>These metrics do not capture open-endedness, creativity, or many subjective qualities; sensitive to reference diversity and quality.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Reference metrics are straightforward and reproducible but often correlate poorly with human judgments on open-ended generation; FreeEval recommends supplementing them with human or LLM-based evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use reference metrics for tasks with good references, but validate with human judgments or LLM-based evaluators for open-ended tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9674.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9674.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Benchmarks (ARC/MMLU/HellaSwag/CEval/GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Standard Dataset Benchmarks (ARC-Challenge, MMLU, HellaSwag, CEval, GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Well-known benchmark datasets used to evaluate different capabilities of LLMs: reasoning (ARC), broad multi-task knowledge (MMLU), commonsense completion (HellaSwag), multi-discipline Chinese evaluation (CEval), and math reasoning (GSM8K).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>llama-2-7b-chat (reported in Table 2) and LLaMA-2-70B (example config)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Versions of LLaMA-2 used to demonstrate how inference settings and prompts affect benchmark scores.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP benchmark evaluation (reasoning, knowledge, commonsense, mathematics)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Run few-shot or prompt-based evaluation (e.g., 25-shot on ARC-Challenge) and report standard metrics such as accuracy on multiple-choice tasks or task-specific scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Task accuracy (percent correct) on benchmark test sets; choice of prompt template (CP/MCP) and few-shot examples influences results.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>ARC-Challenge: reasoning QA; MMLU: massive multitask language understanding; HellaSwag: commonsense completion; CEval: Chinese multi-discipline evaluation; GSM8K: math word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Table 2 demonstrates that prompting style and prompt templates substantially change measured accuracy (e.g., llama-2-7b-chat 25-shot ARC-C accuracy ranged from 47.53% to 54.18% across prompt variants). FreeEval uses these benchmarks in demonstration pipelines and supports config-driven experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Benchmarks can be contaminated by pretraining data, prompting and evaluation settings can drastically affect scores, and single-number summaries hide variability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Benchmarks provide objective, repeatable metrics but must be combined with contamination checks and human inspection to ensure validity of claims.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Standardize and document prompting, few-shot selection, and evaluation configs; run contamination detection prior to reporting scores; use multiple prompt variants and meta-evaluation to assess robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9674.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9674.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompting / Inference Implementation Effects</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt Type and Inference Implementation Effects (Cloze Prompt CP, Multiple Choice Prompt MCP, PromptA/PromptB)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical observation that different prompt templates and choice of cloze vs. multiple-choice prompting produce materially different evaluation outcomes on the same model/dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Leveraging large language models for multiple choice question answering.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>llama-2-7b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>A 7B-parameter chat-tuned variant of Llama-2 used in the paper's benchmarking examples.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP evaluation methodology</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compare model accuracy under different prompting strategies (e.g., CP+PromptA vs MCP+PromptA) on benchmarks like ARC, MMLU, HellaSwag.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Downstream task accuracy (%) as reported in Table 2 for each prompt/inference implementation combination.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>ARC-Challenge, MMLU, HellaSwag (results shown in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Table 2 shows substantial variability: e.g., MCP+PromptA yields 54.18% on ARC-C vs CP+PromptB 47.53%; HellaSwag accuracy varied widely depending on choice of CP vs MCP and prompt variant.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Prompt design and selection of few-shot examples are sources of variance and can confound comparisons across models if not standardized.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>This is an automated experimental observation; human evaluation cannot substitute but can validate whether prompt-induced score changes correspond to real quality differences.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Standardize prompt templates and document all inference settings; include multiple prompting strategies in evaluations and report sensitivity to prompting as part of results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9674.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9674.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Efficiency / Runtime Benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inference Efficiency Benchmarks (Toolkit runtime comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical runtime comparisons across evaluation toolkits showing FreeEval's concurrent inference yields substantial speedups on single-GPU runs for common benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>llama-2-7b-chat (used in comparative timing experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>7B-chat variant evaluated on ARC-C, MMLU, HellaSwag to compare execution times across toolkits.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Systems/infrastructure evaluation for LLM benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Measure wall-clock execution time (hours) for running full evaluations on ARC-C, MMLU, HellaSwag using different toolkits on the same hardware (single NVIDIA A800 80GB GPU).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Total execution time (hours) to complete the benchmark runs; concurrency/sequential modes compared.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>ARC-C, MMLU, HellaSwag (as used in Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Table 3: FreeEval (Concurrent) achieves 0.067h on ARC-C, 0.233h on MMLU, 0.357h on HellaSwag, outperforming Eval-Harness and OpenCompass on the same machine; FreeEval (Sequential) numbers are also provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Reported times exclude download times and are specific to the hardware and model used; results depend on backend tuning (TGI, batching, caching).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>This is an infrastructure-level comparison; human factors are not directly relevant here.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use concurrent inference, caching, and high-performance inference backends (e.g., Huggingface TGI) to reduce evaluation costs; report runtime and hardware to enable reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Detecting pretraining data from large language models. <em>(Rating: 2)</em></li>
                <li>NLP evaluation in trouble: On the need to measure llm data contamination for each benchmark. <em>(Rating: 2)</em></li>
                <li>Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. <em>(Rating: 2)</em></li>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena. <em>(Rating: 2)</em></li>
                <li>AlpacaEval: An Automatic Evaluator of Instruction-following Models <em>(Rating: 2)</em></li>
                <li>Gptscore: Evaluate as you desire. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9674",
    "paper_id": "paper-957afdde0cf5f812d4cd1534f066414eaf088f61",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "Min-K% Prob",
            "name_full": "Min-K Percent Probability (Min-K% Prob)",
            "brief_description": "A contamination-detection statistic that examines the minimum top-K token probability or related scoring over dataset instances to identify examples likely seen during model pretraining, used to flag potential test-set leakage.",
            "citation_title": "Detecting pretraining data from large language models.",
            "mention_or_use": "use",
            "llm_name": "LLaMA-2 (examples in paper used llama-2-7b-chat and llama-2-70b)",
            "llm_description": "Variants of Meta's LLaMA-2 family; the paper reports experiments with llama-2-7b-chat for accuracy comparisons and shows configs for llama-2-70b in example pipelines.",
            "scientific_domain": "General LLM evaluation (natural language tasks); not specific to a single scientific discipline",
            "evaluation_method": "Automatically compute Min-K% Prob per example as a meta-evaluation step to detect pretraining/test contamination prior to reporting downstream evaluation metrics.",
            "evaluation_criteria": "Higher Min-K% Prob signals that an example may have been memorized or present in pretraining; used as a binary/score cue to question the validity of downstream performance numbers.",
            "benchmark_or_dataset": "Integrated as a step applicable to any dataset (example config shows ARC-Challenge used in pipeline).",
            "results_summary": "FreeEval implements Min-K% Prob as a built-in contamination detection module and demonstrates it as a configurable pipeline step (example shown in Figure 2/config).",
            "limitations_or_challenges": "Min-K% Prob is a heuristic and requires calibration per model and dataset; detection is suggestive rather than definitive and may produce false positives/negatives.",
            "comparison_to_human_or_traditional": "Automated contamination detection complements but does not replace human inspection; it is presented as a meta-evaluation tool to increase trustworthiness of automated benchmarks.",
            "recommendations_or_best_practices": "Run contamination detection (e.g., Min-K% Prob) as a standard pre-processing meta-evaluation step in any benchmark evaluation pipeline to avoid inflated performance claims.",
            "uuid": "e9674.0",
            "source_info": {
                "paper_title": "FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Average Loss (contamination)",
            "name_full": "Average Loss over Dataset (used for contamination detection)",
            "brief_description": "A contamination detection approach that computes models' average loss (or perplexity) on test items to identify unusually low losses that may indicate exposure during pretraining.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "LLaMA-2 (paper examples: llama-2-7b-chat, llama-2-70b)",
            "llm_description": "See LLaMA-2 entry above; used as evaluated models in FreeEval pipelines and examples.",
            "scientific_domain": "General LLM evaluation (NLP tasks)",
            "evaluation_method": "Compute average loss/perplexity of the evaluated model on the target dataset as a meta-evaluation signal to detect memorization/contamination.",
            "evaluation_criteria": "Low average loss or abnormally low perplexity on test instances relative to expectations suggests possible contamination.",
            "benchmark_or_dataset": "Applicable to any dataset (paper mentions using GSM8K example and many standard benchmarks).",
            "results_summary": "FreeEval includes average loss as a contamination-detection module; users can run it as a step in evaluation pipelines to flag suspect examples.",
            "limitations_or_challenges": "Average loss can be influenced by model capacity, domain mismatch, and formatting; it requires baselines and care to interpret.",
            "comparison_to_human_or_traditional": "Automated loss-based checks provide scalable detection compared to manual inspection, but should be combined with other meta-evaluation signals and human review.",
            "recommendations_or_best_practices": "Use average-loss checks alongside other contamination detectors (e.g., Min-K% Prob) and follow up flagged cases with manual analysis.",
            "uuid": "e9674.1",
            "source_info": {
                "paper_title": "FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Human Evaluation (pairwise/direct)",
            "name_full": "Human Preference Evaluation (Pairwise Comparison and Direct Scoring)",
            "brief_description": "Human annotations capturing preferences over model outputs via pairwise comparisons or direct scores; treated as the gold standard for subjective quality and meta-evaluation of automated judges.",
            "citation_title": "A survey on evaluation of large language models.",
            "mention_or_use": "use",
            "llm_name": "Not an LLM; human annotators are used to judge LLM outputs (FreeEval supports collecting human judgments for any model outputs).",
            "llm_description": null,
            "scientific_domain": "General evaluation of LLM outputs (human-likeness, preference, quality) across NLP tasks",
            "evaluation_method": "Set up human annotation sessions via FreeEval Visualizer to collect pairwise comparisons or direct scores on outputs; use these annotations to calibrate or validate automatic evaluators.",
            "evaluation_criteria": "Human preference, perceived quality, correctness, fluency, relevance â€” captured either as pairwise wins or scalar ratings.",
            "benchmark_or_dataset": "FreeEval ships with meta-evaluation datasets from PandaLM, MT-Bench, LLMBar, AlpacaEval and allows creating new preference datasets via the interface.",
            "results_summary": "FreeEval includes a human-evaluation interface (creation, management, annotation) and integrates existing human-preference datasets as meta-evaluation resources.",
            "limitations_or_challenges": "Human annotation is costly, subjective, and can be inconsistent without careful protocol design; subjectivity can introduce bias into meta-evaluation.",
            "comparison_to_human_or_traditional": "Human evaluation is explicitly stated as the gold standard and is used to validate automated LLM-based evaluators and metrics.",
            "recommendations_or_best_practices": "Use standardized human annotation protocols (pairwise or direct scoring), provide clear instructions, and combine human judgments with automated meta-evaluation checks to ensure fairness.",
            "uuid": "e9674.2",
            "source_info": {
                "paper_title": "FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LLM-based evaluators",
            "name_full": "LLM-Based Evaluators (PandaLM, MT-Bench, AlpacaEval, GPTScore, PRD, KIEval)",
            "brief_description": "Automated evaluators that use strong LLMs (e.g., GPT-4) or curated evaluation prompts to judge outputs of other models, capturing subjective qualities beyond reference-based metrics.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "GPT-4 and other strong LLMs (referenced generically); FreeEval integrates multiple LLM-judge designs and can call proprietary models as judges.",
            "llm_description": "Judge LLMs like GPT-4 are high-capacity instruction-following models used to score or compare outputs; FreeEval supports both open-source and proprietary judge LLMs.",
            "scientific_domain": "General LLM output evaluation across multiple NLP tasks",
            "evaluation_method": "Deploy a judge LLM with carefully designed prompts (or multi-agent debate protocols) to rank, score, or compare candidate outputs; integrate as an evaluation step.",
            "evaluation_criteria": "Subjective judgments such as overall quality, correctness, helpfulness; specific rubric depends on the judge prompt and dataset.",
            "benchmark_or_dataset": "FreeEval integrates existing LLM-evaluator datasets/frameworks: PandaLM, MT-Bench, AlpacaEval, KIEval, and can run these judge protocols on target datasets.",
            "results_summary": "FreeEval unifies and provides modular implementations of LLM-based evaluators and supports meta-evaluation (bias checks, human validation) for these judges.",
            "limitations_or_challenges": "Evaluator LLMs introduce evaluator bias (position bias, length bias) and their judgments depend on prompts; they require meta-evaluation and human calibration.",
            "comparison_to_human_or_traditional": "LLM-based evaluators can approximate human judgments and scale better, but the paper emphasizes validating them against human preference datasets and using meta-evaluation to detect bias.",
            "recommendations_or_best_practices": "Always meta-evaluate LLM-based judges (e.g., with human annotations and bias checks), document prompting and settings, and include these judges as configurable steps in evaluation pipelines.",
            "uuid": "e9674.3",
            "source_info": {
                "paper_title": "FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Reference-based metrics",
            "name_full": "Classic Reference-Based Metrics (BLEU, ROUGE, BERTScore, BARTScore, etc.)",
            "brief_description": "Automated metrics that compare model-generated text against human references to score overlap (BLEU/ROUGE) or semantic similarity using pretrained encoders (BERTScore/BARTScore).",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": null,
            "llm_description": null,
            "scientific_domain": "Natural language generation evaluation across tasks like summarization, translation, and open-ended generation",
            "evaluation_method": "Compute standard overlap or embedding-based similarity metrics between generated outputs and reference texts as an automated quality signal.",
            "evaluation_criteria": "n-gram overlap (BLEU/ROUGE), learned semantic similarity (BERTScore), and other reference-based scores reflect closeness to references but may miss open-ended qualities.",
            "benchmark_or_dataset": "Applied to classic reference-based tasks; FreeEval supports ROUGE and BERTScore as integrated evaluation steps.",
            "results_summary": "FreeEval incorporates classic reference-based metrics (ROUGE, BERTScore, etc.) as part of the modular evaluation toolkit to provide baseline automated assessments.",
            "limitations_or_challenges": "These metrics do not capture open-endedness, creativity, or many subjective qualities; sensitive to reference diversity and quality.",
            "comparison_to_human_or_traditional": "Reference metrics are straightforward and reproducible but often correlate poorly with human judgments on open-ended generation; FreeEval recommends supplementing them with human or LLM-based evaluators.",
            "recommendations_or_best_practices": "Use reference metrics for tasks with good references, but validate with human judgments or LLM-based evaluators for open-ended tasks.",
            "uuid": "e9674.4",
            "source_info": {
                "paper_title": "FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Benchmarks (ARC/MMLU/HellaSwag/CEval/GSM8K)",
            "name_full": "Standard Dataset Benchmarks (ARC-Challenge, MMLU, HellaSwag, CEval, GSM8K)",
            "brief_description": "Well-known benchmark datasets used to evaluate different capabilities of LLMs: reasoning (ARC), broad multi-task knowledge (MMLU), commonsense completion (HellaSwag), multi-discipline Chinese evaluation (CEval), and math reasoning (GSM8K).",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "llama-2-7b-chat (reported in Table 2) and LLaMA-2-70B (example config)",
            "llm_description": "Versions of LLaMA-2 used to demonstrate how inference settings and prompts affect benchmark scores.",
            "scientific_domain": "NLP benchmark evaluation (reasoning, knowledge, commonsense, mathematics)",
            "evaluation_method": "Run few-shot or prompt-based evaluation (e.g., 25-shot on ARC-Challenge) and report standard metrics such as accuracy on multiple-choice tasks or task-specific scoring.",
            "evaluation_criteria": "Task accuracy (percent correct) on benchmark test sets; choice of prompt template (CP/MCP) and few-shot examples influences results.",
            "benchmark_or_dataset": "ARC-Challenge: reasoning QA; MMLU: massive multitask language understanding; HellaSwag: commonsense completion; CEval: Chinese multi-discipline evaluation; GSM8K: math word problems.",
            "results_summary": "Table 2 demonstrates that prompting style and prompt templates substantially change measured accuracy (e.g., llama-2-7b-chat 25-shot ARC-C accuracy ranged from 47.53% to 54.18% across prompt variants). FreeEval uses these benchmarks in demonstration pipelines and supports config-driven experiments.",
            "limitations_or_challenges": "Benchmarks can be contaminated by pretraining data, prompting and evaluation settings can drastically affect scores, and single-number summaries hide variability.",
            "comparison_to_human_or_traditional": "Benchmarks provide objective, repeatable metrics but must be combined with contamination checks and human inspection to ensure validity of claims.",
            "recommendations_or_best_practices": "Standardize and document prompting, few-shot selection, and evaluation configs; run contamination detection prior to reporting scores; use multiple prompt variants and meta-evaluation to assess robustness.",
            "uuid": "e9674.5",
            "source_info": {
                "paper_title": "FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Prompting / Inference Implementation Effects",
            "name_full": "Prompt Type and Inference Implementation Effects (Cloze Prompt CP, Multiple Choice Prompt MCP, PromptA/PromptB)",
            "brief_description": "Empirical observation that different prompt templates and choice of cloze vs. multiple-choice prompting produce materially different evaluation outcomes on the same model/dataset.",
            "citation_title": "Leveraging large language models for multiple choice question answering.",
            "mention_or_use": "use",
            "llm_name": "llama-2-7b-chat",
            "llm_description": "A 7B-parameter chat-tuned variant of Llama-2 used in the paper's benchmarking examples.",
            "scientific_domain": "NLP evaluation methodology",
            "evaluation_method": "Compare model accuracy under different prompting strategies (e.g., CP+PromptA vs MCP+PromptA) on benchmarks like ARC, MMLU, HellaSwag.",
            "evaluation_criteria": "Downstream task accuracy (%) as reported in Table 2 for each prompt/inference implementation combination.",
            "benchmark_or_dataset": "ARC-Challenge, MMLU, HellaSwag (results shown in Table 2).",
            "results_summary": "Table 2 shows substantial variability: e.g., MCP+PromptA yields 54.18% on ARC-C vs CP+PromptB 47.53%; HellaSwag accuracy varied widely depending on choice of CP vs MCP and prompt variant.",
            "limitations_or_challenges": "Prompt design and selection of few-shot examples are sources of variance and can confound comparisons across models if not standardized.",
            "comparison_to_human_or_traditional": "This is an automated experimental observation; human evaluation cannot substitute but can validate whether prompt-induced score changes correspond to real quality differences.",
            "recommendations_or_best_practices": "Standardize prompt templates and document all inference settings; include multiple prompting strategies in evaluations and report sensitivity to prompting as part of results.",
            "uuid": "e9674.6",
            "source_info": {
                "paper_title": "FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Efficiency / Runtime Benchmarks",
            "name_full": "Inference Efficiency Benchmarks (Toolkit runtime comparisons)",
            "brief_description": "Empirical runtime comparisons across evaluation toolkits showing FreeEval's concurrent inference yields substantial speedups on single-GPU runs for common benchmarks.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "llama-2-7b-chat (used in comparative timing experiments)",
            "llm_description": "7B-chat variant evaluated on ARC-C, MMLU, HellaSwag to compare execution times across toolkits.",
            "scientific_domain": "Systems/infrastructure evaluation for LLM benchmarking",
            "evaluation_method": "Measure wall-clock execution time (hours) for running full evaluations on ARC-C, MMLU, HellaSwag using different toolkits on the same hardware (single NVIDIA A800 80GB GPU).",
            "evaluation_criteria": "Total execution time (hours) to complete the benchmark runs; concurrency/sequential modes compared.",
            "benchmark_or_dataset": "ARC-C, MMLU, HellaSwag (as used in Table 3).",
            "results_summary": "Table 3: FreeEval (Concurrent) achieves 0.067h on ARC-C, 0.233h on MMLU, 0.357h on HellaSwag, outperforming Eval-Harness and OpenCompass on the same machine; FreeEval (Sequential) numbers are also provided.",
            "limitations_or_challenges": "Reported times exclude download times and are specific to the hardware and model used; results depend on backend tuning (TGI, batching, caching).",
            "comparison_to_human_or_traditional": "This is an infrastructure-level comparison; human factors are not directly relevant here.",
            "recommendations_or_best_practices": "Use concurrent inference, caching, and high-performance inference backends (e.g., Huggingface TGI) to reduce evaluation costs; report runtime and hardware to enable reproducibility.",
            "uuid": "e9674.7",
            "source_info": {
                "paper_title": "FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Detecting pretraining data from large language models.",
            "rating": 2,
            "sanitized_title": "detecting_pretraining_data_from_large_language_models"
        },
        {
            "paper_title": "NLP evaluation in trouble: On the need to measure llm data contamination for each benchmark.",
            "rating": 2,
            "sanitized_title": "nlp_evaluation_in_trouble_on_the_need_to_measure_llm_data_contamination_for_each_benchmark"
        },
        {
            "paper_title": "Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization.",
            "rating": 2,
            "sanitized_title": "pandalm_an_automatic_evaluation_benchmark_for_llm_instruction_tuning_optimization"
        },
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena.",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "AlpacaEval: An Automatic Evaluator of Instruction-following Models",
            "rating": 2,
            "sanitized_title": "alpacaeval_an_automatic_evaluator_of_instructionfollowing_models"
        },
        {
            "paper_title": "Gptscore: Evaluate as you desire.",
            "rating": 1,
            "sanitized_title": "gptscore_evaluate_as_you_desire"
        }
    ],
    "cost": 0.016308,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models</h1>
<p>Zhuohao Yu ${ }^{1}$, Chang Gao ${ }^{1}$, Wenjin Yao ${ }^{1}$, Yidong Wang ${ }^{1}$, Zhengran Zeng ${ }^{1}$, Wei $\mathrm{Ye}^{1 *}$, Jindong Wang ${ }^{2}$, Yue Zhang ${ }^{3}$, Shikun Zhang ${ }^{1}$<br>${ }^{1}$ Peking University. ${ }^{2}$ Microsoft Research. ${ }^{3}$ Westlake University.<br>zyu@stu.pku.edu.cn, wye@pku.edu.cn</p>
<h4>Abstract</h4>
<p>The rapid growth of evaluation methodologies and datasets for large language models (LLMs) has created a pressing need for their unified integration. Meanwhile, concerns about data contamination and bias compromise the trustworthiness of evaluation findings, while the efficiency of evaluation processes remains a bottleneck due to the significant computational costs associated with LLM inference. In response to these challenges, we introduce FreeEval, a modular framework not only for conducting trustworthy and efficient automatic evaluations of LLMs but also serving as a platform to develop and validate new evaluation methodologies. FreeEval addresses key challenges through: (1) unified abstractions that simplify the integration of diverse evaluation methods, including dynamic evaluations requiring complex LLM interactions; (2) built-in meta-evaluation techniques such as data contamination detection and human evaluation to enhance result fairness; (3) a high-performance infrastructure with distributed computation and caching strategies for efficient large-scale evaluations; and (4) an interactive Visualizer for result analysis and interpretation to support innovation of evaluation techniques. We opensource all our code at https://github.com/ WisdomShell/FreeEval ${ }^{1}$.</p>
<h2>1 Introduction</h2>
<p>Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) with their impressive performance across various tasks (Brown et al., 2020; Zhang et al., 2022; Bubeck et al., 2023; OpenAI, 2023). As LLMs play a critical role in academia and industry, evaluating their capabilities has become essential (Guo et al., 2023). Consequently, researchers have proposed</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>automatic evaluation methodologies using benchmark datasets (Clark et al., 2018; Zellers et al., 2019; Cobbe et al., 2021; Bang et al., 2023) for objective assessments, and LLM-based subjective evaluation tools (Wang et al., 2023c; Zheng et al., 2023b; Li et al., 2023b; Chan et al., 2023).</p>
<p>The rapid emergence of evaluation data and methods has intensified the challenge of incorporating cutting-edge techniques cost-effectively and conducting reliable evaluations. In response, several open-source evaluation platforms for LLMs have been proposed, each with unique features. Table 1 provides a comprehensive comparison. Specifically, Eval-Harness (Gao et al., 2021) evaluates LLMs using various benchmark datasets. HELM (Liang et al., 2022) offers metrics beyond accuracy on custom datasets and models. OpenAI Evals (Contributors, 2023) implements interfaces for LLM-based judges and their meta-evaluation. OpenCompass (Contributors, 2023b) introduces distributed inference with SLURM (Yoo et al., 2003) on clusters. PromptBench (Zhu et al., 2023b) incorporates prompt attacks and DyVal (Zhu et al., 2023a) in its framework.</p>
<p>Despite these promising efforts, current evaluation platforms still face three bottlenecks.</p>
<p>First, a unified and extensible framework is required to integrate evaluation methods seamlessly. This consequently affects the flexibility, transparency, and interpretability of the evaluation. The evaluation results are highly dependent on the deployment settings and prompting techniques, as LLMs are not robust enough for these intricate settings (Zheng et al., 2023a). For example, Table 2 shows that these settings can significantly influence results, confirming the need for standardized implementation of evaluation methods to assure consistent assessment.</p>
<p>Second, the reliability of results from these platforms cannot always be guaranteed. Automatic evaluation of LLMs remains a challenging</p>
<p>Table 1: Comparison of popular evaluation toolkits on features.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Toolkit</th>
<th style="text-align: center;">Custom <br> Datasets</th>
<th style="text-align: center;">Custom <br> Models</th>
<th style="text-align: center;">Custom <br> Prompting</th>
<th style="text-align: center;">LLM <br> Judges</th>
<th style="text-align: center;">Dynamic <br> Evaluation</th>
<th style="text-align: center;">Distributed <br> Inference</th>
<th style="text-align: center;">Contamination <br> Detection</th>
<th style="text-align: center;">Meta <br> Evaluation</th>
<th style="text-align: center;">Visual <br> Analysis</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Eval-Harness (Gao et al., 2021)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">HELM (Liang et al., 2022)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">OpenAI Evals (Contributors, 2023)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">BIG-Bench (Contributors, 2023)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">OpenCompass (Contributors, 2023b)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">PromptBench (Zhu et al., 2023b)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">UltraEval (He et al., 2024)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">FreeEval (Ours)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison of different inference implementations. We report 25 -shot accuracy of llama-2-7b-chat on ARC-Challenge (Clark et al., 2018), 5-shot accuracy on MMLU (Hendrycks et al., 2020) and HellaSwag (Zellers et al., 2019). 'CP' and 'MCP' denote Cloze Prompt and Multiple Choice Prompt from Robinson et al. (2022).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">ARC-C</th>
<th style="text-align: center;">MMLU</th>
<th style="text-align: center;">HellaSwag</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CP+PromptA</td>
<td style="text-align: center;">$51.11 \%$</td>
<td style="text-align: center;">$40.65 \%$</td>
<td style="text-align: center;">$50.07 \%$</td>
</tr>
<tr>
<td style="text-align: left;">CP+PromptB</td>
<td style="text-align: center;">$47.53 \%$</td>
<td style="text-align: center;">$38.72 \%$</td>
<td style="text-align: center;">$50.19 \%$</td>
</tr>
<tr>
<td style="text-align: left;">MCP+PromptA</td>
<td style="text-align: center;">$54.18 \%$</td>
<td style="text-align: center;">$42.73 \%$</td>
<td style="text-align: center;">$30.61 \%$</td>
</tr>
<tr>
<td style="text-align: left;">MCP+PromptB</td>
<td style="text-align: center;">$54.10 \%$</td>
<td style="text-align: center;">$41.28 \%$</td>
<td style="text-align: center;">$30.96 \%$</td>
</tr>
</tbody>
</table>
<p>task (Chang et al., 2023) due to their open-ended nature and the presence of data contamination, which lead to inflated performance metrics (Schaeffer, 2023; Sainz et al., 2023; Yu et al., 2024). Moreover, the lack of tools for in-depth analysis and visualization of evaluation results makes it difficult for researchers to interpret the performance of LLMs across different tasks and scenarios.</p>
<p>Third, the efficiency of previous evaluation toolkits has significant room for improvement. LLM inference could be a substantial challenge for both industry and researchers, since it requires strong GPUs or paid APIs, especially for large-scale evaluations (Wang et al., 2023c). Optimizing inference computation is crucial for reducing the costs of LLM evaluation and supporting rapid iteration in both evaluation and development.</p>
<p>To address these challenges, we propose FreeEval, a modular and extensible framework for trustworthy and efficient automatic evaluation of LLMs, as well as a platform for developing new evaluation methodologies. The main features of FreeEval are:</p>
<p>Unified abstraction and modular implementation of various evaluation methods. We introduce concepts of step, dataset, and config to uniformly describe dataset-based, classic referencebased, and LLM-based evaluators. Dataset-based evaluators include task-specific datasets along with
dataset operations such as custom prompting, data augmenting and generation. LLM-based evaluators, such as MT-Bench (Zheng et al., 2023b), AlpacaEval (Li et al., 2023b), PandaLM (Wang et al., 2023c) and KIEval (Yu et al., 2024), are also integrated to provide subjective assessment. Classic Judges, which utilize reference-based evaluation metrics like ROUGE (Lin, 2004) and BERTScore (Zhang et al., 2019) to examine model output. FreeEval's modular design allows for easy implementation of new evaluation protocols and supports evaluating both open-source and proprietary models. The abstractions also bring transparency to the evaluation process since all the evaluation settings are open to users.</p>
<p>Practical meta-evaluation modules for trustworthiness. FreeEval incorporates contamination detection, human judgment, case analysis, and bias evaluation. These features mitigate overfitting risks, enhance interpretability, and support the development and validation of new evaluation methods. A user-friendly interface for human annotation further improves explainability and reliability of results.</p>
<p>Optimized distributed and concurrent inference with load balancing and caching mechanisms. Leveraging cutting-edge inference engines with concurrency and caching strategies, FreeEval efficiently handles large-scale evaluations on multinode multi-GPU clusters. This infrastructure supports both open-source models and proprietary APIs, ensuring scalability and cost-effectiveness.</p>
<p>Intuitive Visualizer for result analysis and interpretation. This component provides interactive tools for exploring results, conducting case studies, and identifying patterns. It enhances interpretability and supports the development of new evaluation methods through visual feedback.</p>
<p>By combining these features, FreeEval addresses key challenges in LLM evaluation while serving as a powerful platform for researchers to build new evaluation methods.</p>
<h2>2 Background</h2>
<p>In this section, we provide an overview of the current landscape of LLM evaluation methods, the challenges posed by data contamination, and the importance of meta-evaluation in assessing the reliability and validity of evaluation protocols.</p>
<h3>2.1 Automatic Evaluation Methods for LLMs</h3>
<p>The rapid development of Large Language Models (LLMs) has led to the emergence of various evaluation methods, each aiming to assess different aspects of model performance. These methods can be broadly categorized into three groups: classic reference-based evaluation, dataset-based benchmarks, and LLM-based evaluators.
Reference-Based Evaluation methods, such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and BERTScore (Zhang et al., 2019), assess the quality of generated text by comparing it against human-written references. While straightforward, they may not fully capture the open-ended nature of LLM-generated outputs and can be sensitive to reference quality and diversity (Wang et al., 2023c).
Dataset-Based Benchmarks, such as ARC (Clark et al., 2018), HellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al., 2020), and CEval (Huang et al., 2023), evaluate LLMs using carefully curated datasets that test specific skills or knowledge. However, they may not fully capture the open-ended nature of LLMs and can be vulnerable to data contamination (Schaeffer, 2023; Wei et al., 2023).
LLM-Based Evaluators leverage strong LLMs, such as GPT-4 (OpenAI, 2023), to assess the performance of other models. Examples include PandaLM (Wang et al., 2023c), MT-Bench (Zheng et al., 2023b), GPTScore (Fu et al., 2023), PRD (Li et al., 2023a), and KIEval (Yu et al., 2024). These evaluators can capture nuanced aspects of language understanding and generation, but their performance is influenced by the evaluator LLM and prompting strategies. Biases present in the evaluator LLM may propagate to the evaluation process (Zeng et al., 2023; Wang et al., 2023b), requiring careful meta-evaluation. Additionally, the inference cost of LLMs necessitates optimization for large-scale evaluation.</p>
<h3>2.2 Meta-Evaluation of LLMs</h3>
<p>Meta-evaluation refers to the process of evaluating the fairness, reliability, and validity of evaluation protocols themselves. We incorporate several metaevaluation methods into FreeEval.
Data Contamination occurs when an LLM is exposed to test data during training, leading to inflated performance scores and an inaccurate assessment of the model's true capabilities (Schaeffer, 2023; Sainz et al., 2023; Zhu et al., 2023a). This issue is particularly important due to its impact on evaluation fairness, and should be considered. We implement data contamination detection methods like Min-K prob (Shi et al., 2023) and average loss (Wei et al., 2023) in FreeEval as modules, to make contamination detection a fundamental process in evaluating LLMs or creating a new evaluation protocol.
Human Evaluation is the gold standard for metaevaluation (Chang et al., 2023), as it directly reflects human preferences on generated texts. This is particularly important for LLM-based evaluators, which subjectively evaluate output quality like human experts. However, the lack of standardized platforms or guidelines for human annotation can lead to biased, inconsistent, and unfair judgments. To address this, we incorporate meta-evaluation protocols from Wang et al. (2023c); Zeng et al. (2023); Zheng et al. (2023b), as they reflect preferences from human experts in different scenarios. Additionally, we create a user-friendly interface for human experts to create new preference datasets, facilitating the collection of high-quality human evaluations for meta-evaluation purposes.</p>
<h2>3 Design and Implementation</h2>
<p>In this section, we present the design and implementation of FreeEval, we discuss the framework's architecture, its key components, and how they address the challenges identified previously.</p>
<h3>3.1 Design Principles</h3>
<p>To build a flexible, efficient research tool for LLM evaluation we make sure the architecture of FreeEval follows the following principles:</p>
<ul>
<li>Modular: Enables easy integration of new evaluation methods, datasets, and protocols. Ensures transparency by making all evaluation settings and details openly accessible to users.</li>
<li>Trustworthy: Promotes fair and effective evaluation processes. Supports meta-evaluation for validating evaluation methods and ensures result interpretability.</li>
</ul>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overall architecture of FreeEval.</p>
<ul>
<li>Efficient: Minimizes computational costs for LLM inference, enabling large-scale evaluations and rapid prototyping of new methodologies.</li>
</ul>
<h3>3.2 FreeEval Architecture Overview</h3>
<p>FreeEval's architecture, illustrated in Figure 1, features a modular design that could be separated into Evaluation Methods, Meta-Evaluation and LLM Inference Backends. Evaluation Methods contain different datasets and implementation for evaluation methods. The Meta-Evaluation module ensures the integrity and fairness of assessments by providing data contamination detection methods and popular meta-evaluation method implementation. LLM Inference Backends form the computational backbone, as it provide distributed and concurrent inference of LLMs featuring performance optimization techniques.</p>
<h3>3.3 Extensible Modular Design</h3>
<p>FreeEval's modular architecture is designed to accommodate the rapidly evolving landscape of LLM evaluation. To help users implement evaluation methods without complexity, FreeEval is implemented around the concept of step, dataset and config, which serve as the building blocks for creating flexible and extensible evaluation pipelines:</p>
<ul>
<li>step: A step encapsulates a specific evaluation method, data augmentation technique, or metric calculation. Each step contain three phases: preprocess handles initializing the required dataset or models; run handles the execution; postprocess parse the outputs, collects evaluation results and free up the resources.</li>
<li>dataset: Data used by the evaluators are defined as dataset. Each dataset handles the preprocessing required to load data, few-shot settings, prompting, augmentation of instances, and postprocessing of inference results.</li>
<li>config: A config file is used to compose evaluation pipelines with steps and datasets. The config file contains all the details and settings. steps defined in the config are executed sequentially, and they share the same context which stores intermediate results.</li>
</ul>
<p>These abstractions improve transparency in evaluations by providing users with full access to the configuration details for each evaluation pipeline. The config file also serves as a complete record of the evaluation process, including all necessary hyperparameters and settings. The modular design also allow data to be re-used in different scenarios without effort. For example, GSM8K (Cobbe</p>
<p>et al., 2021) is a evaluation dataset, we could simply calculate perplexity of models on this dataset, or we could use a data generation step to generate new data with GPT-4 in the same distribution to detect data contamination following Wei et al. (2023). The modular approach allows researchers to easily add new evaluation methods or modify existing ones without disrupting the overall structure of the framework. By defining each evaluator as a self-contained unit, FreeEval promotes code reusability and maintainability.</p>
<p>This configuration-driven approach eliminates the need for users to write Python code when defining and running an evaluation pipeline. All settings and parameters for each step and dataset are specified within the config, making the evaluation process highly customizable and accessible to researchers with varying levels of programming expertise. Figure 2 shows an example config for a pipeline evaluating LLaMA-2 70B (Touvron et al., 2023b) on ARC-Challenge (Clark et al., 2018) dataset with a fixed seed for sampling 25 shot examples and custom prompt. The model can be deployed locally or on a remote machine. The pipeline also include detecting data contamination with Min-K\% Prob (Shi et al., 2023).</p>
<h3>3.4 Trustworthy Evaluation</h3>
<p>FreeEval prioritizes trustworthiness and fairness in evaluations by incorporating a range of metaevaluation modules that validates the evaluation results and processes. As human preference remain the gold standard for measuring the effectiveness of evaluation protocols, FreeEval model human preference into two types: pairwise comparison and direct scoring. We incorporate existing meta-evaluation datasets from PandaLM (Wang et al., 2023c), MT-Bench (Zheng et al., 2023b), LLMBar (Guo et al., 2023), AlpacaEval (Li et al., 2023b), and provide a user-friendly interface for annotating and curating human evaluation datasets.</p>
<p>To ensure the trustworthiness of the evaluation results, we also implement data contamination detection methods, as introduced in subsection 2.2, into our toolkit as steps. Understanding whether the tested dataset appear in the training phase of the evaluated models would help users assess the validity and reliability of evaluation results. We also provide bias evaluation modules and visualization tools specifically for LLM-based evaluators, as previous studies have reported they exhibit position bias and length bias (Zheng et al., 2023b;</p>
<div class="codehilite"><pre><span></span><code><span class="p">{</span>
<span class="w">    </span><span class="s">&quot;steps&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="s">&quot;step_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;ARC-Challenge 25-shot MCP&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;step_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;simple_multiple_choice&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;dataset_config&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="s">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;arc_challenge&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;dataset_kwargs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="s">&quot;seed&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;fewshot_split&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;train&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;fewshot_num&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">25</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;multiple_choice_template_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;prompt1&quot;</span><span class="p">}</span>
<span class="w">        </span><span class="p">},</span>
<span class="w">        </span><span class="s">&quot;inference_config&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="s">&quot;type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;remote_hf&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;inference_kwargs&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="s">&quot;model_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;llama2-70b&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;base_url&quot;</span><span class="p">:</span><span class="w"> </span><span class="o">...</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;generation_config&quot;</span><span class="p">:</span><span class="w"> </span><span class="o">...</span><span class="w"> </span><span class="p">}</span>
<span class="w">        </span><span class="p">},</span>
<span class="w">        </span><span class="s">&quot;eval_config&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;aggregate_mode&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;mean&quot;</span><span class="p">}</span>
<span class="w">        </span><span class="p">},</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="s">&quot;step_name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Contamination Detection&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;step_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;min_k_prob&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;dataset_config&quot;</span><span class="p">:</span><span class="w"> </span><span class="o">...</span><span class="p">,</span>
<span class="w">            </span><span class="s">&quot;inference_config&quot;</span><span class="p">:</span><span class="w"> </span><span class="o">...</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<p>Figure 2: Config for an example pipeline, evaluating LLaMA-2 70B (Touvron et al., 2023b) on ARCChallenge (Clark et al., 2018) dataset and then detecting data contamination with Min-K\% Prob (Shi et al., 2023).</p>
<p>Wang et al., 2023c). These meta-evaluation modules can be easily integrated into existing evaluation pipelines, allowing researchers to understand the effectiveness of their results, the fairness of the evaluation process, and study bad cases that lead to unexpected evaluation results.</p>
<h3>3.5 Efficient Inference Backends</h3>
<p>FreeEval's high-performance inference backends are designed to efficiently handle the computational demands of large-scale LLM evaluations.</p>
<p>The inference backends in FreeEval support both open-source models and proprietary models with APIs. For all models, FreeEval support concurrent inference given a fixed number of workers. We implement a caching mechanism for queries based on hash values of the request. We hash the request prompt and inference config, and store locally the request content and response for each individual request. By checking the cache before making a query, FreeEval skips cached requests, enabling quick recovery from exceptions and saving inference costs. This is particularly beneficial when implementing and debugging new evaluation methods. Caching also ensures reproducibility, as all requests, settings, and responses are saved and can</p>
<p>from freeeval.models import load_inference_function
# Initialize inference backends
openai_inference = load_inference_function("openai")
huggingface_inference = load_inference_function("remote_hf")
# Parallel inference with load balancing and caching
huggingface_inference{
requests,
output_path,
max_concurrency $=128$,
num_workers $=8$
}
openai_inference(
requests,
output_path,
openai_model,
api_key,
num_workers $=4$,
request_per_min $=100$
}
Figure 3: Example code for running FreeEval's inference backends. We rely on these backends for efficient inference and provide a simple abstraction.
be inspected using FreeEval's visualization tools.
For open-source models, we leverage Huggingface's text-generation-inference (TGI, Contributors (2023a)) package which is a productionready high-performance inference toolkit. We implement a load-balancing technique in conjunction with the continuous batching feature provided by TGI to maximize GPU utilization on multi-node multi-GPU clusters. For proprietary models, we have a rate-limiting mechanism to avoid causing too much stress on API providers.</p>
<p>We evaluate FreeEval's performance by comparing the execution times (excluding downloading times) for llama-2-7b-chat-hf model on 3 common datasets using different toolkits. Our experiments are done on the same Ubuntu machine with a single NVIDIA A800 80GB PCIe GPU. As shown in Table 3, even on a single GPU, FreeEval exhibit significant advantage on all benchmark datasets.</p>
<p>The inference backends in FreeEval are designed to seamlessly integrate with the evaluation methods of the framework. As illustrated in Figure 3, initializing the inference backends and running parallel inference is straightforward and user-friendly. This simplicity allows developers of new evaluation methods to focus on prompting or interactions between models, using the backends sequentially. As a result, implementing interactive evaluation methods, such as those proposed by Li et al. (2023a); Chan et al. (2023); Yu et al. (2024), becomes much easier and more accessible to researchers.</p>
<p>Table 3: Comparison of execution time (in hours) of different toolkits. All experiments are done on the same machine with a single NVIDIA A800 80GB PCIe GPU.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Toolkit</th>
<th style="text-align: center;">ARC-C</th>
<th style="text-align: center;">MMLU</th>
<th style="text-align: center;">HellaSwag</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Eval-Harness</td>
<td style="text-align: center;">0.160</td>
<td style="text-align: center;">0.510</td>
<td style="text-align: center;">1.080</td>
</tr>
<tr>
<td style="text-align: left;">OpenCompass</td>
<td style="text-align: center;">0.084</td>
<td style="text-align: center;">1.431</td>
<td style="text-align: center;">1.716</td>
</tr>
<tr>
<td style="text-align: left;">FreeEval (Sequential)</td>
<td style="text-align: center;">0.211</td>
<td style="text-align: center;">0.949</td>
<td style="text-align: center;">0.966</td>
</tr>
<tr>
<td style="text-align: left;">FreeEval (Concurrent)</td>
<td style="text-align: center;">$\mathbf{0 . 0 6 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 3 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 5 7}$</td>
</tr>
</tbody>
</table>
<h3>3.6 FreeEval Visualizer</h3>
<p>Unlike traditional evaluation toolkits that provide only accuracy or performance scores, FreeEval automatically converts and saves evaluation results for comprehensive visualization. Users can launch the Visualizer with a simple command for an intuitive web interface for detailed analysis.</p>
<p>The FreeEval Visualizer offers a dashboard overview of evaluation results and settings, indepth analysis tools, a case browser for examining individual cases and a human evaluation toolkit. These features enable researchers to explore outcomes, identify patterns, and study potential biases or anomalies. By providing immediate visual feedback, the Visualizer aids in rapid prototyping and refinement of new evaluation methodologies, contributing to the trustworthiness and interpretability of the evaluation process.</p>
<p>For detailed screenshots and a comprehensive introduction to the Visualizer's functionalities, please refer to Appendix B. A demonstration video and live demo are also available on our project website.</p>
<h2>4 Conclusion</h2>
<p>We introduce FreeEval, a modular and extensible framework for trustworthy and efficient automatic evaluation of LLMs. FreeEval innovatively addresses key challenges in LLM evaluation by providing a unified implementation of various evaluation methods, incorporating meta-evaluation modules, and leveraging high-performance inference backends. The framework's modular design facilitates easy integration of new evaluation protocols and improves transparency. The integrated Visualizer enhances result interpretation and analysis, supporting comprehensive evaluation and the development of new methodologies. We will continue to maintain and expand the FreeEval toolkit, striving to provide deeper insights into the capabilities and limitations of LLMs and contribute to the development of more robust and trustworthy language models.</p>
<h2>A Limitations and Ethical Considerations</h2>
<p>In this Appendix section, we discuss the limitations and ethical considerations of FreeEval. While FreeEval addresses several challenges in LLM evaluation, it has limitations and raises ethical considerations:</p>
<ul>
<li>Bias and Discrimination: FreeEval includes bias evaluation modules but cannot eliminate biases inherent in training data or models. Researchers should strive for more inclusive and equitable LLMs.</li>
<li>Environmental Impact: Despite efficient inference backends, the overall environmental impact of LLM development remains a concern requiring further innovation.</li>
<li>Human Evaluation Subjectivity: The human evaluation component may introduce subjective biases, necessitating careful design of evaluation protocols.</li>
<li>Accountability and Misuse: While FreeEval enhances transparency in evaluation, ethical deployment and appropriate safeguards in real-world applications remain the responsibility of researchers and developers.</li>
</ul>
<p>These points highlight the need for ongoing research in LLM evaluation methodologies and responsible AI development practices.</p>
<h2>B FreeEval Visualizer</h2>
<p>The FreeEval Visualizer is a web-based interface designed to enhance the interpretability and analysis of LLM evaluation results. It provides an intuitive platform for researchers to explore evaluation data, conduct case studies, and perform human evaluations.</p>
<p>The Visualizer consists of six main components:</p>
<ul>
<li>Dashboard: Offers an overview of evaluation results, including distribution charts and summary statistics.</li>
<li>Analysis Tools: Provides detailed visualizations and statistical analyses of evaluation data.</li>
<li>Case Browser: Allows users to search, filter, and examine individual evaluation cases.</li>
<li>Human Evaluation Creator: Enables researchers to set up new human evaluation sessions.</li>
<li>Human Evaluation Session: Manages ongoing human evaluation tasks.</li>
<li>Case Annotation Interface: Facilitates detailed annotation of individual cases.</li>
</ul>
<p>The Visualizer is built using Flask, a lightweight Python web framework, and incorporates modern front-end technologies for responsive design. It integrates seamlessly with FreeEval's core evaluation modules, providing a unified workflow for LLM assessment.</p>
<p>Key features of the Visualizer include interactive data exploration, customizable visualizations, and support for various evaluation types (e.g., pairwise comparisons, direct scoring). The human evaluation interfaces facilitate the creation, management, and execution of expert judgment collection, which can be used for meta-evaluation or to create new evaluation datasets.</p>
<p>Figure 4 showcases the main interfaces of the FreeEval Visualizer. The dashboard (Figure 4a) provides an overview of evaluation results, while the analysis page (Figure 4b) offers more detailed statistical insights. The case browser (Figure 4c) allows for detailed exploration of individual cases.</p>
<p>The human evaluation workflow is supported by three interfaces: the creation page for setting up new evaluation sessions (Figure 4d), the session management page (Figure 4e) for overseeing ongoing evaluations, and the case annotation interface (Figure 4f) for collecting detailed judgments on specific outputs.</p>
<p>By providing these visual and interactive tools, the FreeEval Visualizer aims to streamline the process of analyzing LLM evaluation results, enabling researchers to gain deeper insights and make more informed decisions in their work with large language models. The comprehensive set of features supports the entire evaluation lifecycle, from initial data exploration to in-depth analysis and human-in-the-loop assessment.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />
(a) Dashboard overview
(b) Overall analysis
<img alt="img-2.jpeg" src="img-2.jpeg" />
(c) Case Browser
(d) Creating human evaluation session
<img alt="img-3.jpeg" src="img-3.jpeg" />
(e) Human evaluation session
(f) Case Annotation</p>
<p>Figure 4: Screenshots of the FreeEval Visualizer web application</p>
<h2>References</h2>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403.</p>
<p>Devansh Arpit, StanisÅ‚aw JastrzÄ™bski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. 2017. A closer look at memorization in deep networks. In International conference on machine learning, pages 233-242. PMLR.</p>
<p>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 65-72.</p>
<p>Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023.</p>
<p>Edward Beeching, ClÃ©mentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. 2023. Open llm leaderboard. https://huggingface.co/ spaces/HuggingFaceH4/open_llm_leaderboard.</p>
<p>Yoshua Bengio and Yann LeCun. 2007. Scaling learning algorithms towards AI. In Large Scale Kernel Machines. MIT Press.</p>
<p>Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. 2023. The reversal curse: Llms trained on" a is b" fail to learn" b is a". arXiv preprint arXiv:2309.12288.</p>
<p>Stella Biderman, USVSN Sai Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, Shivanshu Purohit, and Edward Raf. 2023a. Emergent and predictable memorization in large language models. arXiv preprint arXiv:2304.11158.</p>
<p>Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle Oâ€™Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023b. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373.</p>
<p>Rishi Bommasani, Percy Liang, and Tony Lee. 2023. Holistic evaluation of language models. Annals of the New York Academy of Sciences.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>SÃ©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.</p>
<p>Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2023. Chateval: Towards better llm-based evaluators through multi-agent debate. arXiv preprint arXiv:2308.07201.</p>
<p>Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2023. A survey on evaluation of large language models. arXiv preprint arXiv:2307.03109.</p>
<p>Cheng-Han Chiang and Hung-yi Lee. 2023. Can large language models be an alternative to human evaluations? arXiv preprint arXiv:2305.01937.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023).</p>
<p>Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Contributors. 2023. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research.</p>
<p>Contributors. 2023. Openai evals. https://github. com/openai/evals.</p>
<p>Contributors. 2023a. Text generation inference: A rust, python and grpc server for text generation inference. https://github.com/huggingface/ text-generation-inference.</p>
<p>OpenCompass Contributors. 2023b. Opencompass: A universal evaluation platform for foundation models. https://github.com/open-compass/ opencompass.</p>
<p>Luigi Daniele and Suphavadeeprasit. 2023. Amplifyinstruct: Synthetically generated diverse multi-turn conversations for effecient llm training. arXiv preprint arXiv:(comming soon).</p>
<p>Nolan Dey, Gurpreet Gosal, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, Joel Hestness, et al. 2023. Cerebras-gpt: Open compute-optimal language models trained on the cerebras wafer-scale cluster. arXiv preprint arXiv:2304.03208.</p>
<p>Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith. 2020. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping. arXiv preprint arXiv:2002.06305.</p>
<p>Mengnan Du, Subhabrata Mukherjee, Yu Cheng, Milad Shokouhi, Xia Hu, and Ahmed Hassan. 2023. Robustness challenges in model distillation and pruning for natural language understanding. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages $1758-1770$.</p>
<p>Michael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, Weijia Shi, Luke Zettlemoyer, Yulia Tsvetkov, Yejin Choi, David Evans, and Hannaneh Hajishirzi. 2024. Do membership inference attacks work on large language models? arXiv preprint arXiv:2402.07841.</p>
<p>Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Alpacafarm: A simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387.</p>
<p>Dom Eccleston. 2023. Sharegpt dataset. https:// sharegpt.com/.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166.</p>
<p>Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2021. A framework for few-shot language model evaluation.</p>
<p>Varun Godbole, George E. Dahl, Justin Gilmer, Christopher J. Shallue, and Zachary Nado. 2023. Deep learning tuning playbook. Version 1.0.</p>
<p>Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. 2016. Deep learning, volume 1. MIT Press.</p>
<p>Google. 2023. Bard.
Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, Deyi Xiong, et al. 2023. Evaluating large language models: A comprehensive survey. arXiv preprint arXiv:2310.19736.</p>
<p>Chaoqun He, Renjie Luo, Shengding Hu, Yuanqian Zhao, Jie Zhou, Hanghao Wu, Jiajie Zhang, Xu Han, Zhiyuan Liu, and Maosong Sun. 2024. Ultraeval: A lightweight platform for flexible and comprehensive evaluation for llms.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300.</p>
<p>Geoffrey E. Hinton, Simon Osindero, and Yee Whye Teh. 2006. A fast learning algorithm for deep belief nets. Neural Computation, 18:1527-1554.</p>
<p>Lynette Hirschman and Robert Gaizauskas. 2001. Natural language question answering: the view from here. natural language engineering, 7(4):275-300.</p>
<p>Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):17351780 .</p>
<p>Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2021. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations.</p>
<p>Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et al. 2023. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322.</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825.</p>
<p>Marzena Karpinska, Nader Akoury, and Mohit Iyyer. 2021. The perils of using mechanical turk to evaluate open-ended text generation. arXiv preprint arXiv:2109.06835.</p>
<p>Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pages 4171-4186.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466.</p>
<p>Ruosen Li, Teerth Patel, and Xinya Du. 2023a. Prd: Peer rank and discussion improve large language model based evaluations. arXiv preprint arXiv:2307.02762.</p>
<p>Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023b. AlpacaEval: An Automatic Evaluator of Instruction-following Models.</p>
<p>Yucheng Li. 2023. An open source data contamination report for llama series models. arXiv preprint arXiv:2310.17589.</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110.</p>
<p>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. 2021. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958.</p>
<p>Yen-Ting Lin and Yun-Nung Chen. 2023. Llm-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models. arXiv preprint arXiv:2305.13711.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. Gpteval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634.</p>
<p>Carlo A Mallio, Andrea C Sertorio, Caterina Bernetti, and Bruno Beomonte Zobel. 2023. Large language models for structured reporting in radiology: performance of gpt-4, chatgpt-3.5, perplexity and bing. La radiologia medica, pages 1-5.</p>
<p>MosaicML. 2023. Introducing mpt-7b: A new standard for open-source, commercially usable llms.</p>
<p>Jekaterina Novikova, OndÅ™ej DuÅ¡ek, Amanda Cercas Curry, and Verena Rieser. 2017. Why we need new evaluation metrics for nlg. arXiv preprint arXiv:1707.06875.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Yonatan Oren, Nicole Meister, Niladri Chatterji, Faisal Ladhak, and Tatsunori B Hashimoto. 2023. Proving test set contamination in black box language models. arXiv preprint arXiv:2310.17623.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311-318.</p>
<p>Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277.</p>
<p>Kaiping Peng, Richard E Nisbett, and Nancy YC Wong. 1997. Validity problems comparing values across cultures and possible solutions. Psychological methods, 2(4):329.</p>
<p>Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, and Zaid Harchaoui. 2021. Mauve: Measuring the gap between neural text and human text using divergence frontiers. Advances in Neural Information Processing Systems, 34:4816-4828.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training.</p>
<p>Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, and Yuxiong He. 2021. Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1-14.</p>
<p>Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining, pages 3505-3506.</p>
<p>Joshua Robinson, Christopher Rytting, and David Wingate. 2022. Leveraging large language models for multiple choice question answering. ArXiv, abs/2210.12353.</p>
<p>Oscar Sainz, Jon Ander Campos, Iker GarcÃ­a-Ferrero, Julen Etxaniz, Oier Lopez de Lacalle, and Eneko Agirre. 2023. Nlp evaluation in trouble: On the need to measure llm data contamination for each benchmark. arXiv preprint arXiv:2310.18018.</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana IliÄ‡, Daniel Hesslow, Roman CastagnÃ©, Alexandra Sasha Luccioni, FranÃ§ois Yvon, Matthias GallÃ©, et al. 2022. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.</p>
<p>Rylan Schaeffer. 2023. Pretraining on the test set is all you need. arXiv preprint arXiv:2309.08632.</p>
<p>Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. 2023. Detecting pretraining data from large language models. arXiv preprint arXiv:2310.16789.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, AdriÃ  Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the</p>
<p>capabilities of language models. arXiv preprint arXiv:2206.04615.</p>
<p>Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang. 2019. How to fine-tune bert for text classification? In Chinese Computational Linguistics: 18th China National Conference, CCL 2019, Kunming, China, October 18-20, 2019, Proceedings 18, pages 194206. Springer.</p>
<p>Ekaterina Svikhnushina, Anastasiia Filippova, and Pearl Pu. 2022. iEval: Interactive evaluation framework for open-domain empathetic chatbots. In Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 419-431, Edinburgh, UK. Association for Computational Linguistics.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.</p>
<p>Lewis Tunstall, Leandro Von Werra, and Thomas Wolf. 2022. Natural language processing with transformers. " O'Reilly Media, Inc.".</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations.</p>
<p>Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru Tang, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao, Wenyang Gao, Xuming Hu, Zehan Qi, Yidong Wang, Linyi Yang, Jindong Wang, Xing Xie, Zheng Zhang, and Yue Zhang. 2023a. Survey on factuality in large language models: Knowledge, retrieval and domainspecificity.</p>
<p>Cunxiang Wang, Ruoxi Ning, Boqi Pan, Tonghui Wu, Qipeng Guo, Cheng Deng, Guangsheng Bao, Qian Wang, and Yue Zhang. 2024. Novelqa: A benchmark for long-range novel question answering.</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023b. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926.</p>
<p>Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, et al. 2023c. Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. arXiv preprint arXiv:2306.05087.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560.</p>
<p>Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei LÃ¼, Rui Hu, et al. 2023. Skywork: A more open bilingual foundation model. arXiv preprint arXiv:2310.19341.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, et al. 2019. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771.</p>
<p>BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana IliÄ‡, Daniel Hesslow, Roman CastagnÃ©, Alexandra Sasha Luccioni, FranÃ§ois Yvon, et al. 2022. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.</p>
<p>Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu, et al. 2020. Clue: A chinese language understanding evaluation benchmark. In Proceedings of the 28th International Conference on Computational Linguistics, pages 4762-4772.</p>
<p>Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, and Yue Zhang. 2022. Glue-x: Evaluating natural language understanding models from an out-of-distribution generalization perspective. arXiv preprint arXiv:2211.08073.</p>
<p>Linyi Yang, Shuibai Zhang, Zhuohao Yu, Guangsheng Bao, Yidong Wang, Jindong Wang, Ruochen Xu, Wei Ye, Xing Xie, Weizhu Chen, and Yue Zhang. 2023. Supervised knowledge makes large language models better in-context learners.</p>
<p>Andy B Yoo, Morris A Jette, and Mark Grondona. 2003. Slurm: Simple linux utility for resource management. In Workshop on job scheduling strategies for parallel processing, pages 44-60. Springer.</p>
<p>Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Wei Ye, Jindong Wang, Xing Xie, Yue Zhang, and Shikun Zhang. 2024. Kieval: A knowledge-grounded</p>
<p>interactive evaluation framework for large language models. arXiv preprint arXiv:2402.15043.</p>
<p>Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text generation. Advances in Neural Information Processing Systems, 34:27263-27277.</p>
<p>Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830.</p>
<p>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414.</p>
<p>Zhengran Zeng, Yidong Wang, Rui Xie, Wei Ye, and Shikun Zhang. 2024. Coderujb: An executable and unified java benchmark for practical programming scenarios.</p>
<p>Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. 2023. Evaluating large language models at evaluating instruction following. arXiv preprint arXiv:2310.07641.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675.</p>
<p>Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. 2023a. Large language models are not robust multiple choice selectors. In The Twelfth International Conference on Learning Representations.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023b. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685.</p>
<p>Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, and Jiawei Han. 2023. Don't make your llm an evaluation benchmark cheater. arXiv preprint arXiv:2311.01964.</p>
<p>Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang Gong, Diyi Yang, and Xing Xie. 2023a. Dyval: Graph-informed dynamic evaluation of large language models. arXiv preprint arXiv:2309.17167.</p>
<p>Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, et al. 2023b.</p>
<p>Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. arXiv preprint arXiv:2306.04528.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>Corresponding author.
${ }^{1}$ Our demonstration video, live demo, and installation guides are available at: https://freeeval.zhuohao.me/</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
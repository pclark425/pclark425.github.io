<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8067 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8067</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8067</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-8221f1597000543432b7021ca79dbc51a7a63f9c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8221f1597000543432b7021ca79dbc51a7a63f9c" target="_blank">Is ChatGPT a Good NLG Evaluator? A Preliminary Study</a></p>
                <p><strong>Paper Venue:</strong> NEWSUM</p>
                <p><strong>Paper TL;DR:</strong> Experimental results show that compared with previous automatic metrics, ChatGPT achieves state-of-the-art or competitive correlation with human judgments in most cases and it is hoped this preliminary study could prompt the emergence of a general-purposed reliable NLG metric.</p>
                <p><strong>Paper Abstract:</strong> Recently, the emergence of ChatGPT has attracted wide attention from the computational linguistics community. Many prior studies have shown that ChatGPT achieves remarkable performance on various NLP tasks in terms of automatic evaluation metrics. However, the ability of ChatGPT to serve as an evaluation metric is still underexplored. Considering assessing the quality of natural language generation (NLG) models is an arduous task and NLG metrics notoriously show their poor correlation with human judgments, we wonder whether ChatGPT is a good NLG evaluation metric. In this report, we provide a preliminary meta-evaluation on ChatGPT to show its reliability as an NLG metric. In detail, we regard ChatGPT as a human evaluator and give task-specific (e.g., summarization) and aspect-specific (e.g., relevance) instruction to prompt ChatGPT to evaluate the generated results of NLG models. We conduct experiments on five NLG meta-evaluation datasets (including summarization, story generation and data-to-text tasks). Experimental results show that compared with previous automatic metrics, ChatGPT achieves state-of-the-art or competitive correlation with human judgments in most cases. In addition, we find that the effectiveness of the ChatGPT evaluator might be influenced by the creation method of the meta-evaluation datasets. For the meta-evaluation datasets which are created greatly depending on the reference and thus are biased, the ChatGPT evaluator might lose its effectiveness. We hope our preliminary study could prompt the emergence of a general-purposed reliable NLG metric.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8067.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8067.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT vs Human (SummEval)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of ChatGPT evaluator and human judgments on SummEval (news summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical comparison showing ChatGPT used as an automatic evaluator (prompted DA or star, with/without reference) achieves the highest sample-level correlation with human judgments on SummEval for several aspects (coherence, relevance, consistency, fluency) relative to many prior automatic metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Is ChatGPT a Good NLG Evaluator? A Preliminary Study</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Is ChatGPT a Good NLG Evaluator? A Preliminary Study</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Text summarization</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>SummEval (CNN/DM; aspects: coherence, relevance, consistency, fluency)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI ChatGPT (web interface, default temperature). Version not explicitly specified; experiments run Feb 24-27 and Mar 17-22, 2023.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human annotators used by SummEval dataset (dataset-provided human scores on multiple aspects); paper uses those human judgments as ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Sample-level Spearman correlation (also Pearson and Kendall's Tau reported)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.425</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>prompt sensitivity; reduced effectiveness on datasets with strong reference/lexical bias</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>ChatGPT attains state-of-the-art sample-level correlation with human judgments on SummEval, but its performance is sensitive to prompt design and can be influenced by dataset annotation biases (it performs best when human evaluation is reference-free). It also outputs illustrative descriptions alongside numeric scores.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>High correlation with human judgments on this dataset, potential to reduce human labeling effort via automatic, interpretable-scoring with textual rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Prompts: DA (0-100) and Star (1-5), with and without explicit human reference; ChatGPT queried via web UI (default settings); correlations computed at sample-level and dataset-level using Spearman, Pearson, and Kendall's Tau; numerical scores extracted from ChatGPT outputs via heuristics; 16 model-generated summaries per example in SummEval.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is ChatGPT a Good NLG Evaluator? A Preliminary Study', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8067.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8067.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT vs Human (NewsRoom)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of ChatGPT evaluator and human judgments on NewsRoom (news summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation showing ChatGPT (with specific prompts) achieves competitive sample-level correlation with human judgments on NewsRoom and outperforms several similarity-based metrics (ROUGE, BERTScore) though absolute performance depends on prompt and whether references are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Is ChatGPT a Good NLG Evaluator? A Preliminary Study</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Is ChatGPT a Good NLG Evaluator? A Preliminary Study</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Text summarization</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>NewsRoom (human judgments on coherence, relevance, informativeness, fluency)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI ChatGPT (web interface, default temperature); version unspecified; experiments Feb-Mar 2023.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Human annotators used by the NewsRoom meta-evaluation (dataset-provided annotations across aspects).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Sample-level Spearman correlation (also Pearson and Kendall's Tau reported)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.504</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>sensitivity to prompt; varying performance across aspects (some aspects depend more on reference similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>ChatGPT outperforms many common summarization metrics on NewsRoom at sample-level when prompted appropriately, but results vary by prompt type (DA vs stars) and whether the reference is provided; ChatGPT still sensitive to prompt wording.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Better correlation than many surface- and embedding-based metrics on this dataset; automates multi-aspect judgements.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Prompts: DA and Star, with/without reference; ChatGPT via web UI (default settings); sample-level correlations computed; NewsRoom provides 7 model-generated summaries per input; scores extracted heuristically from ChatGPT outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is ChatGPT a Good NLG Evaluator? A Preliminary Study', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8067.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8067.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT vs Human (RealSumm)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of ChatGPT evaluator and human judgments on RealSumm (reference-oriented pyramid summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>On RealSumm (pyramid-based human annotations that emphasize overlap with reference content units), ChatGPT performs substantially worse than traditional lexical-overlap metrics (e.g., ROUGE), showing low correlation with human scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Is ChatGPT a Good NLG Evaluator? A Preliminary Study</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Is ChatGPT a Good NLG Evaluator? A Preliminary Study</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Text summarization (pyramid-style evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>RealSumm (pyramid recall evaluated over 25 model-generated summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI ChatGPT (web interface, default temperature), version unspecified.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Pyramid-method human annotators who extract semantic content units from references and score systems by overlap with those units (RealSumm annotation protocol).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Sample-level Spearman correlation (also Pearson and Kendall's Tau reported)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.195</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>fails on reference-biased (lexical) evaluation setups; low sensitivity to reference-overlap signals emphasized by pyramid annotation</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>ChatGPT underperforms relative to n-gram based metrics on RealSumm because the human annotations are reference-oriented (lexical bias): similarity to reference strongly predicts human score, which advantages ROUGE and similar metrics but not ChatGPT's more semantic/rationale-based judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Noted potential for more semantic, human-like judgments in non-reference-based settings; however, advantage is limited in reference-centric benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Prompts: DA and Star, with/without reference; sample-level and dataset-level correlations computed; RealSumm contains evaluations over 25 model-generated summaries; paper highlights that ChatGPT's low scores here are due to dataset's pyramid/reference-based annotation method.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is ChatGPT a Good NLG Evaluator? A Preliminary Study', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8067.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8067.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT vs Human (OpenMEVA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of ChatGPT evaluator and human judgments on OpenMEVA-ROC (open-ended story generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT attains the highest reported sample-level and dataset-level correlations with human judgments on OpenMEVA (story generation), outperforming other automatic metrics (notably perplexity and surface-similarity metrics), indicating strong performance on open-ended creative NLG tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Is ChatGPT a Good NLG Evaluator? A Preliminary Study</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Is ChatGPT a Good NLG Evaluator? A Preliminary Study</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Story generation (open-ended)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>OpenMEVA-ROC (OpenMEVA benchmark for story generation; human overall quality annotations over 5 model-generated storylines)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI ChatGPT (web interface, default temperature); no model card or version provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>OpenMEVA dataset human annotators who rated storylines (overall quality, naturalness, informativeness); five model outputs annotated per prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Sample-level Spearman correlation (also Pearson and Kendall's Tau reported)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.507</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>prompt sensitivity (different prompt designs yield notable performance differences)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>ChatGPT significantly outperforms traditional similarity-based metrics on creative, open-ended story generation where many diverse outputs can be high-quality, suggesting ChatGPT captures aspects of quality beyond surface overlap; prompt choice (DA vs DA w/ ref) affects results.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Stronger, more human-aligned judgments on creative/open-ended generation; better handling of diversity and creativity than surface-similarity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Prompts: DA and Star, with/without reference; ChatGPT queried via web UI (default); sample-level and dataset-level correlations computed; OpenMEVA provides 5 model-generated storylines per conditioned prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is ChatGPT a Good NLG Evaluator? A Preliminary Study', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8067.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8067.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT vs Human (BAGEL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of ChatGPT evaluator and human judgments on BAGEL (data-to-text generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>On the BAGEL data-to-text meta-evaluation, ChatGPT achieves competitive dataset-level correlations with human judgments (especially when references are provided) but is constrained for aspects defined relative to references (e.g., informativeness requires providing the reference).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Is ChatGPT a Good NLG Evaluator? A Preliminary Study</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Is ChatGPT a Good NLG Evaluator? A Preliminary Study</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Data-to-text generation (table-to-text)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>BAGEL (data-to-text generation meta-evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI ChatGPT (web interface, default temperature); version unspecified.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>BAGEL dataset human annotators (ratings for informativeness, naturalness, quality); informativeness defined relative to gold reference.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Dataset-level Spearman correlation (also Pearson and Kendall's Tau reported)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.278</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>cannot evaluate some reference-relative aspects (e.g., informativeness) in reference-free mode; sensitivity to prompt and to whether reference is provided</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>ChatGPT attains competitive correlations with state-of-the-art metrics for data-to-text when references are included; however, for aspects that are explicitly reference-dependent (informativeness), reference-free judgments are not applicable and performance depends on prompt and annotation definitions.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Competitive correlations and flexibility (can be prompted to be reference-based or reference-free); can produce nuanced assessments beyond surface overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Prompts: DA and Star, with reference required for some aspects (informativeness); ChatGPT via web UI (default); dataset-level correlations computed; numeric extraction via heuristics; BAGEL protocol (human judgments) used as ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is ChatGPT a Good NLG Evaluator? A Preliminary Study', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models are state-of-the-art evaluators of translation quality <em>(Rating: 2)</em></li>
                <li>OpenMEVA: A benchmark for evaluating open-ended story generation metrics <em>(Rating: 2)</em></li>
                <li>Evaluating large language models at evaluating instruction following <em>(Rating: 2)</em></li>
                <li>Reevaluating evaluation in text summarization <em>(Rating: 2)</em></li>
                <li>Is chatgpt a good translator? a preliminary study <em>(Rating: 1)</em></li>
                <li>Gptscore: Evaluate as you desire <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8067",
    "paper_id": "paper-8221f1597000543432b7021ca79dbc51a7a63f9c",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "ChatGPT vs Human (SummEval)",
            "name_full": "Comparison of ChatGPT evaluator and human judgments on SummEval (news summarization)",
            "brief_description": "Empirical comparison showing ChatGPT used as an automatic evaluator (prompted DA or star, with/without reference) achieves the highest sample-level correlation with human judgments on SummEval for several aspects (coherence, relevance, consistency, fluency) relative to many prior automatic metrics.",
            "citation_title": "Is ChatGPT a Good NLG Evaluator? A Preliminary Study",
            "mention_or_use": "use",
            "paper_title": "Is ChatGPT a Good NLG Evaluator? A Preliminary Study",
            "evaluation_task": "Text summarization",
            "dataset_name": "SummEval (CNN/DM; aspects: coherence, relevance, consistency, fluency)",
            "judge_model_name": "ChatGPT",
            "judge_model_details": "OpenAI ChatGPT (web interface, default temperature). Version not explicitly specified; experiments run Feb 24-27 and Mar 17-22, 2023.",
            "human_evaluator_type": "Human annotators used by SummEval dataset (dataset-provided human scores on multiple aspects); paper uses those human judgments as ground truth.",
            "agreement_metric": "Sample-level Spearman correlation (also Pearson and Kendall's Tau reported)",
            "agreement_score": 0.425,
            "reported_loss_aspects": "prompt sensitivity; reduced effectiveness on datasets with strong reference/lexical bias",
            "qualitative_findings": "ChatGPT attains state-of-the-art sample-level correlation with human judgments on SummEval, but its performance is sensitive to prompt design and can be influenced by dataset annotation biases (it performs best when human evaluation is reference-free). It also outputs illustrative descriptions alongside numeric scores.",
            "advantages_of_llm_judge": "High correlation with human judgments on this dataset, potential to reduce human labeling effort via automatic, interpretable-scoring with textual rationales.",
            "experimental_setting": "Prompts: DA (0-100) and Star (1-5), with and without explicit human reference; ChatGPT queried via web UI (default settings); correlations computed at sample-level and dataset-level using Spearman, Pearson, and Kendall's Tau; numerical scores extracted from ChatGPT outputs via heuristics; 16 model-generated summaries per example in SummEval.",
            "uuid": "e8067.0",
            "source_info": {
                "paper_title": "Is ChatGPT a Good NLG Evaluator? A Preliminary Study",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "ChatGPT vs Human (NewsRoom)",
            "name_full": "Comparison of ChatGPT evaluator and human judgments on NewsRoom (news summarization)",
            "brief_description": "Evaluation showing ChatGPT (with specific prompts) achieves competitive sample-level correlation with human judgments on NewsRoom and outperforms several similarity-based metrics (ROUGE, BERTScore) though absolute performance depends on prompt and whether references are provided.",
            "citation_title": "Is ChatGPT a Good NLG Evaluator? A Preliminary Study",
            "mention_or_use": "use",
            "paper_title": "Is ChatGPT a Good NLG Evaluator? A Preliminary Study",
            "evaluation_task": "Text summarization",
            "dataset_name": "NewsRoom (human judgments on coherence, relevance, informativeness, fluency)",
            "judge_model_name": "ChatGPT",
            "judge_model_details": "OpenAI ChatGPT (web interface, default temperature); version unspecified; experiments Feb-Mar 2023.",
            "human_evaluator_type": "Human annotators used by the NewsRoom meta-evaluation (dataset-provided annotations across aspects).",
            "agreement_metric": "Sample-level Spearman correlation (also Pearson and Kendall's Tau reported)",
            "agreement_score": 0.504,
            "reported_loss_aspects": "sensitivity to prompt; varying performance across aspects (some aspects depend more on reference similarity)",
            "qualitative_findings": "ChatGPT outperforms many common summarization metrics on NewsRoom at sample-level when prompted appropriately, but results vary by prompt type (DA vs stars) and whether the reference is provided; ChatGPT still sensitive to prompt wording.",
            "advantages_of_llm_judge": "Better correlation than many surface- and embedding-based metrics on this dataset; automates multi-aspect judgements.",
            "experimental_setting": "Prompts: DA and Star, with/without reference; ChatGPT via web UI (default settings); sample-level correlations computed; NewsRoom provides 7 model-generated summaries per input; scores extracted heuristically from ChatGPT outputs.",
            "uuid": "e8067.1",
            "source_info": {
                "paper_title": "Is ChatGPT a Good NLG Evaluator? A Preliminary Study",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "ChatGPT vs Human (RealSumm)",
            "name_full": "Comparison of ChatGPT evaluator and human judgments on RealSumm (reference-oriented pyramid summarization)",
            "brief_description": "On RealSumm (pyramid-based human annotations that emphasize overlap with reference content units), ChatGPT performs substantially worse than traditional lexical-overlap metrics (e.g., ROUGE), showing low correlation with human scores.",
            "citation_title": "Is ChatGPT a Good NLG Evaluator? A Preliminary Study",
            "mention_or_use": "use",
            "paper_title": "Is ChatGPT a Good NLG Evaluator? A Preliminary Study",
            "evaluation_task": "Text summarization (pyramid-style evaluation)",
            "dataset_name": "RealSumm (pyramid recall evaluated over 25 model-generated summaries)",
            "judge_model_name": "ChatGPT",
            "judge_model_details": "OpenAI ChatGPT (web interface, default temperature), version unspecified.",
            "human_evaluator_type": "Pyramid-method human annotators who extract semantic content units from references and score systems by overlap with those units (RealSumm annotation protocol).",
            "agreement_metric": "Sample-level Spearman correlation (also Pearson and Kendall's Tau reported)",
            "agreement_score": 0.195,
            "reported_loss_aspects": "fails on reference-biased (lexical) evaluation setups; low sensitivity to reference-overlap signals emphasized by pyramid annotation",
            "qualitative_findings": "ChatGPT underperforms relative to n-gram based metrics on RealSumm because the human annotations are reference-oriented (lexical bias): similarity to reference strongly predicts human score, which advantages ROUGE and similar metrics but not ChatGPT's more semantic/rationale-based judgments.",
            "advantages_of_llm_judge": "Noted potential for more semantic, human-like judgments in non-reference-based settings; however, advantage is limited in reference-centric benchmarks.",
            "experimental_setting": "Prompts: DA and Star, with/without reference; sample-level and dataset-level correlations computed; RealSumm contains evaluations over 25 model-generated summaries; paper highlights that ChatGPT's low scores here are due to dataset's pyramid/reference-based annotation method.",
            "uuid": "e8067.2",
            "source_info": {
                "paper_title": "Is ChatGPT a Good NLG Evaluator? A Preliminary Study",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "ChatGPT vs Human (OpenMEVA)",
            "name_full": "Comparison of ChatGPT evaluator and human judgments on OpenMEVA-ROC (open-ended story generation)",
            "brief_description": "ChatGPT attains the highest reported sample-level and dataset-level correlations with human judgments on OpenMEVA (story generation), outperforming other automatic metrics (notably perplexity and surface-similarity metrics), indicating strong performance on open-ended creative NLG tasks.",
            "citation_title": "Is ChatGPT a Good NLG Evaluator? A Preliminary Study",
            "mention_or_use": "use",
            "paper_title": "Is ChatGPT a Good NLG Evaluator? A Preliminary Study",
            "evaluation_task": "Story generation (open-ended)",
            "dataset_name": "OpenMEVA-ROC (OpenMEVA benchmark for story generation; human overall quality annotations over 5 model-generated storylines)",
            "judge_model_name": "ChatGPT",
            "judge_model_details": "OpenAI ChatGPT (web interface, default temperature); no model card or version provided in paper.",
            "human_evaluator_type": "OpenMEVA dataset human annotators who rated storylines (overall quality, naturalness, informativeness); five model outputs annotated per prompt.",
            "agreement_metric": "Sample-level Spearman correlation (also Pearson and Kendall's Tau reported)",
            "agreement_score": 0.507,
            "reported_loss_aspects": "prompt sensitivity (different prompt designs yield notable performance differences)",
            "qualitative_findings": "ChatGPT significantly outperforms traditional similarity-based metrics on creative, open-ended story generation where many diverse outputs can be high-quality, suggesting ChatGPT captures aspects of quality beyond surface overlap; prompt choice (DA vs DA w/ ref) affects results.",
            "advantages_of_llm_judge": "Stronger, more human-aligned judgments on creative/open-ended generation; better handling of diversity and creativity than surface-similarity metrics.",
            "experimental_setting": "Prompts: DA and Star, with/without reference; ChatGPT queried via web UI (default); sample-level and dataset-level correlations computed; OpenMEVA provides 5 model-generated storylines per conditioned prompt.",
            "uuid": "e8067.3",
            "source_info": {
                "paper_title": "Is ChatGPT a Good NLG Evaluator? A Preliminary Study",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "ChatGPT vs Human (BAGEL)",
            "name_full": "Comparison of ChatGPT evaluator and human judgments on BAGEL (data-to-text generation)",
            "brief_description": "On the BAGEL data-to-text meta-evaluation, ChatGPT achieves competitive dataset-level correlations with human judgments (especially when references are provided) but is constrained for aspects defined relative to references (e.g., informativeness requires providing the reference).",
            "citation_title": "Is ChatGPT a Good NLG Evaluator? A Preliminary Study",
            "mention_or_use": "use",
            "paper_title": "Is ChatGPT a Good NLG Evaluator? A Preliminary Study",
            "evaluation_task": "Data-to-text generation (table-to-text)",
            "dataset_name": "BAGEL (data-to-text generation meta-evaluation)",
            "judge_model_name": "ChatGPT",
            "judge_model_details": "OpenAI ChatGPT (web interface, default temperature); version unspecified.",
            "human_evaluator_type": "BAGEL dataset human annotators (ratings for informativeness, naturalness, quality); informativeness defined relative to gold reference.",
            "agreement_metric": "Dataset-level Spearman correlation (also Pearson and Kendall's Tau reported)",
            "agreement_score": 0.278,
            "reported_loss_aspects": "cannot evaluate some reference-relative aspects (e.g., informativeness) in reference-free mode; sensitivity to prompt and to whether reference is provided",
            "qualitative_findings": "ChatGPT attains competitive correlations with state-of-the-art metrics for data-to-text when references are included; however, for aspects that are explicitly reference-dependent (informativeness), reference-free judgments are not applicable and performance depends on prompt and annotation definitions.",
            "advantages_of_llm_judge": "Competitive correlations and flexibility (can be prompted to be reference-based or reference-free); can produce nuanced assessments beyond surface overlap.",
            "experimental_setting": "Prompts: DA and Star, with reference required for some aspects (informativeness); ChatGPT via web UI (default); dataset-level correlations computed; numeric extraction via heuristics; BAGEL protocol (human judgments) used as ground truth.",
            "uuid": "e8067.4",
            "source_info": {
                "paper_title": "Is ChatGPT a Good NLG Evaluator? A Preliminary Study",
                "publication_date_yy_mm": "2023-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models are state-of-the-art evaluators of translation quality",
            "rating": 2
        },
        {
            "paper_title": "OpenMEVA: A benchmark for evaluating open-ended story generation metrics",
            "rating": 2
        },
        {
            "paper_title": "Evaluating large language models at evaluating instruction following",
            "rating": 2
        },
        {
            "paper_title": "Reevaluating evaluation in text summarization",
            "rating": 2
        },
        {
            "paper_title": "Is chatgpt a good translator? a preliminary study",
            "rating": 1
        },
        {
            "paper_title": "Gptscore: Evaluate as you desire",
            "rating": 1
        }
    ],
    "cost": 0.01411875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Is ChatGPT a Good NLG Evaluator? A Preliminary Study</h1>
<p>Jiaan Wang ${ }^{1}$ 4, Yunlong Liang ${ }^{2}$; Fandong Meng ${ }^{3}$<br>Zengkui Sun ${ }^{2}$, Haoxiang Shi ${ }^{4}$, Zhixu Li ${ }^{5}$, Jinan Xu ${ }^{2}$, Jianfeng Qu ${ }^{1}$ and Jie Zhou ${ }^{3}$<br>${ }^{1}$ Soochow University, Suzhou, China<br>${ }^{2}$ Beijing Jiaotong University, Beijing, China<br>${ }^{3}$ Pattern Recognition Center, WeChat AI, Tencent Inc, China<br>${ }^{4}$ Waseda University, Tokyo, Japan<br>${ }^{5}$ Fudan Unversity, Shanghai, China<br>jawang.nlp@gmail.com {yunlongliang, zengksun}@bjtu.edu.cn<br>fandongmeng@tencent.com hollis.shi@toki.waseda.jp</p>
<h4>Abstract</h4>
<p>Recently, the emergence of ChatGPT has attracted wide attention from the computational linguistics community. Many prior studies have shown that ChatGPT achieves remarkable performance on various NLP tasks in terms of automatic evaluation metrics. However, the ability of ChatGPT to serve as an evaluation metric is still underexplored. Considering assessing the quality of natural language generation (NLG) models is an arduous task and NLG metrics notoriously show their poor correlation with human judgments, we wonder whether ChatGPT is a good NLG evaluation metric.</p>
<p>In this report, we provide a preliminary metaevaluation on ChatGPT to show its reliability as an NLG metric. In detail, we regard ChatGPT as a human evaluator and give task-specific (e.g., summarization) and aspect-specific (e.g., relevance) instruction to prompt ChatGPT to evaluate the generated results of NLG models. We conduct experiments on five NLG metaevaluation datasets (including summarization, story generation and data-to-text tasks). Experimental results show that compared with previous automatic metrics, ChatGPT achieves state-of-the-art or competitive correlation with human judgments in most cases. In addition, we find that the effectiveness of the ChatGPT evaluator might be influenced by the creation method of the meta-evaluation datasets. For the meta-evaluation datasets which are created greatly depending on the reference and thus are biased, the ChatGPT evaluator might lose its effectiveness. We hope our preliminary study could prompt the emergence of a generalpurposed reliable NLG metric. ${ }^{1}$</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Prompting ChatGPT as an evaluator to score the generated results of NLG models (taking news summarization as an example).</p>
<h2>1 Introduction</h2>
<p>Pre-trained large language models (LLMs; e.g., GPT-3.5, ChatGPT and GPT-4), which are performed through chatting (or asking) with it, have obtained promising results on various natural language understanding (NLU) and natural language generation (NLG) downstream tasks (Ouyang et al., 2022; Koco≈Ñ et al., 2023; Qin et al., 2023; Huang et al., 2023; Yang et al., 2023; Rao et al., 2023; Bang et al., 2023; Zuccon and Koopman, 2023). For example, Zhong et al. (2023) show that ChatGPT can attain the comparable understanding abil-</p>
<p>ity to some fine-tuned BERT-style models on NLU tasks while failing to surpass current task-specific NLU models. Wei et al. (2023) prove that ChatGPT can achieve good performance and even surpasses some full-shot models on several datasets through a multi-turn question-answering manner. For NLG tasks, Jiao et al. (2023) claim that ChatGPT performs competitively with commercial translation products (e.g., Google Translator) on high-resource European languages. Wang et al. (2023a) demonstrate that ChatGPT can balance well between informativeness and conciseness, and generate great cross-lingual summaries. Although impressive performance on these tasks in terms of automatic evaluation metrics has been shown, it is still not clear whether ChatGPT can evaluate the quality of textual generations as a human does.</p>
<p>Recently, using pre-trained language models as NLG evaluation metric, e.g., MoverScore (Zhao et al., 2019), BERTScore (Zhang et al., 2020), COMET (Rei et al., 2020), BLEURT (Sellam et al., 2020), BARTScore (Yuan et al., 2021) and MAUVE (Pillutla et al., 2022), receives increasing attention since it offers a decent human-related judgment from a deep semantic perspective. Given the powerful ability of ChatGPT as an intelligent conversational LLM, researchers also attempt to investigate whether it can evaluate the translation quality as a human evaluator (Kocmi and Federmann, 2023). However, the automated assessment of the general generation quality of NLG models still remains underexplored.</p>
<p>In this report, we aim to answer the following research question: Is ChatGPT a good NLG evaluator? To this end, we regard ChatGPT as a human evaluator and give task-specific (e.g., summarization) and aspect-specific (e.g., relevance) instruction to prompt ChatGPT to evaluate the generation of NLG models. As the example shows in Figure 1, we also attempt different scoring criteria and whether to provide golden references in the prompts to systematically test the reliability of the ChatGPT evaluator. We conduct experiments on five widely-used NLG meta-evaluation datasets (including summarization, story generation and data-to-text tasks). Experimental results show that ChatGPT exhibits a high correlation with human judgment in most cases especially for the story generation task, indicating its potential as an NLG metric. In addition, we find that the ChatGPT evaluator is sensitive to the prompts, and for different tasks or aspects, the prompts should be carefully designed. Moreover, the creation method of the meta-evaluation datasets has a significant influence on the effectiveness of different evaluation metrics. If a meta-evaluation dataset is created greatly depending on the reference, the similarity between model generation and references serves as a strong signal to reflect human judgments, where simple similarity-based metrics (e.g., ROUGE) can achieve very strong performance. Therefore, the ChatGPT evaluator might lose its effectiveness in such situations.</p>
<p>Our main contributions are concluded as follows:</p>
<ul>
<li>To our knowledge, we are the first to utilize ChatGPT as a general NLG evaluation metric to study its correlations with human judgments.</li>
<li>We use task-specific and aspect-specific prompts to guide ChatGPT to perform as a referencefree or reference-based NLG metric, and evaluate its effectiveness on five widely-used metaevaluation datasets covering three NLG tasks.</li>
<li>We find that the ChatGPT evaluator has a high correlation with humans in most cases, especially for creative NLG tasks (e.g., story generation) where multiple generations can satisfy humans.</li>
<li>We find that the ChatGPT evaluator is sensitive to the prompts. For different tasks and aspects, the prompt should be carefully designed.</li>
<li>We find that the involved biases of the NLG metaevaluation datasets also influence the effectiveness of NLG metrics, and might lead to the limited effectiveness of the ChatGPT evaluator.</li>
</ul>
<h2>2 Related Work</h2>
<h3>2.1 NLG Metrics</h3>
<p>A good automatic NLG metric can effectively indicate the quality of the textual generations and thus can save lots of human labor from conducting human evaluation. Therefore, it is vital to design automatic evaluation metrics for NLG tasks, e.g., text summarization, story generation, data-to-text generation, machine translation, and many others. Generally, the score that points out how well the systems perform on each task is computed by comparing the system texts with one or more reference texts for semantic matching. In the literature, the metrics can be roughly categorized into four types:
$n$-gram-based Metrics. Essentially, the $n$-grambased metrics aim to measure the lexical overlap between a generated text and a reference text.</p>
<p>The standard $n$-gram overlap-based metrics generally include ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), Distinct-n (Li et al., 2016), and METEOR (Denkowski and Lavie, 2011). For example, ROUGE is the dominant metric in the summarization evaluation area. Its variants consider the overlap of unigrams (ROUGE-1) and bigrams (ROUGE-2), among others. BLEU metric is the common practice for the machine translation evaluation area. Although these metrics achieve good correlations (typically large overlaps) with golden references, they are not general enough because a system summary might convey the same meaning while using different surface forms.</p>
<p>Embedding-based Metrics. To further improve semantic similarity between a generated text and a reference text, embedding-based metrics are proposed based on the word embeddings (e.g., WMD (Kusner et al., 2015)) or sentence embeddings (e.g., BERTScore (Zhang et al., 2020) and MoverScore (Zhao et al., 2019)). These metrics further bridge the gap with human-related judgment while they largely depend on the quality of embeddings, which may limit their potential.</p>
<p>LLM-based Metrics. With the development of LLMs, some researchers show that LLMs could achieve great correlation with human judgment, i.e., BARTScore (Yuan et al., 2021), and GPTScore (Fu et al., 2023). However, ChatGPT, as a more powerful conversational LLM, has not been investigated to evaluate the quality of the NLG model outputs.</p>
<p>Other Metrics. In different research fields, there are some paraphraser-based or task-specific metrics. For example, PRISM (Thompson and Post, 2020) is proposed to evaluate translation outputs based on the pre-trained paraphrase models. StoryER (Chen et al., 2022), a learning metric, mimics human preference when judging a story by three steps: Ranking, Rating, and Reasoning based on a specific story-generation dataset. Besides, a specifically developed metric named PARENT (Dhingra et al., 2019) is designed for the table2text generation. Other statistical indicators, such as omission errors, hallucination errors, addition errors, duplication errors, and extrinsic errors, are also applied in the table2text task. Although these metrics have obtained impressive results, human evaluation is still inevitable in table2text.</p>
<h3>2.2 Research on ChatGPT</h3>
<p>In recent years, from BERT (Devlin et al., 2019) to ChatGPT (OpenAI, 2022), a large number of pretrained language models have been proposed one after another. Both their parameters and ability are gradually increased, facilitating much-advanced techniques. In particular, ChatGPT, which shows us a revolutionary change as an intelligent conversational large language model, sends shock waves through the research community and industries that have continued to reverberate to this day. With the emergence of ChatGPT, there are two growing research interests related to it: (1) leveraging ChatGPT to deal with various NLP tasks and evaluating its performance using traditional task-specific metrics (i.e., evaluation), and (2) using as a metric to evaluate the outputs of other task-specific models (i.e., evaluator) (Kocmi and Federmann, 2023).</p>
<p>Evaluation. Generally, the evaluation tasks on ChatGPT can be divided into two categories, i.e., natural language understanding (NLU) and natural language generation (NLG). For NLU tasks, some researchers find that ChatGPT covers almost all NLU tasks (e.g., sentiment analysis, textual similarity and textual entailment) and achieves competitive or even better performance (Qin et al., 2023; Bang et al., 2023; Zhong et al., 2023). For NLG tasks, machine translation (Jiao et al., 2023), summarization (Yang et al., 2023), query generation (Wang et al., 2023b), and radiology report simplification (Jeblick et al., 2022) are involved. Different from them, we regard ChatGPT as a human evaluator to automatically assess the quality of general textual generations rather than using it for solving tasks.</p>
<p>Evaluator. As an evaluator, there are two studies that evaluate the quality of translation (Kocmi and Federmann, 2023) and human personalities (Rao et al., 2023) by prompting ChatGPT. However, in this work, we aim to evaluate the more general textual outputs to further show the ability of ChatGPT as a general NLG metric.</p>
<h2>3 ChatGPT for NLG Evaluation</h2>
<p>In this section, we discuss how to prompt ChatGPT to serve as a reference-free NLG metric (¬ß 3.1) or a reference-based NLG metric (¬ß 3.2) to evaluate the generation quality of NLG models. We take the news summarization task as an example, and give the details of the prompt templates.</p>
<h3>3.1 Reference-free Metric</h3>
<p>To evaluate the generation quality of NLG models, we regard ChatGPT as a human evaluator and give it evaluation instruction via different prompts. Each prompt should specify (1) which NLG task (e.g., summarization) needs to be evaluated and (2) which aspect (e.g., fluency) of the generation result should be assessed currently.</p>
<p>Inspired by Kocmi and Federmann (2023), we utilize the following two prompts: direct assessment (DA) and one-to-five stars ranking (star).</p>
<h2>(DA Prompt)</h2>
<p>Score the following [task-ins] with respect to [aspect] on a continuous scale from 0 to 100 , where a score of zero means "[ant-aspect]" and score of one hundred means "perfect [aspect]". Note that [aspect] measures [aspect-ins].
[Conditioned Text]
[Generated Text]
Scores:</p>
<h2>(Star Prompt)</h2>
<p>Score the following [task-ins] with respect to [aspect] with one to five stars, where one star means "[ant-aspect]" and five stars means "perfect [aspect]". Note that [aspect] measures [aspect-ins].
[Conditioned Text]
[Generated Text]
Stars:
where [task-ins] and [aspect-ins] are the instructions of the current task and aspect, respectively. [aspect] and [ant-aspect] denote the evaluated aspect and its antonym, respectively. [Conditioned Text] is the input of NLG models while [Generated Text] is the output. For example, when evaluating news summarization models in terms of fluency, the DA prompt may be like this:</p>
<p>Score the following news summarization given the corresponding news with respect to fluency on a continuous scale from 0 to 100 , where a score of zero means "disfluency" and score of one hundred means "perfect fluency". Note that fluency measures the quality of individual sentences, are they well-written and grammati-
cally correct. Consider the quality of individual sentences.
News: [a news article]
Summary: [one generated summary]
Scores:
In this manner, both the details of the task and the evaluation aspect are given to ChatGPT. Next, ChatGPT will give its judgment (e.g., "score: 70") and the corresponding illustrative description (e.g., "the summary covers the main points of the news, but ..."). A specific example is shown in Figure 1. Finally, the numerical scores could be extracted via several simple heuristic rules.</p>
<h3>3.2 Reference-based Metric</h3>
<p>In addition to reference-free metrics, we explicitly mention the golden references in the prompts to make ChatGPT a reference-based NLG metric:
(DA Prompt w/ Reference)
Score the following [task-ins] with respect to [aspect] on a continuous scale from 0 to 100 , where a score of zero means "[ant-aspect]" and score of one hundred means "perfect [aspect]". Note that [aspect] measures [aspect-ins].
[Conditioned Text]
Human reference: [A Reference]
[Generated Text]
Scores:
The star prompt with reference is also formed in a similar way:
(Star Prompt w/ Reference))
Score the following [task-ins] with respect to [aspect] with one to five stars, where one star means "[ant-aspect]" and five stars means "perfect [aspect]". Note that [aspect] measures [aspect-ins].
[Conditioned Text]
Human reference: [A Reference]
[Generated Text]
Stars:
In this way, the ChatGPT evaluator will make its judgment and give the evaluation results under the consideration of the golden references.</p>
<table>
<thead>
<tr>
<th>Metrics</th>
<th>Coherence</th>
<th></th>
<th></th>
<th>Relevance</th>
<th></th>
<th></th>
<th>Consistency</th>
<th></th>
<th></th>
<th>Fluency</th>
<th></th>
<th></th>
<th>Avg.</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Spear.</td>
<td>Pear.</td>
<td>Kend.</td>
<td>Spear.</td>
<td>Pear.</td>
<td>Kend.</td>
<td>Spear.</td>
<td>Pear.</td>
<td>Kend.</td>
<td>Spear.</td>
<td>Pear.</td>
<td>Kend.</td>
<td>Spear.</td>
<td>Pear.</td>
<td>Kend.</td>
</tr>
<tr>
<td>ROUGE-1</td>
<td>0.167</td>
<td>0.160</td>
<td>0.126</td>
<td>0.326</td>
<td>0.359</td>
<td>0.252</td>
<td>0.160</td>
<td>0.224</td>
<td>0.130</td>
<td>0.115</td>
<td>0.158</td>
<td>0.094</td>
<td>0.192</td>
<td>0.225</td>
<td>0.150</td>
</tr>
<tr>
<td>ROUGE-2</td>
<td>0.184</td>
<td>0.174</td>
<td>0.139</td>
<td>0.290</td>
<td>0.327</td>
<td>0.219</td>
<td>0.187</td>
<td>0.246</td>
<td>0.155</td>
<td>0.159</td>
<td>0.185</td>
<td>0.128</td>
<td>0.205</td>
<td>0.233</td>
<td>0.160</td>
</tr>
<tr>
<td>ROUGE-L</td>
<td>0.128</td>
<td>0.102</td>
<td>0.099</td>
<td>0.311</td>
<td>0.342</td>
<td>0.237</td>
<td>0.115</td>
<td>0.189</td>
<td>0.092</td>
<td>0.105</td>
<td>0.141</td>
<td>0.084</td>
<td>0.165</td>
<td>0.194</td>
<td>0.128</td>
</tr>
<tr>
<td>BERTScore</td>
<td>0.283</td>
<td>0.310</td>
<td>0.211</td>
<td>0.311</td>
<td>0.346</td>
<td>0.243</td>
<td>0.110</td>
<td>0.152</td>
<td>0.090</td>
<td>0.192</td>
<td>0.209</td>
<td>0.158</td>
<td>0.224</td>
<td>0.254</td>
<td>0.175</td>
</tr>
<tr>
<td>MoverScore</td>
<td>0.159</td>
<td>0.167</td>
<td>0.118</td>
<td>0.318</td>
<td>0.371</td>
<td>0.244</td>
<td>0.157</td>
<td>0.224</td>
<td>0.127</td>
<td>0.129</td>
<td>0.176</td>
<td>0.105</td>
<td>0.191</td>
<td>0.234</td>
<td>0.148</td>
</tr>
<tr>
<td>PRISM</td>
<td>0.249</td>
<td>0.258</td>
<td>0.196</td>
<td>0.212</td>
<td>0.232</td>
<td>0.163</td>
<td>0.345</td>
<td>0.352</td>
<td>0.285</td>
<td>0.254</td>
<td>0.264</td>
<td>0.205</td>
<td>0.265</td>
<td>0.276</td>
<td>0.212</td>
</tr>
<tr>
<td>BARTScore</td>
<td>0.322</td>
<td>0.345</td>
<td>0.250</td>
<td>0.264</td>
<td>0.290</td>
<td>0.197</td>
<td>0.311</td>
<td>0.321</td>
<td>0.256</td>
<td>0.248</td>
<td>0.260</td>
<td>0.203</td>
<td>0.286</td>
<td>0.304</td>
<td>0.227</td>
</tr>
<tr>
<td>BARTScore+CNN</td>
<td>0.448</td>
<td>0.458</td>
<td>0.342</td>
<td>0.356</td>
<td>0.369</td>
<td>0.273</td>
<td>0.382</td>
<td>0.422</td>
<td>0.315</td>
<td>0.356</td>
<td>0.407</td>
<td>0.292</td>
<td>0.385</td>
<td>0.414</td>
<td>0.305</td>
</tr>
<tr>
<td>BARTScore+CNN+Para</td>
<td>0.424</td>
<td>0.442</td>
<td>0.325</td>
<td>0.313</td>
<td>0.364</td>
<td>0.241</td>
<td>0.401</td>
<td>0.487</td>
<td>0.332</td>
<td>0.378</td>
<td>0.448</td>
<td>0.311</td>
<td>0.379</td>
<td>0.435</td>
<td>0.302</td>
</tr>
<tr>
<td>ChatGPT (DA w/o ref)</td>
<td>0.451</td>
<td>0.456</td>
<td>0.383</td>
<td>0.439</td>
<td>0.473</td>
<td>0.379</td>
<td>0.432</td>
<td>0.512</td>
<td>0.399</td>
<td>0.380</td>
<td>0.443</td>
<td>0.351</td>
<td>0.425</td>
<td>0.471</td>
<td>0.378</td>
</tr>
<tr>
<td>ChatGPT (Stars w/o ref)</td>
<td>0.470</td>
<td>0.484</td>
<td>0.403</td>
<td>0.428</td>
<td>0.454</td>
<td>0.374</td>
<td>0.419</td>
<td>0.517</td>
<td>0.389</td>
<td>0.353</td>
<td>0.415</td>
<td>0.329</td>
<td>0.417</td>
<td>0.468</td>
<td>0.374</td>
</tr>
<tr>
<td>ChatGPT (DA w/ ref)</td>
<td>0.420</td>
<td>0.435</td>
<td>0.346</td>
<td>0.448</td>
<td>0.474</td>
<td>0.378</td>
<td>0.424</td>
<td>0.534</td>
<td>0.384</td>
<td>0.316</td>
<td>0.381</td>
<td>0.285</td>
<td>0.402</td>
<td>0.456</td>
<td>0.348</td>
</tr>
<tr>
<td>ChatGPT (Stars w/ ref)</td>
<td>0.474</td>
<td>0.491</td>
<td>0.407</td>
<td>0.430</td>
<td>0.457</td>
<td>0.378</td>
<td>0.403</td>
<td>0.489</td>
<td>0.375</td>
<td>0.339</td>
<td>0.409</td>
<td>0.319</td>
<td>0.411</td>
<td>0.461</td>
<td>0.370</td>
</tr>
</tbody>
</table>
<p>Table 1: Sample-level Spearman correlation (Spear.) correlation, Pearson (Pear.) correlation and Kendall's Tau (Kend.) of different aspects on SummEval (a text summarization meta-evaluation dataset). " Avg. " indicates the average performance. The bold indicates the best correlation.</p>
<h2>4 Experiments</h2>
<h3>4.1 Experimental Setup</h3>
<p>Metrics. To evaluate how well automatic metrics correlate with human judgment. Two widely-used correlation measures are adopted: (1) Spearman correlation (Zar, 2005) assesses the monotonic relationships between two variables; (2) Pearson correlation (Mukaka, 2012) measures the linear relationships between two sets of data; (3) Kendall's Tau (Kendall, 1938) evaluates the ordinal association between two measured quantities.
Evaluation Strategy. When calculating the correlation scores, there are different aggregation methods. Given a set of conditioned text $\left{c_{1}, c_{2}, \ldots, c_{n}\right}$ (e.g., source documents in text summarization task) and $M$ NLG models. The generated text of $m$-th model for the $i$-th condition text is denoted as $g_{i, m}$. (1) Sample-level evaluation strategy calculates the correlation scores as follows:</p>
<p>$$
\begin{aligned}
\operatorname{Corr}<em 1="1">{\text {sample }}=\frac{1}{n} &amp; \sum</em>\right)\right]\right. \
&amp; \left.\left[f_{\text {human }}\left(g_{i, 1}\right), \ldots, f_{\text {human }}\left(g_{i, M}\right)\right]\right)\right)
\end{aligned}
$$}^{n}\left(\rho\left(\left[f_{\text {auto }}\left(g_{i, 1}\right), \ldots, f_{\text {auto }}\left(g_{i, M</p>
<p>where $\rho$ denotes the correlation metrics like Spearman correlation. $f_{\text {auto }}$ and $f_{\text {human }}$ indicate the automatic evaluation and human judgment functions, respectively.
(2) Dataset-level evaluation strategy calculates the correlation as follows:</p>
<p>$$
\begin{aligned}
\operatorname{Corr}<em _auto="{auto" _text="\text">{\text {dataset }} &amp; =\rho\left(\left[f</em>\right)\right]\right. \
&amp; \left.\left[f_{\text {human }}\left(g_{1,1}\right), \ldots, f_{\text {human }}\left(g_{n, M}\right)\right]\right)
\end{aligned}
$$}}\left(g_{1,1}\right), \ldots, f_{\text {auto }}\left(g_{n, M</p>
<h3>4.2 Baselines</h3>
<p>We compare the ChatGPT evaluator with the following widely-used automatic NLG metrics to provide deeper analyses:</p>
<ul>
<li>ROUGE-1, ROUGE-2 and ROUGE-L (Lin, 2004) measure the lexical overlap between the generated text and corresponding references based on unigram, bigram and longest common subsequence, respectively.</li>
<li>BERTScore (Zhang et al., 2020) and MoverScore (Zhao et al., 2019) evaluate the semantic similarity via pre-trained BERT model (Devlin et al., 2019).</li>
<li>PRISM (Thompson and Post, 2020) is used to evaluate NLG models via pre-trained paraphrase models.</li>
<li>BARTScore (Yuan et al., 2021) is a state-of-theart NLG metrics based on vanilla pre-trained BART model (Lewis et al., 2020).</li>
<li>BARTScore+CNN (Yuan et al., 2021) could be regarded as an enhanced version of BARTScore. This metric is based on the BART fine-tuned on the CNN/DM dataset (Hermann et al., 2015).</li>
<li>BARTScore+CNN+Para (Yuan et al., 2021) is another enhanced version of BARTScore. The metric is based on the BART fine-tuned on both CNN/DM and Paraphrase2.0 (Hu et al., 2019).</li>
<li>Perplexity (PPL) is a commonly-used NLG metric to evaluate whether the generation result is grammatical and fluent.</li>
</ul>
<h3>4.3 Text Summarization</h3>
<p>We conduct meta-evaluation on SummEval (Fabbri et al., 2021), NewsRoom (Grusky et al., 2018) and</p>
<table>
<thead>
<tr>
<th>Metrics</th>
<th>Coherence</th>
<th></th>
<th></th>
<th>Relevance</th>
<th></th>
<th></th>
<th>Informativeness</th>
<th></th>
<th></th>
<th>Fluency</th>
<th></th>
<th></th>
<th>Avg.</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Spear.</td>
<td>Pear.</td>
<td>Kend.</td>
<td>Spear.</td>
<td>Pear.</td>
<td>Kend.</td>
<td>Spear.</td>
<td>Pear.</td>
<td>Kend.</td>
<td>Spear.</td>
<td>Pear.</td>
<td>Kend.</td>
<td>Spear.</td>
<td>Pear.</td>
<td>Kend.</td>
</tr>
<tr>
<td>ROUGE-1</td>
<td>0.095</td>
<td>-0.071</td>
<td>0.076</td>
<td>0.147</td>
<td>-0.001</td>
<td>0.112</td>
<td>0.130</td>
<td>-0.039</td>
<td>0.099</td>
<td>0.104</td>
<td>-0.074</td>
<td>0.082</td>
<td>0.119</td>
<td>-0.046</td>
<td>0.092</td>
</tr>
<tr>
<td>ROUGE-2</td>
<td>0.026</td>
<td>-0.108</td>
<td>0.009</td>
<td>0.091</td>
<td>-0.056</td>
<td>0.065</td>
<td>0.079</td>
<td>-0.087</td>
<td>0.052</td>
<td>0.048</td>
<td>-0.101</td>
<td>0.032</td>
<td>0.061</td>
<td>-0.088</td>
<td>0.092</td>
</tr>
<tr>
<td>ROUGE-L</td>
<td>0.064</td>
<td>-0.091</td>
<td>0.051</td>
<td>0.106</td>
<td>-0.034</td>
<td>0.083</td>
<td>0.089</td>
<td>-0.068</td>
<td>0.064</td>
<td>0.072</td>
<td>-0.090</td>
<td>0.061</td>
<td>0.083</td>
<td>-0.071</td>
<td>0.065</td>
</tr>
<tr>
<td>BERTScore</td>
<td>0.147</td>
<td>0.043</td>
<td>0.116</td>
<td>0.162</td>
<td>0.080</td>
<td>0.126</td>
<td>0.130</td>
<td>0.044</td>
<td>0.105</td>
<td>0.171</td>
<td>0.046</td>
<td>0.128</td>
<td>0.152</td>
<td>0.053</td>
<td>0.119</td>
</tr>
<tr>
<td>MoverScore</td>
<td>0.161</td>
<td>0.008</td>
<td>0.127</td>
<td>0.195</td>
<td>0.077</td>
<td>0.157</td>
<td>0.188</td>
<td>0.045</td>
<td>0.151</td>
<td>0.120</td>
<td>-0.008</td>
<td>0.086</td>
<td>0.166</td>
<td>0.030</td>
<td>0.130</td>
</tr>
<tr>
<td>PRISM</td>
<td>0.573</td>
<td>0.605</td>
<td>0.478</td>
<td>0.553</td>
<td>0.636</td>
<td>0.460</td>
<td>0.561</td>
<td>0.629</td>
<td>0.472</td>
<td>0.532</td>
<td>0.547</td>
<td>0.443</td>
<td>0.555</td>
<td>0.604</td>
<td>0.463</td>
</tr>
<tr>
<td>BARTScore</td>
<td>0.679</td>
<td>0.709</td>
<td>0.568</td>
<td>0.604</td>
<td>0.744</td>
<td>0.507</td>
<td>0.646</td>
<td>0.749</td>
<td>0.543</td>
<td>0.670</td>
<td>0.662</td>
<td>0.564</td>
<td>0.650</td>
<td>0.716</td>
<td>0.545</td>
</tr>
<tr>
<td>BARTScore+CNN</td>
<td>0.653</td>
<td>0.690</td>
<td>0.547</td>
<td>0.567</td>
<td>0.718</td>
<td>0.478</td>
<td>0.616</td>
<td>0.712</td>
<td>0.510</td>
<td>0.640</td>
<td>0.653</td>
<td>0.540</td>
<td>0.619</td>
<td>0.693</td>
<td>0.519</td>
</tr>
<tr>
<td>BARTScore+CNN+Para</td>
<td>0.657</td>
<td>0.675</td>
<td>0.544</td>
<td>0.562</td>
<td>0.739</td>
<td>0.465</td>
<td>0.614</td>
<td>0.727</td>
<td>0.507</td>
<td>0.652</td>
<td>0.630</td>
<td>0.545</td>
<td>0.621</td>
<td>0.693</td>
<td>0.515</td>
</tr>
<tr>
<td>ChatGPT (DA w/o ref)</td>
<td>0.469</td>
<td>0.487</td>
<td>0.405</td>
<td>0.461</td>
<td>0.587</td>
<td>0.392</td>
<td>0.578</td>
<td>0.645</td>
<td>0.498</td>
<td>0.507</td>
<td>0.524</td>
<td>0.427</td>
<td>0.504</td>
<td>0.561</td>
<td>0.430</td>
</tr>
<tr>
<td>ChatGPT (Stars w/o ref)</td>
<td>0.428</td>
<td>0.424</td>
<td>0.375</td>
<td>0.402</td>
<td>0.488</td>
<td>0.348</td>
<td>0.557</td>
<td>0.602</td>
<td>0.487</td>
<td>0.451</td>
<td>0.448</td>
<td>0.385</td>
<td>0.460</td>
<td>0.490</td>
<td>0.399</td>
</tr>
<tr>
<td>ChatGPT (DA w/ ref)</td>
<td>0.431</td>
<td>0.494</td>
<td>0.369</td>
<td>0.436</td>
<td>0.535</td>
<td>0.372</td>
<td>0.429</td>
<td>0.484</td>
<td>0.368</td>
<td>0.459</td>
<td>0.490</td>
<td>0.387</td>
<td>0.439</td>
<td>0.501</td>
<td>0.374</td>
</tr>
<tr>
<td>ChatGPT (Stars w/ ref)</td>
<td>0.423</td>
<td>0.424</td>
<td>0.369</td>
<td>0.443</td>
<td>0.506</td>
<td>0.395</td>
<td>0.404</td>
<td>0.463</td>
<td>0.352</td>
<td>0.503</td>
<td>0.504</td>
<td>0.430</td>
<td>0.443</td>
<td>0.474</td>
<td>0.387</td>
</tr>
</tbody>
</table>
<p>Table 2: Sample-level Spearman correlation (Spear.) correlation, Pearson (Pear.) correlation and Kendall's Tau (Kend.) of different aspects on NewsRoom (a text summarization meta-evaluation dataset). " Avg. " indicates the average performance. The bold indicates the best correlation.</p>
<table>
<thead>
<tr>
<th>Metrics</th>
<th>Sample-level</th>
<th></th>
<th></th>
<th>Dataset-level</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Spear.</td>
<td>Pear.</td>
<td>Kend.</td>
<td>Spear.</td>
<td>Pear.</td>
<td>Kend.</td>
</tr>
<tr>
<td>ROUGE-1</td>
<td>0.498</td>
<td>0.526</td>
<td>0.408</td>
<td>0.533</td>
<td>0.555</td>
<td>0.383</td>
</tr>
<tr>
<td>ROUGE-2</td>
<td>0.423</td>
<td>0.449</td>
<td>0.353</td>
<td>0.514</td>
<td>0.513</td>
<td>0.369</td>
</tr>
<tr>
<td>ROUGE-L</td>
<td>0.488</td>
<td>0.515</td>
<td>0.403</td>
<td>0.533</td>
<td>0.549</td>
<td>0.383</td>
</tr>
<tr>
<td>BERTScore</td>
<td>0.441</td>
<td>0.477</td>
<td>0.347</td>
<td>0.503</td>
<td>0.517</td>
<td>0.358</td>
</tr>
<tr>
<td>MoverScore</td>
<td>0.372</td>
<td>0.400</td>
<td>0.290</td>
<td>0.427</td>
<td>0.451</td>
<td>0.303</td>
</tr>
<tr>
<td>PRISM</td>
<td>0.411</td>
<td>0.458</td>
<td>0.324</td>
<td>0.478</td>
<td>0.494</td>
<td>0.339</td>
</tr>
<tr>
<td>BARTScore</td>
<td>0.441</td>
<td>0.467</td>
<td>0.342</td>
<td>0.467</td>
<td>0.458</td>
<td>0.327</td>
</tr>
<tr>
<td>BARTScore+CNN</td>
<td>0.475</td>
<td>0.500</td>
<td>0.374</td>
<td>0.436</td>
<td>0.455</td>
<td>0.306</td>
</tr>
<tr>
<td>BARTScore+CNN+Para</td>
<td>0.471</td>
<td>0.512</td>
<td>0.374</td>
<td>0.499</td>
<td>0.515</td>
<td>0.357</td>
</tr>
<tr>
<td>ChatGPT (DA w/o ref)</td>
<td>0.173</td>
<td>0.179</td>
<td>0.152</td>
<td>0.185</td>
<td>0.193</td>
<td>0.145</td>
</tr>
<tr>
<td>ChatGPT (Stars w/o ref)</td>
<td>0.145</td>
<td>0.162</td>
<td>0.129</td>
<td>0.170</td>
<td>0.179</td>
<td>0.136</td>
</tr>
<tr>
<td>ChatGPT (DA w/ ref)</td>
<td>0.184</td>
<td>0.208</td>
<td>0.154</td>
<td>0.276</td>
<td>0.288</td>
<td>0.206</td>
</tr>
<tr>
<td>ChatGPT (Stars w/ ref)</td>
<td>0.195</td>
<td>0.207</td>
<td>0.174</td>
<td>0.224</td>
<td>0.261</td>
<td>0.181</td>
</tr>
</tbody>
</table>
<p>Table 3: Sample-level and Dataset-level correlation on RealSumm (a text summarization meta-evaluation dataset) (Spear.: Spearman correlation; Pear.: Pearson correlation; Kend.: Kendall's Tau). The bold indicates the best correlation.</p>
<p>RealSumm (Bhandari et al., 2020) to evaluate the performance of ChatGPT as an NLG metric for text summarization. SummEval collects 16 modelgenerated summaries on the CNN/DM dataset and annotates human judgments upon these summaries covering aspects of coherence, relevance, consistency and fluency. Newsroom, as a text summarization dataset, also provides human judgments on 7 model-generated summaries, including coherence, relevance, informativeness and fluency. RealSumm evaluates the pyramid (Nenkova and Passonneau, 2004) recall of 25 model-generated summaries.</p>
<p>The Potentiality of ChatGPT. Table 1 and Table 2 show the sample-level evaluation results on SummEval and NewsRoom, respectively (dataset-level evaluation results on SummEval and NewsRoom also shown in Table 4 and Table 5 with the similar trends). Experimental results show that ChatGPT
achieves a new state-of-the-art correlation in most aspects of SummEval, demonstrating its potential of serving as an NLG metric. For results on Newsroom, ChatGPT also outperforms dominant summarization metrics (i.e., ROUGE and BERTScore) by a large margin. Note that our experiments only estimate the lower bound of ChatGPT's performance, and better performances would like to be achieved once using better prompts or updated versions of ChatGPT.</p>
<p>The Impact of Dataset Biases. As shown in Table 3, we find that the experimental results on RealSumm show different trends from those on SummEval, i.e., ChatGPT significantly underperforms other baseline metrics. For example, ChatGPT (Stars w/ ref) achieves 0.195 sample-level Spearman correlation, which is far behind the counterpart of ROUGE-1 (i.e., 0.498). We conjecture this is because the human judgments in RealSumm are collected via pyramid method (Nenkova and Passonneau, 2004). In detail, this method first requires human evaluators to extract semantic content units from golden references, and then score each system summary based on how many extracted semantic content units are mentioned in the system summary.</p>
<p>In this manner, the more similarity between one generated summary and the corresponding golden reference, the more human evaluation scores will be achieved. Therefore, this reference-oriented annotation method makes the traditional $n$-grambased metric (such as ROUGE) already achieve well correlations with human judgments, which we named as lexical biases. As for SummEval and NewsRoom, human evaluators are required to directly score different summaries without comparing them with the golden references, and thus do</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metrics</th>
<th style="text-align: center;">Coherence</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Relevance</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Consistency</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Fluency</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg.</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-1</td>
<td style="text-align: center;">0.184</td>
<td style="text-align: center;">0.193</td>
<td style="text-align: center;">0.129</td>
<td style="text-align: center;">0.302</td>
<td style="text-align: center;">0.341</td>
<td style="text-align: center;">0.217</td>
<td style="text-align: center;">0.137</td>
<td style="text-align: center;">0.175</td>
<td style="text-align: center;">0.108</td>
<td style="text-align: center;">0.080</td>
<td style="text-align: center;">0.143</td>
<td style="text-align: center;">0.062</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">0.213</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-2</td>
<td style="text-align: center;">0.145</td>
<td style="text-align: center;">0.140</td>
<td style="text-align: center;">0.102</td>
<td style="text-align: center;">0.245</td>
<td style="text-align: center;">0.254</td>
<td style="text-align: center;">0.175</td>
<td style="text-align: center;">0.129</td>
<td style="text-align: center;">0.152</td>
<td style="text-align: center;">0.102</td>
<td style="text-align: center;">0.062</td>
<td style="text-align: center;">0.092</td>
<td style="text-align: center;">0.048</td>
<td style="text-align: center;">0.145</td>
<td style="text-align: center;">0.160</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-L</td>
<td style="text-align: center;">0.141</td>
<td style="text-align: center;">0.148</td>
<td style="text-align: center;">0.100</td>
<td style="text-align: center;">0.284</td>
<td style="text-align: center;">0.318</td>
<td style="text-align: center;">0.204</td>
<td style="text-align: center;">0.109</td>
<td style="text-align: center;">0.152</td>
<td style="text-align: center;">0.086</td>
<td style="text-align: center;">0.079</td>
<td style="text-align: center;">0.132</td>
<td style="text-align: center;">0.061</td>
<td style="text-align: center;">0.153</td>
<td style="text-align: center;">0.188</td>
</tr>
<tr>
<td style="text-align: center;">BERTScore</td>
<td style="text-align: center;">0.317</td>
<td style="text-align: center;">0.326</td>
<td style="text-align: center;">0.224</td>
<td style="text-align: center;">0.362</td>
<td style="text-align: center;">0.381</td>
<td style="text-align: center;">0.262</td>
<td style="text-align: center;">0.117</td>
<td style="text-align: center;">0.146</td>
<td style="text-align: center;">0.092</td>
<td style="text-align: center;">0.150</td>
<td style="text-align: center;">0.196</td>
<td style="text-align: center;">0.117</td>
<td style="text-align: center;">0.237</td>
<td style="text-align: center;">0.262</td>
</tr>
<tr>
<td style="text-align: center;">MoverScore</td>
<td style="text-align: center;">0.178</td>
<td style="text-align: center;">0.177</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.328</td>
<td style="text-align: center;">0.211</td>
<td style="text-align: center;">0.150</td>
<td style="text-align: center;">0.171</td>
<td style="text-align: center;">0.118</td>
<td style="text-align: center;">0.119</td>
<td style="text-align: center;">0.160</td>
<td style="text-align: center;">0.092</td>
<td style="text-align: center;">0.185</td>
<td style="text-align: center;">0.209</td>
</tr>
<tr>
<td style="text-align: center;">PRISM</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.281</td>
<td style="text-align: center;">0.204</td>
<td style="text-align: center;">0.280</td>
<td style="text-align: center;">0.297</td>
<td style="text-align: center;">0.202</td>
<td style="text-align: center;">0.323</td>
<td style="text-align: center;">0.297</td>
<td style="text-align: center;">0.256</td>
<td style="text-align: center;">0.236</td>
<td style="text-align: center;">0.248</td>
<td style="text-align: center;">0.184</td>
<td style="text-align: center;">0.281</td>
<td style="text-align: center;">0.281</td>
</tr>
<tr>
<td style="text-align: center;">BARTScore</td>
<td style="text-align: center;">0.335</td>
<td style="text-align: center;">0.357</td>
<td style="text-align: center;">0.241</td>
<td style="text-align: center;">0.363</td>
<td style="text-align: center;">0.386</td>
<td style="text-align: center;">0.263</td>
<td style="text-align: center;">0.269</td>
<td style="text-align: center;">0.276</td>
<td style="text-align: center;">0.212</td>
<td style="text-align: center;">0.187</td>
<td style="text-align: center;">0.206</td>
<td style="text-align: center;">0.146</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.306</td>
</tr>
<tr>
<td style="text-align: center;">BARTScore+CNN</td>
<td style="text-align: center;">0.408</td>
<td style="text-align: center;">0.434</td>
<td style="text-align: center;">0.292</td>
<td style="text-align: center;">0.394</td>
<td style="text-align: center;">0.423</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.334</td>
<td style="text-align: center;">0.377</td>
<td style="text-align: center;">0.264</td>
<td style="text-align: center;">0.285</td>
<td style="text-align: center;">0.354</td>
<td style="text-align: center;">0.223</td>
<td style="text-align: center;">0.355</td>
<td style="text-align: center;">0.397</td>
</tr>
<tr>
<td style="text-align: center;">BARTScore+CNN+Para</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.430</td>
<td style="text-align: center;">0.304</td>
<td style="text-align: center;">0.398</td>
<td style="text-align: center;">0.431</td>
<td style="text-align: center;">0.289</td>
<td style="text-align: center;">0.379</td>
<td style="text-align: center;">0.452</td>
<td style="text-align: center;">0.301</td>
<td style="text-align: center;">0.346</td>
<td style="text-align: center;">0.410</td>
<td style="text-align: center;">0.271</td>
<td style="text-align: center;">0.387</td>
<td style="text-align: center;">0.431</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT (DA w/o ref)</td>
<td style="text-align: center;">0.394</td>
<td style="text-align: center;">0.399</td>
<td style="text-align: center;">0.310</td>
<td style="text-align: center;">0.455</td>
<td style="text-align: center;">0.435</td>
<td style="text-align: center;">0.365</td>
<td style="text-align: center;">0.339</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">0.300</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.380</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.368</td>
<td style="text-align: center;">0.428</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT (Stars w/o ref)</td>
<td style="text-align: center;">0.435</td>
<td style="text-align: center;">0.438</td>
<td style="text-align: center;">0.353</td>
<td style="text-align: center;">0.448</td>
<td style="text-align: center;">0.459</td>
<td style="text-align: center;">0.366</td>
<td style="text-align: center;">0.356</td>
<td style="text-align: center;">0.515</td>
<td style="text-align: center;">0.320</td>
<td style="text-align: center;">0.300</td>
<td style="text-align: center;">0.385</td>
<td style="text-align: center;">0.268</td>
<td style="text-align: center;">0.385</td>
<td style="text-align: center;">0.449</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT (DA w/ ref)</td>
<td style="text-align: center;">0.418</td>
<td style="text-align: center;">0.426</td>
<td style="text-align: center;">0.327</td>
<td style="text-align: center;">0.494</td>
<td style="text-align: center;">0.506</td>
<td style="text-align: center;">0.389</td>
<td style="text-align: center;">0.363</td>
<td style="text-align: center;">0.507</td>
<td style="text-align: center;">0.315</td>
<td style="text-align: center;">0.237</td>
<td style="text-align: center;">0.329</td>
<td style="text-align: center;">0.203</td>
<td style="text-align: center;">0.378</td>
<td style="text-align: center;">0.442</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT (Stars w/ ref)</td>
<td style="text-align: center;">0.465</td>
<td style="text-align: center;">0.472</td>
<td style="text-align: center;">0.385</td>
<td style="text-align: center;">0.458</td>
<td style="text-align: center;">0.476</td>
<td style="text-align: center;">0.381</td>
<td style="text-align: center;">0.333</td>
<td style="text-align: center;">0.475</td>
<td style="text-align: center;">0.299</td>
<td style="text-align: center;">0.285</td>
<td style="text-align: center;">0.385</td>
<td style="text-align: center;">0.258</td>
<td style="text-align: center;">0.385</td>
<td style="text-align: center;">0.452</td>
</tr>
</tbody>
</table>
<p>Table 4: Dataset-level Spearman correlation (Spear.) correlation, Pearson (Pear.) correlation and Kendall's Tau (Kend.) of different aspects on SummEval (a text summarization meta-evaluation dataset). " Avg. " indicates the average performance. The bold indicates the best correlation.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metrics</th>
<th style="text-align: center;">Coherence</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Relevance</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Informativeness</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Fluency</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg.</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-1</td>
<td style="text-align: center;">0.100</td>
<td style="text-align: center;">0.015</td>
<td style="text-align: center;">0.070</td>
<td style="text-align: center;">0.122</td>
<td style="text-align: center;">0.061</td>
<td style="text-align: center;">0.084</td>
<td style="text-align: center;">0.149</td>
<td style="text-align: center;">0.043</td>
<td style="text-align: center;">0.106</td>
<td style="text-align: center;">0.064</td>
<td style="text-align: center;">-0.009</td>
<td style="text-align: center;">0.043</td>
<td style="text-align: center;">0.109</td>
<td style="text-align: center;">0.028</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-2</td>
<td style="text-align: center;">0.080</td>
<td style="text-align: center;">0.033</td>
<td style="text-align: center;">0.060</td>
<td style="text-align: center;">0.124</td>
<td style="text-align: center;">0.071</td>
<td style="text-align: center;">0.092</td>
<td style="text-align: center;">0.158</td>
<td style="text-align: center;">0.060</td>
<td style="text-align: center;">0.119</td>
<td style="text-align: center;">0.045</td>
<td style="text-align: center;">0.018</td>
<td style="text-align: center;">0.032</td>
<td style="text-align: center;">0.102</td>
<td style="text-align: center;">0.045</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-L</td>
<td style="text-align: center;">0.079</td>
<td style="text-align: center;">-0.010</td>
<td style="text-align: center;">0.055</td>
<td style="text-align: center;">0.101</td>
<td style="text-align: center;">0.031</td>
<td style="text-align: center;">0.069</td>
<td style="text-align: center;">0.136</td>
<td style="text-align: center;">0.018</td>
<td style="text-align: center;">0.098</td>
<td style="text-align: center;">0.045</td>
<td style="text-align: center;">-0.030</td>
<td style="text-align: center;">0.029</td>
<td style="text-align: center;">0.090</td>
<td style="text-align: center;">0.002</td>
</tr>
<tr>
<td style="text-align: center;">BERTScore</td>
<td style="text-align: center;">0.169</td>
<td style="text-align: center;">0.138</td>
<td style="text-align: center;">0.122</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">0.158</td>
<td style="text-align: center;">0.127</td>
<td style="text-align: center;">0.196</td>
<td style="text-align: center;">0.153</td>
<td style="text-align: center;">0.141</td>
<td style="text-align: center;">0.154</td>
<td style="text-align: center;">0.129</td>
<td style="text-align: center;">0.109</td>
<td style="text-align: center;">0.174</td>
<td style="text-align: center;">0.145</td>
</tr>
<tr>
<td style="text-align: center;">MoverScore</td>
<td style="text-align: center;">0.173</td>
<td style="text-align: center;">0.119</td>
<td style="text-align: center;">0.122</td>
<td style="text-align: center;">0.192</td>
<td style="text-align: center;">0.156</td>
<td style="text-align: center;">0.132</td>
<td style="text-align: center;">0.232</td>
<td style="text-align: center;">0.148</td>
<td style="text-align: center;">0.166</td>
<td style="text-align: center;">0.112</td>
<td style="text-align: center;">0.091</td>
<td style="text-align: center;">0.076</td>
<td style="text-align: center;">0.177</td>
<td style="text-align: center;">0.129</td>
</tr>
<tr>
<td style="text-align: center;">PRISM</td>
<td style="text-align: center;">0.483</td>
<td style="text-align: center;">0.485</td>
<td style="text-align: center;">0.350</td>
<td style="text-align: center;">0.540</td>
<td style="text-align: center;">0.550</td>
<td style="text-align: center;">0.398</td>
<td style="text-align: center;">0.567</td>
<td style="text-align: center;">0.569</td>
<td style="text-align: center;">0.414</td>
<td style="text-align: center;">0.420</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">0.303</td>
<td style="text-align: center;">0.503</td>
<td style="text-align: center;">0.506</td>
</tr>
<tr>
<td style="text-align: center;">BARTScore</td>
<td style="text-align: center;">0.656</td>
<td style="text-align: center;">0.666</td>
<td style="text-align: center;">0.495</td>
<td style="text-align: center;">0.588</td>
<td style="text-align: center;">0.700</td>
<td style="text-align: center;">0.439</td>
<td style="text-align: center;">0.645</td>
<td style="text-align: center;">0.710</td>
<td style="text-align: center;">0.485</td>
<td style="text-align: center;">0.615</td>
<td style="text-align: center;">0.610</td>
<td style="text-align: center;">0.464</td>
<td style="text-align: center;">0.626</td>
<td style="text-align: center;">0.671</td>
</tr>
<tr>
<td style="text-align: center;">BARTScore+CNN</td>
<td style="text-align: center;">0.623</td>
<td style="text-align: center;">0.640</td>
<td style="text-align: center;">0.466</td>
<td style="text-align: center;">0.557</td>
<td style="text-align: center;">0.665</td>
<td style="text-align: center;">0.411</td>
<td style="text-align: center;">0.592</td>
<td style="text-align: center;">0.665</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.596</td>
<td style="text-align: center;">0.592</td>
<td style="text-align: center;">0.448</td>
<td style="text-align: center;">0.592</td>
<td style="text-align: center;">0.641</td>
</tr>
<tr>
<td style="text-align: center;">BARTScore+CNN+Para</td>
<td style="text-align: center;">0.621</td>
<td style="text-align: center;">0.639</td>
<td style="text-align: center;">0.465</td>
<td style="text-align: center;">0.575</td>
<td style="text-align: center;">0.692</td>
<td style="text-align: center;">0.427</td>
<td style="text-align: center;">0.615</td>
<td style="text-align: center;">0.694</td>
<td style="text-align: center;">0.459</td>
<td style="text-align: center;">0.593</td>
<td style="text-align: center;">0.577</td>
<td style="text-align: center;">0.444</td>
<td style="text-align: center;">0.601</td>
<td style="text-align: center;">0.650</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT (DA w/o ref)</td>
<td style="text-align: center;">0.383</td>
<td style="text-align: center;">0.418</td>
<td style="text-align: center;">0.297</td>
<td style="text-align: center;">0.491</td>
<td style="text-align: center;">0.541</td>
<td style="text-align: center;">0.392</td>
<td style="text-align: center;">0.527</td>
<td style="text-align: center;">0.576</td>
<td style="text-align: center;">0.413</td>
<td style="text-align: center;">0.401</td>
<td style="text-align: center;">0.398</td>
<td style="text-align: center;">0.309</td>
<td style="text-align: center;">0.451</td>
<td style="text-align: center;">0.483</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT (Stars w/o ref)</td>
<td style="text-align: center;">0.370</td>
<td style="text-align: center;">0.374</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.422</td>
<td style="text-align: center;">0.444</td>
<td style="text-align: center;">0.343</td>
<td style="text-align: center;">0.518</td>
<td style="text-align: center;">0.527</td>
<td style="text-align: center;">0.423</td>
<td style="text-align: center;">0.381</td>
<td style="text-align: center;">0.362</td>
<td style="text-align: center;">0.302</td>
<td style="text-align: center;">0.423</td>
<td style="text-align: center;">0.427</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT (DA w/ ref)</td>
<td style="text-align: center;">0.381</td>
<td style="text-align: center;">0.407</td>
<td style="text-align: center;">0.292</td>
<td style="text-align: center;">0.434</td>
<td style="text-align: center;">0.458</td>
<td style="text-align: center;">0.339</td>
<td style="text-align: center;">0.377</td>
<td style="text-align: center;">0.412</td>
<td style="text-align: center;">0.291</td>
<td style="text-align: center;">0.386</td>
<td style="text-align: center;">0.403</td>
<td style="text-align: center;">0.298</td>
<td style="text-align: center;">0.394</td>
<td style="text-align: center;">0.420</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT (Stars w/ ref)</td>
<td style="text-align: center;">0.370</td>
<td style="text-align: center;">0.355</td>
<td style="text-align: center;">0.295</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">0.426</td>
<td style="text-align: center;">0.342</td>
<td style="text-align: center;">0.373</td>
<td style="text-align: center;">0.400</td>
<td style="text-align: center;">0.301</td>
<td style="text-align: center;">0.439</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">0.353</td>
<td style="text-align: center;">0.402</td>
<td style="text-align: center;">0.402</td>
</tr>
</tbody>
</table>
<p>Table 5: Dataset-level Spearman correlation (Spear.) correlation, Pearson (Pear.) correlation and Kendall's Tau (Kend.) of different aspects on NewsRoom (a text summarization meta-evaluation dataset). " Avg. " indicates the average performance. The bold indicates the best correlation.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metrics</th>
<th style="text-align: center;">Sample-level</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Dataset-level</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-1</td>
<td style="text-align: center;">0.014</td>
<td style="text-align: center;">0.020</td>
<td style="text-align: center;">0.013</td>
<td style="text-align: center;">-0.023</td>
<td style="text-align: center;">-0.010</td>
<td style="text-align: center;">-0.016</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-2</td>
<td style="text-align: center;">0.035</td>
<td style="text-align: center;">0.041</td>
<td style="text-align: center;">0.035</td>
<td style="text-align: center;">0.009</td>
<td style="text-align: center;">0.012</td>
<td style="text-align: center;">0.007</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-L</td>
<td style="text-align: center;">0.013</td>
<td style="text-align: center;">0.021</td>
<td style="text-align: center;">0.014</td>
<td style="text-align: center;">-0.016</td>
<td style="text-align: center;">-0.004</td>
<td style="text-align: center;">-0.011</td>
</tr>
<tr>
<td style="text-align: left;">BERTScore</td>
<td style="text-align: center;">0.140</td>
<td style="text-align: center;">0.120</td>
<td style="text-align: center;">0.105</td>
<td style="text-align: center;">0.081</td>
<td style="text-align: center;">0.084</td>
<td style="text-align: center;">0.056</td>
</tr>
<tr>
<td style="text-align: left;">BARTScore</td>
<td style="text-align: center;">-0.065</td>
<td style="text-align: center;">-0.082</td>
<td style="text-align: center;">-0.061</td>
<td style="text-align: center;">-0.065</td>
<td style="text-align: center;">-0.092</td>
<td style="text-align: center;">-0.045</td>
</tr>
<tr>
<td style="text-align: left;">BARTScore+CNN</td>
<td style="text-align: center;">0.049</td>
<td style="text-align: center;">0.026</td>
<td style="text-align: center;">0.041</td>
<td style="text-align: center;">0.047</td>
<td style="text-align: center;">0.053</td>
<td style="text-align: center;">0.033</td>
</tr>
<tr>
<td style="text-align: left;">BARTScore+CNN+Para</td>
<td style="text-align: center;">0.064</td>
<td style="text-align: center;">0.050</td>
<td style="text-align: center;">0.062</td>
<td style="text-align: center;">0.062</td>
<td style="text-align: center;">0.074</td>
<td style="text-align: center;">0.043</td>
</tr>
<tr>
<td style="text-align: left;">PPL</td>
<td style="text-align: center;">0.324</td>
<td style="text-align: center;">0.330</td>
<td style="text-align: center;">0.265</td>
<td style="text-align: center;">0.306</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">0.213</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT (DA w/o ref)</td>
<td style="text-align: center;">$\mathbf{0 . 5 0 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 3 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 3 9}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 7 1}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 9 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 6 6}$</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT (Stars w/o ref)</td>
<td style="text-align: center;">0.472</td>
<td style="text-align: center;">0.490</td>
<td style="text-align: center;">0.427</td>
<td style="text-align: center;">0.415</td>
<td style="text-align: center;">0.439</td>
<td style="text-align: center;">0.342</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT (DA w/ ref)</td>
<td style="text-align: center;">0.411</td>
<td style="text-align: center;">0.434</td>
<td style="text-align: center;">0.357</td>
<td style="text-align: center;">0.363</td>
<td style="text-align: center;">0.375</td>
<td style="text-align: center;">0.281</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT (Stars w/ ref)</td>
<td style="text-align: center;">0.478</td>
<td style="text-align: center;">0.493</td>
<td style="text-align: center;">0.435</td>
<td style="text-align: center;">0.346</td>
<td style="text-align: center;">0.372</td>
<td style="text-align: center;">0.291</td>
</tr>
</tbody>
</table>
<p>Table 6: Sample-level and Dataset-level correlation on OpenMEVA (a story generation meta-evaluation dataset) (Spear.: Spearman correlation; Pear.: Pearson correlation; Kend.: Kendall's Tau).
not involve such lexical biases.
The Impact of Different Prompt. In this work, we attempt four prompts to guide ChatGPT to evaluate the generation of NLG models. As we can see, the performances of ChatGPT are sensitive to the
prompt design. For different aspects, the prompt should be carefully designed, just like formulating instructions for human evaluators.</p>
<h3>4.4 Story Generation</h3>
<p>Story generation is another NLG task with more emphasis on open-ended generation compared with text summarization, which also means for a given beginning of a story, various generated storylines and different plots could satisfy people. Therefore, story generation models are extremely challenging to evaluate. The automatic similarity-based metrics between the generated storylines and so-called references cannot fully evaluate the quality of the storylines since they do not consider creativity.</p>
<p>To show the effectiveness of ChatGPT as an NLG metric for the story generation task, we conduct experiments on OpenMEVA-ROC (Guan et al., 2021). The OpenMEVA-ROC dataset manually annotates five model-generated storylines under the consideration of their overall quality.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Metrics</th>
<th style="text-align: center;">Informativeness</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Naturalness</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Quality</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Avg.</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
<td style="text-align: center;">Spear.</td>
<td style="text-align: center;">Pear.</td>
<td style="text-align: center;">Kend.</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-1</td>
<td style="text-align: center;">0.092</td>
<td style="text-align: center;">0.093</td>
<td style="text-align: center;">0.073</td>
<td style="text-align: center;">0.265</td>
<td style="text-align: center;">0.274</td>
<td style="text-align: center;">0.206</td>
<td style="text-align: center;">0.235</td>
<td style="text-align: center;">0.234</td>
<td style="text-align: center;">0.184</td>
<td style="text-align: center;">0.197</td>
<td style="text-align: center;">0.200</td>
<td style="text-align: center;">0.154</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-2</td>
<td style="text-align: center;">0.133</td>
<td style="text-align: center;">0.137</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;">0.233</td>
<td style="text-align: center;">0.241</td>
<td style="text-align: center;">0.177</td>
<td style="text-align: center;">0.192</td>
<td style="text-align: center;">0.192</td>
<td style="text-align: center;">0.145</td>
<td style="text-align: center;">0.186</td>
<td style="text-align: center;">0.190</td>
<td style="text-align: center;">0.142</td>
</tr>
<tr>
<td style="text-align: center;">ROUGE-L</td>
<td style="text-align: center;">0.079</td>
<td style="text-align: center;">0.084</td>
<td style="text-align: center;">0.063</td>
<td style="text-align: center;">0.237</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">0.183</td>
<td style="text-align: center;">0.210</td>
<td style="text-align: center;">0.216</td>
<td style="text-align: center;">0.163</td>
<td style="text-align: center;">0.175</td>
<td style="text-align: center;">0.185</td>
<td style="text-align: center;">0.136</td>
</tr>
<tr>
<td style="text-align: center;">BERTScore</td>
<td style="text-align: center;">0.231</td>
<td style="text-align: center;">0.261</td>
<td style="text-align: center;">0.174</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.327</td>
<td style="text-align: center;">0.216</td>
<td style="text-align: center;">0.264</td>
<td style="text-align: center;">0.304</td>
<td style="text-align: center;">0.197</td>
<td style="text-align: center;">0.261</td>
<td style="text-align: center;">0.297</td>
<td style="text-align: center;">0.196</td>
</tr>
<tr>
<td style="text-align: center;">MoverScore</td>
<td style="text-align: center;">0.284</td>
<td style="text-align: center;">0.276</td>
<td style="text-align: center;">0.209</td>
<td style="text-align: center;">0.189</td>
<td style="text-align: center;">0.189</td>
<td style="text-align: center;">0.140</td>
<td style="text-align: center;">0.152</td>
<td style="text-align: center;">0.161</td>
<td style="text-align: center;">0.114</td>
<td style="text-align: center;">0.208</td>
<td style="text-align: center;">0.209</td>
<td style="text-align: center;">0.154</td>
</tr>
<tr>
<td style="text-align: center;">PRISM</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">0.268</td>
<td style="text-align: center;">0.189</td>
<td style="text-align: center;">0.301</td>
<td style="text-align: center;">0.350</td>
<td style="text-align: center;">0.223</td>
<td style="text-align: center;">0.308</td>
<td style="text-align: center;">0.337</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.318</td>
<td style="text-align: center;">0.213</td>
</tr>
<tr>
<td style="text-align: center;">BARTScore</td>
<td style="text-align: center;">0.234</td>
<td style="text-align: center;">0.270</td>
<td style="text-align: center;">0.174</td>
<td style="text-align: center;">0.221</td>
<td style="text-align: center;">0.280</td>
<td style="text-align: center;">0.164</td>
<td style="text-align: center;">0.186</td>
<td style="text-align: center;">0.245</td>
<td style="text-align: center;">0.139</td>
<td style="text-align: center;">0.214</td>
<td style="text-align: center;">0.265</td>
<td style="text-align: center;">0.159</td>
</tr>
<tr>
<td style="text-align: center;">BARTScore+CNN</td>
<td style="text-align: center;">0.237</td>
<td style="text-align: center;">0.253</td>
<td style="text-align: center;">0.177</td>
<td style="text-align: center;">0.312</td>
<td style="text-align: center;">0.382</td>
<td style="text-align: center;">0.233</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.357</td>
<td style="text-align: center;">0.219</td>
<td style="text-align: center;">0.281</td>
<td style="text-align: center;">0.331</td>
<td style="text-align: center;">0.210</td>
</tr>
<tr>
<td style="text-align: center;">BARTScore+CNN+Para</td>
<td style="text-align: center;">0.240</td>
<td style="text-align: center;">0.266</td>
<td style="text-align: center;">0.177</td>
<td style="text-align: center;">0.335</td>
<td style="text-align: center;">0.416</td>
<td style="text-align: center;">0.248</td>
<td style="text-align: center;">0.343</td>
<td style="text-align: center;">0.383</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">0.306</td>
<td style="text-align: center;">0.355</td>
<td style="text-align: center;">0.227</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT (DA w/o ref)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.243</td>
<td style="text-align: center;">0.293</td>
<td style="text-align: center;">0.202</td>
<td style="text-align: center;">0.310</td>
<td style="text-align: center;">0.319</td>
<td style="text-align: center;">0.260</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT (Stars w/o ref)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.316</td>
<td style="text-align: center;">0.389</td>
<td style="text-align: center;">0.269</td>
<td style="text-align: center;">0.307</td>
<td style="text-align: center;">0.367</td>
<td style="text-align: center;">0.266</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT (DA w/ ref)</td>
<td style="text-align: center;">0.247</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">0.198</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.344</td>
<td style="text-align: center;">0.248</td>
<td style="text-align: center;">0.269</td>
<td style="text-align: center;">0.343</td>
<td style="text-align: center;">0.215</td>
<td style="text-align: center;">0.274</td>
<td style="text-align: center;">0.314</td>
<td style="text-align: center;">0.220</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT (Stars w/ ref)</td>
<td style="text-align: center;">0.266</td>
<td style="text-align: center;">0.262</td>
<td style="text-align: center;">0.224</td>
<td style="text-align: center;">0.293</td>
<td style="text-align: center;">0.374</td>
<td style="text-align: center;">0.235</td>
<td style="text-align: center;">0.276</td>
<td style="text-align: center;">0.328</td>
<td style="text-align: center;">0.239</td>
<td style="text-align: center;">0.278</td>
<td style="text-align: center;">0.321</td>
<td style="text-align: center;">0.233</td>
</tr>
</tbody>
</table>
<p>Table 7: Dataset-level Spearman correlation (Spear.) correlation, Pearson (Pear.) correlation and Kendall's Tau (Kend.) of different aspects on BAGEL (a data-to-text generation meta-evaluation dataset). " Avg. " indicates the average performance. The bold indicates the best correlation.</p>
<p>The Potentiality of ChatGPT. As shown in Table 6, ChatGPT achieves the best performance in terms of all correlations, and significantly outperforms the second-best metric (i.e., PPL). For example, ChatGPT (DA w/o ref) achieves 0.507 sample-level Spearman correlation, while PPL only achieves 0.324 sample-level Spearman correlation. In addition, we also find that all similarity-based metrics (i.e., ROUGE-1, ROUGE-2, ROUGE-L, BERTScore and BARTScore) show their weak correlations with the human judgments. This finding indicates that the ChatGPT evaluator has more powerful and reliable judgments on the open-ended and creative text generation tasks, where many diversified generated results could also be regarded as high-quality.
The Impact of Different Prompt. The results in Table 6 also show the sensitivity of the correlation results led by the different prompts. For example, there are large performance gaps between ChatGPT (DA w/o ref) and ChatGPT (DA w/ ref). This finding is also consistent with that in text summarization (Section 4.3). More recently, some researchers also discuss the robustness of LLMs on different (adversarial) prompts (Zhu et al., 2023), and we think this under-explored LLM research direction deserves more research attention.</p>
<h3>4.5 Data-to-Text Generation</h3>
<p>Data-to-text generation aims at generating a fluent free-text description for a given structured table. We conduct experiments on BAGEL (Mairesse et al., 2010) to show the effectiveness of the ChatGPT evaluator on data-to-text generation.</p>
<p>Table 7 shows the experimental results, where ChatGPT achieves competitive correlations compared with the previous state-of-the-art baselines, indicating its strong potentiality serving as a metric for data-to-text generation. It is worth noting that we do not provide reference-free ChatGPT performance in terms of informativeness because informativeness in BAGEL is defined as "whether the system generation contains all the information in the gold reference", which also means that when evaluating informativeness the golden references must be given.</p>
<h2>5 Conclusion</h2>
<p>In this technical report, we explore a research question: "Is ChatGPT a good NLG evaluator?". To this end, we design task-specific as well as aspectspecific prompts to guide ChatGPT to perform as an NLG metric. Experimental results on five widely-used meta-evaluation datasets, covering text summarization, story generation and data-to-text tasks, show the potentiality of ChatGPT as an NLG metric. ChatGPT achieves the new state-of-theart correlations (with human judgments) on SummEval and OpenMEVA meta-evaluation datasets, and obtains competitive results on NewsRoom and BAGEL datasets.</p>
<p>In addition, we also find that the lexical biases involved in the meta-evaluation datasets would influence the effectiveness of NLG metrics, and might lead to the limited performance of the ChatGPT evaluator. Besides, the performances of ChatGPT as an NLG evaluator are sensitive to the format</p>
<p>of the prompt, for different tasks and aspects, the prompt should be carefully designed.</p>
<p>We believe that ChatGPT will exceed its current performance and provide a reliable NLG metric for the research community in the near future.</p>
<h2>Limitations</h2>
<p>While we show that ChatGPT achieves state-of-the-art or competitive correlation with human judgments on various NLG tasks, there are limitations that provide avenues for future work: (1) ChatGPT's performance as an NLG metric is related to prompts, and future work could explore more powerful prompts to achieve better performance; (2) This preliminary report misses experiments on some mainstream NLG tasks, e.g., dialogue generation and report generation; (3) When we did the experiments, the OpenAI ChatGPT did not release the official API. Thus, we conducted the experiments on the ChatGPT website with default temperature, making the results difficult to reproduce. All experiments related to ChatGPT are conducted between February 24 to February 27, 2023; and March 17 to March 22. (4) The experiments are only conducted on the English NLG meta-evaluation datasets, and future work could extend this method into other languages or cross-lingual scenes. (5) The correlation between the ChatGPT evaluator and humans is also related to the quality and challenge of the corresponding meta-evaluation datasets. Our experiments are conducted on the traditional NLG meta-evaluation datasets (that appear before the LLM era). Recently, Zeng et al. (2023) propose LLM-BAR, a challenging meta-evaluation benchmark to test the ability of an LLM evaluator. Future work could adapt our method to other challenging datasets and study the performance of the ChatGPT evaluator.</p>
<h2>Acknowledgement</h2>
<p>We thank anonymous reviewers for their constructive suggestions and comments.</p>
<h2>References</h2>
<p>Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023.</p>
<p>Manik Bhandari, Pranav Narayan Gour, Atabak Ashfaq, Pengfei Liu, and Graham Neubig. 2020. Reevaluating evaluation in text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9347-9359, Online. Association for Computational Linguistics.</p>
<p>Hong Chen, Duc Vo, Hiroya Takamura, Yusuke Miyao, and Hideki Nakayama. 2022. StoryER: Automatic story evaluation via ranking, rating and reasoning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1739-1753, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 85-91, Edinburgh, Scotland. Association for Computational Linguistics.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Bhuwan Dhingra, Manaal Faruqui, Ankur Parikh, MingWei Chang, Dipanjan Das, and William Cohen. 2019. Handling divergent reference texts when evaluating table-to-text generation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4884-4895, Florence, Italy. Association for Computational Linguistics.</p>
<p>Alexander R. Fabbri, Wojciech Kry≈õci≈Ñski, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. SummEval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9:391-409.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire.</p>
<p>Max Grusky, Mor Naaman, and Yoav Artzi. 2018. Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 708-719, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Jian Guan, Zhexin Zhang, Zhuoer Feng, Zitao Liu, Wenbiao Ding, Xiaoxi Mao, Changjie Fan, and Minlie Huang. 2021. OpenMEVA: A benchmark for evaluating open-ended story generation metrics. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International</p>
<p>Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6394-6407, Online. Association for Computational Linguistics.</p>
<p>Karl Moritz Hermann, Tom√°s Kocisk√Ω, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 16931701.
J. Edward Hu, Abhinav Singh, Nils Holzenberger, Matt Post, and Benjamin Van Durme. 2019. Large-scale, diverse, paraphrastic bitexts via sampling and clustering. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 44-54, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Fan Huang, Haewoon Kwak, and Jisun An. 2023. Is chatgpt better than human annotators? potential and limitations of chatgpt in explaining implicit hate speech.</p>
<p>Katharina Jeblick, Balthasar Schachtner, Jakob Dexl, Andreas Mittermeier, Anna Theresa St√ºber, Johanna Topalis, Tobias Weber, Philipp Wesp, Bastian Sabel, Jens Ricke, and Michael Ingrisch. 2022. Chatgpt makes medicine easy to swallow: An exploratory case study on simplified radiology reports.</p>
<p>Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, and Zhaopeng Tu. 2023. Is chatgpt a good translator? a preliminary study. arXiv preprint arXiv:2301.08745.</p>
<p>Maurice G Kendall. 1938. A new measure of rank correlation. Biometrika, 30(1/2):81-93.</p>
<p>Tom Kocmi and Christian Federmann. 2023. Large language models are state-of-the-art evaluators of translation quality. arXiv preprint arXiv:2302.14520.</p>
<p>Jan Koco≈Ñ, Igor Cichecki, Oliwier Kaszyca, Mateusz Kochanek, Dominika Szyd≈Ço, Joanna Baran, Julita Bielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil Kanclerz, Anna Koco≈Ñ, Bart≈Çomiej Koptyra, Wiktoria Mieleszczenko-Kowszewicz, Piotr Mi≈Çkowski, Marcin Oleksy, Maciej Piasecki, ≈Åukasz Radli≈Ñski, Konrad Wojtasik, Stanis≈Çaw Wo≈∫niak, and Przemys≈Çaw Kazienko. 2023. Chatgpt: Jack of all trades, master of none.</p>
<p>Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. 2015. From word embeddings to document distances. In Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 957966, Lille, France. PMLR.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training
for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.</p>
<p>Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 110-119, San Diego, California. Association for Computational Linguistics.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.</p>
<p>Fran√ßois Mairesse, Milica Ga≈°iƒá, Filip Jurƒç√≠ƒçek, Simon Keizer, Blaise Thomson, Kai Yu, and Steve Young. 2010. Phrase-based statistical language generation using graphical models and active learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 15521561, Uppsala, Sweden. Association for Computational Linguistics.</p>
<p>Mavuto M Mukaka. 2012. A guide to appropriate use of correlation coefficient in medical research. Malawi medical journal, 24(3):69-71.</p>
<p>Ani Nenkova and Rebecca Passonneau. 2004. Evaluating content selection in summarization: The pyramid method. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004, pages 145-152, Boston, Massachusetts, USA. Association for Computational Linguistics.</p>
<p>OpenAI. 2022. Introducing chatgpt. https:// openai.com/blog/chatgpt.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL '02, page 311-318, USA. Association for Computational Linguistics.</p>
<p>Krishna Pillutla, Lang Liu, John Thickstun, Sean Welleck, Swabha Swayamdipta, Rowan Zellers, Sewoong Oh, Yejin Choi, and Zaid Harchaoui. 2022. Mauve scores for generative models: Theory and practice.</p>
<p>Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. 2023. Is chatgpt a general-purpose natural language processing task solver? arXiv preprint arXiv:2302.06476.</p>
<p>Haocong Rao, Cyril Leung, and Chunyan Miao. 2023. Can chatgpt assess human personalities? a general evaluation framework.</p>
<p>Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. COMET: A neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685-2702, Online. Association for Computational Linguistics.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881-7892, Online. Association for Computational Linguistics.</p>
<p>Brian Thompson and Matt Post. 2020. Automatic machine translation evaluation in many languages via zero-shot paraphrasing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 90-121, Online. Association for Computational Linguistics.</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Zhixu Li, Jianfeng Qu, and Jie Zhou. 2023a. Cross-lingual summarization via chatgpt. arXiv preprint.</p>
<p>Shuai Wang, Harrisen Scells, Bevan Koopman, and Guido Zuccon. 2023b. Can chatgpt write a good boolean query for systematic review literature search?</p>
<p>Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, Yong Jiang, and Wenjuan Han. 2023. Zero-shot information extraction via chatting with chatgpt.</p>
<p>Xianjun Yang, Yan Li, Xinlu Zhang, Haifeng Chen, and Wei Cheng. 2023. Exploring the limits of chatgpt for query or aspect-based text summarization. arXiv preprint arXiv:2302.08081.</p>
<p>Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text generation.</p>
<p>Jerrold H Zar. 2005. Spearman rank correlation. Encyclopedia of biostatistics, 7.</p>
<p>Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. 2023. Evaluating large language models at evaluating instruction following. arXiv preprint arXiv:2310.07641.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with BERT. In 8th International Conference on Learning Representations, ICLR 2020,</p>
<p>Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. 2019. MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 563-578, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and Dacheng Tao. 2023. Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert.</p>
<p>Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, et al. 2023. Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. arXiv preprint arXiv:2306.04528.</p>
<p>Guido Zuccon and Bevan Koopman. 2023. Dr chatgpt, tell me what i want to hear: How prompt knowledge impacts health answer correctness.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ Equal Contribution. Work was done when Wang and Liang was interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China.
${ }^{5}$ Corresponding author.
${ }^{1}$ We have released the used data at https://github. com/krystalan/chatgpt_as_nlg_evaluator.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
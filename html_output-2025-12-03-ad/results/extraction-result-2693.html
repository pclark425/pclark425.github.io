<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2693 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2693</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2693</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-70.html">extraction-schema-70</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <p><strong>Paper ID:</strong> paper-273502300</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.14890v1.pdf" target="_blank">Reasoning, Memorization, and Fine-Tuning Language Models for Non-Cooperative Games</a></p>
                <p><strong>Paper Abstract:</strong> We develop a method that integrates the tree of thoughts and multi-agent framework to enhance the capability of pre-trained language models in solving complex, unfamiliar games. The method decomposes game-solving into four incremental tasks -- game summarization, area selection, action extraction, and action validation -- each assigned to a specific language-model agent. By constructing a tree of thoughts, the method simulates reasoning paths and allows agents to collaboratively distill game representations and tactics, mitigating the limitations of language models in reasoning and long-term memorization. Additionally, an automated fine-tuning process further optimizes the agents' performance by ranking query-response pairs based on game outcomes, e.g., winning or losing. We apply the method to a non-cooperative game and demonstrate a 65 percent winning rate against benchmark algorithms, with an additional 10 percent improvement after fine-tuning. In contrast to existing deep learning algorithms for game solving that require millions of training samples, the proposed method consumes approximately 1000 training samples, highlighting its efficiency and scalability.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2693.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2693.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-Agent ToT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Agent Tree-of-Thoughts Game-Solving System</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A four-agent system that integrates the Tree-of-Thoughts search with task-decomposed language-model agents (summarization, area selection, action selection, validation) to solve unfamiliar non-cooperative games while limiting memory growth via task-specific, selective state representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reasoning, Memorization, and Fine-Tuning Language Models for Non-Cooperative Games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Four-task language-model agent system (Summarizer, Area Selector, Action Selector, Validator)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A pipeline of four separate pre-trained language-model agents: (1) summarization agent produces a structured text game representation, (2) area-selection agent chooses a bounded sub-area (subset of states), (3) action-selection agent outputs candidate actions inside the selected area, and (4) validation agent checks rule compliance; the agents communicate via Tree-of-Thoughts branching and back-and-forth prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Goedendag (newly defined variant of Stratego, internal benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>A two-player, imperfect-information board game (variant of Stratego) where pieces have ranks, some pieces are hidden, players alternate moves, and the objective is to capture the opponent's flag; experiments used 4x4 and 10x10 board variants with randomly assigned initial piece placements.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>task-specific selective game-state memory (working/episodic style per agent) / selected-area memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Text-based structured game representations passed between agents (summarization output, selected-area representation); memory is organized as per-agent stored representations (selected area = bounded subset of states) and tree-of-thought branches rather than an external key-value or vector DB.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Structured summaries of the current game state (board description, piece locations and known/unknown identities, allowable actions, possible transitions), representations of a selected area/subset of states (center tile, adjacent tiles, visible pieces), and intermediate reasoning nodes (tree-of-thought branches).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Implicitly bounded by the chosen area size (fixed-size selected area) ‚Üí linear growth in memory with game progression; no numeric capacity (e.g., token count) specified.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Direct passing of the relevant text representation between agents along tree-of-thought branches (i.e., agents receive rep(G) and rep(G') directly in prompts); no external retrieval mechanism like a vector DB is described.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Memory (text representations) is updated each turn/state transition: summarization re-generates rep(G) for the new state, area selection produces a new bounded-area representation, and the pipeline repeats; validation can trigger re-selection (area or action), causing iterative updates.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>To reduce the working state space for reasoning and action selection (planning), to retain task-relevant state information across the subcomponents (avoid having each agent memorize entire exponential state-space), and to improve rule compliance by providing structured context to validators.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Overall method (using the task-specific selective memory via summarization + area selection) obtains ~65% win rate in reported experiments against multiple benchmarks; reported pairwise win-rate means: Ours vs Direct query 0.647 ¬± 0.225, Ours vs CoT query 0.611 ¬± 0.201, Ours vs Random 0.662 ¬± 0.136. Error rate with full method: 0.057 (average across 200 games). Fine-tuning of agents produced an additional ‚âà10 percentage-point improvement in win rate.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Benchmarks without the proposed selective memory/pruning: Direct query error rate 0.2107 (cost 48,1? tokens as shown in paper table; note: table shows 'Cost' column per method); CoT query error rate 0.3627. Ablations that remove memory-relevant tasks: 'Ours w/o summary' (skip summarization) decreased win rate by ‚âà5% (win-rate drop reported), 'Ours w/o area' (skip area selection) decreased win rate by ‚âà2% and increased error rate massively to 0.313 (‚âà5x higher than full method). Exact baseline numeric win rates for Direct/CoT/Random are presented in Table 1 of the paper (see fields above).</td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>1) Area selection (selecting a bounded sub-area to reason over) is highly effective at reducing error rate and improving action correctness: removing area selection increased error rate by ~5x (from 0.057 to 0.313). 2) Summarization (distilling a structured rep(G)) increases win rate (removing it decreased win rate by ‚âà5%), showing that a distilled, consistent representation helps decision-making. 3) The multi-agent decomposition reduces per-agent memory growth to linear (each agent only needs task-relevant information), which the authors argue is preferable to exponential state memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>No explicit persistent memory bank was implemented; memory is limited to the selected area and per-turn structured summaries, so long-term memorization across long horizons is not solved. The approach increases token/cost overhead due to multi-agent communications and back-and-forth validation. The paper also notes general LLM limitations for long-term memorization and suggests a memory bank as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>The paper's experiments indicate that the combination of task-specific text representations (summarization) plus bounded area-selection memory is the most effective configuration tested: area selection drastically reduces error and summarization improves win rate; additionally, fine-tuning agents independently (area and action agents fine-tuned separately) outperformed an end-to-end fine-tuned single model, supporting per-task memory/representation specialization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reasoning, Memorization, and Fine-Tuning Language Models for Non-Cooperative Games', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>On the Planning, Search, and Memorization Capabilities of Large Language Models <em>(Rating: 2)</em></li>
                <li>A survey on large language model-based game agents <em>(Rating: 2)</em></li>
                <li>Llm-deliberation: Evaluating LLMs with interactive multi-agent negotiation games <em>(Rating: 1)</em></li>
                <li>Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2693",
    "paper_id": "paper-273502300",
    "extraction_schema_id": "extraction-schema-70",
    "extracted_data": [
        {
            "name_short": "Multi-Agent ToT",
            "name_full": "Multi-Agent Tree-of-Thoughts Game-Solving System",
            "brief_description": "A four-agent system that integrates the Tree-of-Thoughts search with task-decomposed language-model agents (summarization, area selection, action selection, validation) to solve unfamiliar non-cooperative games while limiting memory growth via task-specific, selective state representations.",
            "citation_title": "Reasoning, Memorization, and Fine-Tuning Language Models for Non-Cooperative Games",
            "mention_or_use": "use",
            "agent_name": "Four-task language-model agent system (Summarizer, Area Selector, Action Selector, Validator)",
            "agent_description": "A pipeline of four separate pre-trained language-model agents: (1) summarization agent produces a structured text game representation, (2) area-selection agent chooses a bounded sub-area (subset of states), (3) action-selection agent outputs candidate actions inside the selected area, and (4) validation agent checks rule compliance; the agents communicate via Tree-of-Thoughts branching and back-and-forth prompts.",
            "base_model_size": null,
            "game_benchmark_name": "Goedendag (newly defined variant of Stratego, internal benchmark)",
            "game_description": "A two-player, imperfect-information board game (variant of Stratego) where pieces have ranks, some pieces are hidden, players alternate moves, and the objective is to capture the opponent's flag; experiments used 4x4 and 10x10 board variants with randomly assigned initial piece placements.",
            "uses_memory": true,
            "memory_type": "task-specific selective game-state memory (working/episodic style per agent) / selected-area memory",
            "memory_structure": "Text-based structured game representations passed between agents (summarization output, selected-area representation); memory is organized as per-agent stored representations (selected area = bounded subset of states) and tree-of-thought branches rather than an external key-value or vector DB.",
            "memory_content": "Structured summaries of the current game state (board description, piece locations and known/unknown identities, allowable actions, possible transitions), representations of a selected area/subset of states (center tile, adjacent tiles, visible pieces), and intermediate reasoning nodes (tree-of-thought branches).",
            "memory_capacity": "Implicitly bounded by the chosen area size (fixed-size selected area) ‚Üí linear growth in memory with game progression; no numeric capacity (e.g., token count) specified.",
            "memory_retrieval_strategy": "Direct passing of the relevant text representation between agents along tree-of-thought branches (i.e., agents receive rep(G) and rep(G') directly in prompts); no external retrieval mechanism like a vector DB is described.",
            "memory_update_strategy": "Memory (text representations) is updated each turn/state transition: summarization re-generates rep(G) for the new state, area selection produces a new bounded-area representation, and the pipeline repeats; validation can trigger re-selection (area or action), causing iterative updates.",
            "memory_usage_purpose": "To reduce the working state space for reasoning and action selection (planning), to retain task-relevant state information across the subcomponents (avoid having each agent memorize entire exponential state-space), and to improve rule compliance by providing structured context to validators.",
            "performance_with_memory": "Overall method (using the task-specific selective memory via summarization + area selection) obtains ~65% win rate in reported experiments against multiple benchmarks; reported pairwise win-rate means: Ours vs Direct query 0.647 ¬± 0.225, Ours vs CoT query 0.611 ¬± 0.201, Ours vs Random 0.662 ¬± 0.136. Error rate with full method: 0.057 (average across 200 games). Fine-tuning of agents produced an additional ‚âà10 percentage-point improvement in win rate.",
            "performance_without_memory": "Benchmarks without the proposed selective memory/pruning: Direct query error rate 0.2107 (cost 48,1? tokens as shown in paper table; note: table shows 'Cost' column per method); CoT query error rate 0.3627. Ablations that remove memory-relevant tasks: 'Ours w/o summary' (skip summarization) decreased win rate by ‚âà5% (win-rate drop reported), 'Ours w/o area' (skip area selection) decreased win rate by ‚âà2% and increased error rate massively to 0.313 (‚âà5x higher than full method). Exact baseline numeric win rates for Direct/CoT/Random are presented in Table 1 of the paper (see fields above).",
            "has_memory_ablation": true,
            "memory_effectiveness_findings": "1) Area selection (selecting a bounded sub-area to reason over) is highly effective at reducing error rate and improving action correctness: removing area selection increased error rate by ~5x (from 0.057 to 0.313). 2) Summarization (distilling a structured rep(G)) increases win rate (removing it decreased win rate by ‚âà5%), showing that a distilled, consistent representation helps decision-making. 3) The multi-agent decomposition reduces per-agent memory growth to linear (each agent only needs task-relevant information), which the authors argue is preferable to exponential state memorization.",
            "memory_limitations": "No explicit persistent memory bank was implemented; memory is limited to the selected area and per-turn structured summaries, so long-term memorization across long horizons is not solved. The approach increases token/cost overhead due to multi-agent communications and back-and-forth validation. The paper also notes general LLM limitations for long-term memorization and suggests a memory bank as future work.",
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": "The paper's experiments indicate that the combination of task-specific text representations (summarization) plus bounded area-selection memory is the most effective configuration tested: area selection drastically reduces error and summarization improves win rate; additionally, fine-tuning agents independently (area and action agents fine-tuned separately) outperformed an end-to-end fine-tuned single model, supporting per-task memory/representation specialization.",
            "uuid": "e2693.0",
            "source_info": {
                "paper_title": "Reasoning, Memorization, and Fine-Tuning Language Models for Non-Cooperative Games",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "On the Planning, Search, and Memorization Capabilities of Large Language Models",
            "rating": 2,
            "sanitized_title": "on_the_planning_search_and_memorization_capabilities_of_large_language_models"
        },
        {
            "paper_title": "A survey on large language model-based game agents",
            "rating": 2,
            "sanitized_title": "a_survey_on_large_language_modelbased_game_agents"
        },
        {
            "paper_title": "Llm-deliberation: Evaluating LLMs with interactive multi-agent negotiation games",
            "rating": 1,
            "sanitized_title": "llmdeliberation_evaluating_llms_with_interactive_multiagent_negotiation_games"
        },
        {
            "paper_title": "Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning",
            "rating": 1,
            "sanitized_title": "smurfs_leveraging_multiple_proficiency_agents_with_contextefficiency_for_tool_planning"
        }
    ],
    "cost": 0.00865875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Reasoning, Memorization, and Fine-Tuning Language Models for Non-Cooperative Games
18 Oct 2024</p>
<p>Yunhao Yang yunhaoyang234@utexas.edu 
The University of Texas at Austin Austin
TexasUnited States</p>
<p>Leonard Berthellemy 
The University of Texas at Austin Austin
TexasUnited States</p>
<p>Ufuk Topcu utopcu@utexas.edu 
The University of Texas at Austin Austin
TexasUnited States</p>
<p>Reasoning, Memorization, and Fine-Tuning Language Models for Non-Cooperative Games
18 Oct 20245DA7A24E054A33A34B0703A76BC6B660arXiv:2410.14890v1[cs.AI]Non-Cooperative GameLanguage ModelTree of ThoughtsFine-Tuning
We develop a method that integrates the tree of thoughts and multiagent framework to enhance the capability of pre-trained language models in solving complex, unfamiliar games.The method decomposes game-solving into four incremental tasks-game summarization, area selection, action extraction, and action validation-each assigned to a specific language-model agent.By constructing a tree of thoughts, the method simulates reasoning paths and allows agents to collaboratively distill game representations and tactics, mitigating the limitations of language models in reasoning and longterm memorization.Additionally, an automated fine-tuning process further optimizes the agents' performance by ranking queryresponse pairs based on game outcomes, e.g., winning or losing.We apply the method to a non-cooperative game and demonstrate a 65 percent winning rate against benchmark algorithms, with an additional 10 percent improvement after fine-tuning.In contrast to existing deep learning algorithms for game solving that require millions of training samples, the proposed method consumes approximately 1000 training samples, highlighting its efficiency and scalability.</p>
<p>INTRODUCTION</p>
<p>While existing deep learning algorithms have demonstrated remarkable efficacy in tacking complex games such as chess and Go, these algorithms are computationally expensive.Existing algorithms typically rely on deep neural networks and reinforcement learning techniques to master these board games' intricate strategies and nuanced gameplay and require massive human-labeled data [16,18,22].</p>
<p>The emergence of pre-trained language models obviates the necessity of training deep learning models for game playing, but their constraints on knowledge [12], long-term memorization [2,6,9,19], and reasoning make them inadequate for playing games beyond their knowledge domain [15,24,26].Although several works address constraints [5,10,13,20,25] through constructing abstractions or representations of the language outputs, they are inadequate or exceedingly inefficient in the phase of encapsulating the exponential growth of the size of game states.</p>
<p>We develop a method that integrates tree of thoughts [27] and multi-agent framework [3,21] to distill game representations and tactics from language models.The method decomposes the distillation process into four smaller, incremental tasks, with four language models designated as agents, each responsible for a specific task.</p>
<p>By building a tree of thoughts, the method simulates branching paths of reasoning and the agents follow these paths to exchange information by formulating queries to other agents.In particular, two agents distill current game representation, which includes the knowledge of the game state, winning conditions, and current objectives.They pass the representation to the third agent and the third agent extracts a tactic based on the representation.Finally, the fourth agent evaluates whether the tactic aligns with the current objectives and broadcasts the feedback.</p>
<p>The proposed method addresses the limitations of language models in handling complex, unfamiliar games.By constructing a tree of thoughts, the method eliminates the need for reasoning through the entire game.Furthermore, the multi-agent collaboration for representation distillation enables each language model to retain task-specific information with linear memory growth, as opposed to exponential expansion.</p>
<p>In addition, we develop an automated fine-tuning process to refine the language models based on feedback from game-solving.We deploy two players, both utilizing the proposed method, to play the game and independently collect query-response pairs from the four agents.We assign higher ranks to the query-response pairs from the winning player.Subsequently, we fine-tune each language model agent separately using the ranked pairs specific to its task, allowing for more effective adaptation and improvement of their task-solving capabilities.</p>
<p>We apply the proposed method to a newly defined non-cooperative game and demonstrate a 65 percent winning rate against benchmark algorithms utilizing language models.Moreover, the fine-tuning process results in an additional 10 percent increase in the winning rate over all the benchmarks.In contrast to existing deep learning algorithms for game-solving, which typically require millions of training samples, our approach achieves this 10 percent improvement with approximately 1000 samples, highlighting its efficiency and scalability.</p>
<p>RELATED WORKS</p>
<p>Existing deep learning methods, such as AlphaGo [22], AlphaZero [18] and DeepChess [4] have showcased the ability to surpass human experts through extensive self-play and neural network training.However, these algorithms require extensive training and large computational power.</p>
<p>Existing approaches [1,8,11,23] ask language models to decide the tactics in a game.They have shown the capability of language models in playing games such as Tik-Tac-Toe and chess.However, they rely on the language model containing prior knowledge of the game, hence inadequate to newly designed games beyond the language model's knowledge.Furthermore, the constraints inherent in language models pertaining to long-term memorization and reasoning significantly circumscribe their utility within intricate gaming contexts [2,6,9,19].The inability for (and the impossibility of) memorizing the game history significantly reduces the language model's performance in playing games [15,24,26].</p>
<p>In contrast to existing approaches, the method we propose leverages the tree of thoughts framework and a multi-agent collaboration system to address the limitations of language models in reasoning about unseen games while also reducing memory requirements.This method decomposes the game-solving process into multiple tasks and requires each language model to focus solely on its task, thereby reducing cognitive load and improving overall efficiency in reasoning and decision-making.Additionally, the method allows each model to maintain only task-relevant information, which significantly reduces memory overhead.</p>
<p>PRELIMINARY</p>
<p>Non-Cooperative Game.The extensive form of a non-cooperative game a tuple G = (, , , , , ,  ), where Goedendag.A variant of the game Stratego that is under development.It is a two-player board game where each player commands an army of pieces, each with a specific rank, and the objective is to capture the opponent's flag (a special piece).We present the game rules in Section 4 and more detailed descriptions of this game in the supplementary.
‚Ä¢ ùëÉ = (1, ..., ùëù) is a set of ùëù rational Players. ‚Ä¢ ùëÑ = {ùëû 1 , ..., ùëû ùëò } is a set of Game States. ‚Ä¢ ùêπ = {ùëû ùëì 1 , ..., ùëû ùëì ùëë } ‚äÇ ùëÑ is a set of Termination States. ‚Ä¢ ùêº : ùëÉ √ó ùëÑ ‚Üí ùëÑ is a
It is a non-cooperative game G = (, , , , , ,  ) with the following components:</p>
<p>‚Ä¢  = (1, 2), there are two players in the game.</p>
<p>‚Ä¢  = {1, ...,  }, each state corresponds to a layout of a game board, including the location and rank of each piece.An example of the Goedendag is in Figure 1.Consider a newly designed non-cooperative game, e.g., Goedendag, where the game developer provides a written description of the rules.The written description can be viewed as a user manual to instruct players how to play and win the game.</p>
<p>Written Description of the Rules.A written description R of the rules of a game G = (, , , , , ,  ) is a set of natural language sentences that describe a function  :  √ó  ‚Üí  (), where  () is the power set (set of all subsets) of .Intuitively, R describes the sets of allowable actions for each player at each state.We present an example of R in Section 4.</p>
<p>Language Model for Game Solving.A language model takes a text-based input prompt and outputs a text-based response.By formulating the input prompts, we can query the language model to generate tactics (actions), understand game states, or predict utilities.However, directly querying the language model to solve games requires the language model containing prior knowledge of the game, e.g., well-known games such as chess and tic-tac-toe.</p>
<p>We aim to use language models to solve unseen games whose rules are beyond their knowledge domain.In this work, we choose Goedendag-a variant of an existing game that is not released publicly-as the unseen game and use language models to extract game tactics.At a high level, we propose a method that iteratively sends game-relevant information to language models and queries for tactics/actions until reaching a termination state.</p>
<p>The proposed method formulates task-specific prompts to guide the language models such that the action  returns from the language model satisfies: (1) For a player  ‚àà  at a state  ‚àà ,  ‚àà (, ), i.e., the action complies with the game rule.(2) Maximize the player's utility.</p>
<p>METHODOLOGY</p>
<p>Consider a non-cooperative game G = (, , , , , ,  ) with a written description of the game rules R. We design a method that enhances pre-trained language models' ability to solve complex, unfamiliar games by integrating the tree of thoughts framework</p>
<p>Action Selection</p>
<p>Action rejection Area rejection</p>
<p>Summarization Area Selection</p>
<p>The enemy pieces in locations (0, 2), (0, 3), (1, 2), and (2, 0) as their identities are unknown.Utilize your Rank 4 and Rank 10 pieces.Protect your Flag at (4, 1)......</p>
<p>Action Validation</p>
<p>Take action a</p>
<p>Game </p>
<p>Transition to the next state with a multi-agent system.The method decomposes the gamesolving process into four tasks: game summarization, area selection, action selection, and action validation.We assign each task to a separate pre-trained language model, denoted a language-model agent, allowing for a collaborative and structured approach to game representation and decision-making.</p>
<p>The method employs the tree of thoughts to simulate multiple reasoning paths, enabling agents to extract game representations and tactics (actions).In particular, the summarization and area selection tasks aim to extract and estimate a game representation from the current state.The tree of thoughts passes the game representation to the agent for area selection.Then, the action selection task aims to extract a game tactic for the current player at the current state.The validation task checks whether the extracted tactic complies with the rules, provided by the written descriptions, and signals to other agents if the tactic fails to meet the rules.We demonstrate the method in Figure 2.</p>
<p>In this section, we use the Goedendag as a running example to clarify the method.</p>
<p>Overview.Given the game G = (, , , , , ,  ), the method starts from a player  ‚àà  at a state  ‚àà .The method guides four language-model agents working collaboratively to extract a game tactic, i.e., action,  ‚àà .If the action  complies with the rules:  ‚àà (, ), the game will transit to a new state  ‚Ä≤ =  (, , ).</p>
<p>If the new state  ‚Ä≤ is one of the termination states:  ‚Ä≤ ‚àà  , the method stops the game and computes the utility for the player:  ( ‚Ä≤ )  .Otherwise, the method extracts a new action given the new state and repeats this procedure until reaching a termination state.</p>
<p>Prompt Format.Recall that we assign four language-model agents to four tasks and each language model   takes the rule descriptions and a task-specific prompt as inputs, and returns text-based responses.Using the tree of thoughts to integrate the four agents, they collaboratively return an action  ‚àà  for the current player  ‚àà .We send the same rule description and different task-specific prompts to each language-model agent, in the following format:</p>
<p><Written Descriptions of Rules>.You are player <player_number>.<Task‚àíSpecific Prompt>.</p>
<p>The written descriptions of rules are global information included in the prompts to all the language-model agents, while the taskspecific prompts are different for each agent.</p>
<p>The written description of rules R are text-based game manuals provided by the game designer.In the Goedendag example, a written description of rules includes the description of the categories of pieces, the rules of moving the pieces, and the winning conditions:</p>
<ol>
<li>Players: player 0 and player 1.Each player will play turn by turn.2. Types of pieces: Rank 1, ......, Rank 10, Bomb, and Flag.3. Moving pieces: All pieces are movable except the bombs and the flag.4. Attacking pieces: The piece moving into the tile is the Attacker and the piece that was already occupying that tile is the Defender...... 5. Winning conditions: One of the player's pieces moves to the tile of the opponent's flag.</li>
</ol>
<p>We attached the complete description in the supplementary.A task-specific prompt includes instructions to the languagemodel agent on what information we want to retrieve and the information retrieved from other agents.We will present more details on the task-specific prompt in Section 4.1.</p>
<p>Note: The descriptions of rules are provided by the game designer.We use the Goedendag as an example for demonstration and empirical analysis, but they can be generalized to any noncooperative games with provided descriptions of rules.</p>
<p>Error Handling.As shown in Figure 2, the last task before returning an action is action validation, where we use a language-model agent to check whether the selected action complies with the game rules.If the action fails to comply with the rules, we will loop back and re-query for a new action.We repeat this procedure until the action is approved, i.e., complies with the rules, or reaches the maximum number of repetitions defined by the user.We will present more details in Section 4.1 Action Validation.</p>
<p>Tasks</p>
<p>The method breaks the game-solving procedure into four tasks and assigns four language-model agents to these tasks.The method builds a tree of thoughts that formulates task-specific prompts to guide the agents in completing their tasks.</p>
<p>Summarization.The first task is using a language-model agent  1 to extract a text-based game representation of the current game state.We denote this extraction procedure as summarization.</p>
<p>Recall that every agent takes the rule description R and a taskspecific prompt as inputs.We present the task-specific prompt for  1 below.The game state description is a text-based description of the current game state.In the Goedendag example, a game state description consists of the size of the game board, the positions of both players' pieces, and their categories:</p>
<p>The board is composed of hexagonal tiles, each one having up to 6 neighbors.They are labeled with 2D coordinates.For a tile (i,j), its neighbors are the tiles (i+1,j), (i,j+1), (i‚àí1,j), (i,j‚àí1), (i‚àí1,j +1), (i+1,j‚àí1).Game representation returned from the language‚àímodel agent Your Pieces: ‚àí Rank 3 at (0, 2): A strong piece that can engage in battle with lower ranks.‚àí Bomb at (2, 1): This piece is non‚àímovable but can take down lower ranks if attacked.‚àí Flag at (2, 0): This is a critical piece; protecting it is paramount since losing it means losing the game....... Enemy Pieces: ‚àí (1, 4), (2,3), (2,4): Unknown, posing potential threats, especially adjacent to your key pieces.......This game representation encapsulates the relevant details of the current game state in a structured and coherent format, reducing the difficulty of game interpretation and facilitating the agents' reasoning and decision-making processes in the subsequent tasks.We empirically demonstrate the importance of this summarization task in Section 5.3.</p>
<p>Area Selection.Define area  ‚Ä≤ as a subset of the game states:  ‚Ä≤ ‚äÜ .Recall that the game states transit based on the current state, the current player, and the action:  :  √ó  √ó  ‚Üí .Given the current game state  ‚àà  and the current player  ‚àà , the area selection task aims to produce a subset of states  ‚Ä≤ ‚äÜ  such that
‚àÄ ùëû ‚Ä≤ ‚ààùëÑ ‚Ä≤ ‚àÉ ùëé‚ààùê¥ ùëá (ùëû, ùëù, ùëé) = ùëû ‚Ä≤ .(1)
By fixing the size of the area, i.e., the number of states in  ‚Ä≤ , the method avoids the need to memorize the exponentially grown game states.Instead, the method only memorizes the selected area, which results in a linear growth in the memory.</p>
<p>In the area selection task, the method formulates a task-specific prompt that consists of the text-based game representation rep(G) obtained from  1 and an instruction on selecting a subset of game states.</p>
<p>A language-model agent  2 takes the descriptions of rules R and the task-specific prompt as inputs.It returns a text-based representation rep(G ‚Ä≤ ) that encodes the information (e.g., states, player, actions, possible transitions) of G ‚Ä≤ , where G ‚Ä≤ = (,  ‚Ä≤ , , , , ,  ),  ‚àà  is the current player and  ‚Ä≤ ‚äÜ  is the set of selected states satisfying Equation 1.We show an example of the representation with a selected area in the next blue text box.</p>
<p>In the Goedendag game, game states correspond to the layouts of the game board.Hence, we can choose a subset of states by choosing a sub-area on the board.The task-specific prompt is: <Text‚àíBased Game Representation> Return the coordinates of the center tile of the area you want to select.Return a description of the tile and its adjacent tiles, including the pieces on them and their owners.</p>
<p>The method first requests the language model to select a tile in the game board and considers it as the center tile of the selected area.Only pieces at the center tile or its adjacent tiles can move.And only pieces within a two-tile distance from the center tile are visible.Figure 3 presents a demonstration of a selected area.</p>
<p>Center Tile</p>
<p>Tiles where a move can start from Visible Tiles Figure 3: An example of a selected area.</p>
<p>After selecting the area,  2 returns a text-based representation of the game rep(G) with only the selected area, i.e., the selected subset of states.We present an example below.Action Selection.Given a game representation rep(G ‚Ä≤ ) for the selected area returned from  2 , where G ‚Ä≤ = (,  ‚Ä≤ , , , , ,  ), the method then extracts an action  such that  (, , ) =  ‚Ä≤ and ,  ‚Ä≤ ‚àà  ‚Ä≤ .</p>
<p>In particular, a language-model agent  3 takes the description of rules R and a task-specific prompt as inputs, then outputs an action .The task-specific prompt consists of the representation rep(G ‚Ä≤ ) (e.g., the previous blue text box) and a short instruction "choose an action from the selected area."</p>
<p>In the Goedendag example, the method selects an action that moves a piece located within the red circle in Figure 3.</p>
<p>An action selected by the language‚àímodel agent Move the Rank 10 piece from (1, 2) to (1, 3).</p>
<p>Action Validation.Given the game representation rep(G) from  1 (summarization task), the representation rep(G ‚Ä≤ ) with the selected area from  2 , and the selected action  from  3 , the method uses a language-model agent  4 to determine whether the action complies with the rules described in R.</p>
<p>The task-specific prompt for the action validation task consists of rep(G), rep(G ‚Ä≤ ), , and instruction "Does this action comply with all the rules of the game?"</p>
<p>The agent  4 takes the description of rules and the task-specific prompt as input, then returns a signal indicating whether the action complies with the rules.The agent returns one of the following three signals:</p>
<p>(1) If  4 returns "approval, " the player can take this action and proceed to the next game state.</p>
<p>(2) If  4 returns "area rejection," indicating the selected area does not include any valid action, hence the method passes the rejection signal to  2 and requests a new round of area selection.</p>
<p>(3) The "action rejection" signal indicates valid actions exist in the selected area but the selected action violates the rules.If  4 rejects the action, the method passes the rejection signal to  3 and queries for a different action  ‚Ä≤ .We present several sample outputs from  4 in Goedendag:</p>
<p>Validation results returned from the language‚àímodel agent Approval: The Rank 10 piece can move safely to (1,3) as it is unoccupied, allowing for future tactical positions.</p>
<p>Action rejection: (0, 2) is occupied by my piece.</p>
<p>Area rejection: there are no movable pieces in the selected area.</p>
<p>The method proceeds the game if and only if  4 returns "approval." The approved action  triggers a transition to a new game state  ‚Ä≤ =  (, , ).If the new state is not a termination state, the method goes back to the first task "summarization" with a game state description of the new state.</p>
<p>EXPERIMENT</p>
<p>We apply the proposed method to play the Goedendag game against several benchmark algorithms.Recall that the Goedendag game requires two players.One player uses the proposed method to decide actions and the other uses the benchmark algorithm.We first evaluate the method by the two metrics described in Section 5.1 and demonstrate how the proposed method outperforms the benchmarks.</p>
<p>Then, we perform a set of ablation studies to demonstrate the necessity of the four agents by removing one of the agents and showing performance degradation.</p>
<p>Lastly, we fine-tune the language models and show that the fine-tuned models further improve the proposed method.</p>
<p>Experiment Setting</p>
<p>Evaluation Metric.We use two metrics to evaluate the proposed method and other benchmarks.</p>
<p>(1) Win Rate: Let   and   be the two players,  1 be the number of games  1 wins and  2 be the number of games  2 wins, the win rate   (  ) of player   is
ùëü ùë§ (ùëù ùëñ ) = ùë§ 1 ùë§ 1 + ùë§ 2 .
(
)2
When we compute the win rates, we only count the games where a winning player exists.If a player returns actions that do not comply with the rules, we randomly select an action that meets the rules for the player.</p>
<p>(2) Error Rate: Let √É = √£1 √£2 √£3 ...... be a sequence of actions player   takes in the game, from the initial state to the termination state.Let   be the number of actions √£ ‚àà √É that violate the rules described in R, | | be the number of players, and | √É | be the total number of actions   have taken to complete the game, the error rate of one game E is the number of actions violating the rules over the total number of actions:
E = |ùëÉ | ‚àëÔ∏Å ùëñ=1 ùúÄ ùëñ | √Éùëñ | .(3)
If the proposed method or the benchmark returns actions that violate the rules, we replace these actions with randomly generated actions that comply with the rules and proceed to the next state, avoiding the game being interrupted by the rule violations.</p>
<p>Benchmark.We select three benchmark methods to determine actions for a player.For a game G = (, , , , , ,  ), all the benchmark methods return an action  ‚àà  for the player  ‚àà  based on the current state  ‚àà .</p>
<p>(1) Direct Query: This method directly sends the written descriptions of rules R and the game state description (defined in the summarization task in Section 4.1) into a language model and queries for an action:</p>
<p><Written Descriptions of Rules>.</p>
<p>The current game state is: <Game State Description>.</p>
<p>What action should the current player take?</p>
<p>The language model returns a text-based description to a selected action  ‚àà , in the same format with  3 's outputs in the action selection task.</p>
<p>(2) Chain-of-Thought (CoT) Query: This benchmark method builds an input prompt based on the chain-of-thought technique [14].The prompt format is <Written Descriptions of Rules>.</p>
<p>The current game state is: <Game State Description>.</p>
<p>Proceed by the following instructions: <first instruction> <second instruction> ... Particular to the Goedendag game, we formulate the instructions as the following:</p>
<p>1: Describe the board.2: Describe all the allowable actions that comply with the rules.3: Select the action that maximizes the possibility of winning.4: Describe the selected action.</p>
<p>A language model takes the descriptions and instructions as inputs, then outputs an action  ‚àà  in natural language, e.g., "move the Rank 1 piece from (0,0) to (1,0)."</p>
<p>The CoT query method uses a single zero-shot language model, as opposed to multiple language models in our method, hence it is less costly.We compare it against our method to show the performance-cost trade-off.In contrast to the direct query method, the CoT query enables the language model to reason through the game before determining the action.</p>
<p>(3) Random: This method first extracts all the valid actions at the current game state and then randomly selects an action  from the set of valid actions.If there are  valid actions, then the probability of each action being chosen is 1/.</p>
<p>Benchmark Comparison</p>
<p>We evaluate our proposed method against the three benchmarks over the win and error rates.In the experiments, we use GPT-4omini in our method and benchmarks that require language models.We conduct pair-wise evaluations, where each evaluation consists of 5 sets of games, and each set includes 40 games.We split each  set of 40 games into two game boards, 20 games over a 4x4 board and 20 games over a 10x10 board.We present the board layouts in Figure 4.Note that the initial positions of pieces are randomly assigned based on the written descriptions of rules presented in the supplementary.</p>
<p>For each pair-wise evaluation, we compute the mean and standard deviation of our method's win rate against the benchmark.For example, if our method wins 15, 20, 25, 30, and 35 games in the five sets of games and loses the remaining games, the mean of its win rate is 25/40 and the standard deviation is ‚àö 50.We report our method's win rates against all the benchmarks in Table 1.The results indicate that our method achieves above 60 percent win rates against the benchmarks, significantly higher than the benchmarks' win rates (40 percent).</p>
<p>Then, we evaluate error rates for our method and the benchmarks for every game using Equation 3 and present the average error rates across 200 games (5 sets x 40 games) in Table 2. Additionally, we record the average number of language-model tokens consumed in a game in Table 2, denoted as cost.</p>
<p>The results showcase that our method achieves a significantly lower error rate than the benchmarks requiring language models (Random does not query language model hence there is no error and cost).However, the downside of our proposed method is the cost, due to the use of four language-model agents and back-and-forth communications between the agents.</p>
<p>Ablation Study</p>
<p>We perform a set of ablation studies to indicate the necessity of the tasks in our proposed method.Recall that the method consists of four tasks: summarization, area selection, action selection, and validation.latter two tasks decide actions.While the tasks for deciding actions are mandatory, we remove the summarization and area selection tasks and observe how these two tasks impact the performance.We revise the tree of thoughts in the proposed method and formulate two variances of our method, denoted as ours w/o area and ours w/o summary.</p>
<p>-Ours w/o summary skips the summarization task and directly passes the game state description into the area selection task.The rest of the tasks proceed as normal.</p>
<p>-Ours w/o area skips the area selection task.The method passes the text-based game representation directly to the action selection task and removes the backward communication from action validation to area selection.</p>
<p>We evaluate the two variances of the method against several benchmarks and present the results in Table 1 and 2. From Table 1, we observe that removing the summarization task decreases the win rate by approximately 5 percent while removing the area selection task decreases the win rate by 2 percent.From the perspective of winning the game, the summarization task distills a complete game representation that helps the understanding of the current game state, leading to a significant improvement in the win rate.</p>
<p>On the other hand, Table 2 shows that our method without area selection results in a 5x higher error rate compared to our method with area selection.The area selection task distills a game representation for a subset of states (i.e., a sub-area of a board), significantly lowering the number of actions to choose from.Hence, the language-model agent for the action selection task can easily determine an action that complies with the rules from a much smaller set of actions.Therefore, we have demonstrated the necessity of both tasks in our method.</p>
<p>Fine-Tuning</p>
<p>To further refine our approach, we implement a self-play strategy, where our method competes against itself to gather input-output pairs from the four tasks performed by the winning agent.These collected pairs are subsequently used to fine-tune the languagemodel agents.Due to the limitation of the computing resources, we only collect data and fine-tune the agents for area selection and action selection tasks.In all the fine-tuning procedures, we use the default supervised fine-tuning algorithm provided by OpenAI [17] with early stopping (at convergence) [7] to fine-tune the GPT-40mini we used in our experiments.</p>
<p>We collect 1060 input/output pairs from each of the two tasks (area and action selection) across 100 games from the 4x4 board.Then, we fine-tune three language-model agents: ‚Ä¢ Area agent: a language model fine-tuned by the data collected from the area selection task.‚Ä¢ Action agent: a language model fine-tuned by the data collected from the action selection task.‚Ä¢ End-to-end agent: a language model fine-tuned by all the collected data.</p>
<p>We present the fine-tuning losses for the three agents in Figure 5.</p>
<p>Subsequently, we substitute the original agents in our method with the fine-tuned agents in various configurations to evaluate how the fine-tuning influences the win rates.We assess four replacement strategies:</p>
<p>(1) replacing only the original agent for the area selection task with the fine-tuned area agent, (2) replacing only the agent for the action selection task with the fine-tuned action agent, (3) replacing agents for both tasks with their respective finetuned agents, (4) replacing agents for both tasks with the end-to-end finetuned agent.</p>
<p>Figure 6 shows the win rate of our method, employed with the four strategies, against the benchmarks and our method before refinement.We fine-tune the agents over 4x4 boards and validate them over 10x10 boards.We show an average of 10 percent improvement over the win rates, demonstrating the effectiveness of our fine-tuning procedure.Additionally, fine-tuning each agent independently achieves significantly higher win rates compared with using a single end-to-end model for multiple tasks, showcasing the necessity of our multi-agent approach.</p>
<p>CONCLUSION</p>
<p>We develop a method that integrates the tree of thoughts with a multi-agent framework, effectively enhancing pre-trained language models' ability to solve complex and unfamiliar games.The method alleviates the limitation of language models on long-term memorization and reasoning by breaking down the game-solving process into incremental tasks.The results, including a 65 percent winning Figure 6: Win rate of our method with fine-tuned language-model agents against the benchmarks.We record a checkpoint for each language model at every 100 training epochs.In the figure, "Area," "move," "combined," and "end-to-end" refer to replacement strategies (1), ( 2), (3), and (4), respectively.Our results indicate that independently fine-tuning agents for distinct tasks results in the highest win rate across all benchmarks, demonstrating the effectiveness of the proposed multi-agent approach.</p>
<p>rate with an additional 10 percent improvement post-fine-tuning, demonstrate the method's potential, especially given its efficiency in requiring only a fraction of the training samples compared to traditional deep learning approaches.</p>
<p>For future research, this method can be extended to a wider variety of game types, such as cooperative games.Additionally, constructing a memory bank to retain past game states could further enhance the method's long-term memorization capabilities.Another avenue for improvement lies in refining the fine-tuning process by implementing more advanced ranking mechanisms based on detailed game metrics, which may result in further performance enhancements.</p>
<p>., game board layout, player's turns, winning conditions, etc.</p>
<p>Figure 2 :
2
Figure 2: Demonstration of the method.The left figure shows the tree of thought that connects multiple tasks to solve the game.The right figure shows four tasks, where we assign a language model agent to solve each task.</p>
<p>An area selected by a language‚àímodel agent My * * Rank 10 * * piece at(1,2).(1,3):No piece present.My * * Rank 3 * * piece at (0,2) has no available moves.......</p>
<p>oFigure 4 :
4
Figure 4: Example of a 4x4 board (left) and a 10x10 board (right).</p>
<p>Figure 5 :
5
Figure 5: Cross entropy loss at every epoch during the finetuning procedure.The area and action agents converge to a lower loss compared to the end-to-end agent, indicating a potential better performance.</p>
<p>‚Ä¢  ‚äÇ , each terminate state indicates a player wins or draws the game.‚Ä¢:  √ó  ‚Üí , each player does not know all the ranks of the opponent's pieces.‚Ä¢= { 1 , ...,   }, each action indicates a player's move of a piece.‚Ä¢:  √ó  √ó  ‚Üí .‚Ä¢ :  ‚Üí {‚àí1, 0, 1}  , a player can win, lose, or draw.
q1q2q4P 1P 1P 2P 1P 2P 2q3q5P 2P 1Figure 1: An example of the Goedendag game: The exampleshows two players ùëù1, ùëù2 and five states ùëû1, ..., ùëû5. The gameboard comprises four hexagons; the blue and red hexagonsindicate where ùëù1 and ùëù2's pieces are located. The solid arrowsare actions the player takes, and the dashed arrows are statetransitions. The action with the same color triggers eachtransition. ùëû5 is a terminated state of drawing, neither ofthem wins.</p>
<p>1 returns a text-based game representation rep(G), which encodes the current game state  ‚àà , the current player , a subset of allowable actions  ‚Ä≤ ‚äÜ , and a set of all possible state transitions { (, , ) :  ‚àà  ‚Ä≤ }.Note that the game state descriptions can be varied depending on the game state.In contrast, the text-based game representation is in a formalized, consistent structure across the entire game.We present a sample game representation in the Goedendag game below.
Description of the board:Tile (0, 2): Your Rank 3; Tile (0, 3): Your Rank 1;......Tile (2, 4): Enemy unknown piece; Tile (3, 0): Enemy Rank 2;......Tile (0, 4): Empty;......</p>
<p>Table 1 :
1
Win rates against benchmarks (mean ¬± standard deviation).
BenchmarkWin rateOurs vs. Direct query0.647 ¬± 0.225Ours vs. CoT query0.611 ¬± 0.201Ours vs. Random0.662 ¬± 0.136Ours w/o summary vs. CoT query 0.586 ¬± 0.205Ours w/o summary vs. Random0.618 ¬± 0.217Ours w/o area vs. CoT query0.615 ¬± 0.231Ours w/o area vs. Random0.633 ¬± 0.159</p>
<p>Table 2 :
2
The former two tasks extract game representations and the Comparison between the error rate and cost (the number of language-model tokens consumed in one game).
BenchmarkError rate CostOurs0.057526211Ours w/o area0.31323519Ours w/o summary0.041717881Direct query0.2107481CoT query0.3626976RandomùëÅ /ùê¥ùëÅ /ùê¥</p>
<p>Llm-deliberation: Evaluating LLMs with interactive multi-agent negotiation games. Sahar Abdelnabi, Amr Gomaa, Sarath Sivaprasad, Lea Sch√∂nherr, Mario Fritz, 2023</p>
<p>Emergent and Predictable Memorization in Large Language Models. Stella Biderman, Lintang Usvsn Sai Prashanth, Hailey Sutawika, Quentin G Schoelkopf, Shivanshu Anthony, Edward Purohit, Raf, 2023</p>
<p>Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning. Junzhi Chen, Juhao Liang, Benyou Wang, 2024</p>
<p>Deepchess: End-to-end deep neural network for automatic learning in chess. Nathan S Omid E David, Lior Netanyahu, Wolf, Artificial Neural Networks and Machine Learning-ICANN 2016: 25th International Conference on Artificial Neural Networks. Barcelona, Spain; New York, USASpringer2016. September 6-9, 2016Proceedings, Part II 25</p>
<p>Commonsense Knowledge Mining from Pretrained Models. Joe Davison, Joshua Feldman, Alexander M Rush, 10.18653/V1/D19-1109Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP, Kentaro Inui. Vincent Jiang, Xiaojun Ng, Wan, the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP, Kentaro InuiHong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Adrian De Wynter, Xun Wang, Alex Sokolov, Qilong Gu, Si-Qing Chen, An Evaluation on Large Language Model Outputs: Discourse and Memorization. 2023</p>
<p>Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping. Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, Noah Smith, 2020</p>
<p>Gtbench: Uncovering the strategic reasoning limitations of LLMs via gametheoretic evaluations. Jinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Elias Stengel-Eskin, Mohit Bansal, Tianlong Chen, Kaidi Xu, 2024</p>
<p>Analysis of large-language model versus human performance for genetics questions. D Duong, B D Solomon, 2023</p>
<p>Symbolic exploration in two-player games: Preliminary results. Stefan Edelkamp, The International Conference on AI Planning &amp; Scheduling (AIPS), Workshop on Model Checking. 2002</p>
<p>ChessGPT: Bridging Policy Learning and Language Modeling. Xidong Feng, Yicheng Luo, Ziyan Wang, Hongrui Tang, Mengyue Yang, Kun Shao, David Mguni, Yali Du, Jun Wang, Advances in Neural Information Processing Systems. Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, Sergey Levine, Alice Oh; New Orleans, LA, USANIPS2023</p>
<p>Sihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Tekin, Gaowen Liu, Ramana Kompella, Ling Liu, A survey on large language model-based game agents. 2024</p>
<p>Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, International Conference on Machine Learning (Proceedings of Machine Learning Research. Baltimore, Maryland, USAPMLR2022162</p>
<p>Large Language Models are Zero-Shot Reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems, Sanmi Koyejo. A Mohamed, Danielle Agarwal, K Belgrave, A Cho, Oh, New Orleans, LA, USANIPS2022</p>
<p>Large Language Models on the Chessboard: A Study on ChatGPT's Formal Language Comprehension and Complex Reasoning Skills. Mu-Tien Kuo, Chih-Chung Hsueh, Richard Tzong-Han Tsai, 2023</p>
<p>Deep Reinforcement Learning Hands-On: Apply modern RL methods, with deep Q-networks, value iteration, policy gradients, TRPO, AlphaGo Zero and more. Maxim Lapan, 2018Packt Publishing LtdBirmingham, UK.</p>
<p>. Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, Jie Tang, GPT understands, too. AI Open. 12023. 2023</p>
<p>Acquisition of chess knowledge in alphazero. Thomas Mcgrath, Andrei Kapishnikov, Nenad Toma≈°ev, Adam Pearce, Martin Wattenberg, Demis Hassabis, Been Kim, Ulrich Paquet, Vladimir Kramnik, Proceedings of the National Academy of Sciences. 119e22066251192022. 2022</p>
<p>Near-Duplicate Sequence Search at Scale for Large Language Model Memorization Evaluation. Zhencan Peng, Zhizhi Wang, Dong Deng, Proceedings of the ACM on Management of Data. 12023. 2023</p>
<p>Language Models as Knowledge Bases?. Fabio Petroni, Tim Rockt√§schel, Sebastian Riedel, S H Patrick, Anton Lewis, Yuxiang Bakhtin, Alexander H Wu, Miller, 10.18653/V1/D19-1250Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. Kentaro Ijcnlp, Jing Inui, Vincent Jiang, Xiaojun Ng, Wan, the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Autoact: Automatic agent learning from scratch via self-planning. Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Eleanor Jiang, Chengfei Lv, Huajun Chen, 2024</p>
<p>Mastering the game of Go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, L Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Nature. 5292016. 2016Thore Graepel, and Demis Hassabis</p>
<p>Benchmarking Large Language Model (LLM) Performance for Game Playing via Tic-Tac-Toe. Oguzhan Topsakal, Jackson B Harper, Electronics. 1315322024. 2024</p>
<p>Chess as a Testbed for Language Model State Tracking. Shubham Toshniwal, Sam Wiseman, Karen Livescu, Kevin Gimpel, AAAI Conference on Artificial Intelligence. VirtualAAAI Press2021</p>
<p>Automaton-Based Representations of Task Knowledge from Generative Language Models. Yunhao Yang, Jean-Raphael Gaglione, Cyrus Neary, Ufuk Topcu, 2022</p>
<p>On the Planning, Search, and Memorization Capabilities of Large Language Models. Yunhao Yang, Anshul Tomar, 2023</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 362024. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
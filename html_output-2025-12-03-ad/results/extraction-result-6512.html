<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6512 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6512</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6512</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-129.html">extraction-schema-129</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-268379377</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.08211v3.pdf" target="_blank">Large Language Models are Contrastive Reasoners</a></p>
                <p><strong>Paper Abstract:</strong> Prompting methods play a crucial role in enhancing the capabilities of pre-trained large language models (LLMs). We explore how contrastive prompting (CP) significantly improves the ability of large language models to perform complex reasoning. We demonstrate that LLMs are decent contrastive reasoners by simply adding"Let's give a correct and a wrong answer."before LLMs provide answers. Experiments on various large language models show that zero-shot contrastive prompting improves the performance of standard zero-shot prompting on a range of arithmetic, commonsense, and symbolic reasoning tasks without any hand-crafted few-shot examples, such as increasing the accuracy on GSM8K from 35.9% to 88.8% and AQUA-RAT from 41.3% to 62.2% with the state-of-the-art GPT-4 model. Our method not only surpasses zero-shot CoT and few-shot CoT in most arithmetic and commonsense reasoning tasks but also can seamlessly integrate with existing prompting methods, resulting in improved or comparable results when compared to state-of-the-art methods. Our code is available at https://github.com/yao8839836/cp</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6512.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6512.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot-CP (GPT-4) - GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Contrastive Prompting (GPT-4) on GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Zero-shot Contrastive Prompting (CP) instructs an LLM to produce both a correct and an incorrect answer for each query and then extracts the correct answer; applied with GPT-4 on the GSM8K grade-school math benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Contrastive Prompting (Zero-shot-CP)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential (contrastive paired outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>grade-school multi-step arithmetic word problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>88.8</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>52.9</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>CP elicits paired correct/incorrect answers (a contrastive local ensemble) that increases model confidence in the ground-truth token probabilities; authors report a large absolute improvement (35.9% -> 88.8%) for GPT-4. They also note that although CP yields large gains, Contrastive-CoT (a different contrastive approach based on human-annotated wrong chains) can outperform CP on GSM8K in some settings, suggesting that the nature of the generated incorrect alternatives (model-internal vs. human-crafted perturbations) matters. Ablations on prompt templates and number of wrong answers show sensitivity (1-2 wrong answers often best).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Contrastive Reasoners', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6512.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6512.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot-CP (GPT-4) - AQUA-RAT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Contrastive Prompting (GPT-4) on AQUA-RAT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of Zero-shot Contrastive Prompting with GPT-4 to the AQUA-RAT multiple-choice arithmetic benchmark, using paired correct/incorrect outputs to filter answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Contrastive Prompting (Zero-shot-CP)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential (contrastive paired outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AQUA-RAT</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>multiple-choice multi-step arithmetic problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>62.2</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>20.9</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors report improvement from 41.3% (Zero-shot baseline) to 62.2% using GPT-4 + Zero-shot-CP. They observe that for multiple-choice tasks, producing multiple incorrect answers can help by effectively eliminating distractors; different prompt templates (order of correct vs wrong answers, phrasing) materially affect performance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Contrastive Reasoners', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6512.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6512.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Contrastive Prompting (CP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contrastive Prompting (template-based contrastive reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that instructs LLMs to output both a correct and one or more incorrect answers and then extracts/verifies the correct answer; can be applied zero-shot or combined with CoT and other prompting methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Contrastive Prompting (CP)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential / contrastive output</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Multiple (12 datasets reported)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>arithmetic reasoning, commonsense QA, symbolic tasks, and other logical reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Zero-shot, Zero-shot-CoT, Few-shot-CoT, Contrastive-CoT, Self-consistency, X-of-thought methods</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>CP encourages model self-awareness by asking it to produce and contrast correct and incorrect answers; authors hypothesize pretraining / instruction tuning / RLHF expose models to patterns of correct vs incorrect answers enabling CP. CP tends to outperform plain zero-shot and zero-shot-CoT on arithmetic and commonsense tasks, but can underperform on symbolic tasks where stepwise correctness (CoT) is more important. Authors provide prompt-template and wrong-answer-count ablations showing prompt wording and 1â€“2 wrong answers work best generally; combining CP with Self-consistency (sampling many contrastive outputs) further improves results.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Contrastive Reasoners', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6512.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6512.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Chain-of-Thought (Think step by step)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Zero-shot chain-of-thought prompting that instructs models to 'Think step by step' to elicit intermediate reasoning without few-shot examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Zero-shot-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential (chain-of-thought)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Multiple (used as baseline across 12 datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>arithmetic, commonsense and symbolic reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Zero-shot-CP (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Zero-shot-CoT sometimes generates incorrect intermediate steps, especially on commonsense tasks; CP often outperforms Zero-shot-CoT on arithmetic and commonsense reasoning but Zero-shot-CoT can be better on symbolic / limited-action-space tasks (e.g., Coin Flip, Shuffled Objects) where explicit stepwise decomposition reduces errors.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Contrastive Reasoners', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6512.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6512.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Few-shot CoT provides human-annotated examples with step-by-step reasoning to guide LLMs; often strong but requires labeled rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Few-shot-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential (chain-of-thought with few-shot examples)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Multiple (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>complex reasoning tasks where stepwise rationales help</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Zero-shot-CP (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>CP sometimes matches or exceeds Few-shot-CoT on arithmetic and commonsense benchmarks; combining CP with few-shot CoT (Few-shot-CoT-CP) yields further gains, showing CP is complementary to few-shot rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Contrastive Reasoners', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6512.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6512.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-consistency (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-consistency (sampling multiple reasoning paths and voting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble method that samples multiple chain-of-thought outputs and selects the most consistent answer via majority vote to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble (diverse sampling + voting)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Multiple (used as baseline / combined with CP)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>mathematical and commonsense reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Zero-shot-CP, Few-shot-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors combine CP with Self-consistency (sampling 10 correct+incorrect answers, temperature=0.7) and observe further score improvements; Self-consistency is noted as especially effective for math and commonsense tasks by aggregating diverse reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Contrastive Reasoners', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6512.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6512.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>X-of-thought (ToT/GoT/PAL/PoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>X-of-thought approaches (Tree-of-Thought, Graph-of-Thought, Program-aided Language models, Program-of-Thought)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Families of reasoning approaches that expand beyond linear chains (trees, graphs, code) to produce and explore multiple reasoning trajectories or use programmatic decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>X-of-thought (ToT/GoT/PAL/PoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>tree-search / graph / programmatic (diverse search)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Multiple (mentioned as strong baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>deliberative problem solving with richer search structure</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Few-shot-CoT, Self-consistency, CP</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors note X-of-thought methods provide richer information and flexibility than simple CoT and can improve Few-shot-CoT; ToT reported particularly prominent gains in cited works. CP is presented as simpler but often comparable.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Contrastive Reasoners', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6512.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6512.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Contrastive CoT (Chia et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contrastive Chain-of-Thought prompting (human-annotated wrong reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior method that extends Few-shot-CoT by creating wrong reasoning chains from annotated correct rationales and using them contrastively.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Contrastive chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Contrastive CoT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential (contrastive; few-shot human-annotated errors)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Multiple (compared in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>reasoning tasks where annotated contrastive chains are used</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Zero-shot-CP (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Contrastive-CoT requires human-annotated wrong reasoning and may reorder entities to produce mistakes; authors contrast it with CP, which generates wrong answers autonomously. Empirically, Contrastive-CoT sometimes outperforms CP on GSM8K, while CP outperforms Contrastive-CoT on StrategyQA and SVAMP in the reported comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Contrastive Reasoners', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6512.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6512.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-reflection methods (RCI, Self-Refine, LEAP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-reflection / iterative critique and improvement methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that incorporate critic or review steps (iterative refinement, recursive criticism) to detect and correct LLM errors, often by internal critique or learned principles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-reflection (RCI / Self-Refine / LEAP)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>iterative / self-improvement</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Multiple (mentioned as related baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>improving LLM outputs via self-critique and revision</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Zero-shot-CP</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors report self-reflection methods perform similarly to self-generated exemplars; CP is simpler and yields comparable or better results in many cases. They also caution that self-critique can reinforce errors if the model's internal critique is erroneous.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Contrastive Reasoners', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Contrastive chain-of-thought prompting <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Program-aided language models <em>(Rating: 1)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6512",
    "paper_id": "paper-268379377",
    "extraction_schema_id": "extraction-schema-129",
    "extracted_data": [
        {
            "name_short": "Zero-shot-CP (GPT-4) - GSM8K",
            "name_full": "Zero-shot Contrastive Prompting (GPT-4) on GSM8K",
            "brief_description": "Zero-shot Contrastive Prompting (CP) instructs an LLM to produce both a correct and an incorrect answer for each query and then extracts the correct answer; applied with GPT-4 on the GSM8K grade-school math benchmark.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": "",
            "reasoning_method_name": "Contrastive Prompting (Zero-shot-CP)",
            "reasoning_method_type": "sequential (contrastive paired outputs)",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "GSM8K",
            "task_description": "grade-school multi-step arithmetic word problems",
            "performance_metric": "accuracy (%)",
            "performance_value": 88.8,
            "comparison_target_method": "Zero-shot",
            "performance_difference": 52.9,
            "statistical_significance": null,
            "analysis_notes": "CP elicits paired correct/incorrect answers (a contrastive local ensemble) that increases model confidence in the ground-truth token probabilities; authors report a large absolute improvement (35.9% -&gt; 88.8%) for GPT-4. They also note that although CP yields large gains, Contrastive-CoT (a different contrastive approach based on human-annotated wrong chains) can outperform CP on GSM8K in some settings, suggesting that the nature of the generated incorrect alternatives (model-internal vs. human-crafted perturbations) matters. Ablations on prompt templates and number of wrong answers show sensitivity (1-2 wrong answers often best).",
            "ablation_study_present": true,
            "uuid": "e6512.0",
            "source_info": {
                "paper_title": "Large Language Models are Contrastive Reasoners",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Zero-shot-CP (GPT-4) - AQUA-RAT",
            "name_full": "Zero-shot Contrastive Prompting (GPT-4) on AQUA-RAT",
            "brief_description": "Application of Zero-shot Contrastive Prompting with GPT-4 to the AQUA-RAT multiple-choice arithmetic benchmark, using paired correct/incorrect outputs to filter answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": "",
            "reasoning_method_name": "Contrastive Prompting (Zero-shot-CP)",
            "reasoning_method_type": "sequential (contrastive paired outputs)",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "AQUA-RAT",
            "task_description": "multiple-choice multi-step arithmetic problems",
            "performance_metric": "accuracy (%)",
            "performance_value": 62.2,
            "comparison_target_method": "Zero-shot",
            "performance_difference": 20.9,
            "statistical_significance": null,
            "analysis_notes": "Authors report improvement from 41.3% (Zero-shot baseline) to 62.2% using GPT-4 + Zero-shot-CP. They observe that for multiple-choice tasks, producing multiple incorrect answers can help by effectively eliminating distractors; different prompt templates (order of correct vs wrong answers, phrasing) materially affect performance.",
            "ablation_study_present": true,
            "uuid": "e6512.1",
            "source_info": {
                "paper_title": "Large Language Models are Contrastive Reasoners",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Contrastive Prompting (CP)",
            "name_full": "Contrastive Prompting (template-based contrastive reasoning)",
            "brief_description": "A prompting technique that instructs LLMs to output both a correct and one or more incorrect answers and then extracts/verifies the correct answer; can be applied zero-shot or combined with CoT and other prompting methods.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "",
            "model_size": "",
            "reasoning_method_name": "Contrastive Prompting (CP)",
            "reasoning_method_type": "sequential / contrastive output",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "Multiple (12 datasets reported)",
            "task_description": "arithmetic reasoning, commonsense QA, symbolic tasks, and other logical reasoning tasks",
            "performance_metric": "accuracy (%)",
            "performance_value": null,
            "comparison_target_method": "Zero-shot, Zero-shot-CoT, Few-shot-CoT, Contrastive-CoT, Self-consistency, X-of-thought methods",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "CP encourages model self-awareness by asking it to produce and contrast correct and incorrect answers; authors hypothesize pretraining / instruction tuning / RLHF expose models to patterns of correct vs incorrect answers enabling CP. CP tends to outperform plain zero-shot and zero-shot-CoT on arithmetic and commonsense tasks, but can underperform on symbolic tasks where stepwise correctness (CoT) is more important. Authors provide prompt-template and wrong-answer-count ablations showing prompt wording and 1â€“2 wrong answers work best generally; combining CP with Self-consistency (sampling many contrastive outputs) further improves results.",
            "ablation_study_present": true,
            "uuid": "e6512.2",
            "source_info": {
                "paper_title": "Large Language Models are Contrastive Reasoners",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Zero-shot-CoT",
            "name_full": "Zero-shot Chain-of-Thought (Think step by step)",
            "brief_description": "Zero-shot chain-of-thought prompting that instructs models to 'Think step by step' to elicit intermediate reasoning without few-shot examples.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "",
            "model_size": "",
            "reasoning_method_name": "Zero-shot-CoT",
            "reasoning_method_type": "sequential (chain-of-thought)",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "Multiple (used as baseline across 12 datasets)",
            "task_description": "arithmetic, commonsense and symbolic reasoning",
            "performance_metric": "accuracy (%)",
            "performance_value": null,
            "comparison_target_method": "Zero-shot-CP (this paper)",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Zero-shot-CoT sometimes generates incorrect intermediate steps, especially on commonsense tasks; CP often outperforms Zero-shot-CoT on arithmetic and commonsense reasoning but Zero-shot-CoT can be better on symbolic / limited-action-space tasks (e.g., Coin Flip, Shuffled Objects) where explicit stepwise decomposition reduces errors.",
            "ablation_study_present": false,
            "uuid": "e6512.3",
            "source_info": {
                "paper_title": "Large Language Models are Contrastive Reasoners",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Few-shot-CoT",
            "name_full": "Few-shot Chain-of-Thought prompting",
            "brief_description": "Few-shot CoT provides human-annotated examples with step-by-step reasoning to guide LLMs; often strong but requires labeled rationales.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_size": "",
            "reasoning_method_name": "Few-shot-CoT",
            "reasoning_method_type": "sequential (chain-of-thought with few-shot examples)",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "Multiple (used as baseline)",
            "task_description": "complex reasoning tasks where stepwise rationales help",
            "performance_metric": "accuracy (%)",
            "performance_value": null,
            "comparison_target_method": "Zero-shot-CP (this paper)",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "CP sometimes matches or exceeds Few-shot-CoT on arithmetic and commonsense benchmarks; combining CP with few-shot CoT (Few-shot-CoT-CP) yields further gains, showing CP is complementary to few-shot rationales.",
            "ablation_study_present": false,
            "uuid": "e6512.4",
            "source_info": {
                "paper_title": "Large Language Models are Contrastive Reasoners",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Self-consistency (SC)",
            "name_full": "Self-consistency (sampling multiple reasoning paths and voting)",
            "brief_description": "An ensemble method that samples multiple chain-of-thought outputs and selects the most consistent answer via majority vote to improve robustness.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_size": "",
            "reasoning_method_name": "Self-consistency",
            "reasoning_method_type": "ensemble (diverse sampling + voting)",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "Multiple (used as baseline / combined with CP)",
            "task_description": "mathematical and commonsense reasoning",
            "performance_metric": "accuracy (%)",
            "performance_value": null,
            "comparison_target_method": "Zero-shot-CP, Few-shot-CoT",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Authors combine CP with Self-consistency (sampling 10 correct+incorrect answers, temperature=0.7) and observe further score improvements; Self-consistency is noted as especially effective for math and commonsense tasks by aggregating diverse reasoning paths.",
            "ablation_study_present": false,
            "uuid": "e6512.5",
            "source_info": {
                "paper_title": "Large Language Models are Contrastive Reasoners",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "X-of-thought (ToT/GoT/PAL/PoT)",
            "name_full": "X-of-thought approaches (Tree-of-Thought, Graph-of-Thought, Program-aided Language models, Program-of-Thought)",
            "brief_description": "Families of reasoning approaches that expand beyond linear chains (trees, graphs, code) to produce and explore multiple reasoning trajectories or use programmatic decomposition.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_size": "",
            "reasoning_method_name": "X-of-thought (ToT/GoT/PAL/PoT)",
            "reasoning_method_type": "tree-search / graph / programmatic (diverse search)",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "Multiple (mentioned as strong baselines)",
            "task_description": "deliberative problem solving with richer search structure",
            "performance_metric": "accuracy (%)",
            "performance_value": null,
            "comparison_target_method": "Few-shot-CoT, Self-consistency, CP",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Authors note X-of-thought methods provide richer information and flexibility than simple CoT and can improve Few-shot-CoT; ToT reported particularly prominent gains in cited works. CP is presented as simpler but often comparable.",
            "ablation_study_present": false,
            "uuid": "e6512.6",
            "source_info": {
                "paper_title": "Large Language Models are Contrastive Reasoners",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Contrastive CoT (Chia et al.)",
            "name_full": "Contrastive Chain-of-Thought prompting (human-annotated wrong reasoning)",
            "brief_description": "A prior method that extends Few-shot-CoT by creating wrong reasoning chains from annotated correct rationales and using them contrastively.",
            "citation_title": "Contrastive chain-of-thought prompting",
            "mention_or_use": "mention",
            "model_name": "",
            "model_size": "",
            "reasoning_method_name": "Contrastive CoT",
            "reasoning_method_type": "sequential (contrastive; few-shot human-annotated errors)",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "Multiple (compared in related work)",
            "task_description": "reasoning tasks where annotated contrastive chains are used",
            "performance_metric": "accuracy (%)",
            "performance_value": null,
            "comparison_target_method": "Zero-shot-CP (this paper)",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Contrastive-CoT requires human-annotated wrong reasoning and may reorder entities to produce mistakes; authors contrast it with CP, which generates wrong answers autonomously. Empirically, Contrastive-CoT sometimes outperforms CP on GSM8K, while CP outperforms Contrastive-CoT on StrategyQA and SVAMP in the reported comparisons.",
            "ablation_study_present": false,
            "uuid": "e6512.7",
            "source_info": {
                "paper_title": "Large Language Models are Contrastive Reasoners",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Self-reflection methods (RCI, Self-Refine, LEAP)",
            "name_full": "Self-reflection / iterative critique and improvement methods",
            "brief_description": "Methods that incorporate critic or review steps (iterative refinement, recursive criticism) to detect and correct LLM errors, often by internal critique or learned principles.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_size": "",
            "reasoning_method_name": "Self-reflection (RCI / Self-Refine / LEAP)",
            "reasoning_method_type": "iterative / self-improvement",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "Multiple (mentioned as related baselines)",
            "task_description": "improving LLM outputs via self-critique and revision",
            "performance_metric": "accuracy (%)",
            "performance_value": null,
            "comparison_target_method": "Zero-shot-CP",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Authors report self-reflection methods perform similarly to self-generated exemplars; CP is simpler and yields comparable or better results in many cases. They also caution that self-critique can reinforce errors if the model's internal critique is erroneous.",
            "ablation_study_present": false,
            "uuid": "e6512.8",
            "source_info": {
                "paper_title": "Large Language Models are Contrastive Reasoners",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Contrastive chain-of-thought prompting",
            "rating": 2,
            "sanitized_title": "contrastive_chainofthought_prompting"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Program-aided language models",
            "rating": 1,
            "sanitized_title": "programaided_language_models"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 1,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        }
    ],
    "cost": 0.016527,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models are Contrastive Reasoners
17 Feb 2025</p>
<p>Liang Yao yaoliang3@mail.sysu.edu.cn 
Sun Yat-sen University
ShenzhenChina</p>
<p>Large Language Models are Contrastive Reasoners
17 Feb 2025D41542A053414B7198DB04138D6E7F91arXiv:2403.08211v3[cs.CL]
Prompting methods play a crucial role in enhancing the capabilities of pre-trained large language models (LLMs).We explore how contrastive prompting (CP) significantly improves the ability of large language models to perform complex reasoning.We demonstrate that LLMs are decent contrastive reasoners by simply adding "Let's give a correct and a wrong answer."before LLMs provide answers.Experiments on various large language models show that zero-shot contrastive prompting improves the performance of standard zero-shot prompting on a range of arithmetic, commonsense, and symbolic reasoning tasks without any handcrafted few-shot examples, such as increasing the accuracy on GSM8K from 35.9% to 88.8% and AQUA-RAT from 41.3% to 62.2% with the state-of-the-art GPT-4 model.Our method not only surpasses zero-shot CoT and few-shot CoT in most arithmetic and commonsense reasoning tasks but also can seamlessly integrate with existing prompting methods, resulting in improved or comparable results when compared to state-of-the-art methods.Our code is available at the following GitHub repository: https://github.com/yao8839836/cp.</p>
<p>Introduction</p>
<p>Recent studies (Zhao et al., 2023;Brown et al., 2020;OpenAI, 2023) have shown that large language models (LLMs) exhibit impressive performance across a wide range of tasks.In particular, the chain-of-thought (CoT) prompting technique has demonstrated the capability of LLMs to handle complex tasks, including math problem solving, by guiding them to generate intermediate reasoning steps (Wei et al., 2022b;Kojima et al., 2022;Zhang et al., 2023).These studies spotlight the significance of developing efficient techniques to direct LLMs in their reasoning processes (Liu et al., 2023;Amatriain, 2024;Chia et al., 2023;Yasunaga et al., 2023).</p>
<p>Nevertheless, the current chain-of-thought (CoT) paradigm encounters two main challenges: offering correct guidance or examples of reasoning and reducing the reliance on manual labeling.In particular, Zero-shot- CoT (Kojima et al., 2022) provides general reasoning guidance by providing instructions like "Think step by step.",but the generated reasoning steps may not be correct and adequate for tasks such as commonsense question-answering (Table 1 and 6).On the other hand, Few-shot-CoT (Wei et al., 2022b) offers more detailed guidance but necessitates labeled examples of the reasoning process, which can be expensive to obtain for each task.This raises an important research question: Is it possible to generate a more accurate reasoning process without relying on human labeling?</p>
<p>In this work, we introduce contrastive prompting, a novel prompting approach that automatically directs the reasoning process of large language models.Our inspiration stems from how humans can learn from both their correct and incorrect actions (Roediger and Finn, 2009).For instance, when confronted with a math problem (as in Figure 1), people may ask "How can we prevent mistakes in each step?"By identifying the steps that are prone to mistakes on their own, they can enhance their ability to avoid mistakes and provide accurate solutions.Our idea is to prompt LLMs to emulate this reasoning process, enabling them to effectively solve new problems.</p>
<p>Specifically, when presented with a problem to solve, we instruct LLMs to generate both correct and incorrect answers within the given context.To achieve this, we provide prompts such as "Let's give a correct and a wrong answer."Following this, we verify and confirm the correct answer.Our proposed approach offers multiple advantages.It not only generates incorrect answers autonomously but also places a greater emphasis on ensuring the accuracy of the answers.This eliminates the need for Q: Tim grows 5 trees.Each year he collects 6 lemons from each tree.How many lemons does he get in a decade?</p>
<p>A:The answer (arabic numerals) is (Output) 3000 ï‘ (a) Zero-shot Q: Tim grows 5 trees.Each year he collects 6 lemons from each tree.How many lemons does he get in a decade?A: Let's give a correct and a wrong answer.</p>
<p>(Output) Correct Answer: Tim grows 5 trees and each tree gives him 6 lemons per year.So, in one year, he gets 5 * 6 = 30 lemons.A decade is 10 years, so in a decade, he would get 30 * 10 = 300 lemons.ï’ Incorrect Answer: In one year, Tim gets 5 * 6 = 30 lemons.A decade is 20 years, so in a decade, he would get 30 * 20 = 600 lemons.(This is incorrect because a decade is 10 years, not 20 years.)(b) Zero-shot-CP (Ours)</p>
<p>Figure 1: Example inputs and outputs of GPT-4 with (a) standard Zero-shot, and (b) ours (Zero-shot-CP).In contrast to Few-shot-CoT, which requires step-by-step reasoning examples for each task, our approach does not rely on any examples.Instead, we use the same prompt "Let's give a correct and a wrong answer" for all tasks, including arithmetic, symbolic, commonsense, and other logical reasoning tasks.manually labeling reasoning examples for each task and problem, effectively addressing the challenges faced by CoT.</p>
<p>We evaluate the proposed approach across various reasoning-intensive tasks, including arithmetic reasoning, commonsense reasoning, symbolic reasoning, and other logical reasoning tasks.We employ two state-of-the-art base LLMs GPT-3.5 and GPT-4 (OpenAI, 2023) and four popular open source LLMs.The experimental findings demonstrate significant improvements in scores compared to the zero-shot baseline across all datasets.Moreover, our method not only surpasses Zero-shot-CoT and Few-shot-CoT in most arithmetic and commonsense reasoning tasks but also achieves better results when combined with zero-shot or few-shot CoT, approaching or even surpassing the performance of existing state-of-the-art methods.These results indicate the effectiveness of generating incorrect answers for individual problems to guide the reasoning process of LLMs.</p>
<p>Related Works</p>
<p>Large language models and prompting Recently, LLMs (Zhao et al., 2023) like ChatGPT and GPT-4 (OpenAI, 2023) have gained significant attention.Researchers find that scaling pre-trained language models often leads to an improved model capacity on downstream tasks.These large-sized models show different behaviors from smaller models and display surprising abilities in solving a series of complex tasks.</p>
<p>Prompt engineering is an emerging field ded-icated to the development and optimization of prompts, enabling efficient utilization of LLMs across diverse applications and research domains (Amatriain, 2024;Sahoo et al., 2024).Zeroshot prompting involves querying the LLM without any examples while few-shot prompting provides models with a few input-output examples (Brown et al., 2020).Chain-of-thought (CoT) (Wei et al., 2022b;Kojima et al., 2022) prompting enables complex reasoning capabilities through intermediate reasoning steps.Despite its success, Fewshot-CoT (Wei et al., 2022b) needs human-labeled reasoning steps for each example, while Zero-shot-CoT (Kojima et al., 2022) may generate incorrect reasoning steps (especially for commonsense and arithmetic reasoning).Several X-of-thought approaches (Yao et al., 2023a,b;Gao et al., 2023;Chen et al., 2022) extend CoT on reasoning tasks, where X can be a tree, a graph, or a program.Auto-CoT (Zhang et al., 2023) improves Zero-shot-CoT by providing similar questions as few-shot examples for the target question.Self-consistency (Wang et al., 2023) sample multiple, diverse reasoning paths through Few-shot-CoT, and use the generations to select the most consistent answer.Analogical prompting (Yasunaga et al., 2023)</p>
<p>LLM</p>
<p>Correct Answer: Tim grows 5 trees and each tree gives him 6 lemons per year.So, in one year, he gets 5 * 6 = 30 lemons.A decade is 10 years, so in a decade, he would get 30 * 10 = 300 lemons.</p>
<p>Incorrect Answer: In one year, Tim gets 5 * 6 = 30 lemons.A decade is 20 years, so in a decade, he would get 30 * 20 = 600 lemons.(This is incorrect because a decade is 10 years, not 20 years.)Q: Tim grows 5 trees.Each year he collects 6 lemons from each tree.How many lemons does he get in a decade?A: Let's give a correct and a wrong answer.</p>
<p>Correct Answer: Tim grows 5 trees and each tree gives him 6 lemons per year.So, in one year, he ... Therefore, the correct answer (arabic numerals) is ã€2nd promptã€‘ Answer Extraction LLM 300..</p>
<p>Figure 2:</p>
<p>The complete process of Zero-shot-CP involves two steps: Firstly, we utilize the initial "reasoning" prompt to extract a comprehensive reasoning process from a LLM.Secondly, we employ the subsequent "answer" prompt to extract the correct answer from the reasoning text.</p>
<p>ing, aims to enhance the quality of learned representations by training models to differentiate between "positive" and "negative" samples (Jaiswal et al., 2020).In the LLMs area, reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) and direct preference optimization (DPO) (Rafailov et al., 2023) fine-tune LLMs with relative human judgments of response quality.Selfreflection (Shinn et al., 2023;Kim et al., 2023;Madaan et al., 2023;Zhang et al., 2024) incorporates "critic" or review steps to identify errors made by the LLM itself and improve upon them.However, it is important to note that the initial output of the LLM may not contain any errors, and there is a potential risk of the model reinforcing its own errors if it inaccurately evaluates the quality of its responses or generates invalid principles.The closest work to ours is the Contrastive CoT (Chia et al., 2023) that extends Few-shot-CoT by creating wrong reasoning processes from annotated correct reasoning steps.The main distinction is that the erroneous answers generated by Contrastive CoT still require human-annotated examples, and the random reordering of entities during the reasoning process may not align with the patterns of errors made by LLMs themselves.On the contrary, our approach enables LLMs to generate erroneous answers on their own, which aligns better with their intrinsic knowledge.It does not require human annotation.</p>
<p>Contrastive Prompting</p>
<p>We propose Contrastive Prompting (CP), a template-based prompting approach for contrastive reasoning.Our method can seamlessly integrate with any prompting technique by incorporating a trigger sentence before the LLM provides answers.</p>
<p>In the following, we first illustrate our method using Zero-shot-CP as an example, which only uses the original question without supporting examples.Next, we will discuss how to combine our method with other prompting techniques.</p>
<p>Two-stage prompting</p>
<p>Although Zero-shot-CP is straightforward in concept, it utilizes prompting twice to extract both reasoning and answer, as illustrated in Figure 2.</p>
<p>1st prompt: reasoning extraction In this step we begin by transforming the input question x into a prompt x â€² using a simple template "Q: [X].A: [T]".Here [X] represents the input slot for x and [T] represents a slot for a manually crafted trigger sentence t that would extract the reasoning process to answer the question x.For instance, if we use "Let's give a correct and a wrong answer."as a trigger sentence, the prompt x â€² would be "Q: [X].A: Let's give a correct and a wrong answer.".Additional trigger examples can be found in Table 3. Prompted text x â€² is then inputted into a LLM, which generates the subsequent sentence z.</p>
<p>2nd prompt: answer extraction In the second step, we utilize the generated sentence z in conjunction with the prompted sentence x â€² to extract the ultimate answer from the LLM.To provide a more specific explanation, we combine three elements by concatenating them as "[X'] [Z] [A]".Here, [X'] represents the 1st prompt x â€² , [Z] represents the sentence z generated in the first step, and [A] represents a trigger sentence used to extract the answer.The prompt for this step is self-augmented, meaning that it includes the sentence z generated by the same LLM.During the experiment, we employed slightly different answer triggers based on the format of the answer.Please refer to Appendix A.2 for the answer trigger sentences we used in each task.Subsequently, the prompted text is inputted into the LLM to generate sentences y and extract the final answer.</p>
<p>Integrating with other prompting methods</p>
<p>We can easily integrate our CP with any advanced prompting methods.We name the combined method X-CP, where X can be Zero-shot-CoT, Fewshot-CoT, or any other method.X-CP also has two steps: reasoning extraction and answer extraction.For Zero-shot-CoT-CP, the only distinction is we replace the trigger sentence "Let's give a correct and a wrong answer."with "Let's think step by step and give both a correct answer and a wrong answer.".For Few-shot-CoT-CP, the distinction is that k few-shot examples with reasoning steps are added before "Q: [X].A: Let's give a correct and a wrong answer.",the resulting prompt x â€² will be "
Q: [X 1 ] A: [Z 1 ]. The answer is [Y 1 ]. Q: [X 2 ] A: [Z 2 ]. The answer is [Y 2 ]. ... Q: [X k ] A: [Z k ]. The answer is [Y k ]. Q: [X]. A: Let's give a correct
and a wrong answer.",where X i , Z i and Y i are the question, reasoning steps and the final answer for each example i.</p>
<p>Experiment</p>
<p>Settings</p>
<p>Datasets We evaluate the effectiveness of our proposal on 12 datasets1 encompassing four categories of reasoning tasks: arithmetic (Sin-gleEq, AddSub, MultiArith, AQUA-RAT, GSM8K, SVAMP), commonsense (CommonsenseQA, Strat-egyQA), symbolic (Last Letter Concatenation, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects).The detailed description of each dataset can be found in (Kojima et al., 2022).We use the few-shot examples with reasoning steps provided by (Wei et al., 2022b).</p>
<p>Baselines We conducted a comprehensive comparison of our CP method with various types of prompting techniques.These include simple zero-shot methods such as Zero-shot and Zeroshot- CoT (Kojima et al., 2022), Few-shot and Few-shot-CoT (Wei et al., 2022b), X-of-thought approaches like Tree of Thoughts (ToT) (Yao et al., 2023a), Graph of Thoughts (GoT) (Yao et al., 2023b), Program-aided Language models (PAL) (Gao et al., 2023), and Program of thoughts prompting (PoT) (Chen et al., 2022).Additionally, we compared our method with other prompting techniques such as Analogical prompting (Selfgenerated Exemplars) (Yasunaga et al., 2023) and Self-consistency (SC) (Wang et al., 2023).Furthermore, we evaluated the effectiveness of selfreflection methods, including Recursive Criticism and Improvement (RCI) (Kim et al., 2023), Self-Refine (Madaan et al., 2023) and Learning Principles from Mistakes (LEAP) (Zhang et al., 2024), as well as the closest related work, Contrastive CoT (Chia et al., 2023).We also experimented with running CP using Self-consistency (SC).Specifically, we set the temperature parameter of LLMs to 0.7 and sampled 10 correct and incorrect answers.Then, we selected the answer that appeared most frequently among the 10 correct answers as the final answer.</p>
<p>Models</p>
<p>We use GPT-4 and GPT-3.5-Turbo(0613) as our base models (accessed between Feb 22nd-May 22nd 2024) for main experiments.We also tested our CP on various open LLMs: LLaMA3-8B, LLaMA3-70B (Touvron et al., 2023), ChatGLM3-6B (Du et al., 2022) and Qwen1.5-72B-Chat(Bai et al., 2023).All generations (except experiments with Self-consistency) are done by greedy decoding (i.e., sampling with zero temperature) as in the original CoT work (Wei et al., 2022b).For GPT models, we use Azure OpenAI services.For open LLMs except ChatGLM3-6B, we use LlamaAPI2 .For ChatGLM3-6B, we downloaded the model and performed the inference on a Linux server with an A100 GPU.</p>
<p>Answer filtering We follow Zero-shot-CoT (Kojima et al., 2022) work and use its original implementation to pick up the final answers.</p>
<p>Results</p>
<p>Zero-shot Results Table 1 presents the accuracy scores achieved by our Zero-shot-CP, standard zero-shot prompting (Zero-shot) and Zeroshot-CoT across five datasets.We ran all methods five times using GPT-3.5-Turboand report the average scores.We found that the differences in each run were minimal.Zero-shot-CP consistently outperformed Zero-shot-CoT and Zero-shot across most (4 out of 5) datasets.</p>
<p>Table 5 in Appendix B presents more comprehensive results.Notably, Zero-shot-CP demonstrates significant improvements over Zero-shot on all 12 datasets across various tasks using GPT-3.5-Turbo.For instance, Zero-shot-CP achieves score gains ranging from 14.3% to 73.2% on GSM8K, from 61.2% to 95.2% on MultiArith and from 4.2% to 41.8% on Last Letter Concatenation.Moreover, Zero-shot-CP outperforms Zero-shot on the majority (9 out of 12) of datasets when using GPT-4, with improvements ranging from 35.9% to 88.8% on GSM8K and from 41.3% to 62.2% on AQUA-RAT.These results indicate that eliciting self-awareness in LLMs to compare incorrect and correct answers can help prevent incorrect responses.</p>
<p>Zero-shot-CP outperforms Zero-shot-CoT in the majority (4 out of 6) of arithmetic reasoning tasks, suggesting that the self-awareness of LLMs regarding incorrect answers may be more crucial than their self-awareness regarding steps in mathematical reasoning.Furthermore, in commonsense reasoning tasks, Zero-shot-CP consistently outperforms Zero-shot (2 out of 2), while Zero-shot-CoT exhibits inferior results.This is likely because commonsense reasoning tasks require fewer steps, making awareness of individual pieces of commonsense knowledge more crucial.However, Zero-shot-CP performs worse than Zero-shot-CoT in symbolic reasoning and other reasoning tasks, indicating that for tasks like Coin Flip and Shuffled Objects, the steps involved are more crucial.Moreover, breaking down the reasoning process into individual steps makes it less likely for LLMs to make mistakes in these tasks, as the action space is very limited.Nevertheless, by combining Zero-shot-CP and Zero-shot-CoT, Zero-shot-CoT-CP achieves better results in most tasks (see Table 4).</p>
<p>Table 8 in Appendix B presents the results of using various open-source LLMs: LLaMA3-8B, LLaMA3-70B, ChatGLM3-6B, and Qwen1.5-72B-Chat as base models.The results demonstrate that Zero-shot-CP not only performs well with state-ofthe-art GPT models, but also exhibits significant improvements across multiple sizes of open-source models.</p>
<p>Qualitative Analysis Table 2 and Table 6 in Appendix B illustrates examples from SVAMP and CommonsenseQA.For the example from the arithmetic reasoning task SVAMP, we found that the reasoning process of zero-shot is correct, but it produces an incorrect answer "-4".Zero-shot-CoT is disrupted by irrelevant information, resulting in incorrect reasoning processes and answers being generated.Zero-shot-CP, on the other hand, is not disrupted and provides both the correct answer and explanation.We can see that the "wrong answer" "-4" from Zero-shot-CP is a real mistake made by Zero-shot.For the example from the common sense reasoning task CommonsenseQA, contrastive prompting is able to recognize the word "work" in the question and provide the correct answer, while Zero-shot and Zero-shot-CoT cannot.</p>
<p>In Appendix B, we present responses generated by Zero-shot-CP for each dataset.Figure 5-16 gives both a positive example and a negative example of Zero-shot-CP on each dataset.From positive examples, we found that Zero-shot-CP can generate "wrong" answers that are indeed incorrect in most cases (11/12), except for Tracking Shuffled Object (Figure 13).Incorrect answers are generated by intentionally calculating inaccurately (Figure 12), disregarding important details (Figure 10), searching for descriptions that are not present in the question (Figure 9), or deliberately providing explanations that contradict common sense (Figure 15).From negative examples, We found that the "wrong answers" provided by Zero-shot-CP can actually be valid answers (Figure 6,7,8,12,14 and 15).In some other negative examples, both the "correct answers" and "incorrect answers" provided by Zero-shot-CP are inconsistent with the ground truth (Figure 5,9,10,11 and 16).From the figures, we found that Zero-shot-CP also outputs reasoning steps in the process of generating correct and incorrect answers, especially for arithmetic reasoning tasks.Furthermore, we manually annotated 10 solved problems and 10 unsolved problems of Zero-shot-CP for each of the 12 datasets.solved problems and 120 unsolved problems.We found that for the solved problems, the majority (112/120) of the given "wrong" answers were indeed incorrect.For the unsolved problems, the majority (91/120) of both the "correct" and "wrong" answers were incorrect, with a portion (23/120) of the "wrong" answers actually being the ground truth.This situation typically occurs in yes or no questions.</p>
<p>The impact of prompt selection on Zero-shot-CP We explore different contrastive prompts and their combination with Zero-shot-CoT.Table 3 outlines performance using 9 different templates with two classes.The first category is related to correct and wrong answers.We found "Let's give a correct and a wrong answer."achieves the best results in general."Let's first give a wrong answer, then give the correct answer."performs well on AQUA-RAT but it performs worse on other datasets."Let's first give the correct answer, then give a wrong answer."generally performs well on the four datasets, meaning that providing the correct answer first and then the incorrect answer generally leads to better results.The trigger word "incorrect" performs similarly to "wrong", and the trigger word "Please" performs much worse than "Let's".This is likely because, in the pre-training and fine-tuning data, there are slightly fewer occurrences of "incorrect" compared to "wrong" in samples related to correct and incorrect answers, and "Please" is rarely present as this type of data is generally not dialogue data."Let's give a correct answer."performs well on the multiple-choice question dataset AQUA-RAT, but the performances on other three mathematical reasoning tasks are not satisfactory.This indicates that, for multiple-choice questions, only providing a correct answer is equivalent to eliminating several incorrect answers.However, for questions without options, outputting an incorrect answer is helpful.Table 9 in Appendix B gives more comparative results between "Let's give a correct and a wrong answer."and "Let's give a correct answer."We find that, except for multiple-choice reasoning tasks, providing a wrong answer is more effective than only giving the correct answer.We also printed the token output probabilities for different prompts.As shown in Figure 4 in Appendix B, we find that adding prompts to generate incorrect answers changes the output probability distribution, Zeroshot-CP makes GPT-4 more confident in the ground truth answer.The second type of template in Table 3 connects to Zero-shot-CoT, and we found that starting with the steps performs better than starting with the correct and wrong answers.Overall, it appears that Zero-shot-CoT-CP ("Let's think step by step and give both a correct answer and a wrong answer.")performs the best.</p>
<p>The impact of number of wrong answers on Zero-shot-CP We explored the impact of the number of incorrect answers on accuracy.We vary the number of wrong answers from 0 to 4, where 0 means standard zero-shot prompting.For k = 1, 2, 3, 4, we use the template "Let's give a correct and k â€² wrong answer(s).",where k â€² can be "a", "two", "three" and "four".Figure 3 plots the results.We found that providing 1-2 incorrect answers yielded the best results in general.The only exception is on AQUA-RAT, where providing more incorrect answers resulted in higher accuracy.This is because the task involves a multiple-choice question with five options, and excluding more incorrect answers makes the LLMs more certain about the correct answer.For math reasoning tasks with an infinite number of answers, providing just one incorrect answer seems to be sufficient.4 compares the performances on four mathematical reasoning datasets (MultiArith, GSM8K, AQUA-RAT and SVAMP) and one common sense reasoning dataset (StrategyQA) across CP and baselines.We find that Zero-shot-CP not only outperforms Fewshot, but also achieves comparable or even superior results to Few-shot-CoT.For instance, on GSM8K, the absolute accuracy has improved by 2.1%, and on StrategyQA, the absolute accuracy has improved by 5.1%.This suggests that in certain cases, the provided examples and reasoning steps may not be as effective as directly triggering the LLM's self-awareness of errors.By combining CP and Few-shot-CoT, we can achieve even better results.Furthermore, if we utilize the GPT-4 model, we can attain performance that is comparable to or even superior to the current state-of-the-art methods.For example, in AQUA, SVAMP, and GSM8K, we have achieved higher accuracy scores compared to recently published results.When running CP with Self-consistency (SC), the scores can be further improved in both zero-shot and few-shot settings.</p>
<p>Comparison with other baselines Table</p>
<p>For a more in-depth performance analysis, we note that X-of-thought methods can improve the effectiveness of Few-shot-CoT, indicating that trees, graphs, and code indeed provide richer information and greater flexibility compared to simple chains of thought.Among them, the results reported by the ToT work seem to be more prominent.By sampling multiple reasoning paths and selecting the most consistent answer, Self-consistency (SC) demonstrates excellent performance in mathematical and commonsense reasoning tasks.It can also be effectively combined with other methods such as PoT.Self-generated Exemplars also show better performance than CoT, indicating that allowing the LLM to recall relevant questions and answer them before responding to the original question is helpful.The performance of Self-reflection methods, such as Self-Refine and LEAP, is similar to that of Self-generated Exemplars.RCI performs even better, primarily due to its direct combination with the CoT method.Compared to these methods, our approach is simpler and can also yield comparable results.Compared to the most relevant method Contrastive CoT, our Zero-shot-CP performs better on the StrategyQA and SVAMP datasets.Zero-shot-CoT-CP performs better on AQUA-RAT.However, on GSM8K, Contrastive CoT performs better, indicating that generating incorrect answers by swapping the order of entities is useful for this task.</p>
<p>The main reasons why CP works well are fourfold: 1) the pre-training data of LLMs contains many correct and incorrect answers to different types of questions.For instance, many web pages and books in Appendix A.3 provide correct and incorrect answers to math reasoning and common sense reasoning questions.Answers to questions on social media platforms like Reddit, Quora, and Zhihu can be voted on by others through "upvotes" or "downvotes".Highly upvoted answers are more likely to be correct answers while others may be incorrect.Pre-training LLMs with massive text containing these correct and wrong answers can encode general patterns (token probability) of these answers in LLM parameters.When prompted with contrastive prompts, LLMs can leverage these patterns to generate both a correct and a wrong answer.The "correct" answer is more likely to align with ground truth, as the model has learned to eliminate possible wrong answers.2) Instruction tuning unlocks the abilities of LLMs to give correct and incorrect answers by fine-tuning on various natural language processing tasks including reasoning tasks (Wei et al., 2022a).3) RLHF fine-tunes LLMs using human feedback data, which offers relative judgments on the quality of answers.This feedback is valuable for enhancing the LLMs' capability to distinguish between correct and incorrect answers.4) In CP, correct and wrong answers are returned by the LLM in a single output.The correct answers are generally distinct from the incorrect ones (as shown in Figure 5-16), thereby excluding the (mostly indeed) incorrect answers and reducing the probability of the correct answers being wrong.Before outputting the two answers, the LLM engages in "contrastive thinking" to determine which answer is correct and which is incorrect.</p>
<p>Conclusion</p>
<p>We propose CP, a template-based prompting approach for contrastive reasoning.Quantitative and qualitative results indicate that Zero-Shot-CP shows significant improvements across various reasoning tasks.Our method can seamlessly integrate with any prompting technique by incorporating a trigger sentence before the LLM provides answers.</p>
<p>CP not only outperforms Zero-shot-CoT and Fewshot-CoT in most arithmetic and commonsense reasoning tasks, but also achieves comparable or even superior results when compared to state-of-the-art methods.</p>
<p>Limitations</p>
<p>Our work has some limitations and there is room for further exploration and improvement.Firstly, we have not yet validated the effectiveness of CP on smaller models such as Gemma-2B and Qwen1.5-0.5B.Secondly, we can further explore the combination of contrastive prompting with other prompting methods, such as X-of-thought approaches.Lastly, exploring the impact of contrastive prompting on LLM parameters and visualizing it would be an interesting future direction.</p>
<p>A Details of Experimental Setup</p>
<p>A.1 Code, Prompts, Logs All code is available at https://github.com/yao8839836/cp.</p>
<p>All prompts are available at https://github.com/yao8839836/cp/blob/master/main.py.</p>
<p>Our experimental logs are available at https://github.com/yao8839836/cp/tree/master/log.</p>
<p>A.2 Prompts For Answer Extraction</p>
<p>Table 7 summarizes the answer extraction prompt for each task used for the CP experiments.</p>
<p>A.3 Pre-training data examples</p>
<p>For instance, many web pages and books provide correct and incorrect answers to math reasoning3 4 5 and common sense reasoning6 7 questions.</p>
<p>B Additional Experimental Results</p>
<p>In this section, we provide a summary of additional example texts generated by Zero-shot-CP.GPT-3.5-Turbo is used as the model if not specified.Table 6 illustrates example outputs of zero-shot prompting methods from CommonsenseQA. Figure 5-16 show a positive example and a negative example of Zero-shot-CP on each dataset."GT" in the figures means "Ground Truth".</p>
<p>The 240 examples, along with our annotations, can be accessed at the following link: https://anonymous.4open.science/r/cp-712E/results/zero_shot_cp_gpt4_240_ examples_labeled.txt.</p>
<p>Table 5 presents the comparison of Zero-shot-CP with Zero-shot and Zero-shot-CoT on all 12 datasets using GPT-3.5-Turboand GPT-4.Table 6 presents an example question from Com-monsenseQA and responses from different methods.</p>
<p>Table 8 presents the results of using various open-source LLMs: LLaMA3-8B, LLaMA3-70B, ChatGLM3-6B, and Qwen1.5-72B-Chat as base models.</p>
<p>Table 9 presents the comparison of the results using "Let's give a correct and a wrong answer."and "Let's give a correct answer."prompts.</p>
<p>Table 10 provides the categorization and counts of these 120 solved problems and 120 unsolved problems.</p>
<p>In Figure 4, we printed the token output probabilities for different prompts.We provide an example in StrategyQA.The values on the left-hand side of each task represent the results obtained using GPT-4, while the values on the right-hand side represent the results obtained using GPT-3.5-Turbo.A: Among A through E, the answer is (E) aviary.However, it should be noted that bald eagles can also be found in rural areas (A), in Canada (C), and depicted in books (B) and paintings (D).</p>
<p>The question seems to be asking where a bald eagle is likely to be found physically, and an aviary is a place where birds are kept.</p>
<p>Zero-shot-CoT</p>
<p>A: Let's think step by step.A bald eagle is a bird, so it could be found in an aviary (E), which is a place where birds are kept.It could also be found in a rural area (A), as these birds often live in areas with less human activity.A bald eagle could be depicted in a book (B) or a painting (D), as it is a popular symbol in various cultures, especially in the United States.Lastly, bald eagles are also found in Canada (C), as their habitat ranges across most of North America.Therefore, all of the options could be correct depending on the context of the question.Therefore, among A through E, the answer is (A) rural area, (B) book, (C) Canada, (D) painting, (E) aviary.</p>
<p>Zero-shot-CP</p>
<p>A: Let's give a correct and a wrong answer.Correct Answer: (D) painting</p>
<p>Wrong Answer: (A) rural area Therefore, among A through E, the correct answer is (D) painting Ground Truth D  Table 9: Comparison of the results using "Let's give a correct and a wrong answer."and "Let's give a correct answer."prompts.</p>
<p>Category # Examples</p>
<p>The given "correct" answer is the GT, and the given "wrong" answer is indeed incorrect.112 The given "correct" answer is the GT, and the given "wrong" answer is also the GT. 4 The given "correct" answer is the GT, no "wrong" answer is given.4</p>
<p>The given "correct" answer is incorrect, and the given "wrong" answer is the GT.23 The given "correct" answer is incorrect, and the given "wrong" answer is also incorrect.91 The given "correct" answer is incorrect, no "wrong" answer is given.6  The log probability of "Yes": -0.020574939.</p>
<hr />
<p>Correct answer Only</p>
<p>Q: Can the Palace of Westminster tell time in the dark?</p>
<p>A: Let's give a correct answer.Yes, the Palace of Westminster can tell time in the dark.</p>
<p>The log probability of "Yes": -0.027396876.</p>
<p>Figure 4: By setting the logprobs (log probabilities) parameter of the OpenAI API (using GPT-4), we printed the token output probabilities for different prompts.We provide an example in StrategyQA.The ground truth is "Yes".Note that the higher the logprobs value, the greater the probability.Zero-shot-CP makes GPT-4 more confident in the answer compared to Zero-shot and "Let's give a correct answer.".</p>
<p>Q: Mary found 2 seashells and Keith found 5 seashells on the beach .When they cleaned them , they discovered that 9 were cracked .How many seashells did they find together ?</p>
<p>A: Let's give a correct and a wrong answer.Correct answer: To find the total number of seashells they found together, we need to add the number of seashells Mary found (2) to the number of seashells Keith found (5).</p>
<p>2 + 5 = 7</p>
<p>Therefore, Mary and Keith found a total of 7 seashells together.</p>
<p>Wrong answer: To find the total number of seashells they found together, we can subtract the number of cracked seashells (9) from the sum of the seashells Mary and Keith found.</p>
<p>(2 + 5) -9 = -2</p>
<p>This answer doesn't make sense because we can't have a negative number of seashells.Therefore, the correct answer (arabic numerals) is 7. ï’ Q: Sam 's dog had puppies and 8 had spots .He gave 2 to his friends .He now has 6 puppies .How many puppies did he have to start with ?A: Let's give a correct and a wrong answer.Correct answer: To find out how many puppies Sam had to start with, we can work backwards from the information given.We know that Sam currently has 6 puppies after giving 2 to his friends.Before giving away the 2 puppies, he had 8 puppies with spots.Therefore, the total number of puppies he had to start with is 8 + 2 = 10.</p>
<p>Wrong answer: To find out how many puppies Sam had to start with, we can subtract the number of puppies he currently has from the number of puppies he gave away.Since he currently has 6 puppies and he gave away 2, we can subtract 6 from 2, which gives us -4.Therefore, Sam had -4 puppies to start with.Therefore, the correct answer (arabic numerals) is The correct answer is 10.ï‘ (GT: 8) Q: A coin is heads up.Emanuel does not flip the coin.Cheyenne does not flip the coin.Perez flips the coin.Yvette does not flip the coin.Is the coin still heads up?Note that "flip" here means "reverse".A: Let's give a correct and a wrong answer.Correct answer: No, the coin is not still heads up.Since Perez flips the coin, it means that the coin is reversed or flipped, resulting in a different side facing up.</p>
<p>Wrong answer: Yes, the coin is still heads up.Since only Perez flips the coin and the others do not, the initial state of the coin remains unchanged.Therefore, the correct answer (Yes or No) is No, the coin is not still heads up.ï’ Q: A coin is heads up.Mya flips the coin.Fernando flips the coin.Bubba does not flip the coin.Tommy does not flip the coin.Is the coin still heads up?Note that "flip" here means "reverse".A: Let's give a correct and a wrong answer.Correct answer: No, the coin is not still heads up.When Mya flips the coin, it will be reversed, so it will be tails up.The subsequent actions of Fernando, Bubba, and Tommy are irrelevant because the coin has already been flipped and its state has changed.</p>
<p>Wrong answer: Yes, the coin is still heads up.Since Bubba and Tommy did not flip the coin, its original state remains unchanged.Therefore, the correct answer (Yes or No) is No, the coin is not still heads up.ï‘ (GT: Yes) Q: A pet supply store has 600 bags of dog food and 327 bags of cat food.How many more bags of dog food are there than cat food?A: Let's give a correct and a wrong answer.Correct answer: To find out how many more bags of dog food there are than cat food, we subtract the number of bags of cat food from the number of bags of dog food.</p>
<p>Number of bags of dog food -Number of bags of cat food = 600 -327 = 273</p>
<p>Therefore, there are 273 more bags of dog food than cat food.</p>
<p>Wrong answer: To find out how many more bags of dog food there are than cat food, we add the number of bags of dog food and cat food together.Wrong answer: If we assume that all the fleas perished, then the total number of fleas that perished would be 36 + 85 = 121 fleas.Therefore, the correct answer (arabic numerals) is 0 fleas perished.ï‘ (GT: 121)</p>
<p>Figure 3 :
3
Figure 3: Accuracy scores by varying the number of wrong answers.We test GPT-4 and GPT-3.5-Turbo on (a) AQUA-RAT, (b) GSM8K, (c) AddSub and (d) MultiArith.The range of the number of wrong answers is from 0 (Zero-shot) to 4.</p>
<p>Question Q: A bald eagle is likely to be found on what kind of work?Answer Choices: (A) rural area (B) book (C) canada (D) painting (E) aviary Zero-shot</p>
<p>the Palace of Westminster tell time in the dark?A: Let's give a correct and a wrong answer.Correct Answer: Yes, the Palace of Westminster can tell time in the dark.The clock tower, commonly known as Big Ben, is equipped with illuminated clock faces, allowing it to be visible and tell time even at night.Wrong Answer: No, the Palace of Westminster cannot tell time in the dark because the clock faces are not illuminated and are only visible during daylight.The log probability of "Yes": -0.0022649246.â˜‘ (The highest) -------------------------------------------------------------Zero-Shot Q: Can the Palace of Westminster tell time in the dark?A: The answer (Yes or No) is Yes.</p>
<p>Figure 5 :Figure 7 :
57
Figure 5: Example outputs by Zero-shot-CP for AddSub.</p>
<p>Figure 8 :
8
Figure 8: Example outputs by Zero-shot-CP for Coin Flip.</p>
<p>Number of bags of dog food + Number of bags of cat food = 600 + 327 = 927 Therefore, there are 927 more bags of dog food than cat food.Therefore, the correct answer (arabic numerals) is 273.ï’ Q: Harry Hound had a terrible earache yesterday.When I peered into his ears yesterday, I found 36 frisky fleas having a party in his right ear and 85 baby fleas sleeping peacefully in his left ear.I cleaned out Harry Hound's ears.How many fleas perished?A: Let's give a correct and a wrong answer.Correct answer: If you found 36 fleas in Harry Hound's right ear and 85 fleas in his left ear, and you cleaned out his ears, it can be assumed that all the fleas were removed and none perished.</p>
<p>Figure 14 :
14
Figure 14: Example outputs by Zero-shot-CP for SingleEq.</p>
<p>'s give a correct and a wrong answer.
ã€1st promptã€‘Reasoning ExtractionQ: Tim grows 5 trees. Each year he collects 6lemons from each tree. How many lemons doeshe get in a decade?A: Let
leverages LLMs to automatically generate relevant fewshot examples for each question.In contrast to these works, our method emphasizes eliciting selfawareness in LLMs regarding potential errors and actively avoiding them.Learning from Negative Examples Contrastive learning, a widely adopted technique in deep learn-</p>
<p>Table 1 :
1
Table10provides the categorization and counts of these 120 Accuracy (in percentage) comparison of Zero-shot-CP with Zero-shot and Zero-shot-CoT on five datasets.We run all methods 5 times using GPT-3.5-Turboandreportaveragescores.Zero-shot-CP significantly outperforms baselines on StrategyQA, AddSub, SVAMP and CommonsenseQA based on student t-test (p &lt; 0.05).Danny collects bottle caps and wrappers.He found 46 wrappers and 50 bottle caps at the park.Now he has 21 bottle caps and 52 wrappers in his collection.How many more bottle caps than wrappers did danny find at the park?Let's think step by step.First, we need to find out how many bottle caps and wrappers Danny had before he found the new ones at the park.We know that after finding the new ones, he has 50 bottle caps and 52 wrappers.Before finding the new ones, he had 21 bottle caps and 46 wrappers.So, the number of bottle caps he found at the park is 50 (total now) -21 (total before) = 29 bottle caps.The number of wrappers he found at the park is 52 (total now) -46 (total before) = 6 wrappers.Therefore, Danny found 29 -6 = 23 more bottle caps than wrappers at the park.Therefore, the answer (arabic numerals) is 23Zero-shot-CP A: Let's give a correct and a wrong answer.Correct Answer: Danny found 50 bottle caps and 46 wrappers at the park.So, he found 50 -46 = 4 more bottle caps than wrappers at the park.Danny found 50 bottle caps and 46 wrappers at the park.So, he found 46 -50 = -4 more bottle caps than wrappers at the park.Therefore, the correct answer (arabic numerals) is 4
MultiArith GSM8K StrategyQA AddSub SVAMP CommonsenseQAZero-shot60.9714.3965.0282.7869.7471.33Zero-shot-CoT94.8775.5660.7486.1681.7868.96Zero-shot-CP95.1373.2267.3990.4683.0873.81Question Q: Zero-shot A: The answer (arabic numerals) is -4. Danny found 50 bottle caps and 46 wrappers at the park, so he found 4 more bottle caps than wrappers.Zero-shot-CoT A: Incorrect Answer: Ground Truth 4</p>
<p>Table 2 :
2
An example question from SVAMP and responses from Zero-shot, Zero-shot-CoT, Zero-shot-CP and Zero-shot-CoT-CP.We use GPT-4 as the base model.</p>
<p>Table 3 :
3
Comparison prompting templates using accuracies (in percentage) on AQUA-RAT, GSM8K, AddSub and MultiArith in zero-shot setting.GPT-4 is used as the model.Bolded numbers indicate the best results within each block's column, while underlined numbers indicate the best results across the entire column.
GPT-4</p>
<p>Table 4 :
4
Comparison with baseline methods using accuracies (in percentage) on MultiArith, GSM8K, StrategyQA, AQUA-RAT and SVAMP.GPT-3.5-Turbo is used as the model if not specified.The baseline results with citations are obtained from corresponding papers.Bolded numbers indicate the best results within each block's column, while underlined numbers indicate the best results across the entire column.</p>
<p>Table 5 :
5
Accuracy (in percentage) comparison of Zero-shot-CP with Zero-shot and Zero-shot-CoT on each dataset.
Arithmetic</p>
<p>Table 6 :
6
An example question from CommonsenseQA and responses from Zero-shot, Zero-shot-CoT, and Zeroshot-CP.We use GPT-4 as the base model.
No TaskCP answer extraction prompts1SingleEqTherefore, the correct answer (arabic numerals) is2AddSubTherefore, the correct answer (arabic numerals) is3MultiArithTherefore, the correct answer (arabic numerals) is4GSM8KTherefore, the correct answer (arabic numerals) is5AQUA-RATTherefore, among A through E, the correct answer is6SVAMPTherefore, the correct answer (arabic numerals) is7CommonsenseQATherefore, among A through E, the correct answer is8StrategyQATherefore, the correct answer (Yes or No) is9Date Understanding Therefore, among A through F, the correct answer is10Shuffled ObjectsTherefore, among A through C, the correct answer is11Last LettersTherefore, the correct answer is12Coin FlipTherefore, the correct answer (Yes or No) is</p>
<p>Table 7 :
7
Answer extraction prompts used in our CP experiments.
MultiArith GSM8K StrategyQA AQUA SVAMPLLaMA3-8BZero-shot31.038.1--52.8Zero-shot-CP57.354.9--61.4LLaMA3-70BZero-shot86.563.754.538.2-Zero-shot-CP97.066.157.562.2-ChatGLM3-6BZero-shot5.34.3---Zero-shot-CP67.040.0---Qwen1.5-72B-ChatZero-shot54.719.371.231.165.2Zero-shot-CP75.552.173.545.377.4</p>
<p>Table 8 :
8
Accuracy (in percentage) comparison of Zero-shot-CP with Zero-shot using open LLMs.
CommonsenseQA StrategyQA</p>
<p>Table 10 :
10
Categorization results of Zero-shot-CP output (with GPT-4) on 240 problems.We manually annotated 10 solved problems and 10 unsolved problems for each of the 12 datasets.GT means Ground Truth.See Appendix B for the link of the examples.</p>
<p>The datasets are available at https://github. com/kojima-takeshi188/zero_shot_cot/tree/main/ dataset.
https://docs.llama-api.com/quickstart
https://prek-math-te.stanford.edu/operations/ analyzing-thinking-underlying-wrong-answers
https://mathmistakes.org/category/ elementary-school/
https://www.gutenberg.org/ebooks/38769
https://www.proprofs.com/quiz-school/story. php?title=common-sense-quiz_1
https://www.wikihow.com/Common-Sense-Quiz
Q: Machine A puts out a yo-yo every 6 minutes.Machine B puts out a yo-yo every 9 minutes.After how many minutes will they have produced 10 yo-yos?Answer Choices: (A) 24 minutes (B) 32 minutes (C) 36 minutes (D) 64 minutes (E) 72 minutes A: Let's give a correct and a wrong answer.Correct answer: (C) 36 minutes Explanation: Machine A produces a yo-yo every 6 minutes, so in 36 minutes, it will produce 36/6 = 6 yo-yos.Machine B produces a yo-yo every 9 minutes, so in 36 minutes, it will produce 36/9 = 4 yo-yos.Therefore, in a total of 36 minutes, both machines will have produced 6 + 4 = 10 yo-yos.Wrong answer: (B) 32 minutes Explanation: Machine A produces a yo-yo every 6 minutes, so in 32 minutes, it will produce 32/6 = 5.33 yo-yos (approximately).Machine B produces a yo-yo every 9 minutes, so in 32 minutes, it will produce 32/9 = 3.56 yo-yos (approximately).Therefore, in a total of 32 minutes, both machines will have produced 5.33 + 3.56 = 8.89 yo-yos (approximately), which is less than 10 yo-yos.Therefore, among A through E, the correct answer is (C) 36 minutes.ï’ Q: On a map, the length of the road from Town F to Town G is measured to be 20 inches.On this map, 1/4 inch represents an actual distance of 10 miles.What is the actual distance, in miles, from Town F to Town G along this road?Answer Choices: (A) 800 (B) 720 (C) 960 (D) 1140 (E) 1160 A: Let's give a correct and a wrong answer.Correct Answer: (C) 960 miles Explanation: According to the map, 1/4 inch represents 10 miles.Therefore, 1 inch represents 4 times that, which is 40 miles.Since the length of the road from Town F to Town G on the map is 20 inches, the actual distance in miles would be 20 inches multiplied by 40 miles per inch, which equals 800 miles.Wrong Answer: (A) 800 milesExplanation: This answer is incorrect because it does not take into account the scale of the map.The scale of the map indicates that 1/4 inch represents 10 miles, so the actual distance from Town F to Town G is greater than 800 miles.Therefore, among A through E, the correct answer is (C) 960 miles.ï‘ (GT: A) The music had a soothing effect on the man, causing him to close his eyes and relax.Wrong answer: (A) comaThe music did not cause the man to enter a coma.Therefore, among A through E, the correct answer is (C) soothing.ï’ Q: As he looked out the window, he knew the landing was happening soon, and it made him nervous, but where would he be soon?Answer Choices: (A) apartment building (B) disembark (C) stairwell (D) deplane (E) airport A: Let's give a correct and a wrong answer.Correct answer: (D) deplane Wrong answer: (C) stairwell Therefore, among A through E, the correct answer is (D) deplane.ï‘ (GT: E) Q: Jeff' s work is 3 miles away.He walks there and back each day he works.How many miles does he walk if he has to work 5 times a week?A: Let's give a correct and a wrong answer.Correct answer: Jeff walks 3 miles to work and 3 miles back, so he walks a total of 6 miles each day he works.If he works 5 times a week, he would walk a total of 6 miles/day * 5 days/week = 30 miles in a week.Wrong answer: Jeff walks 3 miles to work and 3 miles back, so he walks a total of 3 miles each day he works.If he works 5 times a week, he would walk a total of 3 miles/day * 5 days/week = 15 miles in a week.Therefore, the correct answer (arabic numerals) is 30 miles.ï’ Q: If one person can paint half a house in 5 days, how many hours would it take for 5 people to paint a whole house?A: Let's give a correct and a wrong answer.Correct answer: If one person can paint half a house in 5 days, it means that one person can paint one whole house in 10 days.Since there are 5 people working together, we can divide the time by 5. Therefore, it would take 5 people 2 days to paint a whole house.Wrong answer: If one person can paint half a house in 5 days, it means that one person can paint one whole house in 10 days.Since there are 5 people working together, we can divide the time by 5. Therefore, it would take 5 people 2 hours to paint a whole house.Therefore, the correct answer (arabic numerals) is 2 days.ï‘ (GT: 48) Q: Take the last letters of each words in "Marlen Sonja Anita Dale" and concatenate them.A: Let's give a correct and a wrong answer.Correct answer: The last letters of each word in "Marlen Sonja Anita Dale" are "n a a e." Therefore, when concatenated, they form the word "naae."Wrong answer: The last letters of each word in "Marlen Sonja Anita Dale" are "n j a e." Therefore, when concatenated, they form the word "njae."Therefore, the correct answer is "naae."ï’ Q: Take the last letters of each words in "Rich Parker Edward Milagros" and concatenate them.A: Let's give a correct and a wrong answer.Correct answer: The last letters of each word in "Rich Parker Edward Milagros" "h r d s s".Therefore, when concatenated, they form the word "hrdss".Wrong answer: The last letters of each word in "Rich Parker Edward Milagros" are "r r d s s".Therefore, when concatenated, they form the word "rrdss".Therefore, the correct answer is "hrdss".ï‘ (GT: hrds) Q: Roger is at the library helping put away books.There are 14 book to put away total but a librarian takes 2 of them and leaves Roger with the rest.If he can fit 3 books on a shelf, how many shelves will he need?A: Let's give a correct and a wrong answer.Correct answer: If there are 14 books in total and the librarian takes 2 of them, then Roger is left with 14 -2 = 12 books to put away.Since he can fit 3 books on a shelf, he will need 12 / 3 = 4 shelves.Wrong answer: If there are 14 books in total and the librarian takes 2 of them, then Roger is left with 14 -2 = 12 books to put away.Since he can fit 3 books on a shelf, he will need 12 / 3 = 3 shelves.Therefore, the correct answer (arabic numerals) is 4 shelves.ï’ Q: John earned 8 dollars for each lawn he mowed.If he had 15 lawns to mow, but forgot to mow 7 of them, how much money did he actually earn?A: Let's give a correct and a wrong answer.Correct answer: John earned 8 dollars for each lawn he mowed.Since he mowed 15 lawns, he would have earned 8 * 15 = 120 dollars.Wrong answer: John earned 8 dollars for each lawn he mowed.Since he forgot to mow 7 lawns, he actually earned 8 * (15 -7) = 8 * 8 = 64 dollars.Therefore, the correct answer (arabic numerals) is 120 dollars.ï‘ (GT: 64) Explanation: At the start of the dance, Bob is dancing with Rodrigo.After the first partner switch, Bob and Alice switch partners, so Bob is now dancing with Karl.Then, Alice and Claire switch partners, so Bob is still dancing with Karl.Finally, Alice and Bob switch partners, so Bob is now dancing with Helga.Therefore, at the end of the dance, Bob is dancing with Helga.Therefore, among A through C, the correct answer is (C) Helga.ï’ Q: Alice, Bob, and Claire are dancers at a square dance.At the start of a song, they each have a partner: Alice is dancing with Sam, Bob is dancing with Karl, and Claire is dancing with Helga.Throughout the song, the dancers often trade partners.First, Alice and Claire switch partners.Then, Alice and Bob switch partners.Finally, Claire and Bob switch partners.At the end of the dance, Claire is dancing with Which choice is true ?Answer Choices: (A) Sam.(B) Karl.(C) Helga.A: Let's give a correct and a wrong answer.Correct answer: (B) Karl.Explanation:At the start of the dance, Alice is dancing with Sam, Bob is dancing with Karl, and Claire is dancing with Helga.First, Alice and Claire switch partners.After this switch, Alice is now dancing with Helga, and Claire is dancing with Sam.Bob's partner remains the same (Karl).Next, Alice and Bob switch partners.After this switch, Alice is now dancing with Karl, and Bob is dancing with Helga.Claire's partner remains the same (Sam).Finally, Claire and Bob switch partners.After this switch, Claire is now dancing with Karl, and Bob is dancing with Sam.Alice's partner remains the same (Helga).Therefore, at the end of the dance, Claire is dancing with Karl.So the correct answer is (B) Karl.Therefore, among A through C, the correct answer is (B) Karl.ï‘ (GT: C) Q: Is it unusual to play Happy hardcore music at a funeral?A: Let's give a correct and a wrong answer.Correct answer: Yes, it is highly unusual to play Happy hardcore music at a funeral.Funerals are typically solemn occasions where mourners gather to pay their respects and honor the life of the deceased.The music played at funerals is usually chosen to reflect the somber and reflective atmosphere of the event.Genres such as classical music, hymns, or soft instrumental pieces are more commonly associated with funerals.Wrong answer: No, it is not unusual to play Happy hardcore music at a funeral.Funerals are evolving events, and people have different ways of expressing their grief and celebrating the life of the deceased.Some individuals may have had a strong connection to Happy hardcore music, and playing it at the funeral could be a way to honor their memory and bring a sense of joy and energy to the ceremony.Ultimately, the choice of music at a funeral is a personal decision, and if it reflects the wishes and personality of the deceased, then it can be considered appropriate.Therefore, the correct answer (Yes or No) is Yes, it is highly unusual to play Happy hardcore music at a funeral.ï’ Q: Would a pear sink in water?A: Let's give a correct and a wrong answer.Correct answer: Yes, a pear would sink in water.Pears have a density greater than that of water, which means they are denser and will sink when placed in water.Wrong answer: No, a pear would not sink in water.Pears are known for their buoyancy and ability to float in water due to their unique composition.Therefore, the correct answer (Yes or No) is Yes, a pear would sink in water.ï‘ (GT: No) Q: Mary is baking a cake.The recipe calls for 7 cups of sugar and 10 cups of flour.She already put in 4 cups of sugar.How many more cups of sugar does she need to add? A: Let's give a correct and a wrong answer.Correct answer: Mary needs to add 3 more cups of sugar.Wrong answer: Mary needs to add 6 more cups of sugar.Therefore, the correct answer (arabic numerals) is 3. ï’ Q: The grasshopper and the frog had a jumping contest.The grasshopper jumped 13 inches.The grasshopper jumped 2 inches farther than the grasshopper.How far did the frog jump?A: Let's give a correct and a wrong answer.Correct answer: The grasshopper jumped 13 inches, and the frog jumped 2 inches farther than the grasshopper.Therefore, the frog jumped 13 + 2 = 15 inches.Wrong answer: The grasshopper jumped 13 inches, and the frog jumped 2 inches farther than the grasshopper.Therefore, the frog jumped 13 + 2 = 15 inches.Therefore, the correct answer (arabic numerals) is 15 inches.ï‘ (GT: 11)
Xavier Amatriain, arXiv:2401.14423Prompt design and engineering: Introduction and advanced methods. 2024arXiv preprint</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, arXiv:2309.16609Qwen technical report. Xiaohuan Zhou, and Tianhang Zhu2023arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, arXiv:2211.125882022arXiv preprint</p>
<p>Ken Yew, Guizhen Chia, Chen, Anh Luu, Soujanya Tuan, Lidong Poria, Bing, arXiv:2311.09277Contrastive chain-of-thought prompting. 2023arXiv preprint</p>
<p>Glm: General language model pretraining with autoregressive blank infilling. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Pal: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, International Conference on Machine Learning. PMLR2023</p>
<p>A survey on contrastive selfsupervised learning. Ashish Jaiswal, Ramesh Ashwin, Mohammad Zaki Babu, Debapriya Zadeh, Fillia Banerjee, Makedon, 202092Technologies</p>
<p>Large language models are zero-shot reasoners. Geunwoo Kim, Pierre Baldi, Stephen Mcaleer, Advances in Neural Information Processing Systems. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, 2023. 202236Advances in neural information processing systems</p>
<p>Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, ACM Computing Surveys. 5592023</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Advances in Neural Information Processing Systems. 202336</p>
<p>arXiv:2303.08774Gpt-4 technical report. 2023OpenAIPreprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances in Neural Information Processing Systems. 202336</p>
<p>Getting it wrong: Surprising tips on how to learn. L Henry, Bridgid Roediger, Finn, Scientific American. 2009</p>
<p>Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, Aman Chadha, arXiv:2402.07927A systematic survey of prompt engineering in large language models: Techniques and applications. 2024arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202336</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, TimothÃ©e Lachaux, Baptiste Lacroix, Naman RoziÃ¨re, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, Quoc V Le, International Conference on Learning Representations. 2022a</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022b35</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 2023a36</p>
<p>Beyond chain-of-thought, effective graph-of-thought reasoning in large language models. Yao Yao, Zuchao Li, Hai Zhao, arXiv:2305.165822023barXiv preprint</p>
<p>Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure Leskovec, Percy Liang, Ed H Chi, Denny Zhou, arXiv:2310.01714Large language models as analogical reasoners. 2023arXiv preprint</p>
<p>Niket Tandon, and Uri Alon. Tianjun Zhang, Aman Madaan, Luyu Gao, Steven Zheng, Swaroop Mishra, Yiming Yang, arXiv:2402.05403-context principle learning from mistakes. 2024arXiv preprint</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, The Eleventh International Conference on Learning Representations. 2023ICLR 2023</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
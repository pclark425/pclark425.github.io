<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6557 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6557</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6557</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-129.html">extraction-schema-129</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-275906690</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2501.13947v3.pdf" target="_blank">A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods</a></p>
                <p><strong>Paper Abstract:</strong> The rapid development of artificial intelligence has led to marked progress in the field. One interesting direction for research is whether Large Language Models (LLMs) can be integrated with structured knowledge-based systems. This approach aims to combine the generative language understanding of LLMs and the precise knowledge representation systems by which they are integrated. This article surveys the relationship between LLMs and knowledge bases, looks at how they can be applied in practice, and discusses related technical, operational, and ethical challenges. Utilizing a comprehensive examination of the literature, the study both identifies important issues and assesses existing solutions. It demonstrates the merits of incorporating generative AI into structured knowledge-base systems concerning data contextualization, model accuracy, and utilization of knowledge resources. The findings give a full list of the current situation of research, point out the main gaps, and propose helpful paths to take. These insights contribute to advancing AI technologies and support their practical deployment across various sectors.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6557.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6557.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits step-by-step intermediate reasoning from LLMs to improve multi-step problem solving such as arithmetic and commonsense reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>multi-step reasoning / arithmetic and commonsense tasks (general)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step reasoning tasks (e.g., math problems, commonsense reasoning, symbolic tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy / solve rate (task-dependent)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Described as a step-by-step prompting style that improves interpretability by exposing intermediate reasoning; treated as a baseline sequential reasoning technique contrasted with newer approaches that explore multiple reasoning paths or use strategy elicitation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6557.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6557.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Buffer of Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A meta-buffer approach that stores distilled high-level reasoning templates from previous problem-solving episodes to instantiate for new tasks and improve multi-step reasoning efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Buffer of Thoughts (BoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential / template-augmented</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>multi-step reasoning / mathematical and logical tasks (general)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step reasoning requiring reusable heuristics and templates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>task-specific accuracy / solve rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>standard multi-query prompting methods</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Paper notes BoT outperforms standard multi-query prompting methods by using reusable 'mental templates', increasing reasoning efficiency and accuracy; positioned as introducing more diverse reusable reasoning patterns compared to vanilla CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6557.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6557.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Strategic Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An enhanced CoT prompting technique that elicits explicit strategy planning via a two-stage prompt to increase stability and generate accurate single-query reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Strategic Chain-of-Thought (SCoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential / strategy-elicitation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>homogeneous / strategy-focused</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>multi-step reasoning benchmarks (general)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Complex reasoning requiring strategy selection and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>task accuracy / correctness (reported as improvements)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Self-Consistency; Buffer of Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Reported to outperform Self-Consistency and Buffer of Thoughts by using strategic elicitation to produce more stable and consistent reasoning; authors emphasize improved single-query stability rather than aggregating multiple diverse chains.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6557.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6557.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-of-Thoughts / Graph-of-Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique that represents the reasoning process as a graph allowing exploration of multiple non-linear reasoning paths and evaluation of alternatives to improve complex problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Graph-of-Thoughts (GoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>tree/graph-search</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>multi-path / multi-hop reasoning tasks (general)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Problems benefitting from exploring multiple reasoning branches and evaluating alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>task-specific accuracy / solution quality</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>sequential CoT methods</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Described as enabling non-linear exploration of multiple reasoning paths, thus increasing diversity of internal reasoning and potentially improving solution quality for tasks where branching search is beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6557.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6557.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid approach combining retrieval from external corpora/KBs with LLM generation to ground outputs in up-to-date factual information and reduce hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RAG-based LLMs (various)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>retrieval-based</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Natural Questions (NQ)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-domain question answering requiring factual grounding from external documents.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM) improvement over DPR baseline</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>3.4</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>DPR</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>3.4</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Survey reports aggregated RAG improvements (e.g., +3.4% EM on NQ over DPR baseline). RAG is characterized as retrieval-grounded (mixed reasoning style: internal chain-of-thought plus retrieved evidence) and effective for keeping outputs current and reducing hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6557.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6557.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphRAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphRAG (graph-augmented RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of RAG that integrates graph-structured representations or reranking to enable multi-hop inference and entity disambiguation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GraphRAG variants (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>GraphRAG</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>retrieval-based / graph-enhanced</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse / multi-hop</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>SQuALITY (example)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reading comprehension / summarization tasks requiring multi-hop evidence synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BERTScore improvement over RAG</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>7.04</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>7.04</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Reported gains (e.g., +7.04% BERTScore on SQuALITY) attributed to graph-based reranking and structured retrieval that reduce noise and enable multi-hop reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6557.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6557.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (Reasoning + Acting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that interleaves reasoning and external actions/tools (e.g., retrieval, calculators) to improve performance on complex reasoning and tool-use tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>tool-augmented / sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>HotpotQA (example reported)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-hop question answering requiring retrieval and reasoning across documents.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1 improvement over RAG</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>8.7</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>8.7</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Survey reports ReAct achieving substantial F1 improvements (+8.7% over RAG in the reported table), attributed to its ability to combine reasoning steps with actions (retrieval/tool calls) to gather evidence and refine answers.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6557.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6557.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ToG / Think-on-Graph</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A knowledge-graph-guided reasoning framework that constructs evidence chains over a KG to support interpretable multi-hop reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ToG variants (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Think-on-Graph (ToG)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>graph-based / multi-hop</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>WebQSP</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Knowledge-base question answering over graph-structured data (multi-hop).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy improvement over CoT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>14.2</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>14.2</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Reported to yield large accuracy gains (e.g., +14.2% on WebQSP over CoT) by leveraging graph-structured retrieval and explicit evidence chains; positions graph methods as enabling richer/diverse multi-hop reasoning versus sequential CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6557.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6557.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KAG (Knowledge-Augmented Generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A knowledge-augmented generation framework that tightly integrates structured knowledge (KGs) with generation to boost factual accuracy and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>KAG variants (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Knowledge-Augmented Generation (KAG)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>retrieval-based / KG-enhanced</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>WikiMulti-HopQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-hop question answering requiring synthesis of KG and text evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM) improvement over RAG</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>34.4</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>34.4</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Survey notes very large EM gains for KAG-style KG-enhanced approaches (e.g., +34.4% EM on WikiMulti-HopQA over RAG), attributing improvements to structured multi-hop inference and better entity linking; suggests graph-enhanced methods produce diverse reasoning trajectories that outperform plain retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6557.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6557.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KGAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KGAgent (KG-based autonomous agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autonomous agent framework that explores and reasons over knowledge graphs to perform complex KG-QA and retrieval tasks efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>KGAgent (framework)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>KGAgent (autonomous KG exploration and reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>graph-based / agentic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse / agentic</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>WebQSP</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Knowledge-base question answering and multi-hop retrieval over graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Hits@1 improvement over GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>16.6</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>16.6</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Survey reports KGAgent producing substantial Hits@1 and F1 gains over GPT-4 (e.g., +16.6% Hits@1 on WebQSP), explained by KGAgent's focused graph traversal and structured inference enabling more precise, diversified reasoning paths than unguided LLM responses.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6557.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e6557.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BloombergGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BloombergGPT (financial FinLLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 50B-parameter finance-focused LLM pre-trained on general and large financial corpora (FinPile) to improve downstream financial NLP tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BloombergGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>50B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>domain-specific pretraining / FinLLM adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>domain-specialized</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style (domain-specialized)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ConvFinQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Conversational financial question answering (financial reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>43.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>GPT-4 (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Reported ConvFinQA EM = 43%; authors compare this to a general crowd baseline (47%) and GPT-4 zero-shot (6976%), noting that domain pretraining improved financial adaptation but still trails strongest general-purpose LLM reasoning (GPT-4) on this benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6557.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e6557.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetaRAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MetaRAG (metacognitive RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A RAG variant that integrates metacognitive monitoring, evaluation, and planning to self-assess reasoning paths and iteratively adjust retrieval/inference for multi-hop QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MetaRAG (framework)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>MetaRAG (metacognitive retrieval-augmented generation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>retrieval-based / metacognitive</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed / adaptive</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>multi-hop QA (general)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-hop question answering requiring iterative retrieval and self-assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>task-specific accuracy / F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>standard RAG</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Described as adding metacognitive capabilities (monitoring/evaluation/planning) to RAG to improve multi-hop QA by enabling the model to detect knowledge gaps and re-query or adjust reasoning paths, i.e., promoting adaptive (diverse) reasoning behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Buffer of thoughts: Thought-augmented reasoning with large language models <em>(Rating: 2)</em></li>
                <li>Strategic chain-of-thought: Guiding accurate reasoning in LLMs through strategy elicitation <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for large language models: A survey <em>(Rating: 2)</em></li>
                <li>Think-on-graph: Deep and responsible reasoning of large language model on knowledge graph <em>(Rating: 2)</em></li>
                <li>GraphRAG (local) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6557",
    "paper_id": "paper-275906690",
    "extraction_schema_id": "extraction-schema-129",
    "extracted_data": [
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that elicits step-by-step intermediate reasoning from LLMs to improve multi-step problem solving such as arithmetic and commonsense reasoning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLMs (unspecified)",
            "model_size": null,
            "reasoning_method_name": "Chain-of-Thought (CoT)",
            "reasoning_method_type": "sequential",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "multi-step reasoning / arithmetic and commonsense tasks (general)",
            "task_description": "Multi-step reasoning tasks (e.g., math problems, commonsense reasoning, symbolic tasks).",
            "performance_metric": "accuracy / solve rate (task-dependent)",
            "performance_value": null,
            "comparison_target_method": "",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Described as a step-by-step prompting style that improves interpretability by exposing intermediate reasoning; treated as a baseline sequential reasoning technique contrasted with newer approaches that explore multiple reasoning paths or use strategy elicitation.",
            "ablation_study_present": null,
            "uuid": "e6557.0",
            "source_info": {
                "paper_title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "BoT",
            "name_full": "Buffer of Thoughts",
            "brief_description": "A meta-buffer approach that stores distilled high-level reasoning templates from previous problem-solving episodes to instantiate for new tasks and improve multi-step reasoning efficiency.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLMs (unspecified)",
            "model_size": null,
            "reasoning_method_name": "Buffer of Thoughts (BoT)",
            "reasoning_method_type": "sequential / template-augmented",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "multi-step reasoning / mathematical and logical tasks (general)",
            "task_description": "Multi-step reasoning requiring reusable heuristics and templates.",
            "performance_metric": "task-specific accuracy / solve rate",
            "performance_value": null,
            "comparison_target_method": "standard multi-query prompting methods",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Paper notes BoT outperforms standard multi-query prompting methods by using reusable 'mental templates', increasing reasoning efficiency and accuracy; positioned as introducing more diverse reusable reasoning patterns compared to vanilla CoT.",
            "ablation_study_present": null,
            "uuid": "e6557.1",
            "source_info": {
                "paper_title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "SCoT",
            "name_full": "Strategic Chain-of-Thought",
            "brief_description": "An enhanced CoT prompting technique that elicits explicit strategy planning via a two-stage prompt to increase stability and generate accurate single-query reasoning paths.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLMs (unspecified)",
            "model_size": null,
            "reasoning_method_name": "Strategic Chain-of-Thought (SCoT)",
            "reasoning_method_type": "sequential / strategy-elicitation",
            "reasoning_style_diversity": "homogeneous / strategy-focused",
            "benchmark_name": "multi-step reasoning benchmarks (general)",
            "task_description": "Complex reasoning requiring strategy selection and planning.",
            "performance_metric": "task accuracy / correctness (reported as improvements)",
            "performance_value": null,
            "comparison_target_method": "Self-Consistency; Buffer of Thoughts",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Reported to outperform Self-Consistency and Buffer of Thoughts by using strategic elicitation to produce more stable and consistent reasoning; authors emphasize improved single-query stability rather than aggregating multiple diverse chains.",
            "ablation_study_present": null,
            "uuid": "e6557.2",
            "source_info": {
                "paper_title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "GoT",
            "name_full": "Graph-of-Thoughts / Graph-of-Thoughts",
            "brief_description": "A technique that represents the reasoning process as a graph allowing exploration of multiple non-linear reasoning paths and evaluation of alternatives to improve complex problem solving.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLMs (unspecified)",
            "model_size": null,
            "reasoning_method_name": "Graph-of-Thoughts (GoT)",
            "reasoning_method_type": "tree/graph-search",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "multi-path / multi-hop reasoning tasks (general)",
            "task_description": "Problems benefitting from exploring multiple reasoning branches and evaluating alternatives.",
            "performance_metric": "task-specific accuracy / solution quality",
            "performance_value": null,
            "comparison_target_method": "sequential CoT methods",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Described as enabling non-linear exploration of multiple reasoning paths, thus increasing diversity of internal reasoning and potentially improving solution quality for tasks where branching search is beneficial.",
            "ablation_study_present": null,
            "uuid": "e6557.3",
            "source_info": {
                "paper_title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation",
            "brief_description": "A hybrid approach combining retrieval from external corpora/KBs with LLM generation to ground outputs in up-to-date factual information and reduce hallucinations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "RAG-based LLMs (various)",
            "model_size": null,
            "reasoning_method_name": "Retrieval-Augmented Generation (RAG)",
            "reasoning_method_type": "retrieval-based",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "Natural Questions (NQ)",
            "task_description": "Open-domain question answering requiring factual grounding from external documents.",
            "performance_metric": "Exact Match (EM) improvement over DPR baseline",
            "performance_value": 3.4,
            "comparison_target_method": "DPR",
            "performance_difference": 3.4,
            "statistical_significance": null,
            "analysis_notes": "Survey reports aggregated RAG improvements (e.g., +3.4% EM on NQ over DPR baseline). RAG is characterized as retrieval-grounded (mixed reasoning style: internal chain-of-thought plus retrieved evidence) and effective for keeping outputs current and reducing hallucinations.",
            "ablation_study_present": null,
            "uuid": "e6557.4",
            "source_info": {
                "paper_title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "GraphRAG",
            "name_full": "GraphRAG (graph-augmented RAG)",
            "brief_description": "An extension of RAG that integrates graph-structured representations or reranking to enable multi-hop inference and entity disambiguation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GraphRAG variants (unspecified)",
            "model_size": null,
            "reasoning_method_name": "GraphRAG",
            "reasoning_method_type": "retrieval-based / graph-enhanced",
            "reasoning_style_diversity": "diverse / multi-hop",
            "benchmark_name": "SQuALITY (example)",
            "task_description": "Reading comprehension / summarization tasks requiring multi-hop evidence synthesis.",
            "performance_metric": "BERTScore improvement over RAG",
            "performance_value": 7.04,
            "comparison_target_method": "RAG",
            "performance_difference": 7.04,
            "statistical_significance": null,
            "analysis_notes": "Reported gains (e.g., +7.04% BERTScore on SQuALITY) attributed to graph-based reranking and structured retrieval that reduce noise and enable multi-hop reasoning.",
            "ablation_study_present": null,
            "uuid": "e6557.5",
            "source_info": {
                "paper_title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct (Reasoning + Acting)",
            "brief_description": "A framework that interleaves reasoning and external actions/tools (e.g., retrieval, calculators) to improve performance on complex reasoning and tool-use tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLMs (unspecified)",
            "model_size": null,
            "reasoning_method_name": "ReAct",
            "reasoning_method_type": "tool-augmented / sequential",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "HotpotQA (example reported)",
            "task_description": "Multi-hop question answering requiring retrieval and reasoning across documents.",
            "performance_metric": "F1 improvement over RAG",
            "performance_value": 8.7,
            "comparison_target_method": "RAG",
            "performance_difference": 8.7,
            "statistical_significance": null,
            "analysis_notes": "Survey reports ReAct achieving substantial F1 improvements (+8.7% over RAG in the reported table), attributed to its ability to combine reasoning steps with actions (retrieval/tool calls) to gather evidence and refine answers.",
            "ablation_study_present": null,
            "uuid": "e6557.6",
            "source_info": {
                "paper_title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "ToG",
            "name_full": "ToG / Think-on-Graph",
            "brief_description": "A knowledge-graph-guided reasoning framework that constructs evidence chains over a KG to support interpretable multi-hop reasoning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "ToG variants (unspecified)",
            "model_size": null,
            "reasoning_method_name": "Think-on-Graph (ToG)",
            "reasoning_method_type": "graph-based / multi-hop",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "WebQSP",
            "task_description": "Knowledge-base question answering over graph-structured data (multi-hop).",
            "performance_metric": "accuracy improvement over CoT",
            "performance_value": 14.2,
            "comparison_target_method": "CoT",
            "performance_difference": 14.2,
            "statistical_significance": null,
            "analysis_notes": "Reported to yield large accuracy gains (e.g., +14.2% on WebQSP over CoT) by leveraging graph-structured retrieval and explicit evidence chains; positions graph methods as enabling richer/diverse multi-hop reasoning versus sequential CoT.",
            "ablation_study_present": null,
            "uuid": "e6557.7",
            "source_info": {
                "paper_title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "KAG",
            "name_full": "KAG (Knowledge-Augmented Generation)",
            "brief_description": "A knowledge-augmented generation framework that tightly integrates structured knowledge (KGs) with generation to boost factual accuracy and reasoning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "KAG variants (unspecified)",
            "model_size": null,
            "reasoning_method_name": "Knowledge-Augmented Generation (KAG)",
            "reasoning_method_type": "retrieval-based / KG-enhanced",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "WikiMulti-HopQA",
            "task_description": "Multi-hop question answering requiring synthesis of KG and text evidence.",
            "performance_metric": "Exact Match (EM) improvement over RAG",
            "performance_value": 34.4,
            "comparison_target_method": "RAG",
            "performance_difference": 34.4,
            "statistical_significance": null,
            "analysis_notes": "Survey notes very large EM gains for KAG-style KG-enhanced approaches (e.g., +34.4% EM on WikiMulti-HopQA over RAG), attributing improvements to structured multi-hop inference and better entity linking; suggests graph-enhanced methods produce diverse reasoning trajectories that outperform plain retrieval.",
            "ablation_study_present": null,
            "uuid": "e6557.8",
            "source_info": {
                "paper_title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "KGAgent",
            "name_full": "KGAgent (KG-based autonomous agent)",
            "brief_description": "An autonomous agent framework that explores and reasons over knowledge graphs to perform complex KG-QA and retrieval tasks efficiently.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "KGAgent (framework)",
            "model_size": null,
            "reasoning_method_name": "KGAgent (autonomous KG exploration and reasoning)",
            "reasoning_method_type": "graph-based / agentic",
            "reasoning_style_diversity": "diverse / agentic",
            "benchmark_name": "WebQSP",
            "task_description": "Knowledge-base question answering and multi-hop retrieval over graphs.",
            "performance_metric": "Hits@1 improvement over GPT-4",
            "performance_value": 16.6,
            "comparison_target_method": "GPT-4",
            "performance_difference": 16.6,
            "statistical_significance": null,
            "analysis_notes": "Survey reports KGAgent producing substantial Hits@1 and F1 gains over GPT-4 (e.g., +16.6% Hits@1 on WebQSP), explained by KGAgent's focused graph traversal and structured inference enabling more precise, diversified reasoning paths than unguided LLM responses.",
            "ablation_study_present": null,
            "uuid": "e6557.9",
            "source_info": {
                "paper_title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "BloombergGPT",
            "name_full": "BloombergGPT (financial FinLLM)",
            "brief_description": "A 50B-parameter finance-focused LLM pre-trained on general and large financial corpora (FinPile) to improve downstream financial NLP tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "BloombergGPT",
            "model_size": "50B",
            "reasoning_method_name": "domain-specific pretraining / FinLLM adaptation",
            "reasoning_method_type": "domain-specialized",
            "reasoning_style_diversity": "single style (domain-specialized)",
            "benchmark_name": "ConvFinQA",
            "task_description": "Conversational financial question answering (financial reasoning).",
            "performance_metric": "Exact Match (EM)",
            "performance_value": 43.0,
            "comparison_target_method": "GPT-4 (zero-shot)",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Reported ConvFinQA EM = 43%; authors compare this to a general crowd baseline (47%) and GPT-4 zero-shot (6976%), noting that domain pretraining improved financial adaptation but still trails strongest general-purpose LLM reasoning (GPT-4) on this benchmark.",
            "ablation_study_present": null,
            "uuid": "e6557.10",
            "source_info": {
                "paper_title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "MetaRAG",
            "name_full": "MetaRAG (metacognitive RAG)",
            "brief_description": "A RAG variant that integrates metacognitive monitoring, evaluation, and planning to self-assess reasoning paths and iteratively adjust retrieval/inference for multi-hop QA.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "MetaRAG (framework)",
            "model_size": null,
            "reasoning_method_name": "MetaRAG (metacognitive retrieval-augmented generation)",
            "reasoning_method_type": "retrieval-based / metacognitive",
            "reasoning_style_diversity": "mixed / adaptive",
            "benchmark_name": "multi-hop QA (general)",
            "task_description": "Multi-hop question answering requiring iterative retrieval and self-assessment.",
            "performance_metric": "task-specific accuracy / F1",
            "performance_value": null,
            "comparison_target_method": "standard RAG",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Described as adding metacognitive capabilities (monitoring/evaluation/planning) to RAG to improve multi-hop QA by enabling the model to detect knowledge gaps and re-query or adjust reasoning paths, i.e., promoting adaptive (diverse) reasoning behavior.",
            "ablation_study_present": null,
            "uuid": "e6557.11",
            "source_info": {
                "paper_title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Buffer of thoughts: Thought-augmented reasoning with large language models",
            "rating": 2,
            "sanitized_title": "buffer_of_thoughts_thoughtaugmented_reasoning_with_large_language_models"
        },
        {
            "paper_title": "Strategic chain-of-thought: Guiding accurate reasoning in LLMs through strategy elicitation",
            "rating": 2,
            "sanitized_title": "strategic_chainofthought_guiding_accurate_reasoning_in_llms_through_strategy_elicitation"
        },
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Retrieval-augmented generation for large language models: A survey",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_large_language_models_a_survey"
        },
        {
            "paper_title": "Think-on-graph: Deep and responsible reasoning of large language model on knowledge graph",
            "rating": 2,
            "sanitized_title": "thinkongraph_deep_and_responsible_reasoning_of_large_language_model_on_knowledge_graph"
        },
        {
            "paper_title": "GraphRAG (local)",
            "rating": 1,
            "sanitized_title": "graphrag_local"
        }
    ],
    "cost": 0.02285,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Knowledge-Based Systems
12 April 2025</p>
<p>Wenli Yang yang.wenli@utas.edu.au 
University of Tasmania
Churchill Ave7005HobartTASAustralia</p>
<p>Lilian Some 
University of Tasmania
Churchill Ave7005HobartTASAustralia</p>
<p>Michael Bain 
University of New South Wales
High St2052SydneyNSWAustralia</p>
<p>Byeong Kang 
University of Tasmania
Churchill Ave7005HobartTASAustralia</p>
<p>Knowledge-Based Systems
12 April 202560C708921B5B50F15E2A502AE245101110.1016/j.knosys.2025.113503Received 19 January 2025; Received in revised form 1 April 2025; Accepted 3 April 2025 Knowledge-Based Systems 318 (2025) 113503LLMs Knowledge-based Knowledge integration RAG KG
The rapid development of artificial intelligence has led to marked progress in the field.One interesting direction for research is whether Large Language Models (LLMs) can be integrated with structured knowledge-based systems.This approach aims to combine the generative language understanding of LLMs and the precise knowledge representation systems by which they are integrated.This article surveys the relationship between LLMs and knowledge bases, looks at how they can be applied in practice, and discusses related technical, operational, and ethical challenges.Utilizing a comprehensive examination of the literature, the study both identifies important issues and assesses existing solutions.It demonstrates the merits of incorporating generative AI into structured knowledge-base systems concerning data contextualization, model accuracy, and utilization of knowledge resources.The findings give a full list of the current situation of research, point out the main gaps, and propose helpful paths to take.These insights contribute to advancing AI technologies and support their practical deployment across various sectors.</p>
<p>Introduction</p>
<p>The rapid development of Large Language Models (LLMs) has shaped the contours of AI.This has landed the models in areas where they can excel in generating natural text, never reaching any peak before.LLMs are based on deep learning structures, and can therefore perform effectively in many natural language processing tasks, such as text generation, sentiment analysis, or sophisticated dialogue systems.</p>
<p>Recent surveys have explored diverse aspects of LLMs, such as their architectures, training methodologies, and performance evaluation benchmarks.Many focus on specific topics, such as detailed analyses of state-of-the-art models [1], scaling laws innovations((i.e.what a new law brings to make old predictions still work)) [2], and pretraining methods used with large datasets [3].Others investigate domain-specific fine-tuning effects from reinforcement learning or human feedback [4,5], and transfer learning strategies [6].Despite these valuable contributions, however, there remains a paucity of comprehensive perspectives that connect the foundational principles of LLMs with practical applications and the challenges faced as these models are implemented in real-world situations.</p>
<p>This survey addresses this gap by presenting a comprehensive analysis of LLMs' foundational principles and their applications across diverse domains.Although LLMs have achieved remarkable progress,  Providing a comprehensive analysis of LLMs, focusing on their foundational principles, architectures, and real-world applications, while highlighting their capabilities, as well as technical, operational, and ethical limitations, and their transformative impact across diverse domains. Investigating the integration of LLMs with knowledge-based methods, analyzing their technical capabilities, hybrid integration strategies, and real-world case studies, while summarizing evaluation metrics used to assess the effectiveness of LLM-knowledge integration. Critically evaluating the challenges of LLM-knowledge integration and presenting actionable recommendations to enhance efficiency, interpretability, and scalability in future AI research and applications.</p>
<p>This survey paper begins with an overview of Large Language Models (LLMs), detailing their evolution, underlying architecture, and diverse real-world applications.It then explores the challenges associated with implementing these models, categorizing them into technical, operational, and ethical/social dimensions and discussing potential solutions.Building on this foundation, the Integrating LLMs with Knowledge Bases section examines integration techniques designed to enhance LLMs with structured knowledge, advanced hybrid approaches and prompt augmentation strategies.This section also provides practical insights through case studies, illustrating real-world applications and evaluation metrics for knowledge-integrated LLMs.Finally, the paper discusses the challenges of integration and offers recommendations for future development and implementation, providing actionable guidance to advance the field.Fig. 1 shows the overall structure of the paper organization.</p>
<p>Overview of LLMs</p>
<p>LLM evolution</p>
<p>Large Language Models (LLMs) are advanced AI systems capable of understanding and generating human-like text by leveraging deep learning architectures trained on vast amounts of textual data.Built primarily on the transformer architecture, LLMs power applications in machine translation, question-answering, dialogue systems, and generative AI [7][8][9].Fig. 2 visually represents the progression of LLMs over time, highlighting key milestones and advancements in AI language processing.The development of LLMs can be categorized into three major phases:</p>
<p>Early NLP Models (Pre-2010s): Early natural language processing (NLP) models relied on rule-based and statistical methods, focusing on predefined patterns and probability-based predictions.Statistical NLP techniques, such as n-grams, TF-IDF (Term Frequency-Inverse Document Frequency), and Nave Bayes, analyzed word frequency and co-occurrence patterns but lacked contextual understanding and semantic depth [10].As the field progressed, Neural NLP (NLMs) introduced word embeddings like Word2Vec and GloVe, which captured word relationships in continuous vector spaces, significantly enhancing representation learning [11].The development of Recurrent Neural Networks (RNNs), Long Short-Term Memory networks (LSTMs), and Gated Recurrent Units (GRUs) further improved sequential data modeling, allowing for better context retention.However, these models still struggled with long-range dependencies, limiting their effectiveness in processing complex linguistic structures [12].</p>
<p>Transformer Era (2017-2022): The introduction of scalable, selfattention-based models revolutionized natural language processing (NLP) by replacing Recurrent Neural Networks (RNNs) with the selfattention mechanism, enabling parallel computation and improved scalability [13].This breakthrough led to the development of Pre-Trained Language Models (PLMs), such as BERT, which introduced bidirectional encoding, and GPT, which leveraged auto-regressive generation, setting new benchmarks for NLP performance across various tasks [7].As LLMs scaled, GPT-3 demonstrated zero-shot and few-shot learning capabilities, showcasing the potential of large-scale models in handling diverse tasks with minimal fine-tuning.However, this rapid expansion also introduced challenges such as bias, factual inaccuracies, and high computational costs, necessitating further advancements in model efficiency and knowledge augmentation [14].</p>
<p>Next-Gen LLMs (2023-Present):Recent advancements in Large Language Models (LLMs) have been guided by four interrelated priorities: multimodal capabilities, computational efficiency, knowledge augmentation, and advanced reasoning abilities.These directions aim to improve the performance, usability, and reliability of LLMs in increasingly complex and diverse applications.First, in the area of multimodal capabilities, models such as GPT-4 and Google Gemini represent significant progress by incorporating multiple modalities -including text, images, and audio -into a unified framework.This integration enables richer contextual understanding and supports more advanced reasoning across domains involving heterogeneous data sources [15].Second, computational efficiency and accessibility have been key drivers in the development of open-source LLMs.DeepSeek LLM [16] exemplifies this trend, offering a scalable and transparent alternative to proprietary models.Trained on high-quality multilingual and code datasets, it demonstrates competitive performance across a range of NLP tasks.Within the DeepSeek ecosystem, DeepSeek-Coder [17] addresses the growing demand for efficient code generation, while DeepSeek-MoE [18] introduces a Mixture-of-Experts (MoE) architecture that improves inference efficiency by activating only a subset of model parameters during prediction.Similarly, LLaMA 3 (Meta AI) and Mistral 7B prioritize lightweight model design and training optimization, enabling faster deployment and wider accessibility for both academic and industrial applications [19].Third, growing interest in knowledge augmentation reflects the need to overcome LLM limitations in factual accuracy and domain-specific reasoning.Hybrid approaches such as Retrieval-Augmented Generation (RAG), Graph-Enhanced LLMs (e.g., GraphRAG, KAG) [20], and LLM-driven reasoning frameworks (e.g., ToG, KGAgent) [21] integrate structured knowledge sources, including knowledge graphs and document corpora, to ground model outputs and reduce hallucinations [15,22].Finally, advanced reasoning abilities have emerged as a distinct focus, emphasizing the development of models capable of logical inference, multi-step reasoning, and dynamic problem-solving.A representative example is OpenAI-o1 model [23], released in late 2024, which introduces a reasoningoptimized architecture.By dynamically allocating more computational resources to tasks of higher complexity, o1 improves performance in areas that demand deeper cognitive processing and task-aware inference.</p>
<p>LLM architecture</p>
<p>Modern LLMs are primarily based on the transformer architecture, which can be easily adapted for specific tasks with robust performance, leading to large gains on downstream tasks like text classification, language understanding, reference resolution, common-sense inference, summarization, and machine translation [7].Building upon the transformer, LLMs are typically designed using one of three main architectures: encoder, decoder and encoder-decoder [24,25].The Table 1 summarized these three main architectural design, highlighting their examples, primary use cases, and knowledge integration benefits.</p>
<p>Encoder LLMs: These models, such as BERT, RoBERTa, focus on text classification and retrieval tasks.They process input text bidirectionally, allowing them to generate high-quality embeddings that improve retrieval-augmented systems.By learning contextual relationships, encoder models enhance semantic search, information extraction, and retrieval-based NLP applications.</p>
<p>Decoder LLMs: Examples include GPT-4, LLaMA-3, DeepSeek LLM, and OpenAI o1, which are mainly designed for tasks such as text generation, dialogue systems, code synthesis, and creative writing.These models generate text auto-regressively, predicting each token based on prior context.DeepSeek LLM exemplifies efficient open-source performance across both understanding and generation tasks, while OpenAI o1 introduces a novel reasoning-optimized architecture that differentiates it from conventional decoder models.Instead of allocating a fixed amount of computational resources uniformly across all inputs, o1 dynamically adjusts its internal computation in response to task complexity.Although highly capable in generative tasks, decoder models often benefit from retrieval-augmented generation (RAG), which grounds outputs in external knowledge to reduce hallucinations and improve factual accuracy in knowledge-intensive domains.</p>
<p>Encoder-Decoder LLMs: Models like T5 and BART leverage both an encoder and a decoder to transform input text into output sequences, making them well-suited for machine translation, text summarization, and paraphrasing.Their dual-component structure allows them to integrate effectively with structured knowledge bases (KBs) and semantic reasoning systems, enhancing their ability to provide factually accurate and contextually relevant outputs.</p>
<p>Furthermore, specialized transformers such as Reformer, Longformer, and knowledge-enhanced LLMs address specific challenges in efficiency, sequence length processing, and factual grounding.Reformer reduces memory and computational costs by leveraging localitysensitive hashing and reversible layers, enabling efficient handling of long sequences [34].Similarly, Longformer optimizes attention mechanisms to improve context retention in lengthy documents, allowing tokens to focus on globally significant and locally relevant information [35].The evolution of LLMs has also seen a shift towards task-specific, decoder-only models for applications like text generation.Additionally, the rise of open-source models such as LLaMA and BLOOM has driven the democratization of LLM research, fostering transparency, reproducibility, and collaborative development [36].These open-access models reduce dependence on proprietary systems, support domain-specific adaptations, and promote innovation across academia and industry, ensuring broader accessibility and practical advancements in knowledge-augmented AI [14,37].</p>
<p>LLM capabilities and real-world applications</p>
<p>Large Language Models (LLMs) exhibit a diverse range of capabilities that enable their application across various domains.While their core functionality revolves around text generation, their broader abilities extend to language understanding, reasoning, decision-making, knowledge management, multimodal processing, and adaptability [26,38,39].Table 2 presents a structured summary of LLMs' key capabilities, categorized alongside real-world applications and supporting references.</p>
<p>LLMs excel in natural language processing (NLP) tasks such as translation, summarization, and text completion, generating contextually relevant responses [9,40].They also engage in complex reasoning</p>
<p>Machine translation, summarization</p>
<p>Works well with structured KBs and semantic reasoning [30][31][32][33] tasks, including question answering (QA) and logical problem-solving, making them valuable in decision support, scientific research, and automation [41].Furthermore, LLMs facilitate large-scale information retrieval, integrating knowledge from external sources like ERNIE and E-BERT to enhance their contextual awareness [42].</p>
<p>Beyond structured tasks, LLMs contribute to creative domains, generating storytelling, poetry, and other artistic content.Their adaptability is further demonstrated through fine-tuning for domain-specific applications such as financial forecasting, medical diagnosis, and legal analysis.Additionally, multimodal capabilities allow LLMs to process and generate content combining text, images, audio, and video, expanding their applicability to sectors such as entertainment, healthcare, and security [43,44].</p>
<p>Prompting techniques, including chain-of-thought reasoning, enhance LLMs' problem-solving abilities, while in-context learning enables them to adapt to new tasks with minimal supervision [45,46].Domain-specific models, such as Financial LLMs and Medical LLMs, illustrate the trend towards specialization, allowing for more accurate and efficient task execution in industry-specific scenarios [41].Furthermore, LLMs are evolving into autonomous agents capable of executing tasks based on natural language instructions, controlling software applications, and integrating with robotics [47].</p>
<p>Despite their vast potential, LLMs face challenges, particularly in specialized domains requiring high precision, such as medicine and law.Their reasoning capabilities, factual consistency, and computational resource demands remain significant hurdles [10,39].Nevertheless, LLMs continue to revolutionize industries, from policy-making and finance to healthcare and crisis management, transforming how knowledge is processed and applied.</p>
<p>One emerging approach to overcoming these challenges is the development of cross-domain LLMs.While there is no explicit definition of ''cross-domain LLMs'', discussions around mixed-domain training and the integration of external knowledge bases suggest that these models are designed to function effectively across multiple knowledge domains.Cross-domain LLMs exhibit key characteristics such as knowledge generalization, multitasking, transfer learning, fine-tuning, and scalability [76][77][78].Their inherent versatility allows them to be applied across various industries, including legal document analysis, customer service, and academic research.For instance, Financial LLMs (FinLLMs) leverage mixed-domain training on both general and financial corpora, incorporating prompt engineering and instruction finetuning to adapt models to financial tasks [59].Similarly, Medical LLMs (MedLLMs) apply LLM technology to healthcare, utilizing specialized models such as PharmacyGPT and Psy-LLM to enhance domain-specific performance [79].</p>
<p>Challenges in implementing LLMs for real-world scenarios</p>
<p>Implementing LLMs in real-world applications presents numerous challenges that span technical, operational, and ethical dimensions.Despite the extensive capabilities, LLMs experience limitations in the following areas: accuracy and reliability, explainability, reliance on data lacking updating mechanisms, and broader concerns such as information provenance, privacy, data security, and potential plagiarism.These challenges necessitate ongoing research and development to ensure the responsible and effective use of LLMs in real-world contexts.</p>
<p>Technical challenges</p>
<p>Technical challenges with LLMs include model interpretability, as their complex decision-making processes are difficult to understand, and the need for substantial computational resources, which can be costly and inefficient.</p>
<p>Model interpretability</p>
<p>Even though LLM models show remarkable performance across a range of tasks, it is still challenging to comprehend the logic underlying the text predictions or generation.Concerns arise from this lack of openness, particularly in important applications where justifiable decision-making is imminent as often users seek to understand how and why a model is returning specific outputs.Such transparency helps build or create confidence, ensures ethical application, or encourages further innovation on the platform [80].Other concerns are caused by model complexity explainability in decision making, bias [81], debugging and error identification [82], trust in high-stake use [15] and ethical concerns.More so, LLM models are referred to as 'black boxes' because they are intrinsically complex and difficult to decipher [10,14].The models frequently have millions to billions of parameters, resulting in a complex network of linked nodes that support generative functionality.Because of this complexity, it is difficult for humans to trace the specific model's inputs and their outputs [77].An example of such a problem is GPT-3, which has 175 billion parameters.LLMs are trained using deep learning methods, often called ''black-box'' models, which means that even experts cannot fully access or understand their internal mechanisms.This lack of transparency hinders efforts to comprehend how particular inputs lead to specific outcomes [14].LLM interpretability poses challenges to trust and adoption.Thus, in many real-world use cases, it would be advantageous for the stakeholders to understand and justify what the LLMs are doing, especially in areas that are strictly governed by laws, such as medicine, finance, and legal systems [83].It is equally challenging but important to ensure that LLMs are not perpetuating prejudices or rendering unjust or discriminatory outcomes if we do not comprehend the LLM decision-making process.</p>
<p>Managing computational resources</p>
<p>The development of LLMs remains overly complex and needs robust computational power for both training and inference operations.The associated computational costs in LLMs range from training cost, inference time, resource restraints in IDE, and token costs in conversationstyle APR, prompting length costs to optimization of multiple sub-tasks.Training big language models is an expensive undertaking that calls for a lot of computer power.For instance, 64 32-GB V100 GPUs were Ideation in creative arts, scientific innovation, entertainment.</p>
<p>[ [72][73][74][75] -Creative problem-solving: Generating solutions for ill-defined challenges.</p>
<p>utilized in training Microsoft's InferFix model [15].This underscores the necessity of significant hardware investments, which may be a challenge for investors and institutions with constrained budgets.Sources cite that energy consumption during training is a growing concern due to the carbon footprint being generated by the AI industry, raising environmental sustainability issues [84][85][86].Additionally, longer inference times can cause delays in response generation, even when larger LLM's hardware is well equipped [87].In real-time applications like chatbots, virtual assistants and interactive systems where prompt replies are essential, this could be troublesome.Moreover, LLMs require substantial storage and runtime memory, and integrating them into Integrated Development Environments (IDEs) can be difficult, posing challenges for code completion, bug-detection and code generation [88].Performance problems may arise from coding LLMs, particularly on devices with limited resources.This demand for high computational power can limit their use, especially for small organizations [89].In addition, the cost of generated tokens and input can add up in conversation-style Automated Program Repair (APR), where LLMs work together iteratively to produce fixes.This can result in considerable costs [15].</p>
<p>Operational challenges</p>
<p>Operational challenges are equally significant, particularly concerning the integration of LLMs with existing infrastructures and ensuring scalability and reliability; maintenance and updates are prevalent across domains.</p>
<p>Integration with existing systems</p>
<p>The LLM integration process is not simply about connecting software or systems; the process requires a multifaceted approach that considers numerous factors, including architectural compatibility, data flow management, and operational efficiency.LLMs are typically based on intricate deep learning architectures, which may not easily interface with systems that may use outdated technologies or have distinct architectural designs.For instance, integrating with legacy systems that do not have access to contemporary APIs or that use different data formats can be quite difficult [90].</p>
<p>Concurrently, during the integration process, data flow management between LLMs and systems is vital for data consistency [91][92][93].Research reiterates the significance of improving data access and suggests time-and-space efficient methods for LLMs as it impacts cost, latency, and hardware strain because integration processes require a large amount of processing power [41,92].Nevertheless, it may not always be possible to effectively optimize operational LLMs for specific scenarios since it demands a significant amount of computational resources.Therefore, it is essential to lessen the pre-training load and increase retrieval efficiency to improve operational efficiency in production-ready systems [94].</p>
<p>Scalability and reliability</p>
<p>Scalability refers to an LLM system's capacity to handle increasing workloads and data volumes while maintaining performance, whereas reliability concerns the system's ability to consistently produce accurate and trustworthy outputs, regardless of the operating environment or task complexity [95].Research investigates computational demands, data complexity sensibility to model fine-tuning and adaptability as the major concerns for scaling LLMs.Scalability is a salient feature exhibited by LLMs that enables them to effectively manage workload increase, which becomes challenging because of the enormous amount of processing power needed for both training and inference.Maintaining performance while scaling becomes more difficult as the model size and data volume increase.This frequently leads to increased expenses, resource usage, and possible processing bottlenecks [96,97].For example, to ensure that jobs are equitably dispersed over various servers and that network latencies are kept to a minimum when growing LLMs across distributed infrastructures, complex orchestration is needed.The difficulty of guaranteeing reliability -the system's capacity to operate consistently under a range of loads and conditions -also confronts large-scale deployments.Due to LLM's intricate structures, reliance on knowledge bases, and hardware constraints, LLMs are prone to malfunctions [7].LLM systems must be created to function at various levels of throughput and be optimal in any operating environment.This is particularly critical in industries where availability and uptime are critical, such as the computer, automation, manufacturing, navigation, and software industries [89].Striking a balance between data inputs involves multiple trial-and-error attempts, making the finetuning process difficult.This unpredictable nature of model training underscores the need for flexible LLM infrastructure and subsequently the computing power [98].Multi-bucketing and continuous batching techniques optimize the computational efficiency of LLMs by selecting appropriate buckets for data input and enhance multi-text request processing thus, using computational resources efficiently.These problems are mitigated by constructing a perspective and robust infrastructure, incorporating redundancy, continuous management, and iterative improvement of the system.Robust error handling, redundancy methods, and system monitoring are strategies that provide continuous availability and minimize downtime in high-demand systems.Additionally, upholding scalability and reliability calls for eliminating failure points, and preventing service degradation to support LLM functions in a production environment.</p>
<p>Maintenance and updates challenges</p>
<p>Large language models (LLMs) present maintenance and updating difficulties that must be resolved to maintain system stability and dependability.To evolve and address ongoing LLM problems, models undergo continual updates.However, ensuring compatibility with new versions, managing model updates without disrupting existing workflows, and addressing security vulnerabilities require robust maintenance procedures [93].Robust maintenance procedures are necessary to provide compatibility with new versions, manage upgrades without interfering with existing operations, and address security risks as models undergo regular modifications to adapt and handle ongoing LLM problems [83,93,99].Managing numerous versions of LLMs requires efficient version control and rollback methods that enable prompt recovery in the event of malfunctions by rolling back to earlier versions.Furthermore, to avoid unanticipated problems or performance degradation and to make sure that any changes enhance the system without creating new problems, comprehensive testing and validation of updates before deployment are essential.Both the ongoing system performance monitoring and the gathering of user input preceding LLM updates are crucial because they reveal hidden issues and potential areas for development and allow for prompt adjustments to increase reliability.Adaptability is an essential feature that new LLM versions must incorporate for LLMs to stay useful and efficient in a quickly changing technical environment [29].Finally, to prevent creating vulnerabilities or jeopardizing sensitive data during updates/upgrades, security and privacy issues must be properly mitigated.Strong security protocols are essential to safeguard information security and maintain user trust during the update process.</p>
<p>Ethical and social implications</p>
<p>LLMs are plagued by multiple ethical and social problems related to bias perpetuation, misinformation, the need for transparency in AI-driven decision-making, and potential impacts on employment.A primary concern is whether the algorithm operates fairly and responsibly, given the general risk that the technology can be misused [80,83].Additionally, the credibility of sources daunts the LLMs family, as the models do not trace the provenance of the content they generate, creating uncertainty around source reliability [100].</p>
<p>The reliance on proprietary datasets such as those used in OpenAI poses risks to privacy and data security.In some instances, LLMs may produce data similar to those from training data without attribution, causing ripples in creative and academic spheres as it raises concerns about plagiarism and intellectual property [29,101].Financial LLMs, for example, face unique challenges in ensuring data privacy while utilizing proprietary data without breaches [12].Moreover, the automation of mundane tasks by LLMs could impact employment, displacing roles traditionally performed by humans.</p>
<p>Addressing these social issues calls for the development of new ethical standards within the generative AI sphere and the implementation of measures to reduce bias and embrace transparency in Gen AI operations [102].Techniques like RAG are increasingly adopted in financial domains to address these privacy and trust challenges.By tackling these ethical and social issues, stakeholders can work to ensure the responsible use of LLMs [101].</p>
<p>In conclusion, LLMs are capable of extending new opportunities for innovative development, but it is necessary to resolve the associated challenges.In this way, focusing on the improvement of the technical characteristics of the LLM systems and the operational issues -such as interpretability, efficiency, integration, and scalability, ensures that the LLM technologies are applied appropriately and responsibly [89].Emergent and evolving challenges and broader concerns necessitate adaptive strategies involving regulatory frameworks, ethical guidelines, and technical solutions like differential privacy and federated learning [38].Such approaches assist in developing trustworthy and accurate LLM systems that will be useful in generating reliable outcomes in various applications [103].</p>
<p>Solutions to address LLM challenges</p>
<p>As large language models (LLMs) continue to face significant challenges in real-world applications, addressing these issues is essential for ensuring their responsible and effective deployment.Key challenges such as interpretability, computational resource demands, and data quality require targeted solutions.Enhanced data training methods are crucial to enhance the accuracy and relevance of LLM outputs, by ensuring these models are trained on diverse, high-quality datasets.To tackle the challenge of computational inefficiency, the development of efficient algorithms and advanced hardware is necessary, enabling LLMs to operate at scale without excessive resource consumption.Explainable AI (XAI) techniques will address concerns about model interpretability, helping users understand the reasoning behind LLM outputs and increasing trust in these systems.Additionally, addressing privacy, data security, and ethical concerns requires the integration of regulatory frameworks, ethical guidelines, and technical solutions like differential privacy and federated learning.By focusing on these solutions, the challenges surrounding LLMs can be mitigated, ensuring that these technologies can generate reliable, ethical, and efficient outcomes across diverse sectors.</p>
<p>Enhanced data training techniques</p>
<p>Research stresses that high-quality training data is the fundamental component of robust and effective LLMs because extensive text datasets teach LLM patterns, trends, and other insights.However, incomplete, biased, or out-of-date data will be passed over and retained thus compromising the resulting LLM [90,104].Domain-specific fine-tuning improves LLMs by training them on specialized datasets catered to specific industries, such as banking, biomedicine, and medical education [12,105,106].In finance, fine-tuning can be done with news articles, market data, and company financials to enhance their ability to perform financial forecasting, risk analysis and investment analysis [12].This increases LLM proficiency with domain-specific tasks by acquainting them with pertinent terminology and information.Data cleaning and filtering address problems with noisy or biased data, guaranteeing high-quality inputs that prevent models from learning undesirable patterns.Likewise, data augmentation creates new data to expand training datasets when available data is scarce, improving the model's robustness [7].</p>
<p>Integrating several data sources, including text, code, and transcripts of conversations, enhances an LLM's comprehension across different knowledge areas [87,107].For example, an LLM integrated with data from GitHub, text from Wikipedia and chats from chatbots can perform a wide range of tasks including code generation, text summary and dialogue.However, more research is needed to refine methods for integrating these sources effectively.</p>
<p>Scalability of data curation remains challenging, requiring strong infrastructure and efficient data handling to support storage, indexing, updating, and retrieval of large datasets [95].As such, robust infrastructure is needed to handle the storage, processing, and analysis of vast data, while efficient handling of data is a prerequisite for indexing, updating and retrieval of heterogeneous data.Given that quality datasets are necessary for the best possible LLM performance, measuring and ensuring data quality is also critical.To fully utilize the promise of these models, research and innovation in scalable data curation and data training are required, a continual action that is critical for enhancing the accuracy, reliability, and fairness of LLMs.</p>
<p>Optimizing algorithms and hardware for efficiency</p>
<p>To overcome computational cost challenges, optimized algorithms and hardware should be implemented for improved performance and efficiency.</p>
<p>Research highlights large computational demands placed on Large Language Models (LLMs) during deployment and training, and suggests ways to increase effectiveness.By selectively activating specific parts of the network to optimize resources, techniques such as mixture-of-experts explained (MoE) models can improve performance while decreasing complexity.Distributed computing Frameworks can be adopted for distributing the processing load among several machines and accelerating training [93,108].More so, deep learning operations have been accelerated by specialized hardware, such as GPUs, and TPUs and further innovation on the hardware will keep computing costs down.Originally intended for rendering graphics in video games and visual effects industries, their parallel processing efficiently, has made GPUs extremely effective in deep learning applications.On the other hand, TPUs which are tailored specifically for deep learning applications, offer an even greater level of computational power and efficiency than GPUs, providing a significant advantage in performance for such tasks [12,36].Additionally, model compression techniques-such as knowledge distillation and model pruning-enhance deployability on devices with limited resources without compromising performance [95].Pruning eliminates crucial connections or neurons within the model, while quantization, reduces the precision of numerical values, and knowledge distillation, to replicate the performance of a larger ''teacher'' model in a smaller 'student' model [29].Similarly, Parameter-Efficient Finetuning (PEFT) like LOoRA reduce processing needs by focusing on fine-tuning a limited set of parameters [14], thus reducing computational overload.This strategy proves beneficial in financial models to improve effectiveness and flexibility for financial functions where tasks necessitate minor adjustments to specific model parameters rather than a complete overhaul [109].PEFT can boost the efficiency and adaptability of retrieval models in recommendation systems that cater to changing user behaviors [29] Using distributed computing systems improves productivity by facilitating model training and inference on numerous devices simultaneously.Expanding the workload among multiple sources can greatly diminish the time needed for training and inference processes.This enables the creation and implementation of extensive and intricate models.Examples of frameworks leveraging distributed systems include DenseX, EAR, UPRISE, RAST, Self-MEM, FLARE, Filter-rerank, R-GQA, LLM-R, LM-Indexer, and BEQUE, each optimizing efficiency in large-scale deep learning [77,107].Finally, developing expansive hardware infrastructure, cloud technology offers scalable, on-demand computing capabilities that enable LLM training and implementation at a reasonable cost.</p>
<p>Explainable AI (XAI) for transparency and trust</p>
<p>The key obstacles to making LLM explicable include limited context window, implicit bias, lack of transparency, model complexity and scale, data complexity, and assessment complexity.Despite this, there is considerable research interest in this area to unravel LLMs explainability.</p>
<p>Models have a restricted limited context windows, as a result, LLM limits the input and output of text LLM generates.This constraint hinders the explainability of the process, particularly in summarization tasks, where LLMS struggle due to limited text access [26,87].Implicit biases from training data can also surface in outputs, making it hard to trace why a model generates biased responses.The bias can be subtle and hard to decipher.For example, gender bias can be exhibited when a nuanced news article is used as part of a training dataset.The internal logic of LLMs is also often opaque, with their complex neural networks hindering transparency and trust in their decisions.As models grow larger, with billions of parameters, explaining their behavior becomes increasingly challenging.The diversity and scale of training data make it hard to identify specific influences on outputs, and the lack of standardized metrics complicates efforts to assess and compare explainability across different LLMs [14].</p>
<p>There are several methods or approaches to improve LLM explainability.Engineering prompts elicit more explicable responses from LLMs through the use of strategies such as interaction and iteration, formatting with an example, explicit instruction, system-specific instruction, and control tokens.An illustration of this might be an explainability prompt like ''Explain the concept of LLM as if I am 10 years old'' [104,110].Model interpretation tools like benchmark tests and tools like RGB, RECALL, and CRUD [110,111] and reference tools like model agents like AutoGPT, Toolformer, and Graph-Toolformer where LLMs employ active judgment in their operations can be used to gain insight into the decision-making process of LLMs [104,105,110,112].Based on the particular requirements of each task, meta-reasoning prompting (MRP) assists LLMs in dynamically choosing and utilizing various reasoning strategies, potentially increasing the transparency of their reasoning process [107].</p>
<p>Knowledge-Based Systems 318 (2025) 113503</p>
<p>Integrating LLMs with knowledge bases</p>
<p>This section explores various techniques for integrating Large Language Models (LLMs) with knowledge bases, highlighting the potential solutions these integrations offer.By examining the methods used to combine LLMs with structured knowledge, we discuss how this synergy enhances the models' ability to provide more accurate, contextually relevant, and data-driven outputs.The integration strategies also open doors to improved reasoning capabilities, allowing LLMs to access and leverage external knowledge to address domain-specific queries and tasks more effective.</p>
<p>Integration techniques for enhancing LLMs with structured knowledge</p>
<p>Techniques for integrating LLMs with knowledge bases aim to enhance the models' performance by grounding their outputs in structured information.Key methods include using knowledge bases (KBs), which provide structured data for LLMs to query, and knowledge graphs, which organize information in interconnected nodes and relationships to improve context understanding.Additionally, Retrieval-Augmented Generation (RAG) combines document retrieval and generation, enabling LLMs to access relevant data from external sources and generate more accurate, up-to-date responses.These integration techniques enhance LLMs' ability to produce reliable, context-aware outputs.</p>
<p>Basic knowledge bases</p>
<p>Basic Knowledge Bases (KBs) are structured repositories that store facts in a relational format, providing an organized way to query and retrieve predefined facts, relationships, rules, and context, offering high precision when factual data is required.</p>
<p>In their inherent nature, Knowledge Bases help improve factual accuracy as they store verified information.Integrating LLM with the Knowledge base enables the models to cross-reference outputs with factual data stored in KBs thereby reducing hallucination and improving reliability.LLMs that access Wikidata, an open data knowledge base storing structured data on various domains (about people, places, and events), can generate factual outputs such as birthdate or scientific facts [87].Studies show that Wikidata integration with Knowledge Enhanced PLMs promotes entity-aware training, through entity linking to Wikipedia, and joint training based on WiKidata's Knowledge Graph [87].Several notable models, including K-BERT, KgPLM, FaE, JAKET, LUKE, WKLM, and CoLAKE, utilize Wikidata as a knowledge source in various ways including knowledge injection, knowledgeguided attention, and Knowledge Graph embedding, which improves factual accuracy, reasoning and enhanced interpretability and explainability [87].Specialized knowledge bases like UMLS (Unified Medical Language System) for medical terminology or Freebase for general knowledge, provide LLMs with domain-specific data, enhancing their performance on tasks requiring specialized expertise [87].For instance, LLMs integrated with the UMLS can retrieve up-to-date information on drug interactions or disease symptoms, which would significantly improve the model's ability to provide medical diagnoses or treatment recommendations [113].However, most KBs are static and riddled with limited reasoning capacities, often requiring manual upgrades.This makes them unsuitable for real-time or fast-changing information and limits their utility in fields such as current events or rapidly evolving research domains.While KBs store factual data, they do not inherently capture complex relationships or provide the inference capabilities of more advanced systems like Knowledge Graphs.For applications requiring real-time updates or complex inference, knowledge bases alone are insufficient, and more advanced systems are needed.</p>
<p>Knowledge graphs</p>
<p>Knowledge Graphs (KGs) provide a structured approach to representing knowledge, enabling LLMs to overcome inherent limitations in understanding and reasoning about the connections between realworld entities and concepts.This integration is valuable in complex reasoning tasks like multi-hop question answering and semantic disambiguation where connection between concepts is valuable.Additionally, the KGs allow LLMs access to up-to-date knowledge without the need for retraining, making them a powerful tool to keep LLMs relevant and current [114].</p>
<p>Knowledge Graphs (KGs) are integrated into Large Language Models (LLMs) using several advanced methods designed to enhance their reasoning capabilities: embedding, Graph Neural Networks, prompting and semi-structured chain of thought.These techniques vary in approaches to processing and utilizing structured knowledge.</p>
<p>Knowledge Graph Embedding transforms entities and relationships into numerical vectors for efficient processing, while Graph Neural Networks capture complex relationships within the KG, enhancing multi-hop reasoning abilities [115].Knowledge Graph prompting injects relevant KG data directly into the LLM's input, guiding reasoning and improving accuracy [8].Semi-structured chain of thought combines both structured KG data and unstructured text to generate reasoning chains, leveraging all available knowledge for more complex reasoning tasks [94,104,115].Each method provides a distinct way to enhance the LLM's reasoning and response capabilities.Key techniques for integration include KG-enhanced LLMs, LLM-augmented KGs and synergized LLMs and KGs.In KG-Enhanced LLMs, KGs are used to augment the training or inference process of LLMs, supplying them with structured information to support reasoning and understanding.Whereas in LLM-augmented KGs, LLMs assist in various KG-related tasks, such as embedding, completion, construction, and question answering [116].Lastly, in synergized LLMs and KGs, the unified framework combines the strengths of both KGs and LLMs, creating a model that leverages KGs for structured knowledge and LLMs for language understanding [117].</p>
<p>Notable advantages of Knowledge Graphs over knowledge bases include increased reasoning, inference, and improved interpretability.KGs store information in a network of nodes (entities)and edges (relationships) and emphasize relationships between entities, making them more flexible and dynamic than knowledge bases with unstructured text.Unlike KBs, which store isolated facts, integrated LLMs can reason about complex relationships and infer new information based on existing graphs.As such Knowledge Graphs such as Google Knowledge Graph, ConceptNet and DBPedia, LLMs can answer questions by traversing relationships between entities.Specifically, ConceptNet supports common-sense reasoning, DBPedia provides structured data for general knowledge queries, and Google Knowledge Graph aids in answering factual questions by linking related entities [87,104].</p>
<p>Research demonstrates several ways in which Knowledge Graphs (KGs) can be used to enhance the capabilities of large language models (LLMs).KGs naturally support explainability by visualizing the graph of entities and relationships.KGs can enhance explainability by showing how the model arrived at a specific conclusion.A KG can map the relationships between legal precedents, statutes, and case law, providing an explainable pathway that LLMs follow to generate legal opinions [7,10,114].</p>
<p>KGs can be updated with new information, making it ideal for realtime applications (like stock exchanges, active medical diagnosis, and navigation) where information must be frequently refreshed.Applicable frameworks like MedGraph, WeKnow-RAG and Think on Graph enable LLMs can retrieve real-time knowledge about evolving topics, such as the latest news developments or ongoing live events [76,81,83,109].</p>
<p>KGs offer a structured and interconnected way to represent knowledge, which is beneficial for LLMs in understanding relationships and making inferences.Unlike relational databases or NoSQL solutions, KGs Knowledge-Based Systems 318 (2025) 113503 use graph structure to explicitly link information, making it easier for LLMs to understand relationships and make inferences [36,104,115].This interconnected structure facilitates the use of specialized query languages (like SPARQL and Cypher) that are well-suited for navigating complex relationships between entities [92,114].</p>
<p>KGs enhance factual accuracy and knowledge probing by providing LLMs with a factual grounding that might be missing or weakly represented in their training data.This is especially helpful for tasks such as answering questions, where providing accurate information is crucial [85].For example, the LAMA (Language Model Analysis) dataset is designed to test how well LLMs have internalized factual knowledge and by incorporating knowledge from KGs, LLMs can perform better on such knowledge probing tasks [85].KGs that encode commonsense knowledge, like ConceptNet and ATOMIC, are invaluable for LLMs, they boost common-sense reasoning and natural language generation [85,87].</p>
<p>While KGs offer significant advantages, the tradeoffs include incompleteness, complexity, scalability, and performance.KGs are often plagued with missing links and relationships between entities.LLMs can be trained to predict these missing links, effectively completing and refining the KG, thus benefiting both LLMs, by providing them with a more complete knowledge base, and KGs, by improving their accuracy and coverage [104].Creating and maintaining large-scale KGs is a resource-intensive process that requires careful design and data curation [95].Scaling across broad domains or diverse industries can be difficult and costly.Although KGs improve reasoning capabilities, integration with LLMs may introduce latency and reduce overall system performance.Hence, though Knowledge Graphs may offer a more advanced solution than KBs by providing a dynamic, relationshipfocused structure that enhances reasoning and interpretability, the complexity and scalability challenges make KGs less feasible for broad, real-time applications unless coupled with significant computational resources.Therefore, KG is best suited for high-stakes domain-specific tasks where reasoning and relationships between entities are critical, such as pharmaceutical drug discovery and research, Failure Mode and Effects Analysis (FMEA), financial fraud detection, and support threat intelligence in cyber-security [114,115,118].</p>
<p>Retrieval augmented generation</p>
<p>Retrieval-augmented generation (RAG) is a hybrid approach that integrates LLMs with a retrieval mechanism to fetch relevant information from external documents or knowledge in real-time during the text generation process.This technique allows LLMs to dynamically access vast external corpora (like knowledge graphs, databases or search engines) to retrieve the most relevant information [41,90,105,107].RAG enables language models to retrieve factual information and generate more accurate and contextually aware outputs, especially in cases where a language model's training data may be outdated or incomplete [99].This approach provides the LLM with an ''external memory'' to supplement its internal knowledge base, thus enhancing LLM quality and accuracy.</p>
<p>RAG comprise of retrievers, generators, and knowledge bases [41].The retriever dynamically fetches relevant information from external corpora, the generator uses this retrieved information to generate a response, and the knowledge base is a collection of text such as scientific articles, news articles, or Knowledge Graphs [95,119,120].This architecture allows the LLM to access up-to-date knowledge beyond its static training data, alleviating hallucination issues by grounding the generated response in factual data.</p>
<p>RAG enhances retrieval and generation through several key techniques which may include document chunking, embedding models, retrieval techniques, querying, knowledge graph integration, iterative retrieval and generation and self-reflection.Chunking breaks down large text into manageable sizes using a mix of static and semantic methods to maintain context [119,121].Helps embedding models to represent information and queries in a way that maintains semantic meaning, improving retrieval accuracy [25].RAG employs retrieval methods such as dense retrieval, which uses vector representations (cosine similarity) for semantic matching, and sparse encoding for keyword matches between queries and documents; with hybrid approaches combining the strengths of both [95,119].Dense Passage Retrieval (DPR) technique utilizes dense embeddings to match semantically relevant document chunks to queries and the combination of DPR with traditional sparse retrieval (i.e., BM25) has been shown to further enhance retrieval precision in complex or high-precision tasks [82,109].</p>
<p>Query expansion using techniques like Query2doc involves expanding the original query to include additional terms, increasing the effectiveness of retrieval [122].Furthermore, Knowledge Graphs can be integrated into RAG systems for structured reasoning, allowing more accurate and meaningful results [76,114].Iterative retrieval and generation involve repeating these processes to refine outputs, guided by self-evaluation mechanisms that assess the adequacy of retrieved information [25,123].Self-reflection features in advanced systems like Self-RAG allow the model to assess the relevance and accuracy of the information it retrieves and generates, improving overall output quality and contributing to better explainability [105].</p>
<p>Retrieval-augmented generation (RAG) excels in complex, knowledge-driven tasks by linking large language models (LLMs) with real-time or domain-specific retrieval, significantly enhancing the scope and accuracy of AI applications.RAG capabilities are experienced in varied settings.In cross-domain applicability, RAG is used for question answering, dialogue generation, summarization, fact-checking or verification, information extraction, and reasoning [25,36,78,82,114].</p>
<p>In question-answering (QA) systems, Naive RAG improves response accuracy, especially for multi-hop queries or long-form answers where standalone LLMs might lack context.This is crucial for fact-checking, where retrieving authoritative sources ensures the reliability of generated content [35,120].RAG's role in dialogue systems is equally transformative, as it enriches real-time responses, especially in taskoriented conversations like customer support [124].Modular RAG capabilities such as text summarization are invaluable for condensing large documents into coherent, concise summaries, boosting content generation efficiency [125].Additionally, RAG's ability to retrieve and integrate specialized knowledge makes it indispensable in fields such as law and medicine, where precision is critical [126].</p>
<p>Furthermore, RAG strengthens recommendation systems by providing personalized suggestions based on user data, and in code search and generation, it supports developers by retrieving relevant code snippets to address complex technical challenges.Overall, RAG's versatility across diverse applications highlights its vital role in enhancing LLMs' performance, making them more accurate and contextually aware in real-world scenarios.</p>
<p>Overall, RAG systems are ideal for real-time data retrieval of unstructured data in environments with dynamic data volumes.For example, OpenAI RAG offers LLM the ability to query data and retrieve and utilize vast external data in its response, which improves the relevance and factual accuracy of generated text and real-time knowledge access [120].Therefore, a knowledge graph is best suited for high-stakes, domain-specific tasks where reasoning and relationships between entities are critical.RAG renders domain flexibility through retrieving domain-specific documents from external sources making it easier for LLMs to answer specialized queries without needing extensive domain-specific training.Table 3 summarized techniques for integrating Large Language Models (LLMs) with knowledge bases (KBs), Knowledge Graphs (KGs), and Retrieval-Augmented Generation (RAG).</p>
<p>Advanced techniques and hybrid approaches</p>
<p>The integration of Retrieval-Augmented Generation (RAG) with large language models (LLMs) has significantly improved their ability to handle complex tasks by incorporating external knowledge sources.</p>
<p>Knowledge retrieval and integration</p>
<p>At this level, the primary focus is on equipping LLMs with the ability to efficiently retrieve and integrate structured, unstructured, and multi-modal knowledge sources.This foundational stage ensures that LLMs can access relevant, up-to-date, and contextually meaningful information while reducing hallucination risks.</p>
<p>The first category, ''Advanced Knowledge Acquisition Strategies'', emphasizes developing techniques that allow LLMs to search, filter, and extract information from massive datasets, ensuring high-quality knowledge retrieval.By incorporating retrieval-augmented generation (RAG) techniques and context-aware indexing, LLMs can perform enterprise search, biomedical knowledge retrieval, and open-domain question answering (QA) more effectively.Models such as T-RAG [127] and REALM [82] demonstrate how intelligent retrieval can enhance LLMs' ability to provide reliable, up-to-date responses.</p>
<p>The second category, ''Structured Knowledge Graph Integration'', further refines retrieval by leveraging knowledge graphs (KGs) and hierarchical structures to organize and enhance reasoning efficiency.By reducing retrieval noise and improving factual consistency, this approach enables LLMs to support domain-specific QA and more structured information synthesis.Key models such as TRACE [123], GraphRAG [128] exemplify how structured knowledge enhances explainability and contextual reasoning.TRACE improves knowledge tracing and retrieval alignment by dynamically filtering and structuring relevant information, reducing retrieval noise, and ensuring contextually appropriate knowledge utilization.GraphRAG extends traditional retrieval-augmented generation (RAG) approaches by leveraging graphbased knowledge representations, enabling multi-hop inference, entity disambiguation, and hierarchical knowledge synthesis, which are critical for high-stakes applications such as biomedical QA and legal AI.</p>
<p>Knowledge utilization and reasoning</p>
<p>Building upon retrieval capabilities, Level 2 shifts the focus from knowledge access to knowledge reasoning, inference, and multi-hop processing.LLMs at this stage are expected to go beyond simply retrieving facts and instead synthesize, analyze, and logically structure information for complex decision-making.</p>
<p>The first category, ''Contextual Knowledge Reasoning and Inference'', enhances LLMs' ability to process and connect multi-hop information across diverse data sources.This allows for advanced reasoning tasks such as multi-hop QA, legal AI, and decision support systems.LLMs must reduce logical inconsistencies and enhance interpretability to support users in making informed decisions.For example, FlexKBQA [129] utilizes KG embeddings to improve entity linking and retrieval efficiency.ToG [130] enables LLMs to perform multihop reasoning over knowledge graphs, improving structured inference and knowledge grounding.Additionally, models such as ReAct [131], and Think-on-Graph 2.0 [132], Chain-of-knowledge [133] focus on structured inference and enhanced logical synthesis, further strengthening LLMs' ability to generate well-reasoned and contextually accurate responses.</p>
<p>The second category, ''Adaptive Knowledge Infusion for Decision-Making'', ensures that LLMs can dynamically ground their reasoning in real-world data, selectively incorporating relevant information based on the task's needs.This is particularly critical for interactive AI agents and high-stakes applications such as medical decision-making, financial analysis, and crisis response systems.A notable example is MetaRAG, which integrates retrieval-augmented generation with metacognitive capabilities to enhance multi-hop question-answering tasks.It applies metacognitive principles -monitoring, evaluating, and planningenabling LLMs to self-assess their reasoning paths and dynamically adjust based on identified errors or knowledge gaps [41].</p>
<p>Optimization and specialization</p>
<p>At the highest level of integration, LLMs are optimized for long-term performance, self-improving retrieval strategies, and domain-specific knowledge refinement.This stage ensures that models are highly precise, reliable, and adaptable for critical industries such as healthcare, legal compliance, and scientific research.</p>
<p>The first category, ''Self-Optimizing Knowledge Systems'', enables LLMs to continuously learn and adapt through reinforcement learning, retrieval fine-tuning, and real-time system updates.By self-improving over time, these models enhance their reasoning capabilities while minimizing errors in high-stakes applications.Key techniques such as CRAG [36] and WeKnow-RAG [134] employ adaptive retrieval enhancement and self-supervised fine-tuning, ensuring that LLMs dynamically refine their knowledge-grounding processes.Similarly, KGAgent [135] enables LLMs to autonomously explore, retrieve, and infer structured knowledge from knowledge graphs, providing real-time reasoning adjustments and optimized knowledge integration.Additionally, Coarse-to-Fine Highlighting [136]  Self-BioRAG, MedGraphRAG, SAC-KG hierarchical knowledge filtering, allowing AI models to prioritize and contextualize retrieved information more effectively.</p>
<p>The second category, ''Domain-Specific Knowledge Customization'', focuses on fine-tuning LLMs for specialized applications that demand precision, compliance, and factual reliability.This is particularly crucial in healthcare, legal AI, and scientific research, where incorrect or imprecise knowledge can lead to significant consequences.Traditional fine-tuning approaches rely on static knowledge integration, but recent advancements in Knowledge-Augmented Generation (KAG) [137] have introduced adaptive knowledge grounding, enabling LLMs to dynamically incorporate, verify, and refine domain-specific knowledge during inference.Models such as Self-BioRAG [105], MedGraphRAG [121], and SAC-KG [138] exemplify how domain-specialized expertise can be embedded and dynamically retrieved within LLMs.Self-BioRAG improves biomedical knowledge synthesis by integrating structured retrieval mechanisms with adaptive fine-tuning, ensuring high accuracy in medical decision-making.MedGraphRAG enhances medical AI applications by leveraging graph-based knowledge augmentation, allowing LLMs to navigate complex, multi-relational medical datasets for improved clinical reasoning and diagnostic accuracy.Meanwhile, SAC-KG optimizes legal and regulatory compliance AI, ensuring that retrieved knowledge aligns with jurisdictional regulations, ethical guidelines, and factual accuracy requirements.</p>
<p>Leveraging on prompt augmentation and engineering for LLM-knowledge base integration</p>
<p>This section introduces foundational prompt engineering techniques designed to enhance the retrieval and generation capabilities of large language models (LLMs).By strategically adapting prompts, these techniques guide models to produce more accurate, contextually relevant, and domain-specific outputs.This foundational approach sets the stage for the subsequent exploration of advanced integration methods, focusing on optimizing the interaction between LLMs and knowledge bases for improved performance across various applications.</p>
<p>Chain-of-thought (CoT) prompting has emerged as a powerful technique for guiding LLMs in complex reasoning tasks by encouraging Knowledge-Based Systems 318 (2025) 113503 them to embrace step-by-step reasoning pathways, before formulating a response.CoT exhibits human-like patterns in solving multi-step problems such as arithmetic problems, commonsense reasoning, and symbolic tasks.[83,90,107,110].</p>
<p>Building upon CoT, several enhancements, including Buffer of Thoughts (BoT), Strategic Chain-of-Thought (SCoT), and Graph of Thought have been developed.BoT introduces a meta-buffer that stores high-level thought templates distilled from previous problem-solving processes.These templates can be retrieved and instantiated for new tasks, enabling more efficient and accurate reasoning.BoT outperforms standard multi-query prompting methods by achieving superior performance in tasks requiring multi-step reasoning, such as mathematical problem-solving and logical reasoning, while reducing computational costs.The use of a meta-buffer allows for the creation of generalizable and reusable reasoning structures, optimizing both reasoning accuracy and efficiency.This approach mirrors human cognitive processes of using ''mental templates'' or heuristics to solve problems [112].Strategic Chain-of-Thought (SCoT) enhances CoT by incorporating strategy elicitation to guide reasoning processes.It employs a two-stage process within a single prompt, enhancing the stability and consistency of reasoning paths [122].SCoT outperforms existing methods like Selfconsistency and Buffer of Thoughts by using strategic knowledge to generate accurate single-query reasoning paths, demonstrating the importance of strategic planning in LLM reasoning.Both BoT and SCoT leverage cognitive-inspired methods to refine reasoning paths, demonstrating a trend towards incorporating human-like cognitive processes, such as strategy selection and problem decomposition, into LLMs.</p>
<p>Graph-of-Thoughts (GoT) technique extends CoT by representing the reasoning process as a graph, allowing for more complex and non-linear reasoning patterns.This approach enables the exploration of multiple reasoning paths and the evaluation of different options, potentially leading to more accurate and insightful solutions [91,97,112].Other chains of thought techniques include Mindmap, IRCOT, Reasoning on Graphs, CoT with Consistency, Program Aided Language Model (PAL), Reason and Act, reflection and Tree of thought [40,41,43,45,115] Advanced prompting augmented methods are critical for maximizing LLM performance across various tasks.This is exhibited in prompt augmentation systems (PAS), and Slim Proxy Language (SlimPLM) alongside Self-memory techniques.The Prompt Augmentation System (PAS) introduces a novel and data-efficient method for enhancing prompts.As a plug-and-play automatic prompt engineering system, generating complementary prompts to enhance LLM outputs.This method aligns with high data efficiency, achieving state-of-the-art results with minimal data.PAS significantly outperforms Automatic Prompt Engineering (APE) models, demonstrating the effectiveness of generating high-quality complementary prompts automatically.The emphasis on data efficiency and automatic augmentation aligns with broader trends in developing scalable and adaptable prompt engineering solutions for LLMs.It also underscores the importance of reducing the dependency on large datasets, making LLM applications more accessible and sustainable [97].</p>
<p>SlimPLM (Slim Proxy Language Model), is a smaller proxy language model that assesses the LLM's knowledge and determines whether retrieval is necessary, optimizing the use of external resources [139].Selfmemory prompting is an adaptive form of prompting where the LLM's own generated outputs are used to enrich subsequent prompts through self-feedback loops [78], hence improving the quality and consistency of responses.This prospective prompting framework solves the limitations of using the internal memory of traditional RAG by incorporating a retrieval-augmented generator and a memory selector.The generator uses both the input text and retrieved memory to generate output, while the memory selector refines this output to create an unbounded memory pool that is iteratively used for subsequent generations with quality improvement in tasks such as Neural Machine Translation, abstractive text summarization, and dialogue generation [78].</p>
<p>In summary, prompt engineering techniques play a crucial role in enhancing the accuracy, efficiency, and reasoning capabilities of large language models (LLMs).By refining how prompts are structured, these techniques optimize retrieval and generation processes, enabling LLMs to perform more effectively across diverse tasks.They serve as a foundational element for developing robust, context-aware AI applications, laying the groundwork for more sophisticated and adaptable models in real-world scenarios.</p>
<p>LLM capabilities within integrated environment</p>
<p>This section focuses on how integrated large language models (LLMs) effectively address the challenges encountered by traditional LLMs, as discussed in Chapter 2. By leveraging different integration techniques, such as the incorporation of knowledge graphs, knowledge bases, and specialized retrieval mechanisms, these integrated models overcome limitations in accuracy, scalability, and interpretability, providing more robust and contextually aware solutions.The Table 5 highlights the features of different methods for integrating knowledge with Large Language Models (LLMs), focusing on key aspects such as data structure, accuracy, reasoning ability, and application areas.</p>
<p>Evaluation metrics for knowledge-integrated LLMs</p>
<p>Enhancing interpretability in integrated LLMs</p>
<p>Retrieval-augmented generation (RAG) serves as a bridge between the generation and retrieval processes, allowing users to directly trace the pathways of information used by the LLM.By integrating retrievalbased methods, RAG systems give LLMs real-time access to external, up-to-date data sources, such as large document databases, the web, or specialized knowledge bases [36,120].This provides anchoring for the model's generated responses, enhancing interpretability and transparency by allowing users to verify the source of the information.It also reduces the risk of hallucinations, a common issue with LLMs that rely on static training data [90].</p>
<p>RAG is especially effective for tasks that require current or specific knowledge.For instance, in domains such as biomedicine or law, where verifying conclusions against well-established knowledge is essential, RAG ensures that the LLM's responses are grounded in authoritative, real-world data [105].The Chronicles of RAG has demonstrated how effective external knowledge integration can be in improving trust and transparency by allowing users to track the origin of the generated content [119].</p>
<p>Knowledge Graphs (KGs) provide a structured representation of relationships between entities.By allowing LLMs to access well-organized, structured information, KGs enable LLMs to reason through complex tasks in a logical and traceable manner [36].This enhances model transparency, as users can follow the reasoning process more clearly.For instance, systems like TRACE use KGs to map logical connections between retrieved evidence, enabling multi-hop reasoning (i.e., connecting multiple sources to answer complex queries [123].The integration of KGs into LLMs thus, strengthens the interpretability and traceability of their outputs.</p>
<p>Advanced prompting methods, such as Chain-of-Thought (CoT) prompting, further improve interpretability by breaking down complex questions into smaller, more manageable sub-tasks.This step-by-step reasoning enables LLMs to adopt intermediate reasoning that can be traced and verified, making the decision-making process clearer to the user [83,124].</p>
<p>Systems like TRACE construct reasoning chains that decompose a task into sub-steps, guiding the LLM through each sub-task by identifying key pieces of evidence and logically connecting them.CoT is particularly useful for tasks that require logical reasoning or multi-step calculations, such as mathematical problem-solving, multi-hop question answering, or decision-making tasks [36].When combined, RAG, Knowledge Graphs, and Chain-of-Thought prompting create a powerful framework for enhancing interpretability in large language models (LLMs).This integrated approach grounds LLM outputs in verifiable real-world sources reduces hallucinations and offers a structured reasoning framework.Together, these methods increase traceability and make the model's decision-making processes more understandable and easier to validate.This combined framework significantly improves both the reliability and interpretability of LLMs.</p>
<p>Managing computational costs in integrated LLMs</p>
<p>Techniques like Retrieval-Augmented Generation (RAG) aim to mitigate these challenges by offloading knowledge retrieval to external databases, reducing the need for models to store all knowledge internally.This external retrieval system allows for more efficient use of resources, both in memory and processing and enables the model to access up-to-date information dynamically.For instance, an exploration of RAG demonstrates how retrieving external documents allows the LLM to reduce its internal storage needs, thus lowering computational demands during both training and inference [119].This flexibility enhances the scalability of the models, making them more adaptable to real-time updates without requiring retraining, which is resource-intensive [119].</p>
<p>Further improvements to RAG's computational efficiency have been achieved through optimized retrieval methods, such as graph-based reranking (G-RAG).G-RAG refines the retrieval process by prioritizing the most relevant documents through a semantic graph that links related documents.This targeted retrieval not only reduces the volume of irrelevant information processed but also increases the precision of the responses, thus reducing overall computational costs [119].This re-ranking technique significantly minimizes the workload on LLMs while maintaining high response quality.However, while re-ranking shows promise, there is still room for improvement, particularly in reducing the time overhead associated with complex knowledge graph constructions.</p>
<p>Knowledge-Based Systems 318 (2025) 113503</p>
<p>Another critical technique contributing to computational efficiency is few-shot prompting, which allows LLMs to adapt to new tasks without requiring full retraining.Few-shot prompting operates by providing a few examples to the model during inference, enabling it to generalize across similar tasks [108,120].This method, as exemplified in RAG, reduces the need for continuous updates or domain-specific retraining, a process that traditionally demands significant computational resources.The ability of models to adapt dynamically via few-shot learning echoes broader trends in transfer learning, where models pre-trained on large datasets are fine-tuned on smaller, task-specific data.Few-shot prompting, though efficient, may have limitations in complex domains where more extensive task-specific training might be necessary for optimal performance [78,140].</p>
<p>While RAG, optimized retrieval methods, and few-shot prompting present robust solutions for computational efficiency, some challenges remain unaddressed.For example, RAG's reliance on external databases introduces potential delays in retrieval and integration, which may not always align with real-time processing needs.Furthermore, as noted in the critique of RAG, over-reliance on external documents can lead to instances where the model unnecessarily retrieves information it already possesses, contributing to inefficiency.Additionally, research suggests that even with optimization strategies, LLMs sometimes struggle with long or complex queries, such as those affected by the ''Lost in the Middle'' problem, where the model focuses too much on the start and end of a document while neglecting the middle sections [83,141]</p>
<p>Solving scalability and reliability in integrated LLMs</p>
<p>Scaling Large Language Models (LLMs) across various domains presents challenges in terms of handling diverse queries, and vast datasets, and ensuring relevant, accurate responses.A key solution to enhance scalability is to combine an external Retrieval-Augmented Generation (RAG) with Knowledge Graphs (KGs) [41,118,121].This hybrid system leverages the dynamic knowledge retrieval capabilities of RAG with the structured reasoning of KGs, allowing LLMs to efficiently scale across different domains while maintaining factual accuracy.RAG enables real-time retrieval of external information, hence preventing the need for LLMs to store all knowledge internally.When integrated with KGs, which organize and structure domain-specific data such as medical or legal information, the system ensures precise reasoning and decision-making, especially in large specialized LLM [81,95].Similarly, in the Graph-Based Retrieverapproach discussed, a hybrid RAG-KG system enhances scalability by addressing the issue of information overload in biomedical literature [81,109,118,142].In the medical domain, repetitive and redundant data can overwhelm retrieval systems.By using KGs to down-sample over-represented topics and ensure balanced information retrieval, the hybrid RAG-KG system ensures that critical but less prominent information is not overshadowed, making it scalable across large, complex datasets.This hybrid approach broadens the applicability of LLMs to fields requiring precision, such as biomedicine and law, without sacrificing accuracy or efficiency [118].</p>
<p>RAG enhances reliability by grounding responses in external, verifiable sources, rather than relying solely on pre-trained model weights.This grounding ensures that LLM outputs are based on up-to-date, trustworthy information.Structured retrieval methods, particularly graphbased reranking (G-RAG), further improve reliability by optimizing which documents are retrieved [143].Graph neural networks (GNNs) evaluate the relationships between retrieved documents, elevating the most contextually relevant ones, and reducing the chance of irrelevant or inaccurate information being used in the final output.</p>
<p>In tasks like open-domain question answering, where reasoning often requires drawing from multiple sources, G-RAG's structured retrieval process ensures the model has access to the most reliable documents.Research has demonstrated that by employing Abstract Meaning Representation (AMR) graphs to map semantic relationships between documents, G-RAG filters out less relevant data and prioritizes documents that offer more accurate answers.This significantly reduces the risk of hallucinations -situations where the model generates inaccurate or fictional information -making the system more dependable for open-domain tasks [143].Lastly, leveraging smaller proxy models to identify knowledge gaps in the LLM, the system can selectively engage retrieval mechanisms, optimizing resource utilization [139].</p>
<p>Seamless integration of LLMs with existing systems</p>
<p>Integrating Large Language Models (LLMs) into existing systems presents a variety of challenges, including data access, real-time updates, scalability, security, and interpretability.To address these, multiple advanced techniques have been developed beyond the common methods like Retrieval-Augmented Generation (RAG) and Knowledge Graphs (KGs).</p>
<p>RAG allows LLMs to dynamically retrieve relevant information from external sources during inference, bridging gaps in knowledge and providing access to both structured and unstructured data.LLMs are further enhanced with API integration and data pipelines, ensuring seamless real-time access to diverse data sources such as legacy systems, external APIs, and proprietary databases [41,92].This integration can be simplified through RESTful APIs or more complex solutions like GraphQL, which allows querying across multiple data endpoints efficiently.</p>
<p>In scenarios where keeping LLMs updated is critical, especially in dynamic fields like healthcare and finance, RAG retrieves the latest information at inference time, eliminating the need for constant retraining.Knowledge Graphs (KGs), which offer structured data and incremental updates, further allow LLMs to access evolving knowledge without undergoing full retraining cycles.This combination is particularly useful in specialized domains where new information is frequently introduced, such as in BIORAG for biomedical data.</p>
<p>Tool-integrated reasoning through search engines, calculators, and code interpreters can extend LLM capabilities, and enhance accuracy, efficiency and integration with specific tasks or domains [29].Incorporating tools into the thinking process of LLMs can boost effectiveness in tasks that require calculations or specialized algorithms, hence enhancing their performance and lessening their workload.Models such as TORA use tool-integrated reasoning to tackle challenging problems by interweaving natural language reasoning with external tools, like computation libraries and symbolic solvers [144].</p>
<p>Additionally, adapters can be used to fine-tune LLMs for specific tasks without altering the core model parameters [124].These lightweight neural modules can be plugged into the LLM and trained with domain-specific data, making them highly adaptable to different tasks and reducing computational costs.For example, the UP-RISE framework employs adapters to select the most appropriate prompts from a pool for a specific zero-shot task [36].This flexibility is particularly useful for enterprise systems, where multiple domain-specific adaptations may be required.</p>
<p>Another promising method for improving scalability is multi-agent architectures, where distinct agents handle specific system tasks, such as data ingestion, knowledge retrieval, and response generation.This division of labor distributes the workload more efficiently and allows LLMs to integrate smoothly with complex systems [145].</p>
<p>To further enhance the system's ability to scale, model merging can combine the parameters of multiple pre-trained models into a unified model, integrating different task-specific capabilities into a single, more versatile LLM.Additionally, tool augmentation extends the LLM abilities by integrating it with external tools, APIs, and realtime data streams for specialized tasks like scheduling, booking, or domain-specific reasoning.</p>
<p>Adapting LLMs to changing knowledge bases</p>
<p>One prominent issue is limited temporal knowledge, how models handle time-related information and reasoning.Despite their vast capacity, traditional LLMs are bound by the constraints of their training data.They do not automatically acquire new information after training, which can lead to outdated or irrelevant responses, particularly concerning recent events or discoveries.For example, an LLM trained before a significant geopolitical event or medical breakthrough may still produce content that reflects an outdated understanding.To address this, research explored methods like continuous learning and dynamic training.Continuous learning enables LLMs to incrementally adapt to new information, allowing them to incorporate evolving language and updated knowledge without needing to retrain from scratch [146].This is particularly valuable in fields where timely and accurate information is crucial.Dynamic training similarly focuses on updating models continuously, ensuring that LLMs remain relevant over time as new research and discoveries emerge [147].Another substantial challenge is the difficulty in updating knowledge.Traditionally, modifying an LLM's knowledge base requires retraining the entire model, a process that is both computationally intensive and costly [148].This has spurred the development of techniques like Retrieval-Augmented Generation (RAG), fine-tuning, and incremental learning.RAG systems are particularly effective because they allow LLMs to retrieve external information during inference, circumventing the need for constant retraining [95,109,127].</p>
<p>Through retrieval, models can access up-to-date knowledge from databases, Knowledge Graphs, or other dynamic sources like the Internet.Sources advocate future research on several areas time-aware retrieval, forward-looking active retrieval augmented generation, realtime QA, and timeliness in Gen IR [36,99,147].</p>
<p>Time-aware retrieval potential incorporates temporal metadata like timestamps into the retrieval process of RAG to access the up-to-date data.FLARE anticipates future queries during text generation enabling more timely and relevant access [99].Realtime QA evaluates the LLM's ability to handle recent events whilst GenIR advocates for research in real-time knowledge access and continual learning and editing to maintain knowledge currency [29].Notable research is demonstrated with common sense reasoning explored reasoning about time, where LLMs are instructed to retrieve information given a specific time or events [112].</p>
<p>Knowledge-integrated Large Language Models (LLMs) leverage external knowledge sources, such as knowledge graphs (KGs), domainspecific corpora, and databases, to enhance factual accuracy and reasoning capabilities.Evaluating these models requires a structured approach that extends beyond traditional NLP benchmarks, incorporating tailored metrics that assess knowledge retrieval, reasoning, and text generation.</p>
<p>To systematically assess the performance of knowledge-enhanced LLMs, we categorize key evaluation metrics into three main groups:</p>
<p> Retrieval-based metrics: These metrics assess the efficiency and accuracy of retrieving relevant knowledge from external sources, whether structured (KGs) or unstructured (text-based documents). Knowledge consistency and reasoning metrics: These metrics evaluate the model's ability to generate responses that are logically coherent and aligned with factual knowledge. Generation and fluency metrics: These metrics measure the overall quality and readability of the generated output, which is crucial for effective knowledge communication.</p>
<p>Retrieval-based metrics</p>
<p>Retrieval mechanisms are foundational for knowledge-enhanced LLMs, determining how effectively relevant information is identified and incorporated into responses.These metrics focus on both the accuracy of retrieved content and the efficiency of the retrieval process.</p>
<p>Recall@K: Measures the proportion of relevant documents or KG entities retrieved within the top K results [137].A high Recall@K suggests that the model effectively retrieves a significant portion of the relevant knowledge, minimizing the risk of missing critical information.This is particularly important in applications such as medical diagnosis and legal document retrieval, where omitting key details could lead to incomplete or incorrect conclusions.</p>
<p>Hits@K: Evaluates whether at least one correct knowledge element appears in the top K retrieved items, ensuring that relevant knowledge is present within the retrieved results [135].Unlike Recall@K, which measures the proportion of relevant items retrieved, Hits@K focuses solely on the presence of at least one relevant result, making it highly useful for applications such as question-answering systems, semantic search, and recommendation systems.For example, in chatbot knowledge retrieval, a high Hits@K ensures that at least one retrieved document contains useful and factually accurate information, allowing the chatbot to provide meaningful responses.</p>
<p>Mean Reciprocal Rank (MRR): assesses the position of the first correct result in the retrieved list, rewarding models that rank relevant knowledge higher.It is calculated as the average reciprocal rank of the first relevant document across multiple queries, ensuring that knowledge-integrated LLMs prioritize the most relevant information [149].MRR is particularly crucial in applications where users expect immediate access to useful information, such as search engines and automated legal or financial assistants.For example, in a legal AI system, if a user is searching for case precedents related to intellectual property law, a high MRR ensures that the most relevant case appears at the top of the retrieved results, significantly improving efficiency and decision-making.However, MRR is inherently user-centric, as different users may have varying expectations regarding what constitutes the most relevant result.For example, a legal expert might prioritize precedent-setting cases, while a general user might prefer easily interpretable summaries.This subjectivity means that while MRR is useful for ranking efficiency, it may not fully capture individual user preferences.</p>
<p>Area Under the Curve (AUC):is another important metric for evaluating retrieval performance, particularly in ranking-based retrieval systems.AUC measures the model's ability to distinguish between relevant and non-relevant retrieved items, offering an aggregate assessment of ranking quality [138].A higher AUC score indicates that the model consistently ranks relevant documents higher than irrelevant ones, making it a valuable metric in information retrieval, recommendation systems, and search engine optimization.Additionally, AUC accuracy, as an extension of general accuracy [130], assesses how effectively a model classifies relevant and irrelevant information, ensuring that the retrieved content aligns with user expectations and query intent.</p>
<p>Needle-in-a-Haystack (NIH) Test : evaluates the model's ability to retrieve rare but critical facts hidden within a vast amount of information, making it essential for assessing long-tail knowledge retrieval performance [150].Many real-world applications, such as biomedical research, financial fraud detection, and cybersecurity threat analysis, require identifying rare but highly significant details that could be easily overlooked.For example, in medical research, a rare but crucial study linking a genetic mutation to a disease may be buried among thousands of more general studies.A knowledge-integrated LLM with strong NIH performance would successfully surface such rare insights, ensuring that vital but infrequent knowledge is not lost.This metric is particularly valuable in high-stakes decision-making environments where missing a rare but critical fact could lead to erroneous conclusions.</p>
<p>Knowledge consistency and reasoning metrics</p>
<p>For a knowledge-integrated Large Language Model (LLM), ensuring that the retrieved information is accurately interpreted, logically structured, and coherently applied in its generated responses is just as important as retrieving relevant knowledge itself.A model that retrieves the right information but misinterprets, distorts, or fails to integrate it meaningfully can still produce misleading or incorrect outputs.Knowledge consistency and reasoning metrics are essential for evaluating whether an LLM's responses align with factual accuracy, maintain logical coherence, and demonstrate sound reasoning.</p>
<p>Knowledge-Based Systems 318 (2025) 113503</p>
<p>Exact Match (EM): determines whether the generated response precisely matches the ground truth, making it a crucial metric for applications where absolute factual correctness is required [149].This is particularly important in question-answering systems, regulatory compliance tools, and automated legal assistants, where even minor deviations in wording could alter the intended meaning.In some cases, the percentage of correct responses [150] may be used as an alternative metric, as it is conceptually similar to Exact Match (EM) .This approach allows for greater flexibility in evaluation, depending on how correctness is defined, for example, by permitting paraphrased responses that preserve the original meaning or responses with minor formatting variations.</p>
<p>BERTScore: leverages deep contextual embeddings to measure semantic similarity between the generated text and reference answers, capturing meaning beyond simple word-matching [151].Unlike Exact Match, which requires a verbatim correspondence, BERTScore assesses whether a response conveys the same meaning, even if expressed differently.This makes it highly valuable in open-ended tasks such as summarization, essay scoring, and open-domain question answering.Additionally, it is particularly useful for evaluating responses in multilingual settings, where direct translation may alter word choices but still preserve meaning.</p>
<p>Logical Consistency: refers to the ability of the model to generate outputs that follow structured reasoning principles.This ensures stable and coherent decision-making in applications such as knowledge-based reasoning, automated decision systems, and logical inference engines.Logical consistency in knowledge-integrated LLMs ensures structured reasoning by maintaining transitivity, commutativity, and negation invariance, which are crucial for stable decision-making.Quantitative metrics such as Stran, Scomm, and Sneg [152] can potentially help assess logical coherence.However, logical consistency is inherently human-centric, as it depends on how individuals interpret reasoning and make judgments, which can vary based on context, cultural background, and subjective preferences.</p>
<p>Generation and fluency metrics</p>
<p>F1 Score: measures the balance between precision and recall in retrieved information, making it particularly useful for evaluating RAG models in knowledge-integrated LLMs [153].High precision ensures retrieved documents are accurate and contextually relevant, while high recall prevents missing critical knowledge needed for generating responses.Some studies report precision or recall separately based on evaluation needs-precision-focused assessments minimize false positives (e.g., medical diagnosis), whereas recall-focused evaluations ensure comprehensive retrieval (e.g., legal document discovery).</p>
<p>BLEU (Bilingual Evaluation Understudy):</p>
<p>evaluates n-gram overlap between generated text and reference responses, primarily assessing fluency and lexical similarity [154].BLEU is commonly applied in machine translation and text generation, but has limitations in capturing semantic correctness and contextual relevance.A variant, BLEU-1 (B-1) [120], measures unigram overlap, making it particularly effective for evaluating short responses and phrase-level accuracy.While a higher B-1 score indicates better word choice alignment with reference text, it does not account for word order, coherence, or deep semantic understanding.Another refinement, Q-BLEU-1 [120], has been designed specifically for question-answering tasks, prioritizing meaningful term matches over verbatim replication.</p>
<p>ROUGE (Recall-Oriented Understudy for Gisting Evaluation):</p>
<p>focuses on content coverage and information recall rather than exact lexical matches.This metric is particularly relevant for summarization tasks, where preserving the key information from a reference text is more important than replicating exact phrasing.ROUGE evaluates the extent to which generated text retains the essential meaning of a source document through several variations [154].ROUGE-N measures ngram recall [155], emphasizing how much of the original wording is retained.ROUGE-2 [151], which specifically considers bigram recall, provides a better measure of coherence and phrase-level similarity between the generated and reference texts.ROUGE-L [120], on the other hand, leverages the longest common subsequence (LCS) between the reference and generated text, capturing sentence structure while allowing for variations in word choice.ROUGE-W introduces weighted scoring, placing higher importance on longer matching sequences [155], making it particularly effective for structured or factual text summarization.These methods enable ROUGE to assess knowledge-driven text generation, making it particularly suitable for domains such as legal and scientific document summarization, where content completeness and factual integrity are essential.</p>
<p>Generation Time: measures the computational efficiency of response generation, which is critical for real-time applications such as conversational AI, search engines, and customer service chatbots [156].A reduction in generation time enhances inference speed, thereby improving the scalability and practicality of models in production environments.However, the optimization of response time must be carefully balanced against output quality and factual accuracy, as excessive emphasis on speed may increase the likelihood of hallucinations, incomplete responses, or reduced coherence.Ensuring an optimal tradeoff between computational efficiency and response reliability is therefore essential for the effective deployment of knowledge-integrated language models in real-world applications.</p>
<p>The evaluation of knowledge-integrated Large Language Models (LLMs) relies on a diverse set of quantitative metrics that assess retrieval effectiveness, knowledge consistency, reasoning capabilities, and text generation quality.</p>
<p>Tables 6 and 7 provide a structured comparison of Retrieval-Augmented Generation (RAG)-based LLMs and Knowledge Graph (KG)-based LLMs, presenting key datasets, performance improvements, and essential evaluation metrics.</p>
<p>Based on the above results, Table 6 summarizes typical RAG-based LLM models, highlighting their reliance on external unstructured corpora for knowledge retrieval and response generation.The evaluation primarily focuses on Exact Match (EM), ROUGE, and BLEU, which assess text overlap, fluency, and factual alignment with reference responses.The results indicate that many variations of RAG models significantly enhance retrieval accuracy over baseline models, particularly in EM.However, notable performance variations are observed across different datasets, with some models exhibiting declines in EM and BLEU scores, underscoring the dataset-dependent nature of retrieval augmentation.This suggests that while RAG-based approaches effectively improve knowledge integration, their performance varies based on dataset characteristics and complexity, necessitating further optimization and adaptation for domain-specific applications.</p>
<p>Table 7, on the other hand, focuses on KG-based LLM models, which leverage structured knowledge representations to enhance factual accuracy and reasoning.Many recent developments have focused on integrating RAG and KG, with some hybrid approaches combining retrieval-augmented generation (RAG) and knowledge graph (KG) reasoning to leverage the strengths of both retrieval mechanisms, such as GraphRAG [151].These hybrid models aim to enhance retrieval accuracy and reasoning capabilities while reducing hallucinations and improving factual correctness.Metrics such as EM, Hits@1, and Recall@5 are used to evaluate these integrated models, demonstrating improvements in knowledge completeness, response fluency, and factual reliability.However, the combination of text-based retrieval and structured knowledge reasoning introduces computational trade-offs, necessitating further research on scalability and efficiency for real-world deployment.</p>
<p>Finally, based on Tables 1 and 2, both RAG-based, KG-based, and hybrid LLM models assess the quality of generated responses using F1 Score, Precision, and Recall, ensuring a balanced evaluation of accuracy and completeness.FUNNEL-RAG Natural Questions (NQ) 3.93% recall over RAG Recall [161] TriviaQA (TQA) 0.99% recall over RAG</p>
<p>Case studies: Practical applications of LLM-knowledge base integration</p>
<p>This section explores real-world case studies that demonstrate the practical applications of integrating Large Language Models (LLMs) with knowledge bases.The cases discussed include FinAgent, a multimodal foundation agent designed for financial trading; the Unified Medical Language System, which facilitates the integration of medical knowledge for improved healthcare outcomes; Codex LLMs, which enhance programming capabilities by connecting code generation with vast knowledge bases; and BloombergGPT, a specialized LLM built for financial data analysis and decision-making.These case studies highlight the diverse ways in which LLM-Knowledge Base integration is applied across various industries, showcasing the potential for enhanced accuracy, decision-making, and efficiency.</p>
<p>FinAgent: A multimodal foundation agent for financial trading</p>
<p>FinAgent gathers, processes, and analyzes diverse financial data sources, including news, stock prices, and financial reports-through a RAG-like pipeline that enhances its market intelligence capabilities.By integrating historical market trends and real-time data, FinAgent develops a comprehensive understanding of market conditions, allowing it to make data-driven and context-aware trading decisions.Additionally, it incorporates established trading strategies and expert  insights as augmented tools, enabling it to combine traditional financial knowledge with its AI-driven analytical capabilities for more robust and explainable decision-making [51,98].</p>
<p>To enhance trading adaptability, FinAgent employs a task-specific prompt generator that tailors prompts to different trading scenarios and market conditions.This ensures that the LLM outputs are aligned with actionable trading strategies, improving its ability to respond to dynamic financial environments.By iteratively learning from both immediate market conditions and historical performance, FinAgent enhances its trading precision and adaptability [98].</p>
<p>Between 2022 and 2024, FinAgent was tested on real financial datasets, with its performance evaluated against 12 baseline trading methods, including rule-based strategies, machine learning and deep learning models, reinforcement learning algorithms, and LLM-based trading systems [98].The evaluation dataset included five major tech stocks (AAPL, AMZN, GOOGL, MSFT, TSLA) and one cryptocurrency (ETHUSD).Results showed that FinAgent consistently outperformed baseline methods across key financial performance metrics, particularly in terms of profitability.Its success is attributed to its multimodal market intelligence, augmented tools, diversified memory retrieval, and dual-level reflection, all of which contribute to its ability to adapt and optimize trading strategies dynamically.</p>
<p>FinAgent' architecture and functionality align with core RAG Principles: which are external knowledge integration, contextualized decision-making, leverages memory and retrieval and reasoning and reflective capabilities [95,98,107].However, it differs from a traditional RAG, it handles multimodal data including text, numerical and visual information while rag deals with text.It integrates the RAG Principle in reinforcement Learning enabling dynamic adaptation of the market not typically found in a standard RAG.Sources lack insights on retrieval methods such as keyword-based, and semantic similarity which is discussed in detail in context.Overall, it integrates an rag-approach to enhance decision-making in financial trading.</p>
<p>Overall, FinAgent incorporates a RAG-inspired approach to financial trading, enhancing decision-making through knowledge integration, multimodal analysis, and adaptive reinforcement learning.Its combination of LLMs, market intelligence, and memory-enhanced learning positions it as a powerful tool for real-time financial strategy optimization.</p>
<p>The unified medical language system</p>
<p>The Unified Medical Language System (UMLS) is a comprehensive and influential resource developed by the U.S. National Library of Medicine to integrate a vast array of medical vocabularies and standards.Positioned at the foundation of multi-level medical graphs, UMLS plays a crucial role in enhancing interoperability across healthcare systems while facilitating medical diagnosis and treatment recommendations [52,162].</p>
<p>Strategically, UMLS serves as a foundational data source within multi-level medical graphs, anchoring medical entities and their relationships in a structured framework.By enhancing entity linking and bridging knowledge gaps, UMLS provides a robust foundation for Retrieval-Augmented Generation (RAG) applications in the medical domain [105,148,162].Additionally, its standardized language structure supports the development of AI-driven tools for medical diagnosis, treatment planning, and decision support.</p>
<p>UMLS comprises three core components that contribute to its role in medical knowledge representation and integration [52,163]:</p>
<p> Meta-Thesaurus: Integrates over 2 million medical vocabularies (e.g., SNOMED CT, ICD-10) encompassing nearly one million medical concepts from more than 60 biomedical vocabulary families, alongside 12 million interrelationships, ensuring a unified knowledge structure. Semantic Network: Organizes medical concepts hierarchically, defining relationships between them to enhance reasoning and contextual understanding in AI-driven applications. Lexicon: Provides linguistic insights to facilitate natural language processing (NLP) tasks within the medical domain.</p>
<p>Recent studies have demonstrated that integrating UMLS knowledge pathways into LLMs results in significant improvements in diagnostic accuracy and medical question answering.Research findings indicate that grounding LLM-generated responses in UMLS-enhanced knowledge graphs leads to performance gains in terms of factual correctness, completeness, and contextual relevance [162].</p>
<p>Moreover, studies have highlighted UMLS-augmented LLM frameworks as effective in enhancing factual accuracy and reducing hallucinations in medical AI systems.These improvements are particularly relevant for clinical decision support systems, where the quality and reliability of AI-generated outputs are critical for patient safety and medical efficacy [52].</p>
<p>Despite its benefits, the integration of UMLS with LLMs presents several challenges.Knowledge base integration remains a technical hurdle, as seamless retrieval and utilization of UMLS knowledge within LLM-generated outputs require sophisticated methodologies.Regular updates to UMLS are essential to incorporate new medical discoveries, evolving guidelines, and updated clinical practices, necessitating ongoing maintenance to ensure the accuracy and relevance of the knowledge base.The vast size of UMLS introduces computational overhead, requiring optimized retrieval mechanisms to balance efficiency and accuracy in AI-driven applications.Additionally, UMLS follows a symbolic representation of knowledge, whereas LLMs operate on subsymbolic representations, creating a gap that must be bridged to enable seamless integration.Bias mitigation is another critical concern, as there is limited research on how biases in UMLS data are addressed in LLM-based applications, raising concerns about fairness and accuracy in AI-driven medical decision-making [52,162].</p>
<p>In conclusion, UMLS represents a critical resource in the development of sophisticated and reliable medical AI systems by enhancing interoperability, factual accuracy, and clinical decision support.However, addressing challenges related to knowledge integration, scalability, and bias mitigation will be essential to maximize its potential.Continued research and development efforts will play a vital role in optimizing the integration of UMLS with LLMs, paving the way for more reliable, explainable, and clinically relevant AI applications in healthcare.</p>
<p>Codex LLMs</p>
<p>Codex, a variant of GPT-3 developed by OpenAI, is a large language model (LLM) known for its capabilities in code generation, test assertion generation, program repair, and documentation [36,164,165].Unlike other GPT models, Codex is specifically trained on publicly available data from GitHub, programming documentation, and coding forums, enhancing its proficiency in understanding and generating code.While it follows the transformer-based architecture of its predecessors, its specialized training dataset makes it particularly effective for software development tasks.</p>
<p>One of Codex's key advancements is its integration with retrieval mechanisms, forming Retrieval-Augmented Generation (RAG) systems.This approach enhances generative capabilities by enabling Codex to access and retrieve relevant external knowledge, improving the accuracy, contextuality, and relevance of generated code [36].By incorporating retrieval-based augmentation, Codex can leverage programming documentation, API references, and contextual search methods, reducing hallucinations and improving code quality.</p>
<p>A significant implementation of Codex is its integration into GitHub Copilot, a code assistant that operates within integrated development environments (IDEs) such as Visual Studio Code and JetBrains [165,166].Copilot is facilitated by an intermediary plugin that captures user inputs, analyzes contextual information (from comments, function signatures, or existing code), and generates relevant code suggestions.The architectural components of Codex in this system include an API layer, context engine, model fine-tuning, and security mechanisms.The data flow begins with input capture within the IDE, which is then parsed for contextual analysis.The parsed input is transmitted to the Codex model for code generation, and the generated code is returned to the GitHub Copilot plugin, where it is presented for user review, selection, and feedback [166,167].</p>
<p>Codex has been integrated into multiple RAG-based systems, each employing unique retrieval techniques.The EPR system, introduced in 2021, integrates Codex with GPT-3, J, and Neo, utilizing demonstration-based retrieval and inference-time reasoning [148].In 2022, OpenBook was developed, incorporating GOPHER LLM for question answering and fact verification, combining semantic encoding and symbolic reasoning to improve retrieval efficiency [148].Another model, DSP, released in the same year, uses GPT-3.5 for open-domain and multiple-choice question answering, leveraging ColBERTv2 for retrieval augmentation [148].In 2023, IRCoT was introduced, integrating GPT-3 and Flan-T5 for question answering while utilizing BM25 for search optimization [36,148].Additionally, CodeLLaMA has been explored as a specialized model for code-related tasks, demonstrating overlaps with Codex's capabilities and applications [148].</p>
<p>Despite its versatility, Codex encounters several challenges in realworld implementations.One of the primary concerns is its limited contextual understanding in large codebases, which affects its ability to maintain coherence and logical consistency across extensive projects [36].The computational demands of Codex contribute to latency and performance bottlenecks, making real-time inference costly.Additionally, security concerns related to code vulnerabilities and biases in training data pose risks to software integrity and ethical programming practices [165].Codex inherits biases present in open-source code repositories, potentially propagating insecure coding patterns or reinforcing suboptimal design choices.Furthermore, an over-reliance on AI-generated code without a deep understanding of underlying programming principles can lead to misinterpretations and erroneous implementations, underscoring the need for responsible AI-assisted coding practices.</p>
<p>The evaluation of Codex-generated code remains an area of active research.While platforms such as Leetcode provide standardized datasets for assessing AI-generated code solutions, existing benchmarks -including noise robustness, negative rejection, information integration, and counterfactual robustness -exhibit limitations in evaluating LLM-generated code [90].To ensure reliable assessments of code-generating LLMs, future research should focus on developing comprehensive evaluation frameworks that account for logical correctness, security vulnerabilities, maintainability, and efficiency in real-world software development.</p>
<p>BloombergGPT</p>
<p>BloombergGPT is recognized as the first mixed-domain financial LLM (FinLLM), built upon the BLOOM model and designed specifically for financial applications [12,104].With 50 billion parameters, BloombergGPT aligns with the domain-specific pre-training paradigm, combining a general corpus of 345 billion tokens from books, articles, websites, and code with a larger financial dataset known as FinPile, comprising 363 billion tokens.FinPile includes data from diverse financial sources, such as web content, news articles, regulatory filings, press releases, and Bloomberg's proprietary datasets.This extensive financial pre-training makes BloombergGPT well-suited for specialized financial analysis and decision-making tasks.</p>
<p>As one of the most advanced financial LLMs, BloombergGPT represents a major milestone in the evolution of AI-driven financial applications.The performance of BloombergGPT has been evaluated across multiple task categories, including five benchmark financial NLP tasks, 12 internal Bloomberg tasks, and 42 general-purpose NLP tasks.One notable result is its 43% exact match (EM) accuracy score on the ConvFinQA dataset, which, while demonstrating strong financial domain adaptation, remains below the performance of a general crowd (47%) and significantly lower than GPT-4 with zero-shot prompting (69%-76%) [12].Despite this, BloombergGPT's design and training data suggest its applicability across a variety of financial use cases, including sentiment analysis, risk assessment, portfolio management, financial forecasting, fraud detection, and regulatory compliance.These capabilities position BloombergGPT as a specialized tool for financial professionals, enhancing insights and streamlining analytical processes.</p>
<p>BloombergGPT also faces a range of challenges and limitations.Data quality and bias remain key concerns, as biases in financial data can lead to skewed predictions or reinforce pre-existing market inequalities.The lack of model explainability poses risks in financial decisionmaking, where transparency is crucial for regulatory compliance and investor trust.Additionally, there are ethical considerations regarding the potential for market manipulation, the creation of unfair advantages for select investors, and the displacement of human financial professionals.Addressing these issues requires responsible AI development, enhanced regulatory oversight, and transparent governance frameworks to ensure that FinLLMs are deployed in an ethical and accountable manner [12,104].</p>
<p>Challenges in LLM-knowledge integration</p>
<p>Knowledge drift and long-term reliability</p>
<p>A fundamental challenge in integrating LLMs with knowledge bases is ensuring data reliability and sustainability over time.Knowledge bases, whether structured (e.g., knowledge graphs) or unstructured (e.g., retrieval-augmented sources), require continuous updates to reflect new discoveries, evolving policies, and domain-specific advancements.However, without efficient mechanisms for incremental learning and adaptive updates, LLMs risk retrieving outdated, inaccurate, or inconsistent information, leading to knowledge drift.This issue is particularly critical in high-stakes applications, such as law, medicine, and finance, where factual accuracy directly influences decision-making and regulatory compliance.Additionally, many knowledge bases are statically curated, making it challenging to incorporate real-time updates, dynamic corrections, or evolving contextual nuances.Addressing these limitations requires hybrid retrieval and knowledge management techniques, including automated knowledge synchronization, federated updates, and dynamic fact-checking mechanisms to ensure that LLMs continuously access relevant, current, and contextually accurate information.</p>
<p>Complexity in knowledge representation and utilization</p>
<p>Effectively representing domain-specific knowledge in a format that LLMs can accurately interpret remains a significant challenge in knowledge integration.Many knowledge bases (KBs) and knowledge graphs (KGs) contain complex hierarchical structures, logical dependencies, and specialized terminologies that do not seamlessly align with LLM embeddings.If representations are too rigid, LLMs struggle to generalize across queries, whereas overly abstract formats may lead to hallucinated relationships and factual inconsistencies.This challenge is particularly pronounced in medical AI, legal reasoning, and financial analytics, where structured knowledge follows precise ontological frameworks and domain-specific constraints.Furthermore, effective knowledge integration requires LLMs to process both explicit symbolic representations and implicit contextual relationships, which often exceeds their native capabilities.To bridge this gap, hybrid symbolic-neural approaches, including graph-based attention mechanisms, ontologyguided prompt engineering, and knowledge-enhanced embeddings, are essential for improving LLMs' ability to retrieve, interpret, and reason over structured domain-specific knowledge accurately.</p>
<p>Scalability and efficient knowledge retrieval</p>
<p>As the size and complexity of knowledge bases grow, managing efficient retrieval and integration with LLMs becomes increasingly challenging.Knowledge bases, especially large-scale domain-specific repositories, contain vast amounts of structured and unstructured data, making it difficult for retrieval systems to filter, rank, and process the most relevant information in real-time.This challenge is exacerbated in retrieval-augmented generation (RAG) models, where retrieving lowquality, noisy, or redundant data can negatively impact response accuracy and efficiency.Additionally, computational constraints limit the feasibility of querying large-scale knowledge bases, particularly in real-time applications where latency must be minimized.Potential solutions such as more efficient multi-stage retrieval pipelines, memory-efficient indexing, and distributed retrieval architectures can help improve scalability by ensuring that LLMs retrieve, process, and generate knowledge-enhanced responses efficiently without excessive resource consumption.</p>
<p>Evaluation and benchmarking limitations</p>
<p>Defining standardized benchmarks for LLM-knowledge base (KB) integration remains a significant challenge due to the diverse applications, empirical nature, and human-centric evaluation criteria involved in many real-world use cases.Unlike traditional NLP tasks, which rely on well-defined quantitative metrics, evaluating knowledge-enhanced LLMs necessitates measuring both retrieval effectiveness and reasoning depth, which are inherently difficult to quantify.Many applications, such as medical diagnosis, legal analysis, and financial forecasting, involve complex decision-making processes where empirical validation and expert human judgment play a crucial role, making objective benchmarking inconsistent across domains.</p>
<p>Furthermore, as discussed in Section 5.5, evaluating knowledgeintegrated LLMs requires assessing both knowledge consistency and reasoning quality, which remain challenging to measure comprehensively.While existing evaluation methods incorporate knowledge reasoning and consistency metrics, such as Stran, Scomm, and Sneg for logical consistency, these metrics do not fully capture the depth of knowledge integration, adaptability to evolving information, or realworld applicability in empirical and human-centric applications.Addressing these limitations requires hybrid benchmarking approaches, integrating automated performance metrics with expert-driven qualitative evaluations, alongside the development of domain-specific benchmarks that account for the contextual nuances and dynamic nature of knowledge-intensive applications.</p>
<p>Recommendations for future development and implementation</p>
<p>This section outlines key recommendations for improving the modularity, adaptability, and efficiency of LLM systems.These recommendations focus on enhancing pipeline modularity, implementing iterative refinement processes, optimizing integration strategies, and strategically selecting between open-source and proprietary models.By adopting these recommendations, organizations can enhance the adaptability, scalability, and efficiency of LLMs, ensuring they remain relevant and effective in dynamic, real-world applications.</p>
<p>Modularity in pipelines</p>
<p>A modular architecture is essential for enhancing the flexibility, maintainability, and scalability of LLM-knowledge systems.Given the challenge of knowledge drift and long-term reliability, modular pipelines allow individual components, such as retrieval, reasoning, and generation modules, to be independently updated and optimized.The adoption of self-improving frameworks, such as DsPy, facilitates the seamless integration of incremental updates, advanced retrieval mechanisms, and evolving domain-specific knowledge, ensuring that the system remains accurate and contextually relevant over time.</p>
<p>Furthermore, the modular approach supports the continuous incorporation of emerging technologies and optimization strategies without necessitating a complete system overhaul.By enabling the interchangeability and independent enhancement of key components, such as retrievers and generators, this approach ensures that LLMs remain adaptable to specific domain requirements.This adaptability enhances the system's applicability across diverse industries, including healthcare, finance, and legal AI, where domain-specific knowledge updates and retrieval efficiency are critical to maintaining high-performance and trustworthiness.</p>
<p>Additionally, modularity facilitates the development of hybrid benchmarking frameworks, which are necessary to address the limitations of standardized LLM evaluation methods.Given the lack of consistent benchmarks for knowledge integration, modular benchmarking pipelines enable adaptive evaluation strategies by incorporating both automated metrics and expert-driven qualitative assessments.This approach ensures that evaluation methodologies remain scalable, flexible, and aligned with evolving domain-specific needs, allowing organizations to refine LLM performance assessment over time.</p>
<p>Iterative process</p>
<p>To mitigate knowledge drift and ensure long-term model relevance, LLMs must operate within an iterative learning framework rather than as static systems.Given the scalability challenges in integrating vast and evolving knowledge bases, an iterative process enables continuous refinement of retrieval mechanisms, reasoning capabilities, and factual consistency.Regular updates to knowledge bases, fine-tuning on fresh datasets, and optimization of retrieval techniques are necessary to align LLMs with emerging trends and domain-specific advancements.This iterative approach also ensures that LLMs remain agile and responsive to industry changes, addressing shifting regulatory standards, ethical considerations, and advancements in reasoning capabilities.Particularly in highly dynamic fields such as healthcare, finance, and legal AI, iterative updates help models maintain factual accuracy, minimize outdated information, and improve real-time decision-making.</p>
<p>Optimization integration strategies</p>
<p>Maximizing the efficiency, trustworthiness, and adaptability of LLMknowledge integration requires addressing complex knowledge representation and scalability in retrieval systems.A key optimization strategy is enhancing retrieval mechanisms to improve efficiency and knowledge selection, ensuring that LLMs retrieve high-quality, domainrelevant information while minimizing noise and redundancy.Implementing hybrid retrieval strategies, such as combining sparse and dense retrieval techniques with multi-stage ranking, allows LLMs to process and utilize structured and unstructured knowledge sources more effectively.</p>
<p>Beyond retrieval, improving text generation and contextual integration ensures that LLMs incorporate retrieved knowledge more accurately and coherently.Current models often struggle with factual inconsistencies or hallucinations when integrating external knowledge.Optimizing input-layer fusion, intermediate-layer integration, and retrievalconditioned decoding enhances contextual coherence and domainspecific accuracy, allowing LLMs to better reason over structured knowledge representations.</p>
<p>Additionally, domain-specific adaptation is essential for improving structured reasoning in fields such as medicine, finance, and legal AI, where knowledge follows precise ontological structures.Fine-tuning models on specialized datasets ensures more accurate interpretation and application of domain-specific knowledge, reducing reliance on generic pre-trained data.</p>
<p>Strategic selection between open-source and proprietary models</p>
<p>The choice between open-source and proprietary LLMs is crucial in balancing cost, performance, customization, and regulatory compliance.Proprietary models often deliver high performance, but they come with significant financial costs, limited transparency, and potential vendor lock-in.In contrast, open-source models offer greater control, adaptability, and domain-specific fine-tuning capabilities, making them preferable for knowledge-intensive applications that require data privacy and regulatory compliance.</p>
<p>Fine-tuning smaller open-source models on specialized medical, legal, or financial datasets can yield competitive performance compared to proprietary alternatives while ensuring data sovereignty.Additionally, hybrid retrieval-augmented generation (RAG) pipelines can bridge the gap between open-source and proprietary models, enhancing retrieval precision and reasoning quality while maintaining costeffectiveness.In highly regulated sectors such as healthcare and finance, open-source models provide an advantage by ensuring full oversight of data processing and compliance with privacy regulations.</p>
<p>Complementary AI approaches for LLM enhancement</p>
<p>While this survey primarily focuses on the integration of Large Language Models (LLMs) with knowledge-based methods, recent research has explored several alternative AI augmentation approaches to enhance model efficiency, reasoning capabilities, and factual accuracy beyond traditional knowledge integration.One such approach is Symbolic AI, which has been integrated into LLMs to improve logical reasoning and interpretability by combining rule-based knowledge representation with neural network-based learning [168].Similarly, Neuroscience-Inspired AI leverages cognitive science principles, incorporating architectures such as hierarchical temporal memory and spiking neural networks to enhance energy efficiency and mimic human-like reasoning processes [169].Another promising direction is multi-agent collaboration, where multiple specialized AI agents interact and coordinate to solve complex tasks [170], improving task decomposition, modular problem-solving, and adaptability in real-time environments.Additionally, Self-Optimizing [171] and Reinforcement-Based Learning [172] employs adaptive feedback mechanisms that allow models to iteratively refine their performance and decision-making capabilities based on continuous learning and user interactions.While these approaches differ from knowledge-based integration, they offer complementary advantages.Future research may explore hybrid models that synergistically combine these techniques with knowledge-integrated LLMs, further expanding AI's ability to generate reliable, efficient, and contextually aware outputs.</p>
<p>Conclusion</p>
<p>This paper has explored the integration of Large Language Models (LLMs) with knowledge bases, beginning with an overview of LLMs to understand their key capabilities and real-world applications.It summarizes the challenges faced in implementing LLMs for real-world scenarios.We also examined existing solutions and innovations aimed at overcoming these challenges, emphasizing the value of hybrid approaches that leverage the strengths of both LLMs and knowledge bases.</p>
<p>Subsequently, we conducted a comprehensive analysis of AI enhancement through the integration of LLMs with knowledge-based systems.By reviewing the current state of integration, key techniques, evaluation metrics, and representative case studies, we identified critical benefits, challenges, and future directions.Our findings demonstrate that this integration enhances data contextualization, improves model accuracy, and facilitates more reliable knowledge retrieval across various domains.However, challenges remain, particularly in dynamic knowledge management and model adaptability.Future research should focus on refining integration techniques, optimizing retrieval processes, and ensuring that knowledge bases remain current and relevant.</p>
<p>In conclusion, integrating LLMs with knowledge bases offers significant potential to advance AI technology and improve its application across diverse sectors.The continued evolution of this field promises to result in more intelligent, accurate, and context-aware AI systems, with substantial benefits for various industries and organizations.</p>
<p>CRediT authorship contribution statement</p>
<p>Declaration of competing interest</p>
<p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
<p>Fig. 1 .
1
Fig. 1.Overall structure of the paper organization.</p>
<p>Fig. 2 .
2
Fig. 2. The evolution of large language models.</p>
<p>Knowledge-Based Systems 318 (2025) 113503</p>
<p>Knowledge-Based Systems 318 (2025) 113503</p>
<p>Wenli Yang :
Yang
Writing -review &amp; editing, Writing -original draft, Visualization, Validation, Supervision, Resources, Methodology, Data curation, Conceptualization.Lilian Some: Writing -original draft, Resources, Methodology, Data curation.Michael Bain: Writing -review &amp; editing, Supervision, Resources.Byeong Kang: Writing -review &amp; editing, Supervision, Methodology, Conceptualization.</p>
<p>Table 1
1
Categorization of Large Language Models (LLMs) based on architecture.
Model typeExamplesPrimary use casesKnowledge integrationRefsbenefitsEncoder-OnlyBERT, RoBERTa,Text classification, retrievalImproves embeddings for[8,26,27]ModernBERTretrieval-augmentedsystemsDecoder-OnlyGPT-4, LLaMA-3, DeepSeekText generation, dialogue,Can be enhanced with[15,17,26,28,LLM, OpenAI-o1creative writingknowledge retrieval29]techniques like RAGEncoder-T5, BART, mBART,DecoderDeltaLM</p>
<p>Table 2
2
Categorization of key capabilities of LLMs.
CategoryKey capabilitiesApplicationsRefsLanguage understanding-Natural language processingGenerating legal, medical, and[48-50]and generation(NLP): Translation,business documents; automatingsummarization, text completion.content for journalism, marketing,and education.-Creative content generation:Storytelling, poetry,conversations.-Document creation andautomated drafting.Reasoning and-Complex reasoning: QuestionMedical diagnostics, fraud[12,29,38,45,decision-makinganswering (QA), logicaldetection, legal case evaluation.51-53]reasoning, problem-solving.-Decision support: Data-driveninsights, strategic planning, riskassessment.Knowledge management-Information retrieval: DataMedical knowledge extraction,[54-59]and retrievalextraction, synthesis,legal case research, AI-poweredsummarization.chatbots, enhancing searchengines.-Knowledge integration:Leveraging external knowledgebases (e.g., ERNIE, E-BERT).Code &amp; program-Code generation: Writing,Intelligent script generation, task[47,60,61]developmentdebugging, optimizingautomation.-Automation: Generating scripts,optimizing workflows.Multimodal processing-Multimodal integration:AI-powered radiology image[62-64]Processing and generating outputsanalysis, video summarization forcombining text, images, audio,media, education, and securityand video.monitoring.Adaptability and learning-Prompt engineering: Few-shotFinancial trend analysis, supply[38,53,59,65,and zero-shot learning,chain optimization, adaptive66]chain-of-thought reasoning.tutoring systems in education.-Domain specialization: Tailoringmodels for industries (FinanceLLMs, Medical LLMs.)Autonomous functionality-Autonomous agents: InteractingAI-driven robotic process[67-71]with environments, planning,automation, AI-powered customerexecuting tasks.service agents.Sectoral impact and-Cross-industry applications:creativityTransforming healthcare,education, finance, research.</p>
<p>Table 3
3
Integration techniques for enhancing LLMs with structured knowledge.
TechniqueDescriptionTypical methodsAdvantagesChallengesBasic Knowledge BasesStructured repositories storingKnowledge Injection,High factual accuracy, reducesStatic, limited reasoning, slow(KBs)predefined facts, relationships,Knowledge-guided Attention,hallucinations, domain-specificupdates, lacks complexand rules.Knowledge Graph Embeddingknowledge (e.g., UMLS,relationship handling.Wikidata)Knowledge Graphs (KGs)Represent knowledge throughGraph Neural Networks,Enhanced reasoning, dynamicMissing links, scalabilityinterconnected nodes (entities)Knowledge Graph Embedding,updates, supports multi-hopissues, complexity,and edges (relationships).KG Prompting, Semi-structuredreasoning, explainability,resource-intensiveChain of Thoughtimproves interpretabilitymaintenance.Retrieval-AugmentedCombines document retrievalDocument Chunking,Real-time access to up-to-dateRequires efficient retrievalGeneration (RAG)with generation, allowingEmbedding Models,information, dynamic context,systems, latency,LLMs to access external dataDense/Sparse Retrieval,alleviates hallucination,computational complexity.in real-time.Knowledge Graph Integration,accurate outputs.Self-ReflectionWhile traditional RAG enhances factual grounding, advanced tech-niques extend beyond simple retrieval by incorporating structuredknowledge graphs, adaptive reasoning frameworks, and self-optimizinglearning mechanisms. This section categorizes LLM enhancement tech-niques into three progressive levels. The Table 4 summarizes the threelevels of advanced integration. Each level highlights specific cate-gories, key applications, and associated typical models, demonstratinghow knowledge integration enhances LLMs for improved retrieval,reasoning, decision-making, and domain-specific optimization.
 Level 1: Knowledge Retrieval &amp; Integration focuses on how LLMs acquire and integrate external knowledge, including structured, unstructured, and multi-modal data. Level 2: Knowledge Utilization &amp; Reasoning covers how LLMs process, synthesize, and apply retrieved knowledge to produce logically consistent and explainable responses. Level 3: Optimization &amp; Specialization: explores adaptive learning techniques, reinforcement-based retrieval refinement, and domain-specific customizations to continuously improve LLM decision-making.</p>
<p>Table 4
4
improves retrieval granularity and Three levels of advanced integration.
Knowledge-Based Systems 318 (2025) 113503</p>
<p>Table 5
5
Comparative analysis of integrating LLMs with knowledge bases.
Knowledge-Based Systems 318 (2025) 113503</p>
<p>Table 6
6
[157][158][159][160][161] RAG-based LLMs[157][158][159][160][161].
RAG-based LLMsDatasetImprovement over baselineKey metricsRefsNatural Questions (NQ)3.4% EM over DPRWebQuestions (WQ)4.1% EM over DPREM, BLEU-1, Q-BLEU-1, ROUGE-L[120]CuratedTrec (CT)1.6% EM over DPRRAG (Seq-Level)TriviaQA (TQA)0.9% EM over DPRJeopardy0.4% B-1 over BART1.7% QB-1 over BARTOpen MS MARCO2.6% R-L over BART2.6% R-L over BARTHotpotQA0.2% EM over RAGEM, F1[131]ReAct8.7% F1 over RAG2WikiMulti-HopQA2.2% EM over RAG2.8% F1 over RAGREALMNatural Questions (NQ)6.9% EM over ORQAEM, Recall[158]24.6% Recall over ORQANatural Questions (NQ)2.25% F1 over RAGF1 Score[151]HotpotQA1.17% F1 over RAGGraphRAG (local)SQuALITY0.02% ROUGE-2 over RAG7.04% BERTScore overROUGE-2, BERTScoreRAGQMSum0.02% ROUGE-2 over RAG0.06% BERTScore overRAGT-RAGCustomized dataset (1,614 QA pairs)16.2% Perc. over RAGPerc. of correct responses, NIH Test[150]23.07% NIH (k=10) overRAGNatural Questions (NQ)8.2% EM over QRQADPR-RAGTriviaQA (TQA)11.8% EM over QRQAEM[159]WebQuestions (WQ)6% EM over QRQAMoE-RAGGoogle BIG-Bench0.07% F1 over RAGF1[160]</p>
<p>Table 7
7
Performance comparison of KG-based LLMs.
KG-based LLMsDatasetImprovement over baselineKey metricsRefsCWQ10.1% Hits@1 over GPT-418.7% F1 over GPT-4Hits@1, F1[135]KGAgentWebQSP16.6% Hits@1 over GPT-419.9% F1 over GPT-423.7% recall@5 over RAGHotpotQA18.5% F1 over RAG19.1% EM over RAGKAG2WikiMulti-HopQA6.2% recall@5 over RAG 32.9% F1 over RAGExact Match (EM), F1, Recall@5[137]34.4% EM over RAG16.5% recall@5 over RAGMuSiQue22.3% F1 over RAG21.2% EM over RAGCWQ21.2% accuracy over CoTAccuracy[130]ToGWebQSP14.2% accuracy over CoTMIMIC0.13 s generation time over GPT4o-minisame F1 with GPT4o-miniSynthea6.45% F1 over GPT4o-miniF1, generation time[156]KG-RAG4SM0.09 s generation time overGPT4o-miniCMS1.67% F1 over GPT4o-mini0.01 s generation time overGPT4o-miniEMED0.08% F1 over GPT4o-mini0.06 s generation time overGPT4o-miniOIE20164.3% F1 over PIVE2.1% AUC over PIVEF1, AUC[138]WEB6.8% F1 over PIVE9.7% AUC over PIVESAC-KGNYT5.3% F1 over PIVE9.4% AUC over PIVEPENN5.1% F1 over PIVE8.9% AUC over PIVE
Knowledge-Based Systems 318 (2025) 113503
Data availabilityNo data was used for the research described in the article.
Large language models: A survey. Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, Jianfeng Gao, 2024</p>
<p>Zichong Wang, Zhibo Chu, Thang Viet Doan, Shiwen Ni, Min Yang, Wenbin Zhang, History, development, and principles of large language models: an introductory survey. 2024</p>
<p>Abhijit Dasgupta, Debashis Guha, A literature survey on open source large language models. Sanjay Kukreja, Tarun Kumar, Amit Purohit, 2024</p>
<p>Automating research synthesis with domain-specific large language model fine-tuning. Teo Susnjak, Peter Hwang, Napoleon H Reyes, L C Andre, Timothy R Barczak, Surangika Mcintosh, Ranathunga, 2024</p>
<p>The inadequacy of reinforcement learning from human feedbackradicalizing large language models via semantic vulnerabilities. Timothy R Mcintosh, Teo Susnjak, Tong Liu, Paul Watters, Malka N Halgamuge, 2024</p>
<p>Evaluation of transfer learning and adaptability in large language models with the glue benchmark. Nuraini Sulaiman, Farizal Hamzah, 2024</p>
<p>A review on large language models: Architectures, applications, taxonomies, open issues and challenges. Mohaimenul Azam Khan Raiaan, Md Saddam Hossain Mukta, Kaniz Fatema, Nur Mohammad Fahad, Sadman Sakib, Most Marufatul, Jannat Mim, Jubaer Ahmad, Mohammed Eunus Ali, Sami Azam, IEEE Access. 122024</p>
<p>Combining knowledge graphs and large language models. Amanda Kau, Xuzeng He, Aishwarya Nambissan, Aland Astudillo, Hui Yin, Amir Aryani, arXiv:2407.065642024</p>
<p>Towards efficient large language models for scientific text: A review. Quoc Huy, Ming To, Guangyan Liu, Huang, arXiv:2408.107292024</p>
<p>Qasem Muhammad Usman Hadi, Rizwan Al Tashi, Abbas Qureshi, Amgad Shah, Muhammad Muneer, Anas Irfan, Muhammad Zafar, Naveed Bilal Shaikh, Jia Akhtar, Seyedali Wu, Mirjalili, A survey on large language models: Applications, challenges, limitations, and practical usage. 2023</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 20231arXiv preprint</p>
<p>A survey of large language models in finance (FinLLMs. Jean Lee, Nicholas Stevens, Caren Soyeon, Minseok Han, Song, arXiv:2402.023152024cs, q-fin</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 2018</p>
<p>ChatGPT for good? On opportunities and challenges of large language models for education. Enkelejda Kasneci, Kathrin Sessler, Stefan Kchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Gnnemann, Eyke Hllermeier, Stephan Krusche, Gitta Kutyniok, Tilman Michaeli, Claudia Nerdel, Jrgen Pfeffer, Oleksandra Poquet, Michael Sailer, Albrecht Schmidt, Tina Seidel, Matthias Stadler, Jochen Weller, Jochen Kuhn, Gjergji Kasneci, Learn. Individ. Differ. 1031022742023</p>
<p>A systematic literature review on large language models for automated program repair. Quanjun Zhang, Chunrong Fang, Yang Xie, Yuxiang Ma, Weisong Sun, Yun Yang, Zhenyu Chen, arXiv:2405.014662024</p>
<p>Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, arXiv:2401.02954DeepSeek LLM: Scaling open-source language models with longtermism. 2024arXiv preprint</p>
<p>Deepseek-coder: When the large language model meets programming-the rise of code intelligence. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, Y K Li, arXiv:2401.141962024arXiv preprint</p>
<p>DeepSeekMoE: Towards ultimate expert specialization in mixture-of-experts language models. Damai Dai, Chengqi Deng, Chenggang Zhao, R X Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Yu Wu, arXiv:2401.060662024arXiv preprint</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024arXiv preprint</p>
<p>A survey of graph retrieval-augmented generation for customized large language models. Qinggang Zhang, Shengyuan Chen, Yuanchen Bei, Zheng Yuan, Huachi Zhou, Zijin Hong, Junnan Dong, Hao Chen, Yi Chang, Xiao Huang, arXiv:2501.139582025arXiv preprint</p>
<p>Learning to retrieve and reason on knowledge graph through active self-reflection. Han Zhang, Langshi Zhou, Hanfang Yang, arXiv:2502.149322025arXiv preprint</p>
<p>A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, ACM Trans. Inf. Syst. 4322025</p>
<p>Tianyang Zhong, Zhengliang Liu, Yi Pan, Yutong Zhang, Yifan Zhou, Shizhe Liang, Zihao Wu, Yanjun Lyu, Peng Shu, Xiaowei Yu, arXiv:2409.18486Evaluation of OpenAI o1: Opportunities and challenges of AGI. 2024arXiv preprint</p>
<p>Domain adaptation of LLMs: A study of content generation, RAG, and fine-tuning. Erik Keanius, 2024</p>
<p>BioRAG: A RAG-LLM framework for biological question reasoning. Chengrui Wang, Qingqing Long, Meng Xiao, Xunxin Cai, Chengjun Wu, Zhen Meng, Xuezhi Wang, Yuanchun Zhou, arXiv:2408.011072024</p>
<p>Recent advances in generative AI and large language models: Current status, challenges, and perspectives. Rick Desta Haileselassie Hagos, Battle, B Danda, Rawat, arXiv:2407.149622024</p>
<p>Smarter, better, faster, longer: A modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference. Benjamin Warner, Antoine Chaffin, Benjamin Clavi, Orion Weller, Oskar Hallstrm, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, arXiv:2412.136632024arXiv preprint</p>
<p>Gpts are gpts: An early look at the labor market impact potential of large language models. Tyna Eloundou, Sam Manning, Pamela Mishkin, Daniel Rock, 2023</p>
<p>LLatrieval: LLM-verified retrieval for verifiable generation. Xiaonan Li, Changtai Zhu, Linyang Li, Zhangyue Yin, Tianxiang Sun, Xipeng Qiu, arXiv:2311.078382024</p>
<p>Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer, arXiv:1910.134612019BartarXiv preprint</p>
<p>Deltalm: Encoder-decoder pre-training for language generation and translation by augmenting pretrained multilingual encoders. Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, Alexandre Muzio, Saksham Singhal, Hany Hassan Awadalla, Xia Song, Furu Wei, arXiv:2106.137362021arXiv preprint</p>
<p>Large language models (llms): survey, technical frameworks, and future challenges. Pranjal Kumar, Artif. Intell. Rev. 57102602024</p>
<p>A review on large language models: Architectures, applications, taxonomies, open issues and challenges. Mohaimenul Azam Khan Raiaan, Md Saddam Hossain Mukta, Kaniz Fatema, Nur Mohammad Fahad, Sadman Sakib, Most Marufatul, Jannat Mim, Jubaer Ahmad, Mohammed Eunus Ali, Sami Azam, IEEE Access. 122024</p>
<p>Reformer: The efficient transformer. N Kitaev, 2020</p>
<p>Ernie-doc: A retrospective long-document modeling transformer. Siyu Ding, Junyuan Shang, Shuohuan Wang, Yu Sun, Hua Hao Tian, Haifeng Wu, Wang, arXiv:2012.156882020arXiv preprint</p>
<p>Retrieval-augmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang, arXiv:2312.109972024</p>
<p>A survey on augmenting knowledge graphs (kgs) with large language models (llms): models, evaluation metrics, benchmarks, and challenges. Nourhan Ibrahim, Samar Aboulela, Ahmed Ibrahim, Rasha Kashef, Discov. Artif. Intell. 41762024</p>
<p>Empowering generative ai with knowledge base 4.0: Towards linking analytical, cognitive, and generative intelligence. Amin Beheshti, 2023</p>
<p>A survey on large language models with some insights on their capabilities and limitations. Andrea Matarazzo, Riccardo Torlone, arXiv:2501.040402025arXiv preprint</p>
<p>Taiyu Zhang, Xuesong Zhang, Robbe Cools, Adalberto L Simeone, arXiv:2409.01907Focus agent: LLM-powered virtual focus group. 2024</p>
<p>Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, Qing Li, arXiv:2405.06211A survey on RAG meeting LLMs: Towards retrieval-augmented large language models. 2024</p>
<p>Lilian Some, Wenli Yang, Michael Bain, Byeong Kang, arXiv:2501.13947A comprehensive survey on integrating large language models with knowledge-based methods. 2025arXiv preprint</p>
<p>LLMs meet multimodal generation and editing: A survey. Yingqing He, Zhaoyang Liu, Jingye Chen, Zeyue Tian, Hongyu Liu, Xiaowei Chi, Runtao Liu, Ruibin Yuan, Yazhou Xing, Wenhai Wang, Jifeng Dai, Yong Zhang, Wei Xue, Qifeng Liu, Yike Guo, Qifeng Chen, arXiv:2405.193342024</p>
<p>Large language models for anomaly and out-ofdistribution detection: A survey. Ruiyao Xu, Kaize Ding, arXiv:2409.019802024</p>
<p>Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, arXiv:2310.01061Shirui Pan, Reasoning on graphs: Faithful and interpretable large language model reasoning. 2024</p>
<p>The prompt report: A systematic survey of prompting techniques. Sander Schulhoff, Michael Ilie, Nishant Balepur, Konstantine Kahadze, Amanda Liu, Chenglei Si, Yinheng Li, Aayush Gupta, Hyojung Han, Sevien Schulhoff, arXiv:2406.066082024arXiv preprint</p>
<p>LLM agents can autonomously exploit one-day vulnerabilities. Richard Fang, Rohan Bindu, Akul Gupta, Daniel Kang, arXiv:2404.081442024</p>
<p>Let's get to the point: Llm-supported planning, drafting, and revising of research-paper blog posts. Marissa Radensky, Daniel S Weld, Joseph Chee Chang, Pao Siangliulue, Jonathan Bragg, arXiv:2406.103702024arXiv preprint</p>
<p>Xiaofei Sun, Xiaoya Li, Shengyu Zhang, Shuhe Wang, Fei Wu, Jiwei Li, Tianwei Zhang, Guoyin Wang, arXiv:2311.01876Sentiment analysis through llm negotiations. 2023arXiv preprint</p>
<p>A review of current trends, techniques, and challenges in large language models (llms). Rajvardhan Patil, Venkat Gudivada, Appl. Sci. 14520742024</p>
<p>A survey of large language models for financial applications: Progress, prospects and challenges. Yuqi Nie, Yaxuan Kong, Xiaowen Dong, John M Mulvey, H Vincent Poor, Qingsong Wen, Stefan Zohren, arXiv:2406.119032024q-fin</p>
<p>Integrating umls knowledge into large language models for medical question answering. Rui Yang, Edison Marrese-Taylor, Yuhe Ke, Lechao Cheng, Qingyu Chen, Irene Li, 2023</p>
<p>Large language models in healthcare and medical domain: A review. Al Zabir, Wei Nazi, Peng, Informatics2024MDPI1157</p>
<p>Large language models in medicine. Arun James Thirunavukarasu, Darren Shu, Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Fang Ting, Daniel Tan, Wei Shu, Ting, Nature Med. 2982023</p>
<p>A view of how language models will transform law. Frank Fagan, Tenn. Law Rev. 9212024</p>
<p>Large language models in finance: A survey. Yinheng Li, Shaofei Wang, Han Ding, Hang Chen, Proceedings of the Fourth ACM International Conference on AI in Finance. the Fourth ACM International Conference on AI in Finance2023</p>
<p>Leveraging medical knowledge graphs into large language models for diagnosis prediction: Design and application study. Yanjun Gao, Ruizhe Li, Emma Croxford, John Caskey, Brian W Patterson, Matthew Churpek, Timothy Miller, Dmitriy Dligach, Majid Afshar, JMIR AI. 4e586702025</p>
<p>Jurex-4e: Juridical expert-annotated four-element knowledge base for legal reasoning. Huanghai Liu, Quzhe Huang, Qingjing Chen, Yiran Hu, Jiayu Ma, Yun Liu, Weixing Shen, Yansong Feng, arXiv:2502.171662025arXiv preprint</p>
<p>Large language models in finance (finllms). Jean Lee, Nicholas Stevens, Caren Soyeon, Han, Neural Comput. Appl. 2025</p>
<p>Towards an understanding of large language models in software engineering tasks. Zibin Zheng, Kaiwen Ning, Qingyuan Zhong, Jiachi Chen, Wenqing Chen, Lianghong Guo, Weicheng Wang, Yanlin Wang, Empir. Softw. Eng. 302502025</p>
<p>Language models for code optimization: Survey, challenges and future directions. Jingzhi Gong, Vardan Voskanyan, Paul Brookes, Fan Wu, Wei Jie, Jie Xu, Rafail Giavrimis, Mike Basios, Leslie Kanthan, Zheng Wang, arXiv:2501.012772025arXiv preprint</p>
<p>Multimodal large language models: A survey. Jiayang Wu, Wensheng Gan, Zefeng Chen, S Yu Shicheng Wan, Philip, 2023 IEEE International Conference on Big Data. IEEE2023</p>
<p>From large language models to large multimodal models: A literature review. Dawei Huang, Chuan Yan, Qing Li, Xiaojiang Peng, Appl. Sci. 141250682024</p>
<p>A survey of multimodel large language models. Zijing Liang, Yanjie Xu, Yifan Hong, Penghui Shang, Qi Wang, Qiang Fu, Ke Liu, Proceedings of the 3rd International Conference on Computer. the 3rd International Conference on Computer2024</p>
<p>Adaptive intelligence: Gpt-powered language models for dynamic responses to emerging healthcare challenges. Karthik Meduri, Hari Gonaygunta, Geeta Sandeep Nadella, Priyanka Pramod Pawar, Deepak Kumar, IJARCCE. 132024</p>
<p>Medical mt5: an open-source multilingual text-to-text llm for the medical domain. Iker Garca-Ferrero, Rodrigo Agerri, Atutxa Aitziber, Elena Salazar, Iker Cabrio, Alberto De La Iglesia, Bernardo Lavelli, Benjamin Magnini, Johana Molinet, German Ramirez-Romero, Rigau, arXiv:2404.076132024arXiv preprint</p>
<p>Large language models for software engineering: Survey and open problems. Angela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho Sengupta, Shin Yoo, Jie M Zhang, 2023 IEEE/ACM International Conference on Software Engineering: Future of Software Engineering, ICSE-FoSE. IEEE2023</p>
<p>Large language models for robotics: Opportunities, challenges, and perspectives. Jiaqi Wang, Enze Shi, Huawen Hu, Chong Ma, Yiheng Liu, Xuhui Wang, Yincheng Yao, Xuan Liu, Bao Ge, Shu Zhang, J. Autom. Intell. 2024</p>
<p>Christophe Van Gysel, Synthetic query generation using large language models for virtual assistants. Sonal Sannigrahi, Thiago Fraga-Silva, Youssef Oualil, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024</p>
<p>Large language model-based chatbots in higher education. Defne Yigci, Merve Eryilmaz, Ail K Yetisen, Savas Tasoglu, Aydogan Ozcan, Adv. Intell. Syst. 24004292024</p>
<p>Ting-Chi Chang, Yu-Jou Chen, Sheng Hung, Ning-Hsuan, Chih-Hao Chang, Szu-Yin Ku, Shih-Yi Lin, Chien, A review on shaping chatbot personalities via large language models. 2025</p>
<p>Foundation and large language models: fundamentals, challenges, opportunities, and social impacts. Devon Myers, Rami Mohawesh, Ishwarya Venkata, Anantha Chellaboina, Praveen Lakshmi Sathvik, Yi-Hui Venkatesh, Hanna Ho, Muna Henshaw, David Alhawawreh, Yaser Berdik, Jararweh, Clust. Comput. 2712024</p>
<p>Exploring the potential of large language models in artistic creation: Collaboration and reflection on creative programming. Anqi Wang, Zhizhuo Yin, Yulu Hu, Yuanyuan Mao, Pan Hui, arXiv:2402.09750Knowledge-Based Systems. 3181135032024. 2025arXiv preprint</p>
<p>A framework for collaborating a large language model tool in brainstorming for triggering creative thoughts. Hung-Fu Chang, Tong Li, Think. Ski. Creativity. 1017552025</p>
<p>Large language models: from entertainment to solutions. Robin Qiu, Digit. Transform. Soc. 322024</p>
<p>Biomedical knowledge graph-optimized prompt generation for large language models. Karthik Soman, Peter W Rose, John H Morris, Rabia E Akbas, Brett Smith, Braian Peetoom, Catalina Villouta-Reyes, Gabriel Cerono, Yongmei Shi, Angela Rizk-Jackson, Sharat Israni, Charlotte A Nelson, Sui Huang, Sergio E Baranzini, Bioinformatics. 409e5602024</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe, 2022</p>
<p>Lift yourself up: Retrieval-augmented text generation with self memory. Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, Rui Yan, 2023</p>
<p>Jing-min Yang, Applications of large models in medicine. Yunhe Su, Zhengyang Lu, Junhui Liu, Ke Pang, Haoran Dai, Sa Liu Yuxin, Lujia Jia, Ge, arXiv:2502.171322025arXiv preprint</p>
<p>Triple-aware reasoning: A retrievalaugmented generationapproach for enhancing question-answering tasks withknowledge graphs and large language models. M Omair Hongzhi Zhang, Shafiq, 2024</p>
<p>Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Jian Guo, arXiv:2407.10805Thinkon-graph 2.0: Deep and interpretable large language model reasoning with knowledge graph-guided retrieval. 2024</p>
<p>REALM: RAGdriven enhancement of multimodal electronic health records analysis via large language models. Yinghao Zhu, Changyu Ren, Shiyun Xie, Shukai Liu, Hangyuan Ji, Zixiang Wang, Tao Sun, Long He, Zhoujun Li, Xi Zhu, Chengwei Pan, arXiv:2402.070162024</p>
<p>Unveiling the statistical foundations of chain-of-thought prompting methods. Xinyang Hu, Fengzhuo Zhang, Siyu Chen, Zhuoran Yang, arXiv:2408.145112024cs, math, stat</p>
<p>Sadam Alabed Alrazak, Javaid Sheikh, Large language models in medical education: Opportunities, challenges, and future directions. Alaa Abd-Alrazaq, Rawan Alsaad, Dari Alhuwail, Arfan Ahmed, Padraig Mark Healy, Syed Latifi, Sarah Aziz, Rafat Damseh, JMIR Med. Educ. 9e482912023</p>
<p>Topological or not? A unified pattern description in the one-dimensional anisotropic quantum XY model with a transverse field. Yun-Tong Yang, Hong-Gang Luo, arXiv:2302.138662023cond-mat</p>
<p>Pruning and quantization for deep neural network acceleration: A survey. Tailin Liang, John Glossner, Lei Wang, Shaobo Shi, Xiaotong Zhang, 2021</p>
<p>Knowledge enhanced pretrained language models: A compreshensive survey. Xiaokai Wei, Shen Wang, Dejiao Zhang, Parminder Bhatia, Andrew Arnold, arXiv:2110.084552021</p>
<p>Sbastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, Sparks of artificial general intelligence: Early experiments with gpt-4. 2023</p>
<p>Training compute-optimal large language models. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, arXiv:2203.155562022arXiv preprint</p>
<p>Benchmarking large language models in retrieval-augmented generation. Jiawei Chen, Hongyu Lin, Xianpei Han, Le Sun, Proc. AAAI Conf. AAAI Conf202438</p>
<p>KRAGEN: a knowledge graphenhanced RAG framework for biomedical problem solving using large language models. Nicholas Matsumoto, Jay Moran, Hyunjun Choi, Miguel E Hernandez, Mythreye Venkatesan, Paul Wang, Jason H Moore, Bioinformatics. 406e3532024</p>
<p>Integrating generative AI in knowledge building. Bodong Chen, Xinran Zhu, Fernando Daz, Del Castillo, H , ID: 4388599367Comput. Educ.: Artif. Intell. 51001842023</p>
<p>Navigating challenges and technical debt in large language models deployment. Ahmed Menshawy, Zeeshan Nawaz, Mahmoud Fahmy, Proceedings of the 4th Workshop on Machine Learning and Systems. the 4th Workshop on Machine Learning and Systems2024</p>
<p>Semi-structured chain-ofthought: Integrating multiple sources of knowledge for improved language model reasoning. Xin Su, Tiep Le, Steven Bethard, Phillip Howard, arXiv:2311.085052024</p>
<p>Rag does not work for enterprises. Tilmann Bruckhaus, 2024</p>
<p>HuggingFace's transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rmi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Mariama Gugger, Drame, 2019Quentin Lhoest, Alexander M. Rush</p>
<p>Pas: Data-efficient plug-and-play prompt augmentation system. Miao Zheng, Hao Liang, Fan Yang, Haoze Sun, Tianpeng Li, Lingchu Xiong, Yan Zhang, Youzhen Wu, Kun Li, Yanjun Shen, Mingan Lin, Tao Zhang, Guosheng Dong, Yujing Qiao, Kun Fang, Weipeng Chen, Bin Cui, Wentao Zhang, Zenan Zhou, arXiv:2407.060272024</p>
<p>An, A multimodal foundation agent for financial trading: Tool-augmented, diversified, and generalist. Wentao Zhang, Lingxuan Zhao, Haochong Xia, Shuo Sun, Jiaze Sun, Molei Qin, Xinyi Li, Yuqing Zhao, Yilei Zhao, Xinyu Cai, Longtao Zheng, Xinrun Wang, arXiv:2402.184852024Boq-fin</p>
<p>Active retrieval augmented generation. Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig, arXiv:2305.069832023</p>
<p>Trustworthy image semantic communication with GenAI: Explainablity, controllability, and efficiency. Xijun Wang, Dongshan Ye, Chenyuan Feng, Howard H Yang, Xiang Chen, Tony Q S Quek, S2ID: 3e5473ecb44e13e6bb08b477623ab39da551943b2024</p>
<p>Generative AI and ChatGPT: Applications, challenges, and AI-human collaboration. Fiona Fui-Hoon, Ruilin Nah, Jingyuan Zheng, Keng Cai, Langtao Siau, Chen, J. Inf. Technol. Case Appl. Res. 2532023</p>
<p>. Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, 2014Generative adversarial networks</p>
<p>Auto-encoding variational bayes. D P Kingma, M Welling, 2013</p>
<p>Knowledge graphs. Aidan Hogan, Eva Blomqvist, Michael Cochez, Claudia Amato, Gerard De Melo, Claudio Gutierrez, Jos Emilio Labra, Sabrina Gayo, Sebastian Kirrane, Axel Neumaier, Roberto Polleres, Axel-Cyrille Ngonga Navigli, Ngomo, M Sabbir, Anisa Rashid, Lukas Rula, Juan Schmelzeisen, Steffen Sequeda, Antoine Staab, Zimmermann, arXiv:2003.02320ACM Comput. Surv. 5442022</p>
<p>Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models. Minbyul Jeong, Jiwoong Sohn, Mujeen Sung, Jaewoo Kang, arXiv:2401.152692024</p>
<p>From turing to transformers: A comprehensive review and tutorial on the evolution and applications of generative transformer models. Emma Yann Zhang, Adrian David Cheok, Zhigeng Pan, Jun Cai, Ying Yan, Sci. 54462023</p>
<p>Peizhong Gao, Ao Xie, Shaoguang Mao, Wenshan Wu, Yan Xia, Haipeng Mi, Furu Wei, arXiv:2406.11698Meta reasoning for large language models. 2024</p>
<p>Augmentation of large language model capabilities with knowledge graphs. Dr, Kasnesis, 2024</p>
<p>Weknow-RAG: An adaptive approach for retrieval-augmented generation integrating web search and knowledge graphs. Weijian Xie, Xuefeng Liang, Yuhui Liu, Kaihua Ni, Hong Cheng, Zetian Hu, arXiv:2408.076112024</p>
<p>Boosting of thoughts: Trial-and-error problem solving with large language models. Sijia Chen, Baochun Li, Di Niu, arXiv:2402.111402024</p>
<p>How to build self-explaining fuzzy systems: From interpretability to explainability. Ilia Stepin, Muhammad Suffian, Alejandro Catal, J Alonso-Moral, S2ID: 1d5127686d01fe1e66805c10f19284f8654846322024AI-eXplained</p>
<p>Buffer of thoughts: Thought-augmented reasoning with large language models. Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph E Gonzalez, Bin Cui, arXiv:2406.042712024</p>
<p>On the role of the umls in supporting diagnosis generation proposed by large language models. Yanjun Majid Afshar, Deepak Gao, Emma Gupta, Dina Croxford, Demner-Fushman, J. Biomed. Inform. 1571047072024</p>
<p>Rdiger Daub, Daub knowledge graph enhanced retrieval-augmented generation for failure mode and effects analysis. Lukas Bahr, Christoph Wehner, Judith Wewerka, Jos Bittencourt, Ute Schmid, arXiv:2406.181142024</p>
<p>Can knowledge graphs reduce hallucinations in LLMs? : A survey. Garima Agrawal, Tharindu Kumarage, Zeyad Alghamdi, Huan Liu, arXiv:2311.079142024</p>
<p>Knowledge graphs as context sources for LLM-based explanations of learning recommendations. Christian Hasan Abu-Rasheed, Madjid Weber, Fathi, arXiv:2403.030082024</p>
<p>Unifying large language models and knowledge graphs: A roadmap. Linhao Shirui Pan, Yufei Luo, Chen Wang, Jiapu Chen, Xindong Wang, Wu, arXiv:2306.08302IEEE Trans. Knowl. Data Eng. 3672024</p>
<p>Graphbased retriever captures the long tail of biomedical knowledge. Julien Delile, Srayanta Mukherjee, Anton Van Pamel, Leonid Zhukov, arXiv:2402.123522024</p>
<p>The chronicles of RAG: The retriever, the chunk and the generator. Paulo Finardi, Leonardo Avila, Rodrigo Castaldoni, Pedro Gengo, Celio Larcher, Marcos Piau, Pablo Costa, Vinicius Carid, arXiv:2401.078832024</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kttler, Mike Lewis, Wen-Tau Yih, Tim Rocktschel, Adv. Neural Inf. Process. Syst. 332020</p>
<p>Medical graph RAG: Towards safe medical large language model via graph retrieval-augmented generation. Junde Wu, Jiayuan Zhu, Yunli Qi, arXiv:2408.041872024</p>
<p>Strategic chain-of-thought: Guiding accurate reasoning in LLMs through strategy elicitation. Yu Wang, Shiwan Zhao, Zhihu Wang, Heyuan Huang, Ming Fan, Yubo Zhang, Zhixing Wang, Haijun Wang, Ting Liu, arXiv:2409.032712024</p>
<p>TRACE the evidence: Constructing knowledge-grounded reasoning chains for retrieval-augmented generation. Jinyuan Fang, Zaiqiao Meng, Craig Macdonald, arXiv:2406.114602024</p>
<p>. Knowledge-Based Systems. 3181135032025</p>
<p>Rag at large enterprises. Davit Janezashvili, 2024. 25 October 2024</p>
<p>BlendFilter: Advancing retrieval-augmented large language models via query generation blending and knowledge filtering. Haoyu Wang, Ruirui Li, Haoming Jiang, Jinjin Tian, Zhengyang Wang, Chen Luo, Xianfeng Tang, Monica Cheng, Tuo Zhao, Jing Gao, arXiv:2402.111292024</p>
<p>Metacognitive retrieval-augmented large language models. Yujia Zhou, Zheng Liu, Jiajie Jin, Jian-Yun Nie, Zhicheng Dou, arXiv:2402.116262024</p>
<p>Masoomali Fatehkia, Ji Kim Lucas, Sanjay Chawla, arXiv:2402.07483Lessons from the LLM trenches. 2024</p>
<p>From local to global: A graph rag approach to query-focused summarization. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, Jonathan Larson, arXiv:2404.161302024arXiv preprint</p>
<p>Flexkbqa: A flexible llm-powered framework for few-shot knowledge base question answering. Zhenyu Li, Sunqi Fan, Yu Gu, Xiuxing Li, Zhichao Duan, Bowen Dong, Ning Liu, Jianyong Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Lionel M Ni, Heung-Yeung Shum, Jian Guo, arXiv:2307.07697Think-on-graph: Deep and responsible reasoning of large language model on knowledge graph. 2023arXiv preprint</p>
<p>ReAct: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.036292023</p>
<p>Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Jian Guo, Thinkon-graph 2.0: Deep and interpretable large language model reasoning with knowledge graph-guided retrieval. 20242407arXiv e-prints</p>
<p>Chain-of-knowledge: Integrating knowledge reasoning into large language models by learning from knowledge graphs. Yifei Zhang, Xintao Wang, Jiaqing Liang, Sirui Xia, Lida Chen, Yanghua Xiao, arXiv:2407.006532024arXiv preprint</p>
<p>Weknow-rag: An adaptive approach for retrieval-augmented generation integrating web search and knowledge graphs. Weijian Xie, Xuefeng Liang, Yuhui Liu, Kaihua Ni, Hong Cheng, Zetian Hu, arXiv:2408.076112024arXiv preprint</p>
<p>Kg-agent: An efficient autonomous agent framework for complex reasoning over knowledge graph. Jinhao Jiang, Kun Zhou, Wayne Xin Zhao, Yang Song, Chen Zhu, Hengshu Zhu, Ji-Rong Wen, arXiv:2402.111632024arXiv preprint</p>
<p>Coarse-tofine highlighting: Reducing knowledge hallucination in large language models. Qitan Lv, Jie Wang, Hanzhu Chen, Bin Li, Yongdong Zhang, Feng Wu, arXiv:2410.151162024arXiv preprint</p>
<p>Kag: Boosting llms in professional domains via knowledge augmented generation. Lei Liang, Mengshu Sun, Zhengke Gui, Zhongshu Zhu, Zhouyu Jiang, Ling Zhong, Yuan Qu, Peilong Zhao, Zhongpu Bo, Jin Yang, arXiv:2409.137312024arXiv preprint</p>
<p>Sac-kg: Exploiting large language models as skilled automatic constructors for domain knowledge graphs. Hanzhu Chen, Xu Shen, Qitan Lv, Jie Wang, Xiaoqi Ni, Jieping Ye, arXiv:2410.028112024arXiv preprint</p>
<p>Small models, big insights: Leveraging slim proxy models to decide when and what to retrieve for LLMs. Jiejun Tan, Zhicheng Dou, Yutao Zhu, Peidong Guo, Kun Fang, Ji-Rong Wen, arXiv:2402.120522024</p>
<p>Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, Weizhu Chen, arXiv:2305.152942023</p>
<p>RA-ISF: Learning to answer and understand from retrieval augmentation via iterative self-feedback. Yanming Liu, Xinyue Peng, Xuhong Zhang, Weihao Liu, Jianwei Yin, Jiannan Cao, Tianyu Du, arXiv:2403.068402024</p>
<p>Retrieval-augmented generation with knowledge graphs for customer service question answering. Zhentao Xu, Mark Jerome Cruz, Matthew Guevara, Tie Wang, Manasi Deshpande, Xiaofeng Wang, Zheng Li, arXiv:2404.177232024</p>
<p>Don't forget to connect! improving RAG with graph-based reranking. Jialin Dong, Bahare Fatemi, Bryan Perozzi, Lin F Yang, Anton Tsitsulin, arXiv:2405.184142024</p>
<p>ToRA: A tool-integrated reasoning agent for mathematical problem solving. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, arXiv:2309.174522024</p>
<p>Large language model based multi-agents: A survey of progress and challenges. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, 2024</p>
<p>Embracing large language models for medical applications: Opportunities and challenges. M Karabacak, K Margetis, Cureus. 155e393052023</p>
<p>From matching to generation: A survey on generative information retrieval. Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yuyao Zhang, Peitian Zhang, Yutao Zhu, Zhicheng Dou, arXiv:2404.148512024</p>
<p>Almanac -retrieval-augmented language models for clinical medicine. C Zakka, R Shad, A Chaurasia, A R Dalal, J L Kim, M Moor, R Fong, C Phillips, K Alexander, E Ashley, J Boyd, K Boyd, K Hirsch, C Langlotz, R Lee, J Melia, J Nelson, K Sallam, S Tullis, M A Vogelsong, W Hiesinger, 2024</p>
<p>Unveiling llm evaluation focused on metrics: Challenges and solutions. Taojun Hu, Xiao-Hua Zhou, arXiv:2404.091352024arXiv preprint</p>
<p>Masoomali Fatehkia, Ji Kim Lucas, Sanjay Chawla, arXiv:2402.07483T-rag: lessons from the llm trenches. 2024arXiv preprint</p>
<p>Jiliang Tang, Rag vs. graphrag: A systematic evaluation and key insights. Haoyu Han, Harry Shomer, Yu Wang, Yongjia Lei, Kai Guo, Zhigang Hua, Bo Long, Hui Liu, arXiv:2502.113712025arXiv preprint</p>
<p>Aligning with logic: Measuring, evaluating and improving logical consistency in large language models. Yinhong Liu, Zhijiang Guo, Tianya Liang, Ehsan Shareghi, Ivan Vuli, Nigel Collier, arXiv:2410.022052024arXiv preprint</p>
<p>Evaluation of retrieval-augmented generation: A survey. Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, Zhaofeng Liu, CCF Conference on Big Data. Springer2024</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, ACM Trans. Intell. Syst. Technol. 1532024</p>
<p>Rouge-ss: A new rouge variant for evaluation of text summarization. Sandeep Kumar, Arun Solanki, 2023Authorea Preprints</p>
<p>Knowledge graph-based retrieval-augmented generation for schema matching. Chuangtao Ma, Sriom Chakrabarti, Arijit Khan, Blint Molnr, arXiv:2501.086862025arXiv preprint</p>
<p>Realm: reference resolution as language modeling. Joel Ruben, Antony Moniz, Soundarya Krishnan, Melis Ozyildirim, Prathamesh Saraf, Cagri Halim, Yuan Ates, Hong Zhang, Yu, arXiv:2403.203292024arXiv preprint</p>
<p>Realm: Rag-driven enhancement of multimodal electronic health records analysis via large language models. Yinghao Zhu, Changyu Ren, Shiyun Xie, Shukai Liu, Hangyuan Ji, Zixiang Wang, Tao Sun, Long He, Zhoujun Li, Xi Zhu, arXiv:2402.070162024arXiv preprint</p>
<p>Benjamin Reichman, Larry Heck, arXiv:2402.11035Dense passage retrieval: Is it retrieving? 2024. arXiv preprint</p>
<p>Merging mixture of experts and retrieval augmented generation for enhanced information retrieval and reasoning. Xingyu Xiong, Mingliang Zheng, 2024</p>
<p>Funnelrag: A coarse-to-fine progressive retrieval paradigm for rag. Xinping Zhao, Yan Zhong, Zetian Sun, Xinshuo Hu, Zhenyu Liu, Dongfang Li, Baotian Hu, Min Zhang, arXiv:2410.102932024arXiv preprint</p>
<p>On the role of the umls in supporting diagnosis generation proposed by large language models. Yanjun Majid Afshar, Deepak Gao, Emma Gupta, Dina Croxford, Demner-Fushman, J. Biomed. Inform. 1571047072024</p>
<p>The unified medical language system (umls): integrating biomedical terminology. Olivier Bodenreider, Nucleic Acids Res. 32suppl_12004</p>
<p>A performance study of llm-generated code on leetcode. Tristan Coignion, Clment Quinton, Romain Rouvoy, 2024</p>
<p>Unilog: Automatic logging via llm and in-context learning. Junjielong Xu, Ziang Cui, Yuan Zhao, Xu Zhang, Shilin He, Pinjia He, Liqun Li, Yu Kang, Qingwei Lin, Yingnong Dang, Saravan Rajmohan, Dongmei Zhang, 2024</p>
<p>The impact of github copilot on developer productivity from a software engineering body of knowledge perspective. Danie Smit, Hanlie Smuts, Paul Louw, Julia Pielmeier, Christina Eidelloth, 2024</p>
<p>ChatGPT is not all you need. A state of the art review of large generative AI models. Roberto Gozalo-Brizuela, Eduardo C Garrido-Merchan, 2023</p>
<p>Neuro-symbolic integration brings causal and reliable reasoning proofs. Sen Yang, Xin Li, Leyang Cui, Lidong Bing, Wai Lam, arXiv:2311.098022023arXiv preprint</p>
<p>Decoding in-context learning: Neuroscience-inspired analysis of representations in large language models. Safoora Yousefi, Leo Betthauser, Hosein Hasanbeig, Raphal Millire, Ida Momennejad, arXiv:2310.003132023arXiv preprint</p>
<p>Large language model based multiagents: A survey of progress and challenges. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, arXiv:2402.016802024arXiv preprint</p>
<p>Selftaught optimizer (stop): Recursively self-improving code generation. Eric Zelikman, Eliana Lorch, Lester Mackey, Adam Tauman Kalai, First Conference on Language Modeling. 2024</p>
<p>Refine-lm: Mitigating language model stereotypes via reinforcement learning. Rameez Qureshi, Nam Es-Sebbani, Luis Galrraga, Yvette Graham, Miguel Couceiro, Zied Bouraoui, ECAI 2024. IOS Press2024. 2025318113503</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2270 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2270</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2270</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-118877157</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1707.04593v1.pdf" target="_blank">Unveiling two types of local order in liquid water using machine learning</a></p>
                <p><strong>Paper Abstract:</strong> Machine learning methods are being explored in many areas of science, with the aim of finding solution to problems that evade traditional scientific approaches due to their complexity. In general, an order parameter capable of identifying two different phases of matter separated by a correspond- ing phase transition is constructed based on symmetry arguments. This parameter measures the degree of order as the phase transition proceeds. However, when the two distinct phases are highly disordered it is not trivial to identify broken symmetries with which to find an order parameter. This poses an excellent problem to be addressed using machine learning procedures. Room tem- perature liquid water is hypothesized to be a supercritical liquid, with fluctuations of two different molecular orders associated to two parent liquid phases, one with high density and another one with low density. The validity of this hypothesis is linked to the existence of an order parameter capable of identifying the two distinct liquid phases and their fluctuations. In this work we show how two different machine learning procedures are capable of recognizing local order in liquid water. We argue that when in order to learn relevant features from this complexity, an initial, physically motivated preparation of the available data is as important as the quality of the data set, and that machine learning can become a successful analysis tool only when coupled to high level physical information.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2270.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2270.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Supervised ML local-order</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Supervised machine learning workflow to learn a local order parameter for liquid water (SVM + DNN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised ML pipeline that learns a local order parameter O(x) mapping local geometric features of individual water molecules to binary classes (low-density LD vs high-density HD) derived from inherent-structure Local Structure Index (LSI) thresholds; implemented with Support Vector Machines (RBF kernel) and feed-forward Deep Neural Networks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Condensed-matter / molecular simulation — local structural classification of liquid water</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Identify and classify two hypothesized molecular local environments (LD and HD) in liquid water by learning a local order parameter from simulation data that maps finite-temperature molecular geometries to their corresponding inherent-structure class (binary labels derived from inherent LSI).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Abundant within the simulated ensemble: MD on 128-molecule cells at seven temperatures produced ~1,792,000 initial data points aggregated across temperatures (256,000 per temperature), from which balanced labeled subsets were sampled for training/validation/testing; labels are supervised (binary) obtained by thresholding the inherent-structure LSI (I0 = 0.13 Å^2). Data quality is high (classical MD with TTM3-F, relaxation to inherent structures), but finite-temperature noise and class imbalance required balancing, sampling, and time-averaging; data are accessible within the simulation workflow but are synthetic (simulation-derived) rather than experimental.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured numerical feature vectors derived from atomic coordinates: high-dimensional continuous features including intermolecular distances (up to 17 O–O distances), angular descriptors (O–O–O, O–H–O angles), Voronoi-derived scalar descriptors (inverse volume, asphericity, polygon counts), tetrahedral order parameters (q, Sk), local structure index (LSI), H-bond counts and loop topology — essentially multivariate numeric vectors, with optionally short-time series/time-averaged versions (temporal dimension).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: nonlinear classification in a high-dimensional feature space with substantial thermal noise and short-time fluctuations; mapping from noisy finite-temperature geometries to inherent-structure classes requires learning nonlinear decision boundaries and coping with correlated features, variable importance across feature subsets, and temperature-dependent degradation of separability; search/feature space includes dozens of features and combinations; classification is sensitive to feature selection and time-averaging.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Moderately mature but active/debated: liquid-water local-structure analysis has a long history (LSI and multiple local order parameters exist) and extensive prior computational and experimental literature, but the LL (liquid-liquid) hypothesis and interpretation of bimodality remain contested, motivating new analysis tools; strong domain expertise and physical priors available.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — interpretability and physically meaningful order parameters are required: the goal is scientific insight (identify physically interpretable local order), so models must be interpretable or trained on physically-motivated features rather than being pure black boxes.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Support Vector Machine (RBF kernel) and Feed-forward Deep Neural Network (DNN)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Supervised classification pipeline. Targets: binary labels (τ = +1 LD, τ = −1 HD) assigned from inherent-structure LSI threshold I0 = 0.13 Å^2 after energy minimization (inherent structures). Features: various combinations (feature sets A–E) of Voronoi metrics, tetrahedral order parameters, LSI, first 17 O–O distances, 5 O–O–O angles, 6 O–H–O angles, H-bond counts and loop counts. SVM: Gaussian (RBF) kernel, hyperparameters C (box penalty) and σ (RBF width) tuned on validation grid; soft-margin formulation used to allow misclassification. DNN: feed-forward network trained with batch gradient descent (TensorFlow), architectures explored; for inherent-structure classification a 4-hidden-layer network with sizes [80, 100, 200, 75] was used; for per-temperature FT training a smaller 2-layer network [80, 30] was used to avoid overfitting. Training/validation/testing splits described (balanced datasets, e.g., 20k train/20k val/20k test for SVM; larger training for DNN using batch training). Time-averaging of features and/or targets over short windows (∆t = 100 fs, 250 fs) was also applied to reduce short-time noise.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning (classification)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and appropriate for learning a discriminative local order parameter when (i) labels are available from a physically-motivated order parameter on inherent structures, and (ii) feature vectors include the relevant intermolecular distances; limitations include sensitivity to thermal noise (finite-temperature → inherent mapping), feature irrelevance/noise (irrelevant features can degrade accuracy), requirement for balanced labeled data, and need for physical feature engineering; time-averaging and physically-informed label construction improved applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td>Inherent-structure classification (aggregated across temperatures) — Table II: Feature sets A–E, SVM accuracies: A 0.96, B 0.74, C 0.78, D 0.99, E 0.97; DNN accuracies: A 0.97, B 0.82, C 0.82, D 0.99, E 0.98. Finite-temperature classification (aggregated) — Table III: SVM accuracies: A 0.69, B 0.65, C 0.67, D 0.64, E 0.67; DNN accuracies: A 0.74, B 0.65, C 0.63, D 0.64, E 0.69. Baseline: finite-temperature LSI alone yields accuracy ≈ 0.60. Time-averaging improvements: short-time averaging of features (∆t = 100 fs or 250 fs) increased DNN accuracy by ≈5% at low T; time-averaging both features and target (inherent LSI) increased accuracy by ≈7–10% across temperatures.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Worked very well for inherent (relaxed) structures when features included first-neighbor distances (near-perfect classification ≈99%); mapping from finite-temperature instantaneous geometries to inherent classes is substantially harder (accuracies <75%) because thermal fluctuations obscure the inherent-class signal. Including more features not directly relevant (e.g., some Voronoi/H-bond topology metrics) can introduce noise and reduce accuracy; DNN and SVM gave comparable performance, with SVM simpler and interpretable (kernel + support vectors) and DNN requiring more data/hyperparameter tuning. Time-averaging and constructing temporally-averaged targets mitigated short-time fluctuations and improved performance.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for scientific analysis tools: provides a flexible, data-driven yet physically-grounded way to define local order parameters in complex disordered systems (liquids, interfaces, extreme conditions); can resolve or at least quantify contributions of thermal noise vs. inherent bimodality, inform debates on water's two-state hypothesis, and be generalized to other materials; downstream potential includes automated local-structure classification in large-scale simulations, aiding interpretation of spectroscopy and scattering experiments; limitations: reliance on simulated labels and sensitivity to thermal averaging reduce direct applicability to instantaneous experimental snapshots.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared directly to the original LSI baseline (finite-temperature LSI gave ~0.60 accuracy) and to different handcrafted feature subsets (A–E) showing that inclusion of first-neighbor distances (feature set D) is critical for near-perfect classification of inherent structures; SVM vs DNN were empirically similar on this task (both nearly identical on IS, DNN slightly better on some FT cases), and including extra, less relevant features sometimes degraded performance relative to minimal informative sets. The paper also notes prior ML studies (CNN on Ising and unsupervised ML for H-bonds) but does not directly compare methods beyond the described experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Key success factors: physically-motivated feature engineering (especially inclusion of first-neighbor intermolecular distances), use of inherent-structure labels derived from energy minimization (reduces configurational noise for IS classification), proper dataset balancing to avoid class bias, hyperparameter tuning (SVM grid search), and mitigating short-time thermal fluctuations via time averaging of features/targets. Failures/limitations arose when irrelevant/noisy features were added or when attempting to classify noisy finite-temperature instantanous structures without temporal smoothing.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Supervised ML can accurately learn a physically meaningful local order parameter for inherent (relaxed) molecular structures when given physically-informed features (notably first-neighbor distances), but thermal fluctuations in finite-temperature structures substantially limit instantaneous predictability — temporal averaging or models that incorporate time correlations are necessary to bridge finite-temperature geometries to inherent-state labels.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2270.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2270.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CNN on Ising (ref)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Convolutional neural network classification of Monte Carlo-drawn Ising configurations (referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior work where a convolutional neural network was trained to classify Monte Carlo-drawn configurations of two-dimensional Ising models above and below the critical point, demonstrating ML ability to detect phase transitions from raw configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Statistical physics / phase classification (Ising model)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Detect phase (ordered vs disordered) from raw Monte Carlo configurations of the 2D Ising model using a convolutional neural network.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Not specified in this paper beyond the mention; Monte Carlo-generated labeled configurations typically abundant and labeled by simulation temperature.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured 2D lattice spin configurations (image-like grid data).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate: detection of phase boundary in lattice models; nonlinear spatial correlations important near criticality.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Mature computational/statistical physics domain with established Monte Carlo methods and analytic background; ML applications are emerging.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium — ML used to detect phases; understanding ML internal features desirable but not strictly required.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Convolutional Neural Network (CNN)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>CNN trained on 2D spin-lattice configurations to classify phases above/below critical temperature; details not provided in this paper (referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Supervised learning (deep learning)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Mentioned as an example that ML can identify phases in simulated physical systems.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Reported to successfully classify Ising configurations across the critical point (as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Demonstrates potential of ML to detect phase transitions from raw data, motivating similar approaches in complex 3D systems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>CNNs exploit spatially-local convolutional filters matching the translational symmetry of lattice data.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Convolutional architectures can learn spatially-local features sufficient to classify phases in lattice models, supporting ML use in physics phase-identification tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2270.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2270.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Unsupervised H-bond ML (ref)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unsupervised machine learning definition of hydrogen bonds (referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior study where unsupervised learning was used to construct a data-driven, agnostic structural definition of the hydrogen bond solely from structural information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Molecular structure analysis / hydrogen-bond detection</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Infer an agnostic, data-driven structural definition of hydrogen bonds in water, ice and ammonia using unsupervised learning on structural features.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Not specified here; referenced work likely used simulation data with abundant unlabeled structural snapshots.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Structured geometric descriptors from molecular configurations (distances, angles — multivariate numeric data).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate to high: hydrogen bonding is a continuous geometric concept lacking an absolute threshold; goal is to discover clusters/patterns in high-dimensional structural descriptors.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Mature domain with many ad-hoc H-bond definitions; this work represents an unsupervised, data-driven refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High — definitions sought for interpretability and physical insight rather than black-box predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Unsupervised machine learning (clustering / pattern recognition)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Unsupervised learning algorithms (not specified in detail here) applied to structural descriptors to identify cluster(s) corresponding to hydrogen-bonded configurations, yielding an agnostic structural definition.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>Unsupervised learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Mentioned as a successful example in condensed-matter/molecular systems where ML found physically-meaningful structural motifs without labels.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Cited as successful in producing an agnostic, data-driven hydrogen-bond definition that generalizes across phases/species.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Significant — demonstrates unsupervised ML can discover chemically/physically meaningful structural concepts and informs supervised approaches that require labels.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Using relevant structural features and unsupervised clustering allowed discovery of robust structural definitions without labelled data.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Unsupervised ML can derive physically-interpretable structural motifs (e.g., H-bonds) from raw geometrical data, supporting ML as a discovery tool in molecular systems.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Machine learning phases of matter <em>(Rating: 2)</em></li>
                <li>Recognizing molecular patterns by machine learning: An agnostic structural definition of the hydrogen bond <em>(Rating: 2)</em></li>
                <li>Machine learning phases of matter <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2270",
    "paper_id": "paper-118877157",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "Supervised ML local-order",
            "name_full": "Supervised machine learning workflow to learn a local order parameter for liquid water (SVM + DNN)",
            "brief_description": "A supervised ML pipeline that learns a local order parameter O(x) mapping local geometric features of individual water molecules to binary classes (low-density LD vs high-density HD) derived from inherent-structure Local Structure Index (LSI) thresholds; implemented with Support Vector Machines (RBF kernel) and feed-forward Deep Neural Networks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Condensed-matter / molecular simulation — local structural classification of liquid water",
            "problem_description": "Identify and classify two hypothesized molecular local environments (LD and HD) in liquid water by learning a local order parameter from simulation data that maps finite-temperature molecular geometries to their corresponding inherent-structure class (binary labels derived from inherent LSI).",
            "data_availability": "Abundant within the simulated ensemble: MD on 128-molecule cells at seven temperatures produced ~1,792,000 initial data points aggregated across temperatures (256,000 per temperature), from which balanced labeled subsets were sampled for training/validation/testing; labels are supervised (binary) obtained by thresholding the inherent-structure LSI (I0 = 0.13 Å^2). Data quality is high (classical MD with TTM3-F, relaxation to inherent structures), but finite-temperature noise and class imbalance required balancing, sampling, and time-averaging; data are accessible within the simulation workflow but are synthetic (simulation-derived) rather than experimental.",
            "data_structure": "Structured numerical feature vectors derived from atomic coordinates: high-dimensional continuous features including intermolecular distances (up to 17 O–O distances), angular descriptors (O–O–O, O–H–O angles), Voronoi-derived scalar descriptors (inverse volume, asphericity, polygon counts), tetrahedral order parameters (q, Sk), local structure index (LSI), H-bond counts and loop topology — essentially multivariate numeric vectors, with optionally short-time series/time-averaged versions (temporal dimension).",
            "problem_complexity": "High: nonlinear classification in a high-dimensional feature space with substantial thermal noise and short-time fluctuations; mapping from noisy finite-temperature geometries to inherent-structure classes requires learning nonlinear decision boundaries and coping with correlated features, variable importance across feature subsets, and temperature-dependent degradation of separability; search/feature space includes dozens of features and combinations; classification is sensitive to feature selection and time-averaging.",
            "domain_maturity": "Moderately mature but active/debated: liquid-water local-structure analysis has a long history (LSI and multiple local order parameters exist) and extensive prior computational and experimental literature, but the LL (liquid-liquid) hypothesis and interpretation of bimodality remain contested, motivating new analysis tools; strong domain expertise and physical priors available.",
            "mechanistic_understanding_requirements": "High — interpretability and physically meaningful order parameters are required: the goal is scientific insight (identify physically interpretable local order), so models must be interpretable or trained on physically-motivated features rather than being pure black boxes.",
            "ai_methodology_name": "Support Vector Machine (RBF kernel) and Feed-forward Deep Neural Network (DNN)",
            "ai_methodology_description": "Supervised classification pipeline. Targets: binary labels (τ = +1 LD, τ = −1 HD) assigned from inherent-structure LSI threshold I0 = 0.13 Å^2 after energy minimization (inherent structures). Features: various combinations (feature sets A–E) of Voronoi metrics, tetrahedral order parameters, LSI, first 17 O–O distances, 5 O–O–O angles, 6 O–H–O angles, H-bond counts and loop counts. SVM: Gaussian (RBF) kernel, hyperparameters C (box penalty) and σ (RBF width) tuned on validation grid; soft-margin formulation used to allow misclassification. DNN: feed-forward network trained with batch gradient descent (TensorFlow), architectures explored; for inherent-structure classification a 4-hidden-layer network with sizes [80, 100, 200, 75] was used; for per-temperature FT training a smaller 2-layer network [80, 30] was used to avoid overfitting. Training/validation/testing splits described (balanced datasets, e.g., 20k train/20k val/20k test for SVM; larger training for DNN using batch training). Time-averaging of features and/or targets over short windows (∆t = 100 fs, 250 fs) was also applied to reduce short-time noise.",
            "ai_methodology_category": "Supervised learning (classification)",
            "applicability": "Applicable and appropriate for learning a discriminative local order parameter when (i) labels are available from a physically-motivated order parameter on inherent structures, and (ii) feature vectors include the relevant intermolecular distances; limitations include sensitivity to thermal noise (finite-temperature → inherent mapping), feature irrelevance/noise (irrelevant features can degrade accuracy), requirement for balanced labeled data, and need for physical feature engineering; time-averaging and physically-informed label construction improved applicability.",
            "effectiveness_quantitative": "Inherent-structure classification (aggregated across temperatures) — Table II: Feature sets A–E, SVM accuracies: A 0.96, B 0.74, C 0.78, D 0.99, E 0.97; DNN accuracies: A 0.97, B 0.82, C 0.82, D 0.99, E 0.98. Finite-temperature classification (aggregated) — Table III: SVM accuracies: A 0.69, B 0.65, C 0.67, D 0.64, E 0.67; DNN accuracies: A 0.74, B 0.65, C 0.63, D 0.64, E 0.69. Baseline: finite-temperature LSI alone yields accuracy ≈ 0.60. Time-averaging improvements: short-time averaging of features (∆t = 100 fs or 250 fs) increased DNN accuracy by ≈5% at low T; time-averaging both features and target (inherent LSI) increased accuracy by ≈7–10% across temperatures.",
            "effectiveness_qualitative": "Worked very well for inherent (relaxed) structures when features included first-neighbor distances (near-perfect classification ≈99%); mapping from finite-temperature instantaneous geometries to inherent classes is substantially harder (accuracies &lt;75%) because thermal fluctuations obscure the inherent-class signal. Including more features not directly relevant (e.g., some Voronoi/H-bond topology metrics) can introduce noise and reduce accuracy; DNN and SVM gave comparable performance, with SVM simpler and interpretable (kernel + support vectors) and DNN requiring more data/hyperparameter tuning. Time-averaging and constructing temporally-averaged targets mitigated short-time fluctuations and improved performance.",
            "impact_potential": "High for scientific analysis tools: provides a flexible, data-driven yet physically-grounded way to define local order parameters in complex disordered systems (liquids, interfaces, extreme conditions); can resolve or at least quantify contributions of thermal noise vs. inherent bimodality, inform debates on water's two-state hypothesis, and be generalized to other materials; downstream potential includes automated local-structure classification in large-scale simulations, aiding interpretation of spectroscopy and scattering experiments; limitations: reliance on simulated labels and sensitivity to thermal averaging reduce direct applicability to instantaneous experimental snapshots.",
            "comparison_to_alternatives": "Compared directly to the original LSI baseline (finite-temperature LSI gave ~0.60 accuracy) and to different handcrafted feature subsets (A–E) showing that inclusion of first-neighbor distances (feature set D) is critical for near-perfect classification of inherent structures; SVM vs DNN were empirically similar on this task (both nearly identical on IS, DNN slightly better on some FT cases), and including extra, less relevant features sometimes degraded performance relative to minimal informative sets. The paper also notes prior ML studies (CNN on Ising and unsupervised ML for H-bonds) but does not directly compare methods beyond the described experiments.",
            "success_factors": "Key success factors: physically-motivated feature engineering (especially inclusion of first-neighbor intermolecular distances), use of inherent-structure labels derived from energy minimization (reduces configurational noise for IS classification), proper dataset balancing to avoid class bias, hyperparameter tuning (SVM grid search), and mitigating short-time thermal fluctuations via time averaging of features/targets. Failures/limitations arose when irrelevant/noisy features were added or when attempting to classify noisy finite-temperature instantanous structures without temporal smoothing.",
            "key_insight": "Supervised ML can accurately learn a physically meaningful local order parameter for inherent (relaxed) molecular structures when given physically-informed features (notably first-neighbor distances), but thermal fluctuations in finite-temperature structures substantially limit instantaneous predictability — temporal averaging or models that incorporate time correlations are necessary to bridge finite-temperature geometries to inherent-state labels.",
            "uuid": "e2270.0"
        },
        {
            "name_short": "CNN on Ising (ref)",
            "name_full": "Convolutional neural network classification of Monte Carlo-drawn Ising configurations (referenced work)",
            "brief_description": "Referenced prior work where a convolutional neural network was trained to classify Monte Carlo-drawn configurations of two-dimensional Ising models above and below the critical point, demonstrating ML ability to detect phase transitions from raw configurations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Statistical physics / phase classification (Ising model)",
            "problem_description": "Detect phase (ordered vs disordered) from raw Monte Carlo configurations of the 2D Ising model using a convolutional neural network.",
            "data_availability": "Not specified in this paper beyond the mention; Monte Carlo-generated labeled configurations typically abundant and labeled by simulation temperature.",
            "data_structure": "Structured 2D lattice spin configurations (image-like grid data).",
            "problem_complexity": "Moderate: detection of phase boundary in lattice models; nonlinear spatial correlations important near criticality.",
            "domain_maturity": "Mature computational/statistical physics domain with established Monte Carlo methods and analytic background; ML applications are emerging.",
            "mechanistic_understanding_requirements": "Medium — ML used to detect phases; understanding ML internal features desirable but not strictly required.",
            "ai_methodology_name": "Convolutional Neural Network (CNN)",
            "ai_methodology_description": "CNN trained on 2D spin-lattice configurations to classify phases above/below critical temperature; details not provided in this paper (referenced).",
            "ai_methodology_category": "Supervised learning (deep learning)",
            "applicability": "Mentioned as an example that ML can identify phases in simulated physical systems.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Reported to successfully classify Ising configurations across the critical point (as cited).",
            "impact_potential": "Demonstrates potential of ML to detect phase transitions from raw data, motivating similar approaches in complex 3D systems.",
            "comparison_to_alternatives": null,
            "success_factors": "CNNs exploit spatially-local convolutional filters matching the translational symmetry of lattice data.",
            "key_insight": "Convolutional architectures can learn spatially-local features sufficient to classify phases in lattice models, supporting ML use in physics phase-identification tasks.",
            "uuid": "e2270.1"
        },
        {
            "name_short": "Unsupervised H-bond ML (ref)",
            "name_full": "Unsupervised machine learning definition of hydrogen bonds (referenced work)",
            "brief_description": "Referenced prior study where unsupervised learning was used to construct a data-driven, agnostic structural definition of the hydrogen bond solely from structural information.",
            "citation_title": "",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Molecular structure analysis / hydrogen-bond detection",
            "problem_description": "Infer an agnostic, data-driven structural definition of hydrogen bonds in water, ice and ammonia using unsupervised learning on structural features.",
            "data_availability": "Not specified here; referenced work likely used simulation data with abundant unlabeled structural snapshots.",
            "data_structure": "Structured geometric descriptors from molecular configurations (distances, angles — multivariate numeric data).",
            "problem_complexity": "Moderate to high: hydrogen bonding is a continuous geometric concept lacking an absolute threshold; goal is to discover clusters/patterns in high-dimensional structural descriptors.",
            "domain_maturity": "Mature domain with many ad-hoc H-bond definitions; this work represents an unsupervised, data-driven refinement.",
            "mechanistic_understanding_requirements": "High — definitions sought for interpretability and physical insight rather than black-box predictions.",
            "ai_methodology_name": "Unsupervised machine learning (clustering / pattern recognition)",
            "ai_methodology_description": "Unsupervised learning algorithms (not specified in detail here) applied to structural descriptors to identify cluster(s) corresponding to hydrogen-bonded configurations, yielding an agnostic structural definition.",
            "ai_methodology_category": "Unsupervised learning",
            "applicability": "Mentioned as a successful example in condensed-matter/molecular systems where ML found physically-meaningful structural motifs without labels.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Cited as successful in producing an agnostic, data-driven hydrogen-bond definition that generalizes across phases/species.",
            "impact_potential": "Significant — demonstrates unsupervised ML can discover chemically/physically meaningful structural concepts and informs supervised approaches that require labels.",
            "comparison_to_alternatives": null,
            "success_factors": "Using relevant structural features and unsupervised clustering allowed discovery of robust structural definitions without labelled data.",
            "key_insight": "Unsupervised ML can derive physically-interpretable structural motifs (e.g., H-bonds) from raw geometrical data, supporting ML as a discovery tool in molecular systems.",
            "uuid": "e2270.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Machine learning phases of matter",
            "rating": 2,
            "sanitized_title": "machine_learning_phases_of_matter"
        },
        {
            "paper_title": "Recognizing molecular patterns by machine learning: An agnostic structural definition of the hydrogen bond",
            "rating": 2,
            "sanitized_title": "recognizing_molecular_patterns_by_machine_learning_an_agnostic_structural_definition_of_the_hydrogen_bond"
        },
        {
            "paper_title": "Machine learning phases of matter",
            "rating": 1,
            "sanitized_title": "machine_learning_phases_of_matter"
        }
    ],
    "cost": 0.01415075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Unveiling two types of local order in liquid water using machine learning</p>
<p>Adrián Soto 
Department of Physics and Astronomy
Stony Brook University
11794-3800Stony BrookNY</p>
<p>Institute for Advanced Computational Science
Stony Brook University
11794Stony BrookNY</p>
<p>Deyu Lu 
Center for Functional Nanomaterials
Brookhaven National Laboratory
Upton11793NY</p>
<p>Shinjae Yoo 
Institute for Advanced Computational Science
Stony Brook University
11794Stony BrookNY</p>
<p>Computational Science Initiative
Brookhaven National Laboratory
Upton11793NY</p>
<p>Mariví Fernández-Serra 
Department of Physics and Astronomy
Stony Brook University
11794-3800Stony BrookNY</p>
<p>Institute for Advanced Computational Science
Stony Brook University
11794Stony BrookNY</p>
<p>Unveiling two types of local order in liquid water using machine learning</p>
<p>Machine learning methods are being explored in many areas of science, with the aim of finding solution to problems that evade traditional scientific approaches due to their complexity. In general, an order parameter capable of identifying two different phases of matter separated by a corresponding phase transition is constructed based on symmetry arguments. This parameter measures the degree of order as the phase transition proceeds. However, when the two distinct phases are highly disordered it is not trivial to identify broken symmetries with which to find an order parameter. This poses an excellent problem to be addressed using machine learning procedures. Room temperature liquid water is hypothesized to be a supercritical liquid, with fluctuations of two different molecular orders associated to two parent liquid phases, one with high density and another one with low density. The validity of this hypothesis is linked to the existence of an order parameter capable of identifying the two distinct liquid phases and their fluctuations. In this work we show how two different machine learning procedures are capable of recognizing local order in liquid water. We argue that when in order to learn relevant features from this complexity, an initial, physically motivated preparation of the available data is as important as the quality of the data set, and that machine learning can become a successful analysis tool only when coupled to high level physical information.arXiv:1707.04593v1 [cond-mat.soft]</p>
<p>I. INTRODUCTION</p>
<p>Perhaps the most important open debate about the physics of water is the existence of a liquid-liquid phase transition, hypothesized 25 years ago [1]. The existence of this phase transition with a critical point at supercooled temperatures and high pressures provides a mechanism to explain some of the anomalous macroscopic properties of water, such as the decrease of the heat capacity and the isothermal compressibility of water at ambient conditions upon heating [2] or the anomalous behavior of the density with temperature, reaching a maximum at 4 • C and showing a negative coefficient of thermal expansion below this temperature into the supercooled regime [3]. In this picture, water molecules above the critical temperature fluctuate between a low density, enthalpy-favored environment with a highly tetrahedral coordination and a high density, entropy-favored environment with highly distorted structures (we adopt the standard nomenclature low density/high density (LD/HD) for the former/latter respectively). The ratio of the HD to LD populations increases with temperature, yielding a normal liquid behavior at high temperatures but an anomalous behavior from the vicinity of ambient conditions down in temperature into the supercooled region [4], where the populations of the two types of molecules becomes comparable.</p>
<p>Since it was proposed, this hypothesis has motivated intensive experimental and computational investigations * adrian.soto-cambres@stonybrook.edu † dlu@bnl.gov ‡ syoo@bnl.gov § maria.fernandez-serra@stonybrook.edu on the existence of a dual microscopic nature of water. X-ray spectroscopy experiments have shown that pre-and post-edge peaks, which characterize distorted H-bonds and strong H-bonds respectively, have a distribution whose temperature dependence agrees with the above hypothesis [5]. Recently, novel X-ray experiments observed a continuous transition between HD liquid and LD liquid at ambient pressure and temperatures between 110 K and 130 K, agreeing with the hypothesis of a first order liquid-liquid phase transition at elevated pressures [6]. Molecular dynamics (MD) simulation studies of liquid water also suggest this trend [7][8][9]. Furthermore, recent studies [10,11] show that the inherent structures (IS) of MD trajectories of water, which are obtained by relaxing the finite temperature structures to their corresponding potential energy minima, display a bimodal distribution of the Local Structure Index (LSI) [12]. The sizes of the modes of the distribution vary with temperature and pressure of the finite temperature (unrelaxed) simulation, supporting the hypothesis of the existence of two molecular environments even at temperatures well above the predicted critical temperature.</p>
<p>This prevaling view, however, has been heavily contested. A lack of evidence of heterogeneity in simulations [13][14][15][16][17] and experiments [14,18] has been pointed out, attributing the seemingly biphasic character to noncritical thermal fluctuations. Over the past couple of decades the community has been putting great effort into developing local order parameters for water. These capture different properties such as packing, tetrahedral arrangement and angular distribution of the first few neighboring molecules [12,[19][20][21]. While the LSI is bimodal at the IS, none of them has successfully shown the expected bimodality when evaluated at the finite temperature struc-tures near ambient conditions. Therefore the extraction of local order parameters in liquid water presents an ideal problem to treat with machine learning due to its capability of unveiling information from complex data. Recent studies have successfully utilized machine learning in condensed matter physics to study structure and order: in Ref. [22] a convolutional neural network was trained to successfully classify Monte Carlo-drawn configurations of twodimensional Ising models above and below the critical point, and in Ref. [23] unsupervised learning was used to construct an agnostic, data-driven definition of a hydrogen bond in ice, water and ammonia based only on structural information. These developments prove that machine learning can be used for phase identification as well as for finding subtleties in the geometrical structure of complex three-dimensional systems.</p>
<p>Inspired by these studies, in this paper we propose a method for finding local order parameters with machine learning which, if successful, will provide a more flexible means to classify local structures of complex materials in bulk but also in more challenging scenarios such as near interfaces or at extreme thermodynamic conditions. We then turn our attention to liquid water, where we employ this machine learning methodology to study whether the IS are truly revealing two types of molecular arrangements based on local geometrical information. Finally we discuss the effects of statistical fluctuations.</p>
<p>II. METHODS</p>
<p>A. Formalism</p>
<p>Let r t = ( r 1,t , r 2,t , · · · , r n,t ) be the molecular (or atomic) coordinates at simulation at step t [24]. In the following, we drop the step index t and write it explicitly when necessary. Let us define a feature vector x i = x i ( r) of dimension D whose components contain information about the geometry of molecule i and its environment. In addition, a target value that characterizes the distinct molecular environments needs to be provided in order to perform supervised machine learning. Both the feature vector and the target should be specified from educated physical intuition. These two elements can then be combined to train a machine learning algorithm and machine learn a local order parameter O. A schematic representation of this procedure is shown in Fig. 1.</p>
<p>Let us now focus now on the procedure that will be employed in the rest of this work to obtain the target values. Assume that we know a local order parameter, O i ( r), function of the molecular coordinates only, whose probability distribution P (Õ( r)) is bimodal, thereby allowing to interpret each of the modes of the distribution as a distinct class of local order. Based on this, one could attempt to machine learn a function O( x i ) that approximates the order parameterÕ i ( r). Notice that O is an explicit function of the local features x only and implicitly of the atomic coordinates r [25]. Alternatively, instead of a continuous order parameter one could learn a binary output function, with one output value for each of the two peaks of the probability distribution P (O). With an appropriate feature vector x, a machine learning algorithm can find the desired approximate order parameter O provided that the local features contain the necessary information.</p>
<p>In order to bring this formalism to practice the following aspects need to be properly regarded: (i) appropriate sampling of the thermodynamic ensemble, (ii) feature set choice that contains enough information to describe the binary character of the molecular environment, and (iii) a suitable machine learning model, training scheme and hyperparameter optimization. In the following we describe how we approach each of those items.</p>
<p>B. Ensemble sampling with molecular dynamics</p>
<p>We performed a series of classical MD simulations in the canonical (NVT) ensemble at ambient density (1.0 kg/l) and at seven different temperatures. We employed the TTM3-F force field [26], which is a flexible and polarizable water model fitted to MP2/aug-cc-pVDZ calculations of water clusters, yielding very good agreement with experiment in the structure and the vibrational spectra of liquid water at T = 298 K. Our simulations were performed using a cubic periodic cell of length L = 15.64 Ang containing 128 molecules. This cell allows us to study correlations of up to ∼ 7.8 Ang, sufficient for our study of local order in the 6.0 Ang range. We control the temperature by means of a Nosé-Hoover thermostat [27,28].</p>
<p>All of our MD productive runs are 50 ps long with a Verlet integration step of 0.5 fs following 50 ps of equilibration. The simulations were performed with our own MD code [29].</p>
<p>From each of those simulations 2000 snapshots were relaxed using the conjugate gradient method [30], converging the maximum atomic force below 5 meV/Ang. I values for distorted coordination shells and high LSI lues for very regular coordination shells. We calculate the inherent LSI, the LSI on the relaxed omic coordinates, at several di↵erent simulation temratures. The inherent LSI distribution in Fig. 1 shows bimodal character. As it can be seen in Fig. ?? this is t the case for other suggested order parameters. The herent LSI distribution shows a decreasing LD populaon with increasing temperature, and it has been argued at each mode characterizes a distinct class of molecar environment suggesting that the LSI at the IS is a eful local order parameter[4, 9,10]. Following these udies we propose to use inherent structures of water to aracterize their corresponding finite temperature strucres. Since the location of the maximum separating the o peaks of P (I) does not change substantially with the mulation temperature, we can apply a unique threshd value I 0 to separate the two modes, each representg their HD or LD character respectively (dashed lines Fig. 1). This provides a way to assign binary target lues of 1 or +1 to each data point enabling us to perrm supervised machine learning. In this work we choose e threshold value to be at the minimum between the o modes of the distribution, I 0 = 0.13 Ang 2 , in agreeent with similar studies with TIP4P/Ice force field and A B C D E I y y y n n II y y y n n III y/n y/n y/n n n IV y n n 6 n.n. y V &amp; VI y n n n y VII y n y n n  [32,33], where the fir tion ensures that the two molecules are within coordination shell of the pair distribution func the second that the molecules are properly al form an H-bond.</p>
<p>In Table I are listed the feature selections we lized in this study. They correspond order pa only, atomic coordinates only and a combination types of features. We select di↵erent combina these in order to assess which features are most to a successful description of the molecular envir E. Supervised machine learning order fun Once the feature vectorsx i and their corres target values ⌧ i are generated, we machine lea </p>
<p>C. Learning order from inherent structures</p>
<p>The partition function of a classical liquid can be written (up to a constant kinetic prefactor) as [31] 
Z N ∼ α 1 σ α e −βVα Rα d 3N r e −β∆Vα( r) ,(1)
where the sum runs over all the local minima of the potential energy surface, α. V α is the potential energy at minimum α, and ∆ α V ( r) = V ( r) − V α is the energy difference above the local minimum α to which the system configuration with coordinates r would relax. Each integral is performed over the coordinate space subvolume R α which is the coordinate relaxation basin of attraction of the points r α . β = 1/k B T is the usual inverse temperature and σ α is a translational symmetry number (in our case, since we have periodic boundary conditions, α σ σ = N particles ). The coordinates at the local minima of the potential energy are the IS and they play a crucial role in our analysis as we shall see shortly.</p>
<p>The LSI is defined as
I = 1 N s Ns j=i (δ j+1,j − δ ) 2 (2)
where the sum is carried out over the N s pairs of molecules within a cutoff shell of radius r c = 3.7 Ang. δ j+1,j = r j+1 −r j is the difference in radii of the shells occupied by molecules j and j+1, and δ = 1</p>
<p>Ns</p>
<p>Ns j=i δ j+1,j is the average radii difference. This quantity is designed to measure the degree to which first and second H-bond coordination shells in water are well-defined, with low LSI values for distorted coordination shells and high LSI values for very regular coordination shells.</p>
<p>We calculate the inherent LSI, the LSI on the relaxed atomic coordinates, at several different simulation tem-peratures. While the finite temperature (FT) LSI distribution is unimodal, the inherent LSI distribution in Fig. 2 shows a bimodal character. As it can be seen in Fig. 3 this is not the case for other suggested order parameters. The inherent LSI distribution shows a decreasing LD population with increasing temperature, and it has been argued that each mode characterizes a distinct class of molecular environment suggesting that the inherent LSI is a useful local order parameter[4, 10,11]. Following these studies we propose to use inherent structures of water to characterize their corresponding finite temperature structures. Since the location of the maximum separating the two peaks of P (I) does not change substantially with the simulation temperature, we can apply a unique threshold value I 0 to separate the two modes, each representing their HD or LD character respectively (dashed lines in Fig. 2). This provides a way to assign binary target values of −1 (HD) or +1 (LD) to each data point, enabling us to perform supervised machine learning. In this work we choose the threshold value to be at the minimum between the two modes of the distribution, I 0 = 0.13 Ang 2 , in agreement with similar studies with TIP4P/Ice force field and slightly below values I 0 = 0.14 − 0.16 Ang 2 obtained from AIMD [11].</p>
<p>D. Molecular features and target values</p>
<p>From the atomic coordinates of the MD trajectories we compute the local features of all the molecules in our sample and aggregate them into feature vectors x i . In this work we construct our feature vectors with the following local geometrical quantities: (I) inverse volume ρ, asphericity η, number of sides s, edges e and vertices v of the Voronoi cell of each molecule, determined by the position of its O atom [32], (II) orientational and translational tetrahedral order parameters q and S k [19], (III) local structure index I  [20]. In order to evaluate VII and VIII we choose a standard geometrical definition of the H-bond in which two molecules are hydrogen bonded if 33,34]. The first condition ensures that the two molecules are within the first coordination shell of the pair distribution function and the second that the molecules are properly aligned to form an H-bond.
r OO &lt; 3.5 Ang and O a O d H d &lt; 30 • [
In order to identify which of these features are most relevant to a successful description of the molecular environment, we select different combinations of them, isolating local order parameters and Voronoi cell quantities from the local H-bond network topology and from intermolecular distances and angles. The feature set choices utilized in this study are listed in Table I. . From left to right: normalized histograms of Voronoi cell inverse volume, ρ, Voronoi cell asphericity, η, orientational tetrahedral order parameter, q, and translational tetrahedral order parameter, S k , evaluated at finite temperature (solid lines) and at the inherent structure (dashed lines). The simulation temperatures range from 240 K (blue) to 370 K (red). No evident bimodality is observed in these quantities at the IS or at finite temperature.</p>
<p>A B C D E I y y y n n II y y y n n III y/n y/n y/n n n IV y n n 6 n.n. y V &amp; VI y n n n y VII &amp; VIII y n y n n  The entries with value "y/n" signify that that feature was used for finite temperature structure classification but not used for classification of inherent structures. Entries with a numeric value followed by "n.n." denote up to how many nearest neighbors are included. If "y" is specified, all of the features in that category are included.</p>
<p>E. Order parameters from supervised machine learning</p>
<p>Once the feature vectors x i and their corresponding target values τ i are generated, we machine learn a local order parameter O ( x). We use two different machine learning algorithms for this task: a support vector machine (SVM) and a deep neural network (DNN). SVMs [35] are a popular class of classification methods that operate by finding an optimal hypersurface in the space of features x that acts as boundary between two classes of data points (see Fig. 4) [36]. There are two hyperparameters in the SVM that require user tuning: the box parameter C, which sets the scale of the penalty of misclassified points during the training process (see Appendix A), and the radial basis function width, σ, which determines the sharpness of the decision boundary in the neighborhood of the support vectors (see Appendix A 1). It is customary to train and test the SVM over a grid of (C, σ) values in order to choose the one that maximizes the classification accuracy
A = 1 − M/N p ,(3)
where M is the number of misclassified points and N p is the total number of data points.</p>
<p>In a similar fashion to the SVM, supervised learning can be performed by means of deep neural networks (DNN). These machine learning algorithms have revolutionized the applications of machine learning in the recent years in fields as diverse as image recognition, natural language processing, and many others. They can learn highly nonlinear properties of the data with a relatively low algorithmic complexity. There is no standard or systematic procedure to choose a DNN architecture. In general, a DNN with more layers has increased ability to perform complex nonlinear transformations of the features. On the other hand, a DNN with more neurons has higher number of degrees of freedom to perform these nonlinear operations. While conceptually simpler, one restriction of DNNs over SVMs is that they tend to need a larger number of data points to be trained successfully. Therefore we will need to be cautious about our choices of network size and topology. We refer the reader to Appendix B for more details on DNNs.</p>
<p>We developed the codes to perform the SVM analyses with the MATLAB software package and the DNN were implemented with the open source Tensorflow package 4. 2d illustration of a SVM with a nonlinear kernel. It separates the two classes of data points (circles and diamonds) by finding the optimal decision hypersurface y( x) = 0. The dashed lines parallel to the decision surface represent the margins, on which the support vectors lie (filled circles and diamonds). When a point is misclassified, as the circle below the surface, it incurs a penalty in the cost function. [37]. Example codes can be found online [38]. In Appendices A and B we provide more details about these methods for the interested reader.
y ( x ) = 0 FIG.</p>
<p>III. RESULTS</p>
<p>A. Characterization of inherent structures</p>
<p>We begin by machine learning a local order parameter for inherent structures of liquid water. Note that in this case there is no need of using machine learning as we could directly calculate the inherent LSI from Eq.</p>
<p>(2) and apply the threshold manually as discussed above. However, this will serve us as a proof of concept as well as a test case to assess the effectiveness of our procedure depending on the features we include in x. The data set is comprised of feature vectors from all our simulation temperatures. We begin from N init p = 1792000 points, 256000 from each temperature. It can be seen by the size of each mode of the histogram in Fig. 2 that our initial data set is unbalanced, meaning that N p (τ = −1) = N p (τ = +1). In order to prevent artificially biasing the classifier toward the most populated class (HD in our case), we balance the data set by randomly discarding a large number of HD points. From the balanced set, for SVM classification we randomly sampled 20000 molecules from the remaining data for model training, 20000 more for hyperparameter validation and another 20000 for classification testing. In the case of the DNN, the computationally efficient batch gradient descent method allows to perform training with a significantly larger number of training points: 30000 points were sampled for testing and all the remaining points were used for training. After multiple trials with several DNN number of layers and layer sizes, we chose a network with 4 hidden layers containing 80, 100, 200 and 75 neurons respectively (Fig. 5).</p>
<p>In Table II we show the classification accuracies of classification of inherent structures of water as obtained with SVM and DNN. We first observe that both ML models produce similar errors across feature set choices A-E. The classification accuracy is 99% for features D, showing that this method is able to identify very accurately to which of the HD and LD modes of the LSI distribution any given inherent structure belongs to if the only features in x are the first few intermolecular distances, which are the quantities that enter the LSI formula Eq. (2). When additional intermolecular distances and angles are included in x, as in case E, the accuracy decreases slightly, but both models still provide an excellent classification. If the Voronoi cell information, tetrahedral order parameters and H-bond network structure features are included (case A), the accuracy decreases by about 1%. This is due to the noise that additional features introduce, which has a larger negative effect than the additional information they may provide relative to features C. Both D and E cases show that the introduction of additional features with irrelevant information can decrease the classification accuracy. In the case where intermolecular distances and angles are not included in x, as in B and C, the accuracy decreases to about 75% − 80%, showing that the tetrahedral order parameters, the single-molecule Voronoi cell parameters and the local H-bond topology features together do not contain enough information to identify reliably which of the two modes of the inherent LSI distribution a given molecule belongs to.</p>
<p>B. Characterization of finite temperature structures</p>
<p>We now turn to the more challenging case of finite temperature (FT) structures. We use the same procedure as for the IS, with the only difference that the feature vectors x are constructed from FT structures. In Table III  additional features, yielding a higher accuracy for larger feature spaces. Very importantly, all feature set choices yield a better classification than that provided solely by the finite temperature LSI, which we find to give an accuracy of 0.60. This proves that, despite the low overall accuracy, the ML algorithm is benefiting from the information provided in the other features to improve the classification accuracy relative to the FT LSI. It is clear that this procedure struggles to accurately map FT local features to their corresponding HD/LD inherent LSI state. Formally, since inherent structures are derived from FT structures by a relaxation procedure R : r → r 0 , there should exist a one-to-one mapping from finite temperature structures to inherent structures, and therefore also to the inherent LSI. From Eq. (1), it is clear that as the system temperature increases and the structures start to oscillate around the inherent minima, the potential energy also increases, causing the partition function (and hence the free energy) to depart from that of the inherent structures. Therefore it is expected that the classification accuracy of our ML procedure would decrease with the system temperature. In order to verify this hypothesis, we performed training and classification independently at each temperature. As above, we balance the data set to prevent heavily biased solutions, keeping all LD data points and just as many HD points, randomly discarding the rest. We do not aggregate now the data from all temperatures, so in order to keep our training set large enough we reduced the number of test points use N test = max(8000, N left /4) for classification testing and all the remaining for data points are used for training. Indeed, in Fig. 6 we see that the classification accuracy decreases systematically with the system temperature, indicating that the local environment described by x i becomes less informative about the HD/LD character of its IS as T increases. We note that since each data set size is now significantly smaller, in order to avoid an excessively complex model we reduced the number of hidden layers in the DNN from four to two, with the first one containing 80 neurons and the second one containing 30 neurons.</p>
<p>C. The effects of short time fluctuations</p>
<p>So far our analysis has been instantaneous: feature vectors and target values were generated based on geometrical properties of the environment of each molecule in a single time point. In Fig. 7 we observe that the amplitude of the short time fluctuations of the inherent structure potential energies increase with temperature. Likewise, the inherent LSI as well as the components of the feature vectors fluctuate very significantly in timescales 500 fs. It is therefore necessary to understand whether these fluctuations are hindering the classification of the molecular geometries. First, we observe that the FT LSIs of individual molecules are correlated with their corresponding inherent LSIs. An example is shown in Fig. 8 but this property holds for every molecule (see Appendix C for more examples.) So there is clearly a correspondence between FT structures and inherent structures that the ML classifier, when applied to instantaneous local data, is not able to reveal. </p>
<p>Short-time averaging</p>
<p>In order to suppress the noise introduced by the fast fluctuations and capture the time correlations we perform time averages over a short time window of length ∆t of the molecular features, which we then include in the feature vector x. In addition we also time average the inherent LSI and generate new target values based on these averages. It would be unphysical to take time averages on a scale much larger than the intermolecular vibrations as they are a main driver for structural transitions. In addition we note that the correlation function of the fluctuation of number of H-bonds that any given molecule forms has been reported to show a characteristic fast decay time of 70 − 80 fs followed by a slower decay in the 0.8 − 0.9 ps range corresponding to H-bond formation and rupture [39]. With this in mind, we timeaverage over a ∆t of 100 fs and 250 fs, values slightly above the periods of the librational modes of the water molecule and the H-bond bending mode respectively.</p>
<p>We observe that training a DNN with time-averaged features included in x increases the classification accuracy at low temperatures by about 5% where the inherent LSI fluctuates across the threshold I 0 at longer timescales, while at high temperatures the increase in accuracy is minimal or not significant. However, when the target values are constructed based on the time-averaged inherent LSI, the classification accuracy increases at all temperatures, with improvements ranging between 7% and 10%. This demonstrates that the short timescale fluctuations are a fundamental reason to the difficulty of finding a one-to-one correspondence between local FT structures and inherent structures. It is also possible that time averaging over longer time windows would yield higher classification accuracy, but it would become questionable whether one would be finding a local (in space and in time) order parameter anymore as the time average would be taken at the vicinity of the H-bond lifetimes.  </p>
<p>Free energy and equilibrium two-state model</p>
<p>With the evidences accumulated above it is worth asking the following question: can the transitions between the LD and HD inherent structures be understood in thermodynamic grounds? Let us regard the inherent LSI as an order parameter or a reaction coordinate and compute the free energy associated to the inherent structures along this coordinate. We call this quantity inherent free energy and it is given by
F (I) = −k B T ln P (I) (4)
where I is the inherent LSI and T is the temperature of the original simulation. This is the free energy one would obtain from the partition function Eq. (1) with integrand equal to 1. In Fig. 10  HD LSI minimum goes up as T increases, clearly causing a decrease of the LD population in favor of the HD-type molecules.</p>
<p>To go beyond this qualitative statement, the equilibrium conditions of the inherent structures can be obtained using a simple model for state transitions. Denoting the population of LD and HD molecules with n L = N LD /N and n H = N HD /N respectively and the transition rates as r L→H = r LH and r H→L = r HL , the rate equations of this two-state system are
dn L dt = r HL n H − r LH n L (5) dn H dt = − dn L dt (6)
where the second equation is given by the conservation condition n L + n H = 1. Eliminating he second rate equation we obtain
dn L dt = r HL − (r HL + r LH ) n L .(7)
Imposing the equilibrium condition dn L /dt = 0 one obtains the equilibrium population as
n eq L = r HL r HL + r LH .(8)
In order to make use of this equation a model for the transition rates needs to be provided. Based on collision theory [40], we propose a phenomenological Arrheniustype expression of the form r = νe −β∆E where ∆E is the activation energy (i.e. the barrier height E max − E min ) and ν is the frequency of microscopic events that can lead to a state transition in the system. Using this model in Eq. (8) and assuming that the L → H and H → L are driven by the same frequency ν we obtain
n eq L = 1 1 + e β(E min L −E min H ) .(9)
Notice that the equilibrium population does not depend on ν or on the barrier height separating the two minima. In Table IV we show the difference between energy minima at these three example temperatures and we compare the populations predicted by Eq. (9), n FE to the populations obtained directly from the inherent LSI probability distribution, n LSI . We observe a qualitative agreement between the populations found from the distributions of the LSI at the IS and from the two-state model. In addition, this model provides an alternative mechanism for determining the threshold value I 0 : instead of using the value that minimizes P (I) between the two maxima one could choose an I 0 that minimizes the squared-error in the populations 1 N T T (n LSI (T ) − n FE (T )) 2 . This yields a value of I = 0.143 Ang 2 . Finally, we note that an accurate computation of the free energy would require much larger data sets than those employed here so the conclusions obtained from our free energy values are subject to error due to finite sampling size.</p>
<p>There is no clear way to obtain the frequency of events ν that regulates the transitions between the two inherent free energy minima. It is reasonable to think that the intermolecular vibrations, responsible for distortion of the H-bond network, are a main contributor to this transition. Setting ν to the vibrational frequencies of in water, this Arrhenius model for the rates allows us to calculate the time between state transitions, allowing us to have an estimate of the transition timescales. The values in Table V show that the time averages of Fig. 8 are performed over potential libration-induced transitions but not over the H-bond bend-and H-bond stretch-induced transitions.</p>
<p>IV. SUMMARY AND CONCLUSIONS</p>
<p>We proposed a method to characterize local order via supervised machine learning. The classification is based solely on the local geometrical features extracted from simulation data. Intuition about the system is necessary in order to choose feature vectors that describe appropriately the molecular environment as well as to design appropriate target values for the training data. Once the machine learning classifier is trained, it can be used determine the type of local structure associated to a given molecule and its environment, effectively providing a local order parameter.</p>
<p>We demonstrate the validity of this method on inherent structures of water. First, for each data point, we choose its target value based on the instantaneous value of the inherent LSI after an appropriate thresholding. By selecting different groups of features, we observe that an order parameter can be learnt to classify inherent structures with nearly perfect accuracy provided that the intermolecular distances of the first few neighbors are included in the feature vectors. This proves that machine learning methods combined with a physically motivated feature set choice can be a very powerful tool for identification and characterization of local order in complex environments.</p>
<p>We then explore the case of finite temperature structures, characterized by their corresponding inherent structures. Here classification accuracies decrease to below 75% for both of our machine learning models, challenging the prevailing view in which local order properties can be attributed to molecules based on their inherent IS [4, 10,11,41]. We argue that thermal effects are the main contributor to the decrease in the accuracy of our machine learning models as the molecular structures depart from the inherent structures increasingly with temperature. There are two pieces of evidence supporting this claim. First, the classification accuracy decreases monotonically with the simulation temperature, worsening by over 11% between from lowest temperature (240 K) to the highest (370 K). And second, introducing time averages of the FT features and the inherent LSI (and hence the target values) improves the classification by approximately 10%. Interestingly, this improvement is more prominent at high temperatures, reducing the accuracy gap between 240 K and 360 K to about 7%.</p>
<p>Finally we estimate the free energy along the inherent LSI and write a phenomenological two-state equilibrium model for the inherent structures. This allows us to estimate the transition rates between IS states associated with different vibrational modes of water, providing us with sensible choices of our time average windows. We would like to note that it is likely that incorporating time correlations in a more sophisticated manner, such as recurrent neural networks or hidden Markov models, could further improve the identification of two classes of structures. We postpone the exploration of these ideas for future work.</p>
<p>In the recent years machine learning methods have excelled in image recognition, text translation, self-driving vehicles and other technological applications. They are often thought of as blackbox models with thousands of adjustable parameters that can learn features from the data but without providing understanding of the system of interest or ability to generalize outside its training data. It is primarily for these reasons that they have not become common tools of choice in the physical sciences. In this work we demonstrated that, on the contrary, machine learning can be a very insightful device only when combined with scientific understanding of the problem at hand. We expect a steep growth in the usage of these methods as scientists in these disciplines become accustomed to them.</p>
<p>ACKNOWLEDGMENTS</p>
<p>We are grateful to Philip Allen, and Jose Soler for many useful discussions. We are particularly thankful to Daniel Elton for helping set up the MD simulations and for comments on the manuscript. A.S. and M.V.F.S. acknowledge support from U.S. Department of Energy grant de-sc0001137. This work was partially supported by BNL   Support vector machines [35] have been a popular method for machine learning classification due to their algorithmic simplicity and the ease of interpretation. In this section we explain the basic formalism following [42]. Let x n = (x n,1 , · · · , x n,D ) with n = 1, · · · , N p be our collection of data points in feature space and let τ n ∈ {−1, +1} be a binary label that categorizes each point that we call target value. The set { x n , τ n } N n=1 is linearly separable if there exists a hyperplane y ( x) ≡ w · x + b = 0 such that y ( x n ) &lt; 0 for points with τ n = −1 and y ( x n ) &gt; 0 for points with τ n = +1. It is clear that a data set is, in general, not linearly separable. On the other hand, a curved surface that separates the data in the D-dimensional feature space can always be found. A convenient way of tackling the problem of finding such surface is by formulating it as a linear separation problem in a higher dimensional auxiliary space. Letting the decision boundary be now
y ( x) ≡ w · φ ( x) + b = 0 where φ ( x)
is a non-linear mapping from the space of features to the auxiliary space, the margin is defined as the normal distance from closest point to the decision surface, τ n y ( x n ) /|| w||. The desired surface then maximizes the margin over w, b and n. Taking advantage of the fact that y ( x) is invariant under rescaling ( w, b) → (α w, αb), the points closest to the surface can be set to satisfy τ n y ( x n ) = 1. This turns the problem of maximizing the margin into maximizing 1/|| w||, which is in turn equivalent to the quadratic programming problem of minimizing || w|| 2 under the constraints
τ n y ( x n ) ≥ 1 (A1)
which ensure exact classification. Introducing the constraints via Lagrange multipliers a i we are left with the Lagrangian function
L hard = || w|| 2 − N n=1 a n τ n w · φ ( x n ) (A2)
where the minus sign between the two terms denotes that we are minimizing with respect to w and b and maximizing with respect to a n . Defining the kernel function as the scalar product in the auxiliary space
k ( x 1 , x 2 ) = φ ( x 1 ) · φ ( x 2 )(A3)
and setting to zero the derivatives with respect to w and b, the Lagrangian can be rewritten in its dual representatioñ
L hard = Np n=1 a n   1 − Np m=1 a m τ m τ m k ( x n , x m )   (A4)
now subject to the constraints a n ≥ 0 and Np n=1 a n τ n = 0. Notice that the Lagrangian now depends on the kernel function and no longer on the explicit mapping to the auxiliary space φ. In practice the user specifies a kernel function, which may have adjustable hyperparameters (this is the case of the RBF kernel described in Appendix A 1). After the parameters a n have been optimized, the decision boundary can be expressed as a function of the kernel function as
y ( x) = Np n=1 a n τ n k ( x n , x) + b (A5)
This method provides a formalism to find a curved decision boundary. The decision surface, however, is at risk of being highly overfitted to the training data points and hence be very uninformative about the structure of the data distribution, achieving very limited predictive power. The idea of soft margin relaxes the condition of strict separability, finding a hypersurface that approximately separates the data set while allowing for some misclassified points. Defining the slack variables as
ζ n = 0, if x n is correctly classified |y ( x n ) − τ n |, otherwise(A6)
the exact classification constraints (A1) are replaced with
τ n y ( x n ) ≥ 1 − ζ n (A7)
Now we can include a penalty for misclassification into the Lagrangian (A2), controlled by the box parameter C, as well as the Lagrange multipliers µ i for the constraints ζ i &gt; 0, resulting in
L soft = || w|| 2 + C Np n=1 ζ n − Np n=1 a n (τ n y ( x n ) − 1 + ζ n ) − Np n=1 µ n ζ n .(A8)
It can be shown that the soft margin Lagrangian can also adopt a dual representation with the same functional form as (A4) but subject to different constraints, namely 0 ≤ a n ≤ C and Np n=1 a n τ n = 0. Now the Lagrange multipliers a n need to be bounded below the user-specified box parameter.</p>
<p>Gaussian kernel</p>
<p>One essential piece of the SVM is the kernel function. When a non-linear kernel function is chosen, the decision boundary, calculated via Eq. (A5), is a curved surface in D-dimensional space. Gaussian kernels of the form
K( x, x ) = exp − x − x 2 2σ 2 (A9)
are commonly used in a variety of machine learning algorithms. This function contains a free (hyper)parameter: the Gaussian width σ. If σ is small, the Gaussian decays rapidly as x moves away from x . On the other hand, when σ is large, K varies slowly as x moves away from x . For this reason, small σ values can adjust the surface better to the training data though at the risk of overfitting and losing predictive power. A very large σ, however, could make it difficult for the training algorithm to properly fit the training data producing larger errors and, in the worst case, convergence problems. For these reasons, tuning σ properly is a crucial step during while training this model.</p>
<p>Appendix B: Neural Networks for classification</p>
<p>Here we introduce the concepts of neuron and neural network, show how they can be used for classification and explain how the network is trained. We follow mostly the notation in [43], where we point the interested reader for an in-depth discussion.</p>
<p>A neuron is a unit that provides an output y for any given input vector x. It contains weights w 1 , · · · , w D associated to each input dimension and a bias parameter b. The activation of the neuron given an input x is a linear function given by
a = i w i x i + b.(B1)
So the neuron can be seen as a linear function, which could in turn be used to perform linear binary classification if we regard the sign of a as to identify each class of data points upon an appropriate selection of the weights and the bias parameters. In addition, this can be generalized by introducing a (generally non-linear) activity function y (a). By choosing suitable activity functions such as f (a) = 1/(1 + e −a ) or f (a) = tanh(a) one can mimic the biological behavior of a neuron, which propagates information by enabling an action potential whose value is 11. Graphical representation of one neuron. Activation a is obtained from the features x by a linear mapping. The activity is the output of the neuron, given by the nonlinear function y(a).
x 1 x 2 x D w D w 2 w 1 a w 0 b y FIG.
bounded above and below. Interestingly, the virtue of the abstract neuron presented here relies on two ideas: the use of nonlinear activities and combination of multiple neurons together in a feed-forward network topology (see Fig. 5). Let us imagine a collection of l 1 neurons, each acting on the input data x of dimension D and with output
z (1) ( x) = f (w (1) x + b (1) ) (B2)
where w (1) is a D × l 1 matrix with rows containing the weights corresponding to each neuron and b (1) is a 1 × l 1 vector containing the biases of all neurons. Now each output unit z</p>
<p>(1) i is a nonlinear function of the input data x. These output units can themselves be input to a new set of l 2 neurons, producing the output
z (2) ( z (1) ) = f (w (2) z (1) + b (2) ) (B3)
By adding this second layer of neurons we obtain a nonlinear function acting on a nonlinear function of the input data x. More layers can be added to systematically create a more complex nonlinear function of the input x.</p>
<p>Proceeding with H hidden layers, z (h k ) , and terminating this sequence with an output layer with linear activity, we obtain
y( z (H) ) = w (out) z (H) + b (out) .(B4)
This formalism can tackle classification problems of great complexity with nonlinearly separable data if the weights and biases are properly adjusted. The number of such adjustable parameters in the network is given by the sum of the number of weight parameters and bias parameters in the connections between adjacent layers. Denoting the number of hidden layers by H and the number of units in each layer by l k , we have
N p = H k=0 (l h k + 1)l h k+1(B5)
where k = 0 corresponds to the input layer and k = H + 1 to the output layer. Notice that the output y is a composition of functions
y ( x) = · · · z (3) (z (2) (z (1) ( x))) (B6)
and therefore the output can be written as a function of x with a parametric dependence on the biases and weights of each layer.</p>
<p>Let us briefly discuss how the free parameters of the network are optimized. To simplify the discussion, we assume in the following that the output layer consists of a single unit y with values between 0 and 1 and that the target values for binary classification are t n = 0, 1. [44] We define the error function
E = − Np n=1
[t n ln y ( x n ) + (1 − t n ) ln(1 − y ( x n ))] (B7) which is the relative entropy between the probability distribution generated by the network, (y, 1 − y) and the "observed" probability distribution of the data (t, 1 − t). This is a function of all weights and biases in the network and it is bounded by zero from below, corresponding to perfectly matching network prediction with the observed data. This error function can be minimized by "descending" it along the direction of the gradient. This requires computing derivatives of this function with respect to the network parameters. Luckily, since the output y is a composition of analytically known functions y ( x; {w, b}) = · · · z (2) (z (1) ( x; {w (1) , b (1) }); {w (2) , b (2) }) (B8) derivatives can be readily calculated applying the chain rule. This gradient can computed for each data point n, allowing to adjust the weights and the biases a small amount in the direction of descending gradient for x n . Alternatively, in order to make this computation more efficient, instead of computing the gradient at each data point one can calculate these gradients for batches of data points, averaging. This procedure is called batch gradient descent, which we used in this work.  Ang, obtained from imposing the equilibrium condition on the rate equations with the energy minima from the free energy curve and populations obtained from data adjusting the threshold I 0 to minimize the error relative to the FE estimate.</p>
<p>of the LSI at the IS and from the two-state model. In addition, this model provides an alternative mechanism for determining the threshold value I 0 : instead of using the value that minimizes P (I) between the two maxima one could choose an I 0 that minimizes the squared-error in the populations 1 NT P T (n LSI (T ) n FE (T )) 2 . This yields a value of I = 0.143 Ang 2 . Finally, we note that an accurate computation of the free energy would require much larger data sets than those employed here so the conclusions obtained from our free energy values are subject to error.</p>
<p>There is no clear way to obtain the frequency of events ⌫ that regulates the transitions between the two inherent free energy minima. It is reasonable to think that the intermolecular vibrations, responsible for distortion of the H-bond network, are a main contributor to this transition. Setting ⌫ to the vibrational frequencies of in water, this Arrhenius model for the rates allows us to calculate the time between state transitions, allowing us to have an estimate of the transition timescales. The values in Table V show that the time averages of Fig. 8 are performed over potential libration-induced transitions but not over the H-bond bend-and H-bond stretch-induced transitions. </p>
<p>FIG. 2 .
2Normalized histograms of the LSI evaluated at finite temperature (solid lines) and at the inherent structure (dashed lines). The simulation temperatures range from 240 K (blue) to 370 K (red). A threshold value of I0 = 0.13 Ang 2 separating the HD from the LD target values is shown with a vertical dashed line.</p>
<p>[12], (IV) 17 intermolecular O-O distances, (V) 5 O-O-O angles, (VI) 6 O-H-O angles, (VII) number of donated and accepted H-bonds and (VIII) number of H-bond loops of lengths 3-12 that the i participates in</p>
<p>FIG. 3. From left to right: normalized histograms of Voronoi cell inverse volume, ρ, Voronoi cell asphericity, η, orientational tetrahedral order parameter, q, and translational tetrahedral order parameter, S k , evaluated at finite temperature (solid lines) and at the inherent structure (dashed lines). The simulation temperatures range from 240 K (blue) to 370 K (red). No evident bimodality is observed in these quantities at the IS or at finite temperature.</p>
<p>I. Description of different local feature choices, A-E. The features are grouped by category as follows: (I) molecular Voronoi cell parameters, (II) tetrahedral order parameters, (III) LSI, (IV) O-O distances, (V) O-O-O angles, (VI) O-H-O angles, (VII) number of H-bonds and (VIII) number of H-bond loops (see II D for further details).</p>
<p>FIG. 5 .
5Schematic representation of our feed-forward deep neural network model for supervised learning local order parameters. The input nodes take feature vectors xi, which get computed forward into the network, connecting with all other features via nonlinear functions. The output layer contains as many nodes as classes, outputting the predicted class.</p>
<p>FIG. 7 .FIG. 8 .
78Inherent structure potential energies as a function of the simulation time for three different temperatures: 240 K (top), 300 K (center), 370 K (bottom). The amplitude of the short timescale ( 0.2 ps) fluctuations increases with temperature. Finite temperature LSI (top) and inherent LSI (bottom) trajectories of 3 example molecules from MD simulations at different temperatures: 240 K (left), 300 K (center), 360 K (right). The dashed lines show the threshold inherent LSI value I0 = 0.13 Ang 2 . The trajectories of these two quantities are evidently correlated. This effect is more evident at low temperatures where the inherent LSI fluctuations have smaller amplitude.</p>
<p>feat. only 250 fs feat. only 100 fs feat. &amp; targ. 250 fs feat. &amp; targ.</p>
<p>FIG. 9 .
9Comparison of classification accuracies using instantaneous features and targets (blue), time-averaged features and instantaneous targets (red) and time-averaged features and targets (green) obtained with a Deep Neural Network. Diamonds and hexagons represent time averages with ∆t = 100 fs and ∆t = 250 fs respectively.</p>
<p>[ 3 ]
3R. J. Speedy and C. A. Angell. Isothermal compressibility of supercooled water and evidence for a thermodynamic singularity at -45C. The Journal of Chemical Physics, 65(3):851-858, 1976. [4] A. Nilsson and L. G. M. Pettersson. The structural origin of anomalous properties of liquid water. Nature Communications, 6:8998 EP -, Dec 2015. Review Article. classification</p>
<p>FIG. 8 :
8FT LSI (top) and inherent LSI (bottom) trajectories of 3 example molecules from MD simulations at di↵erent temperatures: 240 K (left), 300 K (center), 360 K (right). The dashed lines show the threshold inherent LSI value I 0 = 0.13 Ang 2 . The trajectories of these two quantities are evidently correlated. This e↵ect is more evident at low temperatures where the inherent LSI fluctuations have smaller amplitude. FIG. 9: Comparison of classification accuracies using instantaneous features and targets (blue), time-averaged features and instantaneous targets (red) and time-averaged features and targets (green). Diamonds and hexagons represent time averages with t = 100 fs and t = 250 fs respectively. FIG. 10: Inherent free energy surface as a function of the LSI for three di↵erent simulation temperatures. The curves are obtained from the LSI histograms.</p>
<p>FIG. 12 .FIG. 13 .
1213LSI trajectories at FT (top, blue), LSI at the IS (bottom, red) for 9 example molecules at 3 different temperatures: 240 K (left), 300 K (center), 360 K (right). The dashed lines show the threshold LSI value I0 = 0.13 Ang 2 . Histograms of instantaneous inherent LSI (solid black) and time averaged inherent LSI over 100 fs (dashed blue) and 250 fs (dashed-dotted red). The bimodal character of the probability distribution is disappears upon time averaging, revealing that this bimodality is a short-time effect.</p>
<p>FIG. 1. Machine learning local order parameter workflow. Feature vectors describing the molecular environment and target values describing distinct molecular classes need to be provided by physical intuition. The machine learning algorithm learns from these data, allowing to characterize a new molecular environment.machine learning 
algorithm 
(SVM, DNN, … ) </p>
<p>training 
molecular 
features </p>
<p>x ix j </p>
<p>new 
molecule 
features </p>
<p>local order 
parameter </p>
<p>⌦ j </p>
<p>simulation 
coordinates </p>
<p>r </p>
<p>trained model </p>
<p>target 
values </p>
<p>⌧ i </p>
<p>prior 
physical 
knowledge </p>
<p>separating the HD from the LD target values is shown with a vertical dashed line.0 
0.1 
0.2 
0.3 </p>
<p>I(Ang 2 ) </p>
<p>0 </p>
<p>5 </p>
<p>10 </p>
<p>15 </p>
<p>20 </p>
<p>25 </p>
<p>30 </p>
<p>probability density </p>
<p>240 K 
260 K 
300 K 
330 K 
370 K </p>
<p>HD </p>
<p>⌧ i = 1 </p>
<p>LD </p>
<p>⌧ i = +1 </p>
<p>0 
0.1 
0.2 
0.3 </p>
<p>I(Ang 2 ) </p>
<p>0 </p>
<p>5 </p>
<p>10 </p>
<p>15 </p>
<p>20 </p>
<p>25 </p>
<p>30 </p>
<p>probability density </p>
<p>240 K 
260 K 
300 K 
330 K 
370 K </p>
<p>HD </p>
<p>⌧ i = 1 </p>
<p>LD </p>
<p>⌧ i = +1 </p>
<p>IG. 1: Normalized histograms of the LSI evaluated at 
finite temperature (solid lines) and at the inherent 
tructure (dashed lines). The simulation temperatures 
range from 240 K (blue) to 370 K (red). A threshold 
lue of I 0 = 0.13 Ang 2 </p>
<p>TABLE I :
IDescription of di↵erent local feature 
A-E. The entries with value "y/n" signify tha 
feature was used for finite temperature struc 
classification but not used for classification of i 
structures. Entries with a numeric value follow 
"n.n." up to how many nearest neighbors are in 
If "y" is specified, all of the features in that ca 
(see II D) are included. </p>
<p>lowing local geometrical quantities: (I) inverse 
⇢, asphericity ⌘, number of sides s, edges e 
tices v of the Voronoi cell of each molecule, det 
by the position of its O atom [31] (II) orientati 
translational tetrahedral order parameters q and 
(III) local structure index I [11] (IV) 17 interm 
O-O distances (V) 5 O-O-O angles (VI) 6 O-H 
gles (VII) Number of donated and accepted h 
bonds (VIII) Number of hydrogen bond loops o 
3-12 that the i participates in [19] In order to 
VII and VIII we are required to choose a H-bon 
tion. Here considered a standard geometrical d 
in which two molecules are hydrogen bonded if r O 
Ang and \ 
O a O d H d &lt; 30 </p>
<p>TABLE</p>
<p>TABLE II. Classification accuracies of a random mixture of inherent structures from initial simulation temperatures ranging between 240 K and 370 K obtained with a Support Vector Machine and with a Deep Neural Network for several feature choices.feature set choices A 
B 
C 
D 
E </p>
<p>A SVM 
0.96 0.74 0.78 0.99 0.97 </p>
<p>A DNN 
0.97 0.82 0.82 0.99 0.98 </p>
<p>we observe that this ML classification method yields classification accuracies below 75% for all feature set choices and for both ML models. The best results are obtained for A, where all the features are included. It is worth noting that, as opposed to the IS case, here the algorithm benefits from the information provided fromfeature set choice A 
B 
C 
D 
E </p>
<p>A SVM 
0.69 0.65 0.67 0.64 0.67 </p>
<p>A DNN 
0.74 0.65 0.63 0.64 0.69 </p>
<p>TABLE III. Classification accuracies of a random mixture of 
finite temperature structures from initial simulation temper-
atures ranging between 240 K and 370 K obtained with a 
Support Vector Machine and with a Deep Neural Network for 
several feature choices. </p>
<p>240 260 280 300 320 340 360 
T (K) </p>
<p>0.62 </p>
<p>0.64 </p>
<p>0.66 </p>
<p>0.68 </p>
<p>0.70 </p>
<p>0.72 </p>
<p>0.74 </p>
<p>0.76 </p>
<p>0.78 </p>
<p>Accuracy </p>
<p>SVM 
DNN </p>
<p>FIG. 6. Classification accuracy of finite temperature struc-
tures for features A, training independently at each tempera-
ture a Support Vector Machine and a Deep Neural Network. 
The accuracy decreases with temperature, suggesting that 
thermal fluctuations are hindering the identification of the 
molecular environments. </p>
<p>TABLE IV. Populations of LD component as determined by a LSI at the IS with value &lt; I0 = 0.13 Ang, obtained from imposing the equilibrium condition on the rate equations with the energy minima from the free energy curve and populations obtained from data adjusting the threshold I0 to minimize the error relative to the FE estimate.T (K) 
240 300 360 
∆ELH (meV) 18.54 39.80 55.98 
nLSI 
0.335 0.186 0.159 
nFE 
0.282 0.172 0.136 
n LSI, optimal 0.309 0.159 0.136 </p>
<p>T (K) 
240 
300 
360 
r −1 
L\H,str (fs) 3270\1270 4190\818 4980\786 
r −1 
L\H,bend (fs) 882\347 1143\237 1355\214 
r −1 
L\H,libr (fs) 409\161 530\110 629\99 </p>
<p>TABLE V. L\R inverse transition rates given by the the 
Arrhenius model with event frequencies of three vibrational 
modes in water: two slow intermolecular modes (H-bond 
stretch, 1.5 THz and H-bond bend, 5.5 THz) and one in-
tramolecular mode (L1 libration, 11.9 THz). </p>
<p>039 project. This research used resources of the National Energy Research Scientific Computing Center, a DoE Office of Science User Facility supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231, resources of the Center for Functional Nanomaterials, which is a U.S. DOE Office of Science User Facility, at Brookhaven National Laboratory under Contract No. de-sc0012704, as well as the Handy and LI-Red computer clusters at the Stony Brook University Institute for Advanced Computational Science.[1] P. H. Poole, F. Sciortino, U. Essmann, and H. E. Stan-
ley. Phase behaviour of metastable water. Nature, 
360(6402):324-328, Nov 1992. 
[2] F. J. Millero, R. W. Curry, and W. Drost-Hansen. 
Isothermal compressibility of water at various tem-
peratures. Journal of Chemical &amp; Engineering Data, 
14(4):422-425, 1969. </p>
<p>TABLE IV :
IVPopulations of LD component as determined by a LSI at the IS with value &lt; I 0 = 0.13
Appendix A: Support Vector Machine for binary Appendix C: Time fluctuations of the LSI We argued above that short time fluctuations are responsible (at least partly) of the difficulty of identifying an instantaneous FT structure with its corresponding low-LSI or high-LSI inherent structure, even if using a very flexible pattern recognition device like a DNN.The LSI plays a central role in our analysis since it is our basis to define the target values for supervised machine learning. To show further evidence on our claim about the correlation between the FT LSI and the inherent LSI discussed in III C, inFig. 12we show a LSI trajectories of 3 example molecules at 3 different temperatures. In addition we show inFig. 13the histograms of the inherent LSI as well as the histograms of the timeaveraged inherent LSI for our choices of ∆t = 100 fs and 250 fs. The bimodal character vanishes upon averaging, this effect being more pronounced at higher temperatures. It is therefore no longer well defined to split the distribution into two, one corresponding to each type of molecular environment.
The inhomogeneous structure of water at ambient conditions. C Huang, K T Wikfeldt, T Tokushima, D Nordlund, Y Harada, U Bergmann, M Niebuhr, T M Weiss, Y Horikawa, M Leetmaa, M P Ljungberg, O Takahashi, A Lenz, L Ojame, A P Lyubartsev, S Shin, L G M Pettersson, A Nilsson, Proceedings of the National Academy of Sciences. 10636C. Huang, K. T. Wikfeldt, T. Tokushima, D. Nordlund, Y. Harada, U. Bergmann, M. Niebuhr, T. M. Weiss, Y. Horikawa, M. Leetmaa, M. P. Ljungberg, O. Taka- hashi, A. Lenz, L. Ojame, A. P. Lyubartsev, S. Shin, L. G. M. Pettersson, and A. Nilsson. The inhomogeneous structure of water at ambient conditions. Proceedings of the National Academy of Sciences, 106(36):15214-15218, 2009.</p>
<p>Diffusive dynamics during the high-to-low density transition in amorphous ice. F Perakis, K Amann-Winkel, F Lehmkhler, M Sprung, D Mariedahl, J A Sellberg, H Patak, A Sph, F Cavalca, D Schlesinger, A Ricci, A Jain, B Massani, F Aubree, C J Benmore, T Loerting, G Grbel, L G M Pettersson, A Nilsson, Proceedings of the National Academy of Sciences. the National Academy of SciencesF. Perakis, K. Amann-Winkel, F. Lehmkhler, M. Sprung, D. Mariedahl, J. A. Sellberg, H. Patak, A. Sph, F. Cav- alca, D. Schlesinger, A. Ricci, A. Jain, B. Massani, F. Aubree, C. J. Benmore, T. Loerting, G. Grbel, L. G. M. Pettersson, and A. Nilsson. Diffusive dynamics during the high-to-low density transition in amorphous ice. Proceedings of the National Academy of Sciences, 2017.</p>
<p>. N Giovambattista, T Loerting, B R Lukanov, F W Starr, Interplay of the glass transition and the liquid-liquid phase transition in water. 2:390 EP -. ArticleN. Giovambattista, T. Loerting, B. R. Lukanov, and F. W. Starr. Interplay of the glass transition and the liquid-liquid phase transition in water. 2:390 EP -, May 2012. Article.</p>
<p>Liquid-liquid transition in ST2 water. Y Liu, J C Palmer, A Z Panagiotopoulos, P G Debenedetti, The Journal of Chemical Physics. 13721214505Y. Liu, J. C. Palmer, A. Z. Panagiotopoulos, and P. G. Debenedetti. Liquid-liquid transition in ST2 water. The Journal of Chemical Physics, 137(21):214505, Dec 2012.</p>
<p>Understanding waters anomalies with locally favoured structures. J Russo, H Tanaka, Nature Communications. 53556ArticleJ. Russo and H. Tanaka. Understanding waters anoma- lies with locally favoured structures. Nature Communi- cations, 5:3556 EP -, Apr 2014. Article.</p>
<p>Spatially inhomogeneous bimodal inherent structure of simulated liquid water. K T Wikfeldt, A Nilsson, L G M Pettersson, Phys. Chem. Chem. Phys. 13K. T. Wikfeldt, A. Nilsson, and L. G. M. Pettersson. Spa- tially inhomogeneous bimodal inherent structure of simu- lated liquid water. Phys. Chem. Chem. Phys., 13:19918- 19924, 2011.</p>
<p>Local structure analysis in ab initio liquid water. Molecular. B Santra, R A DistasioJr, F Martelli, R Car, Physics. 113B. Santra, R. A. DiStasio Jr., F. Martelli, and R. Car. Local structure analysis in ab initio liquid water. Molec- ular Physics, 113(17-18):2829-2841, 2015.</p>
<p>Growth and collapse of structural patterns in the hydrogen bond network in liquid water. E Shiratani, M Sasai, The Journal of Chemical Physics. 10419E. Shiratani and M. Sasai. Growth and collapse of struc- tural patterns in the hydrogen bond network in liquid water. The Journal of Chemical Physics, 104(19):7671- 7680, 1996.</p>
<p>Growing correlation length in supercooled water. E B Moore, V Molinero, The Journal of Chemical Physics. 13024244505E. B. Moore and V. Molinero. Growing correlation length in supercooled water. The Journal of Chemical Physics, 130(24):244505, Jun 2009.</p>
<p>Small-angle scattering and the structure of ambient liquid water. G N I Clark, G L Hura, J Teixeira, A K Soper, T Head-Gordon, Proceedings of the National Academy of Sciences. 10732G. N. I. Clark, G. L. Hura, J. Teixeira, A. K. Soper, and T. Head-Gordon. Small-angle scattering and the struc- ture of ambient liquid water. Proceedings of the National Academy of Sciences, 107(32):14003-14007, 2010.</p>
<p>The putative liquidliquid transition is a liquid-solid transition in atomistic models of water. D T Limmer, D Chandler, The Journal of Chemical Physics. 13513134503D. T. Limmer and D. Chandler. The putative liquid- liquid transition is a liquid-solid transition in atomistic models of water. The Journal of Chemical Physics, 135(13):134503, 2011.</p>
<p>The putative liquidliquid transition is a liquid-solid transition in atomistic models of water. ii. D T Limmer, D Chandler, The Journal of Chemical Physics. 13821214504D. T. Limmer and D. Chandler. The putative liquid- liquid transition is a liquid-solid transition in atomistic models of water. ii. The Journal of Chemical Physics, 138(21):214504, 2013.</p>
<p>Tetrahedrality and hydrogen bonds in water. E Szkely, I K Varga, A Baranyai, The Journal of Chemical Physics. 14422224502E. Szkely, I. K. Varga, and A. Baranyai. Tetrahedrality and hydrogen bonds in water. The Journal of Chemical Physics, 144(22):224502, 2016.</p>
<p>A K Soper, J Teixeira, T Head-Gordon, Is ambient water inhomogeneous on the nanometer-length scale? Proceedings of the National Academy of Sciences. 10744A. K. Soper, J. Teixeira, and T. Head-Gordon. Is ambient water inhomogeneous on the nanometer-length scale? Proceedings of the National Academy of Sciences, 107(12):E44, 2010.</p>
<p>A new order parameter for tetrahedral configurations. P.-L Chau, A J Hardwick, Molecular Physics. 933P.-L. Chau and A. J. Hardwick. A new order param- eter for tetrahedral configurations. Molecular Physics, 93(3):511-518, 1998.</p>
<p>Hydrogen bond network topology in liquid water and methanol: a graph theory approach. I Bako, A Bencsura, K Hermannson, S Balint, T Grosz, V Chihaia, J Olah, Phys. Chem. Chem. Phys. 15I. Bako, A. Bencsura, K. Hermannson, S. Balint, T. Grosz, V. Chihaia, and J. Olah. Hydrogen bond net- work topology in liquid water and methanol: a graph theory approach. Phys. Chem. Chem. Phys., 15:15163- 15171, 2013.</p>
<p>Characterization of the local structure in liquid water by various order parameters. E Dubou-Dijon, D Laage, 26054933The Journal of Physical Chemistry B. 11926E. Dubou-Dijon and D. Laage. Characterization of the local structure in liquid water by various order parame- ters. The Journal of Physical Chemistry B, 119(26):8406- 8418, 2015. PMID: 26054933.</p>
<p>Machine learning phases of matter. J Carrasquilla, R G Melko, Nat Phys. Letteradvance online publicationJ. Carrasquilla and R. G. Melko. Machine learning phases of matter. Nat Phys, advance online publication, Feb 2017. Letter.</p>
<p>Recognizing molecular patterns by machine learning: An agnostic structural definition of the hydrogen bond. P Gasparotto, M Ceriotti, The Journal of Chemical Physics. 14117174110P. Gasparotto and M. Ceriotti. Recognizing molecular patterns by machine learning: An agnostic structural def- inition of the hydrogen bond. The Journal of Chemical Physics, 141(17):174110, 2014.</p>
<p>t can be a time step in MD or a trial index if Monte Carlo methods are used to sample configurational space. t can be a time step in MD or a trial index if Monte Carlo methods are used to sample configurational space.</p>
<p>Note that the original order parameter can be trivially recast in this new formalism by setting x( r) = r and O =Õi. Note that the original order parameter can be trivially recast in this new formalism by setting x( r) = r and O =Õi.</p>
<p>Development of transferable interaction potentials for water. v. extension of the flexible, polarizable, thole-type model potential (ttm3-f, v. 3.0) to describe the vibrational spectra of water clusters and liquid water. G S Fanourgakis, S S Xantheas, The Journal of Chemical Physics. 128774506G. S. Fanourgakis and S. S. Xantheas. Development of transferable interaction potentials for water. v. extension of the flexible, polarizable, thole-type model potential (ttm3-f, v. 3.0) to describe the vibrational spectra of wa- ter clusters and liquid water. The Journal of Chemical Physics, 128(7):074506, 2008.</p>
<p>A unified formulation of the constant temperature molecular dynamics methods. S Nos, The Journal of Chemical Physics. 811S. Nos. A unified formulation of the constant temperature molecular dynamics methods. The Journal of Chemical Physics, 81(1):511-519, 1984.</p>
<p>Canonical dynamics: Equilibrium phasespace distributions. W G Hoover, Phys. Rev. A. 31W. G. Hoover. Canonical dynamics: Equilibrium phase- space distributions. Phys. Rev. A, 31:1695-1697, Mar 1985.</p>
<p>Methods of Conjugate Gradients for Solving Linear Systems. M R Hestenes, E Stiefel, 49Journal of Research of the National Bureau of StandardsM. R. Hestenes and E. Stiefel. Methods of Conjugate Gradients for Solving Linear Systems. Journal of Re- search of the National Bureau of Standards, 49(6):409- 436, December 1952.</p>
<p>Hidden structure in liquids. F H Stillinger, T A Weber, Phys. Rev. A. 25F. H. Stillinger and T. A. Weber. Hidden structure in liquids. Phys. Rev. A, 25:978-989, Feb 1982.</p>
<p>Voro++: A three-dimensional voronoi cell library in c++. C H Rycroft, Chaos: An Interdisciplinary Journal of Nonlinear Science. 19441111C. H. Rycroft. Voro++: A three-dimensional voronoi cell library in c++. Chaos: An Interdisciplinary Journal of Nonlinear Science, 19(4):041111, 2009.</p>
<p>Hydrogen bonding definitions and dynamics in liquid water. R Kumar, J R Schmidt, J L Skinner, The Journal of Chemical Physics. 12620204107R. Kumar, J. R. Schmidt, and J. L. Skinner. Hydrogen bonding definitions and dynamics in liquid water. The Journal of Chemical Physics, 126(20):204107, 2007.</p>
<p>Room temperature compressibility and diffusivity of liquid water from first principles. F Corsetti, E Artacho, J M Soler, S S Alexandre, M.-V Fernndez-Serra, The Journal of Chemical Physics. 13919194502F. Corsetti, E. Artacho, J. M. Soler, S. S. Alexandre, and M.-V. Fernndez-Serra. Room temperature compressibil- ity and diffusivity of liquid water from first principles. The Journal of Chemical Physics, 139(19):194502, 2013.</p>
<p>Support-vector networks. Machine Learning. C Cortes, V Vapnik, 20C. Cortes and V. Vapnik. Support-vector networks. Ma- chine Learning, 20(3):273-297, 1995.</p>
<p>In the case of perfectly linear separation two support vectors are sufficient to generate the hyperplane (as they define the hyperplane normal), but in a more complicated scenario the number of support vectors will be much larger. The data points that define the decision boundary are called support vectors and the number of them is determined by the data and the algorithm. often a significant fraction of the data setThe data points that define the decision boundary are called support vectors and the number of them is deter- mined by the data and the algorithm. In the case of per- fectly linear separation two support vectors are sufficient to generate the hyperplane (as they define the hyperplane normal), but in a more complicated scenario the number of support vectors will be much larger, often a significant fraction of the data set.</p>
<p>J J Allaire, D Eddelbuettel, N Golding, Y Tang, tensorflow: R Interface to TensorFlow. J.J. Allaire, D. Eddelbuettel, N. Golding, and Y. Tang. tensorflow: R Interface to TensorFlow, 2016.</p>
<p>. A Soto, A. Soto. https://github.com/adrian-soto/localop.</p>
<p>Are water simulation models consistent with steady-state and ultrafast vibrational spectroscopy experiments?. J R Schmidt, S T Roberts, J J Loparo, A Tokmakoff, M D Fayer, J L Skinner, Chemical Physics. 34113Ultrafast Dynamics of Molecules in the Condensed Phase: Photon Echoes and. Coupled ExcitationsA Tribute to Douwe A. WiersmaJ. R. Schmidt, S. T. Roberts, J. J. Loparo, A. Tok- makoff, M. D. Fayer, and J. L. Skinner. Are water simula- tion models consistent with steady-state and ultrafast vi- brational spectroscopy experiments? Chemical Physics, 341(13):143 -157, 2007. Ultrafast Dynamics of Molecules in the Condensed Phase: Photon Echoes and Coupled ExcitationsA Tribute to Douwe A. Wiersma.</p>
<p>Das gesetz der reaktionsgeschwindigkeit und der gleichgewichte in gasen. besttigung der additivitt von cv-3/2r. neue bestimmung der integrationskonstanten und der molekldurchmesser. Zeitschrift fr anorganische und allgemeine Chemie. M Trautz, 96M. Trautz. Das gesetz der reaktionsgeschwindigkeit und der gleichgewichte in gasen. besttigung der additivitt von cv-3/2r. neue bestimmung der integrationskonstan- ten und der molekldurchmesser. Zeitschrift fr anorganis- che und allgemeine Chemie, 96(1):1-28, 1916.</p>
<p>The structure of water; from ambient to deeply supercooled. L G M Pettersson, A Nilsson, 7th IDM-RCS: Relaxation in Complex Systems. 407L. G. M. Pettersson and A. Nilsson. The structure of water; from ambient to deeply supercooled. Journal of Non-Crystalline Solids, 407:399 -417, 2015. 7th IDM- RCS: Relaxation in Complex Systems.</p>
<p>Pattern Recognition and Machine Learning. C M Bishop, SpringerC. M. Bishop. Pattern Recognition and Machine Learn- ing. Springer, 2006.</p>
<p>Information Theory and Learning Algorithms. D J C Mckay, CambridgeD. J. C. McKay. Information Theory and Learning Al- gorithms. Cambridge, 2003.</p>
<p>The extension to multiclass classification is straightforward by including as many output units as classes and interpreting the output value of each unit as the probability that the data point belongs to that class. The extension to multiclass classification is straightfor- ward by including as many output units as classes and interpreting the output value of each unit as the proba- bility that the data point belongs to that class.</p>            </div>
        </div>

    </div>
</body>
</html>
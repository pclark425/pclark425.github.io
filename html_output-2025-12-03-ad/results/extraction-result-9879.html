<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9879 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9879</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9879</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-94972e30504017156ef5b5debc419bf6edc67384</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/94972e30504017156ef5b5debc419bf6edc67384" target="_blank">MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> The proposed MM-Vet is an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities.</p>
                <p><strong>Paper Abstract:</strong> We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combination. For evaluation metrics, we propose an LLM-based evaluator for open-ended outputs. The evaluator enables the evaluation across different question types and answer styles, resulting in a unified scoring metric. We evaluate representative LMMs on MM-Vet, providing insights into the capabilities of different LMM system paradigms and models.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9879.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9879.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MM-Vet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal benchmark introduced in this paper that assesses integrated vision-language capabilities by defining six core VL capabilities (Recognition, OCR, Knowledge, Language Generation, Spatial awareness, Math) and 16 capability integrations across 200 images and 218 questions with open-ended answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>various (evaluated LMMs) / GPT-4 (as evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>The benchmark evaluates a range of large multimodal models (LMMs) and multimodal agents (e.g., LLaVA variants, InstructBLIP, MiniGPT-4, MM-ReAct with GPT-4) and uses GPT-4 (0613) as the automated evaluator; exact model sizes and vision encoders vary per evaluated system and are reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>vision-language / multimodal AI evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Collect open-ended question-answer samples (200 images, 218 Qs) that require one or more core VL capabilities; obtain model outputs and score them using an LLM-based evaluator (GPT-4 few-shot prompt) producing a 0–1 correctness score per sample; aggregate per-sample scores into overall and per-capability metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Per-sample soft correctness score in {0.0,0.1,...,1.0} produced by GPT-4 using few-shot examples; AND/OR semantics for multi-element ground truths; aggregated metrics: S = mean(s_i)*100% and per-capability S_c = mean(s_i over samples in capability)*100%. Evaluation also inspects per-capability and per-integration breakdowns (Recognition, OCR, Knowledge, Generation, Spatial, Math).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>MM-Vet dataset: 200 images, 218 questions (155 human-annotated ground truths + 50 internet-sourced answers + extra VCR and ChestX-ray samples); each question labeled with required capabilities and often open-ended outputs of variable length.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Reported model totals and per-capability scores (examples): GPT-4V ≈ 67.7% total, MM-ReAct-GPT-4 ≈ 44.6%, best open-source end-to-end models ≈ 32.9% (LLaVA-13B LLaMA-2); MM-ReAct (tool-using) excels at OCR/math/spatial integrations; per-capability differences reveal strengths/weaknesses across paradigms.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Focus limited to image-text input/output (no other modalities); evaluator relies on GPT-4 (cost, access); relatively small dataset (200 images) and long-tailed distribution for many capability integrations; closed-source models compared but flagged separately to avoid unfair direct comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Human-annotated ground truths used as gold standard for many samples; LLM-based evaluator (GPT-4) scores were validated against human annotations (average absolute difference reported) and found to be closest among tested LLM evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Report per-capability and per-integration scores (not just total) to provide actionable insights; use LLM-based few-shot evaluation for open-ended outputs with diverse few-shot examples; average multiple evaluator runs to mitigate variance; keep closed-source systems separately tagged to avoid unfair comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9879.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9879.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based evaluator (GPT-4 few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based evaluator using GPT-4 with few-shot grading prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated evaluator that prompts GPT-4 with a set of few-shot grading examples (short and long answers, fully/partially correct cases) to produce a per-sample correctness score in {0.0,...,1.0} comparing model prediction to ground truth for open-ended outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4 (0613)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>GPT-4 (0613) configured as deterministic as possible (temperature 0) and prompted with seven in-context grading examples; used to output soft correctness scores from 0 to 1 for each sample. Multiple evaluator LLMs were also tested (GPT-3.5, LLaMA-2 variants, Mistral, Gemini Pro, Claude 3) for validation.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>automated evaluation / NLP evaluation methodology</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Few-shot prompting: feed question, ground truth, and model prediction into GPT-4 with grading examples; GPT-4 returns a discrete soft grade (0.0,0.1,..,1.0). Repeat evaluation (5 runs) to reduce evaluator output variance; compute averages and aggregated metrics across samples and capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Correctness score per sample (0.0 to 1.0 in 0.1 increments); AND semantics require all elements present in prediction to be fully correct; OR semantics allow any element to be correct; evaluator accounts for both factual correctness and text quality via examples shown in prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to MM-Vet outputs (all evaluated LMMs) and validated on an objective subset (138 objective questions) against human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GPT-4-based evaluator achieved the best alignment with human scores among tested LLM evaluators (average absolute difference Δ̄ = 0.0423). Combining GPT-4 with other LLMs did not reduce Δ̄. Using all seven few-shot examples gave the best alignment from ablation studies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Evaluator cost (GPT-4 usage fees); some evaluator variance remains even at temperature 0; other LLMs (open-source) yielded larger discrepancies and were more 'creative' (less prompt-adherent).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Quantitatively compared to human annotations via average absolute difference per sample; GPT-4 closely matched humans while keyword-matching baseline performed poorly (Δ̄ ≈ 0.273).</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use GPT-4 as the LLM-based evaluator for open-ended multimodal outputs; include diverse few-shot examples (short/long, fully/partially correct cases) to define grading behavior; average multiple evaluator runs to smooth variance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9879.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9879.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scoring rubric (0–1 with AND/OR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Soft correctness scoring rubric with AND/OR semantics and aggregate S, S_c metrics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unified scoring scheme where GPT-4 outputs a discrete score from 0.0 to 1.0 (in 0.1 steps) per sample; ground-truth lists use AND/OR operators to specify element-level correctness; aggregated metrics S (overall) and S_c (per-capability) computed as mean scores ×100%.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4 (as grader)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>GPT-4 uses the few-shot prompt where ground truth may include multi-element lists with <AND>/<OR> semantics; evaluator maps prediction→score according to examples.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>evaluation metrics / benchmark scoring</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Per-sample scoring by GPT-4 with examples that define thresholds for partial correctness; overall score S = (Σ s_i / N) × 100%; per-capability S_c computed similarly over subset of samples requiring a capability.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Discrete soft correctness levels reflecting fully/partially correct responses; explicit AND/OR interpretation for multi-element answers; both factual correctness and language quality considered implicitly via few-shot examples.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Used throughout MM-Vet to compute per-model, per-capability, and per-integration results.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Enabled unified scoring across highly diverse answer styles (short numeric answers to long essays) and supported per-capability breakdowns presented in Tables 2–5; permitted direct numeric comparisons between models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Scoring relies on the evaluator LLM's judgment and may vary with evaluator choice and prompt design; discrete 0.1 steps may not capture very fine-grained differences; AND/OR element matching can be brittle if ground-truth elements are phrased differently.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Validated against human annotations; GPT-4-based rubric aligned closely with human scores (see evaluator validation study).</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Include explicit in-context examples covering fully correct, partially correct, and incorrect cases across short and long answers to train the evaluator on expected grading behavior; report aggregated and per-capability S and S_c.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9879.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9879.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluator human-alignment study</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Averaged absolute difference (Δ̄) comparison of LLM evaluators to human annotations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An experiment validating various LLM evaluators by computing the average absolute difference between LLM-produced sample scores and human-annotated scores on 138 objective MM-Vet questions; used to select the evaluator and prompt design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4, GPT-3.5, LLaMA-2 variants, Mistral variants, Gemini Pro, Claude 3 (tested as evaluators)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Multiple LLMs (open and commercial) were tested as graders; GPT-4 attained the smallest Δ̄ (0.042), commercial models outperformed open-source ones on this task.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>evaluation methodology validation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute |score_LLM - score_human| per sample; report average Δ̄ across samples for each evaluator; compare with a keyword-matching baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Δ̄ (lower is better) with max possible discrepancy 1.0; keyword matching baseline Δ̄ ≈ 0.273; best LLM (GPT-4) Δ̄ ≈ 0.0423.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>MM-ReAct-GPT4 outputs on 138 objective MM-Vet questions (objective subset used for human annotation comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GPT-4 had the lowest Δ̄ (0.0423), indicating closest alignment to humans; commercial models (Gemini, Claude) performed better than open-source LLaMA/Mistral variants; combining other LLMs with GPT-4 did not improve alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Open-source LLMs were sometimes 'more creative' and did not adhere strictly to grading prompt, yielding higher Δ̄; results depend on the subset of objective questions and human annotation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Direct numerical comparison to human annotations used as gold standard; demonstrated LLM-based evaluator (GPT-4) can approximate human grading on objective questions.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use quantitative Δ̄ validation against human annotations when selecting an LLM evaluator; prefer GPT-4 currently for close human alignment; be cautious combining evaluators unless proven to improve Δ̄.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9879.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9879.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot prompt ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation study of few-shot examples in the GPT-4 evaluator prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A controlled ablation that adds progressively more grading examples (from none to all seven) to the GPT-4 prompt and measures the change in Δ̄ to human annotations, showing that including all seven examples produces the best human alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4 (0613)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>The prompt contains seven in-context examples: a mix of short/long answers and fully/partially correct/incorrect cases; adding examples reduces evaluator-human discrepancy.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>prompt engineering / evaluator design</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compute Δ̄ for different prompt designs: None, (1)(5), (1,2,3,5), (1-5), (1-5,7), and (1-7); report Δ̄ for each configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Δ̄ to human annotations as primary metric; lower Δ̄ indicates better prompt design.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Validation subset of MM-Vet (objective questions) used to compute Δ̄.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Using all seven examples yields lowest Δ̄ = 0.0423; progressively adding diverse example types (short/long, partial/full correctness) decreased Δ̄ from 0.0630 (no examples) down to 0.0423.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Ablation performed on this dataset and evaluator; optimal example set may differ for other tasks or domains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Shows that carefully constructed few-shot examples enable the LLM evaluator to better mimic human grading behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Include a variety of grading examples (fully correct, fully wrong, partially correct short, partially correct long) in the evaluator prompt to closely align with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9879.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9879.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MM-Vet model results (per-capability)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Per-capability and integration evaluation results of multiple LMMs on MM-Vet</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Quantitative results reported in the paper: per-model, per-capability scores and capability-integration results (tables) revealing relative strengths/weaknesses across models and paradigms (end-to-end LMMs vs tool-using agents).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Evaluated models include GPT-4V, GPT-4V-Turbo variants, MM-ReAct (GPT-4 / GPT-3.5), LLaVA variants, InstructBLIP, MiniGPT-4, OpenFlamingo, LLaMA-Adapter v2, Otter, BLIP-2 etc.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Evaluated systems range from closed-source commercial multimodal models (GPT-4V, Bard) to open-source end-to-end LMMs and tool-using agents; sizes and vision encoders vary per model (see paper's Table 11).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>multimodal model benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Each model generates open-ended answers on MM-Vet; GPT-4 evaluator assigns per-sample scores; results averaged (and evaluator runs repeated 5× for variance); per-capability and per-integration aggregates computed.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Aggregated correctness percentages per capability (Recognition, OCR, Knowledge, Generation, Spatial, Math) and per-capability-integration columns; overall percentage total reported with variance.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>MM-Vet</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Key quantitative findings: GPT-4V ~67.7% overall; GPT-4V-Turbo (detail:high) ~67.6%; MM-ReAct-GPT-4 ~44.6%; top open-source end-to-end LMM (LLaVA-13B LLaMA-2) ~32.9%; tool-using approaches show large gains on OCR/math/spatial tasks due to specialized tools.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Direct comparisons to closed-source systems flagged separately; some capability integrations are long-tailed (few samples) leading to high variance; small dataset size limits statistical power for some integrations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Results are scoring-model outputs against human-constructed ground truths via GPT-4 evaluator; human annotations provide the gold standard for validation subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use capability-wise breakdowns to diagnose model weaknesses; consider integrating specialized tools for OCR/math; report evaluator variance and validate automated evaluator against human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>MME: A comprehensive evaluation benchmark for multimodal large language models <em>(Rating: 2)</em></li>
                <li>MMBench: Is your multi-modal model an all-around player? <em>(Rating: 2)</em></li>
                <li>MM-ReAct: Prompting chatgpt for multimodal reasoning and action <em>(Rating: 2)</em></li>
                <li>GPT-4 technical report <em>(Rating: 2)</em></li>
                <li>Can large language models be an alternative to human evaluations? <em>(Rating: 1)</em></li>
                <li>GPTEVAL: NLG evaluation using GPT-4 with better human alignment <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9879",
    "paper_id": "paper-94972e30504017156ef5b5debc419bf6edc67384",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "MM-Vet",
            "name_full": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities",
            "brief_description": "A multimodal benchmark introduced in this paper that assesses integrated vision-language capabilities by defining six core VL capabilities (Recognition, OCR, Knowledge, Language Generation, Spatial awareness, Math) and 16 capability integrations across 200 images and 218 questions with open-ended answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "various (evaluated LMMs) / GPT-4 (as evaluator)",
            "llm_description": "The benchmark evaluates a range of large multimodal models (LMMs) and multimodal agents (e.g., LLaVA variants, InstructBLIP, MiniGPT-4, MM-ReAct with GPT-4) and uses GPT-4 (0613) as the automated evaluator; exact model sizes and vision encoders vary per evaluated system and are reported in the paper.",
            "scientific_domain": "vision-language / multimodal AI evaluation",
            "evaluation_method": "Collect open-ended question-answer samples (200 images, 218 Qs) that require one or more core VL capabilities; obtain model outputs and score them using an LLM-based evaluator (GPT-4 few-shot prompt) producing a 0–1 correctness score per sample; aggregate per-sample scores into overall and per-capability metrics.",
            "evaluation_criteria": "Per-sample soft correctness score in {0.0,0.1,...,1.0} produced by GPT-4 using few-shot examples; AND/OR semantics for multi-element ground truths; aggregated metrics: S = mean(s_i)*100% and per-capability S_c = mean(s_i over samples in capability)*100%. Evaluation also inspects per-capability and per-integration breakdowns (Recognition, OCR, Knowledge, Generation, Spatial, Math).",
            "benchmark_or_dataset": "MM-Vet dataset: 200 images, 218 questions (155 human-annotated ground truths + 50 internet-sourced answers + extra VCR and ChestX-ray samples); each question labeled with required capabilities and often open-ended outputs of variable length.",
            "results_summary": "Reported model totals and per-capability scores (examples): GPT-4V ≈ 67.7% total, MM-ReAct-GPT-4 ≈ 44.6%, best open-source end-to-end models ≈ 32.9% (LLaVA-13B LLaMA-2); MM-ReAct (tool-using) excels at OCR/math/spatial integrations; per-capability differences reveal strengths/weaknesses across paradigms.",
            "limitations_or_challenges": "Focus limited to image-text input/output (no other modalities); evaluator relies on GPT-4 (cost, access); relatively small dataset (200 images) and long-tailed distribution for many capability integrations; closed-source models compared but flagged separately to avoid unfair direct comparisons.",
            "comparison_to_human_or_traditional": "Human-annotated ground truths used as gold standard for many samples; LLM-based evaluator (GPT-4) scores were validated against human annotations (average absolute difference reported) and found to be closest among tested LLM evaluators.",
            "recommendations_or_best_practices": "Report per-capability and per-integration scores (not just total) to provide actionable insights; use LLM-based few-shot evaluation for open-ended outputs with diverse few-shot examples; average multiple evaluator runs to mitigate variance; keep closed-source systems separately tagged to avoid unfair comparisons.",
            "uuid": "e9879.0",
            "source_info": {
                "paper_title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "LLM-based evaluator (GPT-4 few-shot)",
            "name_full": "LLM-based evaluator using GPT-4 with few-shot grading prompts",
            "brief_description": "An automated evaluator that prompts GPT-4 with a set of few-shot grading examples (short and long answers, fully/partially correct cases) to produce a per-sample correctness score in {0.0,...,1.0} comparing model prediction to ground truth for open-ended outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-4 (0613)",
            "llm_description": "GPT-4 (0613) configured as deterministic as possible (temperature 0) and prompted with seven in-context grading examples; used to output soft correctness scores from 0 to 1 for each sample. Multiple evaluator LLMs were also tested (GPT-3.5, LLaMA-2 variants, Mistral, Gemini Pro, Claude 3) for validation.",
            "scientific_domain": "automated evaluation / NLP evaluation methodology",
            "evaluation_method": "Few-shot prompting: feed question, ground truth, and model prediction into GPT-4 with grading examples; GPT-4 returns a discrete soft grade (0.0,0.1,..,1.0). Repeat evaluation (5 runs) to reduce evaluator output variance; compute averages and aggregated metrics across samples and capabilities.",
            "evaluation_criteria": "Correctness score per sample (0.0 to 1.0 in 0.1 increments); AND semantics require all elements present in prediction to be fully correct; OR semantics allow any element to be correct; evaluator accounts for both factual correctness and text quality via examples shown in prompt.",
            "benchmark_or_dataset": "Applied to MM-Vet outputs (all evaluated LMMs) and validated on an objective subset (138 objective questions) against human annotations.",
            "results_summary": "GPT-4-based evaluator achieved the best alignment with human scores among tested LLM evaluators (average absolute difference Δ̄ = 0.0423). Combining GPT-4 with other LLMs did not reduce Δ̄. Using all seven few-shot examples gave the best alignment from ablation studies.",
            "limitations_or_challenges": "Evaluator cost (GPT-4 usage fees); some evaluator variance remains even at temperature 0; other LLMs (open-source) yielded larger discrepancies and were more 'creative' (less prompt-adherent).",
            "comparison_to_human_or_traditional": "Quantitatively compared to human annotations via average absolute difference per sample; GPT-4 closely matched humans while keyword-matching baseline performed poorly (Δ̄ ≈ 0.273).",
            "recommendations_or_best_practices": "Use GPT-4 as the LLM-based evaluator for open-ended multimodal outputs; include diverse few-shot examples (short/long, fully/partially correct cases) to define grading behavior; average multiple evaluator runs to smooth variance.",
            "uuid": "e9879.1",
            "source_info": {
                "paper_title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Scoring rubric (0–1 with AND/OR)",
            "name_full": "Soft correctness scoring rubric with AND/OR semantics and aggregate S, S_c metrics",
            "brief_description": "A unified scoring scheme where GPT-4 outputs a discrete score from 0.0 to 1.0 (in 0.1 steps) per sample; ground-truth lists use AND/OR operators to specify element-level correctness; aggregated metrics S (overall) and S_c (per-capability) computed as mean scores ×100%.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-4 (as grader)",
            "llm_description": "GPT-4 uses the few-shot prompt where ground truth may include multi-element lists with &lt;AND&gt;/&lt;OR&gt; semantics; evaluator maps prediction→score according to examples.",
            "scientific_domain": "evaluation metrics / benchmark scoring",
            "evaluation_method": "Per-sample scoring by GPT-4 with examples that define thresholds for partial correctness; overall score S = (Σ s_i / N) × 100%; per-capability S_c computed similarly over subset of samples requiring a capability.",
            "evaluation_criteria": "Discrete soft correctness levels reflecting fully/partially correct responses; explicit AND/OR interpretation for multi-element answers; both factual correctness and language quality considered implicitly via few-shot examples.",
            "benchmark_or_dataset": "Used throughout MM-Vet to compute per-model, per-capability, and per-integration results.",
            "results_summary": "Enabled unified scoring across highly diverse answer styles (short numeric answers to long essays) and supported per-capability breakdowns presented in Tables 2–5; permitted direct numeric comparisons between models.",
            "limitations_or_challenges": "Scoring relies on the evaluator LLM's judgment and may vary with evaluator choice and prompt design; discrete 0.1 steps may not capture very fine-grained differences; AND/OR element matching can be brittle if ground-truth elements are phrased differently.",
            "comparison_to_human_or_traditional": "Validated against human annotations; GPT-4-based rubric aligned closely with human scores (see evaluator validation study).",
            "recommendations_or_best_practices": "Include explicit in-context examples covering fully correct, partially correct, and incorrect cases across short and long answers to train the evaluator on expected grading behavior; report aggregated and per-capability S and S_c.",
            "uuid": "e9879.2",
            "source_info": {
                "paper_title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Evaluator human-alignment study",
            "name_full": "Averaged absolute difference (Δ̄) comparison of LLM evaluators to human annotations",
            "brief_description": "An experiment validating various LLM evaluators by computing the average absolute difference between LLM-produced sample scores and human-annotated scores on 138 objective MM-Vet questions; used to select the evaluator and prompt design.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-4, GPT-3.5, LLaMA-2 variants, Mistral variants, Gemini Pro, Claude 3 (tested as evaluators)",
            "llm_description": "Multiple LLMs (open and commercial) were tested as graders; GPT-4 attained the smallest Δ̄ (0.042), commercial models outperformed open-source ones on this task.",
            "scientific_domain": "evaluation methodology validation",
            "evaluation_method": "Compute |score_LLM - score_human| per sample; report average Δ̄ across samples for each evaluator; compare with a keyword-matching baseline.",
            "evaluation_criteria": "Δ̄ (lower is better) with max possible discrepancy 1.0; keyword matching baseline Δ̄ ≈ 0.273; best LLM (GPT-4) Δ̄ ≈ 0.0423.",
            "benchmark_or_dataset": "MM-ReAct-GPT4 outputs on 138 objective MM-Vet questions (objective subset used for human annotation comparison).",
            "results_summary": "GPT-4 had the lowest Δ̄ (0.0423), indicating closest alignment to humans; commercial models (Gemini, Claude) performed better than open-source LLaMA/Mistral variants; combining other LLMs with GPT-4 did not improve alignment.",
            "limitations_or_challenges": "Open-source LLMs were sometimes 'more creative' and did not adhere strictly to grading prompt, yielding higher Δ̄; results depend on the subset of objective questions and human annotation quality.",
            "comparison_to_human_or_traditional": "Direct numerical comparison to human annotations used as gold standard; demonstrated LLM-based evaluator (GPT-4) can approximate human grading on objective questions.",
            "recommendations_or_best_practices": "Use quantitative Δ̄ validation against human annotations when selecting an LLM evaluator; prefer GPT-4 currently for close human alignment; be cautious combining evaluators unless proven to improve Δ̄.",
            "uuid": "e9879.3",
            "source_info": {
                "paper_title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Few-shot prompt ablation",
            "name_full": "Ablation study of few-shot examples in the GPT-4 evaluator prompt",
            "brief_description": "A controlled ablation that adds progressively more grading examples (from none to all seven) to the GPT-4 prompt and measures the change in Δ̄ to human annotations, showing that including all seven examples produces the best human alignment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-4 (0613)",
            "llm_description": "The prompt contains seven in-context examples: a mix of short/long answers and fully/partially correct/incorrect cases; adding examples reduces evaluator-human discrepancy.",
            "scientific_domain": "prompt engineering / evaluator design",
            "evaluation_method": "Compute Δ̄ for different prompt designs: None, (1)(5), (1,2,3,5), (1-5), (1-5,7), and (1-7); report Δ̄ for each configuration.",
            "evaluation_criteria": "Δ̄ to human annotations as primary metric; lower Δ̄ indicates better prompt design.",
            "benchmark_or_dataset": "Validation subset of MM-Vet (objective questions) used to compute Δ̄.",
            "results_summary": "Using all seven examples yields lowest Δ̄ = 0.0423; progressively adding diverse example types (short/long, partial/full correctness) decreased Δ̄ from 0.0630 (no examples) down to 0.0423.",
            "limitations_or_challenges": "Ablation performed on this dataset and evaluator; optimal example set may differ for other tasks or domains.",
            "comparison_to_human_or_traditional": "Shows that carefully constructed few-shot examples enable the LLM evaluator to better mimic human grading behavior.",
            "recommendations_or_best_practices": "Include a variety of grading examples (fully correct, fully wrong, partially correct short, partially correct long) in the evaluator prompt to closely align with human judgments.",
            "uuid": "e9879.4",
            "source_info": {
                "paper_title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "MM-Vet model results (per-capability)",
            "name_full": "Per-capability and integration evaluation results of multiple LMMs on MM-Vet",
            "brief_description": "Quantitative results reported in the paper: per-model, per-capability scores and capability-integration results (tables) revealing relative strengths/weaknesses across models and paradigms (end-to-end LMMs vs tool-using agents).",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Evaluated models include GPT-4V, GPT-4V-Turbo variants, MM-ReAct (GPT-4 / GPT-3.5), LLaVA variants, InstructBLIP, MiniGPT-4, OpenFlamingo, LLaMA-Adapter v2, Otter, BLIP-2 etc.",
            "llm_description": "Evaluated systems range from closed-source commercial multimodal models (GPT-4V, Bard) to open-source end-to-end LMMs and tool-using agents; sizes and vision encoders vary per model (see paper's Table 11).",
            "scientific_domain": "multimodal model benchmarking",
            "evaluation_method": "Each model generates open-ended answers on MM-Vet; GPT-4 evaluator assigns per-sample scores; results averaged (and evaluator runs repeated 5× for variance); per-capability and per-integration aggregates computed.",
            "evaluation_criteria": "Aggregated correctness percentages per capability (Recognition, OCR, Knowledge, Generation, Spatial, Math) and per-capability-integration columns; overall percentage total reported with variance.",
            "benchmark_or_dataset": "MM-Vet",
            "results_summary": "Key quantitative findings: GPT-4V ~67.7% overall; GPT-4V-Turbo (detail:high) ~67.6%; MM-ReAct-GPT-4 ~44.6%; top open-source end-to-end LMM (LLaVA-13B LLaMA-2) ~32.9%; tool-using approaches show large gains on OCR/math/spatial tasks due to specialized tools.",
            "limitations_or_challenges": "Direct comparisons to closed-source systems flagged separately; some capability integrations are long-tailed (few samples) leading to high variance; small dataset size limits statistical power for some integrations.",
            "comparison_to_human_or_traditional": "Results are scoring-model outputs against human-constructed ground truths via GPT-4 evaluator; human annotations provide the gold standard for validation subsets.",
            "recommendations_or_best_practices": "Use capability-wise breakdowns to diagnose model weaknesses; consider integrating specialized tools for OCR/math; report evaluator variance and validate automated evaluator against human annotations.",
            "uuid": "e9879.5",
            "source_info": {
                "paper_title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "MME: A comprehensive evaluation benchmark for multimodal large language models",
            "rating": 2
        },
        {
            "paper_title": "MMBench: Is your multi-modal model an all-around player?",
            "rating": 2
        },
        {
            "paper_title": "MM-ReAct: Prompting chatgpt for multimodal reasoning and action",
            "rating": 2
        },
        {
            "paper_title": "GPT-4 technical report",
            "rating": 2
        },
        {
            "paper_title": "Can large language models be an alternative to human evaluations?",
            "rating": 1
        },
        {
            "paper_title": "GPTEVAL: NLG evaluation using GPT-4 with better human alignment",
            "rating": 1
        }
    ],
    "cost": 0.01840575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities</h1>
<p>Weihao $\mathbf{Y u}{ }^{<em> 1}$ Zhengyuan Yang ${ }^{</em> 2}$ Linjie $\mathbf{L i}^{2}$ Jianfeng Wang ${ }^{2}$ Kevin Lin ${ }^{2}$<br>Zicheng Liu ${ }^{2}$ Xinchao Wang ${ }^{1}$ Lijuan Wang ${ }^{2}$</p>
<h4>Abstract</h4>
<p>We propose MM-Vet ${ }^{1}$, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MMVet, designed based on the insight that the intriguing ability to solve complicated tasks often stems from a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from their combinations. For evaluation metrics, we propose an LLM-based evaluator for open-ended outputs. The evaluator enables the evaluation across different question types and answer styles, resulting in a unified scoring metric. We evaluate representative LMMs on MMVet, providing insights into the capabilities of different LMM system paradigms and model designs. Code and data are available at https: / / github.com/yuweihao/MM-Vet, and the online evaluator at https://huggingface. co/spaces/whyu/MM-Vet_Evaluator.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The benchmarks differ in their required capabilities. While standard VL benchmarks (Chen et al., 2015; Antol et al., 2015; Singh et al., 2019) typically require only one or two capabilities, MM-Vet focuses on the integration of multiple core VL capabilities. These include recognition, OCR, knowledge, language generation, spatial awareness, and math.</p>
<h2>1. Introduction</h2>
<p>The breakthroughs in large language models (LLMs) (Brown et al., 2020a; OpenAI, 2023c; Chowdhery et al., 2022; Anil et al., 2023; Touvron et al., 2023a; Hoffmann et al., 2022) bring generalist AI models that can solve a wide range of complicated natural language tasks, many approaching the human-expert-level performance (OpenAI, 2023c; Bubeck et al., 2023; Yang et al., 2023b). Large multimodal models (LMMs) aim to achieve even stronger general intelligence via extending LLMs with multimodal inputs. Since more than $80 \%$ of our human being's perception, learning, cognition, and activities are mediated through vision (Politzer), it is natural to start the exploration by equipping LLMs with "eyes." One main thread of LMM works, represented by Frozen (Tsimpoukelli et al.,</p>
<p>2021), Flamingo (Alayrac et al., 2022), PaLM-E (Driess et al., 2023), GPT-4V (OpenAI, 2023c;a), extend LLMs with the visual understanding capability via end-to-end tuning. There also exists the exploration (Yang et al., 2022b; Zeng et al., 2022; Yang et al., 2023c; Shen et al., 2023; Gao et al., 2023a) on the modular combination of LLMs and image-to-text vision-language models. Recently, thanks to the open-source of powerful LLMs like LLaMA (Touvron et al., 2023a), more open-sourced LMMs are built, including OpenFlamingo (Awadalla et al., 2023a), LLaVA (Liu et al., 2023c), MiniGPT-4 (Zhu et al., 2023a), Otter (Li et al., 2023c), InstructBLIP (Dai et al., 2023), and many more (Gong et al., 2023; Liu et al., 2023b; Ye et al., 2023). These studies showcase the intriguing ability to solve various complicated multimodal tasks, such as open-world recognition, multimodal knowledge and commonsense, scene text understanding, and so on.</p>
<p>Despite the promising qualitative results on LMM's capabilities, it remains unclear how to systematically evaluate those showcased complicated multimodal tasks, and what are the relationships among evaluated tasks, which is the first step in developing a quantitative evaluation benchmark. As shown in Figure 1, existing VL benchmarks (Antol et al., 2015; Chen et al., 2015; Singh et al., 2019) focus on straightforward VL tasks that test specific one or two capabilities, such as recognition, language generation, or OCR, but fall short in benchmarking more complicated tasks. In contrast, we examine the integration of multiple core VL capabilities for more complicated tasks. This is based on the insight that the intriguing ability to solve complicated multimodal tasks can be achieved by a generalist model mastering and integrating these core capabilities. Following this insight, we propose a new benchmark for evaluating LMMs, namely MM-Vet. MM-Vet defines six core VL capabilities, including recognition, OCR, knowledge, language generation, spatial awareness, and math, which integrate to solve various complicated multimodal tasks. MM-Vet contains 16 tasks for quantitative evaluation. For example, in Figure 1(d), answering the question "What will the girl on the right write on the board?" in MM-Vet requires recognizing the genders of the three kids, locating queried girl spatially, recognizing the scene text written by the girl, and finally calculating the result.</p>
<p>Other than the evaluation category topology, finding effective evaluation metric is another challenge in benchmark development, given the diverse answer styles and question types. Specifically: (1) The desired outputs in different multimodal tasks have diverse formats, e.g., Figure 1(d)'s math problem can be answered by a single word, while outputs for the essay writing question are hundred-words long; (2) The core aspect to evaluate in different tasks varies, e.g., text generation focuses more on the text quality, recognition can be considered correct with the key concept recognized.</p>
<p>Most integrated tasks would require comprehensive evaluations from multiple dimensions. Inspired by recent NLP studies (Chiang \&amp; Lee, 2023; Liu et al., 2023e; Fu et al., 2023b) that use LLMs for model evaluation, we propose an LLM-based evaluator as the evaluation metric for openended model outputs. As shown in Table 1, we prompt GPT4 (OpenAI, 2023c) with few-shot evaluation prompts to obtain an evaluation score ranging from 0 to 1 , conditioned on the question, prediction, and GT annotation. Instead of manually defining the possible answer styles and question types, we include different sample types as few-shot examples and let LLMs infer the scoring criteria automatically. Such metric design eases the future extension to more question types, such as box localization (Chen et al., 2022; Yang et al., 2022a; Wang et al., 2023).</p>
<p>MM-Vet's evaluation category and metric designs allow users to obtain per-capability insights for different LMMs. Such model analyses can be more informative than a single overall ranking, which highly depends on the dataset sample composition and might be biased. We evaluate two sets of multimodal systems, i.e., the end-to-end tuned LMMs including OpenFlamingo (Awadalla et al., 2023a), LLaVA (Liu et al., 2023c), MiniGPT-4 (Zhu et al., 2023a), Otter (Li et al., 2023c), InstructBLIP (Dai et al., 2023), etc, and the LLM-tool-using systems (Yang et al., 2023c; Shen et al., 2023; Gao et al., 2023a; Huggingface, 2023) such as MM-ReAct (Yang et al., 2023c) and Transformers Agent (Huggingface, 2023). Despite not knowing model details, we also evaluate industry solutions such as GPT4V (OpenAI, 2023a) and Bard (Google, 2023), which are separately tagged to avoid unfair direct comparisons. We first discuss the capability analyses of these two system paradigms and their representative models. We then dive deeper into the open-sourced LMMs and examine how the training data, vision encoder, and LLM selection influence the performance on different capabilities.</p>
<p>Our contributions are summarized as follows.</p>
<ul>
<li>We propose MM-Vet to evaluate LMMs' ability on complicated multimodal tasks. MM-Vet considers 16 emergent tasks, integrated from 6 defined core VL capabilities.</li>
<li>We propose an LLM-based evaluator for open-ended outputs from LMMs, which unifies the evaluation across different answer styles and question types. The evaluation metrics ensure the thorough evaluation of both the factual correctness and text quality of the responses.</li>
<li>We benchmark representative LMMs on MM-Vet, revealing the relative strengths and weaknesses of different system paradigms and models, as summarized in Section 4.6.</li>
</ul>
<h2>2. Related work</h2>
<p>Multimodal models. Vision-language models (Chen et al., 2015; Goyal et al., 2017; Lu et al., 2019; Chen et al., 2020; Li et al., 2020; Kim et al., 2021; Wang et al., 2022b;a; Yang et al., 2022a; Gan et al., 2022) approach multimodal intelligence of jointly understanding and generating vision and language signals. Inspired by the impressive quality and genericity in recent large language models (LLMs) (Brown et al., 2020b; OpenAI, 2023c; Chowdhery et al., 2022; Touvron et al., 2023a), researchers explore large multimodal models (LMMs) that seamlessly integrate different visionlanguage capabilities to solve complicated multimodal tasks. In approaching such multimodal generalist systems, one direction is to extend LLMs with the multi-sensory ability, such as pioneer works Frozen (Tsimpoukelli et al., 2021), Flamingo (Alayrac et al., 2022), PaLM-E (Driess et al., 2023) and GPT-4V (OpenAI, 2023c;a). Recent open-sourced LLMs (Zhang et al., 2022; Touvron et al., 2023a; Peng et al., 2023) also facilitate various research studies including OpenFlamingo (Awadalla et al., 2023a), LLaVA (Liu et al., 2023c), MiniGPT-4 (Zhu et al., 2023a), Otter (Li et al., 2023c), InstructBLIP (Dai et al., 2023), and so on (Gong et al., 2023; Liu et al., 2023b; Ye et al., 2023). On the other hand, multimodal agents (Yang et al., 2023c; Shen et al., 2023; Huggingface, 2023; Gao et al., 2023a) explore chaining different vision tools with LLMs (Brown et al., 2020b; OpenAI, 2023c) to achieve integrated visionlanguage capabilities.</p>
<p>VL benchmarks. Classic VL benchmarks focus on specific capabilities of interest, such as visual recognition (Goyal et al., 2017), image description (Chen et al., 2015; Agrawal et al., 2019), as well as other benchmarks for specialized capabilities such as scene text understanding (Singh et al., 2019; Sidorov et al., 2020; Yang et al., 2021), commonsense reasoning (Zellers et al., 2019), and outside knowledge (Marino et al., 2019). The recent development of generalist LMMs posts a strong need for modernized VL benchmarks, which contain complicated multimodal tasks that require integrated VL capabilities.</p>
<p>Our MM-Vet is most related to the concurrent evaluation studies (Fu et al., 2023a; Liu et al., 2023d; Li et al., 2023a; Xu et al., 2023; Liu et al., 2023a) such as MME and MMBench, which design comprehensive evaluation samples to facilitate the LMM evaluation. One major difference is that MM-Vet defines and studies the integrated VL capabilities, allowing the evaluation to provide insights beyond the overall model ranking.</p>
<p>LLM-based evaluation. MM-Vet adopts the open-ended LLM-based evaluator, allowing the evaluation across answer styles and question types without requiring binary or multiple answer choices. The technique of prompting LLMs for model evaluation is related to the explorations
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: MM-Vet proportion of capabilities. (a) The proportion of each capability. The sum of the proportion is larger than $100 \%$ because most samples have more than one capability. (b) The proportion of capability integrations. The sum of the proportion is equivalent to $100 \%$.
in NLP (Chiang \&amp; Lee, 2023; Liu et al., 2023e; Fu et al., 2023b). We show that the technique extends well to multimodal tasks, and presents a unified prompt to evaluate samples with different answer styles and question types.</p>
<h2>3. MM-Vet</h2>
<h3>3.1. Data collection</h3>
<p>Our aim is to develop a multimodal benchmark that requires comprehensive capabilities, corresponding to realistic scenarios an AI agent might encounter. Consider, for instance, this scenario: Awakening from slumber, you reach out for your smartphone (recognition capability) to check the current time (OCR capability). Today, your plan is to visit an unfamiliar grocery store. With the knowledge that it's located opposite a stadium and beside a cinema (spatial awareness), you manage to locate it successfully. Remembering your doctor's advice to lose weight, you avoid highcalorie items and instead pick up milk, vegetables, and fruits (knowledge capability). In the dairy section, you saw two options of pure milk: one liter at $\$ 4$ with a $20 \%$ discount, and 1.5 liters at $\$ 7$ with a $25 \%$ discount. After some quick arithmetic, you find the former is cheaper (math capability) and select the one-liter carton. Later, as you pass by the cinema, you see someone gesturing towards a poster while introducing a new movie (language generation).</p>
<p>From the scenarios of interest, we summarize the following six core VL capabilities for evaluation, with corresponding MM-Vet examples shown in Appendix Tables 12-17.</p>
<ul>
<li>Recognition (Rec). Recognition refers to the general visual recognition capability, including recogniz-</li>
</ul>
<p>ing scenes, objects, object attributes (color, material, shape, etc), counting, and various other high-level visual recognition tasks in computer vision.</p>
<ul>
<li>Knowledge (Know). The knowledge category covers various knowledge-related capabilities, including social and visual commonsense knowledge, encyclopedic knowledge, and time-sensitive knowledge like news. This capability necessitates that the model not only possesses such knowledge, but also effectively utilizes it to solve complicated tasks as required.</li>
<li>OCR. Optical character recognition (OCR) refers to the scene text understanding and reasoning capability. The models are tested to read the scene text in images, and reason over the texts to solve various tasks.</li>
<li>Spatial awareness (Spat). Spatial awareness embodies a diverse spectrum of capabilities related to understanding space, including the comprehension of the spatial relationship among object and scene text regions.</li>
<li>Language generation (Gen). Language generation is a vital ability that empowers models to articulate their responses in a clear, engaging, and informative manner. We use questions that demand more extended answers for language generation capacity evaluation.</li>
<li>Math. Math evaluates the model's arithmetic capability in solving math equations or problems in the wild.</li>
</ul>
<p>In real-world scenarios, various complicated multimodal tasks would require the integrations of different core VL capabilities. For instance, explaining visual jokes as shown in Appendix Table 12(a) requires recognition, knowledge of humor, and language generation; reading documents and solving math problems as shown in Appendix Table 13(a) takes OCR, spatial awareness and math; and answering exam questions given images as shown in Appendix Table 16(b) needs OCR, knowledge, spatial awareness. To solve these complicated tasks, LMMs are expected to seamlessly integrate different VL capabilities. Therefore, it is crucial to establish a benchmark that evaluates the performance of these integrated abilities within LMMs.</p>
<p>To build the benchmark, we have gathered 187 images from various online sources and ask 205 questions, each of which requires one or more capabilities to answer. As shown in Appendix Tables 12-17, these questions are varied in type and entail open-ended responses of differing lengths. The ground truths for 155 questions are carefully annotated by us to ensure high quality, while the remainder of the answers for 50 questions were gathered from the Internet. In addition to the 187 images, ten extra images with highquality questions are collected from VCR (Zellers et al.,</p>
<p>Table 1: Few-shot prompt for evaluating model outputs using GPT-4, where $\mathcal{Q}$ is a sample's question, $\mathcal{G}$ is the ground truth and $\mathcal{P}$ is the model output for the sample. In the prompt, there are examples with short and long openended answers, enabling the evaluation of diverse answer styles. Taking the prompt filled with $\mathcal{Q}, \mathcal{G}$ and $\mathcal{P}$, GPT-4 will generate a soft grading score from 0 to 1 .</p>
<p>Compare the ground truth and prediction from AI models, to give a correctness score for the prediction. <AND> in the ground truth means it is totally right only when all elements in the ground truth are present in the prediction, and <OR> means it is totally right when any one element in the ground truth is present in the prediction. The correctness score is 0.0 (totally wrong), $0.1,0.2,0.3,0.4,0.5,0.6$, $0.7,0.8,0.9$, or 1.0 (totally right). Just complete the last space of the correctness score.</p>
<p>Question I Ground truth I Prediction I Correctness
$-1-1-1-$
What is x in the equation? 1-1 <AND> -51 x $=310.0$
What is x in the equation? 1-1 <AND> -51 x $=-110.5$
What is x in the equation? 1-1 <AND> -51 x $=-310.5$
What is x in the equation? 1-1 <AND> -51 x $=-5$ or 510.5
What is x in the equation? 1-1 <AND> -51 x $=-1$ or x $=-511.0$
Can you explain this meme? I This meme is poking fun at the fact that the names of the countries Iceland and Greenland are misleading. Despite its name, Iceland is known for its beautiful green landscapes, while Greenland is mostly covered in ice and snow. The meme is saying that the person has trust issues because the names of these countries do not accurately represent their landscapes. 1 The meme talks about Iceland and Greenland. It's pointing out that despite their names, Iceland is not very icy and Greenland isn't very green. 10.4 Can you explain this meme? I This meme is poking fun at the fact that the names of the countries Iceland and Greenland are misleading. Despite its name, Iceland is known for its beautiful green landscapes, while Greenland is mostly covered in ice and snow. The meme is saying that the person has trust issues because the names of these countries do not accurately represent their landscapes. I The meme is using humor to point out the misleading nature of Iceland's and Greenland's names. Iceland, despite its name, has lush green landscapes while Greenland is mostly covered in ice and snow. The text 'This is why I have trust issues' is a playful way to suggest that these contradictions can lead to distrust or confusion. The humor in this meme is derived from the unexpected contrast between the names of the countries and their actual physical characteristics. 11.0
$\varnothing \mid \varnothing \mid \mathcal{P} 1$</p>
<p>2019), with the questions and answers modified to an openended answering format. Another three images are from ChestX-ray14 (Wang et al., 2017) to obtain corresponding medical expert knowledge. In total, our MM-Vet contains 200 images, and 218 questions (samples), all paired with their respective ground truths. For each question, we have also identified the capacities required to answer them and displayed this information statistically in Figure 2.</p>
<h3>3.2. LLM-based evaluator for open-ended outputs</h3>
<p>Questions and expected responses in MM-Vet are designed to be open-ended to cover the diverse real-world scenarios. This naturally poses a great challenge in terms of model evaluation and metric design. Drawing inspiration from recent NLP studies (Chiang \&amp; Lee, 2023; Zheng et al., 2023) that utilize LLMs for open-ended evaluations, we leverage</p>
<p>GPT-4 to assist evaluation. As shown in Table 1, we craft a few-shot prompt for model evaluation. The few-shot design allows us to define the scoring metrics via in-context examples and supports easy extension onto new problem sets. Specifically, our implemented prompt incorporates five in-context examples with open-ended short answers and two examples with long answers. We cover examples that are fully correct (i.e., 1.0) or incorrect (i.e., 0.0), as well as examples used to define different types of "partially correct" responses. The LLM-based evaluator allows any style of model outputs to be evaluated with a unified consistent metric. Furthermore, it also supports easy adaptation to diverse question types and answer styles by simply modifying the evaluation examples.</p>
<p>By inputting the prompt, GPT-4 automatically generates scores for each sample, conditioned on each sample's input question, ground truth, and model output. The score for each sample ranges from 0 to 1 . The total scores are computed by</p>
<p>$$
S=\frac{\sum_{i=1}^{N} s_{i}}{N} \times 100 \%
$$</p>
<p>where $s_{i}$ is the score of sample $i$, and $N$ is the sample number. The score regarding each capability or capability integration can be similarly obtained by</p>
<p>$$
S_{c}=\frac{\sum s_{i}}{N_{c}} \times 100 \%, \quad i \in C
$$</p>
<p>where $C$ is the set of samples requiring a specific capability or capability integration, and $N_{c}$ is the sample number of the set.</p>
<h2>4. Evaluation results</h2>
<h3>4.1. Experiment settings</h3>
<p>We utilize MM-Vet to evaluate two types of LMMs, i.e., (1) end-to-end tuned LMMs (OpenFlamingo (Alayrac et al., 2022; Awadalla et al., 2023a;b), BLIP-2 (Li et al., 2023d), LLaVA (Liu et al., 2023c), MiniGPT-4 (Zhu et al., 2023a), LLaMA-Adapter V2 (Gao et al., 2023b), Otter (Li et al., 2023c) and InstructBLIP (Dai et al., 2023)); (2) LLM-toolusing methods (MM-ReAct (Yang et al., 2023c) and Transformers Agent (Huggingface, 2023)). The summary of these methods is shown in Appendix Table 11. As shown in Table 1, for each sample, we fill the prompt template with its question, ground truth, and output from a specific LMM. By taking the filled prompt into GPT-4, GPT-4 will generate a score from 0 to 1 for the sample. It is found that outputs of GPT-4 still exist variance, although the temperature is set as 0 . Therefore, we utilize GPT-4 to evaluate the outputs of LLMs by 5 times. Due to the space limit, we report average scores for capabilities/capability integrations, and average as well as variance for total score.</p>
<p>Table 2: MM-Vet evaluation results on various LMMs regarding each core VL capability. For each column, the highest, second, and third highest figures are highlighted by green, orange and blue colors. Numbers are presented in \% with a full score of $100 \%$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">Rec</th>
<th style="text-align: right;">OCR</th>
<th style="text-align: right;">Know</th>
<th style="text-align: right;">Gen</th>
<th style="text-align: right;">Spat</th>
<th style="text-align: right;">Math</th>
<th style="text-align: right;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Transformers Agent (GPT-4)</td>
<td style="text-align: right;">18.2</td>
<td style="text-align: right;">3.9</td>
<td style="text-align: right;">2.2</td>
<td style="text-align: right;">3.2</td>
<td style="text-align: right;">12.4</td>
<td style="text-align: right;">4.0</td>
<td style="text-align: right;">$13.4 \pm 0.5$</td>
</tr>
<tr>
<td style="text-align: left;">MiniGPT-4-8B</td>
<td style="text-align: right;">27.4</td>
<td style="text-align: right;">15.0</td>
<td style="text-align: right;">12.8</td>
<td style="text-align: right;">13.9</td>
<td style="text-align: right;">20.3</td>
<td style="text-align: right;">7.7</td>
<td style="text-align: right;">$22.1 \pm 0.1$</td>
</tr>
<tr>
<td style="text-align: left;">BLIP-2-12B</td>
<td style="text-align: right;">27.5</td>
<td style="text-align: right;">11.1</td>
<td style="text-align: right;">11.8</td>
<td style="text-align: right;">7.0</td>
<td style="text-align: right;">16.2</td>
<td style="text-align: right;">5.8</td>
<td style="text-align: right;">$22.4 \pm 0.2$</td>
</tr>
<tr>
<td style="text-align: left;">LLaVA-7B</td>
<td style="text-align: right;">28.0</td>
<td style="text-align: right;">17.1</td>
<td style="text-align: right;">16.3</td>
<td style="text-align: right;">18.9</td>
<td style="text-align: right;">21.2</td>
<td style="text-align: right;">11.5</td>
<td style="text-align: right;">$23.8 \pm 0.6$</td>
</tr>
<tr>
<td style="text-align: left;">MiniGPT-4-14B</td>
<td style="text-align: right;">29.9</td>
<td style="text-align: right;">16.1</td>
<td style="text-align: right;">20.4</td>
<td style="text-align: right;">22.1</td>
<td style="text-align: right;">22.2</td>
<td style="text-align: right;">3.8</td>
<td style="text-align: right;">$24.4 \pm 0.4$</td>
</tr>
<tr>
<td style="text-align: left;">Otter-9B</td>
<td style="text-align: right;">27.3</td>
<td style="text-align: right;">17.8</td>
<td style="text-align: right;">14.2</td>
<td style="text-align: right;">13.8</td>
<td style="text-align: right;">24.4</td>
<td style="text-align: right;">3.8</td>
<td style="text-align: right;">$24.7 \pm 0.3$</td>
</tr>
<tr>
<td style="text-align: left;">OpenFlamingo-9B</td>
<td style="text-align: right;">28.7</td>
<td style="text-align: right;">16.7</td>
<td style="text-align: right;">16.4</td>
<td style="text-align: right;">13.1</td>
<td style="text-align: right;">21.0</td>
<td style="text-align: right;">7.7</td>
<td style="text-align: right;">$24.8 \pm 0.2$</td>
</tr>
<tr>
<td style="text-align: left;">InstructBLIP-14B</td>
<td style="text-align: right;">30.8</td>
<td style="text-align: right;">16.0</td>
<td style="text-align: right;">9.8</td>
<td style="text-align: right;">9.0</td>
<td style="text-align: right;">21.1</td>
<td style="text-align: right;">10.5</td>
<td style="text-align: right;">$25.6 \pm 0.3$</td>
</tr>
<tr>
<td style="text-align: left;">InstructBLIP-8B</td>
<td style="text-align: right;">32.4</td>
<td style="text-align: right;">14.6</td>
<td style="text-align: right;">16.5</td>
<td style="text-align: right;">18.2</td>
<td style="text-align: right;">18.6</td>
<td style="text-align: right;">7.7</td>
<td style="text-align: right;">$26.2 \pm 0.2$</td>
</tr>
<tr>
<td style="text-align: left;">LLaVA-13B</td>
<td style="text-align: right;">30.9</td>
<td style="text-align: right;">20.1</td>
<td style="text-align: right;">23.5</td>
<td style="text-align: right;">26.4</td>
<td style="text-align: right;">24.3</td>
<td style="text-align: right;">7.7</td>
<td style="text-align: right;">$26.4 \pm 0.1$</td>
</tr>
<tr>
<td style="text-align: left;">MM-ReAct-GPT-3.5</td>
<td style="text-align: right;">24.2</td>
<td style="text-align: right;">31.5</td>
<td style="text-align: right;">21.5</td>
<td style="text-align: right;">20.7</td>
<td style="text-align: right;">32.3</td>
<td style="text-align: right;">26.2</td>
<td style="text-align: right;">$27.9 \pm 0.1$</td>
</tr>
<tr>
<td style="text-align: left;">LLaVA-7B (LLaMA-2)</td>
<td style="text-align: right;">32.9</td>
<td style="text-align: right;">20.1</td>
<td style="text-align: right;">19.0</td>
<td style="text-align: right;">20.1</td>
<td style="text-align: right;">25.7</td>
<td style="text-align: right;">5.2</td>
<td style="text-align: right;">$28.1 \pm 0.4$</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA-Adapter v2-7B</td>
<td style="text-align: right;">38.5</td>
<td style="text-align: right;">20.3</td>
<td style="text-align: right;">31.4</td>
<td style="text-align: right;">33.4</td>
<td style="text-align: right;">22.9</td>
<td style="text-align: right;">3.8</td>
<td style="text-align: right;">$31.4 \pm 0.1$</td>
</tr>
<tr>
<td style="text-align: left;">LLaVA-13B (V1.3, 336px)</td>
<td style="text-align: right;">38.1</td>
<td style="text-align: right;">22.3</td>
<td style="text-align: right;">23.2</td>
<td style="text-align: right;">25.8</td>
<td style="text-align: right;">31.3</td>
<td style="text-align: right;">11.2</td>
<td style="text-align: right;">$32.5 \pm 0.1$</td>
</tr>
<tr>
<td style="text-align: left;">LLaVA-13B (LLaMA-2)</td>
<td style="text-align: right;">39.2</td>
<td style="text-align: right;">22.7</td>
<td style="text-align: right;">26.5</td>
<td style="text-align: right;">29.3</td>
<td style="text-align: right;">29.6</td>
<td style="text-align: right;">7.7</td>
<td style="text-align: right;">$32.9 \pm 0.1$</td>
</tr>
<tr>
<td style="text-align: left;">MM-ReAct-GPT-4</td>
<td style="text-align: right;">33.1</td>
<td style="text-align: right;">65.7</td>
<td style="text-align: right;">29.0</td>
<td style="text-align: right;">35.0</td>
<td style="text-align: right;">56.8</td>
<td style="text-align: right;">69.2</td>
<td style="text-align: right;">$44.6 \pm 0.2$</td>
</tr>
</tbody>
</table>
<h3>4.2. Result analyses</h3>
<p>The main results of different methods are shown in Table 2 regarding each capability, and Table 3 for each capability integration.</p>
<h3>4.2.1. REGARDING EACH CAPABILITY</h3>
<p>Recognition. The "Recognition" category contains the questions requiring recognition capability to answer. Examples are shown in Appendix Tables 12(a, b), 13(b), 14(a, b), 15(a, b), 16(a, c), and 17(b). The "Rec" column in Table 2 compares the performance on the "Recognition". Among the evaluated models, LLaVA-13B (LLaMA-2) is the best one, obtaining $39.2 \%$. There may be two reasons. First, LLaVA-13B (LLaMA-2) adopts ViT-L/14 (Dosovitskiy et al., 2020) from CLIP (Radford et al., 2021) as a vision model, which is trained by a large amount of data, 400 million image-text pairs; 2) Second, it is surprising that stronger language model can largely boost the recognition performance. LLaVA-13B (LLaMA-2) obtains 8.3\% important over LLaVA-13B (Vicuna-13B). Stronger LLMs may help understand questions better and identify key information from visual inputs.</p>
<p>LLaMA-Adapter v2-7B is another strong model in recognition, achieving $38.5 \%$. This outstanding ability may be obtained from its various and large amounts of tuning data, LAION-400M (Schuhmann et al., 2021), COYO-700M (Byeon et al., 2022), Multimodal C4 (Zhu et al., 2023b) and tuning data of LLaVA (Liu et al., 2023c) etc as shown in Table 11. Besides, InstructBLIP-8B (Dai et al., 2023) attains $32.4 \%$. As shown in Table 11, the tuning data of InstructBLIP includes 26 publicly available datasets, which contain recognition heavily datasets, like VQA v2 (Goyal et al., 2017) and GQA (Hudson \&amp; Manning, 2019). The promising capability of InstructBLIP in recognition may benefit from these datasets.</p>
<p>Table 3: MM-Vet evaluation results on various LMMs regarding each capability integration. Examples of each capability integration are shown in Appendix Tables 12-17. For each column, the highest, second, and third highest figures are highlighted by green, orange and blue colors. Numbers are presented in \% with a full score of $100 \%$.</p>
<p>| Model | Rec <br> Know <br> Gen | Rec | OCR <br> OCR <br> Spat |  |  | Rec <br> Spat |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |</p>
<p>Table 4: MM-Vet evaluation of LLaVA, MM-ReAct and GPT-4V regarding each capability integration. For each column, the highest and second highest figures are highlighted by green and orange colors. Numbers are presented in $\%$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Rec</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">OCR</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Rec</th>
<th style="text-align: center;">Rec</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Know</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">OCR</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">OCR</td>
<td style="text-align: center;">OCR</td>
<td style="text-align: center;">Rec</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">OCR</td>
<td style="text-align: center;">Rec</td>
<td style="text-align: center;">OCR</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Gen</td>
<td style="text-align: center;">Rec</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Spat</td>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">Spat</td>
<td style="text-align: center;">OCR</td>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">Know</td>
<td style="text-align: center;">Gen</td>
<td style="text-align: center;">Gen</td>
<td style="text-align: center;">OCR</td>
<td style="text-align: center;">Rec</td>
<td style="text-align: center;">Know</td>
<td style="text-align: center;">Know</td>
<td style="text-align: center;">Gen</td>
<td style="text-align: center;">Spat</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaVA-13B (LLaMA-2) (Liu et al., 2023c)</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">58.3</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">56.8</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">$\pm 0.1$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">MM-ReAct-GPT-4 (Yang et al., 2023c)</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">44.8</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">88.0</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">$\pm 0.2$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4V (OpenAI, 2023a)</td>
<td style="text-align: center;">55.5</td>
<td style="text-align: center;">89.2</td>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">78.2</td>
<td style="text-align: center;">76.5</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">89.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">$\pm 0.3$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4V-Turbo-detail:low (OpenAI, 2023a)</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">85.8</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">$\pm 0.3$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4V-Turbo-detail:high (OpenAI, 2023a)</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">85.8</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">$\pm 0.1$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 5: MM-Vet evaluation of LLaVA, MM-ReAct and GPT-4V regarding each core VL capability. For each column, the highest and second figures are highlighted by green and orange colors.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">Rec</th>
<th style="text-align: right;">OCR</th>
<th style="text-align: right;">Know</th>
<th style="text-align: right;">Gen</th>
<th style="text-align: right;">Spat</th>
<th style="text-align: right;">Math</th>
<th style="text-align: right;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LLaVA-13B (LLaMA-2)</td>
<td style="text-align: right;">39.2</td>
<td style="text-align: right;">22.7</td>
<td style="text-align: right;">26.5</td>
<td style="text-align: right;">29.3</td>
<td style="text-align: right;">29.6</td>
<td style="text-align: right;">7.7</td>
<td style="text-align: right;">32.9</td>
</tr>
<tr>
<td style="text-align: left;">MM-ReAct-GPT-4</td>
<td style="text-align: right;">33.1</td>
<td style="text-align: right;">65.7</td>
<td style="text-align: right;">29.0</td>
<td style="text-align: right;">35.0</td>
<td style="text-align: right;">56.8</td>
<td style="text-align: right;">69.2</td>
<td style="text-align: right;">44.6</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4V</td>
<td style="text-align: right;">67.5</td>
<td style="text-align: right;">68.3</td>
<td style="text-align: right;">56.2</td>
<td style="text-align: right;">60.7</td>
<td style="text-align: right;">69.4</td>
<td style="text-align: right;">58.6</td>
<td style="text-align: right;">67.7</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4V-Turbo-detail:low</td>
<td style="text-align: right;">61.3</td>
<td style="text-align: right;">59.2</td>
<td style="text-align: right;">54.8</td>
<td style="text-align: right;">60.2</td>
<td style="text-align: right;">58.4</td>
<td style="text-align: right;">46.2</td>
<td style="text-align: right;">60.2</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4V-Turbo-detail:high</td>
<td style="text-align: right;">62.9</td>
<td style="text-align: right;">75.9</td>
<td style="text-align: right;">53.7</td>
<td style="text-align: right;">57.3</td>
<td style="text-align: right;">76.8</td>
<td style="text-align: right;">69.5</td>
<td style="text-align: right;">67.6</td>
</tr>
</tbody>
</table>
<p>performance.
Recognition (sole). This category contains samples that only require recognition, as shown in Appendix Table 12(b). InstructBLIP-14B and InstructBLIP-8B (Dai et al., 2023) achieve the best performance, which may result from the tuning data containing relevant datasets, like VQA (Goyal et al., 2017) and GQA (Hudson \&amp; Manning, 2019).</p>
<p>OCR and spatial awareness. For this integration, an example is shown in Appendix Table 12(c). MM-ReAct-GPT-4 (Yang et al., 2023c) is the best method for this integration. Notably, MM-ReAct-GPT-4 has a significant improvement of over $40 \%$ than MM-ReAct-GPT-3.5, indicating the importance of LLMs in integrating the OCR and location information.</p>
<p>OCR, spatial awareness, and math. An example of this integration is shown in Appendix Table 13(a), which requires reading the floor plan and conducting arithmetic. Compared with the above integration, this combination involves one more capability of math. The observation is similar to the integration of OCR and spatial awareness. MM-ReAct-GPT-4 (Yang et al., 2023c) still achieves the best performance.</p>
<p>Recognition and spatial awareness. Appendix Table 13(b) shows an example for this integration. LLaVA-13B (V1.3, 336px) (Liu et al., 2023c) performs best for this category. Compared with LLaVA-13B (LLaMA-2), LLaVA13B (V1.3, 336px) obtains an improvement of $8.4 \%$, indicating the significant contribution of larger resolution of images.</p>
<p>OCR (sole). This task requires OCR only, as shown in Appendix Table 13(c). MM-ReAct-GPT-4 (Yang et al., 2023c) has the best results for sole OCR due to an OCR
tool from Azure API. Notably, MM-ReAct-GPT-4 is much better than MM-ReAct-GPT-3.5 with an improvement of $23.0 \%$, demonstrating the importance of language models in OCR.</p>
<p>OCR and math. This integration enables reading text from real-world scenarios and solving math problems, as shown in Appendix Table 13(d). MM-ReAct-GPT-4 (Yang et al., 2023c) obtains the best performance in this capability integration, thanks to the speclized OCR and math tools used.</p>
<p>Other capability integrations. 9 other capability integrations are in long-tailed distribution, where MMReAct-GPT4 achieves the best scores in 5 integrations out of 9 . Their examples are shown in Appendix Tables 14-17.</p>
<h3>4.3. Result discussion</h3>
<h3>4.3.1. FOUNDATION MODELS AND TUNING DATA</h3>
<p>In this subsection, we discuss LMM modules and speculate how each component may affect the LMMs' capabilities in different aspects, evaluated by MM-Vet. We mainly consider the models based on open-sourced LLMs, i.e., FlanT5 (Chung et al., 2022), LLaMA (Touvron et al., 2023a), Vicuna (Zheng et al., 2023), and LLaMA-2 (Touvron et al., 2023b).</p>
<p>Vision. For the vision component, two models are popular in our evaluated end-to-end LMMs, i.e., CLIP-ViT/L14 (Radford et al., 2021) (428M) and EVA-ViT-G (1.13B). Determining a superior model is currently not possible due to the absence of a comprehensive ablation study (Zeng et al., 2023). However, it's noteworthy that, when paired with the same language model, Vicuna-7B, InstructBLIP8B excels in recognition tasks, while LLaVA-7B works particularly well for OCR.</p>
<p>Language. There is a notable trend indicating that superior language models (LLMs) typically yield better performance, such as comparing the 7B and 13B variants of different models, except for the outlier of InstructBLIP where the 8B version performs better than the 14B one.</p>
<p>Tuning data. Increasing the volume of data can enhance performance. An example is InstructBLIP-8B (Dai et al.,</p>
<p>Table 6: Averaged absolute differences $(\bar{\Delta})$ between the evaluation scores of various LLM evaluators and those of humanannotated scores, on MM-ReAct-GPT4's results. A smaller discrepancy indicates a better agreement with the gold standard of human evaluation, indicating a better evaluator.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Keyword</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">LLM-based evaluation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">matching</td>
<td style="text-align: center;">LLaMA-2-7B</td>
<td style="text-align: center;">LLaMA-2-13B</td>
<td style="text-align: center;">LLaMA-2-70B</td>
<td style="text-align: center;">Mistral-7B-v0.1</td>
<td style="text-align: center;">Mistral-7B-Instruct-v0.2</td>
<td style="text-align: center;">Mistral-8s7B-Instruct-v0.1</td>
<td style="text-align: center;">Gemini Pro</td>
<td style="text-align: center;">Claude 3 Opus</td>
<td style="text-align: center;">GPT-3.5 (taybo-0613)</td>
<td style="text-align: center;">GPT-4 (0613)</td>
</tr>
<tr>
<td style="text-align: center;">$\bar{\Delta}(\downarrow)$</td>
<td style="text-align: center;">0.273</td>
<td style="text-align: center;">0.307</td>
<td style="text-align: center;">0.254</td>
<td style="text-align: center;">0.316</td>
<td style="text-align: center;">0.188</td>
<td style="text-align: center;">0.173</td>
<td style="text-align: center;">0.234</td>
<td style="text-align: center;">0.144</td>
<td style="text-align: center;">0.137</td>
<td style="text-align: center;">0.178</td>
<td style="text-align: center;">0.042</td>
</tr>
</tbody>
</table>
<p>2023), which utilizes more data from 26 publicly available datasets to tune the model and achieve higher scores than BLIP-2-12B.</p>
<h3>4.3.2. COMPARISON WITH GPT-4V(ISION)</h3>
<p>We evaluate and benchmark the state-of-the-art LMM, GPT$4 \mathrm{~V}($ ison) (OpenAI, 2023c;a;b; gpt, 2023; Yang et al., 2023b) on MM-Vet. In our queries to GPT-4V, we prepend the prompt with "Generate a short and concise response to the following image text pair." The quantitative results are shown in Tables 4 and 5, and the qualitative results are expressed in Appendix Figures 3-6. Remarkably, GPT-4V achieves a score of $67.7 \%$, surpassing both open-sourced LMMs (Liu et al., 2023c) and LLM-based multimodal agents (Yang et al., 2023c) by substantial margins.</p>
<p>We aspire that the detailed per-category performance breakdown sheds light on potential avenues for enhancing model capabilities, thereby bridging the existing performance gap. To illustrate, integrating specialized tools within agent systems proves advantageous for specific functionalities like OCR and math. While other categories, such as recognition and language generation, would require enhancements in the core vision and language modules, respectively. Appendix Figures 3-6 offer an exhaustive analysis, highlighting representative success and failure instances of GPT-4V's performance.</p>
<p>Table 7: Averaged absolute differences $(\bar{\Delta})$ between the evaluation scores of combined LLM evaluators and those of human-annotated scores on MM-ReAct-GPT4's results.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Combined LLMs</th>
<th style="text-align: center;">$\bar{\Delta}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4 solely</td>
<td style="text-align: center;">$\mathbf{0 . 0 4 2 3}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 + Llama-2-13b-chat</td>
<td style="text-align: center;">0.1483</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 + Mistral-7B-Instruct-v0.2</td>
<td style="text-align: center;">0.1078</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 + Gemini Pro</td>
<td style="text-align: center;">0.0933</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 + Claude 3 Opus</td>
<td style="text-align: center;">0.0896</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 + Llama-2-13b-chat +</td>
<td style="text-align: center;">0.1502</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-7B-Instruct-v0.2 + Gemini Pro + Claude 3 Opus</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<h3>4.4. Effectiveness analysis of LLM-based evaluation</h3>
<p>To verify the effectiveness of LLM-based evaluation for LMM predictions, we select the outputs from MMReAct-GPT-4 on 138 objective questions, which can be objectively annotated by humans. We compute the absolute value of the difference between the evaluator's output score and the human-annotated score on each sample. In addition to the default evaluator of GPT-4 (0613) in MM-Vet, we experiment with other LLMs. LLaMa-2 and Mixtral represent open-source LLMs, while GPT-4, Gemini and Claude are
commercial close-sourced LLMs.
The average difference to the human scoring is reported in Table 6, represented as $\bar{\Delta}$. With a maximum potential discrepancy of 1.0 , the baseline evaluation method, keyword matching, results in a high difference of 0.273 . This illustrates the unsuitability of keyword matching for MM-Vet when dealing with open-ended answers. Among the LLaMA-2/Mistral series, LLaMA-2-13B/Mistral-7B-Instruct-v0.2 performs best, respectively. We notice that the performance of the largest LLaMA-2/Mistral is not satisfactory, which may be because their larger models are more creative and do not follow our few-shot prompt strictly. The three commercial LLMs perform better than open-sourced LLMs, while there is still a large gap between Gemini/ Claude and GPT-4.</p>
<p>We also explore whether $\bar{\Delta}$ can be reduced when combining GPT-4 with other LLMs, and the results are reported in Table 7. We find that GPT-4 without extra LLMs performs the best. This may be because other LLMs cannot match the grading accuracy of GPT-4. However, we believe this idea will work in the future when other LLMs become stronger. Therefore, MM-Vet uses GPT-4 (0613) to evaluate the LMM outputs.</p>
<h3>4.5. Effectiveness of few-shot examples for LLM prompt</h3>
<p>In this section, we explore the effectiveness of few-shot examples used in the prompt of LLM-based evaluator. We denote the seven grading examples in Table 1 as (1) - (7) in order for ablation study. As Table 8 shows, using all seven examples together achieves the closest alignment with human evaluations (lowest $\bar{\Delta}$ ).</p>
<h3>4.6. Takeaway notes</h3>
<p>We summarize the analyses and discussions as follows:</p>
<ul>
<li>In the evaluation of integrated capabilities on MM-Vet (Sections 4.2 and 4.3.2), GPT-4V (OpenAI, 2023a) outperforms existing open-sourced methods. The tool-using approach, MM-ReAct-GPT-4 (Yang et al., 2023c), achieves the second-best performance with effective external tools. The pros and cons in different categories motivate future studies on tool-enhanced LMMs. Among end-to-end LMMs, LLaVA-13B (LLaMA-2)/LLaVA-13B (V1.3, 336px) (Liu et al., 2023c) demonstrates the best performance on MMVet.</li>
</ul>
<p>Table 8: Ablation study of few-shot examples in prompt for LLM-based evaluator</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Few-shot examples</th>
<th style="text-align: center;">Remarks</th>
<th style="text-align: center;">$\Delta$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">None</td>
<td style="text-align: center;">No grading examples</td>
<td style="text-align: center;">0.0630</td>
</tr>
<tr>
<td style="text-align: left;">(1) (5)</td>
<td style="text-align: center;">Add two examples with totally right/wrong short answer</td>
<td style="text-align: center;">0.0625</td>
</tr>
<tr>
<td style="text-align: left;">(1) (2) (3) (5)</td>
<td style="text-align: center;">Add more two examples with partially right short answer</td>
<td style="text-align: center;">0.0619</td>
</tr>
<tr>
<td style="text-align: left;">(1) (2) (3) (4) (5)</td>
<td style="text-align: center;">Add more one example with partially right and partially wrong answer</td>
<td style="text-align: center;">0.0551</td>
</tr>
<tr>
<td style="text-align: left;">(1) (2) (3) (4) (5) (7)</td>
<td style="text-align: center;">Add more one example with fully right long answer</td>
<td style="text-align: center;">0.0520</td>
</tr>
<tr>
<td style="text-align: left;">(1) (2) (3) (4) (5) (6) (7)</td>
<td style="text-align: center;">Add more one example with partially right long answer</td>
<td style="text-align: center;">$\mathbf{0 . 0 4 2 3}$</td>
</tr>
</tbody>
</table>
<ul>
<li>Analysis of open-source LMMs (Section 4.3.1) presents some uncertainty about which vision encoders are optimal for LMMs, based on current model comparisons. However, it is evident that stronger LLMs can boost the performance of LMMs.</li>
<li>For open-ended evaluation (Section 4.4), it is effective to use GPT-4 for evaluating the open-ended outputs of LMMs. The use of less powerful LLMs could result in more significant deviations from the gold standard of human evaluation results.</li>
<li>Current top-performing methods, such as GPT-4V (OpenAI, 2023a), only achieve scores of around $68 \%$ on MM-Vet, where full score is $100 \%$. The gap signifies that further effort is necessary to enhance the performance of LMMs in terms of integrated capabilities, e.g., by developing stronger LMMs, finding better prompting techniques (Yang et al., 2023b;a), and extending LMMs with external tools.</li>
</ul>
<h2>5. Conclusion and Limitation</h2>
<p>In this paper, we have presented MM-Vet, a new benchmark designed to evaluate LMMs in terms of their integrated VL capabilities. We have constructed a new multimodal dataset that requires the integration of multiple VL capabilities to solve. To facilitate open-ended evaluation, we adopt an LLM-based evaluator to grade open-ended outputs from LMMs. We then evaluate various LMMs on MM-Vet, analyzing their results to provide insights into different LMM system paradigms and model designs. The evaluation reveals that even advanced models like GPT-4V only score around $68 \%$ on MM-Vet, highlighting the ongoing need to enhance the integrated VL capabilities of LMMs.</p>
<p>For the limitations of this work, firstly, since most popular LMMs only accept image-text input and output text, MM-Vet focuses on evaluating this type of modalities, not covering other modalities. Secondly, we propose to utilize LLMs to automatically grade LMM output results. However, as Table 6 shows, currently only GPT-4 can well match the human grades, so we set the evaluator's LLM as GPT-4, which will bring GPT-4 usage fees. To help researchers from non-profit institutes save GPT-4 fees, we host an MM-Vet online evaluator ${ }^{2}$ with our GPT-4 API key.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Acknowledgements</h2>
<p>This project is supported by the Ministry of Education, Singapore, under its Academic Research Fund Tier 2 (Award Number: MOE-T2EP20122-0006). Weihao was partly supported by Snap Research Fellowship, Google TPU Research Cloud (TRC), and Google Cloud Research Credits program.</p>
<h2>Impact Statement</h2>
<p>This paper presents a new benchmark for evaluating Large Multimodal Models (LMMs). The significance of this contribution extends in several directions. First, the newly developed MM-Vet benchmark assesses the capabilities of existing LMMs, establishing a solid groundwork for future advancements in this domain. The growing interest and demand for advanced LMM infrastructures underscore the timeliness and relevance of our work. Moreover, MM-Vet is pioneering in its approach to evaluating LMMs from integrated capabilities and leveraging LLMs for based openended scoring. These innovative evaluation strategies are poised to profoundly influence future benchmark development. By doing so, MM-Vet aims to not just enhance the field of LMMs but also to play a pivotal role in driving societal changes through the application of advanced foundational models. In the development of MM-Vet, ethical considerations have been paramount. We have rigorously verified the benchmark sample and evaluation criteria to ensure they align with high ethical standards. This diligence is vital in ensuring that the advancements in LMMs are responsible and beneficial for society at large.</p>
<h2>References</h2>
<p>Mpt. https://github.com/mosaicml/ llm-foundry#mpt, 2023.</p>
<p>Chatgpt can now see, hear, and speak. https://openai.com/blog/ chatgpt-can-now-see-hear-and-speak, 2023.</p>
<p>Agrawal, H., Desai, K., Wang, Y., Chen, X., Jain, R., Johnson, M., Batra, D., Parikh, D., Lee, S., and Anderson, P. Nocaps: Novel object captioning at scale. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 8948-8957, 2019.</p>
<p>Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I.,</p>
<p>Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716-23736, 2022.</p>
<p>Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.</p>
<p>Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C. L., and Parikh, D. VQA: Visual Question Answering. In ICCV, 2015.</p>
<p>Ao, J., Wang, R., Zhou, L., Wang, C., Ren, S., Wu, Y., Liu, S., Ko, T., Li, Q., Zhang, Y., et al. Speecht5: Unifiedmodal encoder-decoder pre-training for spoken language processing. arXiv preprint arXiv:2110.07205, 2021.</p>
<p>Awadalla, A., Gao, I., Gardner, J., Hessel, J., Hanafy, Y., Zhu, W., Marathe, K., Bitton, Y., Gadre, S., Jitsev, J., Kornblith, S., Koh, P. W., Ilharco, G., Wortsman, M., and Schmidt, L. Openflamingo, March 2023a. URL https: //doi.org/10.5281/zenodo.7733589.</p>
<p>Awadalla, A., Gao, I., Gardner, J., Hessel, J., Hanafy, Y., Zhu, W., Marathe, K., Bitton, Y., Gadre, S., Sagawa, S., Jitsev, J., Kornblith, S., Koh, P. W., Ilharco, G., Wortsman, M., and Schmidt, L. Openflamingo: An opensource framework for training large autoregressive visionlanguage models. arXiv preprint arXiv:2308.01390, 2023b.</p>
<p>Azure, M. Azure cognitive services apis. https: //azure.microsoft.com/en-us/products/ ai-services/ai-vision, 2023. Accessed: 2023-06-20.</p>
<p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877-1901, 2020a.</p>
<p>Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. In NeurIPS, 2020b.</p>
<p>Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.</p>
<p>Byeon, M., Park, B., Kim, H., Lee, S., Baek, W., and Kim, S. Coyo-700m: Image-text pair dataset. https://github.com/kakaobrain/ coyo-dataset, 2022.</p>
<p>Changpinyo, S., Sharma, P., Ding, N., and Soricut, R. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3558-3568, 2021.</p>
<p>Chen, T., Saxena, S., Li, L., Fleet, D. J., and Hinton, G. Pix2seq: A language modeling framework for object detection. In ICLR, 2022.</p>
<p>Chen, X., Fang, H., Lin, T.-Y., Vedantam, R., Gupta, S., Dollár, P., and Zitnick, C. L. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015.</p>
<p>Chen, Y.-C., Li, L., Yu, L., Kholy, A. E., Ahmed, F., Gan, Z., Cheng, Y., and Liu, J. Uniter: Learning universal image-text representations. In ECCV, 2020.</p>
<p>Chiang, C.-H. and Lee, H.-y. Can large language models be an alternative to human evaluations? arXiv preprint arXiv:2305.01937, 2023.</p>
<p>Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.</p>
<p>Costa-jussà, M. R., Cross, J., Çelebi, O., Elbayad, M., Heafield, K., Heffernan, K., Kalbassi, E., Lam, J., Licht, D., Maillard, J., et al. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672, 2022.</p>
<p>Dai, W., Li, J., Li, D., Tiong, A. M. H., Zhao, J., Wang, W., Li, B., Fung, P., and Hoi, S. Instructblip: Towards general-purpose vision-language models with instruction tuning. arXiv preprint arXiv:2305.06500, 2023.</p>
<p>Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.</p>
<p>Driess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, W., Chebotar, Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint, M., Greff, K., Zeng, A., Mordatch, I., and Florence, P. Palm-e: An embodied multimodal language model. In arXiv preprint arXiv:2303.03378, 2023.</p>
<p>Fang, Y., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X., Huang, T., Wang, X., and Cao, Y. Eva: Exploring the limits of masked visual representation learning at scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19358-19369, 2023.</p>
<p>Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Qiu, Z., Lin, W., Yang, J., Zheng, X., et al. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023a.</p>
<p>Fu, J., Ng, S.-K., Jiang, Z., and Liu, P. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166, 2023b.</p>
<p>Gan, Z., Li, L., Li, C., Wang, L., Liu, Z., and Gao, J. Visionlanguage pre-training: Basics, recent advances, and future trends. arXiv preprint arXiv:2210.09263, 2022.</p>
<p>Gao, D., Ji, L., Zhou, L., Lin, K. Q., Chen, J., Fan, Z., and Shou, M. Z. Assistgpt: A general multi-modal assistant that can plan, execute, inspect, and learn. arXiv preprint arXiv:2306.08640, 2023a.</p>
<p>Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., and Neubig, G. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435, 2022.</p>
<p>Gao, P., Han, J., Zhang, R., Lin, Z., Geng, S., Zhou, A., Zhang, W., Lu, P., He, C., Yue, X., et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010, 2023b.</p>
<p>Gong, T., Lyu, C., Zhang, S., Wang, Y., Zheng, M., Zhao, Q., Liu, K., Zhang, W., Luo, P., and Chen, K. Multimodal-gpt: A vision and language model for dialogue with humans, 2023.</p>
<p>Google. Bard. https://bard.google.com, 2023. Accessed: 2023-07-17.</p>
<p>Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6904-6913, 2017.</p>
<p>Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.</p>
<p>Hudson, D. A. and Manning, C. D. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In CVPR, 2019.</p>
<p>Huggingface. Transformers agent. https: //huggingface.co/docs/transformers/
transformers_agents, 2023. Accessed: 2023-0720.</p>
<p>Kim, G., Hong, T., Yim, M., Nam, J., Park, J., Yim, J., Hwang, W., Yun, S., Han, D., and Park, S. Ocr-free document understanding transformer. In European Conference on Computer Vision, pp. 498-517. Springer, 2022.</p>
<p>Kim, W., Son, B., and Kim, I. Vilt: Vision-and-language transformer without convolution or region supervision. In ICML, 2021.</p>
<p>Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.-J., Shamma, D. A., et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV, 2017.</p>
<p>Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019.</p>
<p>Li, B., Wang, R., Wang, G., Ge, Y., Ge, Y., and Shan, Y. Seed-bench: Benchmarking multimodal llms with generative comprehension, 2023a.</p>
<p>Li, B., Zhang, Y., Chen, L., Wang, J., Pu, F., Yang, J., Li, C., and Liu, Z. Mimic-it: Multi-modal in-context instruction tuning. arXiv preprint arXiv:2306.05425, 2023b.</p>
<p>Li, B., Zhang, Y., Chen, L., Wang, J., Yang, J., and Liu, Z. Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023c.</p>
<p>Li, J., Li, D., Xiong, C., and Hoi, S. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, pp. 12888-12900. PMLR, 2022.</p>
<p>Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023d.</p>
<p>Li, X., Yin, X., Li, C., Hu, X., Zhang, P., Zhang, L., Wang, L., Hu, H., Dong, L., Wei, F., et al. Oscar: Objectsemantics aligned pre-training for vision-language tasks. In ECCV, 2020.</p>
<p>Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In ECCV, 2014.</p>
<p>Liu, F., Lin, K., Li, L., Wang, J., Yacoob, Y., and Wang, L. Aligning large multi-modal model with robust instruction tuning. arXiv preprint arXiv:2306.14565, 2023a.</p>
<p>Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023b.</p>
<p>Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023c.</p>
<p>Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J., He, C., Liu, Z., et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023d.</p>
<p>Liu, Y., Iter, D., Xu, Y., Wang, S., Xu, R., and Zhu, C. Gpteval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023e.</p>
<p>Liu, Y., Li, Z., Li, H., Yu, W., Huang, M., Peng, D., Liu, M., Chen, M., Li, C., Jin, L., et al. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023f.</p>
<p>Lu, J., Batra, D., Parikh, D., and Lee, S. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In NeurIPS, 2019.</p>
<p>Lüddecke, T. and Ecker, A. Image segmentation using text and image prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7086-7096, 2022.</p>
<p>Marino, K., Rastegari, M., Farhadi, A., and Mottaghi, R. Okvqa: A visual question answering benchmark requiring external knowledge. In CVPR, pp. 3195-3204, 2019.</p>
<p>OpenAI. Gpt-4v(ision) system card. 2023a. URL https://cdn.openai.com/papers/GPTV_ System_Card.pdf.</p>
<p>OpenAI. Gpt-4v(ision) technical work and authors. 2023b. URL https://cdn.openai.com/ contributions/gpt-4v.pdf.</p>
<p>OpenAI. Gpt-4 technical report, 2023c.
Ordonez, V., Kulkarni, G., and Berg, T. L. Im2text: Describing images using 1 million captioned photographs. In NeurIPS, 2011.</p>
<p>Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.</p>
<p>Peng, B., Li, C., He, P., Galley, M., and Gao, J. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.</p>
<p>Politzer, T. Vision is our dominant sense. https://www.brainline.org/article/ vision-our-dominant-sense. Accessed: 2023-05-20.</p>
<p>Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.</p>
<p>Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., and Sutskever, I. Robust speech recognition via largescale weak supervision. In International Conference on Machine Learning, pp. 28492-28518. PMLR, 2023.</p>
<p>Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, R., Mullis, C., Katta, A., Coombes, T., Jitsev, J., and Komatsuzaki, A. Laion-400m: Open dataset of clipfiltered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.</p>
<p>Sharma, P., Ding, N., Goodman, S., and Soricut, R. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL, 2018.</p>
<p>Shen, Y., Song, K., Tan, X., Li, D., Lu, W., and Zhuang, Y. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023.</p>
<p>Sidorov, O., Hu, R., Rohrbach, M., and Singh, A. Textcaps: a dataset for image captioning with reading comprehension. In ECCV, pp. 742-758, 2020.</p>
<p>Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D., and Rohrbach, M. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8317-8326, 2019.</p>
<p>Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.</p>
<p>Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023b.</p>
<p>Tsimpoukelli, M., Menick, J., Cabi, S., Eslami, S., Vinyals, O., and Hill, F. Multimodal few-shot learning with frozen language models. arXiv preprint arXiv:2106.13884, 2021.</p>
<p>Wang, J., Yang, Z., Hu, X., Li, L., Lin, K., Gan, Z., Liu, Z., Liu, C., and Wang, L. Git: A generative image-totext transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022a.</p>
<p>Wang, W., Chen, Z., Chen, X., Wu, J., Zhu, X., Zeng, G., Luo, P., Lu, T., Zhou, J., Qiao, Y., et al. Visionllm: Large language model is also an open-ended decoder for visioncentric tasks. arXiv preprint arXiv:2305.11175, 2023.</p>
<p>Wang, X., Peng, Y., Lu, L., Lu, Z., Bagheri, M., and Summers, R. M. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2097-2106, 2017.</p>
<p>Wang, Z., Yu, J., Yu, A. W., Dai, Z., Tsvetkov, Y., and Cao, Y. Simvlm: Simple visual language model pretraining with weak supervision. In $I C L R, 2022 b$.</p>
<p>Xu, P., Shao, W., Zhang, K., Gao, P., Liu, S., Lei, M., Meng, F., Huang, S., Qiao, Y., and Luo, P. Lvlm-ehub: A comprehensive evaluation benchmark for large visionlanguage models. arXiv preprint arXiv:2306.09265, 2023.</p>
<p>Yang, J., Zhang, H., Li, F., Zou, X., Li, C., and Gao, J. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023a.</p>
<p>Yang, Z., Lu, Y., Wang, J., Yin, X., Florencio, D., Wang, L., Zhang, C., Zhang, L., and Luo, J. Tap: Text-aware pre-training for text-vqa and text-caption. In CVPR, pp. 8751-8761, 2021.</p>
<p>Yang, Z., Gan, Z., Wang, J., Hu, X., Ahmed, F., Liu, Z., Lu, Y., and Wang, L. Unitab: Unifying text and box outputs for grounded vision-language modeling. In European Conference on Computer Vision, pp. 521-539. Springer, 2022a.</p>
<p>Yang, Z., Gan, Z., Wang, J., Hu, X., Lu, Y., Liu, Z., and Wang, L. An empirical study of gpt-3 for few-shot knowledge-based vqa. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 3081-3089, 2022b.</p>
<p>Yang, Z., Li, L., Lin, K., Wang, J., Lin, C.-C., Liu, Z., and Wang, L. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 2023b.</p>
<p>Yang, Z., Li, L., Wang, J., Lin, K., Azarnasab, E., Ahmed, F., Liu, Z., Liu, C., Zeng, M., and Wang, L. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023c.</p>
<p>Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P., Shi, Y., et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023.</p>
<p>Zellers, R., Bisk, Y., Farhadi, A., and Choi, Y. From recognition to cognition: Visual commonsense reasoning. In CVPR, pp. 6720-6731, 2019.</p>
<p>Zeng, A., Wong, A., Welker, S., Choromanski, K., Tombari, F., Purohit, A., Ryoo, M., Sindhwani, V., Lee, J., Vanhoucke, V., et al. Socratic models: Composing zeroshot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022.</p>
<p>Zeng, Y., Zhang, H., Zheng, J., Xia, J., Wei, G., Wei, Y., Zhang, Y., and Kong, T. What matters in training a gpt4style language model with multimodal inputs? arXiv preprint arXiv:2307.02469, 2023.</p>
<p>Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.</p>
<p>Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang, H., Gonzalez, J. E., and Stoica, I. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.</p>
<p>Zhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023a.</p>
<p>Zhu, W., Hessel, J., Awadalla, A., Gadre, S. Y., Dodge, J., Fang, A., Yu, Y., Schmidt, L., Wang, W. Y., and Choi, Y. Multimodal c4: An open, billion-scale corpus of images interleaved with text. arXiv preprint arXiv:2304.06939, 2023b.</p>
<h1>A. Model details</h1>
<p>The details of the models we evaluated are shown in the Table 11.</p>
<h2>B. Comparison with Bard</h2>
<p>Bard (Google, 2023) is another representative closed-source commercial LMM system. One problem in evaluation is that Bard refuses to process images containing people faces. To conduct a fair comparison, we constructed a subset of MM-Vet with 168 samples that Bard could process, henceforth referred to as the Bard set. The results on the Bard set are shown in Tables 10 and 9. Bard achieves the highest scores in three out of six capabilities, seven out of fifteen capability integrations, and holds the highest overall score (53.5\%). MM-ReAct-GPT-4 (Yang et al., 2023c) outperforms in the remaining three out of six capabilities, and tops the chart in nine out of the fifteen capability integrations. Particularly, MM-ReAct performs better in OCR, spatial awareness, and math capabilities, indicating the potential benefit of having specialized external tools, even when working with state-of-the-art LMMs. When considering open-sourced end-to-end models such as LLaVA, there is still a considerable gap.</p>
<p>Table 9: MM-Vet (Bard set) evaluation results on various LMMs regarding each core VL capability. For each column, the highest and second figures are highlighted by green and orange colors.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Rec</th>
<th style="text-align: center;">OCR</th>
<th style="text-align: center;">Know</th>
<th style="text-align: center;">Gen</th>
<th style="text-align: center;">Spat</th>
<th style="text-align: center;">Math</th>
<th style="text-align: center;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LLaVA-13B (LLaMA-2) (Liu et al., 2023c)</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">$30.3 \pm 0.1$</td>
</tr>
<tr>
<td style="text-align: left;">LLaVA-13B (V1.3, 336px) (Liu et al., 2023c)</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">$31.5 \pm 0.1$</td>
</tr>
<tr>
<td style="text-align: left;">MM-ReAct-GPT-3.5 (Yang et al., 2023c)</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">16.6</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">$27.6 \pm 0.2$</td>
</tr>
<tr>
<td style="text-align: left;">MM-ReAct-GPT-4 (Yang et al., 2023c)</td>
<td style="text-align: center;">34.3</td>
<td style="text-align: center;">66.3</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">60.6</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">$48.1 \pm 0.2$</td>
</tr>
<tr>
<td style="text-align: left;">Bard (Google, 2023)</td>
<td style="text-align: center;">56.2</td>
<td style="text-align: center;">52.5</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">$53.5 \pm 0.2$</td>
</tr>
</tbody>
</table>
<p>Table 10: MM-Vet (Bard set) evaluation results on various LMMs regarding each capability integration. For each column, the highest and second highest figures are highlighted by green and orange colors. Numbers are presented in \% with a full score of $100 \%$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Rec</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">OCR</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Rec</th>
<th style="text-align: center;">Rec</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Know</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">OCR</td>
<td style="text-align: center;">Spat</td>
<td style="text-align: center;">Rec</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">OCR</td>
<td style="text-align: center;">Rec</td>
<td style="text-align: center;">OCR</td>
<td style="text-align: center;">OCR</td>
<td style="text-align: center;">Rec</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">OCR</td>
<td style="text-align: center;">Rec</td>
<td style="text-align: center;">OCR</td>
<td style="text-align: center;">Rec</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Total</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Gen</td>
<td style="text-align: center;">Rec</td>
<td style="text-align: center;">Spat</td>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">Spat</td>
<td style="text-align: center;">OCR</td>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">Know</td>
<td style="text-align: center;">Gen</td>
<td style="text-align: center;">Spat</td>
<td style="text-align: center;">OCR</td>
<td style="text-align: center;">Rec</td>
<td style="text-align: center;">Know</td>
<td style="text-align: center;">Know</td>
<td style="text-align: center;">Gen</td>
<td style="text-align: center;">Gen</td>
<td style="text-align: center;">Spat</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-13B (LLaMA-2) (Liu et al., 2023c)</td>
<td style="text-align: center;">26.6</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">57.1</td>
<td style="text-align: center;">39.5</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">56.8</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$30.3 \pm 0.1$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-13B (V1.3, 336px) (Liu et al., 2023c)</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">8.2</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$31.5 \pm 0.1$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">MM-ReAct-GPT-3.5 (Yang et al., 2023c)</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$27.6 \pm 0.2$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">MM-ReAct-GPT-4 (Yang et al., 2023c)</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">35.2</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">88.0</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$48.1 \pm 0.2$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Bard (Google, 2023)</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">45.2</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$53.5 \pm 0.2$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<h2>C. Examples of capability integrations</h2>
<p>We show examples of capability integrations in Tables 12-17. More examples are shown on https://github.com/ yuweihao/MM-Vet/blob/main/README_more_samples.md.</p>
<table>
<thead>
<tr>
<th>Table 11: Summary of the evaluated LMMs in this report. We consider both the end-to-end tuned models (i.e., OpenFlamingo (Alayrac et al., 2022; Awadalla et al., 2023a; b), BLIP-2 (Li et al., 2023d), LLaVA (Liu et al., 2023c), MiniGPT-4 (Zhu et al., 2023a), LLaMA-Adapter v2 (Gao et al., 2023b), Otter (Li et al., 2023c), and InstructBLIP (Dai et al., 2023)), and the LLM-tool-using systems (i.e., MM-ReAct (Yang et al., 2023c) and Transformers Agent (Higgingface, 2023)).</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Method</th>
<th>Initial models</th>
<th></th>
<th></th>
<th>Tuning data</th>
<th>Total params</th>
</tr>
</thead>
<tbody>
<tr>
<td>OpenFlamingo-SB</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>(Alayrac et al., 2022; Awadalla et al., 2023a; b)</td>
<td>LLP-Vf-L/14 (Radford et al., 2021)</td>
<td>LLaVA-7B (Zheng et al., 2023)</td>
<td>Other</td>
<td>1. COCO (Lin et al., 2014); 2. Visual Genome (Krishna et al., 2017); 3. CCTM (Sharma et al., 2018); 4. CC12M (Changprjejo et al., 2021); 5. SRC (Ordonez et al., 2011); 6. 115M image from the LABIN-400M (Schuhmann et al., 2021).</td>
<td></td>
</tr>
<tr>
<td>(CapFili (Li et al., 2022) is used to create synthetic</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>captions for the web images).</td>
<td>9B</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>BLIP-2-12B (Li et al., 2023d)</td>
<td>EVA-Vf-G (Fang et al., 2023)</td>
<td>FLi-NA (Radford et al., 2021)</td>
<td>-</td>
<td>1. CCTM (Sharma et al., 2018)</td>
<td></td>
</tr>
<tr>
<td>2. CCTM (Sharma et al., 2018)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>3. LLaVA-7B (Zheng et al., 2023)</td>
<td>7B</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>12B</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LLaVA-7B (Lin et al., 2023c)</td>
<td>LLP-Vf-L/14 (Radford et al., 2021)</td>
<td>CLIP-Vf-L/14 (Radford et al., 2021)</td>
<td>-</td>
<td>1. LLaVA-7B (Zheng et al., 2023)</td>
<td>7B</td>
</tr>
<tr>
<td>13B</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LLaVA-10B (Lia et al., 2023a)</td>
<td>LLP-Vf-L/14 (Radford et al., 2021)</td>
<td>CLIP-Vf-L/14 (Radford et al., 2021)</td>
<td>LLaMA-2-7B-Chat (Teavorn et al., 2023b)</td>
<td>1. LLAVA-10B (Lia et al., 2023a)</td>
<td>7B</td>
</tr>
<tr>
<td>13B</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LLaVA-13B (LLaMA-2) (Lin et al., 2023c)</td>
<td>LLP-Vf-L/14 (Radford et al., 2021)</td>
<td>LLaMA-2-7B-Chat (Teavorn et al., 2023b)</td>
<td>LLaMA-2-13B-Chat (Teavorn et al., 2023b)</td>
<td>1. LLAVA-13B (Lia et al., 2023a)</td>
<td>7B</td>
</tr>
<tr>
<td>13B</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LLaVA-13B (VL-3, 356px) (Lin et al., 2023c)</td>
<td>LLP-Vf-L/14 (Radford et al., 2021)</td>
<td>LLaMA-13B (Zheng et al., 2023)</td>
<td>-</td>
<td>1. LLAVA-13B (Lia et al., 2023a)</td>
<td>7B</td>
</tr>
<tr>
<td>13B</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>MiniGPT-4-6B (Zhu et al., 2023a)</td>
<td>EVA-Vf-G (Fang et al., 2023)</td>
<td>MiniGPT-4-6B (Zhu et al., 2023a)</td>
<td>-</td>
<td>1. LLAVA-6B (Zheng et al., 2023)</td>
<td>8B</td>
</tr>
<tr>
<td>14B</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>LLP-Vf-L/14 (Radford et al., 2021)</td>
<td>LLaMA-7B (Teavorn et al., 2023a)</td>
<td>-</td>
<td>1. LLAVA-7B (Zheng et al., 2023)</td>
<td>7B</td>
</tr>
<tr>
<td>Oter-9B (Li et al., 2023c)</td>
<td>LLP-Vf-L/14 (Radford et al., 2021)</td>
<td>LLaMA-9B (Teavorn et al., 2023a)</td>
<td>-</td>
<td>1. Oter-9B (Li et al., 2023c)</td>
<td>9B</td>
</tr>
<tr>
<td>InstructBLIP-8B (Dai et al., 2023)</td>
<td>EVA-Vf-G (Fang et al., 2023)</td>
<td>InstructBLIP-8B (Dai et al., 2023)</td>
<td>-</td>
<td>1. InstructBLIP-8B (Dai et al., 2023c)</td>
<td>8B</td>
</tr>
<tr>
<td>14B</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>InstructBLIP-14B (Dai et al., 2023)</td>
<td>EVA-Vf-G (Fang et al., 2023)</td>
<td>InstructBLIP-14B (Dai et al., 2023c)</td>
<td>-</td>
<td>1. InstructBLIP-14B (Dai et al., 2023c)</td>
<td>14B</td>
</tr>
<tr>
<td>Transformers Agent</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>(GPT-4 as agent) (Higgingface, 2023)</td>
<td>-</td>
<td>LLaMA-2-7B-Chat (Teavorn et al., 2023b)</td>
<td>LLaMA-2-7B-Chat (Teavorn et al., 2023b)</td>
<td>1. LLaMA-2-7B-Chat (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
<tr>
<td>MM-ReAct-GPT-3.5 (Yang et al., 2023c)</td>
<td>-</td>
<td>LLaMA-3.5 (Fang et al., 2023)</td>
<td>-</td>
<td>1. LLaMA-3.5 (Fang et al., 2023)</td>
<td>Not clear</td>
</tr>
<tr>
<td>MM-ReAct-GPT-4 (Yang et al., 2023c)</td>
<td>-</td>
<td>LLaMA-4.5 (Fang et al., 2023)</td>
<td>-</td>
<td>1. LLaMA-4.5 (Fang et al., 2023)</td>
<td>Not clear</td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>-</td>
<td>LLaMA-7B (Teavorn et al., 2023b)</td>
<td>-</td>
<td>1. LLaMA-7B (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>-</td>
<td>LLaMA-7B (Teavorn et al., 2023b)</td>
<td>-</td>
<td>1. LLaMA-7B (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>-</td>
<td>LLaMA-7B (Teavorn et al., 2023b)</td>
<td>-</td>
<td>1. LLaMA-7B (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>-</td>
<td>LLaMA-7B (Teavorn et al., 2023b)</td>
<td>-</td>
<td>1. LLaMA-7B (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>-</td>
<td>LLaMA-7B (Teavorn et al., 2023b)</td>
<td>-</td>
<td>1. LLaMA-7B (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>-</td>
<td>LLaMA-7B (Teavorn et al., 2023b)</td>
<td>-</td>
<td>1. LLaMA-7B (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>-</td>
<td>LLaMA-7B (Teavorn et al., 2023b)</td>
<td>-</td>
<td>1. LLaMA-7B (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>-</td>
<td>LLaMA-7B (Teavorn et al., 2023b)</td>
<td>-</td>
<td>1. LLaMA-7B (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>-</td>
<td>LLaMA-7B (Teavorn et al., 2023b)</td>
<td>-</td>
<td>1. LLaMA-7B (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>-</td>
<td>LLaMA-7B (Teavorn et al., 2023b)</td>
<td>-</td>
<td>1. LLaMA-7B (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>-</td>
<td>LLaMA-7B (Teavorn et al., 2023b)</td>
<td>-</td>
<td>1. LLaMA-7B (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>-</td>
<td>LLaMA-7B (Teavorn et al., 2023b)</td>
<td>-</td>
<td>1. LLaMA-7B (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>-</td>
<td>LLaMA-7B (Teavorn et al., 2023b)</td>
<td>-</td>
<td>1. LLaMA-7B (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>-</td>
<td>LLaMA-7B (Teavorn et al., 2023b)</td>
<td>-</td>
<td>1. LLaMA-7B (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>-</td>
<td>LLaMA-7B (Teavorn et al., 2023b)</td>
<td>-</td>
<td>1. LLaMA-7B (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>-</td>
<td>LLaMA-7B (Teavorn et al., 2023b)</td>
<td>-</td>
<td>1. LLaMA-7B (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>-</td>
<td>LLaMA-7B (Teavorn et al., 2023b)</td>
<td>-</td>
<td>1. LLaMA-7B (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>-</td>
<td>LLaMA-7B (Teavorn et al., 2023b)</td>
<td>-</td>
<td>1. LLaMA-7B (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>-</td>
<td>LLaMA-7B (Teavorn et al., 2023b)</td>
<td>-</td>
<td>1. LLaMA-7B (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>-</td>
<td>LLaMA-7B (Teavorn et al., 2023b)</td>
<td>-</td>
<td>1. LLaMA-7B (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>-</td>
<td>LLaMA-7B (Teavorn et al., 2023b)</td>
<td>-</td>
<td>1. LLaMA-7B (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>-</td>
<td>LLaMA-7B (Teavorn et al., 2023b)</td>
<td>-</td>
<td>1. LLaMA-7B (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>-</td>
<td>LLaMA-7B (Teavorn et al., 2023b)</td>
<td>-</td>
<td>1. LLaMA-7B (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>-</td>
<td>LLaMA-7B (Teavorn et al., 2023b)</td>
<td>-</td>
<td>1. LLaMA-7B (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>-</td>
<td>LLaMA-7B (Teavorn et al., 2023b)</td>
<td>-</td>
<td>1. LLaMA-7B (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>-</td>
<td>LLaMA-7B (Teavorn et al., 2023b)</td>
<td>-</td>
<td>1. LLaMA-7B (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>-</td>
<td>LLaMA-7B (Teavorn et al., 2023b)</td>
<td>-</td>
<td>1. LLaMA-7B (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>-</td>
<td>LLaMA-7B (Teavorn et al., 2023b)</td>
<td>-</td>
<td>1. LLaMA-7B (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>-</td>
<td>LLaMA-7B (Teavorn et al., 2023b)</td>
<td>-</td>
<td>1. LLaMA-7B (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>-</td>
<td>LLaMA-7B (Teavorn et al., 2023b)</td>
<td>-</td>
<td>1. LLaMA-7B (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
<tr>
<td>LLaMA-Adapter v2-7B (Gao et al., 2023b)</td>
<td>-</td>
<td>LLaMA-7B (Teavorn et al., 2023b)</td>
<td>-</td>
<td>1. LLaMA-7B (Teavorn et al., 2023b)</td>
<td>Not clear</td>
</tr>
</tbody>
</table>
<p>Table 12: Three samples requiring different capability integrations.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Table 13: Four samples requiring different capability integrations.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Q: Which room is bigger, the double garage or the living room?
GT: double garage
Required capabilities: OCR, spatial awareness, math
(b)
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Q: On the right desk, what is to the left of the laptop?
GT: table lamp $&lt;\mathrm{OR}&gt;$ desk lamp
Required capabilities: Recognition, spatial awareness
(c)
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Q: What are all the scene text in the image?
GT: 5:30PM <AND> $88 \%$ <AND> Mario Kart 8 Deluxe <AND> MARIO KART 8 DELUXE <AND> SUPER MARIO ODYSSEY <AND> THE LEGEND OF ZELDA <AND> BREATH OF WILD <AND> Options <AND> Start
Required capabilities: OCR
(d)
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Q: How many gallons of supreme gasoline can I get with $\$ 50$ ?
GT: $13.6&lt;\mathrm{OR}&gt;13.7$
Required capabilities: OCR, math</p>
<p>Table 14: Two samples requiring different capability integrations.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Q: In which country was this photo taken?
GT: Australia
Required capabilities: Recognition, knowledge
(b)</p>
<p>Me: I'll do it at 8
Time: 8.05
Me: looks like I gotta wait till 9 now
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Q: Can you explain this meme?
GT: This meme is a humorous take on procrastination and the tendency to delay tasks until a specific time. The person in the meme plans to do something at 8 o'clock, but when they miss that deadline by a few minutes, they decide to wait until 9 o'clock instead. The image of Kermit the Frog lying in bed represents the person's laziness and lack of motivation to complete the task.
Required capabilities: Recognition, OCR, knowledge, language generation</p>
<p>Table 15: Two samples requiring different capability integrations.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Q: The graph below shows the long-term international migration, UK, 1999-2008. Summarize the information by selecting and reporting the main features, and make comparisons where relevant.
You should write at least 150 words.
GT: The chart gives information about UK immigration, emigration and net migration between 1999 and 2008.</p>
<p>Both immigration and emigration rates rose over the period shown, but the figures for immigration were significantly higher. Net migration peaked in 2004 and 2007.</p>
<p>In 1999, over 450,000 people came to live in the UK, while the number of people who emigrated stood at just under 300,000. The figure for net migration was around 160,000, and it remained at a similar level until 2003. From 1999 to 2004, the immigration rate rose by nearly 150,000 people, but there was a much smaller rise in emigration. Net migration peaked at almost 250,000 people in 2004.</p>
<p>After 2004, the rate of immigration remained high, but the number of people emigrating fluctuated. Emigration fell suddenly in 2007, before peaking at about 420,000 people in 2008. As a result, the net migration figure rose to around 240,000 in 2007, but fell back to around 160,000 in 2008.
Required capabilities: Recognition, OCR, language generation, spatial awareness
(b)
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Q: Which car is on the parking spot 33 ?
GT: no $&lt;\mathrm{OR}&gt;$ empty
Required capabilities: Recognition, OCR, spatial awareness</p>
<p>Table 16: Three samples requiring different capability integrations.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Q: Is this apple organic?
GT: yes
Required capabilities: Recognition, OCR
(b)
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Q: Which are producers in this food web?
GT: Phytoplankton <AND> Seaweed
Required capabilities: OCR, knowledge, spatial awareness
(c)
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Q: Does the person bigger than the car?
GT: no
Required capabilities: Recognition, knowledge, spatial awareness</p>
<p>Table 17: Two samples requiring different capability integrations.
(a)</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Underground Railway Systems</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">City</td>
<td style="text-align: center;">Date opened</td>
<td style="text-align: center;">Kilometres of route</td>
<td style="text-align: center;">Passengers per year <br> (in millions)</td>
</tr>
<tr>
<td style="text-align: center;">London</td>
<td style="text-align: center;">1863</td>
<td style="text-align: center;">394</td>
<td style="text-align: center;">775</td>
</tr>
<tr>
<td style="text-align: center;">Paris</td>
<td style="text-align: center;">1900</td>
<td style="text-align: center;">199</td>
<td style="text-align: center;">1191</td>
</tr>
<tr>
<td style="text-align: center;">Tokyo</td>
<td style="text-align: center;">1927</td>
<td style="text-align: center;">155</td>
<td style="text-align: center;">1927</td>
</tr>
<tr>
<td style="text-align: center;">Washington DC</td>
<td style="text-align: center;">1976</td>
<td style="text-align: center;">126</td>
<td style="text-align: center;">144</td>
</tr>
<tr>
<td style="text-align: center;">Kyoto</td>
<td style="text-align: center;">1981</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">45</td>
</tr>
<tr>
<td style="text-align: center;">Los Angeles</td>
<td style="text-align: center;">2001</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">50</td>
</tr>
</tbody>
</table>
<p>Q: The table below gives information about the underground railway systems in six cities.</p>
<p>Summarise the information by selecting and reporting the main features, and make comparisons where relevant.</p>
<p>You should write at least 150 words.
GT: The table shows data about the underground rail networks in six major cities.
The table compares the six networks in terms of their age, size and the number of people who use them each year. It is clear that the three oldest underground systems are larger and serve significantly more passengers than the newer systems.</p>
<p>The London underground is the oldest system, having opened in 1863. It is also the largest system, with 394 kilometres of route. The second largest system, in Paris, is only about half the size of the London underground, with 199 kilometres of route. However, it serves more people per year. While only third in terms of size, the Tokyo system is easily the most used, with 1927 million passengers per year.</p>
<p>Of the three newer networks, the Washington DC underground is the most extensive, with 126 kilometres of route, compared to only 11 kilometres and 28 kilometres for the Kyoto and Los Angeles systems. The Los Angeles network is the newest, having opened in 2001, while the Kyoto network is the smallest and serves only 45 million passengers per year.
Required capabilities: OCR, language generation, spatial awareness
(b)
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Q: What will the girl on the right write on the board?
GT: 14
Required capabilities: Recognition, OCR, spatial awareness, math</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ hf.co/spaces/whyu/MM-Vet_Evaluator&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
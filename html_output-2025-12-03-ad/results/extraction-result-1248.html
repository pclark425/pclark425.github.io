<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1248 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1248</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1248</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-267412169</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.01695v3.pdf" target="_blank">Language-guided World Models: A Model-based Approach to AI Control</a></p>
                <p><strong>Paper Abstract:</strong> Developing internal world models for artificial agents opens an efficient channel for humans to communicate with and control them. In addition to updating policies, humans can modify the world models of these agents in order to influence their decisions.The challenge, however, is that currently existing world models are difficult for humans to adapt because they lack a natural communication interface. Aimed at addressing this shortcoming, we develop *Language-Guided World Models* (LWMs), which can capture environment dynamics by reading language descriptions. These models enhance agent communication efficiency, allowing humans to simultaneously alter their behavior on multiple tasks with concise language feedback. They also enable agents to self-learn from texts originally written to instruct humans. To facilitate the development of LWMs, we design a challenging benchmark based on the game of MESSENGER (Hanjie et al., 2021), requiring compositional generalization to new language descriptions and environment dynamics. Our experiments reveal that the current state-of-the-art Transformer architecture performs poorly on this benchmark, motivating us to design a more robust architecture. To showcase the practicality of our proposed LWMs, we simulate a scenario where these models augment the interpretability and safety of an agent by enabling it to generate and discuss plans with a human before execution. By effectively incorporating language feedback on the plan, the models boost the agent performance in the real environment by up to three times without collecting any interactive experiences in this environment.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1248.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1248.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EMMA-LWM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EMMA-attention Language-Guided World Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer encoder-decoder world model that replaces standard cross-attention with an EMMA-style multi-step attention to ground language descriptions to disentangled entity tokens, enabling compositional generalization in language-conditioned environment simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>EMMA-LWM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder Transformer that encodes a manual with a pre-trained BERT encoder producing N×L×D tensors, forms per-description key and value tensors via learned linear projections and softmax, and uses a dot-product 'EMMA' attention where identity embeddings of entity tokens query the description keys/values to produce attribute features z_ct which are added to identity tokens; the decoder then autoregressively generates tokenized trajectories (entity identity + (h,w) per entity, reward, done). Cross-attention is intentionally omitted because language information is injected into decoder inputs via EMMA.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>neural simulator (transformer-based, language-conditioned)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Grid-world game simulation (MESSENGER benchmark); language-conditioned world modeling for downstream policy learning and plan discussion</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Cross-entropy loss on next-token trajectory generation (Table 1); ∆dist (average per-timestep difference in Hamming distance between real and imagined player-to-entity distances), non-zero reward prediction precision, termination prediction precision (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Cross-entropy losses: NewCombo 0.08 ± 0.01, NewAttr 0.10 ± 0.02, NewAll 0.13 ± 0.01 (Table 1). Imaginary-trajectory metrics: ∆dist NewCombo 0.57, NewAttr 1.14, NewAll 1.29; Non-zero reward precision NewCombo 0.88, NewAttr 0.69, NewAll 0.70; Termination precision NewCombo 0.88, NewAttr 0.75, NewAll 0.71 (Table 2). Task returns after imitation learning (Table 3): e.g., Online IL (near-optimal) NewCombo 1.01 ± 0.12, NewAttr 0.96 ± 0.17, NewAll 0.62 ± 0.21.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Partially interpretable: architecture explicitly routes language-derived attribute features to entity identity tokens (entity-level attention), making the mapping from description words to entity features inspectable via the EMMA attention weights; internal representations still neural and not symbolic.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>EMMA attention mechanism (dot-product attention between identity embeddings and per-description key/value tensors) which can be inspected to see which description/words influenced each entity; use of BERT encoder produces contextualized word embeddings for inspection; qualitative examples in paper show correct role/movement grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Training: experiments ran on NVIDIA RTX2080 GPUs; to generate Table 1 each world model was trained for ~24 GPU-hours per model with 5 random seeds (paper-level reporting). For broader experiments, some runs used 12 GPU-hours per game configuration. No explicit parameter counts or per-step inference latency reported.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>EMMA-LWM consistently attains lower cross-entropy and better imaginary-trajectory metrics than baselines (Observational, Standard, GPTHard) for the same training regime and dataset; EMMA-LWM conditioned on a one-step history outperforms Observational conditioned on one-third of a ground-truth trajectory (paper claim), indicating improved sample / conditioning efficiency for incorporating language.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>In downstream policy-learning via imaginary imitation learning, EMMA-LWM yields substantially higher average returns than the Observational baseline across evaluation splits (Table 3). Example: Filtered BC (near-optimal): Observational NewAll  -0.30 ± 0.16 vs EMMA-LWM 0.44 ± 0.18; Online IL (near-optimal) NewAll Observational -0.21 ± 0.21 vs EMMA-LWM 0.62 ± 0.21.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High fidelity (lower cross-entropy, better ∆dist and reward/termination precision) correlates with improved downstream policy performance and safer pre-execution plan discussion; EMMA-LWM's improved grounding of language to entity dynamics enables policy learning from imagined trajectories in unseen compositionally novel environments, producing 3–4× higher average reward vs observational model in the hardest setting (claimed in text).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Design that improves compositional generalization (EMMA routing and entity-disentangled tokenization) increases architectural complexity over naive observational models but yields much better fidelity and task utility; still not matching OracleParse (skyline) indicating residual errors due to imperfect parsing/grounding. No explicit runtime latency or parameter-count tradeoff numbers reported.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Key choices: disentangled entity tokenization (3C tokens per state: identity, h, w), BERT-based encoding of manuals, EMMA multi-modal attention (separate key/value projections to extract identity and attribute words), omission of cross-attention (language fused into inputs), autoregressive token generation for states/rewards/dones.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to Observational (history-conditioned neural predictor), EMMA-LWM leverages manuals and explicit attention routing to significantly improve cross-entropy, ∆dist, reward/termination precision, and downstream policy returns. Compared to Standard Transformer baseline with cross-attention, EMMA-LWM is more robust across random initializations and generalizes better to NewAttr/NewAll splits. Compared to GPTHard (ChatGPT identity parsing + learned attribute extraction), EMMA-LWM performs better because GPTHard suffers from imperfect ChatGPT parses (~90% identity accuracy) and decoupled training.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends an architecture that (1) disentangles entities in the state representation, (2) encodes manuals with a strong language encoder (BERT), and (3) uses an EMMA-like attention that first identifies the description mentioning an entity then extracts attribute words for that entity; OracleParse (ground-truth parses) is skyline, so improving parse/grounding quality is a key direction. Authors also advocate modular, language-parameterized components for efficiency and controllability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language-guided World Models: A Model-based Approach to AI Control', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1248.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1248.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Observational</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Observational World Model (history-conditioned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dominant class of world models that predicts next-state, reward, and termination conditioned only on a sequence of past observations and actions (history), trained via in-weight learning or in-context learning on observational data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Observational world model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neural network (Transformer-like in this paper's baselines) that models M_theta(s_{t+1}, r_{t+1}, d_{t+1} | h_t) where h_t is the history of observed states, actions, rewards, and dones; does not take natural-language manuals as input and cannot be directly modulated by language.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>neural predictive model (history-conditioned)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General model-based RL / environment simulation; evaluated on MESSENGER grid-world benchmark in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Cross-entropy loss on trajectory token prediction (Table 1); ∆dist, non-zero reward precision, termination precision for imaginary trajectories (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Cross-entropy losses: NewCombo 0.12 ± 0.04, NewAttr 0.18 ± 0.02, NewAll 0.19 ± 0.01 (Table 1). Imaginary-trajectory metrics (Table 2) show substantially worse ∆dist and lower reward/termination precisions: e.g., Non-zero reward precision NewAll 0.15; Termination precision NewAll 0.28; ∆dist NewAll 3.00.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Largely a black-box neural model with no explicit language-grounding interpretability; prone to learning spurious correlations between observed identity and attributes, making its internal mapping from observation to dynamics opaque.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None reported in paper; shortcomings illustrated qualitatively (Figure 3) where the model confuses entity roles/movements due to spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Same training regime as other baselines in experiments (trained on RTX2080 GPUs; ~24 GPU-hours per model for Table 1 runs). No parameter counts or latency numbers reported.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Requires more observational data to adapt to environment changes (cannot be modulated directly by language), and yields inferior simulation-conditioned policy performance compared to EMMA-LWM for the same dataset; paper argues model-free adaptation cost scales with number of tasks whereas model-based (with a usable world model) can be more efficient, but Observational models specifically still require observational data collection.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Downstream policy returns are substantially lower than EMMA-LWM in all splits (Table 3). Example: Online IL (near-optimal) NewAll Observational -0.21 ± 0.21 vs EMMA-LWM 0.62 ± 0.21.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Although an observational model can simulate dynamics from histories, its inability to incorporate language reduces its controllability and sample efficiency for composing novel dynamics from manuals; high observational prediction accuracy on seen combos does not translate into compositional generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Simplicity and reliance on observations yields ease of training with purely observational data but produces poor compositional generalization and susceptibility to spurious correlations; lacks human-modifiability via language.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Conditioning only on observation histories; no explicit language encoder or per-entity attention routing; standard Transformer-like or RNN architectures as in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Performantly inferior to EMMA-LWM when language manuals are available and environments require compositional generalization; compared to OracleParse, substantially worse. Observational approach remains the baseline for scenarios where no language manual exists.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests that observational models alone are insufficient when human-language modifiability and compositional generalization are required; incorporating explicit language grounding and entity-disentangling inductive biases is recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language-guided World Models: A Model-based Approach to AI Control', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1248.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1248.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Standard</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Standard Transformer LWM (cross-attention baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard encoder-decoder Transformer that encodes the language manual and uses cross-attention to incorporate language into trajectory decoding; used as a baseline to evaluate compositional generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Standard Transformer (with cross-attention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder Transformer: manuals encoded (BERT or transformer encoder) and decoder attends to encoder via standard cross-attention; decodes tokenized trajectories autoregressively. Differs from EMMA-LWM by using cross-attention rather than the EMMA two-step attention.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>neural simulator (transformer-based)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>MESSENGER world modeling; language-conditioned trajectory generation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Cross-entropy loss on next-token prediction (Table 1); imaginary-trajectory metrics (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Cross-entropy losses (Table 1): NewCombo 0.10 ± 0.04, NewAttr 0.15 ± 0.04, NewAll 0.16 ± 0.03. Imaginary-trajectory metrics (Table 2): ∆dist NewAll 1.68; Non-zero reward precision NewAll 0.50; Termination precision NewAll 0.62.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Less interpretable than EMMA-LWM for grounding language to specific entities; authors note Standard is sensitive to initialization and lacks inductive bias to reliably learn disentangled grounding, so its internal attention/representations are not consistently interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Uses standard attention mechanisms which could be inspected, but paper reports inconsistent behavior across seeds indicating attention maps are not a reliable interpretability method for compositional grounding in this baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Same reported training regime (24 GPU-hours per model for Table 1 runs on RTX2080). No additional compute overheads beyond standard Transformer baseline reported.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Sometimes matches EMMA-LWM in particular random seeds, but on average underperforms; inconsistent across initializations indicating poorer robustness/efficiency in finding generalizable solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Intermediate between Observational and EMMA-LWM on most metrics and splits (Table 1-3). For NewAll split, performance is worse than EMMA-LWM but sometimes comparable depending on initialization.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Standard cross-attention architecture does not reliably provide the inductive biases needed for compositional generalization from language; when it does converge to a good solution it can be competitive, but it's less robust than EMMA-LWM.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Standard architecture is simpler (using off-the-shelf cross-attention) but lacks the two-step grounding inductive bias of EMMA, resulting in a tradeoff between simplicity and reliable compositional generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Retains cross-attention between encoder and decoder; uses BERT or transformer encoder for manuals; otherwise standard autoregressive decoding of tokenized trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Worse average performance and robustness than EMMA-LWM; better than Observational in some settings but inconsistent. OracleParse remains the best.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests adding stronger inductive biases (like EMMA) is beneficial to obtain consistent compositional generalization; mere cross-attention may require careful initialization or auxiliary losses to reach comparable performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language-guided World Models: A Model-based Approach to AI Control', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1248.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1248.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPTHard</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPTHard (ChatGPT-assisted parsing + learned grounding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid baseline that uses ChatGPT (few-shot prompts) to parse which description mentions which entity (identities), then routes learned m_val attribute vectors to identity embeddings accordingly; relies on external LLM parsing for identity resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPTHard</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Constructs m_val (value tensors) as in EMMA but uses ChatGPT to produce a hard parse mapping from description index to entity identity j_c, and sets z_ct = m_val_{j_c}; ChatGPT is prompted per description to extract identity/role/movement but only identity is used for routing in this model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>hybrid (LLM-assisted neural world model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>MESSENGER language-conditioned world modeling; experiments evaluate effect of external LLM parsing on grounding and simulation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Same as other models: cross-entropy loss on trajectories, ∆dist, non-zero reward precision, termination precision.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Cross-entropy (Table 1): NewCombo 0.10 ± 0.02, NewAttr 0.15 ± 0.01, NewAll 0.16 ± 0.00. Imaginary-trajectory metrics (Table 2): ∆dist NewAll 2.89; Non-zero reward precision NewAll 0.25; Termination precision NewAll 0.45. Underperforms EMMA-LWM despite using ChatGPT parses.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Partially interpretable because routing decisions come from explicit ChatGPT parses that can be inspected, but overall model still uses learned neural attribute extraction; performance depends on parse correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Uses explicit ChatGPT outputs (text) to assign descriptions to entities; the mapping is human-readable and inspectable. No other interpretability methods reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Requires external API/LLM calls (ChatGPT 'May 3, 2023' release in experiments) for parsing descriptions, adding practical cost and latency; training of the remainder of the model is similar to other baselines (same GPU regimes). The paper reports ChatGPT identity-parsing accuracy around 90% (Appendix B), which limits performance.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>GPTHard leverages off-the-shelf LLM parsing to reduce learning burden but still underperforms EMMA-LWM due to imperfect parses and because EMMA-LWM jointly learns identity+attribute extraction. Practical costs include LLM usage and latency.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Intermediate baseline; falls behind EMMA-LWM on most fidelity and downstream task metrics (Tables 1-3).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Using an external LLM to provide parses is a practical shortcut but errors in parsing propagate to the world model and reduce simulation and downstream policy performance; joint learning of identity+attributes (EMMA-LWM) outperforms this decoupled approach in this setting.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Tradeoff between offloading parsing to a strong LLM (reducing model learning demand) vs. dependence on LLM accuracy and added cost/latency; in practice imperfect parses (~90% identity accuracy) limit utility and do not match joint EMMA learning.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use ChatGPT few-shot prompting to parse identities per description, then use learned m_val to extract attributes and route to identity embeddings; simplified relative to full EMMA pipeline because it uses hard parse indices from an external LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Worse than EMMA-LWM overall (joint learning + EMMA attention is superior) but often better than purely Observational model because it leverages language via external LLM; OracleParse (ground-truth parses) remains the skyline.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper indicates that a skyline of perfect parsing + grounding (OracleParse) yields best results; GPTHard shows that external parsing helps but needs near-perfect accuracy to match joint EMMA learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language-guided World Models: A Model-based Approach to AI Control', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1248.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1248.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OracleParse</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OracleParse (ground-truth semantic parsing and grounding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A skyline baseline that uses ground-truth parsed manuals (perfect identity/attribute parses) to route attribute information to entities, representing an upper bound on performance for parsing+grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OracleParse</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same modeling pipeline as EMMA-LWM except that identity/attribute parses are provided from ground truth rather than learned or parsed by ChatGPT; m_val values are routed to the correct entity indices using oracle parses.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>hybrid (oracle-augmented neural simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>MESSENGER benchmark; used as an upper-bound baseline for language-grounded world models</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Cross-entropy loss on trajectories; imaginary-trajectory metrics (∆dist, reward/termination precision), downstream policy returns.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Cross-entropy losses (Table 1): NewCombo 0.08 ± 0.01, NewAttr 0.09 ± 0.02, NewAll 0.12 ± 0.06. Imaginary-trajectory metrics (Table 2): ∆dist NewAll 0.92; Non-zero reward precision NewAll 0.77; Termination precision NewAll 0.79. Task returns (Table 3) highest among evaluated models in many settings (e.g., Online IL NewAll 0.91 ± 0.18).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Highly interpretable insofar as parsing and grounding are given (symbolic/ground-truth mapping between descriptions and entity attributes); internal simulation is still neural but grounded by explicit symbolic labels.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Relies on ground-truth parses (explicit, human-readable labels) for identity/attribute association; by definition provides the cleanest mapping between language and entity attributes in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Training cost comparable to other world models in experiments (same GPU regimes). In practice, obtaining ground-truth parses is unrealistic; cost is conceptual or oracle rather than computational.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Serves as a skyline upper bound; outperforms learned models (EMMA-LWM, Standard, Observational) when perfect parsing is available, indicating that improvements in parse/grounding quality are a major lever for model performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Best or near-best in fidelity and downstream policy returns across splits (Table 1-3). Example: Cross-entropy NewAll 0.12 ± 0.06 vs EMMA-LWM 0.13 ± 0.01; Task returns generally higher than EMMA-LWM in several settings (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Demonstrates that with perfect semantic parsing/grounding, language-conditioned world models can accurately simulate compositionally novel environments and produce strong downstream policies; highlights gap due to imperfect parsing in learned models.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>OracleParse is impractical (requires ground-truth symbolic parses) but clarifies performance ceiling; indicates that improvements in parsing/grounding yield high returns without necessarily changing simulator architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Uses oracle symbolic parses to route attributes to entities; otherwise follows an EMMA-style routing and decoder for trajectory generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Best-performing baseline; EMMA-LWM approaches OracleParse performance but a gap remains, pointing to parsing/grounding as a primary limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper treats OracleParse as an upper-bound and suggests closing the gap via better learned parsing/grounding (e.g., stronger inductive biases, improved language encoders, joint learning) to approach oracle performance while remaining practical.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language-guided World Models: A Model-based Approach to AI Control', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Grounding language to entities and dynamics for generalization in reinforcement learning <em>(Rating: 2)</em></li>
                <li>Transformers are sample-efficient world models <em>(Rating: 2)</em></li>
                <li>Langwm: Language grounded world model <em>(Rating: 2)</em></li>
                <li>Learning to model the world with language <em>(Rating: 2)</em></li>
                <li>Rtfm: Generalising to new environment dynamics via reading <em>(Rating: 2)</em></li>
                <li>Dream to control: Learning behaviors by latent imagination <em>(Rating: 1)</em></li>
                <li>Mastering diverse domains through world models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1248",
    "paper_id": "paper-267412169",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "EMMA-LWM",
            "name_full": "EMMA-attention Language-Guided World Model",
            "brief_description": "A transformer encoder-decoder world model that replaces standard cross-attention with an EMMA-style multi-step attention to ground language descriptions to disentangled entity tokens, enabling compositional generalization in language-conditioned environment simulation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "EMMA-LWM",
            "model_description": "Encoder-decoder Transformer that encodes a manual with a pre-trained BERT encoder producing N×L×D tensors, forms per-description key and value tensors via learned linear projections and softmax, and uses a dot-product 'EMMA' attention where identity embeddings of entity tokens query the description keys/values to produce attribute features z_ct which are added to identity tokens; the decoder then autoregressively generates tokenized trajectories (entity identity + (h,w) per entity, reward, done). Cross-attention is intentionally omitted because language information is injected into decoder inputs via EMMA.",
            "model_type": "neural simulator (transformer-based, language-conditioned)",
            "task_domain": "Grid-world game simulation (MESSENGER benchmark); language-conditioned world modeling for downstream policy learning and plan discussion",
            "fidelity_metric": "Cross-entropy loss on next-token trajectory generation (Table 1); ∆dist (average per-timestep difference in Hamming distance between real and imagined player-to-entity distances), non-zero reward prediction precision, termination prediction precision (Table 2).",
            "fidelity_performance": "Cross-entropy losses: NewCombo 0.08 ± 0.01, NewAttr 0.10 ± 0.02, NewAll 0.13 ± 0.01 (Table 1). Imaginary-trajectory metrics: ∆dist NewCombo 0.57, NewAttr 1.14, NewAll 1.29; Non-zero reward precision NewCombo 0.88, NewAttr 0.69, NewAll 0.70; Termination precision NewCombo 0.88, NewAttr 0.75, NewAll 0.71 (Table 2). Task returns after imitation learning (Table 3): e.g., Online IL (near-optimal) NewCombo 1.01 ± 0.12, NewAttr 0.96 ± 0.17, NewAll 0.62 ± 0.21.",
            "interpretability_assessment": "Partially interpretable: architecture explicitly routes language-derived attribute features to entity identity tokens (entity-level attention), making the mapping from description words to entity features inspectable via the EMMA attention weights; internal representations still neural and not symbolic.",
            "interpretability_method": "EMMA attention mechanism (dot-product attention between identity embeddings and per-description key/value tensors) which can be inspected to see which description/words influenced each entity; use of BERT encoder produces contextualized word embeddings for inspection; qualitative examples in paper show correct role/movement grounding.",
            "computational_cost": "Training: experiments ran on NVIDIA RTX2080 GPUs; to generate Table 1 each world model was trained for ~24 GPU-hours per model with 5 random seeds (paper-level reporting). For broader experiments, some runs used 12 GPU-hours per game configuration. No explicit parameter counts or per-step inference latency reported.",
            "efficiency_comparison": "EMMA-LWM consistently attains lower cross-entropy and better imaginary-trajectory metrics than baselines (Observational, Standard, GPTHard) for the same training regime and dataset; EMMA-LWM conditioned on a one-step history outperforms Observational conditioned on one-third of a ground-truth trajectory (paper claim), indicating improved sample / conditioning efficiency for incorporating language.",
            "task_performance": "In downstream policy-learning via imaginary imitation learning, EMMA-LWM yields substantially higher average returns than the Observational baseline across evaluation splits (Table 3). Example: Filtered BC (near-optimal): Observational NewAll  -0.30 ± 0.16 vs EMMA-LWM 0.44 ± 0.18; Online IL (near-optimal) NewAll Observational -0.21 ± 0.21 vs EMMA-LWM 0.62 ± 0.21.",
            "task_utility_analysis": "High fidelity (lower cross-entropy, better ∆dist and reward/termination precision) correlates with improved downstream policy performance and safer pre-execution plan discussion; EMMA-LWM's improved grounding of language to entity dynamics enables policy learning from imagined trajectories in unseen compositionally novel environments, producing 3–4× higher average reward vs observational model in the hardest setting (claimed in text).",
            "tradeoffs_observed": "Design that improves compositional generalization (EMMA routing and entity-disentangled tokenization) increases architectural complexity over naive observational models but yields much better fidelity and task utility; still not matching OracleParse (skyline) indicating residual errors due to imperfect parsing/grounding. No explicit runtime latency or parameter-count tradeoff numbers reported.",
            "design_choices": "Key choices: disentangled entity tokenization (3C tokens per state: identity, h, w), BERT-based encoding of manuals, EMMA multi-modal attention (separate key/value projections to extract identity and attribute words), omission of cross-attention (language fused into inputs), autoregressive token generation for states/rewards/dones.",
            "comparison_to_alternatives": "Compared to Observational (history-conditioned neural predictor), EMMA-LWM leverages manuals and explicit attention routing to significantly improve cross-entropy, ∆dist, reward/termination precision, and downstream policy returns. Compared to Standard Transformer baseline with cross-attention, EMMA-LWM is more robust across random initializations and generalizes better to NewAttr/NewAll splits. Compared to GPTHard (ChatGPT identity parsing + learned attribute extraction), EMMA-LWM performs better because GPTHard suffers from imperfect ChatGPT parses (~90% identity accuracy) and decoupled training.",
            "optimal_configuration": "Paper recommends an architecture that (1) disentangles entities in the state representation, (2) encodes manuals with a strong language encoder (BERT), and (3) uses an EMMA-like attention that first identifies the description mentioning an entity then extracts attribute words for that entity; OracleParse (ground-truth parses) is skyline, so improving parse/grounding quality is a key direction. Authors also advocate modular, language-parameterized components for efficiency and controllability.",
            "uuid": "e1248.0",
            "source_info": {
                "paper_title": "Language-guided World Models: A Model-based Approach to AI Control",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Observational",
            "name_full": "Observational World Model (history-conditioned)",
            "brief_description": "A dominant class of world models that predicts next-state, reward, and termination conditioned only on a sequence of past observations and actions (history), trained via in-weight learning or in-context learning on observational data.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Observational world model",
            "model_description": "Neural network (Transformer-like in this paper's baselines) that models M_theta(s_{t+1}, r_{t+1}, d_{t+1} | h_t) where h_t is the history of observed states, actions, rewards, and dones; does not take natural-language manuals as input and cannot be directly modulated by language.",
            "model_type": "neural predictive model (history-conditioned)",
            "task_domain": "General model-based RL / environment simulation; evaluated on MESSENGER grid-world benchmark in the paper",
            "fidelity_metric": "Cross-entropy loss on trajectory token prediction (Table 1); ∆dist, non-zero reward precision, termination precision for imaginary trajectories (Table 2).",
            "fidelity_performance": "Cross-entropy losses: NewCombo 0.12 ± 0.04, NewAttr 0.18 ± 0.02, NewAll 0.19 ± 0.01 (Table 1). Imaginary-trajectory metrics (Table 2) show substantially worse ∆dist and lower reward/termination precisions: e.g., Non-zero reward precision NewAll 0.15; Termination precision NewAll 0.28; ∆dist NewAll 3.00.",
            "interpretability_assessment": "Largely a black-box neural model with no explicit language-grounding interpretability; prone to learning spurious correlations between observed identity and attributes, making its internal mapping from observation to dynamics opaque.",
            "interpretability_method": "None reported in paper; shortcomings illustrated qualitatively (Figure 3) where the model confuses entity roles/movements due to spurious correlations.",
            "computational_cost": "Same training regime as other baselines in experiments (trained on RTX2080 GPUs; ~24 GPU-hours per model for Table 1 runs). No parameter counts or latency numbers reported.",
            "efficiency_comparison": "Requires more observational data to adapt to environment changes (cannot be modulated directly by language), and yields inferior simulation-conditioned policy performance compared to EMMA-LWM for the same dataset; paper argues model-free adaptation cost scales with number of tasks whereas model-based (with a usable world model) can be more efficient, but Observational models specifically still require observational data collection.",
            "task_performance": "Downstream policy returns are substantially lower than EMMA-LWM in all splits (Table 3). Example: Online IL (near-optimal) NewAll Observational -0.21 ± 0.21 vs EMMA-LWM 0.62 ± 0.21.",
            "task_utility_analysis": "Although an observational model can simulate dynamics from histories, its inability to incorporate language reduces its controllability and sample efficiency for composing novel dynamics from manuals; high observational prediction accuracy on seen combos does not translate into compositional generalization.",
            "tradeoffs_observed": "Simplicity and reliance on observations yields ease of training with purely observational data but produces poor compositional generalization and susceptibility to spurious correlations; lacks human-modifiability via language.",
            "design_choices": "Conditioning only on observation histories; no explicit language encoder or per-entity attention routing; standard Transformer-like or RNN architectures as in prior work.",
            "comparison_to_alternatives": "Performantly inferior to EMMA-LWM when language manuals are available and environments require compositional generalization; compared to OracleParse, substantially worse. Observational approach remains the baseline for scenarios where no language manual exists.",
            "optimal_configuration": "Paper suggests that observational models alone are insufficient when human-language modifiability and compositional generalization are required; incorporating explicit language grounding and entity-disentangling inductive biases is recommended.",
            "uuid": "e1248.1",
            "source_info": {
                "paper_title": "Language-guided World Models: A Model-based Approach to AI Control",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Standard",
            "name_full": "Standard Transformer LWM (cross-attention baseline)",
            "brief_description": "A standard encoder-decoder Transformer that encodes the language manual and uses cross-attention to incorporate language into trajectory decoding; used as a baseline to evaluate compositional generalization.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Standard Transformer (with cross-attention)",
            "model_description": "Encoder-decoder Transformer: manuals encoded (BERT or transformer encoder) and decoder attends to encoder via standard cross-attention; decodes tokenized trajectories autoregressively. Differs from EMMA-LWM by using cross-attention rather than the EMMA two-step attention.",
            "model_type": "neural simulator (transformer-based)",
            "task_domain": "MESSENGER world modeling; language-conditioned trajectory generation",
            "fidelity_metric": "Cross-entropy loss on next-token prediction (Table 1); imaginary-trajectory metrics (Table 2).",
            "fidelity_performance": "Cross-entropy losses (Table 1): NewCombo 0.10 ± 0.04, NewAttr 0.15 ± 0.04, NewAll 0.16 ± 0.03. Imaginary-trajectory metrics (Table 2): ∆dist NewAll 1.68; Non-zero reward precision NewAll 0.50; Termination precision NewAll 0.62.",
            "interpretability_assessment": "Less interpretable than EMMA-LWM for grounding language to specific entities; authors note Standard is sensitive to initialization and lacks inductive bias to reliably learn disentangled grounding, so its internal attention/representations are not consistently interpretable.",
            "interpretability_method": "Uses standard attention mechanisms which could be inspected, but paper reports inconsistent behavior across seeds indicating attention maps are not a reliable interpretability method for compositional grounding in this baseline.",
            "computational_cost": "Same reported training regime (24 GPU-hours per model for Table 1 runs on RTX2080). No additional compute overheads beyond standard Transformer baseline reported.",
            "efficiency_comparison": "Sometimes matches EMMA-LWM in particular random seeds, but on average underperforms; inconsistent across initializations indicating poorer robustness/efficiency in finding generalizable solutions.",
            "task_performance": "Intermediate between Observational and EMMA-LWM on most metrics and splits (Table 1-3). For NewAll split, performance is worse than EMMA-LWM but sometimes comparable depending on initialization.",
            "task_utility_analysis": "Standard cross-attention architecture does not reliably provide the inductive biases needed for compositional generalization from language; when it does converge to a good solution it can be competitive, but it's less robust than EMMA-LWM.",
            "tradeoffs_observed": "Standard architecture is simpler (using off-the-shelf cross-attention) but lacks the two-step grounding inductive bias of EMMA, resulting in a tradeoff between simplicity and reliable compositional generalization.",
            "design_choices": "Retains cross-attention between encoder and decoder; uses BERT or transformer encoder for manuals; otherwise standard autoregressive decoding of tokenized trajectories.",
            "comparison_to_alternatives": "Worse average performance and robustness than EMMA-LWM; better than Observational in some settings but inconsistent. OracleParse remains the best.",
            "optimal_configuration": "Paper suggests adding stronger inductive biases (like EMMA) is beneficial to obtain consistent compositional generalization; mere cross-attention may require careful initialization or auxiliary losses to reach comparable performance.",
            "uuid": "e1248.2",
            "source_info": {
                "paper_title": "Language-guided World Models: A Model-based Approach to AI Control",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "GPTHard",
            "name_full": "GPTHard (ChatGPT-assisted parsing + learned grounding)",
            "brief_description": "A hybrid baseline that uses ChatGPT (few-shot prompts) to parse which description mentions which entity (identities), then routes learned m_val attribute vectors to identity embeddings accordingly; relies on external LLM parsing for identity resolution.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPTHard",
            "model_description": "Constructs m_val (value tensors) as in EMMA but uses ChatGPT to produce a hard parse mapping from description index to entity identity j_c, and sets z_ct = m_val_{j_c}; ChatGPT is prompted per description to extract identity/role/movement but only identity is used for routing in this model.",
            "model_type": "hybrid (LLM-assisted neural world model)",
            "task_domain": "MESSENGER language-conditioned world modeling; experiments evaluate effect of external LLM parsing on grounding and simulation",
            "fidelity_metric": "Same as other models: cross-entropy loss on trajectories, ∆dist, non-zero reward precision, termination precision.",
            "fidelity_performance": "Cross-entropy (Table 1): NewCombo 0.10 ± 0.02, NewAttr 0.15 ± 0.01, NewAll 0.16 ± 0.00. Imaginary-trajectory metrics (Table 2): ∆dist NewAll 2.89; Non-zero reward precision NewAll 0.25; Termination precision NewAll 0.45. Underperforms EMMA-LWM despite using ChatGPT parses.",
            "interpretability_assessment": "Partially interpretable because routing decisions come from explicit ChatGPT parses that can be inspected, but overall model still uses learned neural attribute extraction; performance depends on parse correctness.",
            "interpretability_method": "Uses explicit ChatGPT outputs (text) to assign descriptions to entities; the mapping is human-readable and inspectable. No other interpretability methods reported.",
            "computational_cost": "Requires external API/LLM calls (ChatGPT 'May 3, 2023' release in experiments) for parsing descriptions, adding practical cost and latency; training of the remainder of the model is similar to other baselines (same GPU regimes). The paper reports ChatGPT identity-parsing accuracy around 90% (Appendix B), which limits performance.",
            "efficiency_comparison": "GPTHard leverages off-the-shelf LLM parsing to reduce learning burden but still underperforms EMMA-LWM due to imperfect parses and because EMMA-LWM jointly learns identity+attribute extraction. Practical costs include LLM usage and latency.",
            "task_performance": "Intermediate baseline; falls behind EMMA-LWM on most fidelity and downstream task metrics (Tables 1-3).",
            "task_utility_analysis": "Using an external LLM to provide parses is a practical shortcut but errors in parsing propagate to the world model and reduce simulation and downstream policy performance; joint learning of identity+attributes (EMMA-LWM) outperforms this decoupled approach in this setting.",
            "tradeoffs_observed": "Tradeoff between offloading parsing to a strong LLM (reducing model learning demand) vs. dependence on LLM accuracy and added cost/latency; in practice imperfect parses (~90% identity accuracy) limit utility and do not match joint EMMA learning.",
            "design_choices": "Use ChatGPT few-shot prompting to parse identities per description, then use learned m_val to extract attributes and route to identity embeddings; simplified relative to full EMMA pipeline because it uses hard parse indices from an external LLM.",
            "comparison_to_alternatives": "Worse than EMMA-LWM overall (joint learning + EMMA attention is superior) but often better than purely Observational model because it leverages language via external LLM; OracleParse (ground-truth parses) remains the skyline.",
            "optimal_configuration": "Paper indicates that a skyline of perfect parsing + grounding (OracleParse) yields best results; GPTHard shows that external parsing helps but needs near-perfect accuracy to match joint EMMA learning.",
            "uuid": "e1248.3",
            "source_info": {
                "paper_title": "Language-guided World Models: A Model-based Approach to AI Control",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "OracleParse",
            "name_full": "OracleParse (ground-truth semantic parsing and grounding)",
            "brief_description": "A skyline baseline that uses ground-truth parsed manuals (perfect identity/attribute parses) to route attribute information to entities, representing an upper bound on performance for parsing+grounding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OracleParse",
            "model_description": "Same modeling pipeline as EMMA-LWM except that identity/attribute parses are provided from ground truth rather than learned or parsed by ChatGPT; m_val values are routed to the correct entity indices using oracle parses.",
            "model_type": "hybrid (oracle-augmented neural simulator)",
            "task_domain": "MESSENGER benchmark; used as an upper-bound baseline for language-grounded world models",
            "fidelity_metric": "Cross-entropy loss on trajectories; imaginary-trajectory metrics (∆dist, reward/termination precision), downstream policy returns.",
            "fidelity_performance": "Cross-entropy losses (Table 1): NewCombo 0.08 ± 0.01, NewAttr 0.09 ± 0.02, NewAll 0.12 ± 0.06. Imaginary-trajectory metrics (Table 2): ∆dist NewAll 0.92; Non-zero reward precision NewAll 0.77; Termination precision NewAll 0.79. Task returns (Table 3) highest among evaluated models in many settings (e.g., Online IL NewAll 0.91 ± 0.18).",
            "interpretability_assessment": "Highly interpretable insofar as parsing and grounding are given (symbolic/ground-truth mapping between descriptions and entity attributes); internal simulation is still neural but grounded by explicit symbolic labels.",
            "interpretability_method": "Relies on ground-truth parses (explicit, human-readable labels) for identity/attribute association; by definition provides the cleanest mapping between language and entity attributes in experiments.",
            "computational_cost": "Training cost comparable to other world models in experiments (same GPU regimes). In practice, obtaining ground-truth parses is unrealistic; cost is conceptual or oracle rather than computational.",
            "efficiency_comparison": "Serves as a skyline upper bound; outperforms learned models (EMMA-LWM, Standard, Observational) when perfect parsing is available, indicating that improvements in parse/grounding quality are a major lever for model performance.",
            "task_performance": "Best or near-best in fidelity and downstream policy returns across splits (Table 1-3). Example: Cross-entropy NewAll 0.12 ± 0.06 vs EMMA-LWM 0.13 ± 0.01; Task returns generally higher than EMMA-LWM in several settings (Table 3).",
            "task_utility_analysis": "Demonstrates that with perfect semantic parsing/grounding, language-conditioned world models can accurately simulate compositionally novel environments and produce strong downstream policies; highlights gap due to imperfect parsing in learned models.",
            "tradeoffs_observed": "OracleParse is impractical (requires ground-truth symbolic parses) but clarifies performance ceiling; indicates that improvements in parsing/grounding yield high returns without necessarily changing simulator architecture.",
            "design_choices": "Uses oracle symbolic parses to route attributes to entities; otherwise follows an EMMA-style routing and decoder for trajectory generation.",
            "comparison_to_alternatives": "Best-performing baseline; EMMA-LWM approaches OracleParse performance but a gap remains, pointing to parsing/grounding as a primary limitation.",
            "optimal_configuration": "Paper treats OracleParse as an upper-bound and suggests closing the gap via better learned parsing/grounding (e.g., stronger inductive biases, improved language encoders, joint learning) to approach oracle performance while remaining practical.",
            "uuid": "e1248.4",
            "source_info": {
                "paper_title": "Language-guided World Models: A Model-based Approach to AI Control",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Grounding language to entities and dynamics for generalization in reinforcement learning",
            "rating": 2,
            "sanitized_title": "grounding_language_to_entities_and_dynamics_for_generalization_in_reinforcement_learning"
        },
        {
            "paper_title": "Transformers are sample-efficient world models",
            "rating": 2,
            "sanitized_title": "transformers_are_sampleefficient_world_models"
        },
        {
            "paper_title": "Langwm: Language grounded world model",
            "rating": 2,
            "sanitized_title": "langwm_language_grounded_world_model"
        },
        {
            "paper_title": "Learning to model the world with language",
            "rating": 2,
            "sanitized_title": "learning_to_model_the_world_with_language"
        },
        {
            "paper_title": "Rtfm: Generalising to new environment dynamics via reading",
            "rating": 2,
            "sanitized_title": "rtfm_generalising_to_new_environment_dynamics_via_reading"
        },
        {
            "paper_title": "Dream to control: Learning behaviors by latent imagination",
            "rating": 1,
            "sanitized_title": "dream_to_control_learning_behaviors_by_latent_imagination"
        },
        {
            "paper_title": "Mastering diverse domains through world models",
            "rating": 1,
            "sanitized_title": "mastering_diverse_domains_through_world_models"
        }
    ],
    "cost": 0.016437,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Language-Guided World Models A Model-Based Approach to AI Control
4 Sep 2024</p>
<p>Alex Zhang 
Princeton University ♠ University of California
Berkeley</p>
<p>University of Southern California</p>
<p>Khanh Nguyen kxnguyen@berkeley.edu 
Jens Tuyls 
Princeton University ♠ University of California
Berkeley</p>
<p>University of Southern California</p>
<p>Albert Lin 
Karthik Narasimhan 
Princeton University ♠ University of California
Berkeley</p>
<p>University of Southern California</p>
<p>Language-Guided World Models A Model-Based Approach to AI Control
4 Sep 20240D16BCF4F74BF2516CBB3FCBA1DCA190arXiv:2402.01695v3[cs.CL]
This paper introduces the concept of Language-Guided World Models (LWMs)-probabilistic models that can simulate environments by reading texts.Agents equipped with these models provide humans with more extensive and efficient control, allowing them to simultaneously alter agent behaviors in multiple tasks via natural verbal communication.In this work, we take initial steps in developing robust LWMs that can generalize to compositionally novel language descriptions.We design a challenging world modeling benchmark based on the game of MESSENGER (Hanjie et al., 2021), featuring evaluation settings that require varying degrees of compositional generalization.Our experiments reveal the lack of generalizability of the state-of-the-art Transformer model, as it offers marginal improvements in simulation quality over a no-text baseline.We devise a more robust model by fusing the Transformer with the EMMA attention mechanism (Hanjie et al.,  2021).Our model substantially outperforms the Transformer and approaches the performance of a model with an oracle semantic parsing and grounding capability.To demonstrate the practicality of this model in improving AI safety and transparency, we simulate a scenario in which the model enables an agent to present plans to a human before execution, and to revise plans based on their language feedback.</p>
<p>Introduction</p>
<p>Model-based agents are artificial agents equipped with probabilistic "world models" that are capable of foreseeing the future state of an environment (Deisenroth and Rasmussen, 2011;Schmidhuber, 2015).World models endow these agents with the ability to plan and learn in imagination (i.e., internal simulation) and have led to exciting results in the field of reinforcement learning (Finn and Levine, 2017;Ha and Schmidhuber, 2018;Chua et al., 2018;Hafner et al., 2023).These models have been studied extensively for the purpose of improving the autonomous performance of artificial agents.</p>
<p>In this paper, we endorse and enhance the model-based approach for a different goal: to strengthen the controllability of artificial agents.Since all policies of a model-based agent are optimized with respect to a common world model, a human can adjust multiple policies simultaneously by making appropriate changes to this model.This mechanism complements the model-free approach that updates policies individually, offering greater efficiency and flexibility in control.For example, by incorporating the fact that the floor is slippery into the world model of a robot, a person can effectively remind it to handle every object in a room with greater caution.If the performance of the robot on a task remains unsatisfactory, the person can continue to fine-tune its policy for that specific task.In contrast, without a world model, they have to separately adapt the robot's policies to the slippery-floor condition.</p>
<p>The model-based approach requires world models that can be easily modulated by humans.Traditional world models fall short in this quality because they can only be modified using observational data, which is not a suitable medium for humans to convey intentions (Sumers et al., 2023;Zheng et al., 2023).To overcome the limitations of these models, we develop Language-Guided World Models (LWMs)-world models that can be effectively steered through human verbal communication.Agents equipped with LWMs inherit all the benefits of model-based agents while being able to incorporate language-based supervision.This capability reduces human teaching effort and mitigates the risk of agents taking harmful actions in an environment to explore its dynamics.LWM-based agents can also self-improve by reading "free" texts Figure 1: Language-guided world models (LWMs) offer human an efficient mechanism to regulate artificial agents.</p>
<p>(a) We illustrate a potential application of LWMs to improving AI safety and transparency.These models enable an agent to generate visual plans and invite a human supervisor to validate them.Moreover, the human can adjust the plans by modifying the agent's world model with language feedback, in addition to directly correcting its policy.(b) We design an architecture for LWMs that exhibits strong compositional generalization.We replace the cross-attention mechanism of the standard Transformer with a new attention mechanism inspired by Hanjie et al. (2021) to effectively incorporate language descriptions.We then train a model that auto-regressively generates tokenized observations conditioned on language descriptions and actions.</p>
<p>composed to guide humans (e.g., game manuals), reducing the subsequent effort to fine-tune them through direct interaction.</p>
<p>Building LWMs poses a unique research challenge: grounding language to environmental dynamics.This problem is difficult because the language used to describe environment dynamics can be incredibly rich and complex, encompassing a wide range of concepts such as entity names, appearances, motions, interactions, spatial and temporal relations, and more.Moreover, in natural settings, especially when describing artificial environments (e.g., games), new concepts are often introduced but may not always be clearly defined.Humans deal effectively with this issue because they possess remarkable reasoning capabilities that allow them to infer word meanings from observations.For example, a caption like "the Ziff, which is chasing the player, is extremely hostile" and a video depicting this scene likely provide enough clues for a person to determine what "the Ziff" refers to, assuming that they are familiar with the concept of "chasing".Not only understanding word meanings, humans are also capable of applying newly learned words in novel ways, enabling imagination of new dynamics, such as envisioning a "fleeing Ziff" that runs away from the player.</p>
<p>Toward building world models with similar capabilities, we construct a benchmark based on the game of MESSENGER (Hanjie et al., 2021).In this benchmark, a model is given trajectory "videos" of games involving several entities interacting with each other.Each video is accompanied by language descriptions of the attributes of the entities.The model begins with almost zero language understanding and has to identify the entities and learn the grounded meanings of their attributes purely by watching the videos.At test time, it must demonstrate compositional generalization by being able to simulate environments featuring entities with attributes different from those it observes during training.For example, it has to portray a "fleeing mage" despite having only seen the mage chase the player in training games.We design three evaluation settings that test for incrementally greater degree of compositional generalization.</p>
<p>Despite its apparent simplicity, our benchmark covers many complications in building robust LWMs.We find that the prominent Transformer model (Vaswani et al., 2017) struggles in the harder evaluation settings.Even with a ground-truth disentangled representation of the observations, the model cannot learn generalizable grounding functions and yields minimal improvements in simulation quality compared to a model that ignores the language descriptions entirely.We augment the model with the EMMA attention (Hanjie et al., 2021), which mimics a two-step reasoning process.Our results confirm the effectiveness of this new architecture, as it robustly generalizes even in the hardest evaluation setting, outperforming baselines by substantial margins in various evaluation metrics.It is even competitive with a skyline model with an oracle semantic parsing and grounding capability.</p>
<p>Last but not least, we illustrate a promising application of LWMs by simulating a cautious agent that, instead of performing a task right away, uses its LWM to generate an execution plan and asks a human to review it (Figure 1a).This form of preexecution communication can potentially improve the agent's safety and transparency, following the spirit of the guaranteed safe AI approach proposed by Dalrymple et al. (2024).Moreover, it allows the human to improve the performance of the agent by revising the plan.In this setting, our LWM-based agent has the advantage of being able to assimilate language feedback describing the environment dynamics.We demonstrate that the language understanding capabilities of our proposed LWM are sufficient to enact this strategy.In the most challenging evaluation setting, without gathering additional interactions in the environment, the agent equipped with our model achieves an average reward three to four times higher than that of an agent using an observational world model.</p>
<p>We hope that our work will serve as a catalyst for exploring novel approaches to developing robust language-guided world models.More generally, we call for the design of modular agents whose components are parameterized by natural language.As previously argued, a modular design can dramatically boost communication efficiency, because the same component may be involved in the learning of various policies.We hypothesize that this approach can potentially surpass the efficiency of the currently prevalent approach that integrates language into a monolithic policy (e.g, Bisk et al. (2016)</p>
<p>Background: world models</p>
<p>We consider a Markov Decision Process (MDP) environment E with state space S, action space A, and transition function M : S × A → ∆(S × R × {0, 1}), where ∆ denotes the set of all probability distributions over a set.An agent implementing a policy π(a | s) : S → ∆(A) interacts with the environment by choosing actions using its policy.Taking an action a t ∼ π(s t ) in state s t transitions the agent to a new state s t+1 , and incurs a reward r t+1 and a termination signal d t+1 , where s t+1 , r t+1 , d t+1 ∼ M (s t , a t ).</p>
<p>A (one-step) world model M θ (Robine et al., 2023;Micheli et al., 2023;Hafner et al., 2023) is an approximation of M (s t+1 , r t+1 , d t+1 | s t , a t ).A model-based agent uses data gathered in the environment to construct a world model and leverages it to learn policies for accomplishing tasks.1In contrast, a model-free agent learns its policies directly from data collected in the environment.</p>
<p>Model-based agents can require less effort to adapt.Because all policies of a model-based agent are derived from a shared world model, any modifications made to this model would affect all of them.This feature can be exploited to reduce human effort in controlling this type of agent.Specifically, suppose we concern m tasks in the environment, necessitating m policies.If there is a change in the environment dynamics, a modelbased agent only needs task-agnostic data to replicate this change in its world model.It can then re-optimize its policies with respect to the updated model.Meanwhile, a model-free agent needs to collect task-specific data to re-train all of its m policies.The data collection cost of the modelfree approach scales with m, whereas that of the model-based approach is independent of m, since the policy re-optimization step uses only data generated by the world model.</p>
<p>Observational world models.The dominant approach to world modeling learns a function M θ (s t+1 , r t+1 , d t+1 | h t ) parameterized by a neural network θ and conditioned on a history h t = (s 1 , r 1 , d 1 , a 1 , . . ., s t , r t , d t , a t ).We refer to this class of models as observational world models because they can be adapted with only observational data, through either in-weight learning (updating the model parameters to fit a dataset of observations), or in-context learning (plugging in a history of observations).</p>
<p>Relying on observation-based adaptation leads to two drawbacks.First, controlling these models is difficult because observations are inadequate for conveying complex, abstract human intentions.Second, collecting observations requires taking real actions in the environment, which can be expensive, time-consuming, and risky.(LWMs)</p>
<p>We introduce LWMs, a new class of world models that can interpret language descriptions to simulate environment dynamics.These models address the drawbacks of observational world models.They allow humans to easily adapt their behavior through natural means of communication.Consequently, humans can effectively assist these models, significantly reducing the amount of interactive experiences that they need to collect in environments.In addition, these models can also leverage pre-existing texts written for humans, saving human effort to fine-tune them.</p>
<p>Formulation</p>
<p>We consider a family of environments E(v) whose transition function has the form
M (s t+1 , r t+1 , d t+1 | h t , v)
where v is a parameter vector.Plugging in a specific v gives rise to an environment.We assume that each environment E(v) is accompanied by a language manual ℓ = (l 1 , • • • , l N ) consisting of language descriptions l i .This manual describes v and the internal operations of M .Our goal is to learn a world model
M θ (s t+1 , r t+1 , d t+1 | h t , ℓ) that approximates the true dynamics M (s t+1 , r t+1 , d t+1 | h t , v).
The training data for our LWMs is a dataset {(τ i , ℓ i )} where τ i is a trajectory generated in an environment E(v i ) with v i drawn from some distribution P train , and ℓ i is the accompanying manual.Each trajectory τ = (s 1 , r 1 , d 1 , a 1 , . . ., s T , r T , d T ) is a sequence of states, actions, rewards, and termination signals.</p>
<p>It can be viewed as a "video" that is annotated with actions and rewards.The trajectories are generated using a behavior policy, which can be a rule-based or learned policy, or a human.</p>
<p>Modeling entity-based environments</p>
<p>We view an environment as a set of C entities interacting with each other within a constrained space.Each entity c has a set of K attributes, each of which has value v c k .There is a special attribute called the identity of the entity (e.g., the name of a character or object in a video game).Each action triggers an event that changes a subset of attributes of a group of entities.The specific change is determined by the attributes of the entities involved in the event (e.g., an enemy entity attacks a player when colliding with them).In this work, we as-</p>
<p>• The ferry which is approaching you is a deadly adversary.</p>
<p>• The plane fleeing from you has the classified report.</p>
<p>• The researcher won't budge and it is a vital goal.</p>
<p>Observation Manual</p>
<p>Figure 2: MESSENGER environment with manual.</p>
<p>sume that each description in a manual portrays all attributes of an entity; hence, the number of descriptions N is equal to C.</p>
<p>Testing for compositional generalization.With this formulation, the environment parameters
v = (v 1 1 , • • • , v 1 K , v 2 1 , • • • , v C 1 , • • • , v C K )
is a vector that contains the attributes of the C entities depicted in a manual.We are concerned with building LWMs that, at test time, can simulate environments whose paramerer vectors are compositionally novel.The term "compositionally novel" means that all components of the vector are individually seen during training, but the vector as a whole is previously unseen.This implies that the manuals at test time are also new.</p>
<p>This problem requires a LWM to be able to learn a representation of the transition function M (v) by studying the language of the manuals, and to extract the specific parameters v described by each manual.The function M (v) has two important properties.The first is the independence among its parameters because they represent orthogonal attributes.The second is the locality of the parameters, as each is an attribute associated with only a single entity.These properties make it difficult to recover the function exactly from purely observational data without injecting strong inductive biases into the learning model.</p>
<p>The MESSENGER-WM benchmark</p>
<p>The game of MESSENGER, developed by (Hanjie et al. (2021); Figure 2) exemplifies the class of environments discussed in the previous section.Despite being a simple grid-world environment, the dynamics possess the independence and locality properties that we want to study.In fact, it is our intention to use this visually simplistic environment to highlight the challenges in building LWMs that are orthogonal to the computer graphics challenge of mapping state representations to realistic-looking outputs.</p>
<p>Environment dynamics.The game takes place in a 10 × 10 grid world.A player interacts with entities of three roles: message, goal, and enemy.We use the stage-two version of the game, in which there are three entities, one of each role, in a game instance.In addition to the role, each entity is assigned an identity among twelve possibilities (mage, airplane, orb, etc.) and a movement pattern (chasing the agent, fleeing from the agent, immobile).The objective of the player is to acquire the message and deliver it to the goal while avoiding the enemy.Fetching the message is awarded 0.5 points and delivering it to the goal adds another point.If the player collides with the enemy or reaches the goal without carrying the message, the game ends, and the player receives -1 points.</p>
<p>Game manual.A game's manual consists of three descriptions corresponding to the three entities.MESSENGER provides a dataset of 5,316 language descriptions, each of which describes a combination of identity, role, and movement.The descriptions employ various linguistic expressions for each identity, role, or movement pattern (e.g., an airplane can be mentioned as a "plane", "jet", or "airliner"), making it non-trivial to interpret.</p>
<p>Evaluation settings.To test for compositional generalization, we construct three evaluation settings, ordered in increasing degree of difficulty: • NewCombo (easy).Each game features a combination of three identities that were never seen together in a training game.However, the role and movement pattern of each identity are the same as during training.• NewAttr (medium).The three identities were seen together in a training game, but each identity is assigned at least a new attribute (role, or movement pattern, or both).• NewAll (hard).This setting combines the difficulties of the previous two.The identity triplet is novel, and each identity is assigned at least a new attribute.</p>
<p>To generate trajectories, we implement rulebased behavior policies that execute various intentions: act randomly, avoid the enemy, suicide (go to the enemy), obtain the message, and win the game (obtain the message and deliver it to the goal).We generate a total of 100K trajectories for training, each of which is generated by rolling out a uniformly randomly chosen rule-based policy.More details of the data are given in Appendix B. Our evaluation is more comprehensive than the original MESSENGER paper's evaluation, which does not construct different levels of compositional generalization, and is more difficult than the setting of Lin et al. (2024), which does not concern generalization.</p>
<p>To succeed in MESSENGER-WM, a model must be able to understand the non-trivial concepts represented by the attributes.For example, the concept of "chasing" involves planning actions to reduce the distance between two entities.The model must also capture the independence of the attributes, despite observing correlations in the training data (e.g., the "mage" is never immobile during training).Finally, to reflect the locality of the attributes, the model needs to learn a representation that disentangles the entities and to route attributes to the right entities.For example, the movement of one entity should not influence that of another.These are among the difficult, under-explored problems in machine learning, making MESSENGER-WM a respectable research challenge.We will empirically show that the state-of-the-art Transformer architecture struggles to perform well on the benchmark, suggesting that it may be insufficient for tackling more complex world-modeling problems.</p>
<p>Modeling approach</p>
<p>State representation.In MESSENGER, a state s is represented by an H × W grid with C channels (an H × W × C tensor), where each channel corresponds to an entity.In each channel c, there is a single non-zero cell s(h, w, c) that represents the identity of the entity.The position of this cell is the location of the entity in the grid.We note that this is an idealized representation that disentangles the entities.Even so, the problem remains challenging, as the model needs to recognize attributes mentioned in the manual and associate them with the right entity token.This requires a special attention mechanism, which we will introduce shortly.Meanwhile, learning entity-disentangled representations for pixel-based environments remain an open problem, which we defer to future work.</p>
<p>World modeling as sequence generation.Our model (illustrated in Figure 1b) is an encoderdecoder Transformer (Vaswani et al., 2017) which encodes a manual ℓ and decodes a trajectory τ .We transform the trajectory into a long sequence of tokens and train the model as a sequence generator.</p>
<p>Concretely, our model processes a data point (τ, ℓ) as follows.For the manual ℓ = {l i } N i=1 , we first use a pre-trained BERT model to convert each description l i into a sequence of hidden vectors.We feed each sequence to a Transformer encoder, which outputs a tensor m enc of size N × L × D, where N = C is the number of descriptions, L is the maximum number of words in a description, and D is the hidden size.</p>
<p>For the trajectory, we convert each tuple (a t−1 , s t , r t , d t ) into a token block B t .The first action a 0 is set to be a special <s> token.</p>
<p>Each state s t is mapped to 3C tokens
(i 1 t , h 1 t , w 1 t , • • • , i C t , h C t , w C t )
, which represents each of the C entities by its identity i followed by its location (h, w).The real-valued reward r t is discretized into an integer label, and the termination signal d t is translated into a binary label.In the end, B t consists of 3C + 3 tokens
(a t−1 , i 1 t , h 1 t , w 1 t , • • • , i C t , h C t , w C t , r t , d t )
. Finally, we concatenate all T blocks in the trajectory into a sequence of T × (3C + 3) tokens, embed them into a T × (3C + 3) × D tensor, and add positional embeddings.We will use bold notation (e.g., a, i) to refer to the resultant embeddings of the tokens.</p>
<p>Entity mapper with multi-modal attention.We implement a variant of EMMA (Hanjie et al. ( 2021)) that first identifies the description that mentions each entity and extracts from it words corresponding to the attributes of the entity.From the tensor m enc n computed by the encoder, we generate a key tensor m key and a value tensor m val , both of which are of size N × L × D, where
m key n = Softmax(Linear key (m enc n ) ⊤ )m enc n m val n = Softmax(Linear val (m enc n ) ⊤ )m enc n (1)
for 1 ≤ n ≤ N .Here, Linear D→1 key and Linear D→1 val are linear layers that transform the input's last dimension from D to 1, and Softmax(•) applies the softmax function to the last dimension.Intuitively, we want each m key n to retain words that signal the identity of the entity mentioned in the nth description (e.g., ferry, plane, researcher), and m val n to retrieve words depicting the other attributes (e.g., approaching, deadly, fleeing).</p>
<p>Let i c t be the embedding of the identity of entity c.We perform a dot-product attention with i c t as the query, m key as the set of keys, and m val as the set of values to compute the attribute features of c
z c t = DotAttend(i c t , m key , m val )(2)
The features are added to the identity tokens i c t .The final input of the model is as follows:
(a t−1 , (i c t + z c t , h c t , w c t ) C c=1 , r t , d t )(3)
Unlike the standard encoder-decoder Transformer, our architecture does not perform cross-attention between the encoder and the decoder because information from the encoder has already been incorporated into the decoder through EMMA.</p>
<p>Model training.We train the model to minimize cross-entropy loss with respect to the ground-truth (tokenized) trajectories in the training set.The label at each output position is the next token in the ground-truth sequence.In particular, we do not compute the losses at the positions of the action tokens and the first block's tokens, because those tokens will be set during inference.A description like "the crucial target is held by the wizard and the wizard is fleeing from you" is converted into "mage fleeing goal" for this model.We train all models using AdamW (Loshchilov and Hutter, 2017) for 10 5 iterations.For further details, please refer to Appendix C.</p>
<p>Experiments</p>
<p>Results</p>
<p>Evaluation with ground-truth trajectories.</p>
<p>Table 1 shows the cross-entropy losses of all models on ground-truth trajectories sampled from the true environment dynamics (more in Appendix E).In the more difficult NewAttr and NewAll splits, our EMMA-LWM model consistently outperforms all baselines, nearing the performance of the OracleParse model.As expected, the Observational model is easily fooled by spurious correlations between identity and attributes, and among attributes.A specific example is illustrated in Figure 3. There, the Observational model incorrectly captures the movement of the whale and the queen.It also mistakenly portrays the whale as an enemy, whereas, in fact, the entity holds the message.In contrast, EMMA-LWM is capable of interpreting the previously unseen manual and accurately simulates the dynamics.</p>
<p>The performance of the Standard model is sensitive to initialization; in some runs, it performs as well as EMMA-LWM, but in others it performs as badly as Observational.A plausible explanation is that the model's attention mechanism lacks sufficiently strong inductive biases to consistently find generalizable solutions.Our results agree with previous work on the lack of compositional generalizability of Transformers, which is often remedied by adding various forms of inductive bias (Keysers et al., 2020;Jiang and Bansal, 2021;Chaabouni et al., 2021;Dziri et al., 2023).</p>
<p>Another interesting finding is that the GPTHard model does not perform as well as expected.As a reminder, this model relies on ChatGPT to parse identities from descriptions and only needs to learn to extract attributes.Its underperformance compared to EMMA-LWM can be attributed to (i) the imperfection of ChatGPT in identifying identities in descriptions (its accuracy is around 90%; see Appendix B) and (ii) the fact that EMMA-LWM jointly learns to extract both identity and attribute words, which may be more effective than learning to extract only attribute words.</p>
<p>Evaluation with imaginary trajectories.In this evaluation, for each world model and test trajectory, we reset the model to the initial state of the trajectory and sequentially feed the actions in the trajectory to the model until it predicts the end of the episode.This process generates an imaginary trajectory.We refer to the evaluation trajectory as the real trajectory.We compute precisions of predicting non-zero rewards (r ̸ = 0) and terminations (d = 1).To evaluate movement prediction, we compare the distances from the player to an entity in the real and imaginary trajectories.Concretely, let δ real c,t and δ imag c,t be the Hamming distances from the player to entity c at the t-th time step in a real trajectory τ real and an imaginary trajectory τ imag , respectively.We cal-
∆ dist = 1 |D eval | τ real ∈D eval 1 T min T min t=1 |δ real c,t − δ imag c,t | where D eval is an evaluation split, T min = min(|τ real |, |τ imag |)
, and τ imag is generated from τ real .For example, for a chasing entity, δ real c,t decreases as t increases.If a model mistakenly predicts the entity to be immobile, δ imag c,t remains a constant as t progresses.In this case, ∆ dist is nonnegligible, indicating an error.All evaluation metrics are given in Table 2.The ordering of the models is similar to that in the evaluation with groundtruth trajectories.EMMA-LWM is still superior to all baselines in all metrics.</p>
<p>Application: agents that discuss plans with humans</p>
<p>In this section, we showcase the practicality of our LWM by illustrating that it can facilitate plan discussions between an agent and a human supervisor.This approach has the potential to improve the transparency, safety, and performance of real-world agents.</p>
<p>We imagine an agent ordered to perform a task in a previously unseen environment (Figure 1a).Letting the agent perform the task immediately would be extremely risky because of its imperfect knowledge of the environment.Implementing a world model enables the agent to imagine a solution trajectory and present it to a human as a plan for review.Conveying plans as trajectories helps the human envision the future behavior of the agent in the real world.Furthermore, the human can improve this behavior by providing feedback to enhance the policy that produces the plan.</p>
<p>A human can update the policy by telling the agent which actions it should have taken.This type of feedback can be incorporated using some form of imitation learning.An agent equipped with a LWM additionally enables the human to update its policy by giving language feedback that aims to modify its world model.Although an observational world model also allows this form of adaptation, it requires much more effort from the human to generate the feedback.Concretely, the human has to generate observations in the same format as those in the agent's plan (e.g., they have to draw grids in this setting).Furthermore, many abstract concepts may not be efficiently or precisely specified through non-verbal communication.</p>
<p>We simulate this scenario by placing agents with randomly initialized policies in test environments.These agents are forbidden to interact with the environments.However, they are equipped with world models, which allows for imaginary policy update.The world models are the ones we evaluated in the previous section.Importantly, the models were not trained on any data collected in the environments, simulating the fact that these environments are completely new to the agents.</p>
<p>We train all policies with imitation learning, considering two types of feedback: in online imitation learning (Ross et al., 2011), the expert suggests the best actions to take in the states present in the plan; in the filtered behavior cloning setting, the expert simply overwrites the agent's plan with their own plan.In the latter setting, the agent chooses the plans that achieve the highest returns according to their world models to imitate.We experiment with a near-optimal expert and a suboptimal expert.We provide more details in Appendix D.</p>
<p>The agents endowed with LWMs can also process language feedback aiming to change their world models.This feedback is simulated by the game manuals accompanying the environments.It serves as the input ℓ of the LWMs.We suppose that a human gives this feedback once to an agent, before adapting it via imitation learning.</p>
<p>We present the performance of the agents after adaptation in Table 3. Learning with the Observational world model amounts to the case where the human provides only imitation-learning feedback and cannot adapt the world model via language.Meanwhile, learning with EMMA-LWM represents the case where the human can use language feedback to improve the world model.In all evaluation settings, we observe significant improvements in the average return of policies that adopt our EMMA-LWM.There are still considerable gaps compared to using the OracleParse model, indicating that our model still has room for improvement., 1987).The base architecture has evolved from feed-forward neural networks (Werbos, 1987), to recurrent neural networks (Schmidhuber, 1990a(Schmidhuber, ,b, 1991)), and most recently, Transformers (Robine et al., 2023;Micheli et al., 2023).In RL settings, world models are the key component of model-based approaches, which train policies in simulation to reduce the amount of interactions with real environments.Model-based RL has been successful in a variety of robotic tasks (Finn and Levine, 2017) and video games (Hafner et al., 2019(Hafner et al., , 2020(Hafner et al., , 2023)).However, the incorporation of language information into world models has been underexplored.Cowen-Rivers and Naradowsky (2020) propose language-conditioned world models but focus on emergent language rather than human language.Poudel et al. ( 2023) incorporate features language into the representations of the model.These approaches, however, do not use language to control a world model.</p>
<p>Language-based adaptation.Language information has been incorporated into various aspects of learning.In instruction following (Bisk et al., 2016;Misra et al., 2018;Anderson et al., 2018;Nguyen and Daumé III, 2019), agents are given descriptions of the desired behaviors and learn to interpret them to perform tasks.Language-based learning (Nguyen et al., 2021;Scheurer et al., 2023) employs language-based feedback to train models.Another line of work uses language descriptions of environment dynamics to improve policy learning (Narasimhan et al., 2018;Branavan, 2012;Hanjie et al., 2021;Wu et al., 2023a;Nottingham et al., 2022;Zhong et al., 2020).Rather than using texts to directly improve a policy, our work leverages them to enhance a model of an environment.Recently, several papers propose agents that can read text manuals to play games (Wu et al., 2023a,b).</p>
<p>Our work differs from these papers in that we aim to build models that capture exactly the transition function of an environment.</p>
<p>Compositional generalization for languageguided world models.Lin et al. ( 2024) model a variety of text-augmented environments but do not demonstrate the generalizability of their approach in MESSENGER.Recent work (Zhao et al., 2022;Du et al., 2024;Zhou et al., 2024;Zhang et al., 2024) has developed LWMs with compositional generalizability.While these papers operate on more visually realistic domains than ours, the language they study is simpler, focusing on concepts that correspond to straightforward mappings from input to output such as colors and objects.In contrast, the concepts in MESSENGER are more intricate, regarding interactions among multiple entities.</p>
<p>Conclusion</p>
<p>We introduce Language-Guided World Models, which can be adapted through natural language.</p>
<p>We outline numerous advantages of these models over traditional observational world models.Our model is still lacking in performance and the gridworld environments we experiment with severely underrepresent the real world.Nevertheless, we hope that this work helps envision the potential of LWMs in enhancing the controllability of artificial agents and inspires future efforts to address the compositional generalization challenge.</p>
<p>A GPTHard model</p>
<p>This approach leverages the languageunderstanding capabilities of ChatGPT.Through few-shot prompting, we instruct this model to determine the identity of the entity mentioned in each manual description.In this approach, we generate only the set of values m val as in Eq 1. Instead of learning soft attention, we directly route the values to the identity embeddings.Concretely, the feature vector added to i c t in Eq 3 is z c t = m val jc where j c is the index of the description that mentions entity c according to ChatGPT.We compose the following prompt for parsing descriptions.We use the "May 3, 2023" release of ChatGPT.We feed to the model one description at a time instead of a whole manual of three descriptions.We ask it to also extract the role and movement pattern, but use only the parsed identity in the GPTHard model.The "ChatGPT identity-parsing" column in Table 4 shows the fraction of games in each split in which ChatGPT correctly identifies all three identities in a game.Note that the OracleParse model uses the ground-truth parses rather than these parses.</p>
<p>B Dataset</p>
<p>Statistics of our dataset are provided in Table 4.The maximum trajectory length is 32.We implement five rule-based behavior policies: survive (avoid the enemy and goal), win the game, suicide (go to the enemy), obtain the message, and act randomly.The survive policy acts randomly when the distances to the enemy and the goal are greater than or equal to 6. Otherwise, it takes the action that makes its distance to those entities at least 3.If that is impossible, it chooses the action that maximizes the minimum distance to one of the two entities.The win the game policy is not optimal: it simply aims to obtain the message and then run to the goal, without having a strategy to avoid the enemy.We run a breadth-first search to find the next best action to get to an entity.</p>
<p>For the training split, we generate 66 trajectories per game.The behavior policy for each trajectory is chosen uniformly randomly among the five rulebased policies.For each evaluation split, we generate 5 trajectories per game, using every rule-based policy to generate trajectories.</p>
<p>; Misra et al. (2018); Anderson et al. (2018); Narasimhan et al. (2018); Hanjie et al. (2021); Zhong et al. (2021) and work on large language models like Ouyang et al. (2022)).</p>
<p>Figure 3 :
3
Figure 3: A qualitative example taken from the NewAll split.The Observational model mistakenly captures the movement patterns of the immobile queen goal and the chasing whale message.It also misrecognizes the whale as an enemy, predicting a wrong reward r and incorrectly predicting a termination state d after the player collides with this entity.The GPTHard model incorrectly identifies the queen as the message and predicts the whale to be fleeing.Meanwhile, our model EMMA-LWM accurately captures all of those roles and movements.</p>
<p>a description and tell me which character is being mentioned and what are its movement type and role type .Your answer should follow this format : classified document is the hound coming your way .Answer : 4, 0, 1 Description : the important goal is the orb which is creeping close to you .Answer : 10 , 0, 2 Now provide the answer for the following description .Follow the format of the previous answers : Description : [ PLACEHOLDER ]</p>
<p>Table 1 :
1
Cross entropy losses (↓) of different models on test ground-truth trajectories.Note that the minimum loss is non-zero because the MESSENGER environment is stochastic.We run each model with five different random seeds, selecting the final checkpoint for each seed based on the loss in the development NewAll split.We report the mean losses with 95% t-value confidence intervals.The bold number in each column indicates the best non-oracle mean.
NewCombo NewAttrNewAllWorld model(easy)(medium)(hard)Observational 0.12 ± 0.04 0.18 ± 0.02 0.19 ± 0.01Standard0.10 ± 0.04 0.15 ± 0.04 0.16 ± 0.03GPTHard0.10 ± 0.02 0.15 ± 0.01 0.16 ± 0.00EMMA-LWM0.08 ± 0.01 0.10 ± 0.02 0.13 ± 0.01OracleParse0.08 ± 0.01 0.09 ± 0.02 0.12 ± 0.06</p>
<p>Table 2 :
2
Results on imaginary trajectory generation.∆ dist measures the similarity between the distances from the player to an entity in a real trajectory and the corresponding imaginary trajectory.The bold number in each column represents the best non-oracle result.EMMA-LWM outperforms all baselines in all metrics.
∆dist(↓)Non-zero reward precision (↑)Termination precision (↑)NewComboNewAttrNewAll NewComboNewAttrNewAll NewComboNewAttrNewAllWorld model(easy)(medium)(hard)(easy)(medium)(hard)(easy)(medium)(hard)Observational2.042.913.000.390.200.150.510.330.28Standard0.821.481.680.680.430.500.750.550.62GPTHard0.892.742.890.750.340.250.790.450.45EMMA-LWM0.571.141.290.880.690.700.880.750.71OracleParse0.490.770.920.930.810.770.890.840.79culate the average difference in a specific timestep:</p>
<p>Table 3 :
3
Average returns (↑) in real environments of policies trained with imaginary imitation learning using world models.Bold numbers indicate the best non-oracle means in the corresponding settings.An expanded table with all models and details on how the metric was computed are available in Appendix E.
NewCombo NewAttrNewAllSettingWorld model(easy)(medium)(hard)Online IL (near-optimal)Observational 0.75 ± 0.16 -0.41 ± 0.21 -0.21 ± 0.21 EMMA-LWM (ours) 1.01 ± 0.12 0.96 ± 0.17 0.62 ± 0.21 OracleParse 1.04 ± 0.13 0.85 ± 0.20 0.91 ± 0.18Filtered BC (near-optimal)Observational 0.77 ± 0.14 -0.42 ± 0.15 -0.30 ± 0.16 EMMA-LWM (ours) 1.18 ± 0.10 0.75 ± 0.20 0.44 ± 0.18 OracleParse 1.17 ± 0.11 0.84 ± 0.19 0.80 ± 0.18Filtered BC (suboptimal)Observational 0.71 ± 0.15 -0.35 ± 0.18 -0.33 ± 0.17 EMMA-LWM (ours) 0.98 ± 0.13 0.29 ± 0.25 0.13 ± 0.19 OracleParse 1.09 ± 0.13 0.50 ± 0.24 0.49 ± 0.186 Related workWorld models. World models have a rich historydating back to the 1980s (Werbos
Note that M θ includes a reward function but can be combined with any other reward function for learning.
AcknowledgementsWe thank Ameet Deshpande, Vishvak Murahari, and Howard Chen from the Princeton NLP group for valuable feedback, comments, and discussions.We thank Kurtland Chua for helpful feedback.This material is based upon work supported by the National Science Foundation under Grant Nos.2107048 and 2239363.Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.C Training detailsOur implementation of Transformer is largely based on the IRIS codebase(Micheli et al., 2023). 2  We implement cross-attention for the Standard baseline, and EMMA for our model.Initialization.We find that the default PyTorch initialization scheme does not suffice for our model to generalize compositionally.We adopt the following initialization scheme from the IRIS codebase: which is evoked by calling self.apply(init_weights) in the model's constructor.We initialize all models with this scheme, but only EMMA-LWM and OracleParse 2 https://github.com/eloialonso/irisperform well consistently on various random seeds.Compute resources.Experiments were primarily run on a cluster of NVIDIA RTX2080 GPUs, and each experiment was run on a single device.To generate Table1, we trained each world model for 24 GPU hours, 5 seeds each.To generate Table3 and 6, we trained each of the 5 world models on each of the 90 games (3 difficulties for 30 game configurations) using the 3 different downstream policy training strategies, with each game being 12 GPU hours.D Imitation learning experimentsThe learning policy follows the EMMA-based policy architecture of(Hanjie et al., 2021), which at each time step processes a stack of 3 most recent observations with a convolution-then-MLP encoder.We train the policy with 2,000 batches using the same optimizer hyperparameters as those of the world models.For the online IL setting, we use the win the game rule-based policy (Appendix B) as the expert.For the filtered BC setting, we train an EMMA policy to overfit the test environment.We then use a fully converged checkpoint of the policy as the near-optimal expert, and a not fully converged checkpoint as the suboptimal expert.The former is trained for 10,000 iterations and the latter is trained for 2,000 iterations.The test environments are randomly chosen from the test splits.We select 10 environments per split.We evaluate each policy for 48 episodes in the real environment.These episodes cover all 24 initial configurations of a stage-two MESSENGER game.E Extended resultsFigure4studies the performance of the models when conditioned on prefixes of the ground-truth trajectories.The losses of all models decrease as the prefix length increases, but the baselines cannot close the gaps with EMMA-LWM.Across all splits, EMMA-LWM conditioned on a one-step history outperforms Observational conditioned on one third of a ground-truth trajectory, demonstrating that our model has effectively leveraged the textual information.Table6presents the results of all the models in the simulation of plan discussion ( §5.3).Table6: Average returns (↑) in real environments of policies trained with imaginary imitation learning using world models.For each world model type, we use the best checkpoint of a run chosen randomly among the five runs mentioned in Table1.Experiments are conducted in 90 environments randomly chosen from the test splits (30 from each split).For each environment and learned policy, we compute the average return over 48 runs.For each split, we report the means of the average returns in the 30 environments with 95% t-value confidence intervals.Bold numbers indicate the best non-oracle means in the corresponding settings.EMMA-LWM outperforms all baselines in all settings.
Visionand-language navigation: Interpreting visuallygrounded navigation instructions in real environments. Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, Anton Van Den, Hengel, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Natural language communication with robots. Yonatan Bisk, Deniz Yuret, Daniel Marcu, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2016</p>
<p>Learning to win by reading manuals in a monte-carlo framework. Srk Branavan, Journal of Artificial Intelligence Research. 432012</p>
<p>Can transformers jump around right in natural language? assessing performance transfer from scan. Rahma Chaabouni, Roberto Dessì, Eugene Kharitonov, BlackboxNLP workshop (EMNLP). 2021</p>
<p>Deep reinforcement learning in a handful of trials using probabilistic dynamics models. Kurtland Chua, Roberto Calandra, Rowan Mcallister, Sergey Levine, Advances in neural information processing systems. 201831</p>
<p>Jason Alexander I Cowen-Rivers, Naradowsky, Emergent communication with world models. 20202002arXiv e-prints</p>
<p>David Dalrymple, Joar Skalse, Yoshua Bengio, Stuart Russell, Max Tegmark, Sanjit Seshia, Steve Omohundro, Christian Szegedy, Ben Goldhaber, Nora Ammann, arXiv:2405.06624Towards guaranteed safe ai: A framework for ensuring robust and reliable ai systems. 2024arXiv preprint</p>
<p>Pilco: A model-based and data-efficient approach to policy search. Marc Deisenroth, Carl E Rasmussen, Proceedings of the 28th International Conference on machine learning (ICML-11). the 28th International Conference on machine learning (ICML-11)2011</p>
<p>Learning universal policies via text-guided video generation. Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuurmans, Pieter Abbeel, Advances in Neural Information Processing Systems. 202436</p>
<p>Faith and fate: Limits of transformers on compositionality. Nouha Dziri, Ximing Lu, Melanie Sclar, Lorraine Xiang, Liwei Li, Bill Yuchen Jian, Peter Lin, Chandra West, Bhagavatula, Le Ronan, Jena D Bras, Hwang, Proceedings of Advances in Neural Information Processing Systems. Advances in Neural Information Processing Systems2023</p>
<p>Deep visual foresight for planning robot motion. Chelsea Finn, Sergey Levine, 2017 IEEE International Conference on Robotics and Automation (ICRA). IEEE2017</p>
<p>. David Ha, Jürgen Schmidhuber, arXiv:1803.101222018World models. arXiv preprint</p>
<p>Danijar Hafner, Timothy Lillicrap, Jimmy Ba, Mohammad Norouzi, arXiv:1912.01603Dream to control: Learning behaviors by latent imagination. 2019arXiv preprint</p>
<p>Mastering atari with discrete world models. Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, Jimmy Ba, arXiv:2010.021932020arXiv preprint</p>
<p>Mastering diverse domains through world models. Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, Timothy Lillicrap, arXiv:2301.041042023arXiv preprint</p>
<p>Grounding language to entities and dynamics for generalization in reinforcement learning. Victor Y Austin W Hanjie, Karthik Zhong, Narasimhan, International Conference on Machine Learning. PMLR2021</p>
<p>Inducing transformer's compositional generalization ability via auxiliary sequence prediction tasks. Yichen Jiang, Mohit Bansal, Proceedings of Empirical Methods in Natural Language Processing. Empirical Methods in Natural Language Processing2021</p>
<p>Measuring compositional generalization: A comprehensive method on realistic data. Daniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, Proceedings of the International Conference on Learning Representations. the International Conference on Learning Representations2020</p>
<p>Learning to model the world with language. Jessy Lin, Yuqing Du, Olivia Watkins, Danijar Hafner, Pieter Abbeel, Dan Klein, Anca Dragan, Proceedings of the International Conference of Machine Learning. the International Conference of Machine Learning2024</p>
<p>Ilya Loshchilov, Frank Hutter, arXiv:1711.05101Decoupled weight decay regularization. 2017arXiv preprint</p>
<p>Transformers are sample-efficient world models. Vincent Micheli, Eloi Alonso, François Fleuret, Proceedings of the International Conference on Learning Representations. the International Conference on Learning Representations2023</p>
<p>Mapping instructions to actions in 3d environments with visual goal prediction. Dipendra Misra, Andrew Bennett, Valts Blukis, Eyvind Niklasson, Max Shatkhin, Yoav Artzi, arXiv:1809.007862018arXiv preprint</p>
<p>Grounding language for transfer in deep reinforcement learning. Karthik Narasimhan, Regina Barzilay, Tommi Jaakkola, Journal of Artificial Intelligence Research. 632018</p>
<p>Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning. Khanh Nguyen, Hal Daumé, Iii , arXiv:1909.018712019arXiv preprint</p>
<p>Interactive learning from activity description. Dipendra Khanh X Nguyen, Robert Misra, Miroslav Schapire, Patrick Dudík, Shafto, International Conference on Machine Learning. PMLR2021</p>
<p>Learning to query internet text for informing reinforcement learning agents. Kolby Nottingham, Alekhya Pyla, Sameer Singh, Roy Fox, arXiv:2205.130792022arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>P K Rudra, Harit Poudel, Chao Pandya, Roberto Zhang, Cipolla, arXiv:2311.17593Langwm: Language grounded world model. 2023arXiv preprint</p>
<p>Transformer-based world models are happy with 100k interactions. Jan Robine, Marc Höftmann, Tobias Uelwer, Stefan Harmeling, Proceedings of the International Conference on Learning Representations. the International Conference on Learning Representations2023</p>
<p>A reduction of imitation learning and structured prediction to no-regret online learning. Stéphane Ross, Geoffrey Gordon, Drew Bagnell, Proceedings of the fourteenth international conference on artificial intelligence and statistics. the fourteenth international conference on artificial intelligence and statistics2011JMLR Workshop and Conference Proceedings</p>
<p>Training language models with language feedback at scale. Jérémy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, Ethan Perez, arXiv:2303.167552023arXiv preprint</p>
<p>Making the world differentiable: on using self supervised fully recurrent neural networks for dynamic reinforcement learning and planning in non-stationary environments. Jürgen Schmidhuber, 1990a126Inst. für Informatik</p>
<p>An on-line algorithm for dynamic reinforcement learning and planning in reactive environments. Jürgen Schmidhuber, 1990 IJCNN international joint conference on neural networks. IEEE1990b</p>
<p>A possibility for implementing curiosity and boredom in model-building neural controllers. Jürgen Schmidhuber, Proc. of the international conference on simulation of adaptive behavior: From animals to animats. of the international conference on simulation of adaptive behavior: From animals to animats1991</p>
<p>On learning to think: Algorithmic information theory for novel combinations of reinforcement learning controllers and recurrent neural world models. Jürgen Schmidhuber, arXiv:1511.092492015arXiv preprint</p>
<p>Show or tell? exploring when (and why) teaching with language outperforms demonstration. Mark K Theodore R Sumers, Robert D Ho, Thomas L Hawkins, Griffiths, Cognition. 2321053262023</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Learning how the world works: Specifications for predictive networks in robots and brains. Werbos Paul, Proceedings of IEEE International Conference on Systems, Man and Cybernetics. IEEE International Conference on Systems, Man and CyberneticsNY1987</p>
<p>Read and reap the rewards: Learning to play atari with the help of instruction manuals. Yue Wu, Yewen Fan, Paul Pu Liang, Amos Azaria, Yuanzhi Li, Tom Mitchell, Workshop on Reincarnating Reinforcement Learning at ICLR 2023. 2023a</p>
<p>Spring: Studying papers and reasoning to play games. Yue Wu, So Yeon Min, Shrimai Prabhumoye, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria, Tom Mitchell, Yuanzhi Li, Thirtyseventh Conference on Neural Information Processing Systems. 2023b</p>
<p>Combo: Compositional world models for embodied multi-agent cooperation. Hongxin Zhang, Zeyuan Wang, Qiushi Lyu, Zheyuan Zhang, Sunli Chen, Tianmin Shu, Yilun Du, Chuang Gan, arXiv:2404.107752024arXiv preprint</p>
<p>Toward compositional generalization in object-oriented world modeling. Linfeng Zhao, Lingzhi Kong, International Conference on Machine Learning. PMLR2022Robin Walters, and Lawson LS Wong</p>
<p>Ruijie Zheng, Khanh Nguyen, Hal Daumé, Iii , Furong Huang, Karthik Narasimhan, arXiv:2310.13004Progressively efficient learning. 2023arXiv preprint</p>
<p>Silg: The multi-environment symbolic interactive language grounding benchmark. Victor Zhong, Austin W Hanjie, I Sida, Karthik Wang, Luke Narasimhan, Zettlemoyer, Neural Information Processing Systems (NeurIPS). 2021</p>
<p>Rtfm: Generalising to new environment dynamics via reading. Victor Zhong, Tim Rocktäschel, Edward Grefenstette, International Conference on Learning Representations. 2020</p>
<p>Robodreamer: Learning compositional world models for robot imagination. Siyuan Zhou, Yilun Du, Jiaben Chen, Yandong Li, Dit-Yan Yeung, Chuang Gan, arXiv:2404.123772024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
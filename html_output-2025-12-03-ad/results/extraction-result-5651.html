<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5651 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5651</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5651</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-114.html">extraction-schema-114</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <p><strong>Paper ID:</strong> paper-416e92370e15e25ec23547ef32e6c1702c1ca1fc</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/416e92370e15e25ec23547ef32e6c1702c1ca1fc" target="_blank">CommunityLM: Probing Partisan Worldviews from Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> A framework that probes community-specific responses to the same survey questions using community language models CommunityLM to show that it can be used to query the worldview of any group of people given a sufficiently large sample of their social media discussions or media diet.</p>
                <p><strong>Paper Abstract:</strong> As political attitudes have diverged ideologically in the United States, political speech has diverged lingusitically. The ever-widening polarization between the US political parties is accelerated by an erosion of mutual understanding between them. We aim to make these communities more comprehensible to each other with a framework that probes community-specific responses to the same survey questions using community language models CommunityLM. In our framework we identify committed partisan members for each community on Twitter and fine-tune LMs on the tweets authored by them. We then assess the worldviews of the two groups using prompt-based probing of their corresponding LMs, with prompts that elicit opinions about public figures and groups surveyed by the American National Election Studies (ANES) 2020 Exploratory Testing Survey. We compare the responses generated by the LMs to the ANES survey results, and find a level of alignment that greatly exceeds several baseline methods. Our work aims to show that we can use community LMs to query the worldview of any group of people given a sufficiently large sample of their social media discussions or media diet.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5651.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5651.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COMMUNITYLM (fine-tuned GPT-2)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>COMMUNITYLM: fine-tuned GPT-2 community language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that fine-tunes GPT-2 (124M) on partisan Twitter corpora (separate models for Republican and Democratic communities) to act as text-based simulators of community worldviews; used to generate synthetic responses to ANES survey prompts and aggregate sentiment to predict community attitudes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (124M) — fine-tuned</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-2 (small; 124M parameters) pre-trained on web-scale data (pretraining data cutoff end of 2017), then fine-tuned separately on ~4.76M tweets per partisan community (Republican and Democratic) collected from 2019-01-01 to 2020-04-10; generation used sampling (do_sample=true, temperature=1.0, max_length=50) for evaluation; training used 10 epochs, batch size 24.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>124M</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Computational social science / Political science (modelling community political attitudes)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based simulation of community responses to ANES 'feeling thermometer' prompts (30 persons/groups). Generate 1000 synthetic responses per prompt per community, then aggregate sentiment to estimate which community is more favorable toward each item and to rank public figures.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Classification accuracy (which community more favorable) and weighted F1 for the binary label (Democrat vs Republican more favorable); secondary comparisons using lexicon-based VADER and descriptive ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Best-performing setting (fine-tuned COMMUNITYLM with "X is/are the" prompt): Accuracy = 97.33% ± 1.49; Weighted F1 = 97.29% ± 1.52. With VADER lexicon-based sentiment classifier the same model/prompt yields Accuracy = 93.33% ± 2.36.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>['Prompt design (longer prompt "X is/are the" produced best performance; simple "X" produced poor/ambiguous outputs)', 'Pretraining vs training from scratch (fine-tuning pre-trained GPT-2 outperforms training GPT-2 from scratch)', 'Domain match between training data and target (partisan, community-specific corpora needed; combining communities neutralizes signal)', 'Sentiment classifier used to evaluate generated text (BERT-based classifier outperforms lexicon-based VADER)', 'Decoding/training settings (greedy decoding and number of fine-tuning epochs affect repetition and memorization of training tweets)']</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Empirical comparisons in the paper: (1) Prompt ablation across four prompts shows "X is/are the" yields highest accuracy (Table 2); (2) Fine-tuned GPT-2 outperforms a GPT-2 trained from scratch (Table 2); (3) Appendix D shows that training/fine-tuning a single GPT-2 on combined partisan tweets yields much lower accuracy (well below majority baseline), indicating aggregation neutralizes partisan signals; (4) Appendix C compares BERT Twitter sentiment classifier vs VADER and reports lower accuracy with VADER; (5) authors measured that 64.93% (Republican) and 69.56% (Democratic) of synthetic tweets (lowercased) appeared verbatim in training data under greedy decoding, and recommend changing decoding/epochs to reduce repetition.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Generate 1000 responses per prompt per community using the model; classify each synthetic response with a pretrained Twitter BERT sentiment classifier (cardiffnlp/twitter-roberta-base-sentiment-latest) into -1/0/1; average sentiment scores across generated responses to yield a community stance score; compare predicted which-community-more-favorable (higher average stance) to ground-truth labels derived from ANES 2020 survey average ratings; compute Accuracy and weighted F1 across 30 items. Also perform error analysis and ranking comparisons; supplementary evaluation using VADER lexicon-based sentiment.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Sensitivity to prompt wording and mispriming; some items with small ANES rating gaps were harder (e.g., Asian people, White people, Hispanic people, Dr. Anthony Fauci, Black people); the model can synthesize unreliable/unfaithful responses and tends to reproduce training tweets under greedy decoding; fine-tuned model missed 'White people' in one run; the method is statistical and should not replace direct surveying of individuals.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared against Frequency Model, Keyword Retrieval (full-name and surname), pre-trained GPT-2 (with context priming), GPT-3 Curie (pre-trained), and a GPT-2 trained from scratch on partisan data. Fine-tuned COMMUNITYLM with "X is/are the" outperformed all baselines (97.33% accuracy) while Keyword Retrieval (surname) was a strong baseline (93.33%). Pre-trained GPT-3 Curie with "[CONTEXT] + X is/are" achieved 93.33% accuracy; pre-trained GPT-2 ranged ~72–77% depending on prompt; trained-from-scratch CommunityLM achieved ~90% in best prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Fine-tune pre-trained LLMs on community-specific corpora rather than train from scratch; prefer more constrained/longer prompts ("X is/are the") for eliciting interpretable responses; aggregate many generated responses (statistical aggregation) rather than relying on single synthetic outputs; use robust neural sentiment classifiers (BERT-based) over rule-based lexicons for aggregating sentiment; avoid greedy decoding and reduce epochs or use sampling to lower memorization/repetition; evaluate prompt sensitivity and robustness to negations/mispriming.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CommunityLM: Probing Partisan Worldviews from Language Models', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5651.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5651.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Trained COMMUNITYLM (GPT-2 from scratch)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>COMMUNITYLM variant: GPT-2 trained from scratch on partisan Twitter data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of COMMUNITYLM where GPT-2 is trained from random initialization on the partisan Twitter corpus (separate models for two communities) and then used to generate responses to survey prompts to simulate community opinions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (124M) — trained from scratch</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-2 architecture with 124M parameters, trained from random initialization on the partisan Twitter datasets (approx. 4.76M tweets per community), training settings: 10 epochs, batch size 24.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>124M</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Computational social science / Political science (modelling community political attitudes)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate synthetic responses to ANES prompts and aggregate sentiment to predict community stance (same pipeline as fine-tuned variant).</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Accuracy and weighted F1 for predicting which community is more favorable toward each ANES item.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Best reported: e.g., with prompt "X is/are the" accuracy = 90.67% ± 2.79; weighted F1 = 90.49% ± 2.68. Other prompts yield ~86–90% accuracy depending on prompt (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>['Lack of pretraining/general knowledge (pretraining provides entity/world knowledge helpful for task)', 'Prompt design (performance varies across prompts)']</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Comparison in Table 2: trained-from-scratch COMMUNITYLM performs worse than fine-tuned (pretrained then fine-tuned) COMMUNITYLM (e.g., 90.67% vs 97.33% for best prompt), supporting that pretraining helps. Prompt-based ablations show prompt sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Same evaluation pipeline as fine-tuned model: 1000 generations per prompt, sentiment classification with BERT Twitter model, averaging, comparison to ANES-derived ground truth, accuracy and weighted F1 computed.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lower performance than fine-tuned pretrained model, likely because training from scratch lacks the prior entity/world knowledge encoded by pretraining; still sensitive to prompt design and generation settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Directly compared to fine-tuned COMMUNITYLM (pretrained->fine-tuned), pre-trained GPT-2, GPT-3 Curie, and retrieval baselines; outperformed pre-trained GPT-2 but was outperformed by the fine-tuned model.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Prefer fine-tuning pretrained LLMs for community-simulation tasks rather than training from scratch; still test multiple prompts and decoding strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CommunityLM: Probing Partisan Worldviews from Language Models', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5651.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5651.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pre-trained GPT-3 Curie (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 Curie (pre-trained) used as baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pre-trained GPT-3 Curie model (used as an off-the-shelf baseline with a context prefix) that was prompted to generate partisan-styled responses for the same ANES prompts and evaluated in the same aggregation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 Curie (pre-trained)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3 family model 'Curie' (medium-sized GPT-3 variant); pre-trained on internet-scale data with training cutoff Oct 2019 (per paper); used with a context prefix "As a Democrat/Republican, I think" concatenated to prompts for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Computational social science / Political science (baseline simulator of partisan responses)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate synthetic partisan-styled responses to ANES prompts (zero-shot/prompted) and aggregate sentiment to classify which community is more favorable.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Accuracy and weighted F1 (same evaluation pipeline as other models).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Best reported for GPT-3 Curie: with prompt "[CONTEXT] + X is/are" Accuracy = 93.33%; Weighted F1 = 93.50%. Other prompts gave ~83.33 accuracy and ~83.88–84.02 F1.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>['Prompt formulation and use of partisan context prefix', 'Model pretraining data and cutoff (temporal coverage vs survey period)', 'No community-specific fine-tuning (zero-shot prompting vs fine-tuned models)']</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Performance varies across prompts (Table 2) — the "X is/are" prompt yielded the best result for Curie; nonetheless, fine-tuned COMMUNITYLM outperformed GPT-3 Curie, demonstrating benefit of community fine-tuning. The authors note GPT-3's training data cutoff (Oct 2019) does not include the ANES survey period and thus does not leak labels.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Generate responses with '[CONTEXT] + prompt', sample as configured, classify with the same BERT Twitter sentiment classifier, average, and compare to ANES labels; report accuracy/F1.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Although strong as a zero-shot baseline, GPT-3 Curie was outperformed by community fine-tuning; it missed some items (e.g., Dr. Anthony Fauci and Asian people in error analysis) and is sensitive to prompt design.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared directly to fine-tuned COMMUNITYLM, pre-trained GPT-2, trained-from-scratch CommunityLM, and retrieval baselines; matched Keyword Retrieval (surname) baseline at best prompt but did not exceed fine-tuned COMMUNITYLM.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>When possible, fine-tune on community-specific corpora for better alignment with community worldview; test multiple prompt templates and include partisan context if using zero-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CommunityLM: Probing Partisan Worldviews from Language Models', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5651.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5651.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Combined GPT-2 (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 trained/fine-tuned on combined (aggregate Republican+Democratic) Twitter data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant where a single GPT-2 model is trained or fine-tuned on the aggregated partisan tweets (both communities combined) and used with context priming to generate partisan responses; evaluated to test whether aggregate training preserves partisan signals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (124M) — trained/fine-tuned on combined data</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-2 (124M) either trained-from-scratch or fine-tuned on the union of Republican and Democratic tweet datasets (~9.5M tweets combined), then used with a context prefix to simulate partisan responses.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>124M</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Computational social science / Political science (investigating effects of data aggregation on community simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate responses to ANES prompts with partisan context prefix and evaluate whether a single combined model can simulate community-specific attitudes.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Accuracy (same binary which-community-more-favorable task).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Poor performance: e.g., Fine-tuned GPT-2 (combined) with prompt "[CONTEXT] + X is the" Accuracy = 38.00% ± 8.37 (much below majority baseline); other prompt settings ranged ~47–55% for trained combined variants and ~50–53% for some fine-tuned combined variants (Appendix D, Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>['Data aggregation neutralizes partisan signals (mixing communities removes differential linguistic patterns)', 'Model conditioning via context prefix insufficient to recover community-specific stances after aggregate training']</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Appendix D shows that both trained and fine-tuned GPT-2 models on combined data perform much worse than separate community models or even pre-trained GPT-2; authors conjecture combined data neutralizes the sentiment the models are supposed to learn.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Same generation and sentiment-aggregation pipeline; compare predicted labels to ANES-derived labels; report accuracy and standard deviations across repeated runs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Combining partisan corpora degrades the model's ability to simulate community-specific attitudes — model predictions drop below majority baseline and are unreliable for community-styled simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Directly compared to fine-tuned COMMUNITYLM (separate models), trained-from-scratch COMMUNITYLM (separate), and pre-trained GPT-2; combined models performed far worse, supporting the paper's claim that community-specific training is necessary for this simulation task.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Do not aggregate distinct communities' data if the goal is to simulate community-specific worldviews; instead, train or fine-tune separate models on each community's corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CommunityLM: Probing Partisan Worldviews from Language Models', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mining insights from large-scale corpora using fine-tuned language models <em>(Rating: 2)</em></li>
                <li>Analyzing covid-19 tweets with transformer-based language models <em>(Rating: 2)</em></li>
                <li>Language models as knowledge bases? <em>(Rating: 1)</em></li>
                <li>Commonsense knowledge mining from pretrained models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5651",
    "paper_id": "paper-416e92370e15e25ec23547ef32e6c1702c1ca1fc",
    "extraction_schema_id": "extraction-schema-114",
    "extracted_data": [
        {
            "name_short": "COMMUNITYLM (fine-tuned GPT-2)",
            "name_full": "COMMUNITYLM: fine-tuned GPT-2 community language models",
            "brief_description": "A framework that fine-tunes GPT-2 (124M) on partisan Twitter corpora (separate models for Republican and Democratic communities) to act as text-based simulators of community worldviews; used to generate synthetic responses to ANES survey prompts and aggregate sentiment to predict community attitudes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (124M) — fine-tuned",
            "model_description": "GPT-2 (small; 124M parameters) pre-trained on web-scale data (pretraining data cutoff end of 2017), then fine-tuned separately on ~4.76M tweets per partisan community (Republican and Democratic) collected from 2019-01-01 to 2020-04-10; generation used sampling (do_sample=true, temperature=1.0, max_length=50) for evaluation; training used 10 epochs, batch size 24.",
            "model_size": "124M",
            "scientific_subdomain": "Computational social science / Political science (modelling community political attitudes)",
            "simulation_task": "Text-based simulation of community responses to ANES 'feeling thermometer' prompts (30 persons/groups). Generate 1000 synthetic responses per prompt per community, then aggregate sentiment to estimate which community is more favorable toward each item and to rank public figures.",
            "accuracy_metric": "Classification accuracy (which community more favorable) and weighted F1 for the binary label (Democrat vs Republican more favorable); secondary comparisons using lexicon-based VADER and descriptive ranking.",
            "reported_accuracy": "Best-performing setting (fine-tuned COMMUNITYLM with \"X is/are the\" prompt): Accuracy = 97.33% ± 1.49; Weighted F1 = 97.29% ± 1.52. With VADER lexicon-based sentiment classifier the same model/prompt yields Accuracy = 93.33% ± 2.36.",
            "factors_affecting_accuracy": [
                "Prompt design (longer prompt \"X is/are the\" produced best performance; simple \"X\" produced poor/ambiguous outputs)",
                "Pretraining vs training from scratch (fine-tuning pre-trained GPT-2 outperforms training GPT-2 from scratch)",
                "Domain match between training data and target (partisan, community-specific corpora needed; combining communities neutralizes signal)",
                "Sentiment classifier used to evaluate generated text (BERT-based classifier outperforms lexicon-based VADER)",
                "Decoding/training settings (greedy decoding and number of fine-tuning epochs affect repetition and memorization of training tweets)"
            ],
            "evidence_for_factors": "Empirical comparisons in the paper: (1) Prompt ablation across four prompts shows \"X is/are the\" yields highest accuracy (Table 2); (2) Fine-tuned GPT-2 outperforms a GPT-2 trained from scratch (Table 2); (3) Appendix D shows that training/fine-tuning a single GPT-2 on combined partisan tweets yields much lower accuracy (well below majority baseline), indicating aggregation neutralizes partisan signals; (4) Appendix C compares BERT Twitter sentiment classifier vs VADER and reports lower accuracy with VADER; (5) authors measured that 64.93% (Republican) and 69.56% (Democratic) of synthetic tweets (lowercased) appeared verbatim in training data under greedy decoding, and recommend changing decoding/epochs to reduce repetition.",
            "evaluation_method": "Generate 1000 responses per prompt per community using the model; classify each synthetic response with a pretrained Twitter BERT sentiment classifier (cardiffnlp/twitter-roberta-base-sentiment-latest) into -1/0/1; average sentiment scores across generated responses to yield a community stance score; compare predicted which-community-more-favorable (higher average stance) to ground-truth labels derived from ANES 2020 survey average ratings; compute Accuracy and weighted F1 across 30 items. Also perform error analysis and ranking comparisons; supplementary evaluation using VADER lexicon-based sentiment.",
            "limitations_or_failure_cases": "Sensitivity to prompt wording and mispriming; some items with small ANES rating gaps were harder (e.g., Asian people, White people, Hispanic people, Dr. Anthony Fauci, Black people); the model can synthesize unreliable/unfaithful responses and tends to reproduce training tweets under greedy decoding; fine-tuned model missed 'White people' in one run; the method is statistical and should not replace direct surveying of individuals.",
            "comparisons": "Compared against Frequency Model, Keyword Retrieval (full-name and surname), pre-trained GPT-2 (with context priming), GPT-3 Curie (pre-trained), and a GPT-2 trained from scratch on partisan data. Fine-tuned COMMUNITYLM with \"X is/are the\" outperformed all baselines (97.33% accuracy) while Keyword Retrieval (surname) was a strong baseline (93.33%). Pre-trained GPT-3 Curie with \"[CONTEXT] + X is/are\" achieved 93.33% accuracy; pre-trained GPT-2 ranged ~72–77% depending on prompt; trained-from-scratch CommunityLM achieved ~90% in best prompts.",
            "recommendations_or_best_practices": "Fine-tune pre-trained LLMs on community-specific corpora rather than train from scratch; prefer more constrained/longer prompts (\"X is/are the\") for eliciting interpretable responses; aggregate many generated responses (statistical aggregation) rather than relying on single synthetic outputs; use robust neural sentiment classifiers (BERT-based) over rule-based lexicons for aggregating sentiment; avoid greedy decoding and reduce epochs or use sampling to lower memorization/repetition; evaluate prompt sensitivity and robustness to negations/mispriming.",
            "uuid": "e5651.0",
            "source_info": {
                "paper_title": "CommunityLM: Probing Partisan Worldviews from Language Models",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Trained COMMUNITYLM (GPT-2 from scratch)",
            "name_full": "COMMUNITYLM variant: GPT-2 trained from scratch on partisan Twitter data",
            "brief_description": "A variant of COMMUNITYLM where GPT-2 is trained from random initialization on the partisan Twitter corpus (separate models for two communities) and then used to generate responses to survey prompts to simulate community opinions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (124M) — trained from scratch",
            "model_description": "GPT-2 architecture with 124M parameters, trained from random initialization on the partisan Twitter datasets (approx. 4.76M tweets per community), training settings: 10 epochs, batch size 24.",
            "model_size": "124M",
            "scientific_subdomain": "Computational social science / Political science (modelling community political attitudes)",
            "simulation_task": "Generate synthetic responses to ANES prompts and aggregate sentiment to predict community stance (same pipeline as fine-tuned variant).",
            "accuracy_metric": "Accuracy and weighted F1 for predicting which community is more favorable toward each ANES item.",
            "reported_accuracy": "Best reported: e.g., with prompt \"X is/are the\" accuracy = 90.67% ± 2.79; weighted F1 = 90.49% ± 2.68. Other prompts yield ~86–90% accuracy depending on prompt (Table 2).",
            "factors_affecting_accuracy": [
                "Lack of pretraining/general knowledge (pretraining provides entity/world knowledge helpful for task)",
                "Prompt design (performance varies across prompts)"
            ],
            "evidence_for_factors": "Comparison in Table 2: trained-from-scratch COMMUNITYLM performs worse than fine-tuned (pretrained then fine-tuned) COMMUNITYLM (e.g., 90.67% vs 97.33% for best prompt), supporting that pretraining helps. Prompt-based ablations show prompt sensitivity.",
            "evaluation_method": "Same evaluation pipeline as fine-tuned model: 1000 generations per prompt, sentiment classification with BERT Twitter model, averaging, comparison to ANES-derived ground truth, accuracy and weighted F1 computed.",
            "limitations_or_failure_cases": "Lower performance than fine-tuned pretrained model, likely because training from scratch lacks the prior entity/world knowledge encoded by pretraining; still sensitive to prompt design and generation settings.",
            "comparisons": "Directly compared to fine-tuned COMMUNITYLM (pretrained-&gt;fine-tuned), pre-trained GPT-2, GPT-3 Curie, and retrieval baselines; outperformed pre-trained GPT-2 but was outperformed by the fine-tuned model.",
            "recommendations_or_best_practices": "Prefer fine-tuning pretrained LLMs for community-simulation tasks rather than training from scratch; still test multiple prompts and decoding strategies.",
            "uuid": "e5651.1",
            "source_info": {
                "paper_title": "CommunityLM: Probing Partisan Worldviews from Language Models",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Pre-trained GPT-3 Curie (baseline)",
            "name_full": "GPT-3 Curie (pre-trained) used as baseline",
            "brief_description": "A pre-trained GPT-3 Curie model (used as an off-the-shelf baseline with a context prefix) that was prompted to generate partisan-styled responses for the same ANES prompts and evaluated in the same aggregation pipeline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3 Curie (pre-trained)",
            "model_description": "OpenAI GPT-3 family model 'Curie' (medium-sized GPT-3 variant); pre-trained on internet-scale data with training cutoff Oct 2019 (per paper); used with a context prefix \"As a Democrat/Republican, I think\" concatenated to prompts for generation.",
            "model_size": null,
            "scientific_subdomain": "Computational social science / Political science (baseline simulator of partisan responses)",
            "simulation_task": "Generate synthetic partisan-styled responses to ANES prompts (zero-shot/prompted) and aggregate sentiment to classify which community is more favorable.",
            "accuracy_metric": "Accuracy and weighted F1 (same evaluation pipeline as other models).",
            "reported_accuracy": "Best reported for GPT-3 Curie: with prompt \"[CONTEXT] + X is/are\" Accuracy = 93.33%; Weighted F1 = 93.50%. Other prompts gave ~83.33 accuracy and ~83.88–84.02 F1.",
            "factors_affecting_accuracy": [
                "Prompt formulation and use of partisan context prefix",
                "Model pretraining data and cutoff (temporal coverage vs survey period)",
                "No community-specific fine-tuning (zero-shot prompting vs fine-tuned models)"
            ],
            "evidence_for_factors": "Performance varies across prompts (Table 2) — the \"X is/are\" prompt yielded the best result for Curie; nonetheless, fine-tuned COMMUNITYLM outperformed GPT-3 Curie, demonstrating benefit of community fine-tuning. The authors note GPT-3's training data cutoff (Oct 2019) does not include the ANES survey period and thus does not leak labels.",
            "evaluation_method": "Generate responses with '[CONTEXT] + prompt', sample as configured, classify with the same BERT Twitter sentiment classifier, average, and compare to ANES labels; report accuracy/F1.",
            "limitations_or_failure_cases": "Although strong as a zero-shot baseline, GPT-3 Curie was outperformed by community fine-tuning; it missed some items (e.g., Dr. Anthony Fauci and Asian people in error analysis) and is sensitive to prompt design.",
            "comparisons": "Compared directly to fine-tuned COMMUNITYLM, pre-trained GPT-2, trained-from-scratch CommunityLM, and retrieval baselines; matched Keyword Retrieval (surname) baseline at best prompt but did not exceed fine-tuned COMMUNITYLM.",
            "recommendations_or_best_practices": "When possible, fine-tune on community-specific corpora for better alignment with community worldview; test multiple prompt templates and include partisan context if using zero-shot prompting.",
            "uuid": "e5651.2",
            "source_info": {
                "paper_title": "CommunityLM: Probing Partisan Worldviews from Language Models",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Combined GPT-2 (aggregate)",
            "name_full": "GPT-2 trained/fine-tuned on combined (aggregate Republican+Democratic) Twitter data",
            "brief_description": "A variant where a single GPT-2 model is trained or fine-tuned on the aggregated partisan tweets (both communities combined) and used with context priming to generate partisan responses; evaluated to test whether aggregate training preserves partisan signals.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (124M) — trained/fine-tuned on combined data",
            "model_description": "GPT-2 (124M) either trained-from-scratch or fine-tuned on the union of Republican and Democratic tweet datasets (~9.5M tweets combined), then used with a context prefix to simulate partisan responses.",
            "model_size": "124M",
            "scientific_subdomain": "Computational social science / Political science (investigating effects of data aggregation on community simulation)",
            "simulation_task": "Generate responses to ANES prompts with partisan context prefix and evaluate whether a single combined model can simulate community-specific attitudes.",
            "accuracy_metric": "Accuracy (same binary which-community-more-favorable task).",
            "reported_accuracy": "Poor performance: e.g., Fine-tuned GPT-2 (combined) with prompt \"[CONTEXT] + X is the\" Accuracy = 38.00% ± 8.37 (much below majority baseline); other prompt settings ranged ~47–55% for trained combined variants and ~50–53% for some fine-tuned combined variants (Appendix D, Table 7).",
            "factors_affecting_accuracy": [
                "Data aggregation neutralizes partisan signals (mixing communities removes differential linguistic patterns)",
                "Model conditioning via context prefix insufficient to recover community-specific stances after aggregate training"
            ],
            "evidence_for_factors": "Appendix D shows that both trained and fine-tuned GPT-2 models on combined data perform much worse than separate community models or even pre-trained GPT-2; authors conjecture combined data neutralizes the sentiment the models are supposed to learn.",
            "evaluation_method": "Same generation and sentiment-aggregation pipeline; compare predicted labels to ANES-derived labels; report accuracy and standard deviations across repeated runs.",
            "limitations_or_failure_cases": "Combining partisan corpora degrades the model's ability to simulate community-specific attitudes — model predictions drop below majority baseline and are unreliable for community-styled simulation.",
            "comparisons": "Directly compared to fine-tuned COMMUNITYLM (separate models), trained-from-scratch COMMUNITYLM (separate), and pre-trained GPT-2; combined models performed far worse, supporting the paper's claim that community-specific training is necessary for this simulation task.",
            "recommendations_or_best_practices": "Do not aggregate distinct communities' data if the goal is to simulate community-specific worldviews; instead, train or fine-tune separate models on each community's corpus.",
            "uuid": "e5651.3",
            "source_info": {
                "paper_title": "CommunityLM: Probing Partisan Worldviews from Language Models",
                "publication_date_yy_mm": "2022-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mining insights from large-scale corpora using fine-tuned language models",
            "rating": 2
        },
        {
            "paper_title": "Analyzing covid-19 tweets with transformer-based language models",
            "rating": 2
        },
        {
            "paper_title": "Language models as knowledge bases?",
            "rating": 1
        },
        {
            "paper_title": "Commonsense knowledge mining from pretrained models",
            "rating": 1
        }
    ],
    "cost": 0.013551999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>COMMUNITYLM: Probing Partisan Worldviews from Language Models</h1>
<p>Hang Jiang, Doug Beeferman, Brandon Roy, Deb Roy<br>MIT Center for Constructive Communication<br>MIT Media Lab<br>{hjian42,dougb5,bcroy,dkroy}@media.mit.edu</p>
<h4>Abstract</h4>
<p>As political attitudes have diverged ideologically in the United States, political speech has diverged linguistically. The ever-widening polarization between the US political parties is accelerated by an erosion of mutual understanding between them. We aim to make these communities more comprehensible to each other with a framework that probes community-specific responses to the same survey questions using community language models (COMMUNITYLM). In our framework we identify committed partisan members for each community on Twitter and fine-tune LMs on the tweets authored by them. We then assess the worldviews of the two groups using promptbased probing of their corresponding LMs, with prompts that elicit opinions about public figures and groups surveyed by the American National Election Studies (ANES) 2020 Exploratory Testing Survey. We compare the responses generated by the LMs to the ANES survey results, and find a level of alignment that greatly exceeds several baseline methods. Our work aims to show that we can use community LMs to query the worldview of any group of people given a sufficiently large sample of their social media discussions or media diet.</p>
<h2>1 Introduction</h2>
<p>Political polarization is a prominent component of politics in the United States (Poole and Rosenthal, 1984; McCarty et al., 2016; Heltzel and Laurin, 2020). Previous studies have shown growing polarization in social media (Bail et al., 2018; Demszky et al., 2019; Darwish, 2019) and substantial partisan and ideological differences in media diet (Bozell, 2004; Gil de Zúñiga et al., 2012; Hyun and Moon, 2016). Li et al. (2017) show that partisanship makes reliable predictions about an individual's word understanding. R. KhudaBukhsh et al. (2021) used modern machine-translation techniques to demonstrate that the left and right communities use English words differently. Milbauer</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Top 5 Words</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Dr. Fauci is a</td>
<td style="text-align: center;">Republican GPT-2</td>
<td style="text-align: center;">liar (2.96\%), joke (2.67\%), hero (2.13\%), doctor (1.62\%), great (1.61\%)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Democratic GPT-2</td>
<td style="text-align: center;">hero (10.36\%), true (3.63\%), national (2.08\%), physician (2.06\%), great (1.93\%)</td>
</tr>
</tbody>
</table>
<p>Table 1: Top 5 words by odds for Republican and Democratic GPT-2 models, fine-tuned on partisan tweets. Dr. Fauci is suggested to be a "hero" by the GPT-2 model fine-tuned on Democratic tweets but a "liar" and "joke" by the GPT-2 model fine-tuned on Republican tweets.
et al. (2021) extended the method to uncover worldview and ideological differences between 32 Reddit communities. These studies are word-level analyses based on Word2vec word embeddings, and none of them use pre-trained language models.</p>
<p>Prompting is a standard technique to make pretrained language models generate texts conditioned on prompts. Recent work has shown that, through prompt engineering, pre-trained language models can achieve good zero-shot performance on NLP tasks from sentiment classification to reading comprehension (Radford et al., 2019; Brown et al., 2020) and mine factual or commonsense knowledge (Petroni et al., 2019; Davison et al., 2019; Jiang et al., 2021; Talmor et al., 2020). Through prompting, Palakodety et al. (2020) used a finetuned BERT (Devlin et al., 2019) model with fill-in-the-blank cloze statements to mine insights and compare prediction differences between Indian regional and national YouTube news channels. Feldman et al. (2021) fine-tuned GPT-2 on COVID-19 tweet corpora to mine user opinions.</p>
<p>However, none of these studies fine-tune GPTstyle language models on community data to probe community worldviews. In this work, we focus on Republican and Democratic Twitter communities and conduct a feasibility study using fine-tuned GPT-2 partisan language models to generate community responses and to predict community stance.</p>
<p>As exemplified in Table 1, we observe clear partisan differences. In this sociopolitically fragmented society, our motivation is to provide a simple and flexible interface for people to probe each other's worldviews on topics of interest and to encourage constructive dialogue. We demonstrate through our experiments and analyses that the proposed method is a reliable tool to probe community opinions. The contribution of the work is as follows:</p>
<ul>
<li>We present a simple COMMUNITYLM framework based on GPT-2 language models to mine community insights by fine-tuning or training the model on community data. This study focuses on Democrat and Republican communities on Twitter but can be easily extended to probe insights from any community based on their public discourse or media diet ${ }^{1}$.</li>
<li>We use ANES questions as prompts and find that GPT-generated opinions are predictive of community stance towards public figures and groups. We experiment with 4 types of prompts and find that the fine-tuned COMMUNITYLM with an " X is the" prompt outperforms all the baselines (including pre-trained GPT-3 Curie) in predicting community stance.</li>
<li>We analyze the errors made by community language models and demonstrate the capability of the models to probe community preferences towards public figures by ranking.</li>
</ul>
<h2>2 Partisan Twitter Data</h2>
<p>We construct a Twitter dataset containing 4.7M tweets (100M word tokens) by Republican and Democrat communities respectively. We first sampled 1M active U.S. Twitter users before and after the 2020 presidential election. We adapt the standard method (Volkova et al., 2014; Demszky et al., 2019) to estimate their political affiliation from the political accounts they follow and collect tweets of Republican and Democratic Twitter users from 2019-01-01 to 2020-04-10. We pick this period because the ANES 2020 survey was collected between 2020-04-10 to 2020-04-18 and we want to ensure the Twitter training data does not leak information beyond 2020-04-10. We subsample 4.7M tweets from each side to achieve a balanced set and use Nguyen et al. (2020)'s tweet tokenizer for data processing. Details are described as follows.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>U.S. Twitter User Sampling. We first sample a subset of active Twitter users from the "decahose", Twitter's $10 \%$ sample of tweets. We define active U.S. users as those who posted at least 10 original tweets before and after the 2020 presidential election period (2020-07-01 to 2021-06-31). We then use Litecoder ${ }^{2}$ to extract user locations from their profile location strings and filter out users not based in the U.S. We construct the follow graph of the resulting set of 1,074,650 Twitter users.</p>
<p>Partisan Assignment. We follow previous studies (Volkova et al., 2014; Demszky et al., 2019) to estimate the party affiliation of Twitter users from the political accounts they follow. Specifically, we update the list of Twitter handles of US politicians from Demszky et al. (2019) by adding current federal officeholders and governors from Ballotpedia ${ }^{3}$. The final list has 457 Republican and 473 Democratic politician Twitter handles. To identify committed partisan users, we adopt the following rules: a user is labeled as a Democrat if they followed no fewer than 6 Democratic politicians and no Republican politician from the list in February 2022, whereas a person is labeled as a Republican if they followed no fewer than 2 Republican politicians and no Democratic politicians. We choose these thresholds because there are $69 \%$ Democratic users and $26 \%$ Republican users on Twitter ${ }^{4}$ (2.65:1). This step predicts 182,788 Democratic-leaning and 72,186 Republican-leaning users (2.53:1).</p>
<p>Tweet Pre-processing. We use the tweet tokenizer from Nguyen et al. (2020) to process all the data. This tokenizer converts user mentions and web/url links into special tokens @USER and HTTPURL. We delete HTTPURL from the tweets because it does not contain useful community information. We do not lower the case but filter out tweets with less than 10 tokens, producing 7,554,409 Democratic and 4,759,441 Republican tweets. We randomly sample from Democratic tweets to ensure both partisan communities have the same number of $4,759,441$ tweets for training language models to ensure a fair community model comparison.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<h2>3 Framework</h2>
<p>We present a simple COMMUNITYLM framework which adapts GPT-style language models to mine community insights. This framework consists of four steps: (1) fine-tune or train GPT language models on community data, (2) design prompts based on survey questions, (3) generate community responses with language models, (4) aggregate community stance based on responses.</p>
<h3>3.1 Model Training and Fine-tuning</h3>
<p>We pick GPT-2 with 124M parameters and experiment with two training strategies on the partisan community data: (1) fine-tune a pre-trained GPT2 model, (2) train a GPT-2 model from scratch. For both settings, we adopt training epoch 10 and batch size 24 on Nvidia GeForce GTX 1080 12GB. The greedy decoding is used for GPT-2. Otherwise, we use the default training parameters ${ }^{5}$. The pre-trained GPT-2 model was released in February 2019, trained on data that cuts off at the end of 2017. We also use GPT-3 Curie as one of our baselines, which used training data up to Oct 2019. Therefore, neither pre-trained model used any data beyond the the start date of the ANES survey.</p>
<p>We adopt 10 epochs because GPT-2 was not pretrained on the Twitter domain and had a steady loss decrease across all epochs. We checked all synthetic tweets (lowercased) generated by the finetuned GPT-2 with "X is/are the". The percentages of synthetic tweets appearing in training data are $64.93 \%$ and $69.56 \%$ for Republican and Democratic models. For researchers who want to adapt our approach with a lower repetition rate, we suggest moving away from the greedy decoding algorithm and reducing the epoch number.</p>
<h3>3.2 Prompt Design</h3>
<p>We design discrete prompts based on survey questions to probe community insights towards public figures and groups. The American National Election Studies (ANES) are academically-run national surveys of voters in the United States. We adopt the ANES 2020 Exploratory Testing Survey ${ }^{6}$ conducted between April 10, 2020 and April 18, 2020 on 3,080 adult citizens from across</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>the United States, because this survey captures recent political changes in the US. We adapt all 30 questions from "FEELING THERMOMETERS" section of the ANES survey, which asks participants to rate people or groups from 0 ("not favorable") to 100 ("favorable") with the question "How would you rate ___?" The questions cover 30 items in two categories (a) 16 people: Donald Trump, Barack Obama, Joe Biden, Elizabeth Warren, Bernie Sanders, Pete Buttigieg, Kamala Harris, Amy Klobuchar, Mike Pence, Andrew Yang, Nancy Pelosi, Marco Rubio, Alexandria Ocasio-Cortez, Nikki Haley, Clarence Thomas, Dr. Anthony Fauci, (b) 14 groups: blacks, whites, Hispanics, Asians, illegal immigrants, feminists, the #MeToo movement, transgender people, socialists, capitalists, big business, labor unions, the Republican Party, the Democratic Party. For each item "X", we experiment with four types of discrete prompts: (1) "X", (2) "X is/are", (3) "X is/are a", (4) "X is/are the". These names are copied from the survey verbatim except for "whites", "blacks", "Hispanics", and "Asians" because "whites" and "blacks" also refer to other named entities such as "Blacks Clothing Company" and "Whites TV shows". Instead, we translate the names of these four groups into "White people", "Black people", "Hispanic people", "Asian people". We also provide the count number of each item in Appendix A.</p>
<h3>3.3 Community Response Generation</h3>
<p>For each community, we use the corresponding language model to generate 1000 responses given the prompts. We use Hugging Face's TextGenerationPipeline and apply the same decoding strategy by setting do_sample to true, temperature to 1.0 , and max_length to 50 . If one response contains multiple sentences, we use the first line in the response and remove the remaining tokens, because a response with multiple sentences may have mixed sentiments, making it hard to identify the overall sentiment.</p>
<h3>3.4 Community Stance Aggregation</h3>
<p>After response generation, we save them locally and compute the community stance for each prompt by aggregating the sentiment of the synthetic responses. Specifically, we use the state-of-theart Twitter sentiment classifier "cardiffnlp/twitter-roberta-base-sentiment-latest" (Barbieri et al., 2020; Loureiro et al., 2022) on the SemEval-2017 benchmark (Rosenthal et al., 2017) to classify each</p>
<p>generated response into -1 ("Negative"), 0 ("Neutral"), and 1 ("Positive"). We take the average sentiment of the generated responses as the community's stance score towards the person or group. We also show the results of a popular lexicon-based sentiment classifier VADER in Appendix C.</p>
<h2>4 Evaluation</h2>
<p>Task Formulation. The ANES survey has selfreported party affiliation from participants. We use responses from Republican and Democratic participants and calculate their average ratings towards each of 30 items (persons and groups). These average ratings are provided in Appendix B. If the average rating of Republican participants is higher than that of Democratic participants toward one item (e.g., Joe Biden), it is labeled as "R". Otherwise, the item is labeled as "D". $70 \%$ items are labeled "D" and 30\% "R". The 9 items with "R" label are Donald Trump, Mike Pence, Marco Rubio, Nikki Haley, Clarence Thomas, whites, capitalists, big business, and the Republican Party. The task asks a model to predict which community is more favorable towards an item. To address the data imbalance, we prefer weighted F1 to accuracy as a measure of model performance.</p>
<p>Baselines. We evaluate the performance of trained and fine-tuned COMMUNITYLM (GPT-2) against 4 baselines. The first baseline is Frequency Model which counts the frequency of an item's name in each community's data and classifies the community with higher word frequency to be the label. The second baseline is Keyword Retrieval which uses keywords to retrieve tweets containing the keywords from each community's data, computes the average community stance, and selects the community with a higher stance score. Keyword Retrieval (full) means using the full names as keywords and Keyword Retrieval (surname) means using the surname of people. The third and fourth baselines use pre-trained GPT-2 and pre-trained GPT-3 Curie respectively. "[CONTEXT]" is a preceding context "As a Democrat/Republican, I think", which is concatenated with the prompts to generate partisan responses on each item. We compute the average community stance on 1000 synthetic responses and pick the community with a higher average stance score. It is noted that we also fine-tune or train GPT-2 on the aggregate partisan tweets and show their results in Appendix D.</p>
<p>Overall Performance. First, we observe
that fine-tuned COMMUNITYLM with "X is the" prompt achieves the best performance in both accuracy ( $97.33 \%$ ) and weighted F1-score ( $97.29 \%$ ) on the task. The same model's performance is sensitive to the prompt design and the longest prompt out of the four seems to work the best. " X " alone is bad, because it will result in many responses like "X @USER", "X???", "X.", which are common Twitter posts and are too short to interpret their attitudes. Second, fine-tuned COMMUNITYLM outperforms trained COMMUNITYLM from scratch. It indicates that pre-training GPT-2 is helpful, probably because pre-training injects the general knowledge about the named entities into GPT-2. Third, we find that Keyword Retrieval (surname) is a strong baseline in both accuracy ( $93.33 \%$ ) and F1 ( $93.33 \%$ ), but its performance is also sensitive to the selection of keywords. As we see, the weighted F1 performance of Keyword Retrieval (full), which uses a strict full name matching (e.g., "Joe Biden"), drops to $87.00 \%$. In contrast, language models are able to learn the associations between different names for the same person and generalize without worrying about name forms. Last, fine-tuned COMMUNITYLM outperforms pre-trained GPT-2 and GPT-3 baselines. It is worth noting that the performance of pre-trained GPT-3 Curie is consistently better than pre-trained GPT-2. GPT-3 with the " X is/are" prompt achieves the same score as the Keyword Retrieval (surname) baseline.</p>
<p>Error Analysis. The rule-based Keyword Retrieval (surname) baseline misses "illegal immigrants" and "big business". The fine-tuned COMMUNITYLM with "X is/are the" misses "White people". The pre-trained GPT-3 with "X is/are" prompt misses "Dr. Anthony Fauci" and "Asian people'. It is interesting because the top 5 items with the closest average rating gap between ANES partisan participants are Asian people (5.5\%), White people (5.9\%), Hispanic people (7.7\%), Dr. Anthony Fauci ( $8.4 \%$ ), and Black people ( $9.7 \%$ ).</p>
<p>Ranking Public Figures. We use the average community stance scores computed on the generated tweets from the fine-tuned COMMUNITYLM model to rank 16 public figures for each community, hoping to understand how they perceive these people. In Figure 1, we observe that Republican politicians are rated poorly by the Democratic model and vice versa. Overall, the ratings from the Republican model are more negative than the Democratic model. Interestingly, we find that An-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Weighted F1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Frequency Model</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">53.33</td>
<td style="text-align: center;">54.50</td>
</tr>
<tr>
<td style="text-align: center;">Keyword Retrieval (Full)</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">86.67</td>
<td style="text-align: center;">87.00</td>
</tr>
<tr>
<td style="text-align: center;">Keyword Retrieval (Surname)</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">93.33</td>
<td style="text-align: center;">93.33</td>
</tr>
<tr>
<td style="text-align: center;">Pre-trained GPT-2</td>
<td style="text-align: center;">"[CONTEXT] + X"</td>
<td style="text-align: center;">$74.00 \pm 2.79$</td>
<td style="text-align: center;">$66.52 \pm 5.56$</td>
</tr>
<tr>
<td style="text-align: center;">Pre-trained GPT-2</td>
<td style="text-align: center;">"[CONTEXT] + X is/are"</td>
<td style="text-align: center;">$72.00 \pm 1.83$</td>
<td style="text-align: center;">$64.63 \pm 2.35$</td>
</tr>
<tr>
<td style="text-align: center;">Pre-trained GPT-2</td>
<td style="text-align: center;">"[CONTEXT] + X is/are a"</td>
<td style="text-align: center;">$75.33 \pm 1.83$</td>
<td style="text-align: center;">$68.47 \pm 3.35$</td>
</tr>
<tr>
<td style="text-align: center;">Pre-trained GPT-2</td>
<td style="text-align: center;">"[CONTEXT] + X is/are the"</td>
<td style="text-align: center;">$77.33 \pm 2.79$</td>
<td style="text-align: center;">$74.71 \pm 3.22$</td>
</tr>
<tr>
<td style="text-align: center;">Pre-trained GPT-3 Curie</td>
<td style="text-align: center;">"[CONTEXT] + X"</td>
<td style="text-align: center;">83.33</td>
<td style="text-align: center;">83.88</td>
</tr>
<tr>
<td style="text-align: center;">Pre-trained GPT-3 Curie</td>
<td style="text-align: center;">"[CONTEXT] + X is/are"</td>
<td style="text-align: center;">93.33</td>
<td style="text-align: center;">93.50</td>
</tr>
<tr>
<td style="text-align: center;">Pre-trained GPT-3 Curie</td>
<td style="text-align: center;">"[CONTEXT] + X is/are a"</td>
<td style="text-align: center;">83.33</td>
<td style="text-align: center;">83.88</td>
</tr>
<tr>
<td style="text-align: center;">Pre-trained GPT-3 Curie</td>
<td style="text-align: center;">"[CONTEXT] + X is/are the"</td>
<td style="text-align: center;">83.33</td>
<td style="text-align: center;">84.02</td>
</tr>
<tr>
<td style="text-align: center;">Trained CommunityLM</td>
<td style="text-align: center;">"X"</td>
<td style="text-align: center;">$90.00 \pm 0.00$</td>
<td style="text-align: center;">$89.63 \pm 0.27$</td>
</tr>
<tr>
<td style="text-align: center;">Trained CommunityLM</td>
<td style="text-align: center;">"X is/are"</td>
<td style="text-align: center;">$90.00 \pm 0.00$</td>
<td style="text-align: center;">$89.82 \pm 0.00$</td>
</tr>
<tr>
<td style="text-align: center;">Trained CommunityLM</td>
<td style="text-align: center;">"X is/are a"</td>
<td style="text-align: center;">$86.00 \pm 1.49$</td>
<td style="text-align: center;">$86.25 \pm 1.50$</td>
</tr>
<tr>
<td style="text-align: center;">Trained CommunityLM</td>
<td style="text-align: center;">"X is/are the"</td>
<td style="text-align: center;">$90.67 \pm 2.79$</td>
<td style="text-align: center;">$90.49 \pm 2.68$</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tuned CommunityLM</td>
<td style="text-align: center;">"X"</td>
<td style="text-align: center;">$84.67 \pm 2.98$</td>
<td style="text-align: center;">$84.46 \pm 3.18$</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tuned CommunityLM</td>
<td style="text-align: center;">"X is/are"</td>
<td style="text-align: center;">$96.00 \pm 2.79$</td>
<td style="text-align: center;">$96.00 \pm 2.79$</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tuned CommunityLM</td>
<td style="text-align: center;">"X is/are a"</td>
<td style="text-align: center;">$91.33 \pm 1.83$</td>
<td style="text-align: center;">$90.83 \pm 2.05$</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tuned CommunityLM</td>
<td style="text-align: center;">"X is/are the"</td>
<td style="text-align: center;">$\mathbf{9 7 . 3 3} \pm \mathbf{1 . 4 9}$</td>
<td style="text-align: center;">$\mathbf{9 7 . 2 9} \pm \mathbf{1 . 5 2}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance of different approaches in accuracy to predict which community is more favorable towards 30 persons or groups from the ANES survey. Approaches based on GPT-2 are repeated five times to compute the average and standard deviation. GPT-3 is only run once for cost concern. Frequency Model and Keyword Retrieval methods are deterministic. The weighted average F1 is used because of data imbalance.
drew Yang is rated quite highly by both models, likely because of the sampling bias of Twitter. It is noted that "Andrew Yang" is also ranked 1st by the Democrat community and 3rd by the Republican community with the retrieval approach.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Left and right rankings of 16 public figures by their average stance scores calculated on synthetic tweets from their fine-tuned CommunityLM models.</p>
<h2>5 Conclusion</h2>
<p>In this paper, we present a simple COMMUNITYLM framework to evaluate the viability of fine-tuned GPT-2 community language models in mining community insights in the context of political polarization between Republicans and Democrats. We adopt ANES survey questions and experiment with four types of prompts to generate
community responses through GPT-2, showing that generated opinions are predictive about which community is more favorable towards selected public figures and groups. Our results show that finetuned CommunityLM (GPT-2) outperforms the baseline methods. We analyze the model errors and run qualitative analyses to demonstrate that GPT-2 community language models can be used to rank public figures and probe word choices.</p>
<p>There are a few limitations in the current approach. First, language models can synthesize unreliable responses. Structured knowledge (Wang et al., 2021; Yasunaga et al., 2021) can be used to reduce nonsensical or unfaithful generation. Therefore, it is important that we use statistical patterns rather than individual synthesized tweets to draw conclusions (Feldman et al., 2021). Second, language models are shown to be sensitive to prompt design in our experiments and are also vulnerable to negation and misprimed probes (Kassner and Schütze, 2020). In the future, we plan to develop a systematic approach to design effective prompts and evaluate the robustness of COMMUNITYLM. Third, we focus on the classic red and blue polarization and do not consider a more fine-grained segmentation of U.S. politics. We hope to extend this work to study multiple sociopolitical communities in America and surface their unheard voices.</p>
<h2>Ethical Considerations</h2>
<p>We propose a general framework to probe community insights and observe differences between the Democratic and Republican communities on Twitter. While we do not discuss how to react to these findings, the intention of our research is encourage people to escape from their echo chambers, hear voices from other communities, and engage in constructive communication. One reasonable ethical concern is that by using a language model to predict community opinions, instead of asking individuals from the community directly, don't we risk erasing individual voices? To that concern we would like to emphasize that our model is no substitute for deeper engagement with a community; as discussed in the limitation paragraph, the language model is just an entry point for understanding a community's perspective. It serves to synthesize the points expressed by the speakers in the training data more effectively than we know how to do by hand. Any automated or semi-automated prediction system risks misinterpreting or "erasing" an expressed opinion, and we show in our work that the simpler methods of doing so are more errorprone, and hence measurably more unfair than the proposed approach in the paper.</p>
<h2>Acknowledgements</h2>
<p>We would like to thank anonymous reviewers for their helpful comments on our paper. We also want to thank Belén Carolina Saldías Fuentes, Will Brannon, Suyash Fulay, Wonjune Kang, Hope Schroeder, and Shayne Longpre for their discussion and feedback in the early stages of the project.</p>
<h2>References</h2>
<p>Christopher A Bail, Lisa P Argyle, Taylor W Brown, John P Bumpus, Haohan Chen, MB Fallin Hunzaker, Jaemin Lee, Marcus Mann, Friedolin Merhout, and Alexander Volfovsky. 2018. Exposure to opposing views on social media can increase political polarization. Proceedings of the National Academy of Sciences, 115(37):9216-9221.</p>
<p>Francesco Barbieri, Jose Camacho-Collados, Luis Espinosa Anke, and Leonardo Neves. 2020. TweetEval: Unified benchmark and comparative evaluation for tweet classification. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1644-1650, Online. Association for Computational Linguistics.</p>
<p>L Brent Bozell. 2004. Weapons of mass distortion:</p>
<p>The coming meltdown of the liberal media. Crown Forum.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.</p>
<p>Kareem Darwish. 2019. Quantifying polarization on twitter: the kavanaugh nomination. In International conference on social informatics, pages 188-201. Springer.</p>
<p>Joe Davison, Joshua Feldman, and Alexander Rush. 2019. Commonsense knowledge mining from pretrained models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1173-1178, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Dorottya Demszky, Nikhil Garg, Rob Voigt, James Zou, Jesse Shapiro, Matthew Gentzkow, and Dan Jurafsky. 2019. Analyzing polarization in social media: Method and application to tweets on 21 mass shootings. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 29703005, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Philip Feldman, Sim Tiwari, Charissa SL Cheah, James R Foulds, and Shimei Pan. 2021. Analyzing covid-19 tweets with transformer-based language models. arXiv preprint arXiv:2104.10259.</p>
<p>Homero Gil de Zúñiga, Teresa Correa, and Sebastian Valenzuela. 2012. Selective exposure to cable news and immigration in the us: The relationship between fox news, cnn, and attitudes toward mexican immigrants. Journal of Broadcasting \&amp; Electronic Media, 56(4):597-615.</p>
<p>Gordon Heltzel and Kristin Laurin. 2020. Polarization in america: Two possible futures. Current opinion in behavioral sciences, 34:179-184.</p>
<p>Clayton Hutto and Eric Gilbert. 2014. Vader: A parsimonious rule-based model for sentiment analysis of social media text. In Proceedings of the international AAAI conference on web and social media, volume 8, pages 216-225.</p>
<p>Ki Deuk Hyun and Soo Jung Moon. 2016. Agenda setting in the partisan tv news context: Attribute agenda setting and polarized evaluation of presidential candidates among viewers of nbc, cnn, and fox news. Journalism \&amp; Mass Communication Quarterly, 93(3):509-529.</p>
<p>Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. 2021. How can we know when language models know? on the calibration of language models for question answering. Transactions of the Association for Computational Linguistics, 9:962-977.</p>
<p>Nora Kassner and Hinrich Schütze. 2020. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7811-7818, Online. Association for Computational Linguistics.</p>
<p>Ping Li, Benjamin Schloss, and D Jake Follmer. 2017. Speaking two "languages" in america: A semantic space analysis of how presidential candidates and their supporters represent abstract political concepts differently. Behavior research methods, 49(5):16681685 .</p>
<p>Daniel Loureiro, Francesco Barbieri, Leonardo Neves, Luis Espinosa Anke, and Jose Camacho-Collados. 2022. Timelms: Diachronic language models from twitter. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, Online. Association for Computational Linguistics.</p>
<p>Nolan McCarty, Keith T Poole, and Howard Rosenthal. 2016. Polarized America: The dance of ideology and unequal riches. mit Press.</p>
<p>Jeremiah Milbauer, Adarsh Mathew, and James Evans. 2021. Aligning multidimensional worldviews and discovering ideological differences. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4832-4845, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Dat Quoc Nguyen, Thanh Vu, and Anh Tuan Nguyen. 2020. BERTweet: A pre-trained language model for English tweets. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 9-14, Online. Association for Computational Linguistics.</p>
<p>Shriphani Palakodety, Ashiqur R KhudaBukhsh, and Jaime G Carbonell. 2020. Mining insights from largescale corpora using fine-tuned language models. In ECAI 2020, pages 1890-1897. IOS Press.</p>
<p>Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463-2473, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Keith T Poole and Howard Rosenthal. 1984. The polarization of american politics. The journal of politics, 46(4):1061-1079.</p>
<p>Ashiqur R. KhudaBukhsh, Rupak Sarkar, Mark S. Kamlet, and Tom Mitchell. 2021. We don't speak the same language: Interpreting polarization through machine translation. Proceedings of the AAAI Conference on Artificial Intelligence, 35(17):14893-14901.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.</p>
<p>Sara Rosenthal, Noura Farra, and Preslav Nakov. 2017. SemEval-2017 task 4: Sentiment analysis in Twitter. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 502518, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. 2020. oLMpics-on what language model pre-training captures. Transactions of the Association for Computational Linguistics, 8:743-758.</p>
<p>Svitlana Volkova, Glen Coppersmith, and Benjamin Van Durme. 2014. Inferring user political preferences from streaming communications. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 186-196, Baltimore, Maryland. Association for Computational Linguistics.</p>
<p>Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2021. KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation. Transactions of the Association for Computational Linguistics, 9:176-194.</p>
<p>Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec. 2021. QA-GNN: Reasoning with language models and knowledge graphs for question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 535-546, Online. Association for Computational Linguistics.</p>
<h2>Appendix</h2>
<h2>A Keyword counts</h2>
<p>The Keyword Retrieval baseline method retrieves tweets containing the keywords. Here we show the list of full and surname keywords and their counts in tables 3 and 4, respectively, for the Republican and Democratic tweets. For corresponding items between these two tables (e.g. "Asian people" in Table 3 to "Asian" in Table 4) there is a consistent increase in counts, especially for "Asian people" "Anthony Fauci", "Hispanic people", "labor unions", "Clarence Thomas". Some items in Table 4 might have too many counts. For example, we observe that "Trump" has 150,000+ counts in both partisan tweets, which can take a relatively long time for sentiment classifiers to run.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Keyword</th>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Dem</th>
<th style="text-align: center;">Repub</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Asian people</td>
<td style="text-align: center;">fiusian</td>
<td style="text-align: center;">81</td>
<td style="text-align: center;">21</td>
</tr>
<tr>
<td style="text-align: center;">Joe Biden</td>
<td style="text-align: center;">fibiden1</td>
<td style="text-align: center;">4177</td>
<td style="text-align: center;">5377</td>
</tr>
<tr>
<td style="text-align: center;">big business</td>
<td style="text-align: center;">fibigbusiness</td>
<td style="text-align: center;">321</td>
<td style="text-align: center;">291</td>
</tr>
<tr>
<td style="text-align: center;">Black people</td>
<td style="text-align: center;">fiblack</td>
<td style="text-align: center;">3199</td>
<td style="text-align: center;">1278</td>
</tr>
<tr>
<td style="text-align: center;">Pete Buttigieg</td>
<td style="text-align: center;">fibuttigieg1</td>
<td style="text-align: center;">982</td>
<td style="text-align: center;">521</td>
</tr>
<tr>
<td style="text-align: center;">capitalists</td>
<td style="text-align: center;">ficapitalists</td>
<td style="text-align: center;">279</td>
<td style="text-align: center;">187</td>
</tr>
<tr>
<td style="text-align: center;">the Democratic Party</td>
<td style="text-align: center;">fidemocraticparty</td>
<td style="text-align: center;">2094</td>
<td style="text-align: center;">2646</td>
</tr>
<tr>
<td style="text-align: center;">Anthony Fauci</td>
<td style="text-align: center;">fffauci1</td>
<td style="text-align: center;">102</td>
<td style="text-align: center;">85</td>
</tr>
<tr>
<td style="text-align: center;">feminists</td>
<td style="text-align: center;">fifeminists</td>
<td style="text-align: center;">351</td>
<td style="text-align: center;">628</td>
</tr>
<tr>
<td style="text-align: center;">Nikki Haley</td>
<td style="text-align: center;">fihaley1</td>
<td style="text-align: center;">169</td>
<td style="text-align: center;">274</td>
</tr>
<tr>
<td style="text-align: center;">Kamala Harris</td>
<td style="text-align: center;">fiharris1</td>
<td style="text-align: center;">1711</td>
<td style="text-align: center;">1450</td>
</tr>
<tr>
<td style="text-align: center;">Hispanic people</td>
<td style="text-align: center;">fihisp</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">21</td>
</tr>
<tr>
<td style="text-align: center;">illegal immigrants</td>
<td style="text-align: center;">ftillegal</td>
<td style="text-align: center;">251</td>
<td style="text-align: center;">2233</td>
</tr>
<tr>
<td style="text-align: center;">Amy Klobuchar</td>
<td style="text-align: center;">fiklobuchar1</td>
<td style="text-align: center;">451</td>
<td style="text-align: center;">193</td>
</tr>
<tr>
<td style="text-align: center;">labor unions</td>
<td style="text-align: center;">fllaborunions</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">27</td>
</tr>
<tr>
<td style="text-align: center;">the #MeToo movement</td>
<td style="text-align: center;">fimetoo</td>
<td style="text-align: center;">103</td>
<td style="text-align: center;">84</td>
</tr>
<tr>
<td style="text-align: center;">Barack Obama</td>
<td style="text-align: center;">fiobama1</td>
<td style="text-align: center;">684</td>
<td style="text-align: center;">929</td>
</tr>
<tr>
<td style="text-align: center;">Alexandria Ocasio-Cortez</td>
<td style="text-align: center;">focasios1</td>
<td style="text-align: center;">410</td>
<td style="text-align: center;">534</td>
</tr>
<tr>
<td style="text-align: center;">Nancy Pelosi</td>
<td style="text-align: center;">fipelosi1</td>
<td style="text-align: center;">1467</td>
<td style="text-align: center;">3549</td>
</tr>
<tr>
<td style="text-align: center;">Mike Pence</td>
<td style="text-align: center;">fipence1</td>
<td style="text-align: center;">911</td>
<td style="text-align: center;">502</td>
</tr>
<tr>
<td style="text-align: center;">the Republican Party</td>
<td style="text-align: center;">firepublicanparty</td>
<td style="text-align: center;">1681</td>
<td style="text-align: center;">838</td>
</tr>
<tr>
<td style="text-align: center;">Marco Rubio</td>
<td style="text-align: center;">firabio1</td>
<td style="text-align: center;">166</td>
<td style="text-align: center;">132</td>
</tr>
<tr>
<td style="text-align: center;">Bernie Sanders</td>
<td style="text-align: center;">fisanders1</td>
<td style="text-align: center;">4572</td>
<td style="text-align: center;">2711</td>
</tr>
<tr>
<td style="text-align: center;">socialists</td>
<td style="text-align: center;">fisocialists</td>
<td style="text-align: center;">627</td>
<td style="text-align: center;">2697</td>
</tr>
<tr>
<td style="text-align: center;">Clarence Thomas</td>
<td style="text-align: center;">fithomas1</td>
<td style="text-align: center;">157</td>
<td style="text-align: center;">132</td>
</tr>
<tr>
<td style="text-align: center;">transgender people</td>
<td style="text-align: center;">fttranuppl</td>
<td style="text-align: center;">165</td>
<td style="text-align: center;">38</td>
</tr>
<tr>
<td style="text-align: center;">Donald Trump</td>
<td style="text-align: center;">fttrump1</td>
<td style="text-align: center;">8501</td>
<td style="text-align: center;">5479</td>
</tr>
<tr>
<td style="text-align: center;">Elizabeth Warren</td>
<td style="text-align: center;">ftwarren1</td>
<td style="text-align: center;">3132</td>
<td style="text-align: center;">1897</td>
</tr>
<tr>
<td style="text-align: center;">White people</td>
<td style="text-align: center;">fivhite</td>
<td style="text-align: center;">3625</td>
<td style="text-align: center;">1862</td>
</tr>
<tr>
<td style="text-align: center;">Andrew Yang</td>
<td style="text-align: center;">fiyang1</td>
<td style="text-align: center;">585</td>
<td style="text-align: center;">249</td>
</tr>
</tbody>
</table>
<p>Table 3: Counts of full names for each person and group in Republican and Democratic tweets.</p>
<h2>B What are the average ratings between partisan participants in ANES survey?</h2>
<p>We also compute and show in Table 5 the average ratings from Republican and Democratic participants towards each person or group. For most items, we observe quite large rating gaps between the partisans. But the top 5 items with the closest average rating gap between partisans are "Asian people" (5.5\%), "White people" (5.9\%), "Hispanic people" (7.7\%), "Dr. Anthony Fauci" (8.4\%),</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Keyword</th>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Dem</th>
<th style="text-align: center;">Repub</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Asian</td>
<td style="text-align: center;">fiusian</td>
<td style="text-align: center;">2961</td>
<td style="text-align: center;">1917</td>
</tr>
<tr>
<td style="text-align: center;">Biden</td>
<td style="text-align: center;">fibiden1</td>
<td style="text-align: center;">26558</td>
<td style="text-align: center;">21748</td>
</tr>
<tr>
<td style="text-align: center;">big business</td>
<td style="text-align: center;">fibigbusiness</td>
<td style="text-align: center;">321</td>
<td style="text-align: center;">291</td>
</tr>
<tr>
<td style="text-align: center;">Black people</td>
<td style="text-align: center;">fifilack</td>
<td style="text-align: center;">3199</td>
<td style="text-align: center;">1278</td>
</tr>
<tr>
<td style="text-align: center;">Buttigieg</td>
<td style="text-align: center;">fibuttigieg1</td>
<td style="text-align: center;">3514</td>
<td style="text-align: center;">1348</td>
</tr>
<tr>
<td style="text-align: center;">capitalist</td>
<td style="text-align: center;">ficapitalists</td>
<td style="text-align: center;">1393</td>
<td style="text-align: center;">941</td>
</tr>
<tr>
<td style="text-align: center;">Democratic Party</td>
<td style="text-align: center;">fidemocraticparty</td>
<td style="text-align: center;">2677</td>
<td style="text-align: center;">3611</td>
</tr>
<tr>
<td style="text-align: center;">Fauci</td>
<td style="text-align: center;">fffauci1</td>
<td style="text-align: center;">931</td>
<td style="text-align: center;">1219</td>
</tr>
<tr>
<td style="text-align: center;">feminist</td>
<td style="text-align: center;">fifeminists</td>
<td style="text-align: center;">1686</td>
<td style="text-align: center;">1470</td>
</tr>
<tr>
<td style="text-align: center;">Haley</td>
<td style="text-align: center;">fihaley1</td>
<td style="text-align: center;">531</td>
<td style="text-align: center;">712</td>
</tr>
<tr>
<td style="text-align: center;">Harris</td>
<td style="text-align: center;">fiharris1</td>
<td style="text-align: center;">6753</td>
<td style="text-align: center;">5416</td>
</tr>
<tr>
<td style="text-align: center;">Hispanic</td>
<td style="text-align: center;">fihisp</td>
<td style="text-align: center;">1173</td>
<td style="text-align: center;">1693</td>
</tr>
<tr>
<td style="text-align: center;">illegal immigrant</td>
<td style="text-align: center;">ftillegal</td>
<td style="text-align: center;">312</td>
<td style="text-align: center;">2815</td>
</tr>
<tr>
<td style="text-align: center;">Klobuchar</td>
<td style="text-align: center;">fiklobuchar1</td>
<td style="text-align: center;">1958</td>
<td style="text-align: center;">584</td>
</tr>
<tr>
<td style="text-align: center;">labor union</td>
<td style="text-align: center;">fllaborunions</td>
<td style="text-align: center;">110</td>
<td style="text-align: center;">47</td>
</tr>
<tr>
<td style="text-align: center;">#MeToo movement</td>
<td style="text-align: center;">fimetoo</td>
<td style="text-align: center;">114</td>
<td style="text-align: center;">102</td>
</tr>
<tr>
<td style="text-align: center;">Obama</td>
<td style="text-align: center;">fiobama1</td>
<td style="text-align: center;">15390</td>
<td style="text-align: center;">33105</td>
</tr>
<tr>
<td style="text-align: center;">Ocasio-Cortez</td>
<td style="text-align: center;">focasios1</td>
<td style="text-align: center;">751</td>
<td style="text-align: center;">1792</td>
</tr>
<tr>
<td style="text-align: center;">Pelosi</td>
<td style="text-align: center;">fipelosi1</td>
<td style="text-align: center;">5985</td>
<td style="text-align: center;">15844</td>
</tr>
<tr>
<td style="text-align: center;">Pence</td>
<td style="text-align: center;">fipence1</td>
<td style="text-align: center;">5818</td>
<td style="text-align: center;">3021</td>
</tr>
<tr>
<td style="text-align: center;">Republican Party</td>
<td style="text-align: center;">firepublicanparty</td>
<td style="text-align: center;">2251</td>
<td style="text-align: center;">1079</td>
</tr>
<tr>
<td style="text-align: center;">Rubio</td>
<td style="text-align: center;">firabio1</td>
<td style="text-align: center;">508</td>
<td style="text-align: center;">502</td>
</tr>
<tr>
<td style="text-align: center;">Sanders</td>
<td style="text-align: center;">fisanders1</td>
<td style="text-align: center;">16001</td>
<td style="text-align: center;">6568</td>
</tr>
<tr>
<td style="text-align: center;">socialist</td>
<td style="text-align: center;">fisocialists</td>
<td style="text-align: center;">3182</td>
<td style="text-align: center;">12606</td>
</tr>
<tr>
<td style="text-align: center;">Thomas</td>
<td style="text-align: center;">fithomas1</td>
<td style="text-align: center;">2316</td>
<td style="text-align: center;">3348</td>
</tr>
<tr>
<td style="text-align: center;">transgender</td>
<td style="text-align: center;">fttranuppl</td>
<td style="text-align: center;">1309</td>
<td style="text-align: center;">1469</td>
</tr>
<tr>
<td style="text-align: center;">Trump</td>
<td style="text-align: center;">fttrump1</td>
<td style="text-align: center;">188170</td>
<td style="text-align: center;">150589</td>
</tr>
<tr>
<td style="text-align: center;">Warren</td>
<td style="text-align: center;">ftwarren1</td>
<td style="text-align: center;">18954</td>
<td style="text-align: center;">6969</td>
</tr>
<tr>
<td style="text-align: center;">White people</td>
<td style="text-align: center;">fivhite</td>
<td style="text-align: center;">3625</td>
<td style="text-align: center;">1862</td>
</tr>
<tr>
<td style="text-align: center;">Yang</td>
<td style="text-align: center;">fiyang1</td>
<td style="text-align: center;">4443</td>
<td style="text-align: center;">1433</td>
</tr>
</tbody>
</table>
<p>Table 4: Counts of surname names for each person and group in Republican and Democratic tweets.
"Black people" (9.7\%). These items have very close ratings and we confirm in our error analysis that they are also challenging to the GPT-2 models. It is worth noting that the survey was done in early 2020 and at that time "Dr. Fauci" as a topic was not as divisive as it is today on Twitter.</p>
<h2>C How well does the system perform using a lexicon-based sentiment classifier?</h2>
<p>In the main paper, we use a state-of-the-art pretrained BERT Twitter sentiment classifier to classify tweets. Some researchers may be concerned that neural sentiment models may learn and reflect biases in the training data and prefer using lexiconbased approaches. Therefore, we also use VADER (Hutto and Gilbert, 2014) ${ }^{7}$, a popular rule-based model for sentiment analysis of social media texts, and report the performance of our models with VADER in Table 6. Overall, we show that these models perform slightly worse with VADER, but we still see that fine-tuned COMMUNITYLM with " X is the" perform the best ( $93.33 \%$ ) out of these models. This performance is on par with the Keyword Retrieval (surname) approach. We conjecture that using prompts like " X is the" creates many</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Item</th>
<th style="text-align: center;">Dem</th>
<th style="text-align: center;">Repub</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">fiusian</td>
<td style="text-align: center;">Asian people</td>
<td style="text-align: center;">68.95</td>
<td style="text-align: center;">63.44</td>
</tr>
<tr>
<td style="text-align: center;">fivihite</td>
<td style="text-align: center;">White people</td>
<td style="text-align: center;">71.25</td>
<td style="text-align: center;">77.16</td>
</tr>
<tr>
<td style="text-align: center;">fihisp</td>
<td style="text-align: center;">Hispanic people</td>
<td style="text-align: center;">71.27</td>
<td style="text-align: center;">63.60</td>
</tr>
<tr>
<td style="text-align: center;">fifauci1</td>
<td style="text-align: center;">Dr. Anthony Fauci</td>
<td style="text-align: center;">66.67</td>
<td style="text-align: center;">58.28</td>
</tr>
<tr>
<td style="text-align: center;">fifduck</td>
<td style="text-align: center;">Black people</td>
<td style="text-align: center;">76.22</td>
<td style="text-align: center;">66.51</td>
</tr>
<tr>
<td style="text-align: center;">firubio1</td>
<td style="text-align: center;">Marco Rubio</td>
<td style="text-align: center;">31.52</td>
<td style="text-align: center;">43.01</td>
</tr>
<tr>
<td style="text-align: center;">ficepitalists</td>
<td style="text-align: center;">capitalists</td>
<td style="text-align: center;">46.68</td>
<td style="text-align: center;">60.53</td>
</tr>
<tr>
<td style="text-align: center;">fibighusiness</td>
<td style="text-align: center;">big business</td>
<td style="text-align: center;">43.14</td>
<td style="text-align: center;">57.85</td>
</tr>
<tr>
<td style="text-align: center;">fllaborunions</td>
<td style="text-align: center;">labor unions</td>
<td style="text-align: center;">60.67</td>
<td style="text-align: center;">44.87</td>
</tr>
<tr>
<td style="text-align: center;">fllulay1</td>
<td style="text-align: center;">Nikki Haley</td>
<td style="text-align: center;">29.86</td>
<td style="text-align: center;">47.07</td>
</tr>
<tr>
<td style="text-align: center;">fithomax1</td>
<td style="text-align: center;">Clarence Thomas</td>
<td style="text-align: center;">29.95</td>
<td style="text-align: center;">48.63</td>
</tr>
<tr>
<td style="text-align: center;">fiyang1</td>
<td style="text-align: center;">Andrew Yang</td>
<td style="text-align: center;">49.28</td>
<td style="text-align: center;">29.19</td>
</tr>
<tr>
<td style="text-align: center;">fildsbuchar1</td>
<td style="text-align: center;">Amy Klobuchar</td>
<td style="text-align: center;">50.04</td>
<td style="text-align: center;">22.17</td>
</tr>
<tr>
<td style="text-align: center;">fifeminists</td>
<td style="text-align: center;">feminists</td>
<td style="text-align: center;">61.97</td>
<td style="text-align: center;">33.92</td>
</tr>
<tr>
<td style="text-align: center;">fifransppl</td>
<td style="text-align: center;">transgender people</td>
<td style="text-align: center;">63.22</td>
<td style="text-align: center;">35.06</td>
</tr>
<tr>
<td style="text-align: center;">fisecialists</td>
<td style="text-align: center;">socialists</td>
<td style="text-align: center;">54.00</td>
<td style="text-align: center;">24.11</td>
</tr>
<tr>
<td style="text-align: center;">ftillegal</td>
<td style="text-align: center;">illegal immigrants</td>
<td style="text-align: center;">56.17</td>
<td style="text-align: center;">26.25</td>
</tr>
<tr>
<td style="text-align: center;">fimetosi</td>
<td style="text-align: center;">the #MeToo movement</td>
<td style="text-align: center;">63.74</td>
<td style="text-align: center;">32.73</td>
</tr>
<tr>
<td style="text-align: center;">fihuitigieg1</td>
<td style="text-align: center;">Pete Buttiging</td>
<td style="text-align: center;">52.79</td>
<td style="text-align: center;">21.66</td>
</tr>
<tr>
<td style="text-align: center;">fiharris1</td>
<td style="text-align: center;">Kamala Harris</td>
<td style="text-align: center;">52.12</td>
<td style="text-align: center;">18.63</td>
</tr>
<tr>
<td style="text-align: center;">fincasiroc1</td>
<td style="text-align: center;">Alexandria Ocasio-Cortez</td>
<td style="text-align: center;">50.60</td>
<td style="text-align: center;">16.49</td>
</tr>
<tr>
<td style="text-align: center;">fiwarrex1</td>
<td style="text-align: center;">Elizabeth Warren</td>
<td style="text-align: center;">59.84</td>
<td style="text-align: center;">20.46</td>
</tr>
<tr>
<td style="text-align: center;">fihiden1</td>
<td style="text-align: center;">Joe Biden</td>
<td style="text-align: center;">66.50</td>
<td style="text-align: center;">24.40</td>
</tr>
<tr>
<td style="text-align: center;">fisandex1</td>
<td style="text-align: center;">Bernie Sanders</td>
<td style="text-align: center;">63.77</td>
<td style="text-align: center;">20.50</td>
</tr>
<tr>
<td style="text-align: center;">fipelosi1</td>
<td style="text-align: center;">Nancy Pelosi</td>
<td style="text-align: center;">61.76</td>
<td style="text-align: center;">16.10</td>
</tr>
<tr>
<td style="text-align: center;">fidemocraticparty</td>
<td style="text-align: center;">the Democratic Party</td>
<td style="text-align: center;">71.24</td>
<td style="text-align: center;">24.34</td>
</tr>
<tr>
<td style="text-align: center;">fipence1</td>
<td style="text-align: center;">Mike Pence</td>
<td style="text-align: center;">24.09</td>
<td style="text-align: center;">71.12</td>
</tr>
<tr>
<td style="text-align: center;">firepublicanparty</td>
<td style="text-align: center;">the Republican Party</td>
<td style="text-align: center;">25.02</td>
<td style="text-align: center;">74.47</td>
</tr>
<tr>
<td style="text-align: center;">fisbama1</td>
<td style="text-align: center;">Barack Obama</td>
<td style="text-align: center;">81.29</td>
<td style="text-align: center;">29.99</td>
</tr>
<tr>
<td style="text-align: center;">fittump1</td>
<td style="text-align: center;">Donald Trump</td>
<td style="text-align: center;">17.66</td>
<td style="text-align: center;">77.83</td>
</tr>
</tbody>
</table>
<p>Table 5: Average rating of each item (person or group) from Republican and Democratic participants in the ANES survey.
synthetic tweets with only sentiment-neutral lexical items (e.g., "big business is the future") which the lexicon-based VADER is not able to classify as "positive". The BERT sentiment classifier, however, performs better at representing the overall semantics of the sentence and therefore is preferred in our framework.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Keyword Retrieval (Full)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">76.67</td>
</tr>
<tr>
<td style="text-align: left;">Keyword Retrieval (Surname)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">93.33</td>
</tr>
<tr>
<td style="text-align: left;">Pre-trained GPT-2</td>
<td style="text-align: center;">" ${$ CONTEXT $} \times$ X"</td>
<td style="text-align: center;">$76.67 \pm 0.00$</td>
</tr>
<tr>
<td style="text-align: left;">Pre-trained GPT-2</td>
<td style="text-align: center;">" ${$ CONTEXT $} \times$ X is/are"</td>
<td style="text-align: center;">$76.00 \pm 1.49$</td>
</tr>
<tr>
<td style="text-align: left;">Pre-trained GPT-2</td>
<td style="text-align: center;">" ${$ CONTEXT $} \times$ X is/are a"</td>
<td style="text-align: center;">$78.67 \pm 1.83$</td>
</tr>
<tr>
<td style="text-align: left;">Pre-trained GPT-2</td>
<td style="text-align: center;">" ${$ CONTEXT $} \times$ X is/are the"</td>
<td style="text-align: center;">$74.67 \pm 3.06$</td>
</tr>
<tr>
<td style="text-align: left;">Trained COMMUNITYLM</td>
<td style="text-align: center;">"X"</td>
<td style="text-align: center;">$91.33 \pm 3.80$</td>
</tr>
<tr>
<td style="text-align: left;">Trained COMMUNITYLM</td>
<td style="text-align: center;">"X is/are"</td>
<td style="text-align: center;">$84.67 \pm 3.80$</td>
</tr>
<tr>
<td style="text-align: left;">Trained COMMUNITYLM</td>
<td style="text-align: center;">"X is/are a"</td>
<td style="text-align: center;">$82.00 \pm 3.80$</td>
</tr>
<tr>
<td style="text-align: left;">Trained COMMUNITYLM</td>
<td style="text-align: center;">"X is/are the"</td>
<td style="text-align: center;">$93.33 \pm 4.08$</td>
</tr>
<tr>
<td style="text-align: left;">Fine-tuned COMMUNITYLM</td>
<td style="text-align: center;">"X"</td>
<td style="text-align: center;">$92.00 \pm 1.83$</td>
</tr>
<tr>
<td style="text-align: left;">Fine-tuned COMMUNITYLM</td>
<td style="text-align: center;">"X is/are"</td>
<td style="text-align: center;">$92.67 \pm 1.49$</td>
</tr>
<tr>
<td style="text-align: left;">Fine-tuned COMMUNITYLM</td>
<td style="text-align: center;">"X is/are a"</td>
<td style="text-align: center;">$91.33 \pm 1.83$</td>
</tr>
<tr>
<td style="text-align: left;">Fine-tuned COMMUNITYLM</td>
<td style="text-align: center;">"X is/are the"</td>
<td style="text-align: center;">$93.33 \pm 2.36$</td>
</tr>
</tbody>
</table>
<p>Table 6: Performance of different approaches with VADER in predicting which community is more favorable towards 30 persons or groups from the ANES survey. Approaches based on GPT-2 are repeated five times to compute the average and standard deviation.</p>
<h2>D Is fine-tuning or training GPT-2 on combined Twitter data performing better than pre-trained GPT-2?</h2>
<p>In the main paper, we use pre-trained GPT-2 and GPT-3 to predict the community stance. In addition, we also experimented with training and fine-tuning GPT-2 on the combined Twitter corpus (Republican and Democratic tweets). By contrast with COMMUNITYLM, which fine-tunes two GPT-2 models on partisan Twitter data, in this variant we only train or fine-tune one GPT-2 model on the aggregate of the partisan tweets. Similar to what we did in the pre-trained GPT-2 setting, we use [CONTEXT]+prompt to generate responses. The results are quite interesting, because the performance of the resulting models are worse than the pre-trained GPT-2, even below the majority baseline of $70 \%$. We conjecture that this is because the combined data of partisan tweets neutralizes the sentiment that the models were supposed to learn towards the public figures and groups.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Trained GPT-2 (combined)</td>
<td style="text-align: center;">" ${$ CONTEXT $} \times$ X"</td>
<td style="text-align: center;">$48.67 \pm 8.37$</td>
</tr>
<tr>
<td style="text-align: left;">Trained GPT-2 (combined)</td>
<td style="text-align: center;">" ${$ CONTEXT $} \times$ X is/are"</td>
<td style="text-align: center;">$50.67 \pm 7.23$</td>
</tr>
<tr>
<td style="text-align: left;">Trained GPT-2 (combined)</td>
<td style="text-align: center;">" ${$ CONTEXT $} \times$ X is a"</td>
<td style="text-align: center;">$47.33 \pm 2.79$</td>
</tr>
<tr>
<td style="text-align: left;">Trained GPT-2 (combined)</td>
<td style="text-align: center;">" ${$ CONTEXT $} \times$ X is the"</td>
<td style="text-align: center;">$55.33 \pm 8.37$</td>
</tr>
<tr>
<td style="text-align: left;">Fine-tuned GPT-2 (combined)</td>
<td style="text-align: center;">" ${$ CONTEXT $} \times$ X"</td>
<td style="text-align: center;">$53.33 \pm 4.08$</td>
</tr>
<tr>
<td style="text-align: left;">Fine-tuned GPT-2 (combined)</td>
<td style="text-align: center;">" ${$ CONTEXT $} \times$ X is/are"</td>
<td style="text-align: center;">$50.67 \pm 3.65$</td>
</tr>
<tr>
<td style="text-align: left;">Fine-tuned GPT-2 (combined)</td>
<td style="text-align: center;">" ${$ CONTEXT $} \times$ X is a"</td>
<td style="text-align: center;">$52.67 \pm 2.79$</td>
</tr>
<tr>
<td style="text-align: left;">Fine-tuned GPT-2 (combined)</td>
<td style="text-align: center;">" ${$ CONTEXT $} \times$ X is the"</td>
<td style="text-align: center;">$38.00 \pm 8.37$</td>
</tr>
</tbody>
</table>
<p>Table 7: Performance of trained and fine-tuned GPT-2 on combined Twitter data in accuracy to predict which community is more favorable towards 30 persons or groups from the ANES survey. Experiments are repeated five times to compute the average and standard deviation.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ https://github.com/cjhutto/vaderSent iment&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{2}$ https://github.com/social-machines/1 itecoder
${ }^{3}$ We update American politicians from https://ball otpedia.org/List_of_current_members_of_t he_U.S.<em>Congress and https://ballotpedia. org/Governor</em>(state_executive_office)
${ }^{4}$ https://www.pewresearch.org/politics /2020/10/15/differences-in-how-democrats -and-republicans-behave-on-twitter&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
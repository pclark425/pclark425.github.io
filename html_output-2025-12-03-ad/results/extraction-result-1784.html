<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1784 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1784</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1784</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-c0dccd9be123057ec82a6747d8fec9cc34699a6d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c0dccd9be123057ec82a6747d8fec9cc34699a6d" target="_blank">Sim-To-Real via Sim-To-Sim: Data-Efficient Robotic Grasping via Randomized-To-Canonical Adaptation Networks</a></p>
                <p><strong>Paper Venue:</strong> Computer Vision and Pattern Recognition</p>
                <p><strong>Paper TL;DR:</strong> This paper presents Randomized-to-Canonical Adaptation Networks (RCANs), a novel approach to crossing the visual reality gap that uses no real-world data and learns to translate randomized rendered images into their equivalent non-randomized, canonical versions.</p>
                <p><strong>Paper Abstract:</strong> Real world data, especially in the domain of robotics, is notoriously costly to collect. One way to circumvent this can be to leverage the power of simulation to produce large amounts of labelled data. However, training models on simulated images does not readily transfer to real-world ones. Using domain adaptation methods to cross this "reality gap" requires a large amount of unlabelled real-world data, whilst domain randomization alone can waste modeling power. In this paper, we present Randomized-to-Canonical Adaptation Networks (RCANs), a novel approach to crossing the visual reality gap that uses no real-world data. Our method learns to translate randomized rendered images into their equivalent non-randomized, canonical versions. This in turn allows for real images to also be translated into canonical sim images. We demonstrate the effectiveness of this sim-to-real approach by training a vision-based closed-loop grasping reinforcement learning agent in simulation, and then transferring it to the real world to attain 70% zero-shot grasp success on unseen objects, a result that almost doubles the success of learning the same task directly on domain randomization alone. Additionally, by joint finetuning in the real-world with only 5,000 real-world grasps, our method achieves 91%, attaining comparable performance to a state-of-the-art system trained with 580,000 real-world grasps, resulting in a reduction of real-world data by more than 99%.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1784.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1784.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RCAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Randomized-to-Canonical Adaptation Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An image-conditioned cGAN that is trained in simulation to translate heavily randomized simulated images into a predefined non-randomized (canonical) simulation appearance; at inference it converts real camera images to the canonical simulation domain to enable sim-to-real transfer of policies trained in that canonical simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Kuka IIWA robotic grasping system with QT-Opt policy</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A vision-based closed-loop robotic grasping system built on Kuka IIWA arms using an over-the-shoulder RGB camera; the learned policy (QT-Opt) outputs gripper pose displacements and open/close commands to pick up objects in a tray.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation (vision-based grasping)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>PyBullet / Bullet physics engine (custom simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Rigid-body physics simulator (Bullet) with rendered RGB images (default renderer), depth, and segmentation masks; includes models for robot (Kuka IIWA), tray, and thousands of synthetic and ShapeNet objects, and supports randomized textures, lighting and camera/pose perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Approximate physics + non-photorealistic rendering (simulation with randomized visual appearance); high-enough fidelity for grasp dynamics in simulation but not photorealistic.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Rigid-body contact dynamics via Bullet, kinematics of the Kuka IIWA, RGB rendering (textures, lighting), depth and segmentation mask generation, camera viewpoint.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Rendering is not photorealistic (default renderer); detailed material properties, exact sensor noise, high-fidelity illumination/IBL, and potentially fine-grained contact/soft-body interactions are not modeled precisely; real camera calibration differences and subtle gripper appearance differences were not exhaustively modeled.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Multiple real Kuka IIWA robots with an over-the-shoulder RGB camera overlooking a tray, real physical objects (unseen during sim training) randomly placed in tray; grasp attempts evaluated on 5–6 unseen test objects per robot over 102 trials.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Vision-based closed-loop grasping of previously unseen objects (pick-up grasps in tray)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (off-policy QT-Opt trained in simulation); RCAN generator trained supervised as a paired image translation (cGAN) from randomized-sim -> canonical-sim.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Grasp success rate (percentage) averaged over 102 grasp attempts per robot/test set</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>≈99% grasp success in canonical simulation (QT-Opt trained and evaluated in-sim)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Zero-shot (no real grasp training): 70% success (RCAN); After joint finetuning with 5,000 real on-policy grasps: 91% (RCAN); After 28,000 on-policy grasps: 94% (RCAN). (Simulation-only canonical policy without RCAN: 21% zero-shot.)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Randomized textures (from >5,000 images) applied to tray, objects, arm, floor; randomized lighting position/direction/color/brightness; randomized camera position; small random perturbations to arm and tray position/scale; diverse background images (varied across mild/medium/heavy schemes).</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Visual domain shift (appearance, lighting, textures), differences in object distribution, generator artifacts (notably imperfect gripper reconstruction) that hinder detection of small objects/grasp-in-gripper status, and approximate physics/rendering mismatches.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Training a randomized->canonical image translator entirely in simulation and using it to map real images to the canonical training domain; large-scale visual randomization pairs for supervised cGAN training; use of auxiliary depth and segmentation supervision during generator training; providing both source (real) and generated canonical images concatenated to the policy network; joint finetuning with a small number of real on-policy grasps (5k) to recover behaviors like regrasping.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>No strict numeric fidelity thresholds given; paper emphasizes need to model lighting and shadows sufficiently (canonical environment fixes light to require correct shadow re-rendering) and that accurate rendering of gripper appearance/geometry is important because generator artifacts on the gripper harmed regrasping behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Joint on-policy finetuning using real-world collection of 5,000 and 28,000 grasps; during finetuning the generator is fixed (not updated), real images are adapted by RCAN, and both source and adapted images are concatenated and provided to QT-Opt; training leverages a stream of simulated on-policy data in parallel and updates only the grasping network weights.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Compared canonical (no randomization), mild, medium, heavy domain randomization policies and RCAN: canonical sim->real zero-shot: 21%; mild randomization zero-shot: 37%; medium: 35%; heavy: 33%; RCAN zero-shot: 70%. In-sim performance for all sim-trained agents was ≈98–99%. After 5k real finetune: mild randomization reached ≈85%, medium ≈77%, heavy ≈85%, RCAN ≈91%.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Randomized-to-canonical adaptation (RCAN) trained entirely in simulation yields substantially better zero-shot sim-to-real transfer (70% grasp success) than training policies directly on domain-randomized images (33–37%). RCAN allows offloading visual complexity to a generative adapter, provides an interpretable canonical image, and with only 5,000 real on-policy grasps achieves 91% success—surpassing a baseline trained on 580,000 real grasps—demonstrating large reductions in required real data; generator artifacts (especially around the gripper) can hinder regrasping but are ameliorated by small amounts of real finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>additional_notes</strong></td>
                            <td>RCAN is trained using paired randomized->canonical images captured entirely in simulation; the generator produces RGB, depth and segmentation (depth/mask used as auxiliary supervision). The policy input concatenates source and generated images (6-channel).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-To-Real via Sim-To-Sim: Data-Efficient Robotic Grasping via Randomized-To-Canonical Adaptation Networks', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1784.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1784.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DomainRandomization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain Randomization (visual and some pose randomization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transfer technique where the simulator's visual and some scene parameters are randomized during policy training so that the learned model becomes invariant to superficial differences and can generalize to real-world variability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Domain randomization for transferring deep neural networks from simulation to the real world</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>QT-Opt policy trained directly on randomized simulation images (various randomization levels)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Vision-based grasping policy (QT-Opt) trained in simulation with randomized visual properties to induce invariance to domain shifts; intended to be deployed directly on real Kuka IIWA robots.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation (vision-based grasping)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>PyBullet / Bullet physics engine (custom simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Same simulated environment as RCAN experiments (robot, tray, objects, renderer) with varying degrees of applied randomization to textures, lighting, backgrounds and small pose perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Approximate physics with heavy visual randomization applied to reduce dependence on photorealism; rendering non-photorealistic but diversified.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Rigid-body simulation (Bullet), kinematics, rendered RGB images with varied textures, lighting, camera viewpoints, and backgrounds.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Material reflectance and precise sensor noise not fully modeled; canonical lighting/shadow consistency not enforced; gripper visual realism limited.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Same as RCAN: real Kuka IIWA robots with over-the-shoulder RGB camera, real objects in tray, evaluation via repeated grasp attempts on unseen objects.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Vision-based closed-loop grasping</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (QT-Opt) trained directly on domain-randomized simulated images</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Grasp success rate (percentage) over 102 trials per robot/test set</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>≈98% in-sim success for randomized-training variants</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Zero-shot (no real data): mild 37%, medium 35%, heavy 33% (ranges reported in paper). After 5,000 real on-policy finetune: mild ≈85%, medium ≈77%, heavy ≈85% (varied by experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Mild: vary tray texture, object texture/color, robot arm color, lighting direction/brightness, a small set of background images; Medium: adds more varied background images for the floor; Heavy: uses full randomization scheme (textures from >5k images, lighting color/position, camera perturbation, tray/arm pose/scale perturbations).</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Large visual domain gap when no adaptation is used; policies trained on randomization still affected by generator/artifact issues absent; dynamics mismatches possible but visual gap was primary challenge for these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Heavy randomization aids robustness but alone yields limited zero-shot success; combining randomization with RCAN or with real-world joint finetuning (thousands of grasps) substantially improves real performance.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Paper suggests that simply increasing visual randomization helps pretraining but does not fully bridge the reality gap; no quantitative fidelity thresholds provided.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Joint finetuning with 5,000 and 28,000 real on-policy grasps; during finetuning the networks were updated with real on-policy data (and in baselines using real off-policy data when available) to boost real performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Canonical (no randomization) zero-shot: 21% vs mild 37% vs medium 35% vs heavy 33%; after small real finetuning (5k) domain-randomized policies rose dramatically (~77–85%), indicating domain randomization acts as useful pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Training policies directly on domain-randomized images produces more robust pretraining features and can be rapidly improved with modest real finetuning, but performs substantially worse zero-shot than RCAN; domain randomization is a powerful pretraining strategy but may be less sample-efficient compared to sim->sim learned adapters plus small real finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>additional_notes</strong></td>
                            <td>Paper finds QT-Opt is surprisingly stable under heavy domain randomization (contrary to some other RL algorithms), but zero-shot transfer remains limited without explicit image adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-To-Real via Sim-To-Sim: Data-Efficient Robotic Grasping via Randomized-To-Canonical Adaptation Networks', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1784.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1784.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QT-Opt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QT-Opt (Q-function Targets via Optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scalable, off-policy reinforcement learning algorithm for continuous-action Q-learning used for vision-based robotic manipulation; action selection is performed via stochastic optimization (CEM) over the learned Q-function.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>QT-Opt policy for Kuka IIWA grasping</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A deep Q-function learned with off-policy data that maps (image, gripper aperture, height) and candidate actions (pose displacements, open/close) to Q-values, where the argmax is approximated by CEM to select actions.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation (vision-based grasping)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>PyBullet / Bullet physics engine (custom simulator) used to collect training trajectories for QT-Opt</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Same as above: simulated Kuka arm, tray, objects, rendering, depth and segmentation available; used to generate training trajectories for the Q-function.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Approximate physics sufficient for training QT-Opt in-sim; visual fidelity varied via canonical or randomized renders as the policy's input.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Rigid-body dynamics, kinematics, simulated RGB camera, simulated depth/segmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Simulator rendering not photorealistic; potential discrepancies in contact physics and sensor noise relative to the real robots.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Real Kuka IIWA arms with RGB camera, same task setup used for evaluation and on-policy finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Closed-loop grasping policy that plans and executes corrective grasps and regrasp behaviors</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Off-policy deep Q-learning (QT-Opt) in simulation or on real off-policy datasets depending on experiment; action selection via cross-entropy method (CEM).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Grasp success rate (%)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>≈98–99% success when trained and evaluated in simulation</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Varies by input adaptation: when trained on 580k real grasps (real baseline) achieved 87% real success; when trained in canonical sim and run directly in real: 21%; trained with heavy randomization: 33–37%; with RCAN adaptation zero-shot: 70%; with 5k real finetune + RCAN: 91%.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Policy's performance is strongly affected by visual domain shift; regrasping and grasp-detection depend on accurate perception of small objects and gripper contents, which are sensitive to generator and rendering fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Stability of QT-Opt under heavy visual randomization; pairing QT-Opt with RCAN-generated canonical images (and concatenating source+generated images) enabled much better zero-shot transfer; small amounts of real on-policy data rapidly improved behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>No quantitative thresholds; empirical finding that QT-Opt can tolerate heavy visual randomization during training, but perceptual fidelity around gripper and object appearance matters for some corrective behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Fine-tuned with 5,000 and 28,000 real on-policy grasp episodes; in some baselines QT-Opt had been trained with 580,000 off-policy real grasps as an initial dataset in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>QT-Opt performance in real-world is high when trained with massive real data (87% after 580k off-policy), poor when trained only in canonical sim (21%), moderate when trained on domain randomized sim (33–37%), and substantially improved when combined with RCAN (70% zero-shot, 91% after 5k).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>QT-Opt provides a stable RL backbone for sim-to-real experiments; its stability under heavy visual randomization enabled fair comparison between direct randomization training and RCAN; pairing QT-Opt with RCAN significantly improves sim-to-real transfer efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-To-Real via Sim-To-Sim: Data-Efficient Robotic Grasping via Randomized-To-Canonical Adaptation Networks', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Vr goggles for robots: Real-to-sim domain adaptation for visual control <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping <em>(Rating: 2)</em></li>
                <li>Closing the sim-to-real loop: Adapting simulation randomization with real world experience <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation <em>(Rating: 2)</em></li>
                <li>Unsupervised pixel-level domain adaptation with generative adversarial neural networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1784",
    "paper_id": "paper-c0dccd9be123057ec82a6747d8fec9cc34699a6d",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "RCAN",
            "name_full": "Randomized-to-Canonical Adaptation Networks",
            "brief_description": "An image-conditioned cGAN that is trained in simulation to translate heavily randomized simulated images into a predefined non-randomized (canonical) simulation appearance; at inference it converts real camera images to the canonical simulation domain to enable sim-to-real transfer of policies trained in that canonical simulator.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Kuka IIWA robotic grasping system with QT-Opt policy",
            "agent_system_description": "A vision-based closed-loop robotic grasping system built on Kuka IIWA arms using an over-the-shoulder RGB camera; the learned policy (QT-Opt) outputs gripper pose displacements and open/close commands to pick up objects in a tray.",
            "domain": "general robotics manipulation (vision-based grasping)",
            "virtual_environment_name": "PyBullet / Bullet physics engine (custom simulator)",
            "virtual_environment_description": "Rigid-body physics simulator (Bullet) with rendered RGB images (default renderer), depth, and segmentation masks; includes models for robot (Kuka IIWA), tray, and thousands of synthetic and ShapeNet objects, and supports randomized textures, lighting and camera/pose perturbations.",
            "simulation_fidelity_level": "Approximate physics + non-photorealistic rendering (simulation with randomized visual appearance); high-enough fidelity for grasp dynamics in simulation but not photorealistic.",
            "fidelity_aspects_modeled": "Rigid-body contact dynamics via Bullet, kinematics of the Kuka IIWA, RGB rendering (textures, lighting), depth and segmentation mask generation, camera viewpoint.",
            "fidelity_aspects_simplified": "Rendering is not photorealistic (default renderer); detailed material properties, exact sensor noise, high-fidelity illumination/IBL, and potentially fine-grained contact/soft-body interactions are not modeled precisely; real camera calibration differences and subtle gripper appearance differences were not exhaustively modeled.",
            "real_environment_description": "Multiple real Kuka IIWA robots with an over-the-shoulder RGB camera overlooking a tray, real physical objects (unseen during sim training) randomly placed in tray; grasp attempts evaluated on 5–6 unseen test objects per robot over 102 trials.",
            "task_or_skill_transferred": "Vision-based closed-loop grasping of previously unseen objects (pick-up grasps in tray)",
            "training_method": "Reinforcement learning (off-policy QT-Opt trained in simulation); RCAN generator trained supervised as a paired image translation (cGAN) from randomized-sim -&gt; canonical-sim.",
            "transfer_success_metric": "Grasp success rate (percentage) averaged over 102 grasp attempts per robot/test set",
            "transfer_performance_sim": "≈99% grasp success in canonical simulation (QT-Opt trained and evaluated in-sim)",
            "transfer_performance_real": "Zero-shot (no real grasp training): 70% success (RCAN); After joint finetuning with 5,000 real on-policy grasps: 91% (RCAN); After 28,000 on-policy grasps: 94% (RCAN). (Simulation-only canonical policy without RCAN: 21% zero-shot.)",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Randomized textures (from &gt;5,000 images) applied to tray, objects, arm, floor; randomized lighting position/direction/color/brightness; randomized camera position; small random perturbations to arm and tray position/scale; diverse background images (varied across mild/medium/heavy schemes).",
            "sim_to_real_gap_factors": "Visual domain shift (appearance, lighting, textures), differences in object distribution, generator artifacts (notably imperfect gripper reconstruction) that hinder detection of small objects/grasp-in-gripper status, and approximate physics/rendering mismatches.",
            "transfer_enabling_conditions": "Training a randomized-&gt;canonical image translator entirely in simulation and using it to map real images to the canonical training domain; large-scale visual randomization pairs for supervised cGAN training; use of auxiliary depth and segmentation supervision during generator training; providing both source (real) and generated canonical images concatenated to the policy network; joint finetuning with a small number of real on-policy grasps (5k) to recover behaviors like regrasping.",
            "fidelity_requirements_identified": "No strict numeric fidelity thresholds given; paper emphasizes need to model lighting and shadows sufficiently (canonical environment fixes light to require correct shadow re-rendering) and that accurate rendering of gripper appearance/geometry is important because generator artifacts on the gripper harmed regrasping behavior.",
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "Joint on-policy finetuning using real-world collection of 5,000 and 28,000 grasps; during finetuning the generator is fixed (not updated), real images are adapted by RCAN, and both source and adapted images are concatenated and provided to QT-Opt; training leverages a stream of simulated on-policy data in parallel and updates only the grasping network weights.",
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Compared canonical (no randomization), mild, medium, heavy domain randomization policies and RCAN: canonical sim-&gt;real zero-shot: 21%; mild randomization zero-shot: 37%; medium: 35%; heavy: 33%; RCAN zero-shot: 70%. In-sim performance for all sim-trained agents was ≈98–99%. After 5k real finetune: mild randomization reached ≈85%, medium ≈77%, heavy ≈85%, RCAN ≈91%.",
            "key_findings": "Randomized-to-canonical adaptation (RCAN) trained entirely in simulation yields substantially better zero-shot sim-to-real transfer (70% grasp success) than training policies directly on domain-randomized images (33–37%). RCAN allows offloading visual complexity to a generative adapter, provides an interpretable canonical image, and with only 5,000 real on-policy grasps achieves 91% success—surpassing a baseline trained on 580,000 real grasps—demonstrating large reductions in required real data; generator artifacts (especially around the gripper) can hinder regrasping but are ameliorated by small amounts of real finetuning.",
            "additional_notes": "RCAN is trained using paired randomized-&gt;canonical images captured entirely in simulation; the generator produces RGB, depth and segmentation (depth/mask used as auxiliary supervision). The policy input concatenates source and generated images (6-channel).",
            "uuid": "e1784.0",
            "source_info": {
                "paper_title": "Sim-To-Real via Sim-To-Sim: Data-Efficient Robotic Grasping via Randomized-To-Canonical Adaptation Networks",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "DomainRandomization",
            "name_full": "Domain Randomization (visual and some pose randomization)",
            "brief_description": "A transfer technique where the simulator's visual and some scene parameters are randomized during policy training so that the learned model becomes invariant to superficial differences and can generalize to real-world variability.",
            "citation_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "mention_or_use": "use",
            "agent_system_name": "QT-Opt policy trained directly on randomized simulation images (various randomization levels)",
            "agent_system_description": "Vision-based grasping policy (QT-Opt) trained in simulation with randomized visual properties to induce invariance to domain shifts; intended to be deployed directly on real Kuka IIWA robots.",
            "domain": "general robotics manipulation (vision-based grasping)",
            "virtual_environment_name": "PyBullet / Bullet physics engine (custom simulator)",
            "virtual_environment_description": "Same simulated environment as RCAN experiments (robot, tray, objects, renderer) with varying degrees of applied randomization to textures, lighting, backgrounds and small pose perturbations.",
            "simulation_fidelity_level": "Approximate physics with heavy visual randomization applied to reduce dependence on photorealism; rendering non-photorealistic but diversified.",
            "fidelity_aspects_modeled": "Rigid-body simulation (Bullet), kinematics, rendered RGB images with varied textures, lighting, camera viewpoints, and backgrounds.",
            "fidelity_aspects_simplified": "Material reflectance and precise sensor noise not fully modeled; canonical lighting/shadow consistency not enforced; gripper visual realism limited.",
            "real_environment_description": "Same as RCAN: real Kuka IIWA robots with over-the-shoulder RGB camera, real objects in tray, evaluation via repeated grasp attempts on unseen objects.",
            "task_or_skill_transferred": "Vision-based closed-loop grasping",
            "training_method": "Reinforcement learning (QT-Opt) trained directly on domain-randomized simulated images",
            "transfer_success_metric": "Grasp success rate (percentage) over 102 trials per robot/test set",
            "transfer_performance_sim": "≈98% in-sim success for randomized-training variants",
            "transfer_performance_real": "Zero-shot (no real data): mild 37%, medium 35%, heavy 33% (ranges reported in paper). After 5,000 real on-policy finetune: mild ≈85%, medium ≈77%, heavy ≈85% (varied by experiment).",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Mild: vary tray texture, object texture/color, robot arm color, lighting direction/brightness, a small set of background images; Medium: adds more varied background images for the floor; Heavy: uses full randomization scheme (textures from &gt;5k images, lighting color/position, camera perturbation, tray/arm pose/scale perturbations).",
            "sim_to_real_gap_factors": "Large visual domain gap when no adaptation is used; policies trained on randomization still affected by generator/artifact issues absent; dynamics mismatches possible but visual gap was primary challenge for these experiments.",
            "transfer_enabling_conditions": "Heavy randomization aids robustness but alone yields limited zero-shot success; combining randomization with RCAN or with real-world joint finetuning (thousands of grasps) substantially improves real performance.",
            "fidelity_requirements_identified": "Paper suggests that simply increasing visual randomization helps pretraining but does not fully bridge the reality gap; no quantitative fidelity thresholds provided.",
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "Joint finetuning with 5,000 and 28,000 real on-policy grasps; during finetuning the networks were updated with real on-policy data (and in baselines using real off-policy data when available) to boost real performance.",
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Canonical (no randomization) zero-shot: 21% vs mild 37% vs medium 35% vs heavy 33%; after small real finetuning (5k) domain-randomized policies rose dramatically (~77–85%), indicating domain randomization acts as useful pretraining.",
            "key_findings": "Training policies directly on domain-randomized images produces more robust pretraining features and can be rapidly improved with modest real finetuning, but performs substantially worse zero-shot than RCAN; domain randomization is a powerful pretraining strategy but may be less sample-efficient compared to sim-&gt;sim learned adapters plus small real finetuning.",
            "additional_notes": "Paper finds QT-Opt is surprisingly stable under heavy domain randomization (contrary to some other RL algorithms), but zero-shot transfer remains limited without explicit image adaptation.",
            "uuid": "e1784.1",
            "source_info": {
                "paper_title": "Sim-To-Real via Sim-To-Sim: Data-Efficient Robotic Grasping via Randomized-To-Canonical Adaptation Networks",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "QT-Opt",
            "name_full": "QT-Opt (Q-function Targets via Optimization)",
            "brief_description": "A scalable, off-policy reinforcement learning algorithm for continuous-action Q-learning used for vision-based robotic manipulation; action selection is performed via stochastic optimization (CEM) over the learned Q-function.",
            "citation_title": "QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation",
            "mention_or_use": "use",
            "agent_system_name": "QT-Opt policy for Kuka IIWA grasping",
            "agent_system_description": "A deep Q-function learned with off-policy data that maps (image, gripper aperture, height) and candidate actions (pose displacements, open/close) to Q-values, where the argmax is approximated by CEM to select actions.",
            "domain": "general robotics manipulation (vision-based grasping)",
            "virtual_environment_name": "PyBullet / Bullet physics engine (custom simulator) used to collect training trajectories for QT-Opt",
            "virtual_environment_description": "Same as above: simulated Kuka arm, tray, objects, rendering, depth and segmentation available; used to generate training trajectories for the Q-function.",
            "simulation_fidelity_level": "Approximate physics sufficient for training QT-Opt in-sim; visual fidelity varied via canonical or randomized renders as the policy's input.",
            "fidelity_aspects_modeled": "Rigid-body dynamics, kinematics, simulated RGB camera, simulated depth/segmentation.",
            "fidelity_aspects_simplified": "Simulator rendering not photorealistic; potential discrepancies in contact physics and sensor noise relative to the real robots.",
            "real_environment_description": "Real Kuka IIWA arms with RGB camera, same task setup used for evaluation and on-policy finetuning.",
            "task_or_skill_transferred": "Closed-loop grasping policy that plans and executes corrective grasps and regrasp behaviors",
            "training_method": "Off-policy deep Q-learning (QT-Opt) in simulation or on real off-policy datasets depending on experiment; action selection via cross-entropy method (CEM).",
            "transfer_success_metric": "Grasp success rate (%)",
            "transfer_performance_sim": "≈98–99% success when trained and evaluated in simulation",
            "transfer_performance_real": "Varies by input adaptation: when trained on 580k real grasps (real baseline) achieved 87% real success; when trained in canonical sim and run directly in real: 21%; trained with heavy randomization: 33–37%; with RCAN adaptation zero-shot: 70%; with 5k real finetune + RCAN: 91%.",
            "transfer_success": true,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Policy's performance is strongly affected by visual domain shift; regrasping and grasp-detection depend on accurate perception of small objects and gripper contents, which are sensitive to generator and rendering fidelity.",
            "transfer_enabling_conditions": "Stability of QT-Opt under heavy visual randomization; pairing QT-Opt with RCAN-generated canonical images (and concatenating source+generated images) enabled much better zero-shot transfer; small amounts of real on-policy data rapidly improved behaviors.",
            "fidelity_requirements_identified": "No quantitative thresholds; empirical finding that QT-Opt can tolerate heavy visual randomization during training, but perceptual fidelity around gripper and object appearance matters for some corrective behaviors.",
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "Fine-tuned with 5,000 and 28,000 real on-policy grasp episodes; in some baselines QT-Opt had been trained with 580,000 off-policy real grasps as an initial dataset in prior work.",
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "QT-Opt performance in real-world is high when trained with massive real data (87% after 580k off-policy), poor when trained only in canonical sim (21%), moderate when trained on domain randomized sim (33–37%), and substantially improved when combined with RCAN (70% zero-shot, 91% after 5k).",
            "key_findings": "QT-Opt provides a stable RL backbone for sim-to-real experiments; its stability under heavy visual randomization enabled fair comparison between direct randomization training and RCAN; pairing QT-Opt with RCAN significantly improves sim-to-real transfer efficiency.",
            "uuid": "e1784.2",
            "source_info": {
                "paper_title": "Sim-To-Real via Sim-To-Sim: Data-Efficient Robotic Grasping via Randomized-To-Canonical Adaptation Networks",
                "publication_date_yy_mm": "2018-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Vr goggles for robots: Real-to-sim domain adaptation for visual control",
            "rating": 2
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2
        },
        {
            "paper_title": "Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping",
            "rating": 2
        },
        {
            "paper_title": "Closing the sim-to-real loop: Adapting simulation randomization with real world experience",
            "rating": 2
        },
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2
        },
        {
            "paper_title": "QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation",
            "rating": 2
        },
        {
            "paper_title": "Unsupervised pixel-level domain adaptation with generative adversarial neural networks",
            "rating": 1
        }
    ],
    "cost": 0.01506075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks</h1>
<p>Stephen James ${ }^{1}$, Paul Wohlhart ${ }^{2}$, Mrinal Kalakrishnan ${ }^{2}$, Dmitry Kalashnikov ${ }^{3}$, Alex Irpan ${ }^{3}$, Julian Ibarz ${ }^{3}$, Sergey Levine ${ }^{3,5}$, Raia Hadsell ${ }^{1}$, Konstantinos Bousmalis ${ }^{4}$<br>slj12@imperial.ac.uk, {wohlhart, kalakris}@x.team,<br>{dkalashnikov, alexirpan, julianibarz, slevine, raia, konstantinos} @google.com,</p>
<h2>Abstract</h2>
<p>Real world data, especially in the domain of robotics, is notoriously costly to collect. One way to circumvent this can be to leverage the power of simulation to produce large amounts of labelled data. However, training models on simulated images does not readily transfer to realworld ones. Using domain adaptation methods to cross this "reality gap" requires a large amount of unlabelled realworld data, whilst domain randomization alone can waste modeling power. In this paper, we present Randomized-to-Canonical Adaptation Networks (RCANs), a novel approach to crossing the visual reality gap that uses no realworld data. Our method learns to translate randomized rendered images into their equivalent non-randomized, canonical versions. This in turn allows for real images to also be translated into canonical sim images. We demonstrate the effectiveness of this sim-to-real approach by training a vision-based closed-loop grasping reinforcement learning agent in simulation, and then transferring it to the real world to attain $70 \%$ zero-shot grasp success on unseen objects, a result that almost doubles the success of learning the same task directly on domain randomization alone. Additionally, by joint finetuning in the real-world with only 5,000 real-world grasps, our method achieves $91 \%$, attaining comparable performance to a state-of-the-art system trained with 580,000 real-world grasps, resulting in a reduction of real-world data by more than $99 \%$.</p>
<h2>1. Introduction</h2>
<p>Deep learning for vision-based robotics tasks is a promising research direction [58]. However, it necessitates large amounts of real-world data, which is a severe</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: We learn a generator that translates randomized simulation images to a chosen canonical simulation version which are then used to train a robot grasping agent (top). The system can then be used to translate real-world images to canonical images, and consequently allow for Sim-toReal transfer of the agent (bottom). Feeding both source and target images to the agent allows for joint finetuning of the agent in the real world.
limitation, since real-robot data collection is expensive and cumbersome, often requiring days or even months for a single task [34, 44]. Due to the availability of affordable cloud computing services, it is becoming more attractive to leverage large-scale simulations to collect experience from a large number of agents in parallel. But with this comes the issue of transferring gained experience from simulation to the real world - a non-trivial task given the usually large domain shift.</p>
<p>Reducing the reality gap between simulation and reality is possible with recent advances in visual domain adaptation [14, 36, 5, 55, 4, 66, 71, 30, 54, 59, 21]. Such techniques usually require large amounts of unlabelled images from the real world. Although such unlabelled images are easier to capture than labelled, they can still be costly to collect in</p>
<p>robotics tasks. Domain randomization [51, 61, 25, 38, 3, 24] is another technique that is particularly popular in robotics, where an agent is trained on a wide range of variations of sensory inputs, with the intention that this forces the input processing layers of the network to extract semantically relevant features in a way that is agnostic to the superficial properties of the image (such as particular textures or particular ways shadows are cast from a constant light source). The intuition is that this leads to a network that extracts the same information from real-world images, featuring yet another variation of the input. However, performing randomization directly on the input of a learning algorithm, as done in related work, makes the task potentially harder than necessary, as the algorithm has to model both the arbitrary changes in the visual domain, while at the same time trying to decipher the dynamics of the task. Moreover, although randomization has been successful in the supervised learning setting, there is evidence that some popular reinforcement learning (RL) algorithms, such as DDPG [35] and A3C [39], can be destabilized by this transfer method [38, 70].</p>
<p>In this paper, we investigate learning vision-based robotic closed-loop grasping, where a robotic arm is tasked with picking up a diverse range of unseen objects, with the help of simulation and the use of as little real-world data as possible. Robotic grasping is an important application area in robotics, but also an exceptionally challenging problem: since a grasping system must successfully pick up previously unseen objects, it is not enough simply to memorize grasps that work well for individual instances, but to generalize and extrapolate from an internal understanding of geometry and physics. This presents a particularly difficult challenge for simulation-to-real-world transfer: besides the distributional shift from simulated images and physics, the system must also handle domain shift in the distribution of objects themselves.</p>
<p>To that end, we propose Randomized-to-Canonical Adaptation Networks (RCAN), a novel approach to crossing the reality gap that translates real-world images into their equivalent simulated versions, but makes use of no realworld data. This is achieved by leveraging domain randomization in a unique way, where we learn to adapt from one heavily randomized scene to an equivalent non-randomized, canonical version. We are then able to train a robotic grasping algorithm in a pre-defined canonical version of our simulator, and then use our RCAN model to convert the realworld images to the canonical domain our grasping algorithm was trained on.</p>
<p>Using RCAN along with a grasping algorithm that uses QT-Opt, a recent reinforcement learning algorithm, we achieve almost double the performance in comparison to alternative methods of using randomization. Bootstrapping from this performance, and with the addition of only
5,000 real-world grasps, we are able to achieve higher performance than a system trained with 580,000 real-world grasps. In our particular experiment, none of the objects used during testing are seen during either simulated training or real-world joint finetuning.</p>
<p>Our results also show that RCAN (summarized in Figure 1) is superior to learning a grasping network directly with domain randomization. RCAN has additional advantages compared to other simulation-to-real-world transfer methods. Firstly, unlike domain adaptation methods, it does not need any real-world data in order to learn our reality-to-simulation translation function. Secondly, RCAN gives an interpretable intermediate output that would otherwise not be available when performing domain randomization directly on the policy. Finally, as our method is trained in a supervised manner and preprocesses the input to the downstream task, it enables the use of RL methods that currently suffer from the stability issues when learning a policy directly from domain randomization [38, 70].</p>
<p>In summary, our contributions are as follows:</p>
<ul>
<li>We present a novel approach of crossing the reality gap by using an image-conditioned generative adversarial network (cGAN) [23] to transform randomized simulation images into their non-randomized, canonical versions, which in turn enables real-world images to also be transformed to canonical simulation versions.</li>
<li>We show that by using this approach, we are able to train a state-of-the-art vision-based grasping reinforcement learning algorithm (QT-Opt) purely in simulation and achieve $\mathbf{7 0 \%}$ success on the challenging task of grasping previously unseen objects in the real world, almost double the performance obtained by naively using domain randomization on the input of the learning algorithm.</li>
<li>We also show that by using RCAN and joint finetuning in the real-world with only $\mathbf{5 , 0 0 0}$ additional grasping episodes we are able to increase grasping performance to $\mathbf{9 1 \%}$, outperforming QT-Opt when trained from scratch in the real-world with $\mathbf{5 8 0 , 0 0 0}$ grasps a reduction of over $99 \%$ of required real-world samples.</li>
</ul>
<h2>2. Related Work</h2>
<p>Robotic grasping is a well studied problem [2]. Traditionally, grasping was usually solved analytically, where 3D meshes of objects would be used to compute the stability of a grasp against external wrenches [45, 47] or constrain the object's motion [47]. These solutions often assume that the same, or similar objects will be seen during testing, such that point clouds of the test objects can be matched</p>
<p>with stored objects based on visual and geometric similarity [6, 11, 19, 20, 29]. Due to this limitation, data-driven methods have become the dominant way to solve grasping [33, 37]. These methods commonly make use of either hand-labeled grasp positions [33, 28], self-supervision [44], or predicting grasp outcomes [34]. State-of-the-art grasping systems typically either operate in an open-loop style, where grasping locations are chosen, and then a motion is executed to complete the grasp [69, 41, 37, 60], or in a closed-loop manner, where grasp prediction is continuously run during motion, either explicitly [65], or implicitly [27].</p>
<p>Simulation-to-real-world transfer concerns itself with learning skills in simulation and then transferring them to the real world, which reduces the need for expensive realdata collection. However, it is often not possible to naively transfer such skills directly due to the visual and dynamics differences between the two domains [26]. Numerous works have looked into enabling such transfer both in computer vision and robotics. In the context of robotic manipulation in particular, Saxena et al. [53] used rendered objects to learn a vision-based grasping model. Rusu et al. [50] introduced progressive neural networks that help adapt an existing deep reinforcement learning policy trained from pixels in simulation to the real world for a reaching task. Other works have considered simulation-to-real world transfer using only depth images [64, 18]. Although this may be an attractive option, using depth cameras alone is not suitable for all situations, and coupled with the low cost of simple RGB cameras, there is considerable value in studying transfer in systems that solely use monocular RGB images. Although in this work we use depth estimation from RGB input as an auxiliary task to aid with our randomized-to-canonical image translation model, we neither use depth sensors in the real world, nor do we use our estimated depth during training.</p>
<p>Data augmentation has been a standard tool in computer vision for decades. More recently, and as a way to avoid overfitting, the random application of cropping, flipping samples horizontally, and photometric variations to input images were used to train AlexNet [31] and many more subsequent deep learning models. In robotics, a number of recent works have examined using randomized simulated environments [61, 25, 38, 3, 24, 52] specifically for simulation-to-real world transfer for grasping and other similar manipulation tasks, extending on prior work on randomization for collision-free robotic indoor flight [51]. These works apply randomization in the form of random textures, lighting, and camera position, allowing the resulting algorithm to become invariant to domain differences and applicable to the real world. There have been more robotics works that do not use vision, but that apply domain randomization on physical properties of the simulator to aid transferability [40, 46, 1, 68, 43]. Recently, Chebotar et
al. [9] have specifically looked into learning, from few realworld trajectories, the optimal distribution of such simulation properties, for transfer of policies learned in simulation to the real world. All of these methods learn a policy directly on randomization, whilst our method instead utilizes domain randomization in a novel way in order to learn a randomized-to-canonical adaption function to gain an interpretable intermediate representation and achieve superior results in comparison to learning directly on randomization.</p>
<p>Visual domain adaptation [42, 13] is a process that allows a machine learning model trained with samples from a source domain to generalize to a target domain, by utilizing existing but (mostly) unlabeled target data. In simulation-to-reality transfer, the source domain is usually the simulation, whereas the target is the real world. Prior methods can be split into: (1) feature-level adaptation, where domain-invariant features are learned between source and target domains [17, 15, 57, 7, 14, 36, 5, 55], or (2) pixellevel adaptation, which focuses on re-stylizing images from the source domain to make them look like images from the target domain [4, 66, 71, 30, 54, 59, 21]. Pixel-level domain adaptation differs from image-to-image translation techniques [23, 10, 67], which deal with the easier task of learning such a re-stylization from matching pairs of examples from both domains. Our technique can be seen as an image-to-image translation model that transforms randomized renderings from our simulator to their equivalent nonrandomized, canonical ones.</p>
<p>In the context of robotics, visual domain adaptation has also been used for simulation-to-real-world transfer [62, 56, 3]. Bousmalis et al. [3], introduced the GraspGAN method, which combines pixel-level with feature-level domain adaptation to limit the amount of real data needed for learning grasping. Although the task is similar to ours, GraspGAN required significant amounts of unlabeled real-world data that were previously collected by a variety of pre-existing grasping networks. Our method can be viewed as orthogonal to existing domain adaptation methods and GraspGAN: the process of training the adapter could make use of unlabeled real-world data by incorporating ideas from domain adaptation in the form of additional auxiliary losses to improve performance further. Although in this work we do explore using our simulation-trained policy to collect labeled real-world data for joint finetuning, the combination with domain adaptation techniques is proposed as a promising future research direction.</p>
<p>The reverse, i.e. reality-to-simulation transfer, has been examined recently by Zhang et al. [70] in the context of a simple robotic driving task. The approach has certain advantages, namely the learning algorithm is trained only in simulation, and during inference the real-world images are adapted to look like simulated ones. This decouples adaptation from training and if the real-world environment</p>
<p>changes, it is only the adaptation model that needs to be re-learned. We also explore reality-to-simulation transfer, but unlike [70], which uses CyCaDA [21] and unlabeled real-world data, we do so only in simulation, by learning to adapt randomized images from our simulator to their equivalent non-randomized versions, which allows data-efficient transfer of our model to the real-world.</p>
<h2>3. Background</h2>
<p>We demonstrate our approach by using a recent reinforcement algorithm, Q-function Targets via Optimization (QT-Opt) [27], though our method is compatible with any reinforcement learning or imitation learning algorithm, as we are only adapting the input. QT-Opt is a state-of-theart method for vision base grasping, which made it an ideal choice as a baseline for a direct comparison. Below, we will cover the fundamentals of Q-learning and then provide an overview of QT-Opt.</p>
<p>In reinforcement learning, we assume an agent interacting with an environment consisting of states $\mathbf{s} \in \mathcal{S}$, actions $\mathbf{a} \in \mathcal{A}$, and a reward function $r\left(\mathbf{s}<em t="t">{t}, \mathbf{a}</em>}\right)$, where $\mathbf{s<em t="t">{t}$ and $\mathbf{a}</em>$ are the state and action at time step $t$ respectively. The goal of the agent is then to discover a policy that results in maximizing the total expected reward. One way to achieve such a policy is to use the recently proposed $Q T$ Opt [27] algorithm. QT-Opt is an off-policy, continuousaction generalization of Q-learning, where the goal is to learn a parametrized Q-function (or state-action value function). This can be learned by minimizing the Bellman error:</p>
<p>$$
\mathcal{E}(\theta)=\mathbb{E}<em _theta="\theta">{\left(\mathbf{s}, \mathbf{a}, \mathbf{s}^{\prime}\right) \sim p\left(\mathbf{s}, \mathbf{a}, \mathbf{s}^{\prime}\right)}\left[D\left(Q</em>\right)\right)\right]
$$}(\mathbf{s}, \mathbf{a}), Q_{T}\left(\mathbf{s}, \mathbf{a}, \mathbf{s}^{\prime</p>
<p>where $Q_{T}\left(\mathbf{s}, \mathbf{a}, \mathbf{s}^{\prime}\right)=r(\mathbf{s}, \mathbf{a})+\gamma V\left(\mathbf{s}^{\prime}\right)$ is a target value, and $D$ is a divergence metric, defined as the cross-entropy function in this case. Much like other works in RL, stability was improved by the introduction of two target networks. The target value $V\left(\mathbf{s}^{\prime}\right)$ was computed via a combination of Polyak averaging and clipped double Q-learning to give $V\left(\mathbf{s}^{\prime}\right)=\min <em _bar_theta="\bar{\theta">{i=1,2} Q</em><em _mathbf_a="\mathbf{a">{i}}\left(\mathbf{s}^{\prime}, \arg \max </em>}^{\prime}} Q_{\bar{\theta<em _bar_theta="\bar{\theta">{i}}\left(\mathbf{s}^{\prime}, \mathbf{a}^{\prime}\right)\right)$. $Q T$-Opt differs from other methods primarily with regards to action selection. Rather than selecting actions based on the $\operatorname{argmax}: \pi</em><em _mathbf_a="\mathbf{a">{i}}(\mathbf{s})=\arg \max </em>$; in this case, the cross-entropy method (CEM) [49].}} Q_{\bar{\theta}_{i}}(\mathbf{s}, \mathbf{a})$, QT-Opt instead evaluates the argmax via a stochastic optimization algorithm over $\mathbf{a</p>
<h2>4. Method</h2>
<p>Our method, Randomized-to-Canonical Adaptation Networks ( $R C A N$ ), consists of an image-conditioned generative adversarial network (cGAN) [23] that transforms images from randomized simulated environments (an example is show in Figure 2a) into images that seem similar to
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The setup used in our approach. A dataset of observations from a randomized version of a simulated environment (a) are paired with observations from a canonical version of the same environment (b) in order to learn an adaptation function and allow observations from the realworld (c) to be transformed into observations looking as if they came from the canonical simulation environment.
those obtained from a non-randomized, canonical one (Figure 2b). Once trained, the cGAN generator is also able to transform real-world images into images that seem as if they were obtained from the canonical simulation environment. We are then able to train a reinforcement learning algorithm (in this case $Q T$-Opt) fully in simulation, and use such a generator to enable the trained policy to act in the real-world.</p>
<p>The approach assumes 3 domains: the randomized simulation domain, the canonical simulation domain, and the real-world domain. Let $\mathbb{D}=\left{\left(x_{s}, x_{c}, m_{c}, d_{c}\right)<em j="1">{j}\right}</em>$, such that they appear to belong to the canonical domain.}^{N}$ be a dataset of $N$ training samples, where each sample is a tuple containing an RGB image $x_{s}$ from the randomization (source) domain, an RGB image $x_{c}$ from the canonical (target) domain (with semantic content, i.e. scene configuration, matching that of $x_{s}$ ), a segmentation mask $m_{c}$, and a depth image $d_{c}$. Both the segmentation mask and depth mask are only used as auxiliary tasks during the training of our generator. The RCAN generator function $G(x) \rightarrow\left{x_{a}, m_{a}, d_{a}\right}$, maps an image $x$ from any domain to an adapted image $x_{a}$, segmentation mask $m_{a}$, and depth image $d_{a</p>
<h3>4.1. RCAN Data Generation</h3>
<p>In order to learn this translation $G$, we need pairs of observations capturing the robot in interaction with the scene, with one observation showing the scene in its canonical version and the other one showing the same scene but with randomization applied, as shown in image (a) and (b) of Figure 2. Our simulated environments are based on the Bullet physics engine and use the default renderer [12]. They are built to roughly correspond to the real word, and include a Kuka IIWA, a tray, an over-the-shoulder camera aimed</p>
<p>at the tray, and a set of graspable objects. Graspable objects consist of a combination of 1,000 procedurally generated objects (consisting of randomly merged geometric shapes), and 51,300 realistic objects from 55 categories obtained from the ShapeNet repository [8].</p>
<p>We create the trajectories from which we sample paired snapshots by running training of QT-Opt in simulation. At the beginning of each episode, the position of the divider in the tray is randomly sampled, and 5 randomly selected objects are dropped into the tray. Then, at each timestep we freeze the scene, apply a new arbitrary randomization (described below) to capture the randomized observation, reset to and capture an observation of the canonical version, and let QT-Opt proceed. In our case, observations consist of RGB images, depth, and segmentation masks, labeling each pixel with one of 5 categories: graspable objects, tray, tray divider, robot arm, and background.</p>
<p>The randomization includes applying at each timestep randomly selected textures from a set of over 5,000 images to all models, which includes the tray, graspable objects, arm segments, and floor. Additionally we randomize the position, direction and color of the lighting. To further increase the diversity of scene configurations beyond those that the normal robot operation during QT-Opt training gives us, we also slightly randomize the position and size of the arm and tray (sampling from a uniform distribution), applying the same transformation to both the canonical and the randomized scene when creating the snapshot, such that the semantics between the two still match.</p>
<p>One important question is: what should the canonical environment look like? In practice, the canonical environment can be defined in a number of ways. We opt for applying uniform colors to the background, tray and arm, while leaving the textures for the objects from the randomized version in-place, as this preserves the objects' identity and thus opens up the potential for instance-specific grasping in future works. Each link of the arm is colored independently to aid tracking of individual links of the arm. We opt for fixing the light source in the canonical version, requiring the network to learn some aspect of geometry in order to re-render any shadows in the correct shape and direction.</p>
<h3>4.2. RCAN Training Method</h3>
<p>We aim to learn $G\left(x_{s}\right) \rightarrow\left{x_{a}, m_{a}, d_{a}\right}$, which transforms randomized sim images into canonical sim images with matching semantics, with the intuition that the generator will generalize to accept an image from the real world $x_{r}$, and produce a canonical RGB image, segmentation mask, and depth image: $G\left(x_{r}\right) \rightarrow\left{x_{a}, m_{a}, d_{a}\right}$. To train the generator, we encourage visual equality between the generated $x_{a}$ and target $x_{c}$ through a loss function $l_{e q_{x}}$, semantic equality between $m_{c}$ and $m_{a}$ through a function $l_{e q_{m}}$, and depth equality between $d_{c}$ and $d_{a}$ through a func-
tion $l_{e q_{d}}$. Having experimented with L1, L2, and the mean pairwise squared error (MPSE), our solution uses MPSE for $l_{e q_{x}}$ which was found to converge faster with no loss in performance [5], along with the L2 distance for our auxiliary losses $l_{e q_{m}}$ and $l_{e q_{d}}$. This results in the following loss:</p>
<p>$$
\begin{aligned}
\mathcal{L}<em _left_x__s="\left(x_{s">{e q}(G)= &amp; \mathbb{E}</em>\right)+\right. \
&amp; \left.+\lambda_{m} l_{e q_{m}}\left(G_{m}\left(x_{s}\right), m_{c}\right)+\lambda_{d} l_{e q_{d}}\left(G_{d}\left(x_{s}\right), d_{c}\right)\right]
\end{aligned}
$$}, x_{c}, m_{c}, d_{c}\right)}\left[\lambda_{x} l_{e q_{x}}\left(G_{x}\left(x_{s}\right), x_{c</p>
<p>where $G_{x}, G_{m}$, and $G_{d}$ denotes the image, mask, and depth element of the generator output respectively. In addition, $\lambda_{x}, \lambda_{m}$ and $\lambda_{d}$ represent the respective weightings.</p>
<p>It is well known that these equality losses can lead to blurry images [32], and so we employ a sigmoid-cross entropy generative adversarial (GAN) objective [16] to encourage high-frequency sharpness. Let $D(x)$ be a discriminator that outputs the likelihood that a given image $x$ is from the canonical domain. With this, the GAN is trained with the following objective:</p>
<p>$$
\mathcal{L}<em x="x">{G A N}(G, D)=\mathbb{E}</em>}[\log D(x)]+\mathbb{E<em x="x">{x}\left[\log \left(1-D\left(G</em>(x)\right)\right]\right.
$$</p>
<p>where $G_{x}$ denotes the image element of the generator output. The final objective for the generator then becomes:</p>
<p>$$
\hat{G}=\arg \min <em D="D">{G} \max </em>} \mathcal{L<em e="e" q="q">{G A N}(G, D)+\mathcal{L}</em>(G)
$$</p>
<p>The generator $G$ and discriminator $D$ are parameterized by weights of a convolutional neural network; details of which are presented in Appendix A. Qualitative results of our generator can be seen in Figure 3 and on the project web-page ${ }^{6}$.</p>
<h3>4.3. Real World Grasping with QT-Opt</h3>
<p>We use QT-Opt for our grasping algorithm, and follow the same state and action definition as Kalashnikov et al. [27], where the state is defined as $\mathbf{s}<em t="t">{t}=$ $\left(x</em>$.}, g_{\text {apt }, t}, g_{\text {height }, t}\right)$ at each timestep $t$, which includes a $472 \times 472$ image $x_{t}$ taken from a mounted over-the-shoulder camera overlooking the work space, a binary open/close indicator of gripper aperture $g_{\text {apt }, t}$, and the scalar height of the gripper above the bottom of the tray $g_{\text {height }, t</p>
<p>In our case, rather than sending the image directly to the RL algorithm, the image $x_{t}$ is instead passed through the generator $G$, and the resulting generated image $x_{a}$ is extracted and concatenated, channel-wise, with the original source image $x_{t}$. This results in the state $\mathbf{s}<em t="t">{t}=([G\left(x</em>\right]$ represents the concatenation. Note that we do not use the generated depth and segmentation masks of $G$ as input to QT-Opt in order to make a fair comparison to Kalashnikov et al. [27], though these could also be added in practice. The action space of Kalashnikov et al. [27], which consists of gripper}\right)+$ $\left.x_{t}\right], g_{\text {apt }, t}, g_{\text {height }, t}$ ), where $\left[G\left(x_{t}\right)+x_{t</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Sample outputs of our trained generator $G$ when given randomized sim images (3a) and real images (3b). Note the accuracy of the reconstruction of the canonical images from real-world images in complex and cluttered scenes, along with shadows being re-rendered into the canonical representation. However, also note that randomized-to-canonical adaptation performs a noticeably better reconstruction of the gripper in comparison to the real-to-canonical adaptation. This leads to the failure cases discussed in Section 5. The generated depth and segmentation masks are used as auxiliaries during training of the generator. Further examples can be seen in Figure 8 of the Appendix.
pose displacement and an open/close command, remains unchanged. A summary of the Q-function is shown in Figure 6 of the Appendix, and further details of the action space and architecture can be found in Appendix B.</p>
<p>In Kalashnikov et al. [27], the authors take their agent that was trained with 580,000 off-policy real-world grasps, and jointly finetune with an additional 28,000 on-policy grasps. During this joint finetuning process, QT-Opt asynchronously updates target values, collects real on-policy data, reloads real off-policy (offline) data from past experiences, and then trains the Q-network on both the on and off policy data streams within a distributed optimization framework. In the case of jointly finetuning $R C A N$, we also collect real on-policy data, but rather than using real-world past experiences (which we assume we do not have), we instead leverage the power of our simulation to continuously generate on-policy simulation data, and instead train on these streams of data. During the real world on-policy collection of both approaches, a selection of about 1,000 diverse training objects are used; a sample of which are shown in Figure 5 of the Appendix. Between 5 and 10 objects are randomly chosen every few hours to be placed in each of the trays until the desired number of joint finetuning grasps are reached.</p>
<h2>5. Experiments</h2>
<p>Our experimental section aims to answer the following questions: (1) Can we train an agent to grasp arbitrary unseen objects without having seen any real-world images?
(2) How does QT-Opt perform with standard domain randomization, and can our method perform better than this? (3) Does the addition of real-world on-policy training of our method lead to higher grasping performance while still drastically reducing the amount of real-world data required? We answer these questions through a series of rigorous realworld vision-based grasping experiments across multiple Kuka IIWA robots.</p>
<h3>5.1. Evaluation Protocol</h3>
<p>During evaluation, each robot attempts 102 grasps on its own set of 5 to 6 previously unseen test objects (shown in Figure 5 of the Appendix) which are deposited into each robots' respective tray and remain constant across all evaluations. Each grasp attempt (episode) consists of at most 20 time steps. If after 20 time steps no object has been grasped, the attempt is regarded as a failure. Following a grasp attempt, the object is deposited back into the tray at a random location. Although grasping was done with replacement, in practice, QT-Opt was not found attempting a grasp on the same object multiple times in a row. All observations come from an over-the-shoulder RGB camera.</p>
<h3>5.2. Results</h3>
<p>We first focus on the first 4 columns of Table 1. The first row of this section shows the results of QT-Opt reported in Kalashnikov et al. [27]; where following 580,000 off-policy real-world grasps, a performance of $87 \%$ was achieved.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">QT-Opt Data Source</th>
<th style="text-align: center;">Offline</th>
<th style="text-align: center;">Performance</th>
<th style="text-align: center;">Performance</th>
<th style="text-align: center;">Online</th>
<th style="text-align: center;">Performance</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Real Grasps</td>
<td style="text-align: center;">In Sim</td>
<td style="text-align: center;">In Real</td>
<td style="text-align: center;">Real Grasps</td>
<td style="text-align: center;">In Real</td>
</tr>
<tr>
<td style="text-align: center;">Real</td>
<td style="text-align: center;">580,000</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$87 \%$</td>
<td style="text-align: center;">$\begin{gathered} +5,000 \ +28,000 \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 85 \% \ 96 \% \end{gathered}$</td>
</tr>
<tr>
<td style="text-align: center;">Canonical Sim</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">$21 \%$</td>
<td style="text-align: center;">$+5,000$</td>
<td style="text-align: center;">$30 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Mild Randomization</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$98 \%$</td>
<td style="text-align: center;">$37 \%$</td>
<td style="text-align: center;">$+5,000$</td>
<td style="text-align: center;">$85 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Medium Randomization</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$98 \%$</td>
<td style="text-align: center;">$35 \%$</td>
<td style="text-align: center;">$+5,000$</td>
<td style="text-align: center;">$77 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Heavy Randomization</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$98 \%$</td>
<td style="text-align: center;">$33 \%$</td>
<td style="text-align: center;">$\begin{gathered} +5,000 \ +28,000 \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 85 \% \ 92 \% \end{gathered}$</td>
</tr>
<tr>
<td style="text-align: center;">RCAN</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$99 \%$</td>
<td style="text-align: center;">70\%</td>
<td style="text-align: center;">$\begin{gathered} +5,000 \ +28,000 \end{gathered}$</td>
<td style="text-align: center;">$\begin{gathered} 91 \% \ 94 \% \end{gathered}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Average grasp success rate on test objects after 102 grasp attempts on each of the multiple Kuka IIWA robots. The first 4 columns of the table highlight the performance after training on a specified number of real world grasps. Zero grasps implies that all training was done in simulation. The last 2 columns highlight the results of on-policy joint finetuning on a small amount of real-world grasps.</p>
<p>The Canonical Sim data source (second row) takes QT-Opt trained in the canonical simulation environment and then runs this directly in the real-world. The low success rate of $21 \%$ shows the existence of the reality gap. The following three rows show the result of training QT-Opt directly on varying degrees of randomization: mild, medium and heavy. Mild randomization consists of varying tray texture, object texture and color, robot arm color, lighting direction and brightness, and a background image consisting of 6 different images from the view of the real-world camera. Medium randomization adds a diverse mix of background images to the floor. Finally, heavy randomization uses the same scheme used to train RCAN, explained in Section 4.1.</p>
<p>Surprisingly, an unexpected discovery was that QT-Opt responds well to heavy domain randomization during training (i.e. is not destabilized). This is contrary to other RL methods, such as DDPG [35] and A3C [39], where heavy domain randomization has been shown to cause training to fail [38, 70]. Although QT-Opt was able to train stably with randomization, the results show that this does not lead to a successful transfer, achieving between $33 \%$ and $37 \%$ zeroshot grasping performance, whereas RCAN achieves 70\%: over double the success in the real world. This success highlights that RCAN better utilizes domain randomization to achieve sim-to-real transfer, rather than training a policy directly on domain randomization.</p>
<p>We now focus on the remaining 2 columns, that is, the ability to jointly finetune on a small amount of realworld on-policy grasps. We chose to use 5,000 to represent "small", which is less than $1 \%$ of the 580,000 grasps used in Kalashnikov et al. [27] for the off-policy training and takes only a day to collect, instead of months. To make comparison easier, in addition to reporting the 28,000 on-
policy grasps for joint finetuning from [27], we also report the performance after 5,000 grasps. This baseline result of $85 \%$ suggest that 5,000 real-world grasps for joint finetuning a system already trained with 580,000 does not improve performance. For the next joint finetuning experiment, we take each of the agents that were trained directly on domain randomization, and jointly finetune them on 5,000 real grasps, achieving between $77 \%$ and $85 \%$ grasping success. The rapid increase of $\sim 50$ p.p. is very surprising, and to the best of our knowledge, no other related works have shown such a dramatic performance increase from pre-training on domain randomization.</p>
<p>Finally, we look at joint finetuning RCAN with 5,000 and 28,000 real grasps, where the real images are adapted by the generator and then both the source and adapted image are passed to the grasping network; in this case, the gradients are only applied to the grasping network and not the generator network. The result of $91 \%$ for 5,000 shows that the improvement over learning directly on domain randomization holds, though for this result the difference is much smaller. What we believe is incredibly encouraging for the robotics community, is that with $\mathbf{9 1 \%}$ RCAN outperforms a version of QT-Opt that was trained on 580,000 real-world grasps, while using less than $\mathbf{1 \%}$ of the data. Moreover, following joint finetuning with with the same number of online grasps as Kalashnikov et al. [27] (28,000), we are able to achieve an almost equal grasp performance of $94 \%$.</p>
<p>In order to understand how performance varies as we progress from 0 to 5,000 on-policy grasps, we repeat the evaluation protocol set above for intermediate checkpoints. We re-evaluate both agents at every 1,000 grasps for both RCAN and Mild Randomization. The results, presented in Figure 4, show that the majority of the success is gained within the first 2,000 grasps for both approaches. This is</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: A graph showing how the performance of $R C A N$ and directly learning a policy on domain randomization varies with the number of real world on-policy grasps.
encouraging, as we ultimately wish to limit the amount of real-world data that we are reliant on.</p>
<h3>5.3. Failure cases</h3>
<p>A large contributing factor to QT-Opts 96\% grasp success, was its ability to perform corrective behaviors, regrasping, probing motions to ascertain the best grasp, and non-prehensile repositioning of objects. Much of this ability remained with our approach, except for the regrasping ability. This powerful ability allows the policy to detect when there is no object in the closed gripper, and thus, it can decide to re-open it in an attempt to try and re-grasp. Given that our method is not perfect at translating real-world images into simulation ones, artifacts may arise. As objects that we grasp are often small, it can be very difficult for the agent to differentiate between artifacts in the image or if there is indeed an object in the gripper. We observe this to be detrimental to the agents ability to perform regrasping, resulting in only a small amount of regrasps. The main observation from joint finetuning our method with 5,000 realworld grasps, is the re-emergence of the regrasping. We believe that this is contributed by our decision to concatenate the source image to the generated ones, and thus giving the grasping algorithm the option to choose which data source to extract information from for each part of the image as the joint finetuning continues. We hypothesize, that as the number of joint finetuning grasps increase, the network would eventually learn to solely rely on the source (real-world) image, rather than the adapted simulation image. However, we believe that, with a limited amount of labeled real-world data, feeding both the output of $R C A N$ as well as the original image to the agent offers the best combination of a simplified, yet potentially incomplete adapted view and the complex, but complete original real-world view.</p>
<h3>5.4. Discussion</h3>
<p>A number of questions arise from these results. For example: why does our method perform better than learning a policy directly with domain randomization? We hypothesize that our method allows offloading visual complexity to the generator network, thus simplifying the task for the
grasping network and in turn, leading to a higher grasping success. Moreover, having a chosen canonical environment allows us to impose structure on the task which may be beneficial for training the grasping network.. Despite our method achieving over double the zero-shot performance in the real world in comparison to domain randomization, with 5,000 additional real-world grasps, the performance of direct domain randomization also achieves a surprisingly high performance. This leads us to the hypothesis that learning a policy directly on domain randomization can act as a very powerful pre-training regime, where the network is forced to learn a very general feature extractor that can be easily jointly finetuned to a new environment. Having said that, our method outperforms this and has the added benefit of giving us an interpretable output for sim-to-real transfer.</p>
<p>Another question for future work would be: is there a way to better utilize the data collected during the 5,000 onpolicy grasps? Given this real-world data, it is now possible to consider fusing ideas from other transfer methods that require some real-world data, such as PixelDA [5].</p>
<h2>6. Conclusion</h2>
<p>We have presented Randomized-to-Canonical Adaptation Networks ( $R C A N$ ), a sim-to-real method that learns to translate randomized simulation images into a canonical representation, which in turn allows for real-world images to also be translated to this canonical representation. Given that our grasping algorithm ( $Q T$-Opt) is trained in this canonical environment, it is possible to run policies trained in simulation in the real world. We show that this approach is superior to the common domain randomization approach, and argue that it is a much more meaningful use of domain randomization. This general style of transfer has applications beyond just grasping, and can be used in other settings where real world data is expensive to collect, for example, producing segmentation masks for self-driving cars. For future work, we wish to explore further ways of introducing unlabelled real-world data in order to improve the real-to-canonical translation. Moreover, we are interested in exploring the effect of using the auxiliary outputs as additional inputs to the grasping network.</p>
<h2>Acknowledgments</h2>
<p>We would like to give special thanks to Ivonne Fajardo, Peter Pastor, Iñaki Gonzalo and Benjamin Swanson for overseeing the robot operations, Yunfei Bai for discussion on PyBullet, and Serkan Cabi for valuable comments on the paper.</p>
<h2>References</h2>
<p>[1] R. Antonova, S. Cruciani, C. Smith, and D. Kragic. Reinforcement learning for pivoting task. arXiv:1703.00472,</p>
<ol>
<li></li>
</ol>
<p>[2] J. Bohg, A. Morales, T. Asfour, and D. Kragic. Data-driven grasp synthesis-a survey. IEEE Transactions on Robotics, 30(2):289-309, 2014.
[3] K. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kelcey, M. Kalakrishnan, L. Downs, J. Ibarz, P. Pastor, K. Konolige, S. Levine, and V. Vanhoucke. Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping. IEEE Intl. Conference on Robotics and Automation, 2018.
[4] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Krishnan. Unsupervised pixel-level domain adaptation with generative adversarial neural networks. In IEEE Conference on Computer Vision and Pattern Recognition, 2017.
[5] K. Bousmalis, G. Trigeorgis, N. Silberman, D. Krishnan, and D. Erhan. Domain separation networks. Advances in Neural Information Processing Systems, 2016.
[6] P. Brook, M. Ciocarlie, and K. Hsiao. Collaborative grasp planning with multiple object representations. IEEE Intl. Conference on Robotics and Automation, 2011.
[7] R. Caseiro, J. F. Henriques, P. Martins, and J. Batista. Beyond the shortest path: Unsupervised Domain Adaptation by Sampling Subspaces Along the Spline Flow. In IEEE Conference on Computer Vision and Pattern Recognition, 2015.
[8] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, J. Xiao, L. Yi, and F. Yu. ShapeNet: An information-rich 3D model repository. arXiv:1512.03012, 2015.
[9] Y. Chebotar, A. Handa, V. Makoviychuk, M. Macklin, J. Issac, N. Ratliff, and D. Fox. Closing the sim-to-real loop: Adapting simulation randomization with real world experience. arXiv:1810.05687, 2018.
[10] Q. Chen and V. Koltun. Photographic image synthesis with cascaded refinement networks. Intl. Conference on Computer Vision, 2017.
[11] M. Ciocarlie, K. Hsiao, E. G. Jones, S. Chitta, R. B. Rusu, and I. A. Şucan. Towards reliable grasping and manipulation in household environments. In Experimental Robotics, pages 241-252. Springer, 2014.
[12] E. Coumans and Y. Bai. Pybullet, a python module for physics simulation for games, robotics and machine learning. http://pybullet.org, 2016-2018.
[13] G. Csurka. Domain adaptation for visual applications: A comprehensive survey. arxiv:1702.05374, 2017.
[14] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and V. Lempitsky. Domainadversarial training of neural networks. The Journal of Machine Learning Research, 2016.
[15] B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic flow kernel for unsupervised domain adaptation. In IEEE Conference on Computer Vision and Pattern Recognition, 2012.
[16] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. Advances in Neural Information Processing Systems, 2014.
[17] R. Gopalan, R. Li, and R. Chellappa. Domain Adaptation for Object Recognition: An Unsupervised Approach. In Intl. Conference on Computer Vision, 2011.
[18] M. Gualtieri, A. ten Pas, K. Saenko, and R. Platt. High precision grasp pose detection in dense clutter. In IEEE Intl. Conference on Intelligent Robots and Systems, pages 598605, 2016.
[19] C. Hernandez, M. Bharatheesha, W. Ko, H. Gaiser, J. Tan, K. van Deurzen, M. de Vries, B. Van Mil, J. van Egmond, R. Burger, et al. Team delfts robot winner of the amazon picking challenge 2016. In Robot World Cup, pages 613624. Springer, 2016.
[20] S. Hinterstoisser, S. Holzer, C. Cagniart, S. Ilic, K. Konolige, N. Navab, and V. Lepetit. Multimodal templates for real-time detection of texture-less objects in heavily cluttered scenes. Intl. Conference on Computer Vision, 2011.
[21] J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko, A. A. Efros, and T. Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In Intl. Conference on Machine Learning, 2018.
[22] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. Intl. Conference on Machine Learning, 2015.
[23] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-toimage translation with conditional adversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition, 2017.
[24] S. James, M. Bloesch, and A. J. Davison. Task-embedded control networks for few-shot imitation learning. Conference on Robot Learning, 2018.
[25] S. James, A. J. Davison, and E. Johns. Transferring end-toend visuomotor control from simulation to real world for a multi-stage task. Conference on Robot Learning, 2017.
[26] S. James and E. Johns. 3d simulation for robot arm control with deep q-learning. NeurIPS Workshop on Deep Learning for Action and Interaction, 2016.
[27] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly, M. Kalakrishnan, V. Vanhoucke, and S. Levine. QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation. Conference on Robot Learning, 2018.
[28] D. Kappler, J. Bohg, and S. Schaal. Leveraging big data for grasp planning. IEEE Intl. Conference on Robotics and Automation, 2015.
[29] B. Kehoe, A. Matsukawa, S. Candido, J. Kuffner, and K. Goldberg. Cloud-based robot grasping with the google object recognition engine. In IEEE Intl. Conference on Robotics and Automation, 2013.
[30] T. Kim, M. Cha, H. Kim, J. K. Lee, and J. Kim. Learning to discover cross-domain relations with generative adversarial networks. Intl. Conference on Machine Learning, 2017.
[31] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. Advances in Neural Information Processing Systems, 2012.
[32] A. B. L. Larsen, S. K. Sønderby, H. Larochelle, and O. Winther. Autoencoding beyond pixels using a learned similarity metric. Intl. Conference on Machine Learning, 2016.
[33] I. Lenz, H. Lee, and A. Saxena. Deep learning for detecting robotic grasps. The International Journal of Robotics Research, 34(4-5):705-724, 2015.</p>
<p>[34] S. Levine, P. Pastor, A. Krizhevsky, and D. Quillen. Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection. International Symposium on Experimental Robotics, 2016.
[35] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. arXiv:1509.02971, 2015.
[36] M. Long and J. Wang. Learning transferable features with deep adaptation networks. Intl. Conference on Machine Learning, 2015.
[37] J. Mahler, J. Liang, S. Niyaz, M. Laskey, R. Doan, X. Liu, J. A. Ojea, and K. Goldberg. Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics. Robotics: Science and Systems, 2017.
[38] J. Matas, S. James, and A. J. Davison. Sim-to-real reinforcement learning for deformable object manipulation. Conference on Robot Learning, 2018.
[39] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. Intl. Conference on Machine Learning, 2016.
[40] I. Mordatch, K. Lowrey, and E. Todorov. Ensemble-cio: Full-body dynamic motion planning that transfers to physical humanoids. IEEE Intl. Conference on Intelligent Robots and Systems, 2015.
[41] D. Morrison, A. W. Tow, M. McTaggart, R. Smith, N. KellyBoxall, S. Wade-McCue, J. Erskine, R. Grinover, A. Gurman, T. Hunn, et al. Cartman: The low-cost cartesian manipulator that won the amazon robotics challenge. IEEE Intl. Conference on Robotics and Automation, 2018.
[42] V. M. Patel, R. Gopalan, R. Li, and R. Chellappa. Visual domain adaptation: A survey of recent advances. IEEE Signal Processing Magazine, 32(3):53-69, 2015.
[43] X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel. Sim-to-real transfer of robotic control with dynamics randomization. In IEEE Intl. Conference on Robotics and Automation. IEEE, 2018.
[44] L. Pinto and A. Gupta. Supersizing self-supervision: Learning to grasp from 50K tries and 700 robot hours. IEEE Intl. Conference on Robotics and Automation, 2016.
[45] D. Prattichizzo and J. C. Trinkle. Grasping. In Springer handbook of robotics, pages 671-700. Springer, 2008.
[46] A. Rajeswaran, S. Ghotra, B. Ravindran, and S. Levine. Epopt: Learning robust neural network policies using model ensembles. Intl. Conference on Learning Representations, 2017.
[47] A. Rodriguez, M. T. Mason, and S. Ferry. From caging to grasping. The International Journal of Robotics Research, 31(7):886-900, 2012.
[48] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In Intl. Conference on Medical Image Computing and ComputerAssisted Intervention. Springer, 2015.
[49] R. Y. Rubinstein and D. P. Kroese. The cross-entropy method: A unified approach to monte carlo simulation, randomized optimization and machine learning. Information Science \&amp; Statistics, 2004.
[50] A. A. Rusu, M. Vecerik, T. Rothörl, N. Heess, R. Pascanu, and R. Hadsell. Sim-to-real robot learning from pixels with progressive nets. Conference on Robot Learning, 2017.
[51] F. Sadeghi and S. Levine. CAD2RL: Real single-image flight without a single real image. In Robotics: Science and Systems, 2017.
[52] F. Sadeghi, A. Toshev, E. Jang, and S. Levine. Sim2real viewpoint invariant visual servoing by recurrent control. In IEEE Conference on Computer Vision and Pattern Recognition, 2018.
[53] A. Saxena, J. Driemeyer, and A. Y. Ng. Robotic grasping of novel objects using vision. The Intl. Journal of Robotics Research, 2008.
[54] A. Shrivastava, T. Pfister, O. Tuzel, J. Susskind, W. Wang, and R. Webb. Learning from simulated and unsupervised images through adversarial training. In IEEE Conference on Computer Vision and Pattern Recognition, 2017.
[55] R. Shu, H. Bui, H. Narui, and S. Ermon. A DIRT-t approach to unsupervised domain adaptation. In Intl. Conference on Learning Representations, 2018.
[56] G. J. Stein and N. Roy. Genesis-rt: Generating synthetic images for training secondary real-world tasks. In IEEE Intl. Conference on Robotics and Automation, 2018.
[57] B. Sun, J. Feng, and K. Saenko. Return of frustratingly easy domain adaptation. In Association for the Advancement of Artificial Intelligence, 2016.
[58] N. Sünderhauf, O. Brock, W. Scheirer, R. Hadsell, D. Fox, J. Leitner, B. Upcroft, P. Abbeel, W. Burgard, M. Milford, et al. The limits and potentials of deep learning for robotics. The Intl. Journal of Robotics Research, 37(4-5):405-420, 2018.
[59] Y. Taigman, A. Polyak, and L. Wolf. Unsupervised crossdomain image generation. Intl. Conference on Learning Representations, 2017.
[60] A. ten Pas, M. Gualtieri, K. Saenko, and R. Platt. Grasp pose detection in point clouds. The International Journal of Robotics Research, 36(13-14):1455-1473, 2017.
[61] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In IEEE Intl. Conference on Intelligent Robots and Systems, 2017.
[62] E. Tzeng, C. Devin, J. Hoffman, C. Finn, P. Abbeel, S. Levine, K. Saenko, and T. Darrell. Adapting deep visuomotor representations with weak pairwise constraints. Workshop on the Algorithmic Foundations of Robotics, 2016.
[63] D. Ulyanov, A. Vedaldi, and V. Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv:1607.08022, 2016.
[64] U. Viereck, A. t. Pas, K. Saenko, and R. Platt. Learning a visuomotor controller for real world robotic grasping using easily simulated depth images. In Conference on Robot Learning, 2017.
[65] U. Viereck, A. ten Pas, K. Saenko, and R. Platt. Learning a visuomotor controller for real world robotic grasping using simulated depth images. Conference on Robot Learning, 2017.</p>
<p>[66] Z. Yi, H. R. Zhang, P. Tan, and M. Gong. Dualgan: Unsupervised dual learning for image-to-image translation. Intl. Conference on Computer Vision, 2017.
[67] D. Yoo, N. Kim, S. Park, A. S. Paek, and I. S. Kweon. PixelLevel Domain Transfer. European Conference on Computer Vision, 2016.
[68] W. Yu, J. Tan, C. K. Liu, and G. Turk. Preparing for the unknown: Learning a universal policy with online system identification. In Robotics: Science and Systems, 2017.
[69] A. Zeng, S. Song, S. Welker, J. Lee, A. Rodriguez, and T. Funkhouser. Learning synergies between pushing and grasping with self-supervised deep reinforcement learning. IEEE Intl. Conference on Intelligent Robots and Systems, 2018.
[70] J. Zhang, L. Tai, Y. Xiong, M. Liu, J. Boedecker, and W. Burgard. Vr goggles for robots: Real-to-sim domain adaptation for visual control. IEEE Robotics and Automation Letters, 2019.
[71] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired imageto-image translation using cycle-consistent adversarial networks. Intl. Conference on Computer Vision, 2017.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Real-world grasping objects that range greatly in size and appearance. Left: about 1000 visually and physically diverse training objects used for joint finetuning. Right: the unseen test objects.</p>
<h2>A. RCAN Architecture</h2>
<p>The generator $G$ is parameterized by weights of a convolutional neural network, summarized in Figure 7, and follows a U-Net style architecture [48] with downsampling performed via $3 \times 3$ convolutions with stride 2 for the first 2 layers, and average pooling with $3 \times 3$ convolution of stride 1 for the remaining layers. Upsampling was performed via bilinear upsampling, followed by a $3 \times 3$ convolutions of stride 1 , and skip connections were fused back into the network via channel-wise concatenation, followed by a $1 \times 1$ convolution. All layers were followed by instance normalization [63] and ReLU non-linearities. The discriminator $D$ is also parameterized by weights of a convolutional neural network with 2 layers of $32,3 \times 3$ filters, followed by a layer of $64,3 \times 3$ filters, and finally a layer of $128,3 \times 3$ filters. The network follows a multi-scale patch-based design [3], where 3 scales of $472 \times 472,236 \times 236$, and $118 \times 118$, are used to produce domain estimates for all patches which are then combined to compute the joint discriminator loss. The weightings $\lambda_{x}, \lambda_{m}$ and $\lambda_{d}$ in Equation 2 were all set to 1 .</p>
<h2>B. QT-Opt Architecture</h2>
<p>The action space of [27], which consists of gripper pose displacement and an open/close command, remains unchanged in our paper, and is defined as $\mathbf{a}<em t="t">{t}=$ $\left(\mathbf{t}</em>}, \mathbf{r<em _close="{close" _text="\text">{t}, g</em>}, t}, g_{\text {open }, t}, e_{t}\right)$, containing Cartesian translation $\mathbf{t<em t="t">{t} \in \mathbb{R}^{3}$, sine-cosine rotation encoding $\mathbf{r}</em>$. The reward function is sparse, consisting of a reward of 1 following a
} \in \mathbb{R}^{2}$, a onehot vector gripper open/close command $\left[g_{\text {close }, t}, g_{\text {open }, t}\right] \in$ ${0,1}^{2}$, and a learned stopping criterion $e_{t<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: The Q-function of the grasping algorithm. The source image $x$ (either from the randomized domain or realworld domain) and generated canonical image $x_{a}$ are concatenated (channel-wise) and processed by a convolutional neural network (and fused with action and state variables) to produce a scalar representing the Q value $Q_{\theta}(s, a)$.
successful grasp, or 0 for an unsuccessful grasp, and -0.05 on all other transitions. Summarized in Figure 6, the Qfunction follows the same architecture as [27] (originally inspired by [34]).</p>
<p>Rather than a single RGB image input, our network takes in a 6 channel image, consisting of channel-wise concatenation of the source image $x$ (either from the randomized domain or real-world domain) and generated image $x_{a}$. Features are extracted from these images via 7 convolutional layers and then merged with a transformed action and state vector (which have passed through 2 fully-connected layers) via element-wise addition. The merged streams are then processed by a further 9 convolution layers and 2 fullyconnected layers, resulting in a scalar output representing the Q value $Q_{\theta}(s, a)$. Each layer, excluding the final, uses batch normalization [22] and ReLU non-linearities. A summary of the architecture can be seen in Figure 6.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Network architecture of the generator function $G$. An RGB image from the source domain (either from the randomized domain or real-world domain) is processed via a U-Net style architecture [48] to produce a generated RGB image $x_{a}$, and auxiliaries that includes a segmentation mask $m_{a}$ and depth image $d_{a}$. These auxiliaries forces the generator to extract semantic and depth information about the scene and encode them in the intermediate latent representation, which is then available during the generation of the output image.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" />
(b) Real-to-canonical samples.</p>
<p>Figure 8: Additional sample outputs of our trained generator $G$ when given randomized sim images (8a) and real images $(8 b)$.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ https://sites.google.com/view/rcan/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
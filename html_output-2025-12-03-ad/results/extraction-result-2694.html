<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2694 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2694</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2694</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-70.html">extraction-schema-70</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <p><strong>Paper ID:</strong> paper-1c8cab1cba88b270ddcc23f586aaac90b6741cb7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1c8cab1cba88b270ddcc23f586aaac90b6741cb7" target="_blank">Situated language learning via interactive narratives</a></p>
                <p><strong>Paper Venue:</strong> Patterns</p>
                <p><strong>Paper TL;DR:</strong> This paper provides a roadmap that explores the question of how to imbue learning agents with the ability to understand and generate contextually relevant natural language in service of achieving a goal, and hypothesizes that two key components in creating such agents are interactivity and environment grounding.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2694.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2694.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Textual-SLAM / map memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Textual-SLAM / structured map memory for text games</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Conceptual formulation of a mapping / memory problem for text-adventure domains where agents must construct and use maps / structured memories (locations, connectivity, objects) to localize and navigate in partially observable textual worlds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>textual-SLAM / mapping-enabled text agent (general)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A generic class of text-game agents that maintain an explicit map / structured memory of visited locations, connectivity, and local object lists to support navigation and decision-making; described at a conceptual level in this paper rather than instantiated as a single specific LM.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Jericho / TextWorld (general text-adventure benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Text-adventure games and procedurally generated text-worlds with partial observability, many locations, nested objects, and long-horizon puzzle structure requiring navigation and state tracking.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>map / structured memory (graph-like memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>explicit map / graph of locations and connectivity, with per-location lists of objects (supports nested containment)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>discovered locations, connectivity (edges), objects present at locations, nested object relationships, and indicators of whether a place was previously seen</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>updated as agent explores (recording new locations/objects and connectivity when navigational actions succeed and observations are received)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>to localize the agent, avoid getting lost, plan navigation to previously seen locations, and track object locations and nesting needed for long-horizon puzzle solving</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>The paper states that humans create structured memory aids (maps) and that creation of such memory aids has been shown to be critical for automated agents to operate in textual worlds — enabling efficient navigation and avoidance of getting lost; Textual-SLAM frames the need to detect action success/failure and whether a location is previously seen.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Situated language learning via interactive narratives', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2694.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2694.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dynamic Knowledge Graphs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning dynamic knowledge graphs to generalize on text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-based memory approach (cited) that incrementally builds a dynamic knowledge graph of the environment to help agents generalize and track state in text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning dynamic knowledge graphs to generalize on textbased games.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>dynamic-knowledge-graph-enabled agent (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An agent architecture (described in the cited work) that constructs and maintains a dynamic knowledge graph representing entities, relations, and state discovered while interacting with text games to support policy learning and generalization; referenced in this paper as an example of memory aiding agents.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>text-based games (general; referenced works evaluate on standard IF benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Partially observable text-adventure games requiring tracking of entities, relations, and long-horizon dependencies to complete quests.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>dynamic knowledge graph (graph-based memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>incrementally-updated graph (nodes = entities/locations/objects, edges = relations/containment/connectivity)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>entities, relations between entities and locations, object containment, discovered facts about the world useful for planning and generalization</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>graph is updated as new observations are made and as interactions reveal relations and connectivity (dynamic incremental updates)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>to represent and generalize environment state, support planning and action selection across long-horizon dependencies</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Cited as an approach that helps agents generalize in text-based games by maintaining structured state; the paper references this work as evidence that structured memory aids are beneficial but does not report numerical comparisons itself.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Situated language learning via interactive narratives', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2694.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2694.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-based RL / Graph Constrained RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-based deep reinforcement learning / Graph Constrained Reinforcement Learning for natural language action spaces</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Graph-constrained or graph-based RL approaches (cited) leverage an explicit graph-like memory of the environment (maps, object locations) to constrain action selection and improve performance in large natural language action spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Playing text-adventure games with graph-based deep reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>graph-constrained RL agent (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A reinforcement learning agent that uses an explicit graph representation of the environment (constructed from observations) to limit and guide action choices in combinatorial natural language action spaces; referenced in this paper as an approach that uses structured memory to aid agents.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Jericho / human-made interactive fiction games (examples cited in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Parser-based interactive fiction games with combinatorially large language action spaces, partial observability, and long-term puzzles requiring inventory and location-aware reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>graph / map memory integrated with RL (graph-constrained memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>explicit graph/map structure used to constrain action generation and track visited states and object locations</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>visited locations, connectivity, inventories, object placements — used to prune or prioritize actions</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>updated during exploration as the agent discovers new locations and object relations</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>to constrain the combinatorial action space, assist exploration across bottlenecks, and track long-term dependencies (e.g., whether prerequisite items/conditions have been satisfied)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Paper claims (cited works) that creating such structured memory aids is critical for automated agents to navigate and operate in textual worlds, especially for overcoming long-range dependencies and avoiding getting stuck; no quantitative results are provided in this survey paper itself.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Situated language learning via interactive narratives', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Playing text-adventure games with graph-based deep reinforcement learning. <em>(Rating: 2)</em></li>
                <li>Learning dynamic knowledge graphs to generalize on textbased games. <em>(Rating: 2)</em></li>
                <li>Graph Constrained Reinforcement Learning for Natural Language Action Spaces. <em>(Rating: 2)</em></li>
                <li>Enhancing text-based reinforcement learning agents with commonsense knowledge. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2694",
    "paper_id": "paper-1c8cab1cba88b270ddcc23f586aaac90b6741cb7",
    "extraction_schema_id": "extraction-schema-70",
    "extracted_data": [
        {
            "name_short": "Textual-SLAM / map memory",
            "name_full": "Textual-SLAM / structured map memory for text games",
            "brief_description": "Conceptual formulation of a mapping / memory problem for text-adventure domains where agents must construct and use maps / structured memories (locations, connectivity, objects) to localize and navigate in partially observable textual worlds.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "agent_name": "textual-SLAM / mapping-enabled text agent (general)",
            "agent_description": "A generic class of text-game agents that maintain an explicit map / structured memory of visited locations, connectivity, and local object lists to support navigation and decision-making; described at a conceptual level in this paper rather than instantiated as a single specific LM.",
            "base_model_size": null,
            "game_benchmark_name": "Jericho / TextWorld (general text-adventure benchmarks)",
            "game_description": "Text-adventure games and procedurally generated text-worlds with partial observability, many locations, nested objects, and long-horizon puzzle structure requiring navigation and state tracking.",
            "uses_memory": true,
            "memory_type": "map / structured memory (graph-like memory)",
            "memory_structure": "explicit map / graph of locations and connectivity, with per-location lists of objects (supports nested containment)",
            "memory_content": "discovered locations, connectivity (edges), objects present at locations, nested object relationships, and indicators of whether a place was previously seen",
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": "updated as agent explores (recording new locations/objects and connectivity when navigational actions succeed and observations are received)",
            "memory_usage_purpose": "to localize the agent, avoid getting lost, plan navigation to previously seen locations, and track object locations and nesting needed for long-horizon puzzle solving",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": "The paper states that humans create structured memory aids (maps) and that creation of such memory aids has been shown to be critical for automated agents to operate in textual worlds — enabling efficient navigation and avoidance of getting lost; Textual-SLAM frames the need to detect action success/failure and whether a location is previously seen.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2694.0",
            "source_info": {
                "paper_title": "Situated language learning via interactive narratives",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Dynamic Knowledge Graphs",
            "name_full": "Learning dynamic knowledge graphs to generalize on text-based games",
            "brief_description": "A graph-based memory approach (cited) that incrementally builds a dynamic knowledge graph of the environment to help agents generalize and track state in text-based games.",
            "citation_title": "Learning dynamic knowledge graphs to generalize on textbased games.",
            "mention_or_use": "mention",
            "agent_name": "dynamic-knowledge-graph-enabled agent (as cited)",
            "agent_description": "An agent architecture (described in the cited work) that constructs and maintains a dynamic knowledge graph representing entities, relations, and state discovered while interacting with text games to support policy learning and generalization; referenced in this paper as an example of memory aiding agents.",
            "base_model_size": null,
            "game_benchmark_name": "text-based games (general; referenced works evaluate on standard IF benchmarks)",
            "game_description": "Partially observable text-adventure games requiring tracking of entities, relations, and long-horizon dependencies to complete quests.",
            "uses_memory": true,
            "memory_type": "dynamic knowledge graph (graph-based memory)",
            "memory_structure": "incrementally-updated graph (nodes = entities/locations/objects, edges = relations/containment/connectivity)",
            "memory_content": "entities, relations between entities and locations, object containment, discovered facts about the world useful for planning and generalization",
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": "graph is updated as new observations are made and as interactions reveal relations and connectivity (dynamic incremental updates)",
            "memory_usage_purpose": "to represent and generalize environment state, support planning and action selection across long-horizon dependencies",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": "Cited as an approach that helps agents generalize in text-based games by maintaining structured state; the paper references this work as evidence that structured memory aids are beneficial but does not report numerical comparisons itself.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2694.1",
            "source_info": {
                "paper_title": "Situated language learning via interactive narratives",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Graph-based RL / Graph Constrained RL",
            "name_full": "Graph-based deep reinforcement learning / Graph Constrained Reinforcement Learning for natural language action spaces",
            "brief_description": "Graph-constrained or graph-based RL approaches (cited) leverage an explicit graph-like memory of the environment (maps, object locations) to constrain action selection and improve performance in large natural language action spaces.",
            "citation_title": "Playing text-adventure games with graph-based deep reinforcement learning.",
            "mention_or_use": "mention",
            "agent_name": "graph-constrained RL agent (as cited)",
            "agent_description": "A reinforcement learning agent that uses an explicit graph representation of the environment (constructed from observations) to limit and guide action choices in combinatorial natural language action spaces; referenced in this paper as an approach that uses structured memory to aid agents.",
            "base_model_size": null,
            "game_benchmark_name": "Jericho / human-made interactive fiction games (examples cited in the paper)",
            "game_description": "Parser-based interactive fiction games with combinatorially large language action spaces, partial observability, and long-term puzzles requiring inventory and location-aware reasoning.",
            "uses_memory": true,
            "memory_type": "graph / map memory integrated with RL (graph-constrained memory)",
            "memory_structure": "explicit graph/map structure used to constrain action generation and track visited states and object locations",
            "memory_content": "visited locations, connectivity, inventories, object placements — used to prune or prioritize actions",
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": "updated during exploration as the agent discovers new locations and object relations",
            "memory_usage_purpose": "to constrain the combinatorial action space, assist exploration across bottlenecks, and track long-term dependencies (e.g., whether prerequisite items/conditions have been satisfied)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": "Paper claims (cited works) that creating such structured memory aids is critical for automated agents to navigate and operate in textual worlds, especially for overcoming long-range dependencies and avoiding getting stuck; no quantitative results are provided in this survey paper itself.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": false,
            "best_memory_configuration": null,
            "uuid": "e2694.2",
            "source_info": {
                "paper_title": "Situated language learning via interactive narratives",
                "publication_date_yy_mm": "2021-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning.",
            "rating": 2,
            "sanitized_title": "playing_textadventure_games_with_graphbased_deep_reinforcement_learning"
        },
        {
            "paper_title": "Learning dynamic knowledge graphs to generalize on textbased games.",
            "rating": 2,
            "sanitized_title": "learning_dynamic_knowledge_graphs_to_generalize_on_textbased_games"
        },
        {
            "paper_title": "Graph Constrained Reinforcement Learning for Natural Language Action Spaces.",
            "rating": 2,
            "sanitized_title": "graph_constrained_reinforcement_learning_for_natural_language_action_spaces"
        },
        {
            "paper_title": "Enhancing text-based reinforcement learning agents with commonsense knowledge.",
            "rating": 2,
            "sanitized_title": "enhancing_textbased_reinforcement_learning_agents_with_commonsense_knowledge"
        }
    ],
    "cost": 0.009517,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Situated Language Learning via Interactive Narratives</h1>
<p>Prithviraj Ammanabrolu<br>Georgia Institute of Technology<br>raj.ammanabrolu@gatech.edu</p>
<p>Mark O. Riedl<br>Georgia Institute of Technology<br>riedl@cc.gatech.edu</p>
<h4>Abstract</h4>
<p>This paper provides a roadmap that explores the question of how to imbue learning agents with the ability to understand and generate contextually relevant natural language in service of achieving a goal. We hypothesize that two key components in creating such agents are interactivity and environment grounding, shown to be vital parts of language learning in humans, and posit that interactive narratives should be the environments of choice for such training these agents. These games are simulations in which an agent interacts with the world through natural language-"perceiving", "acting upon", and "talking to" the world using textual descriptions, commands, and dialogue-and as such exist at the intersection of natural language processing, storytelling, and sequential decision making. We discuss the unique challenges a text games' puzzle-like structure combined with natural language state-and-action spaces provides: knowledge representation, commonsense reasoning, and exploration. Beyond the challenges described so far, progress in the realm of interactive narratives can be applied in adjacent problem domains. These applications provide interesting challenges of their own as well as extensions to those discussed so far. We describe three of them in detail: (1) evaluating AI system's commonsense understanding by automatically creating interactive narratives; (2) adapting abstract text-based policies to include other modalities such as vision; and (3) enabling multi-agent and human-AI collaboration in shared, situated worlds.</p>
<h2>1 Introduction</h2>
<p>Natural language communication has long been considered a defining characteristic of human intelligence. In humans, this communication is grounded in experience and real world context-"what" we say or do depends on the current context around us and "why" we say or do something draws on commonsense knowledge gained through experience. So how do we imbue learning agents with the ability to understand and generate contextually relevant natural language in service of achieving a goal?</p>
<p>Two key components in creating such agents are interactivity and environment grounding, shown to be vital parts of language learning in humans. Humans learn various skills such as language, vision, motor skills, etc. more effectively through interactive media [Feldman and Narayanan, 2004, Barsalou, 2008]. In the realm of machines, interactive environments have served as cornerstones in the quest to develop more robust algorithms for learning agents across many machine learning sub-communities. Environments such as the Atari Learning Environment [Bellemare et al., 2013] and Minecraft [Johnson et al., 2016] have enabled the development of game agents that perform complex tasks while operating on raw video inputs, and more recently THOR [Kolve et al., 2017] and Habitat [Manolis Savva* et al., 2019] attempt to do the same with embodied agents in simulated 3D worlds.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An excerpt from Zork1, a typical text-based adventure game.</p>
<p>Despite such progress in modern machine learning and natural language processing, agents that can communicate with humans (and other agents) through natural language in pursuit of their goals are still primitive. One possible reason for this is that many datasets and tasks used for NLP are static, not supporting interaction and language grounding [Brooks, 1991, Feldman and Narayanan, 2004, Barsalou, 2008, Mikolov et al., 2016, Gauthier and Mordatch, 2016, Lake et al., 2017] In other words, there has been a void for such interactive environments for purely language-oriented tasks. Building on recent work in this field, we posit that interactive narratives should be the environments of choice for such language-oriented tasks. Interactive Narratives, in general, is an umbrella term, that refers to any form of digital interactive experience in which users create or influence a dramatic storyline through their actions [Riedl and Bulitko, 2013]-i.e. the overall story progression in the game is not pre-determined and is directly influenced by a player's choices. For the purposes of this work, we consider one particular type of interactive narrative, parser-based interactive fiction (or text-adventure) games-though we note that other forms of interactive narrative, including those with visual components, provide closely related challenges.
Figure 1 showcases Zork [Anderson et al., 1979], one of the earliest and most influential text-based interactive narrative. These games are simulations in which an agent interacts with the world through natural language-"perceiving", "acting upon", and "talking to" the world using textual descriptions, commands, and dialogue. The simulations are partially observable, meaning that the agent never has access to the true underlying world state and has to reason about how to act in the world based only on potentially the incomplete textual observations of its immediate surroundings. They provide tractable, situated environments in which to explore highly complex interactive grounded language learning without the complications that arise when modeling physical motor control and vision-situations that voice assistants such as Siri or Alexa might find themselves in when improvising responses. These games are usually structured as puzzles or quests with long-term dependencies in which a player must complete a sequence of actions and/or dialogues to succeed. This in turn requires navigation and interaction with hundreds of locations, characters, and objects. The interactive narrative community is one of the oldest gaming communities and game developers in this genre are quite creative. Put these two things together and we get very large, complex worlds that contain a multitude of puzzles and quests to solve across many different genres-everything from slice of life simulators where the player cooks a recipe in their home to Lovecraftian horror mysteries. The complexity and diversity of topics enable us to build and test agents that go an extra step towards modeling the difficulty of situated human language communication.
As the excerpt of the text-game in Figure 1 shows, humans bring competencies in natural language understanding, commonsense reasoning, and deduction to bear in order to infer the context and objectives of a game. Beyond games, real-world applications such as voice-activated personal as-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: A map of Zork1 by artist ion_bond.
sistants can also benefit from advances in these capabilities at the intersection of natural language understanding, natural language generation, and sequential decision making. These real world applications require the ability to reason with ungrounded natural language (unlike multimodal environments that provide visual grounding for language) and interactive narratives provide an excellent suite of environments to tackle these challenges.</p>
<p>Currently, three primary open-source platforms and baseline benchmarks have been developed so far to help measure progress in this field: Jericho [Hausknecht et al., 2020] ${ }^{1}$ a learning environment for human-made interactive narrative games; TextWorld [Côté et al., 2018] ${ }^{2}$ a framework for procedural generation in text-games; and LIGHT [Urbanek et al., 2019] ${ }^{3}$ a large-scale crowdsourced multi-user text-game for studying situated dialogue.</p>
<h1>2 Challenges</h1>
<p>Interactive narratives exist at the intersection of natural language processing, storytelling, and sequential decision making. Like many NLP tasks, they require natural language understanding, but unlike most NLP tasks, Interactive narratives are sequential decision making problems in which actions change the subsequent world states of the game and choices made early in a game may have long term effects on the eventual endings. Reinforcement Learning [Sutton and Barto, 1998] studies sequential decision making problems and has shown promise in vision-based [Jaderberg et al., 2016] and control-based [OpenAI et al., 2018] environments, but has less commonly been applied in the context of language-based tasks. Text-based games thus pose a different set of challenges than traditional video games such as StarCraft. Their puzzle-like structure coupled with a partially observable state space and sparse rewards require a greater understanding of previous context to enable more effective exploration-an implicit long-term dependency problem not often found in other domains that agents must overcome.</p>
<h3>2.1 Knowledge Representation</h3>
<p>Interactive narratives span many distinct locations, each with unique descriptions, objects, and characters. An example of a world of a interactive fiction game can be seen in Figure 2. Players move between locations by issuing navigational commands like go West.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>This, in conjunction with the inherent partial observability of interactive narratives, gives rise to the Textual-SLAM problem, a textual variant of Simultaneous localization and mapping (SLAM) [Thrun et al., 2005] problem of constructing a map while navigating a new environment. In particular, because connectivity between locations is not necessarily Euclidean, agents need to detect when a navigational action has succeeded or failed and whether the location reached was previously seen or new. Beyond location connectivity, it’s also helpful to keep track of the objects present at each location, with the understanding that objects can be nested inside of other objects, such as food in a refrigerator or a sword in a chest.</p>
<p>Due to the large number of locations in many games, humans often create structured memory aids such as maps to navigate efficiently and avoid getting lost. The creation of such memory aids has been shown to be critical in helping automated learning agents operate in these textual worlds [Ammanabrolu and Riedl, 2019, Murugesan et al., 2020, Adhikari et al., 2020, Ammanabrolu and Hausknecht, 2020]</p>
<h3>2.2 Acting and Speaking in Combinatorially-sized State-Action Spaces</h3>
<p>Interactive narratives require the agent to operate in the combinatorial action space of natural language. To realize how difficult a game such as Zork1 is for standard reinforcement learning agents, we need to first understand how large this space really is. In order to solve solve a popular IF game such as Zork1 it’s necessary to generate actions consisting of up to five-words from a relatively modest vocabulary of 697 words recognized by Zork’s parser. Even this modestly sized vocabulary leads to $\mathcal{O}(697^{5})=1.64 \times 10^{14}$ possible actions at every step—a dauntingly-large combinatorially-sized action space for a learning agent to explore. In comparison, board games such as chess and Go or Atari video games have branching factors of the order of $\mathcal{O}(10^{2})$.</p>
<p>Some text-games extend this even further by requiring agents to engage in dialogue to progress in a task, increasing the space of possibilities exponentially and bringing text environments closer to real-world situations. An example of such an environment—designed explicitly as a research platform—is the large-scale crowdsourced fantasy text-adventure game LIGHT [Urbanek et al., 2019], seen in Figure 3, where characters can act and talk while interacting with other characters. It consists of a set of locations, characters, and objects leading to rich textual worlds in addition to quests demonstrations of humans playing these quests providing natural language descriptions in varying levels of abstraction of motivations for a given character in a particular setting.</p>
<p>On top of the other text-game related challenges, the primary core challenge for the agent here is the recognition that dialogue can also be used to change the environment. With dialogue, an agent can now learn to instruct or convince other characters in the world to achieve the goal for it—e.g. convince the pirate through dialogue to give you their treasure instead of just stealing it yourself. The agent needs to learn to balance both its ability to speak as well as act in order to effectively achieve its goals [Ammanabrolu et al., 2021].</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The LIGHT [Urbanek et al., 2019] environment.</p>
<h3>2.3 Commonsense Reasoning</h3>
<p>Text-games cover a wide variety of genres, as mentioned earlier this ranges from slice of life simulators where the player makes a recipe in their home to Lovecraftian horror mysteries. In order to effectively convey the core narrative or puzzle, text-adventure games make ample use of prior commonsense knowledge. Everyday example could be something as mundane as the fact that an axe can be used to cut wood, or that swords are weapons. Different genres also have specific knowledge</p>
<p>attached to them that wouldn't normally be found in mundane settings, e.g. in a horror or fantasy game, we know that a coffin is likely to contain a vampire or other undead monster or that kings are royalty and must be treated respectfully. When a human enters a particular domain, they already possess priors regarding the specific knowledge relevant to the situations likely to be encounteredthis is thematic commonsense knowledge that a learning agent must acquire to ensure successful interactions.</p>
<p>This is closely related to the problem of transfer, the problem of acquiring and adapting these priors in novel environments through interaction. In this sense, we can think of commonsense knowledge as priors regarding environment dynamics. This problem space can be explored using text-based games. What commonsense can be transferred between two different environments, for example, a horror game and a mundane slice of life game? How do you unlearn, or choose not to apply, a piece of commonsense that no longer fits with the current world. What if the perceived environment dynamics change in novel ways? E.g. some vampires actually love garlic instead of being allergic to them or you suddenly find out that bread can be made without yeast and is known as sourdoughwhole new categories of recipes are now possible.</p>
<h1>2.4 Exploration</h1>
<p>Most text-adventure games have relatively linear plots in which players must solve a sequence of puzzles to advance the story and gain score. To solve these puzzles, players have freedom to a explore both new areas and previously unlocked areas of the game, collect clues, and acquire tools needed to solve the next puzzle and unlock the next portion of the game. From a Reinforcement Learning perspective, these puzzles can be viewed as bottlenecks that act as partitions between different regions of the state space. Whereas the relatively linear progression through puzzles may seem to make the problem easier, the opposite is true. The bottlenecks set up a situation where agents get stuck because they do not see the right action sequence enough times to be sufficiently reinforced. We contend that existing Reinforcement Learning agents are unaware of such latent structure and are thus poorly equipped for solving these types of problems.</p>
<p>Overcoming bottlenecks is not as simple as selecting the correct action from the bottleneck state. Most bottlenecks have long-range dependencies that must first be satisfied: Zork1 for instance features a bottleneck in which the agent must pass through the unlit Cellar where a monster known as a Grue lurks, ready to eat unsuspecting players who enter without a light source. To pass this bottleneck the player must have previously acquired and lit the lantern. Reaching the Cellar without acquiring the lantern results in the player reaching an unwinnable state-the player is unable to go back and acquire a lantern but also cannot progress further without a way to combat the darkness. Other bottlenecks don't rely on inventory items and instead require the player to have satisfied an external condition such as visiting the reservoir control to drain water from a submerged room before being able to visit it. In both cases, the actions that fulfill dependencies of the bottleneck, e.g. acquiring the lantern or draining the room, are not rewarded by the game. Thus agents must correctly satisfy all latent dependencies, most of which are unrewarded, then take the right action from the correct location to overcome such bottlenecks. Consequently, most existing agents-regardless of whether they use a reduced action space [Zahavy et al., 2018, Yuan et al., 2018, Yin and May, 2019] or the full space [Hausknecht et al., 2020, Ammanabrolu and Hausknecht, 2020]-have failed to consistently clear these bottlenecks. It is only recently that works have begun explicitly accounting for and surpassing such bottlenecks-using a reduced action space and Monte-Carlo Planning [Jang et al., 2021] and full action space and intrinsic motivation-based structured exploration [Ammanabrolu et al., 2020c].</p>
<h2>3 Applications and Future Directions</h2>
<p>Beyond the challenges described so far, progress in the realm of interactive narratives can be applied in adjacent problem domains. These applications provide interesting challenges of their own as well as extensions to those discussed so far. This section will describe three of them in detail: (1) evaluating AI system's commonsense understanding by creating interactive narratives; (2) adapting abstract text-based policies to include other modalities such as vision; and (3) enabling multi-agent and human-AI collaboration in shared, situated worlds.</p>
<h1>3.1 Automated World and Quest Generation</h1>
<p>A key consideration in modeling communication through a general purpose interactive narrative solver is that an agent trained to solve these games is limited by the scenarios described in them. Although the range of scenarios is vast, this brings about the question of what the agent is actually capable of understanding even if it has learned to solve all the puzzles in a particular game. Deep (reinforcement) learning systems tend to learn to generalize from the head of any particular data distribution, the "common" scenarios, and memorize the tail, the rarely seen cases. We contend that a potential way of testing an AI system's understanding of a domain is to use the knowledge it has gained in a novel way and to create more instances of that domain.</p>
<p>From the perspective of interactive narratives, this involves automatically creating such gamesthe flip side of the problem of creating agents that operate in these environments-and requires anticipating how people will interact with these environments and conforming to such expected commonsense norms to make a creative and engaging experience. The core experience in an interactive narrative revolves the quest, consisting of the partial ordering of activities that an agent must engage in to make progress toward the end of the game. Quest generation requires narrative intelligence and commonsense knowledge as a quest must maintain coherence throughout while progressing towards a goal [Ammanabrolu et al., 2020a]. Each step of the quest follows logically from the preceding steps much like the steps of a cooking recipe. A restaurant cannot serve a batch of cookies without first gathering ingredients, preparing cooking instruments, mixing ingredients, etc. in a particular sequence. Any generated quest that doesn't follow such an ordering will appear random or nonsensical to a human, betraying the AI's lack of commonsense understanding.</p>
<p>Maintaining quest coherence also means following the constraints of the given game world. The quest has to fit within the confines of the world in terms of both genre and given affordances-e.g. using magic in a fantasy world, placing kitchens next to living rooms in mundane worlds, etc. This gives rise to the concept of world generation, the second half of the automated game generation problem. This refers to generating the structure of the world, including the layout of rooms, textual description of rooms, objects, and characters-setting the boundaries for how an agent is allowed to interact with the world [Ammanabrolu et al., 2020b]. Similarly to quests, a world violating thematically relevant commonsense structuring rules will appear random to humans, providing us with a metric to measure an AI system's understanding.</p>
<h3>3.2 Transfer across domains and modalities</h3>
<p>Many of the core challenges presented by text games manifest themselves across domains with different modalities and it may be possible to transfer progress between the domains. Take the example of a slice-of-life walking simulator text game where the main quest is to complete a recipe as given before. What happens when we encounter a similar situation with the added modality of vision? Can we take the knowledge we've gained from learning a textbased policy by completing the recipe in the original text game and use that to learn how to do something similar with a visually embodied agent? To test this idea, Shridhar et al. [2021] built ALFWorld, a simulator that lets you first learn text-based policies in the "home" textgame TextWorld [Côté et al., 2018], and then execute them in similarly themed scenarios from the visual environment ALFRED [Shridhar et al., 2020]. They find that commonsense priors-regarding things like common object locations, affordances, and causality-learned while playing text-games can be adapted to help create agents that generalize better in visually
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: ALFWorld [Shridhar et al., 2021].</p>
<p>grounded environments. This indicates that text games are suitable environments to train agents to reason abstractly through text which can then be refined and adapted to specific instances in an embodied setting.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: A wet lab protocol as a text game from the X-WLP dataset [Tamari et al., 2021].</p>
<p>Another such cross-domain transfer experiment was tested by Tamari et al. [2021], where they collected and built X-WLP, a corpus of complex wet lab biochemistry protocols that are framed as a quest and could thus be executed via a text-game engine. The annotations themselves are collected using a text-game-like interface, reducing overall data collection cost. Tamari et al. [2019] discuss automatically extracting these protocols from raw lab texts and also training deep reinforcement learning agents on the resulting text-game quest. The ability to automatically frame wet lab experiments in the form of text game quests and leverage the latest text-game agent advances to interactively train agents to perform them has implications for significantly improving procedural text understanding [Levy et al., 2017] and in the reproducibility of scientific experiments [Mehr et al., 2020].</p>
<h1>3.3 Multi-agent and Human-AI Collaboration</h1>
<p>Current work on teaching agents to act and speak in situated, shared worlds such as LIGHT opens the doors for exploring multi-agent communication using natural language, i.e. through dialogue. It has been shown how to teach agents to act and talk in pursuit of a goal in this world leads to them learning multiple ways of achieve the goal: acting to do it themselves, or convincing a partner agent to do it for them. We envision this situated learning paradigm extended to to a multi-agent setting, where there are multiple agents progressing through a world in pursuit of their own motivations that learn to communicate with each other, figuring out what others can do for them. This gives rise to a dynamic world within the bounds of a unified decision making framework, a situation autonomous agents are likely to find themselves in. A village led by an ambitious chief seeking expansion will expand into a town via environment dynamics, or narrative, emerging from this multiagent communication. Agents can further be taught which other agents they should cooperate with and which they should compete with on the basis of the alignment of their motivations. A dragon terrorizing a kingdom and a knight may perhaps be at odds, but the kingdom's ruler will have cause to cooperate and explicitly aid the knight in slaying the dragon. A not-so-fantastic example would be two small clothing businesses cooperating and pooling resources to compete against an encroaching large corporation.
A human-AI collaborative system is an instance of such a multi-agent system where one or more of the agents are humans. These works thus have direct implications for human-AI collaborative systems: from agents that act and talk in multi-user worlds, to improvisational and collaborative storytelling, and creative writing assistants for human authors.</p>
<h2>4 Conclusion</h2>
<p>Interactive narratives provide tractable, situated environments in which to explore highly complex interactive grounded language learning without the complications that arise when modeling physical motor control and vision. The unique challenges a text games' puzzle-like structure combined with natural language state-and-action spaces provides is: knowledge representation, commonsense reasoning, and exploration. These challenges create an implicit long-term dependency problem not often found in other domains that agents must overcome. Text-based games thus pose a different set of challenges than traditional video games such as StarCraft. Beyond the challenges described so far, we have seen how progress in the realm of interactive narratives can be applied in adjacent problem domains, specifically: (1) structured environment creation; (2) transfer to other modalities and domains; and (3) enabling multi-agent and human-AI collaboration in shared, situated worlds.</p>
<h1>Acknowledgements</h1>
<p>We thank Matthew Hausknecht, Xingdi Yuan, and Marc-Alexandre Côté of Microsoft Research for useful discussions on text games and their work on the Jericho and TextWorld platforms. Likewise, thanks to Jack Urbanek, Margaret Li, Arthur Szlam, Tim Rocktäschel, and Jason Weston of Facebook AI Research for their efforts and guidance in the work on the LIGHT framework. We also would like to thank the corresponding authors Mohit Shridar of the University of Washington and Ronen Tamari of the Hebrew University of Jerusalem for discussions regarding their respective works ALFWorld and X-WLP and the images within reproduced accordingly.</p>
<h2>References</h2>
<p>A. Adhikari, X. Yuan, M.-A. Côté, M. Zelinka, M.-A. Rondeau, R. Laroche, P. Poupart, J. Tang, A. Trischler, and W. L. Hamilton. Learning dynamic knowledge graphs to generalize on textbased games. arXiv preprint arXiv:2002.09127, 2020.
P. Ammanabrolu and M. Hausknecht. Graph Constrained Reinforcement Learning for Natural Language Action Spaces. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=B1x6w0EtwH.
P. Ammanabrolu and M. O. Riedl. Playing text-adventure games with graph-based deep reinforcement learning. In Proceedings of 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, 2019.
P. Ammanabrolu, W. Broniec, A. Mueller, J. Paul, and M. O. Riedl. Toward automated quest generation in text-adventure games. In International Conference on Computational Creativity (ICCC), 2020a. URL https://arxiv.org/abs/1909.06283.
P. Ammanabrolu, W. Cheung, D. Tu, W. Broniec, and M. O. Riedl. Bringing stories alive: Generating interactive fiction worlds. In Proceedings of the Sixteenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE-20), 2020b. URL https: //www.aaai.org/ojs/index.php/AIIDE/article/view/7400.
P. Ammanabrolu, E. Tien, M. Hausknecht, and M. O. Riedl. How to avoid being eaten by a grue: Structured exploration strategies for textual worlds. arXiv preprint arXiv:2006.07409, 2020c.
P. Ammanabrolu, J. Urbanek, M. Li, A. Szlam, T. Rocktäschel, and J. Weston. How to motivate your dragon: Teaching goal-driven agents to speak and act in fantasy worlds. In Proceedings of 2021 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, 2021. URL https://arxiv.org/ abs/2010.00685.
T. Anderson, M. Blank, B. Daniels, and D. Lebling. Zork. http://ifdb.tads.org/viewgame?id= 4gxk83ja4twckm6j, 1979.
L. W. Barsalou. Grounded cognition. Annual Review of Psychology, 59(1):617-645, 2008. doi: 10. 1146/annurev.psych.59.103006.093639. URL https://doi.org/10.1146/annurev.psych.59.103006. 093639. PMID: 17705682.
M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279, jun 2013.
R. A. Brooks. Intelligence without representation. Artificial intelligence, 47(1-3):139-159, 1991.
M.-A. Côté, A. Kádár, X. Yuan, B. Kybartas, T. Barnes, E. Fine, J. Moore, M. Hausknecht, L. E. Asri, M. Adada, W. Tay, and A. Trischler. Textworld: A learning environment for text-based games. CoRR, abs/1806.11532, 2018.
J. Feldman and S. Narayanan. Embodied meaning in a neural theory of language. Brain and language, 89:385-92, 06 2004. doi: 10.1016/S0093-934X(03)00355-9.</p>
<p>J. Gauthier and I. Mordatch. A paradigm for situated and goal-driven language learning. arXiv preprint arXiv:1610.03585, 2016.
M. Hausknecht, P. Ammanabrolu, M.-A. Côté, and X. Yuan. Interactive fiction games: A colossal adventure. In Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI), 2020. URL https: //arxiv.org/abs/1909.05398.
M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver, and K. Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. CoRR, abs/1611.05397, 2016.
Y. Jang, S. Seo, J. Lee, and K.-E. Kim. Monte-carlo planning and learning with language action value estimates. In International Conference on Learning Representations, 2021. URL https: //openreview.net/forum?id=7_G8JySGecm.
M. Johnson, K. Hofmann, T. Hutton, and D. Bignell. The malmo platform for artificial intelligence experimentation. In IJCAI, IJCAI'16, pages 4246-4247. AAAI Press, 2016. ISBN 978-1-57735-770-4.
E. Kolve, R. Mottaghi, W. Han, E. VanderBilt, L. Weihs, A. Herrasti, D. Gordon, Y. Zhu, A. Gupta, and A. Farhadi. AI2-THOR: An Interactive 3D Environment for Visual AI. arXiv, 2017.
B. M. Lake, T. D. Ullman, J. B. Tenenbaum, and S. J. Gershman. Building machines that learn and think like people. Behavioral and brain sciences, 40, 2017.
O. Levy, M. Seo, E. Choi, and L. Zettlemoyer. Zero-shot relation extraction via reading comprehension. In Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 333-342, Vancouver, Canada, Aug. 2017. Association for Computational Linguistics. doi: 10.18653/v1/K17-1034. URL https://www.aclweb.org/anthology/K17-1034.</p>
<p>Manolis Savva<em>, Abhishek Kadian</em>, Oleksandr Maksymets*, Y. Zhao, E. Wijmans, B. Jain, J. Straub, J. Liu, V. Koltun, J. Malik, D. Parikh, and D. Batra. Habitat: A Platform for Embodied AI Research. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019.
S. H. M. Mehr, M. Craven, A. I. Leonov, G. Keenan, and L. Cronin. A universal system for digitization and automatic execution of the chemical synthesis literature. Science, 370(6512):101-108, 2020. ISSN 0036-8075. doi: 10.1126/science.abc2986. URL https://science.sciencemag.org/ content/370/6512/101.
T. Mikolov, A. Joulin, and M. Baroni. A roadmap towards machine intelligence. In International Conference on Intelligent Text Processing and Computational Linguistics, pages 29-61. Springer, 2016.
K. Murugesan, M. Atzeni, P. Shukla, M. Sachan, P. Kapanipathi, and K. Talamadupula. Enhancing text-based reinforcement learning agents with commonsense knowledge. arXiv preprint arXiv:2005.00811, 2020.</p>
<p>OpenAI, M. Andrychowicz, B. Baker, M. Chociej, R. Józefowicz, B. McGrew, J. W. Pachocki, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, J. Schneider, S. Sidor, J. Tobin, P. Welinder, L. Weng, and W. Zaremba. Learning dexterous in-hand manipulation. CoRR, abs/1808.00177, 2018.
M. O. Riedl and V. Bulitko. Interactive narrative: An intelligent systems approach. Ai Magazine, 34 (1):67-67, 2013.
M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi, L. Zettlemoyer, and D. Fox. ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. URL https://arxiv.org/ abs/1912.01734.
M. Shridhar, X. Yuan, M.-A. Cote, Y. Bisk, A. Trischler, and M. Hausknecht. {ALFW}orld: Aligning text and embodied environments for interactive learning. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=0IOX0YcCdTn.</p>
<p>R. S. Sutton and A. G. Barto. Introduction to Reinforcement Learning. MIT Press, Cambridge, MA, USA, 1st edition, 1998. ISBN 0262193981.
R. Tamari, H. Shindo, D. Shahaf, and Y. Matsumoto. Playing by the Book: An Interactive Game Approach for Action Graph Extraction from Text. In Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications, pages 62-71, Minneapolis, Minnesota, jun 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-2609. URL https:// www.aclweb.org/anthology/W19-2609.
R. Tamari, F. Bai, A. Ritter, and G. Stanovsky. Process-level representation of scientific protocols with interactive annotation. arXiv preprint arXiv:2101.10244, 2021.
S. Thrun, W. Burgard, and D. Fox. Probabilistic Robotics (Intelligent Robotics and Autonomous Agents). The MIT Press, 2005. ISBN 0262201623.
J. Urbanek, A. Fan, S. Karamcheti, S. Jain, S. Humeau, E. Dinan, T. Rocktäschel, D. Kiela, A. Szlam, and J. Weston. Learning to speak and act in a fantasy text adventure game. CoRR, abs/1903.03094, 2019.
X. Yin and J. May. Comprehensible context-driven text game playing. CoRR, abs/1905.02265, 2019.
X. Yuan, M. Côté, A. Sordoni, R. Laroche, R. T. des Combes, M. J. Hausknecht, and A. Trischler. Counting to explore and generalize in text-based games. CoRR, abs/1806.11525, 2018.
T. Zahavy, M. Haroush, N. Merlis, D. J. Mankowitz, and S. Mannor. Learn what not to learn: Action elimination with deep reinforcement learning. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 3562-3573. Curran Associates, Inc., 2018.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://github.com/microsoft/jericho
${ }^{2}$ https://github.com/microsoft/textworld
${ }^{3}$ https://parl.ai/projects/light&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
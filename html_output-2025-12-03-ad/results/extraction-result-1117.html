<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1117 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1117</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1117</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-271270622</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.13364v1.pdf" target="_blank">Geometric Active Exploration in Markov Decision Processes: the Benefit of Abstraction</a></p>
                <p><strong>Paper Abstract:</strong> How can a scientist use a Reinforcement Learning (RL) algorithm to design experiments over a dynamical system's state space? In the case of finite and Markovian systems, an area called Active Exploration (AE) relaxes the optimization problem of experiments design into Convex RL, a generalization of RL admitting a wider notion of reward. Unfortunately, this framework is currently not scalable and the potential of AE is hindered by the vastness of experiment spaces typical of scientific discovery applications. However, these spaces are often endowed with natural geometries, e.g., permutation invariance in molecular design, that an agent could leverage to improve the statistical and computational efficiency of AE. To achieve this, we bridge AE and MDP homomorphisms, which offer a way to exploit known geometric structures via abstraction. Towards this goal, we make two fundamental contributions: we extend MDP homomorphisms formalism to Convex RL, and we present, to the best of our knowledge, the first analysis that formally captures the benefit of abstraction via homomorphisms on sample efficiency. Ultimately, we propose the Geometric Active Exploration (GAE) algorithm, which we analyse theoretically and experimentally in environments motivated by problems in scientific discovery.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1117.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1117.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Geometric Active Exploration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An Active Exploration algorithm that combines Convex Reinforcement Learning with MDP homomorphisms to exploit known geometric invariances, using a Frank–Wolfe scheme with optimistic (variance-aware) gradient rewards to adaptively design experiments in Markovian environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Geometric Active Exploration (GAE)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GAE is an algorithmic agent that (1) compresses the original MDP into an abstract MDP via a known MDP homomorphism (ψ, {ϕ_s}), (2) optimizes a smoothened convex objective L_{∞,η}(λ) (a geometry-aware estimation objective) using a Frank–Wolfe loop, (3) at each FW iteration forms an optimistic gradient-based reward r^k_λ that incorporates empirical variance estimates and an optimism bonus α(t,s,δ), (4) solves the resulting abstract MDP (e.g., by value iteration) to get an abstract policy, (5) lifts the abstract policy to the original MDP and deploys it to collect samples, and (6) aggregates measurements across equivalence classes to update empirical means and visitation frequencies. Key components: MDP homomorphism-based abstraction, Frank–Wolfe optimization over state-action distributions, optimistic variance concentration bonuses, policy lifting, and geometry-aware inference (weighted averaging across equivalence classes).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Active Exploration via Convex RL (geometry-aware experimental design using optimistic, variance-aware Frank–Wolfe updates)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>GAE adapts its experimental design iteratively: it maintains empirical visitation frequencies and empirical variances per abstract state; at each iteration it computes an optimistic gradient of the geometry-aware convex objective (adding an α(t,s,δ) bonus derived from variance concentration bounds), uses that gradient as a reward to solve an abstract MDP for an optimal abstract policy, lifts that policy to the original MDP, deploys it for τ_k steps to collect observations, aggregates samples across f-equivalence classes (weighted averaging), updates empirical variances and visitation counts, and updates the state-action distribution λ for the next FW step. The adaptation uses (i) empirical means and variances of observations, (ii) visitation counts, (iii) a high-probability optimism bonus α(t,s,δ), and (iv) known symmetry abstraction via the MDP homomorphism.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Two evaluated environments: (1) Pollutant diffusion (radial diffusion grid); (2) Chemical compound toxicity (discrete string-building MDP with permutation invariance).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Discrete finite-state Controlled Markov Processes (CMP/MDP) with Markovian dynamics; partially unknown observation function f(s) (deterministic f with heteroscedastic additive noise observations y(s)=f(s)+ν(s)); environments considered both deterministic and stochastic transitions (stochastic setting with transition success probability q); known dynamics P may be available; environments have exact group-structured symmetries (state and state-dependent action invariances) exploited via an MDP homomorphism; ergodicity is assumed for stationary policies.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Pollutant diffusion: original S=240 states (30 radii × 8 angles), action set A={in,out,clockwise,anticlockwise,stay} (A=5); both deterministic and stochastic transition variants (stochastic q=0.98). Chemical compounds: original S=363 (all strings up to length 5 over 3 base elements), action set A={A,B,C,stay} (A=4); abstract S=55 after permutation invariance (Φ≈0.15). Episode/interaction model is non-episodic (continuous interactions); experiments run with total samples n chosen per experiment (examples: pollutant uses n≈210 in one setup with τ=3 per iteration; toxicity uses n=2400 with τ=20 per iteration).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Empirical: GAE consistently achieves lower geometric estimation error and faster reduction of estimation error than the classic AE baseline (equivalent Convex RL AE that does not exploit symmetries). Reported qualitative results: significant sample-efficiency gains for varying compression Φ in both deterministic and stochastic diffusion settings; significant statistical advantage in the chemical-compound (strings) environment with Φ≈0.15. Computational: GAE achieves reduced runtime (normalized) versus AE due to smaller abstract MDP solves. Numerical specifics provided by the paper are experimental curves (no single scalar summary number); experiments repeated over 15 seeds. Theoretical: regret bound R_n = O( Φ^{1/2} S^{1/2} A F_max σ_max^2 η^{5/2} n^{-1/3} ) (Theorem 6.1), and sample complexity to obtain ε-optimality scales as n = O( Φ^{3/2} S^{3/2} A^3 F_max^3 (σ_max^2)^{3/2} η^{15/2} ε^{-3} ) (Theorem 6.2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baseline AE (Convex RL without symmetry abstraction / identity homomorphism, Φ=1) performs worse empirically: slower reduction in estimation error and higher computational runtime. No absolute numeric baseline RMSE values are reported in the text; comparisons shown as curves (GAE vs AE) in Figures; AE equals GAE when Φ=1.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Empirical: With the same number of interactions GAE attains substantially lower geometric estimation error than AE in the tested tasks (e.g., for pollutant diffusion experiments with n≈210 and τ=3, and for compounds with n=2400 and τ=20). Theoretical: to reach ε suboptimality in L_{∞,η}(λ), required samples scale as O(Φ^{3/2} S^{3/2} A^3 F_max^3 (σ_max^2)^{3/2} η^{15/2} ε^{-3}), showing that increased geometric compression (smaller Φ) reduces sample complexity polynomially.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Exploration is driven by an optimism-under-uncertainty mechanism: empirical variances σ_t^2(s) are combined with a high-probability upper confidence bonus α(t,s,δ) (derived from variance concentration bounds) to form optimistic gradient-rewards; Frank–Wolfe updates mix previous visitation frequency λ_k with the newly obtained υ_{k+1} (mixing weight β_k), balancing policy-driven exploitation (optimal policy for current optimistic reward) and exploration (optimism and repeated FW iterations). The choice of τ_k controls how long each policy is deployed (affecting the empirical vs stationary distribution gap); the FW step sizes β_k and τ_k schedule (e.g., τ_k = 3k^2 - 3k + 1 in theory or small constant τ in practice) manage the exploration–exploitation pacing.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Baseline: classic Active Exploration (AE) implementation based on Convex RL and Tarbouriech & Lazaric (2019) which corresponds to identity homomorphism (Φ=1). Theoretical comparisons reference Convex RL works (Hazan et al., 2019). The paper also discusses relation to methods that learn homomorphisms or discover symmetries but does not empirically compare to learned-homomorphism methods.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>1) Introducing a geometry-aware estimation error and convex AE objective that aggregates measurements across f-equivalence classes, enabling principled use of known symmetries in experimental design. 2) Extending MDP homomorphisms to Convex RL and proposing GAE, which leverages abstraction to (i) reduce computational cost by solving smaller abstract MDPs and (ii) improve statistical efficiency by aggregating data across symmetric states. 3) Theoretical guarantees: a regret upper bound that explicitly depends on a geometric compression term Φ (smaller Φ helps), and a derived sample complexity scaling with Φ^{3/2} showing that abstraction provably reduces sample complexity. 4) Empirical validation on two scientific-discovery-motivated tasks (pollutant diffusion and chemical-toxicity strings) shows significant empirical improvements in sample efficiency and runtime versus AE baseline, across deterministic and stochastic dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Assumptions and limitations reported: (i) GAE assumes the MDP homomorphism (geometric priors) is known and exact; results rely on exact invariances—approximate symmetries are not analyzed in this paper (not evaluated empirically). (ii) Assumption of homogeneous equivalence-class sizes (E_s equal across abstract states) is used in analysis. (iii) Ergodicity of the Markov chain under any stationary policy is assumed. (iv) The algorithm still requires solving MDPs (value iteration) at each FW step; although abstraction reduces that cost, the MDP solver remains the main computational bottleneck. (v) Quantitative empirical metrics (e.g., numeric RMSE reductions) are presented as curves rather than single-number summaries; no experiments on learned/unknown homomorphisms in this paper (though authors note it as future work).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Geometric Active Exploration in Markov Decision Processes: the Benefit of Abstraction', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Active exploration in Markov decision processes <em>(Rating: 2)</em></li>
                <li>Provably efficient maximum entropy exploration <em>(Rating: 2)</em></li>
                <li>Active exploration via experiment design in Markov chains <em>(Rating: 2)</em></li>
                <li>MDP homomorphic networks: Group symmetries in reinforcement learning <em>(Rating: 2)</em></li>
                <li>Active model estimation in markov decision processes <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1117",
    "paper_id": "paper-271270622",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "GAE",
            "name_full": "Geometric Active Exploration",
            "brief_description": "An Active Exploration algorithm that combines Convex Reinforcement Learning with MDP homomorphisms to exploit known geometric invariances, using a Frank–Wolfe scheme with optimistic (variance-aware) gradient rewards to adaptively design experiments in Markovian environments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Geometric Active Exploration (GAE)",
            "agent_description": "GAE is an algorithmic agent that (1) compresses the original MDP into an abstract MDP via a known MDP homomorphism (ψ, {ϕ_s}), (2) optimizes a smoothened convex objective L_{∞,η}(λ) (a geometry-aware estimation objective) using a Frank–Wolfe loop, (3) at each FW iteration forms an optimistic gradient-based reward r^k_λ that incorporates empirical variance estimates and an optimism bonus α(t,s,δ), (4) solves the resulting abstract MDP (e.g., by value iteration) to get an abstract policy, (5) lifts the abstract policy to the original MDP and deploys it to collect samples, and (6) aggregates measurements across equivalence classes to update empirical means and visitation frequencies. Key components: MDP homomorphism-based abstraction, Frank–Wolfe optimization over state-action distributions, optimistic variance concentration bonuses, policy lifting, and geometry-aware inference (weighted averaging across equivalence classes).",
            "adaptive_design_method": "Active Exploration via Convex RL (geometry-aware experimental design using optimistic, variance-aware Frank–Wolfe updates)",
            "adaptation_strategy_description": "GAE adapts its experimental design iteratively: it maintains empirical visitation frequencies and empirical variances per abstract state; at each iteration it computes an optimistic gradient of the geometry-aware convex objective (adding an α(t,s,δ) bonus derived from variance concentration bounds), uses that gradient as a reward to solve an abstract MDP for an optimal abstract policy, lifts that policy to the original MDP, deploys it for τ_k steps to collect observations, aggregates samples across f-equivalence classes (weighted averaging), updates empirical variances and visitation counts, and updates the state-action distribution λ for the next FW step. The adaptation uses (i) empirical means and variances of observations, (ii) visitation counts, (iii) a high-probability optimism bonus α(t,s,δ), and (iv) known symmetry abstraction via the MDP homomorphism.",
            "environment_name": "Two evaluated environments: (1) Pollutant diffusion (radial diffusion grid); (2) Chemical compound toxicity (discrete string-building MDP with permutation invariance).",
            "environment_characteristics": "Discrete finite-state Controlled Markov Processes (CMP/MDP) with Markovian dynamics; partially unknown observation function f(s) (deterministic f with heteroscedastic additive noise observations y(s)=f(s)+ν(s)); environments considered both deterministic and stochastic transitions (stochastic setting with transition success probability q); known dynamics P may be available; environments have exact group-structured symmetries (state and state-dependent action invariances) exploited via an MDP homomorphism; ergodicity is assumed for stationary policies.",
            "environment_complexity": "Pollutant diffusion: original S=240 states (30 radii × 8 angles), action set A={in,out,clockwise,anticlockwise,stay} (A=5); both deterministic and stochastic transition variants (stochastic q=0.98). Chemical compounds: original S=363 (all strings up to length 5 over 3 base elements), action set A={A,B,C,stay} (A=4); abstract S=55 after permutation invariance (Φ≈0.15). Episode/interaction model is non-episodic (continuous interactions); experiments run with total samples n chosen per experiment (examples: pollutant uses n≈210 in one setup with τ=3 per iteration; toxicity uses n=2400 with τ=20 per iteration).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Empirical: GAE consistently achieves lower geometric estimation error and faster reduction of estimation error than the classic AE baseline (equivalent Convex RL AE that does not exploit symmetries). Reported qualitative results: significant sample-efficiency gains for varying compression Φ in both deterministic and stochastic diffusion settings; significant statistical advantage in the chemical-compound (strings) environment with Φ≈0.15. Computational: GAE achieves reduced runtime (normalized) versus AE due to smaller abstract MDP solves. Numerical specifics provided by the paper are experimental curves (no single scalar summary number); experiments repeated over 15 seeds. Theoretical: regret bound R_n = O( Φ^{1/2} S^{1/2} A F_max σ_max^2 η^{5/2} n^{-1/3} ) (Theorem 6.1), and sample complexity to obtain ε-optimality scales as n = O( Φ^{3/2} S^{3/2} A^3 F_max^3 (σ_max^2)^{3/2} η^{15/2} ε^{-3} ) (Theorem 6.2).",
            "performance_without_adaptation": "Baseline AE (Convex RL without symmetry abstraction / identity homomorphism, Φ=1) performs worse empirically: slower reduction in estimation error and higher computational runtime. No absolute numeric baseline RMSE values are reported in the text; comparisons shown as curves (GAE vs AE) in Figures; AE equals GAE when Φ=1.",
            "sample_efficiency": "Empirical: With the same number of interactions GAE attains substantially lower geometric estimation error than AE in the tested tasks (e.g., for pollutant diffusion experiments with n≈210 and τ=3, and for compounds with n=2400 and τ=20). Theoretical: to reach ε suboptimality in L_{∞,η}(λ), required samples scale as O(Φ^{3/2} S^{3/2} A^3 F_max^3 (σ_max^2)^{3/2} η^{15/2} ε^{-3}), showing that increased geometric compression (smaller Φ) reduces sample complexity polynomially.",
            "exploration_exploitation_tradeoff": "Exploration is driven by an optimism-under-uncertainty mechanism: empirical variances σ_t^2(s) are combined with a high-probability upper confidence bonus α(t,s,δ) (derived from variance concentration bounds) to form optimistic gradient-rewards; Frank–Wolfe updates mix previous visitation frequency λ_k with the newly obtained υ_{k+1} (mixing weight β_k), balancing policy-driven exploitation (optimal policy for current optimistic reward) and exploration (optimism and repeated FW iterations). The choice of τ_k controls how long each policy is deployed (affecting the empirical vs stationary distribution gap); the FW step sizes β_k and τ_k schedule (e.g., τ_k = 3k^2 - 3k + 1 in theory or small constant τ in practice) manage the exploration–exploitation pacing.",
            "comparison_methods": "Baseline: classic Active Exploration (AE) implementation based on Convex RL and Tarbouriech & Lazaric (2019) which corresponds to identity homomorphism (Φ=1). Theoretical comparisons reference Convex RL works (Hazan et al., 2019). The paper also discusses relation to methods that learn homomorphisms or discover symmetries but does not empirically compare to learned-homomorphism methods.",
            "key_results": "1) Introducing a geometry-aware estimation error and convex AE objective that aggregates measurements across f-equivalence classes, enabling principled use of known symmetries in experimental design. 2) Extending MDP homomorphisms to Convex RL and proposing GAE, which leverages abstraction to (i) reduce computational cost by solving smaller abstract MDPs and (ii) improve statistical efficiency by aggregating data across symmetric states. 3) Theoretical guarantees: a regret upper bound that explicitly depends on a geometric compression term Φ (smaller Φ helps), and a derived sample complexity scaling with Φ^{3/2} showing that abstraction provably reduces sample complexity. 4) Empirical validation on two scientific-discovery-motivated tasks (pollutant diffusion and chemical-toxicity strings) shows significant empirical improvements in sample efficiency and runtime versus AE baseline, across deterministic and stochastic dynamics.",
            "limitations_or_failures": "Assumptions and limitations reported: (i) GAE assumes the MDP homomorphism (geometric priors) is known and exact; results rely on exact invariances—approximate symmetries are not analyzed in this paper (not evaluated empirically). (ii) Assumption of homogeneous equivalence-class sizes (E_s equal across abstract states) is used in analysis. (iii) Ergodicity of the Markov chain under any stationary policy is assumed. (iv) The algorithm still requires solving MDPs (value iteration) at each FW step; although abstraction reduces that cost, the MDP solver remains the main computational bottleneck. (v) Quantitative empirical metrics (e.g., numeric RMSE reductions) are presented as curves rather than single-number summaries; no experiments on learned/unknown homomorphisms in this paper (though authors note it as future work).",
            "uuid": "e1117.0",
            "source_info": {
                "paper_title": "Geometric Active Exploration in Markov Decision Processes: the Benefit of Abstraction",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Active exploration in Markov decision processes",
            "rating": 2,
            "sanitized_title": "active_exploration_in_markov_decision_processes"
        },
        {
            "paper_title": "Provably efficient maximum entropy exploration",
            "rating": 2,
            "sanitized_title": "provably_efficient_maximum_entropy_exploration"
        },
        {
            "paper_title": "Active exploration via experiment design in Markov chains",
            "rating": 2,
            "sanitized_title": "active_exploration_via_experiment_design_in_markov_chains"
        },
        {
            "paper_title": "MDP homomorphic networks: Group symmetries in reinforcement learning",
            "rating": 2,
            "sanitized_title": "mdp_homomorphic_networks_group_symmetries_in_reinforcement_learning"
        },
        {
            "paper_title": "Active model estimation in markov decision processes",
            "rating": 1,
            "sanitized_title": "active_model_estimation_in_markov_decision_processes"
        }
    ],
    "cost": 0.01293375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Geometric Active Exploration in Markov Decision Processes: the Benefit of Abstraction
18 Jul 2024</p>
<p>Riccardo De Santi <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#114;&#100;&#101;&#115;&#97;&#110;&#116;&#105;&#64;&#101;&#116;&#104;&#122;&#46;&#99;&#104;">&#114;&#100;&#101;&#115;&#97;&#110;&#116;&#105;&#64;&#101;&#116;&#104;&#122;&#46;&#99;&#104;</a>. 
Department of Computer Science
ETH Zurich
ZurichSwitzerland</p>
<p>ETH AI Center
ZurichSwitzerland</p>
<p>Federico Arangath Joseph 
Department of Computer Science
ETH Zurich
ZurichSwitzerland</p>
<p>Noah Liniger 
Department of Computer Science
ETH Zurich
ZurichSwitzerland</p>
<p>Mirco Mutti 
Technion
HaifaIsrael</p>
<p>Andreas Krause 
Department of Computer Science
ETH Zurich
ZurichSwitzerland</p>
<p>Geometric Active Exploration in Markov Decision Processes: the Benefit of Abstraction
18 Jul 20248DF2AAAB8A90B789D4948E65A02EB4A2arXiv:2407.13364v1[cs.LG]
How can a scientist use a Reinforcement Learning (RL) algorithm to design experiments over a dynamical system's state space?In the case of finite and Markovian systems, an area called Active Exploration (AE) relaxes the optimization problem of experiments design into Convex RL, a generalization of RL admitting a wider notion of reward.Unfortunately, this framework is currently not scalable and the potential of AE is hindered by the vastness of experiment spaces typical of scientific discovery applications.However, these spaces are often endowed with natural geometries, e.g., permutation invariance in molecular design, that an agent could leverage to improve the statistical and computational efficiency of AE.To achieve this, we bridge AE and MDP homomorphisms, which offer a way to exploit known geometric structures via abstraction.Towards this goal, we make two fundamental contributions: we extend MDP homomorphisms formalism to Convex RL, and we present, to the best of our knowledge, the first analysis that formally captures the benefit of abstraction via homomorphisms on sample efficiency.Ultimately, we propose the Geometric Active Exploration (GAE) algorithm, which we analyse theoretically and experimentally in environments motivated by problems in scientific discovery.</p>
<p>Introduction</p>
<p>The problem of optimal experimental design (OED) (Chaloner &amp; Verdinelli, 1995) refers to the task of optimally selecting experiments to minimize a measure of uncertainty of an unknown quantity of interest f : S → R, where S denotes a space of experiments.Typically, the problem considers a limited budget of resources, e.g., number of experiments, and assumes the possibility to directly sample f at arbitrary inputs s ∈ S. Conceptually, an optimal design can be interpreted as a distribution over experiments determining the probability with which these should be carried out in order to minimize the uncertainty of f (Pukelsheim, 2006).</p>
<p>Interestingly, in a wide variety of applications the input space S corresponds to the state space of a dynamical system (Mutny et al., 2023).Therefore, the agent carrying out the experiments must respect the underlying dynamics and cannot teleport from any experiment, now interpreted as a state s ∈ S, to any other experiment.For instance, consider the environmental sensing problem illustrated in Figure 1, where an agent aims to actively estimate the amount of air pollution caused by the diffusion of a chemical substance released from a point source.To address this problem, the agent chooses sampling policies to minimize an estimation error of the amount of pollutant f .</p>
<p>In the case of time-discrete and Markovian dynamical systems, this problem is known as Active Exploration (AE) (Mutny et al., 2023;Tarbouriech &amp; Lazaric, 2019;Tarbouriech et al., 2020).AE frames the experiments design task as an instance of Convex Reinforcement Learning (Convex RL) (Hazan et al., 2019;Zahavy et al., 2021), a recent generalization of RL where the agent aims to minimize a convex functional of the state-action distribution induced by a policy interacting with the environment.</p>
<p>The AE formulation of the OED problem on dynamical systems is promising as it allows to learn (from data) optimal sampling policies that respect the system dynamics while minimizing a measure of uncertainty of f .Nonetheless, solving an instance of Convex RL typically entails solving a sequence of Markov Decision Processes (MDPs) and estimating the visitation density at each iteration (Hazan et al., 2019).As a consequence, current Active Exploration methods are not scalable, hindering their use in real-world scientific discovery problems where experiment spaces are generally immense (Wang et al., 2023;Thiede et al., 2022).</p>
<p>Luckily, these spaces are often endowed with natural geometries, as in the case of permutation invariances in molecular design (Cheng et al., 2021;Elton et al., 2019), or rototranslation and reflection invariances in environmental sensing (Krause, 2008;van der Pol et al., 2020b).Let us take the example reported in Figure 1.One can expect locations that are equally distant from the point source (center) to have almost identical amounts of pollutant i.e., the quantity of interest f follows radial symmetries.1 Therefore, by sampling f in one location, the agent gains information on f in all the other symmetric locations as well.As a consequence, in this work we first aim to answer the following question:</p>
<p>How can a RL agent exploit geometric structure to increase the statistical and computational efficiency of AE?</p>
<p>First, we introduce a novel geometric estimation error and corresponding AE objective (Sec. 3 and 4).Then, we find optimal sampling strategies for the introduced AE objective by bridging Active Exploration with the area of MDP homomorphisms (Ravindran &amp; Barto, 2001;van der Pol et al., 2020b), which offers an algorithmic scheme to leverage known geometric structure in RL via abstraction.Unfortunately, MDP homomorphisms are not directly usable in AE as it is not a classic RL problem.Thus, we extend MDP homomorphisms to Convex RL and introduce Geometric Active Exploration (GAE), an algorithm that solves the AE problem by exploiting known geometric structure via abstraction (Sec.5).To the best of our knowledge, we provide the first analysis that formally captures the benefit of abstraction on sample efficiency via MDP homomorphisms (Sec.6).Finally, we showcase experimentally the statistical and computational advantages of GAE in illustrative environments inspired by scientific discovery problems (Sec.7).</p>
<p>To sum up, we make the following contributions:</p>
<p>• An Active Exploration objective that leverages known invariances of the quantity of interest f (Sec.3 and 4).• Geometric Active Exploration (GAE), an algorithm that extends MDP homomorphism to Convex RL and solves AE via abstraction (Sec.5).</p>
<p>• The first analysis that sheds light on the benefits of abstraction on sample efficiency via MDP homomorphisms, here specialized for the AE problem (Sec.6).• An experimental evaluation of the performance of GAE against a classic AE algorithm (Sec.7).</p>
<p>Our analysis capturing the benefit of geometric structure on sample efficiency may be of independent interest for RL and Convex RL.</p>
<p>Background and Notation</p>
<p>Let X be a set, we denote with ∆(X) the probability simplex over X.We define [N ] := {1, . . ., N }, and given a number x, we denote x + := max{1, x}.</p>
<p>Markovian Processes and Active Exploration</p>
<p>In the following, we briefly introduce basic RL notions and the Active Exploration (AE) problem.An agent interacting with a CMP starts from an initial state s 0 ∼ µ.Then, at every time-step t, the agent takes an action a t , collects a reward r(s t , a t ) (when defined) and transitions to s t+1 ∼ P (•|s t , a t ).The agent's actions are sampled from a stationary policy π : S → ∆(A) such that π(a|s) denotes the conditional probability of a in s.</p>
<p>Discrete Markovian</p>
<p>Active Exploration.In the Active Exploration problem, an agent interacts with a MDP M f = (S, A, P, µ, f ) where f is an unknown and deterministic quantity of interest providing a noisy observation x ∼ y(s) = f (s) + ν(s) at state s.Here, ν is a distribution with zero mean and unknown heteroscedastic variance σ2 (s) ∈ [0, σ 2 max ] and x ∈ [0, F max ].In AE, the agent aims to learn a policy to minimize a measure of uncertainty over f through interactions with M f .Notice that, as a sub-case of AE, f can be interpreted as a reward function that an agent wishes to estimate rather than maximize (Lindner et al., 2022).</p>
<p>Invariances and MDP Homomorphisms</p>
<p>In the following, we introduce basic concepts from abstract algebra and the notion of MDP homomorphism.</p>
<p>Equivalence, Invariance, and Symmetries.When a function f : X − → Y maps two inputs x, x ′ to the same value f (x) = f (x ′ ), we say that x and x ′ are f -equivalent.The set [x] of all points f -equivalent to x is called equivalence class of x.We say that f is invariant across
[x]. Con- sider a transformation operator L g : X − → X , where G = ({L g } g∈G , •) is a group, G is an index set, and • denotes composition. Given a function f : X − → Y, if L g satisfies: f (x) = f (L g [x]) ∀g ∈ G, ∀x ∈ X(1)
then we say that f is invariant to L g , and we call {L g } g∈G a set of symmetries of f .MDP Homomorphisms.An MDP homomorphism h is a mapping from an original MDP M r = (S, A, P, µ, r) to an abstract MDP M r = (S, A, P , µ, r) defined by a surjective map h : S × A → S × A. In particular, h is composed of a tuple (ψ, {ϕ s | s ∈ S}), where ψ : S → S is the state map and ϕ s : A → A is the state-dependent action map.These maps are built to satisfy the conditions:
r(ψ(s), ϕ s (a)) = r(s, a)(2)P (ψ(s ′ ) | ψ(s), ϕ s (a)) = s ′′ ∈[s ′ ] P (s ′′ | s, a)(3)
for all s, s ′ ∈ S, a ∈ A. Moreover, given a state s such that s := ψ(s), we denote the equivalence class of s (and s) induced by ψ as
[s] = [s] := {s ′ ∈ S : ψ(s ′ ) = ψ(s)} and indicate with E s := E s = |[s]| its cardinality.
Policy Lifting.</p>
<p>Given a MDP homomorphism h, the Optimal Value Equivalence Theorem by Ravindran &amp; Barto (2001) states that an optimal policy π for the abstract MDP can be transformed to an optimal policy π for the original MDP via the lifting operation:
π(a|s) := π(a|ψ(s)) |{a ∈ ϕ −1 s (a)}| ∀s ∈ S, a ∈ ϕ −1 s (a) (4)</p>
<p>Problem Setting</p>
<p>Consider the AE problem in a MDP M f = (S, A, P, µ, f ) with known dynamics P , and unknown quantity of interest f : S → B ⊂ R. Typically, an agent aims to minimize an estimation error of f of the form:</p>
<p>(Classic) Estimation Error
ξ n = 1 S s∈S fn (s) − f (s)(5)
where fn (s) denotes the empirical estimate of f (s) after n steps in the environment (Tarbouriech &amp; Lazaric, 2019).</p>
<p>In this work, we consider the case where f and the dynamics P have convenient geometric structures.In a vast variety of applications, the quantity of interest f is known to have certain group-structured symmetries L g : S − → S and state-dependent action symmetries K s g : A → A. For all g ∈ G, s ∈ S, a ∈ A, f and P are invariant according to 2
f (s, a) = f (L g [s], K s g [a])(6)P (s ′ | s, a) = P (L g [s ′ ] | L g [s], K s g [a])(7)
For instance, in the diffusion process in Fig. 1, f follows radial symmetries, while P has roto-translation symmetries as in most physical systems (van der Pol et al., 2020b, Table 1).</p>
<p>An MDP with this structure, often denoted as MDP with symmetries (van der Pol et al., 2020b), naturally defines an MDP homomorphism h = (ψ, {ϕ s | s ∈ S}) that can be efficiently built, as illustrated in Figure 1, by mapping state-action pairs across which f and P are invariant to a unique abstract state-action pair (van der Pol et al., 2020b;Ravindran &amp; Barto, 2001).The main intuition with respect to our estimation process, is that all sets of states [s] across which f is invariant, will map along ψ to an abstract state
s := ψ(s) = ψ(s ′ ) ∈ S ∀s, s ′ ∈ [s].
We consider the case where such an MDP homomorphism h encoding the underlying geometric structure is known.This is a fair assumption for a large class of applications, where geometric priors can easily be represented via a MDP homomorphism (van der Pol et al., 2020b).Nonetheless, in Section 9 we briefly discuss how the contributions presented in this work can be leveraged in the case of unknown MDP homomorphism.In the following, we introduce a novel geometric estimation error that makes it possible to leverage geometric priors both while learning an optimal sampling strategy, and in inference, when the gathered data is used to compute estimates of the unknown quantity of interest f .</p>
<p>Geometric Function Estimation</p>
<p>First, we introduce some quantities updated by the agent when obtaining a noisy realization of f at every time-step i, namely x i ∼ y(s i ), where s i indicates the current state at time-step i ∈ [t].After t interaction steps we have:
T t (s) := t i=1 I{s i = s} (8) ft (s) := 1 T + t (s) t i=1 x i I{s i = s} (9) σ2 t (s) := 1 T + t (s) t i=1 x 2 i I{s i = s} − ft (s) 2 (10)
which are respectively the visitation counts, the empirical mean and empirical variance.Now, we define the geometric estimation error given n samples from f as follows.</p>
<p>Geometric Estimation Error
ξn = 1 S s∈S f A n (s) − f (s)(11)
where f A n (s) is an empirical mean obtained by weighted averaging across all states within the same f -equivalence class
[s]. Formally, given T + n ([s]) := s ′ ∈[s] T + n (s ′ ), we have: f A n (s) := 1 T + n ([s]) s ′ ∈[s] T n (s ′ ) fn (s ′ )(12)
Interestingly, the geometric estimation error (Eq.11) generalizes the classic AE estimation error (Tarbouriech &amp; Lazaric, 2019), which corresponds to the limit case of ours where there are no f -invariances and therefore every equivalence class is composed of only one state.</p>
<p>Given the geometric estimation error ξn , any ϵ &gt; 0 and δ ∈ (0, 1), we say that an estimate of f is (ϵ, δ)-accurate if:
P( ξn ≤ ϵ) ≥ 1 − δ (13)
Notice that, in this case, the RL agent is used as an algorithmic tool to estimate an external property of the environment and can be interpreted as an active sampler of the underlying Markov chain.In particular, we aim to design an algorithm that minimizes the sample complexity needed to estimate f nearly-optimally in high probability.</p>
<p>Definition 1 (Sample Complexity Geometric Estimation).</p>
<p>Given an error ϵ &gt; 0 and a confidence level δ ∈ (0, 1), the sample complexity to solve the geometric function estimation problem is:
n ξ (ϵ, δ) := min{n ≥ 1 : P( ξn ≤ ϵ) ≥ 1 − δ} (14)</p>
<p>From Experimental Design to Convex RL</p>
<p>In this section, we derive a principled objective to minimize the sample complexity of geometric estimation (Def.1).</p>
<p>We first show that ξn (Eq.11) can be rewritten as a function of abstract states s ∈ S, making it well-defined over the abstract MDP.</p>
<p>Proposition 1.The geometry-aware estimation error ξn can be rewritten as a function of abstract states as:
ξn = 1 S s∈S E s | fn (s) − f (s) | (15)
While Proposition 1 is proved in Appendix B, here we briefly mention the main intuition.Since the empirical estimator f A n (s) aggregates over experiments s ∈ [s] across which f is invariant, the estimation error can be rewritten by considering only a representative of the f -equivalence class [s], namely an abstract state s = ψ(s) ∈ S.Then, equality is obtained by reweighting with the cardinality E s of [s].</p>
<p>Tractable formulation via Convex RL</p>
<p>Proposition 1 gives ξn as a function of abstract states, for which we derive the upper bound below (proof in Apx.B).</p>
<p>Proposition 2 (Convex Upper Bound of ξn ).With probability at least 1 − δ and n interactions with f we have:
ξn ≤ C(n, S, δ) S s∈S F(s; T + n )(16)
with C(n, S, δ) := max log(nS/δ), log(nS/δ) and
F(s; T + n ) := E s 2σ 2 (s) T + n (s) + F max T + n (s)
where T + n (s) := T + n ([s]) are the visitation counts of s.</p>
<p>As an abstract state represents an f -equivalence class of original states, i.e., s = ψ(s) = ψ(s ′ ) ∀s, s ′ ∈ [s], one can notice that Equation 16 captures two interesting facts.First, equivalence classes [s] that are under-visited (small T + n (s)) with high variance (large σ 2 (s)) lead to higher estimation error.Second, the cardinality of an equivalence class E s is proportional to how much its estimation quality impacts the overall estimation error.</p>
<p>While the upper bound in Equation 16 is convex, the constraint set of admissible visitation counts T + n is non-convex (Tarbouriech et al., 2020), rendering this formalization a NP-hard problem (Welch, 1982;Tarbouriech &amp; Lazaric, 2019).Nonetheless, problems of this form present a hidden convexity in the asymptotic relaxation (n → ∞) of the dual problem.Given the set Λ of admissible asymptotic state-action distributions
Λ := {λ ∈ ∆(S × A) : ∀s ∈ S, b∈A λ(s, b) = (s ′ ,a)∈S×A P (s|s ′ , a)λ(s ′ , a)}
we introduce the following η-smoothened objective.</p>
<p>Geometric Estimation Objective
L ∞,η (λ) := 1 S s∈S E s 2σ 2 (s) s∈[s] (λ(s) + η)(17)
Where λ(s) := a∈A λ(s, a).Crucially, in the following statement, we show that L ∞,η (λ) is an upper bound of the estimation error ξn and therefore that minimizing L ∞,η (λ) is a principled objective to minimize the sample complexity in Definition 1. Proposition 3 (Tractable Convex Upper Bound of ξn ).Let an empirical state-action frequency at time t be defined as λ t (s, a) = T t (s, a)/t, then for E s η ≤ 1 n we have:
ξn ≤ 2S √ n C(n, S, δ) L ∞,η (λ n ) + SF max S √ nη (18)
Interestingly, in the next section we show that this problem can be solved by computing optimal sampling policies for abstract MDPs only.</p>
<p>Geometric Active Exploration (GAE)</p>
<p>In this section, we introduce Geometric Active Exploration (GAE), an algorithm for AE that leverages the power of abstraction to improve statistical and computational efficiency.Alike classic AE algorithms (Tarbouriech &amp; Lazaric, 2019;Hazan et al., 2019), GAE is based on a Frank-Wolfe (FW) scheme (Jaggi, 2013) that reduces the problem
min λ∈Λ L ∞,η (λ)(19)
to a sequence of K linear programs, each corresponding to a classic MDP M k f with reward r k λ defined as
∇L + t k −1 (λ) [s,a] = −E s 2σ 2 t k −1 (s) + α(t k − 1, s, δ) 2S s∈[s] ( b∈A λ(s, b) + η) 3 2
where the unknown variances are optimistically bounded via a quantity α(t, s, δ) according to the following result.Lemma 5.1 (Variance Concentration (Panaganti &amp; Kalathil, 2022)).For all s ∈ S, with probability at least 1 − δ we have
σ 2 (s) − σ2 t (s) ≤ F max 2 log(2St 2 /δ) T + t (s) := α(t, s, δ)
We show that optimistic gradients (r k λ ) of Equation 19 satisfy state-action invariances induced by f (proof in Apx.C).</p>
<p>Algorithm 1 Geometric Active Exploration
(GAE) 1: Input: η, h, M, δ, {τ k } k∈[K−1] 2: Compute abstract CMP M induced by h, M 3: Initialize λ1 = 1/ S Ā 4: for k = 1, 2, ..., K − 1 do 5: Compute abstract reward rk λ k ∀s ∈ A, a ∈ A rk λ k (s, a) := −Es 2σ 2 t k −1 (s) + α(t k − 1, s, δ) 2S λ k (s) + Esη 3 2 6: π + k+1 ← − MDP-SOLVER M k r = S, A, P , µ, rk λ k 7: Lift abstract policy π + k+1 (a|s) = π + k+1 (a|ψ(s)) |{a ∈ ϕ −1 s (a)}| , ∀s ∈ S, a ∈ ϕ −1 s (a)
8: Deploy policy π + k+1 in M f for τ k steps 9: Compute ft k+1 −1 and υk+1 10: Aggregate estimates according to M
k f ft k+1 −1(s) = 1 T + t k+1 −1 (s) s∈[s] T + t k+1 −1 (s) ft k+1 −1(s) υk+1 (s, a) = s∈[s]
υk+1 (s, a)</p>
<p>11: Update the abstract state-action frequency λ k+1
λ k+1 = τ k t k+1 − 1 υk+1 + t k − 1 t k+1 − 1 λ k 12: end for 13: Return: ft K −1 Proposition 4 (Gradient-Reward Invariances). If f is invariant over states s and s ′ , then ∀s, s ′ ∈ [s], ∀a, a ′ ∈ A ∇ λ L + t k −1 (λ)[s, a] = ∇ λ L + t k −1 (λ)[s ′ , a ′ ]
Intuitively, Proposition 4 is due to the fact that the invariances of f propagate to the gradient via σ, because of its definition in Equation 10.As a consequence, we can define the optimistic abstract reward as:
rk λ (s, a) := −E s 2σ 2 t k −1 (s) + α(t k − 1, s, δ) 2S λ(s) + E s η 3 2 (20)
where λ ∈ Λ ⊆ ∆(S × A) is an admissible abstract state-action distribution. 3Given the reward rk λ and the invariances on the dynamics P in Equation 3, the MDP M k f at each step k ∈ [K] of the FW scheme can be solved by computing the optimal policy for the abstract MDP M k f and then lifting it back along the MDP homomorphism h (Eq.4).This observation unlocks the power of abstraction for AE in MDPs, and leads to the GAE algorithm, for which we report the pseudocode in Algorithm 1.</p>
<p>First, GAE computes the abstract CMP M given the homomorphism h and the original CMP M (line 2).This operation is computationally efficient as we can perform one sweep over SA to compute SA by applying ψ and ϕ s to original states and actions respectively.The empirical visitation frequency λ 1 is initialized in line 3.At each iteration, GAE computes the optimal abstract policy π + k+1 (line 6), e.g., via value iteration, for the abstract MDP
M k r = M ∪ {r k λ k }, where rk λ k
is an estimate of the optimistic gradient in (20) based on the samples gathered via policy π + k during the previous iteration.Then, it computes an optimal policy for the original MDP, namely π + k+1 by lifting the optimal abstract policy (line 7), which is deployed for τ k steps (line 8).The gathered samples of f and the state-action visitation counts are used to update the abstract empirical mean of f , namely ft k+1 −1 (s), and compute the abstract empirical state-action distribution υk+1 by aggregating υk+1 across states within the same equivalence class (lines 8-9).Then, the empirical visitation frequency λ k+1 is updated to serve for the gradient estimation at the next iteration.GAE outputs the aggregated estimates of f .Benefits of Abstraction.Since AE typically entails solving a sequence of MDPs, encoding each instance via a (smaller) abstract MDP (line 6) gives significant computational benefits, as shown in Section 7. From a statistical perspective, the fundamental advantage of GAE is leveraging known invariances during the density estimation process (lines 8-10) common in previous works (Hazan et al., 2019;Tarbouriech &amp; Lazaric, 2019), leading to faster convergence.However, how do different degrees of geometric structure benefit the statistical efficiency of the problem?In the next section, we formally answer this question by presenting a sample complexity result showcasing a geometric compression term.In the following, we denote by sampling strategy the empirical distribution λ n over S ×A induced by the policies {π
+ k } k∈[K] .</p>
<p>Theoretical Analysis</p>
<p>In this section, we present an upper bound on the regret and the sample complexity achieved by GAE against an optimal sampling strategy.The latter result captures the impact of abstraction on the complexity of the problem via the following notion of geometric compression.</p>
<p>Definition 2 (Geometric Compression Term).We denote as geometric compression term Φ the ratio between the cardinalities of the abstract and original state spaces, formally:
Φ := S/S ∈ (0, 1](21)
Before presenting these results, we state two assumptions we employed for deriving them.</p>
<p>Assumption 6.1 (Homogeneous Equivalence Classes).The equivalence classes induced by f over S are homogeneous i.e., they have same cardinality,
E s = E s ′ ∀s, s ′ ∈ S.
Moreover, due to the non-episodic nature of our setting we need to assume the following.</p>
<p>Assumption 6.2 (Ergodicity).The Markov chain induced by any Markovian stationary policy is ergordic.</p>
<p>Regret Analysis</p>
<p>Given an empirical state-action distribution λ n ∈ ∆(S × A) induced by a sequence of sampling policies interacting with the environment, we define its regret against the optimal sampling strategy as follows
R n (λ n ) := L ∞,η (λ n ) − L ∞,η (λ * )(22)
where λ * := arg min λ∈Λ L ∞,η (λ) is an optimal stateaction distribution.Notice that, while common in AE (Tarbouriech &amp; Lazaric, 2019), this notion of regret is not standard in RL (Szepesvári, 2009).</p>
<p>Theorem 6.1 (Regret Guarantee).If algorithm GAE is run with a budget of n samples and τ k = 3k 2 − 3k + 1 then w.p. at least 1 − δ, it holds that:
R n = O Φ 1 2 S 1 2 AF max σ 2 max η 5 2 1 n 1/3
In the following, we present a brief sketch of the proof, while complete derivations are deferred to Apx.D.</p>
<p>Step 1.We derive the result w.r.t. the abstract variables via a Frank-Wolfe analysis, taking into account (i) the effect of the optimistic gradient and (ii) the error due to the gap between the empirical and stationary distribution induced by the policy at each iteration (Tarbouriech &amp; Lazaric, 2019).</p>
<p>Step 2. Since the density estimation step of GAE is carried out w.r.t. a distribution defined over S × A, we notice that it can be analysed with respect to the abstract variables.</p>
<p>Step 3. Finally, in order to state the result w.r.t. the original MDP variables, we leverage the geometric compression term Φ (Def.2).</p>
<p>In the following, we report the sample complexity bound capturing the effect of abstraction on learning with high probability a nearly-optimal sampling strategy w.r.t. the geometric estimation objective (17).
T AE GAE, Φ = 1/2 GAE, Φ = 1/4 GAE, Φ = 1/8 (dn = O Φ 3 2 S 3 2 A 3 F 3 max (σ 2 max ) 3 2 η 15 2 ϵ 3
samples, then we have that with probability at least 1−δ:
P L ∞,η (λ n ) − L ∞,η (λ * ) ≤ ϵ ≥ 1 − δ
Crucially, setting Φ = 1 recovers the case where abstraction is not leveraged by the algorithm.</p>
<p>Geometric Compression in MDP with Symmetries</p>
<p>If the MDP M f has symmetries (see Eq. 6 and 7), it is possible to make explicit the dependency of Φ on the cardinality of the state symmetries group G = ({L g } g∈G , •).</p>
<p>Proposition 5 (Compression via Group Cardinality).</p>
<p>Given a set of group-structured state symmetries G = ({L g } g∈G , •) and Stab(s) = Stab(s ′ ) ∀s, s ′ ∈ S then:
Φ = |Stab(s)| |G| where Stab(s) := {g ∈ G : L g [s] = s}.
Proposition 5, which we proved via the Orbit-Stabilizer Theorem (Rotman, 2010, Theorem C-1.16), sheds light on the intuition that a higher number of symmetries leads to a higher degree of compression.</p>
<p>Experiments</p>
<p>In this section, we perform a thorough experimental evaluation of GAE analysing its statistical and computational efficiency on two tasks where the unknown quantity f represents: (1) the amount of pollutant emerging from a point source (see Fig. 1), and (2) the toxicity of chemical compounds generated from a set of base elements (see Fig. 2f).In all experiments, unless otherwise specified, we compare the data gathering performances of GAE with classic AE, an implementation based on Convex RL and analogous to GAE, but not exploiting symmetries (Tarbouriech &amp; Lazaric, 2019).More explicitly, AE is the same as GAE (see Alg. 1) in the case when the homomorphism h is an identity map and hence Φ = 1 and M = M.</p>
<p>(1) Pollutant Diffusion Process.We consider the problem of actively estimating the amount of pollution released in the environment from a point source and following a diffusion process with radial symmetries, as illustrated in Figure 1 and introduced in Section 2. The agent can measure the pollution at 30 different radii and at 8 different angles, resulting in S = 240 states, and can select actions A = {in, out, clockwise, anticlockwise, stay}.In Figure 2a, we show the sample efficiency of GAE compared with AE for several values of Φ in the case of deterministic dynamics.We observe a significant effect of different degrees of compression to the data efficiency of the algorithm.Similarly, Figure 2b shows the same comparison for stochastic dynamics.In Figure 2c, we show that omitting the inductive bias in the inference step, specifically the absence of weighted averaging across equivalence classes, worsens the performance of AE, thus showing the role of exploiting geometric structure also in inference.</p>
<p>Ultimately, in Figure 2d, we compare the normalized runtime of GAE and AE for several degrees of compression, showing the effect of leveraging geometric structure on computational efficiency and hence practical scalability.</p>
<p>(2) Toxicity of Chemical Compounds.In this experiment, we consider the problem of actively estimating the toxicity of chemical compounds that can be generated using some base chemical elements.Similar to prior work (Thiede et al., 2022;Dong et al., 2022), we associate with states chemical compounds represented as strings (see Fig. 2f).Each character of the string thereby stands for a base chemical element.In our simplified setting, we consider three base elements A, B, C and every compound may consist of at most 5 base elements, resulting in a total of S = 363 states.</p>
<p>The actions correspond to the three base elements and a stay action.If the agent picks an action that corresponds to a base element, this is appended to the current string, resulting in a new compound to which the agent transitions and gets a noisy observation of its toxicity.We consider toxicity to be invariant w.r.t.string permutations, resulting in S = 55 states and Φ ≈ 0.15. Figure 2e shows the statistical performances of GAE compared with AE.We observe that even a relatively small compression leads to a significant statistical advantage.</p>
<p>An extensive discussion of the environments, homomorphisms, and implementation details is deferred to Apx.F.</p>
<p>Related Works</p>
<p>In the following, we present relevant works in MDP Homomorphisms, Active Exploration, and Convex RL.</p>
<p>MDP Homomorphisms.</p>
<p>Ravindran &amp; Barto (2001) were among the first to identify the benefits of solving MDPs via MDP homomorphisms.Recently, these have been extended to Deep RL (van der Pol et al., 2020b), approximate invariances (Ravindran &amp; Barto, 2004;Jiang et al., 2014;Ravindran &amp; Barto, 2002), and continuous domains (Rezaei-Shoshtari et al., 2022;Biza &amp; Platt, 2018;Zhao, 2022).While in this work we have built a fundamental connection between Active Exploration and abstraction via MDP homomorphisms, extending the contributions presented to the mentioned settings, namely Deep RL, approximate abstraction, and continuous domains, is an interesting and relevant direction of future work.</p>
<p>Active Exploration.The AE problem has been introduced in (Tarbouriech &amp; Lazaric, 2019) with a non-episodic setting assuming ergodicity and reversibility of the induced Markov chain.Afterwards, the framework has been extended to perform transition dynamics estimation in high probability (Tarbouriech et al., 2020).Compared with these works, by smoothening the objective (Sec.4), we remove the need to solve an LP program at every iteration paving the way for more efficient dynamic programming methods, e.g., value iteration (Puterman, 2014, Chapter 6).Recently, Mutny et al. (2023) introduced an episodic version of the problem, where the agent can reset its state, but ergodicity is not required, while the unknown quantity f is assumed to be an element of a reproducing kernel Hilbert space with known kernel (Mutny et al., 2023).While this setting can capture the correlation structure of f , it does not leverage known geometric structure of the dynamics and therefore cannot compress the original MDP into an abstract one to render the algorithm more scalable from a computational perspective.</p>
<p>Convex RL.The algorithmic scheme presented in this work is an instance of a general framework that has received significant attention recently, generally under the name of Convex RL (Hazan et al., 2019;Zhang et al., 2020;Zahavy et al., 2021;Geist et al., 2022;Mutti et al., 2022a;2023;2022b).In this framework, a learning agent interacts with a CMP to optimize an objective formulated through a convex function of the state-action distribution induced by the agent policy.Different choices of this convex function allow to cover several domains of practical interest beyond active exploration, such as pure exploration (e.g., Hazan et al., 2019), imitation learning (e.g., Abbeel &amp; Ng, 2004), and risk-averse RL (Garcıa &amp; Fernández, 2015) among others.</p>
<p>Conclusions</p>
<p>In this paper, we presented how abstraction can be leveraged to solve the Active Exploration problem with better statistical complexity and computational efficiency.Before presenting some concluding remarks, we briefly mention a few important discussion points.</p>
<p>Beyond Known Geometric Structure.In a wide set of applications e.g., molecular design or environmental sensing, geometric priors on f and P are known and can be easily encoded in an abstract MDP as considered in this work.Nonetheless, for a arguably wider set, geometric structure is not known or it is not human-interpretable.In these cases, one can leverage algorithms that automatically discover symmetries in the environment (Angelotti et al., 2021;Narayanamurthy &amp; Ravindran, 2008) or directly learn a MDP homomorphism (Mavor-Parker et al., 2022;Biza &amp; Platt, 2018;Wolfe &amp; Barto, 2006;Mondal et al., 2022).Interestingly, in both these cases, the GAE algorithm can be run with the machine-learned homomorphism.</p>
<p>Abstraction in Convex RL.The presented algorithmic scheme (Alg. 1) and theoretical analysis (Sec.6) are not tied to the AE problem treated in this paper and can be straightforwardly extended to leverage abstraction in a variety of Convex RL application areas, including those mentioned within Section 8.</p>
<p>Benefit of Abstraction on Statistical Efficiency.Abstraction, via MDP homomorphisms or close variants, has been leveraged in a large body of works (Rezaei-Shoshtari et al., 2022;van der Pol et al., 2020b;a;2022;Soni &amp; Singh, 2006;Ravindran, 2003;Zhu et al., 2022;Ravindran &amp; Barto, 2002;Mahajan &amp; Tulabandhula, 2017;Ravindran, 2003) showcasing experimental advantages on the sample complexity in the context of RL.Nonetheless, to the best of our knowledge, this work is the first that formally captures the effect of abstraction via MDP homomorphisms on sample complexity (Sec.6).Moreover, we believe the main ideas within our analysis can be leveraged to treat a large extent of RL settings.</p>
<p>To summarize, in this work we have presented a principled Active Exploration objective that makes it possible to leverage geometric priors on an unknown quantity f and the system dynamics to solve the estimation problem via an abstraction process, thus increasing statistical and computational efficiency.We introduced an algorithm, Geometric Active Exploration, which we believe could render Active Exploration more practical for a wide variety of real-world settings.Then, we have presented, to the best of our knowledge, the first analysis capturing the effect of abstraction on the sample complexity of the Active Exploration problem, and more in general in RL.Ultimately, we have performed a thorough experimental evaluation of the proposed method on tasks resembling real-world scientific discovery problems while showing promising performances.</p>
<dl>
<dt>A. List of symbols</dt>
<dd>A → A [s] ≜ Equivalence class of f -invariant states mapping to s along ψ, [s] = {s ′ ∈ S : ψ(s ′ ) = ψ(s)} E s ≜ Cardinality of equivalence class [s], E s = |[s]| = |[s]| = E s Φ ≜ Geometric compression coefficient, Φ = S/S GAE Algorithm f ≜ Unknown state quantity f : S → B ⊂ R ν ≜ Noise∇ L t k −1 ≜ empirical gradient of objective L ∞,η ∇ L + t k −1 ≜ empirical optimistic gradient of objective L ∞,η</dd>
</dl>
<p>B. Proofs Section 3</p>
<p>Proposition 1.The geometry-aware estimation error ξn can be rewritten as a function of abstract states as:
ξn = 1 S s∈S E s | fn (s) − f (s) | (15)
Proof.By the definition of ξn we have
ξn := 1 S s∈S | f A n (s) − f (s)|(23)
(1)
= 1 S s∈S 1 T + n (s) s ′ ∈[s] T n (s ′ ) fn (s ′ ) − f (s)(24)
(2)
= 1 S s∈S |[s]| 1 T + n (s) s ′ ∈[s] T n (s ′ ) fn (s ′ ) − f (s) (25) (3) = 1 S s∈S E s fn (s) − f (s)(26)
where in step (1) we used that f A n (s) := 1
T + n ([s]) s ′ ∈[s] T n (s ′ ) fn (s ′ ),T + n (s) s∈[s] T n (s) fn (s).
Proposition 2 (Convex Upper Bound of ξn ).With probability at least 1 − δ and n interactions with f we have:
ξn ≤ C(n, S, δ) S s∈S F(s; T + n )(16)
with C(n, S, δ) := max log(nS/δ), log(nS/δ) and
F(s; T + n ) := E s 2σ 2 (s) T + n (s) + F max T + n (s)
where T + n (s) := T + n ([s]) are the visitation counts of s.</p>
<p>Proof.To prove the statement, we employ a Bernstein type inequality from Lemma 7.37 in (Lafferty et al., 2008), where we upper bound 2 /3 by 1 in the second summand.Then, given a δ ′ , we have that for all t ∈ [n]:
P ft (s) − f (s) ≤ 2σ 2 (s) log(1/δ ′ ) T + n (s) + F max log(1/δ ′ ) T + n (s) ≥ 1 − δ ′(27)
where
T + n (s) = s∈[s] T + n (s) and ft (s) = 1 T + t (s) s∈[s] T t (s) ft (s).
For notational simplicity, we will define:
B δ ′ (s) = 2σ 2 (s) log(1/δ ′ ) T + n (s) + F max log(1/δ ′ ) T + n (s)
Then, by using a standard union bound, over s ∈ S and t ∈ [n]:
P   t∈[n] s∈S ft (s) − f (s) ≤ B δ ′ (s)   = 1 − P   n∈[t] s∈S ft (s) − f (s) &gt; B δ ′ (s)   ≥ 1 − t∈[n] s∈S P ft (s) − f (s) &gt; B δ ′ (s) (4) ≥ 1 − nSδ ′ (28)
where in step (4) we have used Equation ( 27).Then:
1 − δ (5) ≤ P   t∈[n] s∈S ft (s) − f (s) ≤ B δ ′ (s)   = P   t∈[n] s∈S E s ft (s) − f (s) ≤ E s B δ ′ (s)   (29) ≤ P   s∈S E s fn (s) − f (s) ≤ E s B δ ′ (s)   (6) ≤ P   s∈S E s fn (s) − f (s) ≤ s∈S E s B δ ′ (s)   = P   S ξn ≤ s∈S E s B δ ′ (s)   = P   ξn ≤ 1 S s∈S E s B δ ′ (s)  
where in (5) we used Equation 28 and in step (6) we used the trivial fact that since the inequality in Equation 29 holds for every s ∈ S therefore it holds also for the respective sums over S.Then, we get that with probability at least 1 − δ:
ξn ≤ 1 S s∈S E s 2σ 2 (s) log(nS/δ) T + n (s) + F max log(nS/δ) T + n (s) ≤ 1 S max log(nS/δ), log(nS/δ) s∈S E s 2σ 2 (s) T + n (s) + F max T + n (s) = 1 S max log(nS/δ), log(nS/δ) s∈S F(s; T + n ) = C(n, S, δ) S s∈S F(s; T + n )
Proposition 3 (Tractable Convex Upper Bound of ξn ).Let an empirical state-action frequency at time t be defined as λ t (s, a) = T t (s, a)/t, then for E s η ≤ 1 n we have:
ξn ≤ 2S √ n C(n, S, δ) L ∞,η (λ n ) + SF max S √ nη (18)
Proof.First, we define the following:
F(s ; λ) := E s F 1 (s ; λ) + F 2 (s ; λ)(30)
with
F 1 (s ; λ) = 2σ 2 (s) a∈A s∈[s] λ(s, a) + 1 n F 2 (s ; λ) = 1 √ n F max a∈A s∈[s] λ(s, a) + 1 n
and the following auxiliary objective:
L n (λ) := 1 S s∈S F(s ; λ)(31)
Then, given an empirical state-action frequency at time t, defined as λ t (s, a) = T t (s, a)/t, we have that:
ξn (7) ≤ 2 √ n C(n, S, δ) s∈S F(s ; λ n ) (8) = 2S √ n C(n, S, δ)L n (λ n ) ≤ 2S √ n C(n, S, δ) L ∞,η (λ n ) + SF max S √ nη
where in step ( 7) we employed Equations 30 and 16 and the fact that T + n (s) = max(T n (s), 1) ≥ (T n (s) + 1)/2.In step ( 8) we used the definition of the objective in Equation 31and in the last inequality we used Lemma D.4.</p>
<p>C. Proofs Section 5</p>
<p>Lemma 5.1 (Variance Concentration (Panaganti &amp; Kalathil, 2022)).For all s ∈ S, with probability at least 1 − δ we have
σ 2 (s) − σ2 t (s) ≤ F max 2 log(2St 2 /δ) T + t (s) := α(t, s, δ)
Proof.From (Panaganti &amp; Kalathil, 2022), we have that for a fixed time k ≤ t, it holds that with probability at least 1 − δ:
σ 2 (s k ) − σ2 t (s k ) ≤ F max 2 log(2S/δ) T + t (s)
Since we want the result to hold ∀k ≤ t, we simply union bound over time which leads to the desired result.</p>
<p>Proposition 4 (Gradient-Reward Invariances).If f is invariant over states s and s ′ , then ∀s,
s ′ ∈ [s], ∀a, a ′ ∈ A ∇ λ L + t k −1 (λ)[s, a] = ∇ λ L + t k −1 (λ)[s ′ , a ′ ]
Proof.The proof simply follows by computing the gradient explicitly.In particular, we have that
∇ λ L + t k −1 (λ) [s,a] = −E s 2σ 2 t k −1 (s) + α(t k − 1, s, δ) 2S s∈[s] b∈A λ(s, b) + E s η 3 2 = ∇ λ L + t k −1 (λ) [s ′ ,a ′ ]
Indeed, we notice that the gradient does not directly depend on the actions a and a ′ since in the denominator we are summing over all the possible actions.Furthermore, since both s, s ′ ∈ [s], the gradient remains unchanged since it depends exclusively on the abstract state s and not directly on s and s ′ .In particular, since s and s ′ are in the same equivalence class, then the corresponding abstract state s is the same.</p>
<p>D. Proofs Section 6</p>
<p>Theorem 6.1 (Regret Guarantee).If algorithm GAE is run with a budget of n samples and τ k = 3k 2 − 3k + 1 then w.p. at least 1 − δ, it holds that:
R n = O Φ 1 2 S 1 2 AF max σ 2 max η 5 2 1 n 1/3
Proof.The analysis follows a classic Frank-Wolfe (FW) scheme analysis while taking into account the approximation error due to the optimistic estimate of the gradient, and the estimation error due to the gap between the asymptotic distribution associated with π + k+1 and the induced empirical frequences λ k+1 at each iteration k.It diverges from previous analysis for non-episodic AE settings (Tarbouriech &amp; Lazaric, 2019;Tarbouriech et al., 2020) in two main ways: (i) by leveraging ergodicity and hence uniqueness of the stationary distribution induced by Markovian stationary policies, we study the density estimation process via (Mutny et al., 2023, Lemma 5), (ii) we introduce a dependency on the geometric compression term Φ (Definition 2) in order to show the effect of compression on the final sample complexity result in Theorem 6.2.</p>
<p>As the regret in Convex RL is interpreted as a suboptimality gap (Mutti et al., 2023;2022a;Tarbouriech &amp; Lazaric, 2019), we first derive an upper bound on the approximation error, as defined in Equation 22, achieved at the end of iteration k ∈ [K] of Algorithm 1.In the following we denote with t k+1 the number of samples gathered until the end of iteration k, formally t k+1 := k j=1 τ j , where τ j is the number of steps policy π + k+1 has been released in iteration k.Then by defining L := L ∞,η , we derive the following.33) where in step (9) we use C η -smoothness of L, in step (10) we use the definition of the update step of FW and the fact that υ * k+1 is optimal and in (11) we use again C η -smoothness to bound ⟨∇L( λk ), λ * − λk ⟩.Note that in the first term ϵ k+1 , we take into account the discrepancy between the state-action distribution υ * k+1 induced by the optimal policy w.r.t. the MDP with the exact gradient as reward, and our exact solution υ + k+1 of the MDP with the optimistic gradient as reward.In the second term ∆ k+1 , we take into account the error due to the gap between the state-action distribution υ + k+1 and the empirical distribution υk+1 induced by deploying policy π + k+1 for τ k steps.In the following, we upper bound independently the terms ∆ k+1 and ϵ k+1 .
ρ k+1 := L(λ k+1 ) − L(λ * ) (32) = L((1 − β k )λ k + β k υk+1 ) − L(λ * ) (where β k := τ k /(t k+1 − 1)) (9) ≤ L(λ k ) − L(λ * ) + β k ⟨∇L(λ k ), υk+1 − λ k ⟩ + C η β 2 k = L(λ k ) − L(λ * ) + β k ⟨∇L(λ k ), υ * k+1 − λ k ⟩ + β k ⟨∇L(λ k ), υk+1 − υ * k+1 ⟩ + C η β 2 k (10) ≤ L(λ k ) − L(λ * ) + β k ⟨∇L(λ k ), λ * − λ k ⟩ + β k ⟨∇L(λ k ), υk+1 − υ * k+1 ⟩ + C η β 2 k (11) ≤ (1 − β k )ρ k + C η β 2 k + β k ⟨∇L(λ k ), υ + k+1 − υ * k+1 ⟩ ϵ k+1 +β k ⟨∇L(λ k ), υk+1 − υ + k+1 ⟩ ∆ k+1(</p>
<p>D.1. Upper Bound ∆ k+1</p>
<p>We derive a PAC guarantee on the density estimation error by using Lemma E.1 with η t (s) = η(s) = λ(s), as in our case λ is fixed within one FW iteration and set
f s (•) = f s t (•) := 1 T ⟨•, δ s ⟩,
where δ s is the vector of all zeros except in position s where it has a one and where ⟨•, •⟩ denotes the inner product.In particular, in this case, f s corresponds to the evaluation functional of a probability distribution in state s.Note that this functional is clearly linear as requested by the proposition as the inner product is linear.Furthermore, we can notice that ||f s || ∞ = 1</p>
<p>T and hence we can restate the bound presented in the Lemma as follows:
τ t=1 I{s t = s} τ − λ(s) ≤ 2 τ log 2 δ ′ with probability at least 1 − δ ′
Since we want the statement above to hold uniformly for every state s ∈ S and for every possible abstract policy, we set δ = δ ′ SA S and apply a union bound, obtaining:
τ t=1 I{s t = s} τ − λ(s) ≤ 2 τ log   2SA S δ   with probability at least 1 − δ (34)
Then, we can bound ∆ k+1 as follows.
∆ k+1 = ⟨∇L(λ k ), υk+1 − υ + k+1 ⟩ = − 1 2S s E s 2σ 2 (s) λ k (s) + E s η 3 2 a υk+1 (s, a) − υ + k+1 (s, a) (12) = − 1 2S s E s 2σ 2 (s) λ k (s) + E s η 3 2 υk+1 (s) − υ + k+1 (s) (13) ≤ − 1 2S s E − 1 2 s 2σ 2 (s) η 3 2 υk+1 (s) − υ + k+1 (s) (14) ≤ Φ 1/2 S σ 2 max Sη 3/2 υk+1 − υ + k+1 ∞ (15) ≤ Φ 3/2 σ 2 max η 3/2 2 τ k log   2SA S δ   with probability at least 1 − δ(35)
where in ( 12) we consider the state densities by summing over the actions, in (13) we lower bound λ(s) ≥ 0, in ( 14) we use that E s = 1 Φ in addition to taking the infinity norm and in (15) we use the PAC bound in Equation 34.</p>
<p>D.2. Upper Bound ϵ k+1</p>
<p>Next, we define as ∇ L + t k −1 (λ) the empirical optimistic gradient it iteration k, defined as in Equation 20, but replacing the true variance with its empirical counterpart, and by using Lemma 5.1 we upper bound the true gradient ∇L(λ) with a term containing the gradient estimate ∇ L t k −1 (λ) (same as ∇ L + t k −1 (λ) but without the α).More explicitly, we have that:
∇ L + t k −1 (λ)(s, a) = −E s 2σ 2 t k −1 (s) + α(t k − 1, s, δ) 2S λ(s) + E s η 3 2 ∇ L t k −1 (λ)(s, a) = −E s 2σ 2 t k −1 (s) 2S λ(s) + E s η 3 2 ∇L(λ)(s, a) = −E s 2σ 2 (s) 2S λ(s) + E s η 3 2
In particular, we obtain that with probability at least 1 − δ:
∇ L + t k −1 (λ)(s, a) = ∇ L t k −1 (λ)(s, a) − 1 2ΦS α(t k − 1, s, δ) λ(s) + E s η 3/2 ≤ ∇L(λ)(s, a
) and ( 36)
∇L(λ)(s, a) ≤ ∇ L t k −1 (λ)(s, a) + 1 2ΦS α(t k − 1, s, δ) λ(s) + E s η 3/2 (37)
Hence, we get that:
∇L λ , υ + k+1 = s,a υ + k+1 (s, a)∇L λ (s, a) (16) ≤ s,a υ + k+1 (s, a)∇ L t k −1 λ (s, a) + 1 2ΦS s,a υ + k+1 (s, a) α (t k − 1, s, δ) λ(s) + E s η 3/2 (17) ≤ s,a υ + k+1 (s, a)∇ L + t k −1 λ (s, a) + 1 ΦS s,a υ + k+1 (s, a) α (t k − 1, s, δ) λ(s) + E s η 3/2 = ∇ L + t k −1 λ , υ + k+1 + 1 ΦS s,a υ + k+1 (s, a) α (t k − 1, s, δ) λ(s) + E s η 3/2 (18) ≤ ∇ L + t k −1 λ , υ ⋆ k+1 + 1 ΦS s,a υ + k+1 (s, a) α (t k − 1, s, δ) λ(s) + E s η 3/2 (19) ≤ ∇L λ , υ ⋆ k+1 + 3 2ΦS s,a υ + k+1 (s, a) α (t k − 1, s, δ) λ(s) + E s η 3/2(38)
where in ( 16) we used Equation 37, in (18) we used the optimality of υ + k+1 and in ( 17) and ( 19) we used Equation 36.Using the definition of ϵ k+1 from Equation (33), by rearranging the terms in Equation ( 38) we get:
ϵ k+1 = ⟨∇L(λ k ), υ + k+1 − υ * k+1 ⟩ ≤ 3 2ΦS s,a υ + k+1 (s, a) α (t k − 1, s, δ) λ k (s) + E s η 3/2 ≤ 3 2ΦS s,a υ + k+1 (s, a) α (t k − 1, s, δ) (E s η) 3/2(39)
where in the last inequality we simply lower-bound λ k (s) ≥ 0.</p>
<p>In the following we denote with T k (s) := T t k −1 (s) the number of visits of state s from the start until iteration k − 1 of the FW scheme (i.e., at time t k−1 ).We now plug in the definition of α (t, s, δ) = F max 2 log(2St 2 /δ) Tt(s)</p>
<p>, coming from Lemma 5.1, in Equation 39, leading to:
ϵ k+1 ≤ s,a υ + k+1 (s, a) 3F max 2ΦS( 1 Φ η) 3/2 2 log 2S (t k − 1) 2 δ 1 T k (s) = s,a υ + k+1 (s, a) 3Φ 1/2 F max 2Sη 3/2 2 log 2S (t k − 1) 2 δ 1 T k (s) ≤ c 0 s,a υ + k+1 (s, a) 1 T k (s) (20) = c 0 s,a υ k+1 (s, a) 1 T k (s) γ k + c 0 s,a υ + k+1 (s, a) − υ k+1 (s, a) 1 T k (s) Γ k+1(40)
with
c 0 = 3Φ 1/2 Fmax 2Sη 3/2 2 log 2 ST 2 δ
, where T := t K − 1 and K is the total number of FW iterations and where in (20) we simply added and substracted a term c 0 s,a υ
+ k+1 (s, a) 1 √ T k (s)
. We can now bound Γ k+1 similarly to ∆ k+1 using Lemma E.1.In particular, by upper-bounding
1 √ T k (s)
≤ 1, we get that with probability at least 1 − δ, we have that:
Γ k+1 ≤ c 0 S 2 τ k log   2SA S δ  
Finally, by plugging in the bound of Γ k+1 we just derived into Equation 40, we have that with probability at least 1 − δ:
ϵ k+1 ≤ c 0 γ k + c 0 S 2 τ k log   2SA S δ   where c 0 = 3Φ 1/2 F max 2Sη 3/2 2 log 2ST 2 δ and γ k = s,a υk+1 (s, a) T k (s)(41)
In the next steps, we will introduce an explicit time dependency on τ k , which we will use to simplify the expression of the approximation error and finally perform a recursion leading to the final expression of the regret.First, we recall from Equation 33that with probability at least 1 − δ, we have:
ρ k+1 ≤ (1 − β k )ρ k + C η β 2 k + β k ϵ k+1 + β k ∆ k+1
By plugging in the upper bounds of ϵ k+1 and ∆ k+1 that we derived in ( 35) and ( 41), we get:
ρ k+1 ≤ (1 − β k )ρ k + C η β 2 k + β k   c0γk + c 0 S 2 τ k log   2SA S δ      + β k Φ 3/2 σ 2 max η 3/2 2 τ k log   2SA S δ   = (1 − β k )ρ k + C η β 2 k + β k c 0 S + Φ 3/2 σ 2 max η 3/2 2 τ k log   2SA S δ   c1/ √ τ k +β k c 0 γ k(42)
Choosing t k = τ 1 (k − 1) 3 + 1, we get:
τ k = t k+1 − t k = τ 1 (3k 2 − 3k + 1) ≥ 3τ 1 k 2 and β k = τ k t k − 1 ≤ 3 k(43)
Hence, using (43) and the fact that τ 1 ≥ 1, we can further upper-bound together the second and third terms in Equation ( 42) as follows:
C η β 2 k + β k c 1 √ τ k ≤ 9 C η k 2 + √ 3c 1 k 2 =: b δ k 2 (44)
Plugging this in (42) gives:
ρ k+1 ≤ (1 − β k )ρ k + b δ k 2 + β k c 0 γ k (45)
We now want to compute the recursion on ρ in order to find the approximation error after K iterations.In order to do so, we will closely follow the steps from (Tarbouriech &amp; Lazaric, 2019), which we will report for completeness.We choose q ≥ S/τ 1 1 3 + 1 for later use, and we introduce the sequence (u k ) k≥q with u q = ρ q and
u k+1 = 1 − 1 k u k + b δ k 2 + β k c 0 γ k (46)
From Inequality 45, we have ρ k ≤ u k and by induction we can see that (u k ) ≥ 0. By rearranging the terms in (46), we get:
(k + 1)u k+1 − ku k = −u k k + b δ (k + 1) k 2 + (k + 1)β k c 0 γ k ≤ b δ (k + 1) k 2 + (k + 1)β k c 0 γ k Let K ≥ q.
We can now apply a telescoping sum starting at q and ending at K and exploit the fact that β k ≤ 3/k ≤ 6/(k +1) which leads to:
Ku K − qu q ≤ 2b δ K−1 k=q 1 k + 6c 0 K−1 k=q γ k ≤ 2b δ log K − 1 q − 1 + 6c 0 K−1 k=q γ k (47)
where the qu q term appears as it corresponds to the first term of the telescoping sum.</p>
<p>By rearranging the terms in (47) and using that ρ K ≤ u K as previously observed, we have with probability at least 1 − δ:
ρ K ≤ u K ≤ qρ q + 2b δ log K K + 6c 0 K K−1 k=q γ k (48) = τ 1/3 1 (t K − 1) 1/3 + τ 1/3 1   qρ q + 2b δ log K + 6c 0 K−1 k=q γ k  (49)
By employing (Tarbouriech &amp; Lazaric, 2019, Lemma 6) with q ≥ S/τ 1 1/3 + 1 as previously set, we get the following upper bound:
ρ K ≤ τ 1/3 1 (t K − 1) 1/3 + τ 1/3 1 qρ q + 2b δ log K + 6c 0 Σ τ 1
where C η is the smoothness constant of the objective function, which can be bounded as in Lemma E.2 and Σ := S log( τ1(K−1) 3</p>
<p>S</p>
<p>).</p>
<p>To showcase better interpretability of the final result and in particular to show the advantage of exploiting symmetries as in our method, we now upper-bound the error ρ K in such a way to make its dependence on the compression coefficient explicit.We choose the tightest possible q and by using the bounds for qρ q and b δ from Lemma D.3 and Lemma D.2 we get:
ρ K = O τ 1/3 1 (t K − 1) 1/3 + τ 1/3 1 AΦ 1 2 F max σ 2 max √ S η 5 2 + AΦ 2 F max σ 2 max √ S η 5 2 + 3Φ 1/2 F max 2Sη 3 2 2 log 2ST 2 δ S log( τ1(K−1) 3 S ) τ 1 = O AΦ 1 2 S 1 2 F max σ 2 max η 5 2 1 t K 1 3 = O AΦ 1 2 S 1 2 F max σ 2 max η 5 2 1 n 1 3
where in the last step we used that n =
K k=1 τ k = K k=1 Θ(k 2 ) = Θ(K 3 ) = Θ(t K
) and where T = t K − 1.</p>
<p>Theorem 6.2 (Sample Complexity of Geometric Estimation Objective).If algorithm GAE is run with τ k = 3k 2 − 3k + 1, for:
n = O Φ 3 2 S 3 2 A 3 F 3 max (σ 2 max ) 3 2 η 15 2 ϵ 3
samples, then we have that with probability at least 1 − δ:
P L ∞,η (λ n ) − L ∞,η (λ * ) ≤ ϵ ≥ 1 − δ Proof.
The result simply follows by inverting the regret guarantee in Theorem 6.1.</p>
<p>Lemma D.1.We have the following bound for the approximation error at time q:
ρ q ≤ √ Φ 2σ 2 max √ η • q + 2b δ log q q + 9 • Φ 1/2 F max Sη 3/2 2 log 2 Sq 6 δ
Proof.We begin by noting that we can derive an equivalent bound of the error at time q in the same way as in Equation 48by deploying the telescoping series from 1 to q − 1 hence getting:
ρ q ≤ ρ 1 + 2b δ log q q + 6c 0 q q−1 k=1 γ k = ρ 1 + 2b δ log q q + 6c 0 q q−1 k=1 s,a υk+1 (s, a) T k (s) ≤ ρ 1 + 2b δ log q q + 6c 0 q q−1 k=1 s,a υk+1 (s, a) =1 ≤ ρ 1 + 2b δ log q q + 6c 0(50)
We now proceed in bounding the error ρ 1 , by first recalling the definition we gave in Equation 22:
ρ 1 := L λ2 − L (λ * ) ≥0 ≤ L λ2 ≤ √ Φ 2σ 2 max √ η
where in the last inequality we use Lemma E.3.Hence, by plugging this bound in Equation 50 and expliciting c 0 as given in Equation 41, we get:
ρ q ≤ √ Φ 2σ 2 max √ η • q + 2b δ log q q + 6c 0 = √ Φ 2σ 2 max √ η • q + 2b δ log q q + 9 • Φ 1/2 F max Sη 3/2 2 log 2 Sq 6 δ Lemma D.2. It holds that: b δ = O AΦ 2 F max σ 2 max √ S η 5/2 Proof. L n (λ) = 1 S s∈S F(s, λ) = 1 S s∈S E s 2σ 2 (s) b∈A s∈[s] λ(s, b) + 1 n + 1 √ n F max b∈A s∈[s] λ(s, b) + 1 n (23) ≤ 1 S s∈S E s 2σ 2 (s) b∈A s∈[s] λ(s, b) + E s η + 1 √ n F max b∈A s∈[s] λ(s, b) + E s η = L ∞,η (λ) + 1 S s∈S E s 1 √ n F max b∈A s∈[s] λ(s, b) + E s η ≤ L ∞,η (λ) + 1 S s∈[s] E s F max √ nE s η = L ∞,η (λ) + SF max S √ nη
where in ( 23) we used that E s η ≤ 1 n and in the last inequality we lower bounded λ(s, b) ≥ 0.</p>
<p>E. Auxiliary Lemmas</p>
<p>Lemma E.1 ( (Mutny et al., 2023), Lemma 5).Let {η t } t=1 be an adapted sequence of probability distributions on X , P(X ) with respect to filtration F t−1 .Likewise let {f t } t=1 be an adapted sequence of linear functionals f
t : P(X ) → R s.t. ||f t || ∞ ≤ B t .
Also, let x t ∼ η t , and δ t (x) = I {xt=x} , then:
P   T t=1 f t (δ t − η t ) ≥ 2 T t=1 B 2 t log 2 δ   ≤ δ
Lemma E.2 (Bound of Smoothness constant).The smoothness constant C η can be bounded by:
C η ≤ A √ 2Φ 5 σ 2 max η 5/2
Proof.Given the objective L(λ), we have that its Hessian is made up of second order partial derivatives of the form
∂ 2 L(λ) ∂λ(s ′ , a ′ ) 2 = 3 4 1 ΦS 2σ 2 (s ′ ) λ(s ′ ) + E s η 5 (56)
when both partial derivatives are taken w.r.t. the same coordinate, while the mixed second order partial derivatives are given by: ∂L
(λ) ∂λ(s ′ , a ′ )∂λ(s ′′ , a ′′ ) = 0(57)
Hence the the Hessian H(L) is a diagonal matrix, thus containing only its eigenvalues.In particular, we can upper bound the value of the biggest eigenvalue as:
max v∈σ(H(L)) v ≤ (s,a)∈S×A H(L)(λ) ((s,a),(s,a))(58)
which corresponds to summing all entries on the diagonal of the Hessian, and where σ(A) stands for the spectrum of A.</p>
<p>Hence, noting that E s = 1 Φ we have that ∀ λ ∈ Λ:
max v∈σ(H(L)) v ≤ (s,a)∈S×A H(L)(λ) ((s,a),(s,a)) (59) = (s,a)∈S×A ∂ 2 L(λ) ∂λ(s a) 2 (60) = (s,a)∈S×A 3 4 1 ΦS 2σ 2 (s) 1 Φ η + λ(s) 5 (61) ≤ (s,a)∈S×A 1 S 2Φ 3 σ 2 (s) η 5/2 (62)
where in the last step we have used the fact that λ ∈ Λ.</p>
<p>Since L is twice continuously differentiable over Λ, by (Nesterov, 2014, Theorem 2.1.6), we have that:
H(L) λ ⪯ (s,a)∈S×A 1 S 2Φ 3 σ 2 (s) η 5/2 I (63)
where I is the identity matrix, implying that for
C η ≤ (s,a)∈S×A 1 S √ 2Φ 3 σ 2 (s) η 5/2 , L is C η -smooth on Λ.
From this, we have that:
C η ≤ A √ 2Φ 5 σ 2 max η 5/2 ≤ A √ 2Φ 5 σ 2 max η 5/2
where in the last step we used the fact that A ≤ A, since we want to give a bound depending on the quantities of the original MDP.</p>
<p>Lemma E.3.The following bound holds for the asymptotic objective:
L ∞,η (λ) ≤ √ Φ 2σ 2 max √ η
Proof.Using that λ(s, a) := s∈[s] λ(s, a), we get:
L ∞,η λ = 1 ΦS s∈S 2σ 2 (s) λ(s) + E s η ≤ 1 ΦS 2σ 2 max s∈S 1 λ(s) + E s η = 1 ΦS 2σ 2 max s∈S 1 λ(s) + E s η (24) ≤ 2σ 2 max ΦS s∈ S 1 √ E s η (25) = 2σ 2 max ΦS S η Φ (26) = 2σ 2 max √ ΦS ΦS √ η = √ Φ 2σ 2 max √ η
where in (24) we used that λ(s) ≥ 0, in (25) we used that E s = 1 Φ and in (26) we used that S = ΦS.
27) = E s (64) = |[s]|((28) = |Orbit(s)| ((65)) (29) = |G| |Stab(s)|66
where step ( 27) is due to Assumption 6.1, in step ( 28) we employ the definition of Orbit (Rotman, 2010, Chapter C-1, page 6), and in step ( 29) we leverage the Orbit-Stabilizer Theorem (Rotman, 2010, Theorem C-1.16).</p>
<p>F. Experimental Details</p>
<p>In the following, we provide further details about the experiments carried out in this work.We first present the environments, their invariances and resulting abstract environments.Subsequently, we provide additional information regarding the implementation of GAE.</p>
<p>F.1. Pollutant Diffusion Process</p>
<p>MDP: We consider the problem of actively measuring the amount of pollutant released to the environment.The pollutant is released from a point source and spreads radially outwards through a diffusion process.The measurement setup is displayed in Figure 1.As can be seen, the measurement stations are aligned in two ways.1) Radially outwards from the point source.This allows to measure the variation of the pollutant amount in the radial direction.We will refer to a set of states that are aligned in this way as a ray.2) Along circles of different radii to measure the variation in the azimuthal direction.We will refer to a set of states aligned in this way as a circle.Each measurement station corresponds to a state in the MDP where the agent obtains noisy measurements of the pollutant amount.In our setup, there are a total of 30 circles and 8 rays, leading to a total of 240 states.The action space consists of five actions, {in, out, clockwise, anticlockwise, stay}.We consider both deterministic and stochastic dynamics.In the deterministic case, if the agent chooses action in it moves one state closer to the point source along the ray (one further away for out).The actions leading to transitions along the circle are clockwise and anticlockwise.The action stay, makes the agent to remain in the same state and repeat the experiment.In the case of stochastic transitions, the agent moves to it's intended state with probability q and with probability 1 − q to another reachable state randomly chosen.In the experiments conducted we used q = 0.98.</p>
<p>Invariances of f : We consider the case where the diffusion process is radially symmetric, meaning that for all the states on a circle, the pollution is the same.Therefore, f is invariant over different rays, as illustrated already in 1.</p>
<p>Abstract MDP: The aforementioned invariances on f make it possible to define MDP homomorphisms h mapping the original MDP to abstract MDPs.In the experiments conducted, we considered three different homomorphisms, resulting in three different geometric compression terms Φ.The homomorphisms simply differ by how the rays are compressed.</p>
<p>The first homomorphism h 1 , maps two consecutive rays into one, resulting in a compression of the 8 rays into 4.The second homomorphism h 2 , compresses the 8 rays into 2 and the third h 3 maps all 8 rays into 1 resulting in the compression illustrated in Figure 1.The state map ψ therefore maps states on consecutive rays together.The state-dependent action map ϕ s corresponds to the identity map.</p>
<p>Implementation Details: The function f (s), is modeled as increasing the closer the state is to the source.From the first equivalence class on the innermost circle to the 30th equivalence class consisting of the outermost circle, f decreases in steps of 300, starting at 9300.The noise ν(s) is modeled as increasing with function value as often large measurements are associated with larger variances.The corresponding standard deviations are hence also decreasing from the states closest to the source to the outermost states.The standard deviations decrease by steps of 100, starting at 3100.The distribution of ν(s) was taken to be a 0 mean Gaussian with the standard deviations given above.As an example, for a state s on the 21st circle we have f (s) = 3000 and ν(s) ∼ N (0, 1000).The smoothness parameter was chosen to be η = 0.001, and δ = 0.01 for both, deterministic and stochastic dynamics.Furthermore, we found that in practice, a constant number of interactions τ k = τ for all the K iterations of GAE works well, especially for remarkably low τ .In this setting, we chose τ = 3, which makes the algorithm more adaptive, resulting in the rapid exploration of different equivalence classes.To update the abstract state-action frequency λ k+1 , we also use a constant update step of 0.005/S.The initial state of the agent was chosen on the outermost circle.n as 210, resulting in K = 70 iterations of GAE.All the experiments were repeated over 15 random seeds.The computational time was measured using a standard time library in Python.The main part of the computational time can be attributed to solving the MDP using value iteration.We applied the Bellman optimality operator until there was no change in the value function up to the 5th digit.</p>
<p>F.2. Toxicity of Chemical Compounds</p>
<p>MDP: As a second experiment we consider the experimental design problem of estimating the toxicity of chemical compounds.Similarly as in (Schreck et al., 2019;Dong et al., 2022;Thiede et al., 2022), we consider an MDP where a chemical compound is represented as a string where every character in the string stands for a base chemical element.The goal of the agent is to estimate the toxicity associated to all possible compounds that can be generated using the base chemical elements.In our setting we consider three base chemical elements A, B, and C. We limit the maximum length of the string to 5. The state space therefore consists of all possible chemical compounds that have at most 5 base elements and the cardinality is S = 363.The action space consists of the three base elements, and an action that makes the agent stay in the same state A = {A, B, C, stay}.By taking an action corresponding to a base element, the agent appends this element to the current compound.Once the agent reaches a compound with 5 base elements it can either choose to measure the toxicity of that compound again by picking the action stay or pick a new base element to start another compound.The agent may therefore transition from one compound s of length l to another compound s ′ of length l + 1 if the first l base elements are the same.</p>
<p>Invariances of f : We assume that the toxicity of a chemical compound is invariant under permutations of the compound, such that f (s) = f (s ′ ) if s is a permutation of s ′ .The abstract state-space therefore has a cardinality of S = 55</p>
<p>Abstract MDP: These invariances on f again allow us to define an MDP homomorphism that maps the original MDP to an abstract one.In this case the state map ψ maps all the states which are equivalent up to permutation to one abstract state.</p>
<p>The state-dependent action map ϕ s is simply the identity.This results in an abstract MDP where the agent can transition from one abstract state s to another one s ′ if all the base compounds making up s are also contained in s ′ and the agent chooses the action corresponding to the chemical compound that is not yet in s ′ .As an example consider s = CABA and s ′ = AABCC, then the agent may transition from s to s ′ by choosing action C</p>
<p>Implementation Details: We model the toxicity of the chemical compounds and the noise as a piecewise constant functions of it's base chemical elements.For every A in the compound, f is increased by 200, for every B, C there is an increase of 400 and 600 respectively.Similarly, to the diffusion environment, we assume that higher measurements of toxicity are associated with higher standard deviations and that the noise has a Gaussian distribution with 0 mean.For every A the standard deviation increases by 100, for every B, C, the standard deviations increase by 200 and 300 respectively.As an example consider the compound AABC, then f (AABC) = 1400 and ν(AABC) ∼ N (0, 700).We let GAE run for n = 2400 with a constant step size of τ k = 20 ∀k ∈ [K], resulting in a total of K = 80 iterations of GAE.The smoothness constant chosen was η = 0.0007 and δ = 0.01.To update the abstract state-action frequency λ k+1 , we also use a constant update step of 0.005/S.The experiments were repeated over 15 different random seeds.</p>
<p>Figure 1 .
1
Figure 1.Radial diffusion process of a pollutant from a central point source.On the left, original MDP where each circle is an fequivalence class, Lg denotes a state symmetry acting on f , K s g denotes a state-dependent action symmetry acting on P .On the right, the abstract MDP obtained via the MDP homomorphism h = (ψ, {ϕs | s ∈ S}), where ψ maps f -equivalence classes to abstract states.</p>
<p>Processes.A (discrete) Controlled Markov Process (CMP) is a tuple M := (S, A, P, µ), where S is a finite state space (|S| = S), A is a finite action space (|A| = A), P : S × A → ∆(S) is the transition model, such that P (s ′ |s, a) denotes the conditional probability of reaching s ′ ∈ S when selecting a ∈ A in s ∈ S, and µ : ∆(S) is the initial state distribution.A CMP M paired with a function r : S × A → R, i.e., M r := M ∪ r, is a Markov Decision Process (MDP) (Puterman, 2014).</p>
<p>in step (2) we used f invariances and in step (3) we used that E s := |[s]| and fn (s) := 1</p>
<p>Proposition 5 (
5
Compression via Group Cardinality).Given a set of group-structured state symmetries G = ({L g } g∈G , •) and Stab(s) = Stab(s ′ ) ∀s, s ′ ∈ S then: Φ = |Stab(s)| |G| where Stab(s) := {g ∈ G : L g [s] = s}.Proof.We consider the set S, the group G = ({L g } g∈G , •) = (G, •) of transformations acting on S via the group action * : G × S → S. Due to Assumption 6.1 we have the following.1 Φ</p>
<p>) Diffusion Runtime (SD) Comparison of GAE with AE.GAE shows better statistical and computational efficiency.Experiments were carried out over 15 seeds and confidence intervals shown are ± one standard deviation.2a the statistical advantage of GAE with compression Φ against AE for deterministic dynamics in the diffusion environment.2b same setting as 2a, but with stochastic dynamics.2c the (classic) estimation error taken over the abstract state space.2d computational advantage of GAE over AE for different degrees of compression (standardized).2e the statistical advantage of GAE over AE in the strings environment.2f the strings environment and the invariance of f under permutation.
1000GAE, Φ = 0.15AE800ξ n60040020000100200300400500600n(e) Strings Statistical Performance(f) Chemical compound generationFigure 2.
Theorem 6.2 (Sample Complexity of Geometric Estimation Objective).If algorithm GAE is run with τ k = 3k 2 − 3k + 1, for:</p>
<p>≜</p>
<p>Abstract reward estimated at iteration k via empirical density λ k
y T t (s) ft (s) σ2 t (s) f A t (s) ft (s) ξn ϵ δ L ∞,η rk λ k π + k+1 π + k+1 τ k υk+1 υk+1 λ k+1 υ + k+1 λ  *  λ  *  n L n υ  *  krandom variable with zero mean and unknown variance σ 2 ≜ Noisy random variable y(s) = f (s) + ν(s) ≜ Visitation counts for state s after t steps, see Equation (8) ≜ Empirical mean for state s after t steps, see Equation (9) ≜ Empirical variance for state s after t steps, see Equation (10) ≜ Average empirical mean for equivalence class [s], see Equation (12) ≜ Empirical mean for abstract state s after t steps ≜ Geometric estimation error, see Equation (11) ≜ Controllable approximation error for PAC guarantees ≜ Controllable probability of error for PAC guarantees ≜ Finite-samples Convex RL Objective (31) ≜ Asymptotic and smoothened Convex RL Objective (19) ≜ Optimal abstract policy w.r.t. MDP M k r ≜ Optimal original policy at iteration k obtained by lifting π k+1 ≜ Length of trajectory of policy deployed at iteration k ≜ Empirical state-action distribution induced during iteration k ≜ Abstract state action distribution obtained at iteration k by aggregating υk+1 ≜ Updated state-action frequency at end of k-th iteration Regret and Sample Complexity Analysis ≜ state-action distribution induced by π + k+1 ≜ optimal abstract state-action distribution of the learning problem ≜ optimal state-action distribution of the learning problem k ≜ optimal state-action distribution of the MDP M r
ξ (ϵ, δ) ≜ Sample complexity for PAC estimation of geometric estimation error ξn , see def. 1 n ≜ Sample complexity for PAC optimality of geometric estimation objective, see def.6.2</p>
<p>Note that in the paper we conceptualize symmetries to be exact while they may be approximate in practice. Extending this work to deal with approximate symmetries is a nice direction for future works.
Here we extend f such that f (s, a) := f (s) ∀a ∈ A and consider y to satisfy the same set of invariances as f .
Notice that the set of admissible abstract state-action distributions Λ can be defined analogously to Λ.
we do not explicitly specify the identity and inverse elements of the group as we do not use them in the following
AcknowledgementThis publication was made possible by the ETH AI Center doctoral fellowship to Riccardo De Santi.We would like to thank Emanuele Rossi, Ossama El Oukili, Massimiliano Viola, Marcello Restelli, and Michael Bronstein for collaborating on a previous version of this project.Moreover, we wish to thank Mojmír Mutný and Manish Prajapat for the insightful discussions.The project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research, innovation program grant agreement No 815943 and the Swiss National Science Foundation under NCCR Catalysis grant number 180544.Impact StatementThis paper presents work whose goal is to advance the field of Reinforcement Learning.There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.Proof.Using the definition of b δ from Equation44, we have:where in 21 we plug in the value of c 1 from Equation42and we use the upper bound of the smoothness constant from Lemma E.2 and in 22 we plug in the definition of c 0 from Equation50.Hence, we have:(55)Lemma D.3.It holds that:Proof.From Lemma D.1 we get:where in the second equality we used the fact that q ≥ (S/τ 1 ) 1/3 + 1 and 1 τ1 ≤ 1. Next, we plug in the result of Lemma D.2 into b δ in the last equation to get:Lemma D.4.For E s η ≤ 1 n and ∀λ ∈ Λ, we can bound the non asymptotic objective function as:
Apprenticeship learning via inverse reinforcement learning. P Abbeel, A Y Ng, International Conference on Machine Learning. 2004</p>
<p>Expertguided symmetry detection in Markov decision processes. G Angelotti, N Drougard, C P Chanel, arXiv:2111.102972021arXiv preprint</p>
<p>Online abstraction with mdp homomorphisms for deep learning. O Biza, R Platt, International Conference on Autonomous Agents and Multiagent Systems. 2018</p>
<p>Bayesian experimental design: A review. K Chaloner, I Verdinelli, Statistical Science. 1995</p>
<p>Molecular design in drug discovery: a comprehensive review of deep generative models. Y Cheng, Y Gong, Y Liu, B Song, Q Zou, Briefings in Bioinformatics. 2262021</p>
<p>Deep learning in retrosynthesis planning: datasets, models and tools. J Dong, M Zhao, Y Liu, Y Su, X Zeng, Briefings in Bioinformatics. 2312022</p>
<p>Deep learning for molecular design-a review of the state of the art. D C Elton, Z Boukouvalas, M D Fuge, P W Chung, Molecular Systems Design &amp; Engineering. 442019</p>
<p>A comprehensive survey on safe reinforcement learning. J Garcıa, F Fernández, Journal of Machine Learning Research. 1612015</p>
<p>Concave utility reinforcement learning: The mean-field game viewpoint. M Geist, J Pérolat, M Laurière, R Elie, S Perrin, O Bachem, R Munos, O Pietquin, International Conference on Autonomous Agents and Multiagent Systems. 2022</p>
<p>Provably efficient maximum entropy exploration. E Hazan, S Kakade, K Singh, A Van Soest, International Conference on Machine Learning. 2019</p>
<p>Projection-free sparse convex optimization. M Revisiting Jaggi, Frank-Wolfe, International Conference on Machine Learning. 2013</p>
<p>Improving UCT planning via approximate homomorphisms. N Jiang, S Singh, R Lewis, International Conference on Autonomous Agents and Multiagent Systems. 2014</p>
<p>Optimizing sensing: Theory and applications. A Krause, 2008Carnegie Mellon UniversityPhD thesis</p>
<p>Concentration of measure. J Lafferty, H Liu, L Wasserman, 2008</p>
<p>Active exploration for inverse reinforcement learning. D Lindner, A Krause, G Ramponi, Advances in Neural Information Processing Systems. 2022</p>
<p>Symmetry learning for function approximation in reinforcement learning. A Mahajan, T Tulabandhula, arXiv:1706.029992017arXiv preprint</p>
<p>A simple approach for state-action abstraction using a learned mdp homomorphism. A N Mavor-Parker, M J Sargent, A Banino, L D Griffin, C Barry, arXiv:2209.063562022arXiv preprint</p>
<p>Equivariant representations for data-efficient reinforcement learning. A K Mondal, V Jain, K Siddiqi, S Ravanbakhsh, Eqr, International Conference on Machine Learning. 2022</p>
<p>Active exploration via experiment design in Markov chains. M Mutny, T Janik, A Krause, International Conference on Artificial Intelligence and Statistics. 2023</p>
<p>Challenging common assumptions in convex reinforcement learning. M Mutti, R De Santi, P De Bartolomeis, M Restelli, Advances in Neural Information Processing Systems. 2022a</p>
<p>The importance of non-Markovianity in maximum state entropy exploration. M Mutti, R De Santi, M Restelli, International Conference on Machine Learning. 2022b</p>
<p>Convex reinforcement learning in finite trials. M Mutti, R De Santi, P De Bartolomeis, M Restelli, Journal of Machine Learning Research. 242502023</p>
<p>On the hardness of finding symmetries in Markov decision processes. S M Narayanamurthy, B Ravindran, International Conference on Machine Learning. 2008</p>
<p>Introductory Lectures on Convex Optimization: A Basic Course. Y Nesterov, 2014Springer Publishing Company</p>
<p>Sample complexity of robust reinforcement learning with a generative model. K Panaganti, D Kalathil, International Conference on Artificial Intelligence and Statistics. 2022</p>
<p>Markov decision processes: discrete stochastic dynamic programming. F Pukelsheim, M L Puterman, 2006. 2014John Wiley &amp; SonsOptimal design of experiments</p>
<p>Smdp homomorphisms: An algebraic approach to abstraction in semi Markov decision processes. B Ravindran, International Joint Conference on Artificial Intelligence. 2003</p>
<p>Symmetries and model minimization in Markov decision processes. B Ravindran, A G Barto, 2001Technical report</p>
<p>Model minimization in hierarchical reinforcement learning. B Ravindran, A G Barto, Abstraction, Reformulation, and Approximation: 5th International Symposium. 2002</p>
<p>Approximate homomorphisms: A framework for non-exact minimization in Markov decision processes. B Ravindran, A G Barto, 2004Technical report</p>
<p>Continuous mdp homomorphisms and homomorphic policy gradient. S Rezaei-Shoshtari, R Zhao, P Panangaden, D Meger, D Precup, Advances in Neural Information Processing Systems. 2022</p>
<p>Advanced Modern Algebra. J J Rotman, 2010American Mathematical Society114</p>
<p>Learning retrosynthetic planning through simulated experience. J S Schreck, C W Coley, K J Bishop, ACS Central Science. 562019</p>
<p>Using homomorphisms to transfer options across continuous reinforcement learning domains. V Soni, S Singh, AAAI Conference on Artificial Intelligence. 20066</p>
<p>Reinforcement learning algorithms for mdps. C Szepesvári, 2009</p>
<p>Active exploration in Markov decision processes. J Tarbouriech, A Lazaric, International Conference on Artificial Intelligence and Statistics. 2019</p>
<p>Active model estimation in markov decision processes. J Tarbouriech, S Shekhar, M Pirotta, M Ghavamzadeh, A Lazaric, Conference on Uncertainty in Artificial Intelligence. 2020</p>
<p>Curiosity in exploring chemical spaces: intrinsic rewards for molecular reinforcement learning. L A Thiede, M Krenn, A Nigam, A Aspuru-Guzik, Machine Learning: Science and Technology. 332022</p>
<p>Plannable approximations to mdp homomorphisms: Equivariance under actions. E Van Der Pol, T Kipf, F A Oliehoek, M Welling, International Conference on Autonomous Agents and Multiagent Systems. 2020a</p>
<p>Mdp homomorphic networks: Group symmetries in reinforcement learning. E Van Der Pol, D Worrall, H Van Hoof, F Oliehoek, M Welling, Advances in Neural Information Processing Systems. 2020b</p>
<p>Multi-agent mdp homomorphic networks. E Van Der Pol, H Van Hoof, F A Oliehoek, M Welling, International Conference on Learning Representations. 2022</p>
<p>Scientific discovery in the age of artificial intelligence. H Wang, T Fu, Y Du, W Gao, K Huang, Z Liu, P Chandak, S Liu, P Van Katwyk, A Deac, Nature. 62079722023</p>
<p>Algorithmic complexity: three np-hard problems in computational statistics. W J Welch, Journal of Statistical Computation and Simulation. 1511982</p>
<p>Decision tree methods for finding reusable mdp homomorphisms. A P Wolfe, A G Barto, AAAI Conference on Artificial Intelligence. 2006</p>
<p>Reward is enough for convex mdps. T Zahavy, B O'donoghue, G Desjardins, S Singh, Advances in Neural Information Processing Systems. 2021</p>
<p>Variational policy gradient method for reinforcement learning with general utilities. J Zhang, A Koppel, A S Bedi, C Szepesvari, M Wang, Advances in Neural Information Processing Systems. 2020</p>
<p>Continuous Homomorphisms and Leveraging Symmetries in Policy Gradient Algorithms for Markov Decision Processes. R Y Zhao, 2022PhD thesis</p>
<p>Invariant action effect model for reinforcement learning. Z.-M Zhu, S Jiang, Y.-R Liu, Y Yu, K Zhang, AAAI Conference on Artificial Intelligence. 2022</p>            </div>
        </div>

    </div>
</body>
</html>
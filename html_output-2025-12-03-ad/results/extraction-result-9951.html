<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9951 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9951</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9951</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-32d8df3dddfeb5d04326a17c0d506b1304aa8dc1</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/32d8df3dddfeb5d04326a17c0d506b1304aa8dc1" target="_blank">VERISCORE: Evaluating the factuality of verifiable claims in long-form text generation</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work uses VERISCORE to evaluate generations from 16 different models across multiple long-form tasks and finds that while GPT-4o is the best-performing model overall, open-weight models such as Mixtral-8x22 are closing the gap.</p>
                <p><strong>Paper Abstract:</strong> Existing metrics for evaluating the factuality of long-form text, such as FACTSCORE (Min et al., 2023) and SAFE (Wei et al., 2024), decompose an input text into"atomic claims"and verify each against a knowledge base like Wikipedia. These metrics are not suitable for most generation tasks because they assume that every claim is verifiable (i.e., can plausibly be proven true or false). We address this issue with VERISCORE, a metric for diverse long-form generation tasks that contain both verifiable and unverifiable content. VERISCORE can be effectively implemented with either closed or fine-tuned open-weight language models, and human evaluation confirms that VERISCORE's extracted claims are more sensible than those from competing methods across eight different long-form tasks. We use VERISCORE to evaluate generations from 16 different models across multiple long-form tasks and find that while GPT-4o is the best-performing model overall, open-weight models such as Mixtral-8x22 are closing the gap. We show that an LM's VERISCORE on one task (e.g., biography generation) does not necessarily correlate to its VERISCORE on a different task (e.g., long-form QA), highlighting the need for expanding factuality evaluation across tasks with varying fact density.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9951.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9951.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM claim verification vs humans</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based claim verification (using search results) compared to human annotators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of automatic claim verification performed by LLMs (GPT-4, GPT-4o, Claude 3, Mixtral variants and fine-tuned open models) against human annotators verifying claims given Google Search results; highlights where LLM verifiers diverge from humans and what capabilities they lack.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Long-form factuality evaluation / claim verification (using web search evidence)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Evaluated: GPT-4, GPT-4o, Claude 3 variants, Mixtral/Mistral; GPT-4o chosen as best-aligned automatic verifier and used to generate fine-tuning data</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Each extracted claim c was used as a Google Search query (Serper API) to retrieve top-n results (title, snippet, link); an LLM was prompted (few-shot / binary classification prompt) to judge whether the claim is supported, contradicted, or inconclusive given the list of search results; for experiments contradicted+inconclusive were combined into unsupported, yielding a binary supported/unsupported verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Three annotators verified 320 GPT-4-extracted claims with retrieved search results; items split into triple-annotated subsets (50 items) and other subsets; annotators labeled each search-result-level support/contradict/inconclusive and provided claim-level labels. Instructions asked to judge support given evidence snippets.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Human inter-annotator agreement: 82% of 50 triple-annotated items had complete agreement; Fleiss' kappa = 0.7316 (substantial). Automatic verifiers were compared to human labels by precision/recall/F1 on the annotated set; GPT-4o aligned best (reported highest F1 among tested LMs; exact numbers in paper Table 11).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>When replacing humans with LLM verifiers that rely on search snippets, the system loses deep, contextual, and multi-document reasoning ability: (1) many complex or compound claims become inconclusive because snippets do not state the connection between claim parts; (2) verification devolves to semantic/string matching rather than inferential confirmation of relations; (3) inability to judge claims that require domain expertise or aggregation of dispersed evidence; (4) high rate of 'unsupported' labels for claims humans might verify with deeper search or domain reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Concrete examples the paper gives where LLM-as-judge (over search snippets) marked claims unsupported or inconclusive though a human might consider them verifiable with deeper evidence or reasoning: (a) 'Japanese people encountered tigers in the form of stuffed animals before the Meiji era' — unsupported because search snippets did not mention the Meiji era together with stuffed tigers; (b) 'Marshall's leadership and strategic acumen ensured the maneuver was carried out flawlessly during a field maneuver in the Philippines' — requires extensive supporting evidence and inference across documents; (c) 'Germany is maintaining its competitive edge in a rapidly changing global landscape' — a high-level, inferential claim lacking direct snippet evidence. The paper also notes 40% of human-annotated claims were inconclusive overall.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Some claim types are well handled by LLM+search verification: short, encyclopedic, entity-centric claims that admit semantic or string matches in top search results are frequently supported (example: 'Indigenous women in Australia were not fully enfranchised until much later.'). The first five search results are substantially more informative (first result ~35.6% useful). GPT-4o showed the best alignment with human labels among evaluated LLMs, and a fine-tuned open verifier (Llama3 fine-tuned on GPT-4o labels) achieved respectable F1 (reported F1=0.841 on human annotated set), indicating automatic verifiers can approach human agreement on many items.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Sections 2.2, 2.3, 3.3, 3.4, and 4.4.2 (human verification study, automatic verifier evaluation, and qualitative analysis of verification failures)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VERISCORE: Evaluating the factuality of verifiable claims in long-form text generation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9951.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9951.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM claim extraction vs humans</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based verifiable claim extraction (GPT-4 / Claude 3 / fine-tuned Mistral) compared to human judgments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of claim lists produced by LLM extractors against human preferences and human judgments about which extraction contains least unverifiable content; analyzes what is lost when using LLM extractors (especially smaller or fine-tuned open models) instead of human extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Long-form factuality evaluation / claim extraction (decomposition step)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4 and Claude 3 used as prompted extractors; fine-tuned open-weight Mistral (Mistral-7B-Instruct-v0.2) also evaluated as a deterministic extractor.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Sliding-window few-shot extraction prompts: (context1: 0–3 sentences) <SOS> focused sentence <EOS> (context2: 0–1 sentence); prompts direct LLM to extract only verifiable claims, resolve pronouns using context, and ignore unverifiable content. For QA data the question is prepended to the window. Fine-tuned open models were trained on GPT-4/GPT-4o generated extraction data with LoRA.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Pairwise comparison of claim lists extracted by VERISCORE's prompts vs SAFE's pipeline: three annotators, 120 sampled texts (15 per domain across eight datasets), annotators chose which claim list had most verifiable and least unverifiable content; annotators provided brief justification. Also manual quality comparison between GPT-4 and fine-tuned Mistral on 300 pairs with two expert annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Annotator agreement for the pairwise extraction comparison: across 120 items three annotators fully agreed on 99 items; Fleiss' kappa = 0.7662 (substantial). Preferences: VERISCORE extraction preferred overwhelmingly (SAFE preferred only 26/360 annotated choices). For GPT-4 vs Mistral manual comparison: Cohen's kappa = 0.4320 (moderate agreement); exact-match between Mistral and GPT-4 = 43.7%, RougeL = 0.801 on held-out test.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Using LLM extractors (especially smaller or fine-tuned open models) can degrade specificity and granularity: (1) fine-tuned models sometimes omit small but important words, making claims less specific; (2) they sometimes combine multiple pieces of information into a single claim where a human or a stronger LLM would split them (reducing granularity/traceability of errors); (3) occasional extraction beyond the marked sentence (scope errors); (4) for extremely infactual or creative texts a small fraction of unverifiable claims may still be extracted by automated extractors, whereas humans can better filter such content.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Appendix F documents concrete failures of the fine-tuned Mistral extractor: missing small words that reduce claim specificity, grouping multiple facts into one claim (reducing atomicity), and sometimes extracting claims outside the marked span. The paper also gives qualitative examples where SAFE (another automatic extractor) over-extracted subjective/unverifiable statements that humans found irrelevant — illustrating the kinds of errors LLM extractors must avoid.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>VERISCORE's prompted extraction when run with GPT-4 was strongly preferred by human annotators across domains (preferred 93% of the time over SAFE in the abstract and described preferences in Section 3.1); fine-tuned Mistral achieved competitive performance compared to GPT-4 (good RougeL and partial exact matches), making automated extraction cost-efficient. Thus replacing humans with high-quality LLM extractors (e.g., GPT-4 or well-fine-tuned open models) incurs limited losses in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Sections 2.1, 2.1.1, 2.1.2, 3.1, 3.2, Appendix F (claim extraction prompts, human pairwise study, fine-tuning details)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VERISCORE: Evaluating the factuality of verifiable claims in long-form text generation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9951.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9951.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Search-driven LLM judge limitations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Limitations of LLM-as-judge when tied to Google Search snippets (retrieval-limited verification)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analysis of how using LLMs as judges that rely on top-n search snippets degrades evaluation compared to human verification that can perform deeper search, domain reasoning, or aggregate dispersed evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Long-form factuality evaluation / verification relying on web retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>LLM verifiers (GPT-4, GPT-4o, Claude 3, Mixtral, and fine-tuned Llama3/Mistral) operating over Serper/Google Search top-n results</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>For each claim the system used the claim itself as the search query, retrieved up to top-10 search results (title, snippet, link), consolidated those into an evidence list, then prompted an LLM to decide support/contradiction/inconclusive; later combined contradicted+inconclusive to unsupported to form a binary verification.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Annotators judged claim+search-result pairs and identified reasons for inconclusive labels (e.g., claim too general, no direct mention of claim parts, or conflicting snippets). A manual follow-up inspected 15 inconclusive cases by reading full web pages to test whether snippet-level inconclusiveness could be resolved.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Human triple-annotation: Fleiss' kappa = 0.7316; in manual follow-up, reading full pages resolved very few inconclusive cases (only 1 of 15 became verifiable by reading full page; many remained inconclusive). Overall, 55% of annotated claims were supported, ~40% inconclusive (type a), 2.8% contradicted.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Key differences/losses when using LLM judges tied to snippet-level retrieval: (1) many claims become labeled unsupported because snippets omit necessary linking information (loss of inferential aggregation); (2) inability to decide whether an unsupported claim is a hallucination vs. simply unfindable via shallow retrieval; (3) dependency on retrieval quality: top results are more informative but still insufficient for complex claims; (4) systematic under-detection of claims requiring expertise or multi-document synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>The paper gives examples (Section 4.4.2): supported claims are encyclopedic and easily matched by snippets (example 7). Unsupported/inconclusive examples include (8) the stuffed-tiger/Meiji-era claim lacking co-occurrence in snippets, (9) Marshall's leadership during a field maneuver requiring extensive documentary evidence, and (10) Germany's competitive edge claim which needs synthesis/inference.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Top search results (especially the first) are often informative (first result ~35.6% informative); many short factual claims that match encyclopedic content are reliably supported by the LLM+search pipeline. The paper also notes that moving beyond snippet matching (better retrieval or deeper reasoning) could reduce these losses, and that a fine-tuned verifier (Llama3 fine-tuned on GPT-4o labels) achieved strong F1 on human annotations (F1=0.841), suggesting practical viability despite limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Sections 2.2, 2.3, 3.3, 4.4.2, Appendix E (reasons for inconclusive, manual follow-up)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VERISCORE: Evaluating the factuality of verifiable claims in long-form text generation', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Long-form factuality in large language models <em>(Rating: 2)</em></li>
                <li>FActScore: Fine-grained atomic evaluation of factual precision in long form text generation <em>(Rating: 2)</em></li>
                <li>Language models hallucinate, but may excel at fact verification <em>(Rating: 1)</em></li>
                <li>Judging Ilm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9951",
    "paper_id": "paper-32d8df3dddfeb5d04326a17c0d506b1304aa8dc1",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "LLM claim verification vs humans",
            "name_full": "LLM-based claim verification (using search results) compared to human annotators",
            "brief_description": "Comparison of automatic claim verification performed by LLMs (GPT-4, GPT-4o, Claude 3, Mixtral variants and fine-tuned open models) against human annotators verifying claims given Google Search results; highlights where LLM verifiers diverge from humans and what capabilities they lack.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Long-form factuality evaluation / claim verification (using web search evidence)",
            "llm_judge_model": "Evaluated: GPT-4, GPT-4o, Claude 3 variants, Mixtral/Mistral; GPT-4o chosen as best-aligned automatic verifier and used to generate fine-tuning data",
            "llm_judge_setup": "Each extracted claim c was used as a Google Search query (Serper API) to retrieve top-n results (title, snippet, link); an LLM was prompted (few-shot / binary classification prompt) to judge whether the claim is supported, contradicted, or inconclusive given the list of search results; for experiments contradicted+inconclusive were combined into unsupported, yielding a binary supported/unsupported verifier.",
            "human_evaluation_setup": "Three annotators verified 320 GPT-4-extracted claims with retrieved search results; items split into triple-annotated subsets (50 items) and other subsets; annotators labeled each search-result-level support/contradict/inconclusive and provided claim-level labels. Instructions asked to judge support given evidence snippets.",
            "agreement_metric": "Human inter-annotator agreement: 82% of 50 triple-annotated items had complete agreement; Fleiss' kappa = 0.7316 (substantial). Automatic verifiers were compared to human labels by precision/recall/F1 on the annotated set; GPT-4o aligned best (reported highest F1 among tested LMs; exact numbers in paper Table 11).",
            "losses_identified": "When replacing humans with LLM verifiers that rely on search snippets, the system loses deep, contextual, and multi-document reasoning ability: (1) many complex or compound claims become inconclusive because snippets do not state the connection between claim parts; (2) verification devolves to semantic/string matching rather than inferential confirmation of relations; (3) inability to judge claims that require domain expertise or aggregation of dispersed evidence; (4) high rate of 'unsupported' labels for claims humans might verify with deeper search or domain reasoning.",
            "examples_of_loss": "Concrete examples the paper gives where LLM-as-judge (over search snippets) marked claims unsupported or inconclusive though a human might consider them verifiable with deeper evidence or reasoning: (a) 'Japanese people encountered tigers in the form of stuffed animals before the Meiji era' — unsupported because search snippets did not mention the Meiji era together with stuffed tigers; (b) 'Marshall's leadership and strategic acumen ensured the maneuver was carried out flawlessly during a field maneuver in the Philippines' — requires extensive supporting evidence and inference across documents; (c) 'Germany is maintaining its competitive edge in a rapidly changing global landscape' — a high-level, inferential claim lacking direct snippet evidence. The paper also notes 40% of human-annotated claims were inconclusive overall.",
            "counterexamples_or_caveats": "Some claim types are well handled by LLM+search verification: short, encyclopedic, entity-centric claims that admit semantic or string matches in top search results are frequently supported (example: 'Indigenous women in Australia were not fully enfranchised until much later.'). The first five search results are substantially more informative (first result ~35.6% useful). GPT-4o showed the best alignment with human labels among evaluated LLMs, and a fine-tuned open verifier (Llama3 fine-tuned on GPT-4o labels) achieved respectable F1 (reported F1=0.841 on human annotated set), indicating automatic verifiers can approach human agreement on many items.",
            "paper_reference": "Sections 2.2, 2.3, 3.3, 3.4, and 4.4.2 (human verification study, automatic verifier evaluation, and qualitative analysis of verification failures)",
            "uuid": "e9951.0",
            "source_info": {
                "paper_title": "VERISCORE: Evaluating the factuality of verifiable claims in long-form text generation",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLM claim extraction vs humans",
            "name_full": "LLM-based verifiable claim extraction (GPT-4 / Claude 3 / fine-tuned Mistral) compared to human judgments",
            "brief_description": "Comparison of claim lists produced by LLM extractors against human preferences and human judgments about which extraction contains least unverifiable content; analyzes what is lost when using LLM extractors (especially smaller or fine-tuned open models) instead of human extraction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Long-form factuality evaluation / claim extraction (decomposition step)",
            "llm_judge_model": "GPT-4 and Claude 3 used as prompted extractors; fine-tuned open-weight Mistral (Mistral-7B-Instruct-v0.2) also evaluated as a deterministic extractor.",
            "llm_judge_setup": "Sliding-window few-shot extraction prompts: (context1: 0–3 sentences) &lt;SOS&gt; focused sentence &lt;EOS&gt; (context2: 0–1 sentence); prompts direct LLM to extract only verifiable claims, resolve pronouns using context, and ignore unverifiable content. For QA data the question is prepended to the window. Fine-tuned open models were trained on GPT-4/GPT-4o generated extraction data with LoRA.",
            "human_evaluation_setup": "Pairwise comparison of claim lists extracted by VERISCORE's prompts vs SAFE's pipeline: three annotators, 120 sampled texts (15 per domain across eight datasets), annotators chose which claim list had most verifiable and least unverifiable content; annotators provided brief justification. Also manual quality comparison between GPT-4 and fine-tuned Mistral on 300 pairs with two expert annotators.",
            "agreement_metric": "Annotator agreement for the pairwise extraction comparison: across 120 items three annotators fully agreed on 99 items; Fleiss' kappa = 0.7662 (substantial). Preferences: VERISCORE extraction preferred overwhelmingly (SAFE preferred only 26/360 annotated choices). For GPT-4 vs Mistral manual comparison: Cohen's kappa = 0.4320 (moderate agreement); exact-match between Mistral and GPT-4 = 43.7%, RougeL = 0.801 on held-out test.",
            "losses_identified": "Using LLM extractors (especially smaller or fine-tuned open models) can degrade specificity and granularity: (1) fine-tuned models sometimes omit small but important words, making claims less specific; (2) they sometimes combine multiple pieces of information into a single claim where a human or a stronger LLM would split them (reducing granularity/traceability of errors); (3) occasional extraction beyond the marked sentence (scope errors); (4) for extremely infactual or creative texts a small fraction of unverifiable claims may still be extracted by automated extractors, whereas humans can better filter such content.",
            "examples_of_loss": "Appendix F documents concrete failures of the fine-tuned Mistral extractor: missing small words that reduce claim specificity, grouping multiple facts into one claim (reducing atomicity), and sometimes extracting claims outside the marked span. The paper also gives qualitative examples where SAFE (another automatic extractor) over-extracted subjective/unverifiable statements that humans found irrelevant — illustrating the kinds of errors LLM extractors must avoid.",
            "counterexamples_or_caveats": "VERISCORE's prompted extraction when run with GPT-4 was strongly preferred by human annotators across domains (preferred 93% of the time over SAFE in the abstract and described preferences in Section 3.1); fine-tuned Mistral achieved competitive performance compared to GPT-4 (good RougeL and partial exact matches), making automated extraction cost-efficient. Thus replacing humans with high-quality LLM extractors (e.g., GPT-4 or well-fine-tuned open models) incurs limited losses in many settings.",
            "paper_reference": "Sections 2.1, 2.1.1, 2.1.2, 3.1, 3.2, Appendix F (claim extraction prompts, human pairwise study, fine-tuning details)",
            "uuid": "e9951.1",
            "source_info": {
                "paper_title": "VERISCORE: Evaluating the factuality of verifiable claims in long-form text generation",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Search-driven LLM judge limitations",
            "name_full": "Limitations of LLM-as-judge when tied to Google Search snippets (retrieval-limited verification)",
            "brief_description": "Analysis of how using LLMs as judges that rely on top-n search snippets degrades evaluation compared to human verification that can perform deeper search, domain reasoning, or aggregate dispersed evidence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Long-form factuality evaluation / verification relying on web retrieval",
            "llm_judge_model": "LLM verifiers (GPT-4, GPT-4o, Claude 3, Mixtral, and fine-tuned Llama3/Mistral) operating over Serper/Google Search top-n results",
            "llm_judge_setup": "For each claim the system used the claim itself as the search query, retrieved up to top-10 search results (title, snippet, link), consolidated those into an evidence list, then prompted an LLM to decide support/contradiction/inconclusive; later combined contradicted+inconclusive to unsupported to form a binary verification.",
            "human_evaluation_setup": "Annotators judged claim+search-result pairs and identified reasons for inconclusive labels (e.g., claim too general, no direct mention of claim parts, or conflicting snippets). A manual follow-up inspected 15 inconclusive cases by reading full web pages to test whether snippet-level inconclusiveness could be resolved.",
            "agreement_metric": "Human triple-annotation: Fleiss' kappa = 0.7316; in manual follow-up, reading full pages resolved very few inconclusive cases (only 1 of 15 became verifiable by reading full page; many remained inconclusive). Overall, 55% of annotated claims were supported, ~40% inconclusive (type a), 2.8% contradicted.",
            "losses_identified": "Key differences/losses when using LLM judges tied to snippet-level retrieval: (1) many claims become labeled unsupported because snippets omit necessary linking information (loss of inferential aggregation); (2) inability to decide whether an unsupported claim is a hallucination vs. simply unfindable via shallow retrieval; (3) dependency on retrieval quality: top results are more informative but still insufficient for complex claims; (4) systematic under-detection of claims requiring expertise or multi-document synthesis.",
            "examples_of_loss": "The paper gives examples (Section 4.4.2): supported claims are encyclopedic and easily matched by snippets (example 7). Unsupported/inconclusive examples include (8) the stuffed-tiger/Meiji-era claim lacking co-occurrence in snippets, (9) Marshall's leadership during a field maneuver requiring extensive documentary evidence, and (10) Germany's competitive edge claim which needs synthesis/inference.",
            "counterexamples_or_caveats": "Top search results (especially the first) are often informative (first result ~35.6% informative); many short factual claims that match encyclopedic content are reliably supported by the LLM+search pipeline. The paper also notes that moving beyond snippet matching (better retrieval or deeper reasoning) could reduce these losses, and that a fine-tuned verifier (Llama3 fine-tuned on GPT-4o labels) achieved strong F1 on human annotations (F1=0.841), suggesting practical viability despite limitations.",
            "paper_reference": "Sections 2.2, 2.3, 3.3, 4.4.2, Appendix E (reasons for inconclusive, manual follow-up)",
            "uuid": "e9951.2",
            "source_info": {
                "paper_title": "VERISCORE: Evaluating the factuality of verifiable claims in long-form text generation",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Long-form factuality in large language models",
            "rating": 2
        },
        {
            "paper_title": "FActScore: Fine-grained atomic evaluation of factual precision in long form text generation",
            "rating": 2
        },
        {
            "paper_title": "Language models hallucinate, but may excel at fact verification",
            "rating": 1
        },
        {
            "paper_title": "Judging Ilm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2
        }
    ],
    "cost": 0.01596525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>VERISCORE: Evaluating the factuality of verifiable claims in long-form text generation</h1>
<p>Yixiao Song ${ }^{\text {® }}$ Yekyung Kim ${ }^{\text {® }}$ Mohit Iyyer ${ }^{\text {® }}$<br>${ }^{1}$ Manning College of Information and Computer Sciences, UMass Amherst<br>${ }^{2}$ Department of Linguistics, UMass Amherst<br>{yixiaosong, yekyungkim, miyyer}@umass.edu</p>
<h4>Abstract</h4>
<p>Existing metrics for evaluating the factuality of long-form text, such as FActScore (Min et al., 2023) and SAFE (Wei et al., 2024), decompose an input text into "atomic claims" and verify each against a knowledge base like Wikipedia. These metrics are not suitable for most generation tasks because they assume that every claim is verifiable (i.e., can plausibly be proven true or false). We address this issue with VERISCORE, ${ }^{1}$ a metric for diverse long-form generation tasks that contain both verifiable and unverifiable content. VERISCORE can be effectively implemented with either closed or fine-tuned open-weight language models, and human evaluation confirms that VERISCORE's extracted claims are more sensible than those from competing methods across eight different long-form tasks. We use VERISCORE to evaluate generations from 16 different models across multiple long-form tasks and find that while GPT-4o is the best-performing model overall, open-weight models such as Mixtral$8 \times 22$ are closing the gap. We show that an LM's VERISCORE on one task (e.g., biography generation) does not necessarily correlate to its VERISCORE on a different task (e.g., longform QA), highlighting the need for expanding factuality evaluation across tasks with varying fact density.</p>
<h2>1 Introduction</h2>
<p>Modern approaches for evaluating the factuality of LLM-generated long-form text, such as FActSCORE (Min et al., 2023) and SAFE (Wei et al., 2024), proceed in three stages: (1) decomposition of the text into a list of "atomic" (i.e., short) claims; (2) retrieval of relevant evidence for each claim from Wikipedia or Google Search; and (3) verification of each claim against the retrieved evidence. These approaches implicitly assume that the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>input text can be entirely decomposed into atomic and verifiable claims.</p>
<p>Unfortunately, these assumptions do not always apply to complex generation tasks such as longform question answering (LFQA) for two reasons. First, outputs for the biography generation task studied in FActSCORE rarely go beyond introducing entities and events. However, in tasks like LFQA, we observe more complex assertions that cannot be made "atomic" without losing critical context, as in:
(1) The impeachment of Andrew Johnson set a precedent that impeachment should be reserved for clear cases of serious misconduct rather than political disagreements.</p>
<p>Second, many long-form outputs interleave factual claims with unverifiable content, such as Betacyanin is like a superhero cape in Figure 1. Since FActSCORE and SAFE assume all claims are verifiable, they extract everything from the text (including unverifiable content like examples or hypotheticals), which can unfairly penalize models during the final aggregation process. As such, these metrics are limited to fact-dense and formulaic text (e.g., biographies).</p>
<p>We address these issues by developing VERISCORE, an automatic metric that assesses models' factuality against Google Search results. VERISCORE's decomposition and verification steps are initially implemented using few-shot prompting, and extensive human studies confirm the quality of both steps. Subsequently, openweight LLMs are fine-tuned on data generated by GPT-4 ${ }^{2}$ and GPT-4o to create a cost-efficient and reliable implementation. Compared to FActSCORE and SAFE, VERISCORE introduces two key improvements. First, VERISCORE only extracts what we term verifiable claims,</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The pipeline of VERISCORE involving claim extraction and claim verification with Google Search. VERISCORE extracts <em>verifiable</em> claims. Each claim is used as a search query to retrieve evidence via Google Search, and an LLM then verifies the claim against the search results. We also show SAFE's extracted claims from the same text to highlight its propensity to extract unverifiable claims (Claim 2 and 6); see Section 2.1.1 for more discussion.</p>
<p>Unlike FActScore and SAFE which decompose <em>everything</em>. Second, VERISCORE is the first approach that considers inter-sentence context when extracting claims, removing the need for expensive claim revision steps present in SAFE (see Section 2.1.1). In a human study, our extraction method is preferred 93% of the time over SAFE's, even in biography generation.</p>
<p>To benchmark VERISCORE, we gather fact-seeking prompts from eight diverse domains that require long-form responses, ranging from the fact-dense biography generation task of FActScore to the multi-task, open-domain ShareGPT dataset for instruction-following. We evaluate sixteen closed and open-weight LMs with VERISCORE and find that GPT-4o generates the most factually-supported text when averaged across all datasets. Our analyses highlight that (1) multiple tasks (not just biography generation) are needed for comprehensive long-form factuality evaluation because an LLM's factuality varies depending on the task and domain; and (2) verifying complex, lengthy assertions (common in many long-form tasks such as LFQA) against Google Search results can fail due to challenges in retrieving relevant documents from such queries.</p>
<h2>2 VERISCORE: an automatic factuality metric</h2>
<p>This section details the VERISCORE pipeline, covering claim extraction, evidence retrieval, claim verification, and score calculation.</p>
<h3>2.1 Claim extraction</h3>
<p>Claim extraction facilitates factuality evaluation by decomposing sentences with potentially multiple independent facts (Min et al., 2023; Tang et al., 2024). We first examine the shortcomings of FActScore and SAFE before developing a new method that focuses on extracting <em>verifiable</em> claims.</p>
<h3>2.1.1 Issues with claim extraction in FActScore and SAFE</h3>
<p>FActScore (Min et al., 2023) extracts <em>atomic facts</em>—“short statements that each contains one piece of information”. However, their extraction method is optimized for biographies and is inapplicable to other domains. First, it does not resolve pronouns: for example, it extracts “His notable film credits include The Game.” from an LLM-generated biography of Lanny Flaherty. Second, it extracts <em>everything</em> instead of just verifiable claims, an issue that is inherited by SAFE as in Figure 1.</p>
<p>SAFE (Wei et al., 2024) targets domains beyond biography and adapts FActScore's extraction prompt for a three-step pipeline: (1) claim extraction, (2) claim revision to resolve vague references, and (3) a relevance check to decide if a claim is worth checking. While Wei et al. (2024) proclaim SAFE's superior performance, a closer inspection reveals four issues. First, besides adding a brief task description, SAFEuses FActScore's prompt without changes. Second, the revision and relevance check adds significant processing time and cost.<sup>3</sup> Third, the relevance check unexpectedly removes verifiable claims. Lastly, SAFE's generalizability is questionable given that it is only evaluated on FActScore's biography data. More details of these issues are in Appendix A.</p>
<p><sup>3</sup>Processing 100 claims without parallelization takes 35 minutes using GPT-4. SAFE's prompt templates of claim revision and relevance check alone, without filling in content, cost about $1.7 per 100 claims (estimated using https://platform.openai.com/tokenizer).</p>
<p>2.1.2 VERISCORE's extraction approach</p>
<p>FACTSCORE and SAFE extract atomic claims with the implicit assumption that all claims are verifiable; unfortunately, this leads to the extraction of unverifiable claims (e.g., Claim 2 and 6 in Figure 1). Achieving atomicity is also hard as exemplified by Example (1). Hence, we instead aim to extract only verifiable claims. Inspired by frameworks of events and states in linguistics (Maienborn, 2003, 2019), we use the following description as a guideline:
Verifiable claims describe a single event or state ${ }^{4}$ with all necessary modifiers (e.g., spatial, temporal, or relative clauses) that help denote entities or events in the real world.
Formally, our claim extraction process produces a set of verifiable claims $C=\left{c_{1}, c_{2}, \ldots, c_{n}\right}$ from a model response $r$ where $c$ consists of meaningful parts $p$ such that $c=\left{p_{1}, p_{2}, \ldots, p_{n}\right}$. Each $p$ does not have to be a full proposition. ${ }^{5}$
To address the issues in FACTSCORE and SAFE, we design few-shot claim extraction prompts given in Appendix C. A sliding window, formatted as (context1: 0-3 sentences) <SOS>focused sentence<EOS> (context2: 0-1 sentence) is used to guide LLMs to extract verifiable claims from the focused sentence, using the context to ensure the claims are self-contained (e.g., pronouns are resolved). ${ }^{6}$ Unverifiable content such as advice, fictional stories, or subjective opinions are ignored.
A human evaluation study detailed in Section 3.1 confirms the advantages of our extraction method. It effectively addresses the issue of unresolved referents and eliminates the need of claim revision and removal. Additionally, our method correctly avoids extracting claims from non-factual content.</p>
<h3>2.2 Evidence retrieval</h3>
<p>As in SAFE, we use Google Search via the Serper $\mathrm{API}^{7}$ to retrieve evidence. For a claim $c \in C$, we use $c$ as the search query and retrieve the top $n$ search results $E_{c}=\left{e_{1}, e_{2}, \ldots, e_{n}\right}(n \leq 10)$. We use the title, snippet, and the link of each search</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>result returned by Serper and combine the results into an evidence list as in Vu et al. (2023).</p>
<h3>2.3 Claim verification</h3>
<p>Claim verification judges whether a claim $c$ is supported or contradicted by a corresponding evidence list $E_{c}$, or alternatively whether the verification is inconclusive. For a claim to be supported, all parts of the claim need to be supported (i.e., no evidence $e \in E_{c}$ can contradict any $p \in c$ ). For a claim to be contradicted, at least one $p \in c$ is contradicted by some evidence $e \in E_{c}$. Inconclusive cases can be classified into two types: (1) at least one part of the claim is neither supported nor contradicted with respect to $E_{c}$; or (2) at least one part of the claim is both supported and contradicted by different evidences $e \in E_{c}$. A formal definition of the three scenarios is given in Table 5. In Section 3.3's human study, we notice that there are very few claims directly contradicted by the evidence list $E_{c}$. Hence, for all subsequent experiments we combine contradicted and inconclusive into a single unsupported category, which renders claim verification as a binary classification task.</p>
<h3>2.4 Score calculation</h3>
<p>An ideal generation should have both high factual precision (i.e., low hallucination) and high factual recall (i.e., not be too short or incomplete). We adopt the $F_{1} @ K$ metric from SAFE, which considers both factual precision and recall. $K$ is the minimum number of factual claims a model response must contain to achieve perfect recall. For each tested domain, we set $K$ as the median number of extracted facts among all model responses.
Let $\mathcal{M}$ be a language model to be evaluated and $\mathcal{X}$ be a set of prompts of a given domain. Let $r=\mathcal{M}<em C="C" _in="\in" c="c">{x}$ be a response of $\mathcal{M}$ to $x \in X$, and let the transitive predicate support $(a, b)$ take a value of either 1 or $0 . S(r)=\frac{1}{|C|} \sum</em>$ is the average of the responses' $F 1 @ K$ within each domain, defined as:} \operatorname{support}\left(c, E_{c}\right)$ is the number of supported claims of $r . P(r)=\frac{S(r)}{|C|}$ and $R(r)=\min \left(\frac{S(r)}{K}, 1\right)$ are precision and recall. VERISCORE of $\mathcal{M</p>
<p>$$
\begin{gathered}
F_{1} @ K(r)= \begin{cases}\frac{2 P(r) R_{K}(r)}{P(r)+R_{K}(r)} &amp; \text { if } S(r)&gt;0 \
0 &amp; \text { if } S(r)=0\end{cases} \
\text { VERISCORE }=\frac{1}{|X|} \sum_{x \in X} F_{1} @ K\left(\mathcal{M}_{x}\right)
\end{gathered}
$$</p>
<p>3 Validation of VERISCORE's claim extraction and verification</p>
<p>SAFE and FACTSCORE use closed LLMs for claim extraction and verification. Following them, as shown in Figure 2, we develop VERISCORE's extraction and verification in Section 3 by prompting closed LLMs, whose effectiveness is verified by human evaluations. To mitigate the high cost of closed LLMs, we develop a free alternative in Section 4.1 by fine-tuning open-weight LLMs on data from GPT-4 and GPT-4o.</p>
<h3>3.1 Human evaluation on claim extraction</h3>
<p>To verify our extraction method's efficacy, we conducted a pairwise comparison of claims extracted by VERISCORE and SAFE with three human raters. They were asked to choose the claim lists with least unverifiable content. Half of the claims were extracted by GPT-4 and the other half by Claude 3. Our method outperforms SAFE regardless of the model used. Because GPT-4 was preferred more often than Claude 3 with our prompts, we use GPT-4 as the claim extractor in Section 4.
Setup: We extracted claims from 15 randomly sampled long-form texts from the eight datasets in Table 1 (Usage $=$ HE), using both SAFE's method and ours. The datasets were selected to have a range of verifiable factual content. For time and cost efficiency, we only used SAFE's fact extraction and revision steps (see Appendix A and Footnote 3). To ensure the comparison was independent of the model used, we used GPT-4, paired with SAFE's and our methods, to extract claims from half of the data points and Claude 3 for the other half. ${ }^{8}$ For each text, the annotators were asked to choose the claim list that had the most verifiable and the least/no unverifiable content, indicate whether it was hard to choose, and briefly justify their choice. Data preparation and annotation details are in Appendix D. In total, we collected 360 data points.
Results: The three annotators fully agreed on 99 out of 120 annotated data points, resulting in a Fleiss $\kappa=0.7662$ (substantial agreement, Landis and Koch, 1977). Of the 360 annotated items, claims extracted by SAFE were preferred only 26 times, with 19 of those preferences being marginal. The annotator preferences across the data domains are detailed in Figure 5. Notably, our approach is significantly favored even on biography generation.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The development of the open-weight VERISCORE. Details in Section 3 and Section 4.</p>
<p>Annotator comments on SAFE's claims: The annotators identified three major issues with SAFE's extraction pipeline. First, it indiscriminately extracts everything, such as subjective content (2a) and personal experience (2b).
(2) a. I am $1000 \%$ better.
b. My grandpa assembled a TV.</p>
<p>Second, SAFE overly decomposes texts, causing meaning overlaps between claims as in (3), which can disproportionately affect the final score.
(3) Longwood House is a place.</p>
<p>Longwood House is a Napoleonic Museum. Longwood House is one of the best Napoleonic Museums.
Longwood House is one of the best Napoleonic Museums in the world.</p>
<p>Third, SAFE often extracts trivial (4a) or vague claims (4b) that do not need to or cannot be verified.
(4) a. 3.2 is a number.
b. All My Sons has key themes.</p>
<h3>3.2 VERISCORE's claim extractor only extracts verifiable claims</h3>
<p>To further support that our claim extraction method with GPT-4 extracts only verifiable claims, we applied it to LLM-generated responses to 200 prompts from each domain in Table 1 (Usage $=$ Dev) and calculated the average ratio of verifiable claims to sentences per response, shown in the VerRatio column of Table 1. We observe significant and intuitive differences in this ratio across domains: fact-seeking domains (e.g., FreshBooks) have a higher density of verifiable claims, while WritingPrompts' creative story outputs contain almost no verifiable claims with a ratio of 0.03 , despite containing the longest responses. ${ }^{9}$ This level of variation shows that our method effectively discriminates verifiable and unverifiable content.</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
<th>Usage</th>
<th>VerRatio Source</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Scruples</td>
<td>Community judgements on real-life anecdotes from r/AmItheAsshole from November 2018 to April 2019</td>
<td>HE</td>
<td>—</td>
<td><em>Lourie et al. (2021)</em></td>
</tr>
<tr>
<td>CommonCrawl</td>
<td>A corpus of raw web page data, metadata extracts, and text extracts</td>
<td>HE</td>
<td>—</td>
<td><em>CommonCrawl</em></td>
</tr>
<tr>
<td>wikitext-103</td>
<td>Wikipedia articles</td>
<td>HE</td>
<td>—</td>
<td><em>Mority et al. (2016)</em></td>
</tr>
<tr>
<td>WritingPrompts [WP]</td>
<td>Story premises and stories written by online users on r/WritingPrompts</td>
<td>HE/Dev</td>
<td>0.03</td>
<td><em>Fan et al. (2018)</em></td>
</tr>
<tr>
<td>ShareGPT [S.GPT]</td>
<td>User-shared conversations (prompts and responses) with ChatGPT on ShareGPT.com</td>
<td>HE/Dev</td>
<td>0.92</td>
<td><em>Chiang et al. (2023)</em></td>
</tr>
<tr>
<td>ELI5</td>
<td>Questions and layperson-friendly answers posted on r/explainlikeimfive</td>
<td>HE/Dev</td>
<td>1.71</td>
<td><em>Scraped by Xu et al. (2023)</em></td>
</tr>
<tr>
<td>AskHistorians [AskH]</td>
<td>Questions and answers on history topics posted on r/AskHistorians</td>
<td>HE/Dev</td>
<td>1.90</td>
<td>Same as above</td>
</tr>
<tr>
<td>Biography[Bio]</td>
<td>Biography text generated by PerplexityAI, InstructGPT, and ChatGPT</td>
<td>HE/Dev</td>
<td>2.08</td>
<td><em>Min et al. (2023)</em></td>
</tr>
<tr>
<td>LongFact [LF]</td>
<td>A prompt set of 38 topics generated by GPT-4; each topic has prompts about object &amp; concept; we randomly sampled 5 object and 5 concept prompts from 10 topics</td>
<td>Dev</td>
<td>2.24</td>
<td><em>Wei et al. (2024)</em></td>
</tr>
<tr>
<td>FreshQA</td>
<td>A dynamic QA benchmark whose answers can change w.r.t. updated world knowledge; we randomly sampled 200 questions with a true premise from the never- and fast-changing categories in the text set of the April 1st version</td>
<td>Dev</td>
<td>1.00</td>
<td><em>Vu et al. (2023)</em></td>
</tr>
<tr>
<td>FreshBooks [FBs]</td>
<td>We collected 20 non-fictional books that are published in 2023 and 2024. Ten paragraphs are taken from each book. LLMs are prompted to generate a continuation given a paragraph</td>
<td>Dev</td>
<td>2.31</td>
<td>Current paper; Details in Table 13</td>
</tr>
</tbody>
</table>
<p>Table 1: Datasets used in the human evaluation of claim extraction (Usage = HE) in Section 3.1 and in the VERISCORE development (Usage = Dev) in Section 3 and 4.1. Short name of each dataset is in square brackets. VerRatio column presents the ratio of verifiable claims to sentences per domain of GPT-4 generated responses.</p>
<h3>3.3 Human evaluation on claim verification</h3>
<p>We conducted a human study where three annotators verified claims given search results. The study has three purposes: (1) to understand the feasibility of the task, (2) to see the distribution of the labels in Table 5, and (3) to later judge automatic verifiers by their agreement with human annotations.</p>
<p>Setup: We sampled 320 GPT-4-extracted claims from model responses to the prompts from the datasets (Usage = Dev) in Table 1. Evidence was retrieved as described in Section 2.2. The <claim, evidence list> pairs were split into subsets of 50 for agreement analysis and three subsets of 90, with each annotator doing one. The annotators evaluated each claim on two levels: (1) evidence level: assess if each search result supports, contradicts, or is inconclusive for the claim; (2) claim level: whether the claim is supported, contradicted, or inconclusive given all the evidence.</p>
<p>Human agreement: The agreement result shows that the verification task is well-defined and feasible. Of the 50 triple-annotated items, 82% had complete agreement among the annotators, and 14% had two annotators in consensus. The Fleiss $\kappa$ is 0.7316 (substantial agreement). An analysis of annotator disagreements is provided in Appendix E.</p>
<p>Reasons for being inconclusive: Among the 41 fully agreed items, 15 are inconclusive. There are two reasons. First, a claim is too general to be verified (e.g., "A systematic review on sex differences in the reinforcing effects of nicotine was published in Nicotine &amp; Tobacco Research in 2019." without specifying which systematic review it was.) Second, there is no direct mentioning of a part of the claim or no evidence verifies the connection between the parts of a claim (see Table 8). Overall, no triple-annotated item is marked as inconclusive for the reason that there are both supporting and contradicting search results.</p>
<p>Only over half of the claims are supported. We analyzed the distribution of the claim level labels of all annotated items. For the triple-annotated data, we use the majority vote, if there is one, as the final label. Otherwise, the label is inconclusive. Results in Table 2 show that only 55% of the claims are supported. As discussed later in Section 4.3, the low supported rate showcases that open-domain claim verification is beyond identifying exact or related terms but requires extensive reasoning to verify the connection between parts of a claim.</p>
<table>
<thead>
<tr>
<th>Label</th>
<th>Count</th>
<th>%</th>
</tr>
</thead>
<tbody>
<tr>
<td>Claim supported</td>
<td>176</td>
<td>55%</td>
</tr>
<tr>
<td>Claim contradicted</td>
<td>9</td>
<td>2.8%</td>
</tr>
<tr>
<td>Inconclusive (a)</td>
<td>128</td>
<td>40%</td>
</tr>
<tr>
<td>Inconclusive (b)</td>
<td>7</td>
<td>2.2%</td>
</tr>
</tbody>
</table>
<p>Table 2: The distribution of the four labels (Table 5) that can happen in the claim verification step.</p>
<p>Top search results are more informative. We consider a search result informative if it is marked as supporting or contradicting a claim. We analyzed the frequency with which search results were deemed informative. The first five search results show higher utility, with over 30% being useful—the highest being the first search result at 35.6%. For search results six to nine, their usefulness percentages range from 27.0% to 29.2%. The utility of the last search result drops to only 13.3%.</p>
<h3>3.4 Automatic verifier</h3>
<p>To find the best performing LLM on the claim verification task, we tested Mixtral-8×22-Instruct-v0.1, Claude 3, GPT-4, and GPT-4o on the human annotated verification data using the binary classification prompt in Table 12. We calculated the precision, recall, and $F_{1}$ on all items as well as separately on the supported and unsupported items. Results in Table 11 show that GPT-4o aligns the best with the human performance. Hence, we use GPT-4o data for fine-tuning an open claim verifier.</p>
<h2>4 Using VERISCORE to benchmark LM factuality</h2>
<p>In this section, we use VERISCORE to benchmark 16 LMs on 6 long-form fact-seeking domains. We first introduce our fine-tuned models for claim extraction and verification, which are used for our large-scale study. Our results highlight the gap between closed and open-weight LMs, which GPT-4o achieving significantly higher VERISCORE than any open LM. We also note tasks whose VERISCORE does not correlate well, and conclude with qualitative analysis revealing limitations of VERISCORE's verification step.</p>
<h3>4.1 An open-weight VERISCORE pipeline</h3>
<p>To facilitate affordable factuality evaluation, we fine-tuned open LMs for a deterministic and costefficient VERISCORE pipeline. We use the fewshot prompting pipeline developed in Section 3 to generate 13403 training data. We experimented with Llama3-8B-Instruct and Mistral-7B-Instructv0.2 (henceforth Llama3 and Mistral) as the base models (see Appendix G for details of fine-tuning). The benchmark experiments are then performed with the best performing fine-tuned models. The fine-tuned VERISCORE saves considerable money, making the evaluation process more accessible.</p>
<p>For claim extraction, the fine-tuned Mistral on GPT-4 data achieves the most competitive performance. The model sees the whole prompt and model response and extract claims sentence by sentence. In a quality comparison of 300 pairs of</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Mistral and GPT-4 extracted claims in Appendix F, the exact match rate is 43.7% and RougeL is 0.801. For claim verification, a verifier should be equally adept at identifying valid claims as well as recognizing unsupported claims. The fine-tuned Llama3 on GPT-4o data performs the best on the human annotated data in Section 3.3, achieving $F1=0.841$ (see Table 11). Details of the fine-tuning process and quality analysis are in Appendix F and G.</p>
<h3>4.2 Data domains and studied LMs</h3>
<p>VERISCORE aims to operate on a wide range of domains. We prompt 16 LMs using prompts from the datasets in Table 1 and benchmark their factuality. The datasets include prompts that require various degree of factual content, from highly fact-dense (e.g., AskHistorians and ELI5) to moderately factual (e.g., ShareGPT). We also collect a dataset FreshBooks that consists of 10 paragraphs from each of 20 non-fictional books in Table 13 published between 2023 and 2024. Models are required to generate a continuation of the paragraphs.</p>
<p>The three largest model families are the GPT, Claude 3, and Mistral/Mixtral models. We also evaluate LMs of various sizes—Qwen1.5-1.8B-Chat, Gemma-2B-it, OLMo-7B-Instruct, Vicuna-7B-v1.5, and DBRX Instruct (132B). Details of the models are in Table 14. To generate model responses for evaluation, the default model hyperparameters were used. The maximum token length was set to 1024. We used 50 prompts per domain.</p>
<h3>4.3 VERISCORE results</h3>
<p>The factuality performance of the 16 LMs on VERISCORE is reported in Table 3. We tune $K$ in $F_{1}@K$ for each domain, which is the median number of verifiable claims extracted from each response in each domain from all models. From the results, we observe the following:</p>
<p>Closed models are more factual. Overall, the GPT models performs better than the Claude 3 models. DBRX-Instruct and the Mixtral models performs competitively to some versions of the GPT and Claude 3 models. The smaller models fall behind on VERISCORE, with Gemma-2b-it performing the worse across all domains. The overall</p>
<p><sup id="fnref3:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>LF</th>
<th>Bio</th>
<th>ELI5</th>
<th>AskH</th>
<th>FBs</th>
<th>S.GPT</th>
<th>Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td>$K$</td>
<td>(32)</td>
<td>(26)</td>
<td>(21)</td>
<td>(21)</td>
<td>(24)</td>
<td>(11)</td>
<td></td>
</tr>
<tr>
<td>Gemma-2B-it</td>
<td>60.7</td>
<td>4.6</td>
<td>28.8</td>
<td>17.8</td>
<td>25.1</td>
<td>27.6</td>
<td>27.4</td>
</tr>
<tr>
<td>Mist-7B-Inst-v0.1</td>
<td>57.6</td>
<td>20.3</td>
<td>42.2</td>
<td>36.5</td>
<td>39.8</td>
<td>41.2</td>
<td>39.6</td>
</tr>
<tr>
<td>Vicuna-7B-v1.5</td>
<td>63.4</td>
<td>23.0</td>
<td>51.3</td>
<td>39.7</td>
<td>39.0</td>
<td>43.6</td>
<td>43.3</td>
</tr>
<tr>
<td>Qwen1.5-1.8B-Chat</td>
<td>70.3</td>
<td>14.1</td>
<td>57.9</td>
<td>45.2</td>
<td>52.6</td>
<td>49.2</td>
<td>48.2</td>
</tr>
<tr>
<td>OLMo-7B-Inst</td>
<td>73.4</td>
<td>19.4</td>
<td>58.8</td>
<td>43.2</td>
<td>53.7</td>
<td>49.4</td>
<td>49.6</td>
</tr>
<tr>
<td>Mist-7B-Inst-v0.2</td>
<td>72.0</td>
<td>30.0</td>
<td>58.8</td>
<td>41.2</td>
<td>52.4</td>
<td>54.8</td>
<td>51.5</td>
</tr>
<tr>
<td>Mixt-8x7B-Inst-v0.1</td>
<td>77.3</td>
<td>42.5</td>
<td>61.9</td>
<td>50.7</td>
<td>57.4</td>
<td>51.5</td>
<td>56.9</td>
</tr>
<tr>
<td>DBRX-Inst</td>
<td>75.9</td>
<td>46.5</td>
<td>61.9</td>
<td>49.5</td>
<td>60.2</td>
<td>48.9</td>
<td>57.2</td>
</tr>
<tr>
<td>Mixt-8x22B-Inst-v0.1</td>
<td>78.0</td>
<td>47.6</td>
<td>64.9</td>
<td>51.1</td>
<td>58.0</td>
<td>51.4</td>
<td>58.5</td>
</tr>
<tr>
<td>GPT3.5-turbo-1106</td>
<td>64.7</td>
<td>38.1</td>
<td>42.8</td>
<td>40.8</td>
<td>32.5</td>
<td>42.1</td>
<td>43.5</td>
</tr>
<tr>
<td>Claude-3-Haiku</td>
<td>79.4</td>
<td>37.1</td>
<td>58.7</td>
<td>43.5</td>
<td>49.5</td>
<td>44.7</td>
<td>52.2</td>
</tr>
<tr>
<td>Claude-3-Sonnet</td>
<td>80.7</td>
<td>37.6</td>
<td>56.2</td>
<td>40.7</td>
<td>59.3</td>
<td>51.7</td>
<td>54.4</td>
</tr>
<tr>
<td>GPT3.5-turbo-0613</td>
<td>77.6</td>
<td>45.9</td>
<td>62.9</td>
<td>51.8</td>
<td>49.0</td>
<td>48.6</td>
<td>56.0</td>
</tr>
<tr>
<td>Claude-3-Opus</td>
<td>83.6</td>
<td>52.7</td>
<td>63.4</td>
<td>49.8</td>
<td>66.4</td>
<td>51.6</td>
<td>61.2</td>
</tr>
<tr>
<td>GPT4-0125-preview</td>
<td>85.9</td>
<td>56.4</td>
<td>70.7</td>
<td>56.6</td>
<td>69.7</td>
<td>53.5</td>
<td>65.5</td>
</tr>
<tr>
<td>GPT-4o</td>
<td>86.7</td>
<td>56.7</td>
<td>71.7</td>
<td>61.4</td>
<td>70.9</td>
<td>51.5</td>
<td>66.5</td>
</tr>
<tr>
<td>Kendall’s $\tau$ w/ Avg.</td>
<td>0.78</td>
<td>0.83</td>
<td>0.82</td>
<td>0.73</td>
<td>0.73</td>
<td>0.56</td>
<td>1.00</td>
</tr>
</tbody>
</table>
<p>Table 3: VeriScore on 50 responses per LM per dataset (FreshQA and WritingPrompts in Table 16). $K$ is in brackets. Dataset full names are in Table 1. Precision and recall are in Table 16. All correlations are statistically significant. GPT-4o is the best closed LLM and Mixt-8x22B-Inst-v0.1 the best open LLM.
trend underscores the correlation of model size and VeriScore in long-form fact-seeking outputs.
Multiple generation tasks are needed for a comprehensive factuality evaluation. The Kendall’s $\tau$ correlations between LMs’ performance in domains in Figure 3 indicate that LMs’ VeriScore on two fact-seeking domains (e.g., ELI5 and Biography) do not necessarily correlate well. This suggests that LMs exhibit varying strengths in different domains, highlighting the need for diverse tasks to comprehensively assess LMs’ factuality.
$F 1 @ K$ favors longer outputs. $F_{1} @ K$ (Wei et al., 2024) considers both factual precision and recall, which improves on measuring factual precision alone (Min et al., 2023). A model must generate at least $K$ supported claims per response to achieve perfect recall. However, for domains that do not require long generations, models that generate short to-the-point outputs will be penalized if other models generate lengthy outputs with auxiliary information (e.g., FreshQA in Appendix J). It is debatable whether longer responses should always be preferred. They provide more details but, as Min et al. (2023) shows, later facts in long responses tend to be less accurate.</p>
<h3>4.4 Qualitative analysis</h3>
<p>This subsection examines VeriScore’s performance, highlighting the limitations of decompositional factuality evaluation for generations that are</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Kendall’s $\tau$ correlations of LMs’ performance between domains. All correlations are statistically significant. Models’ VeriSCORE on dissimilar tasks do not necessarily have high correlation, highlighting the need of using different tasks to assess LMs’ factuality.
not entity-centric and not formulaic. Two issues are identified: (1) not all claims can be short, and long claims are harder to verify, and (2) search results may be insufficient as expertise or extensive logical reasoning is often needed for verification.</p>
<h3>4.4.1 Claim complexity increases outside of entity-centric tasks</h3>
<p>Shorter, self-contained claims are desired because they help locate factual errors and are easy to be verified as employed by FACTSCORE for biography. However, claims extracted from other fact-seeking generations are often long. ${ }^{16}$ While some long claims could be split at conjunctions like and or or, this does not significantly shorten claims with inherently long core content, as seen in (5).
(5) Travelers and crusaders during the medieval period depended on established infrastructure to secure clean and consistent sources of water.
Occasionally, shorter claims can be extracted from a longer one, as the bracketed content in (6). However, verifying the shorter claims does not mean the longer one is verified because of solidified.
(6) [Chuck Norris’s victory in the 1968 World FullContact Karate Championships] solidified [his reputation as one of the best martial artists in the world].</p>
<p>For these reasons, long and complex claims are likely to be marked as inconclusive.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>4.4.2 Google Search may be insufficient for complex claims</h3>
<p>To understand what types of claims are supported and unsupported by Google Search snippets, we examine 80 claims from GPT-4o generated ELI5 and FreshBooks responses, along with their search and verification results. Half of these claims were verified as supported and the other half were not. ${ }^{17}$</p>
<p>The supported claims resemble encyclopedic writing. The content is supported by search results via semantic or string match, as in (7).
(7) Indigenous women in Australia were not fully enfranchised until much later. ${ }^{18}$</p>
<p>The unsupported claims do not have direct contradicting evidence. They are unsupported because there is no direct mention of (parts of) the claims or the connection between the parts of the claims. Example (8) is judged as unsupported because there is no mention of the Meiji era and stuffed tigers occurring together in the search results. ${ }^{19}$ Snippets do not offer enough background for such reasoning. Expertise or more sophisticated search is needed to verify/falsify such claims.
(8) Japanese people encountered tigers in the form of stuffed animals before the Meiji era.</p>
<p>Some unsupported claims require extensive supporting evidence. This happens the most often in FreshBooks when a claim encapsulates aspects like someone's achievements or historical movements, as in (9-10). Such content might not be directly mentioned in search results but need to be inferred from a large body of documents.
(9) Marshall's leadership and strategic acumen ensured the maneuver was carried out flawlessly during a field maneuver in the Philippines.
(10) Germany is maintaining its competitive edge in a rapidly changing global landscape.</p>
<p>In sum, with the current system, it is hard to decide whether an unsupported claim is hallucinated because it is beyond what reasoning over search snippets can achieve. This indicates the need to move beyond semantic or string matching for verification as they fail to uncover possible hallucination.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>5 Related work</h2>
<p>Our work builds on prior research in claim verification and long-form factuality evaluation. Users rely on the accuracy of LLM-generated content, yet LLMs often produce unreliable information (Maynez et al., 2020; Xu et al., 2023; Huang et al., 2023; Rawte et al., 2023). Research has thus focused on enhancing factual precision (Lin et al., 2024) and identifying inaccuracies.</p>
<p>Factual error detection: Prior research targets error detection in individual sentences (Mihaylova et al., 2019; Wadden et al., 2020; Shaar et al., 2022). FEVER (Thorne et al., 2018) features synthesized incorrect sentences from Wikipedia. FEVEROUS (Aly et al., 2021) and AVERITEC (Schlichtkrull et al., 2023) build on FEVER but remained limited to sentence-level facts. At the paragraph level, Li et al. (2023) test LLMs' detection of synthesized factual errors but do not locate the errors.</p>
<p>Long-form factuality evaluation: Detecting factual errors in a long-form text at once is hard ( Li et al., 2023). Decomposing a piece of long-form text into shorter sentences or search queries for factuality evaluation is commonly implemented in previous works (Kamoi et al., 2023; Gao et al., 2023; Wang et al., 2023; Min et al., 2023; Chern et al., 2023; Wanner et al., 2024; Wei et al., 2024; Guan et al., 2024; Chen et al., 2024). The decomposition helps locate factual errors and offers a fine-grained estimate of models' factuality (Min et al., 2023). Factuality evaluation often requires world knowledge, which can be achieved by employing retrieval (Ram et al., 2023; Vu et al., 2023). It is commonly used in factual error detection (Min et al., 2023; Wei et al., 2024; Thorne et al., 2018) and helps evaluation by providing up-to-date knowledge. The overall evaluation pipeline helps generate post hoc citations and iteratively improve model generations' factuality, which improves models' trustworthiness model (Huang and Chang, 2024; Ye et al., 2024).</p>
<h2>6 Conclusion and future work</h2>
<p>We propose VeriScore, a factuality metric that focuses exclusively on verifiable claims. Human evaluations validate that VeriScore is more effective than existing metrics for diverse long-form generation tasks that contain both verifiable and unverifiable content. We open-source both a closedand open-weight implementation of VeriScore, with the latter's performance approaching that of</p>
<p>the former. Finally, we notice that complex claims (e.g., not entity-centric or formulaic) are challenging to verify against search results. We hope that future work will improve on this aspect to develop more robust factuality metrics.</p>
<h2>Limitations</h2>
<p>We acknowledge further limitations of the current work and the decompositional approach below.</p>
<p>First, formally defining verifiable claims poses a significant challenge. Although our definition advances beyond the concept of atomic facts (Min et al., 2023), it remains a working definition rather than a formal one. For instance, consider the sentence (6): it is problematic to determine whether it describes a single state of "solidifying" or encompasses one event, "Chuck Norris's victory in ... Championships," along with two states, "solidifying" and "as one of the best ... in the world,". We hope future studies can improve on this.</p>
<p>Second, the decomposition method is slow. With one RTX8000 GPU, it takes about 4 hours to extract claims from 400 GPT-4o responses without parallelization. The reason is that VeriScore uses a sliding window to scan through a model response. In our experiments, each response on average has 40 sentences ( 20 sentences on average if excluding WritingPrompts responses). For verification, it takes about two hours to verify 10k claims. Future work can aim for a claim extractor that works without a sliding window to speed up the claim extraction.</p>
<p>Third, for model responses that are extremely infactual (e.g., WritingPrompts in Appendix J), our claim extractor might still extract a small amount of unverifiable claims. However, we contend that the percentage of factual content in creative writing in response to fictional premises is less concerned than in the fact-seeking domains. Hence, we do not consider this as a major concern of VeriScore.</p>
<p>Fourth, in the current work, we did not search exhaustively for the best hyperparameters for finetuning the open-source claim extractor and verifier. It is possible that, after searching, a better performance can be achieved. However, it is resourceintensive and time-consuming.</p>
<h2>Ethics Statement</h2>
<p>Our project aimed to minimize the computational cost by using LoRA (Hu et al., 2022) for efficient model fine-tuning. For the annotation work, an IRB
review was exempted. By signing a data conset, each annotators agreed on the annotated data being used for scientific research and published. No personally identifiable information was collected. We paid annotators $\$ 18$ per hour. Additional bonus were paid for reasonable extra time spent.</p>
<h2>Acknowledgement</h2>
<p>We extend our special gratitude to Kalpesh Krishna, who extensively discussed the project details with us and offered invaluable insights. We extend gratitude to the Upwork annotators for their hard work, and to members from the UMass NLP lab for their feedback. This project was partially supported by awards IIS-2202506, IIS-2046248, and IIS-2312949 from the National Science Foundation (NSF).</p>
<h2>References</h2>
<p>Rami Aly, Zhijiang Guo, Michael Sejr Schlichtkrull, James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Oana Cocarascu, and Arpit Mittal. 2021. The fact extraction and VERification over unstructured and structured information (FEVEROUS) shared task. In Proceedings of the Fourth Workshop on Fact Extraction and VERification (FEVER), pages 1-13, Dominican Republic. Association for Computational Linguistics.</p>
<p>Anthropic. 2023. Model Card: Claude 3. Technical report, Anthropic. Accessed: 2024-03-25.</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen technical report. Preprint, arXiv:2309.16609.</p>
<p>Jifan Chen, Grace Kim, Aniruddh Sriram, Greg Durrett, and Eunsol Choi. 2024. Complex claim verification with evidence retrieved in the wild. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 3569-3587, Mexico City, Mexico. Association for Computational Linguistics.</p>
<p>I-Chun Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, and Pengfei Liu. 2023. Factool: Factuality detection in generative ai - a tool augmented</p>
<p>framework for multi-task and multi-domain scenarios. Preprint, arXiv:2307.13528.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6.</p>
<p>Databricks. 2024. Introducing DBRX: A New State-of-the-Art Open LLM.</p>
<p>Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889-898, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and Kelvin Guu. 2023. RARR: Researching and revising what language models say, using language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16477-16508, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex CastroRos, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikula, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. 2024. Gemma:</p>
<p>Open models based on gemini research and technology. Preprint, arXiv:2403.08295.</p>
<p>Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hannaneh Hajishirzi. 2024. Olmo: Accelerating the science of language models. Preprint, arXiv:2402.00838.</p>
<p>Jian Guan, Jesse Dodge, David Wadden, Minlie Huang, and Hao Peng. 2024. Language models hallucinate, but may excel at fact verification. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 1090-1111, Mexico City, Mexico. Association for Computational Linguistics.</p>
<p>Edward J Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations.</p>
<p>Jie Huang and Kevin Chang. 2024. Citation: A key to building responsible and accountable large language models. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 464-473, Mexico City, Mexico. Association for Computational Linguistics.</p>
<p>Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2023. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. Preprint, arXiv:2311.05232.</p>
<p>Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L'elio Renard Lavaud, Lucile Saulnier, MarieAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2024. Mixtral of experts. ArXiv, abs/2401.04088.</p>
<p>Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier,</p>
<p>L'elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. ArXiv, abs/2310.06825.</p>
<p>Ryo Kamoi, Tanya Goyal, Juan Diego Rodriguez, and Greg Durrett. 2023. WiCE: Real-world entailment for claims in Wikipedia. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7561-7583, Singapore. Association for Computational Linguistics.</p>
<p>J Richard Landis and Gary G Koch. 1977. The measurement of observer agreement for categorical data. Biometrics, pages 159-174.</p>
<p>Junyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. HaluEval: A large-scale hallucination evaluation benchmark for large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6449-6464, Singapore. Association for Computational Linguistics.</p>
<p>Sheng-Chieh Lin, Luyu Gao, Barlas Oguz, Wenhan Xiong, Jimmy Lin, Wen tau Yih, and Xilun Chen. 2024. Flame: Factuality-aware alignment for large language models. Preprint, arXiv:2405.01525.</p>
<p>Nicholas Lourie, Ronan Le Bras, and Yejin Choi. 2021. Scruples: A corpus of community ethical judgments on 32,000 real-life anecdotes. Preprint, arXiv:2008.09094.</p>
<p>Claudia Maienborn. 2003. Event-internal modifiers: Semantic underspecification and conceptual interpretation, pages 475-510. De Gruyter Mouton, Berlin, Boston.</p>
<p>Claudia Maienborn. 2019. Events and States. In The Oxford Handbook of Event Structure. Oxford University Press.</p>
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906-1919, Online. Association for Computational Linguistics.</p>
<p>Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models. Preprint, arXiv:1609.07843.</p>
<p>Tsvetomila Mihaylova, Georgi Karadzhov, Pepa Atanasova, Ramy Baly, Mitra Mohtarami, and Preslav Nakov. 2019. SemEval-2019 task 8: Fact checking in community question answering forums. In Proceedings of the 13th International Workshop on Semantic Evaluation, pages 860-869, Minneapolis, Minnesota, USA. Association for Computational Linguistics.</p>
<p>Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore:</p>
<p>Fine-grained atomic evaluation of factual precision in long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12076-12100, Singapore. Association for Computational Linguistics.</p>
<p>OpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774.</p>
<p>OpenAI. 2024. Model release blog: GPT-4o. Technical report, OpenAI. Accessed: 2024-05-23.</p>
<p>Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented language models. Transactions of the Association for Computational Linguistics, 11:1316-1331.</p>
<p>Vipula Rawte, Amit Sheth, and Amitava Das. 2023. A survey of hallucination in large foundation models. Preprint, arXiv:2309.05922.</p>
<p>Michael Sejr Schlichtkrull, Zhijiang Guo, and Andreas Vlachos. 2023. AVeritec: A dataset for real-world claim verification with evidence from the web. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track.</p>
<p>Shaden Shaar, Firoj Alam, Giovanni Da San Martino, and Preslav Nakov. 2022. The role of context in detecting previously fact-checked claims. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 1619-1631, Seattle, United States. Association for Computational Linguistics.</p>
<p>Liyan Tang, Philippe Laban, and Greg Durrett. 2024. Minicheck: Efficient fact-checking of llms on grounding documents. Preprint, arXiv:2404.10774.</p>
<p>James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809-819, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Maxim Tkachenko, Mikhail Malyuk, Andrey Holmanyuk, and Nikolai Liubimov. 20202022. Label Studio: Data labeling software. Open source software available from https://github.com/heartexlabs/label-studio.</p>
<p>Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, and Thang Luong. 2023. Freshllms: Refreshing large language models with search engine augmentation. Preprint, arXiv:2310.03214.</p>
<p>David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. 2020. Fact or fiction: Verifying scientific claims. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language</p>
<p>Processing (EMNLP), pages 7534-7550, Online. Association for Computational Linguistics.
Yuxia Wang, Revanth Gangi Reddy, Zain Muhammad Mujahid, Arnav Arora, Aleksandr Rubashevskii, Jiahui Geng, Osama Mohammed Afzal, Liangming Pan, Nadav Borenstein, Aditya Pillai, et al. 2023. Factcheck-GPT: End-to-end fine-grained documentlevel fact-checking and correction of LLM output. arXiv preprint arXiv:2311.09000.</p>
<p>Miriam Wanner, Seth Ebner, Zhengping Jiang, Mark Dredze, and Benjamin Van Durme. 2024. A closer look at claim decomposition. Preprint, arXiv:2403.11903.</p>
<p>Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, et al. 2024. Long-form factuality in large language models. arXiv preprint arXiv:2403.18802.</p>
<p>Fangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol Choi. 2023. A critical evaluation of evaluations for long-form question answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3225-3245, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Xi Ye, Ruoxi Sun, Sercan Arik, and Tomas Pfister. 2024. Effective large language model adaptation for improved grounding and citation generation. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 6237-6251, Mexico City, Mexico. Association for Computational Linguistics.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging Ilm-as-a-judge with mt-bench and chatbot arena. Preprint, arXiv:2306.05685.</p>
<h2>A Weaknesses in FACTSCORE and SAFE</h2>
<p>In Section 2.1.1, we pointed out four major issues in SAFE's claim extraction pipeline. Details of the issues are provided in this appendix section.</p>
<p>First, for claim extraction, aside from prepending a brief task description to FACTSCORE's prompt, SAFE does not make other modifications. Consequently, the prompt only focuses on biography.</p>
<p>Second, SAFE's extraction pipleine is multistep. Because FACTSCORE extracts claims by sentence without context, it cannot resolve references. This limitation is not an issue for FACTSCORE because each claim is verified against one predefined Wikipedia article. However, SAFE uses Google Search and thus must resolve all vague references.</p>
<p>SAFE addresses this by deploying claim revision which prompts a language model once for each claim to revise vague references. Following that, a language model reviews each claim again to decide whether they are worth checking. The entire pipeline adds significant processing time and cost.</p>
<p>Third, the relevance check step negatively impacts evaluation. Wei et al. (2024) justifies this step with an example in their Figure 1-when asked about the Eiffel Tower, a model generates The Nile River is in Egypt. First of all, such behaviour is not observed in our experiments. Second, we applied SAFE's extraction pipeline to five texts and examined which claims were removed. It turns out that $11 \%$ of 211 claims were removed, of which $58 \%$ were actually relevant. The remaining $42 \%$ were either tautologies or not claims and should not have been extracted. ${ }^{20}$ Table 4 provides an example of SAFE removing a relevant claim.</p>
<p>Fourth, there is no guarantee that SAFE works across domains. Despite being applied to 38 factseeking topics, SAFE's performance is only evaluated on FACTSCORE's biography data. Among the 38 topics, SAFE is solely applied to model outputs that responses to object-related prompts. Six topics mostly contain biography questions (i.e., Who is). ${ }^{21}$ Some topics (e.g., sports) contain only who, what, and can you tell me about questions, making them fact-dense and entity-centric. A human study in Section 3.1 confirms that SAFE falls short in less entity-centric domains.</p>
<h2>B Formal definition of claim verification</h2>
<p>In Section 2.3, we described the definition of the four possible scenarios that can happen when verifying a claim with respect to evidence. We give a formal definition of such scenarios in Table 5.</p>
<h2>C Claim extraction prompts</h2>
<p>We developed two claim extraction prompts: one for question-answering (QA) type of input data, and the other for non-QA data. For evaluating model outputs, the QA prompt is generally applicable with the prompt being the question. The non-QA prompt is used for cases where neither a question nor a prompt is available.</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>SAFE's relevance check identifies relevant claim as irrelevant.
Question: At their peak, what did the insides of the most beautifully decorated castles look like? Today, castles seem to just be giant fortresses but I would like to know how they looked when they were fully furnished. How were they decorated? What treasures were stored there? Are there a few castles that were especially beautiful?
Human response: It is quite a broad subject because castles varied quite a lot depending on location, time of construction and wealth of the constructor; u/valkine talked about Caenarfon Castle (link) specifically in another question (link) is a part of the inside of Castello Maniance in Siracusa, Italy. It was built from 1232 to 1239. during a large castle-construction effort by Emperor Frederick II. I do find it particularly beautiful but this doesn't really say much about what other castles looked like.
Extracted claim: Castello Maniance in Siracusa, Italy was built from 1232 to 1239.
Authors' note: Although the human answer does not answer all parts of the question, the content that is deemed as irrelevant by SAFE is actually pointing to a castle that is relevant to answering the question.</p>
<p>Table 4: An example illustrating SAFE's relevance assessment does not work as expected.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Scenario</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Claim supported</td>
<td style="text-align: left;">$\forall p \in c .\left[\exists e \in E_{c} . \operatorname{support}(e, p) \wedge \neg \exists e \in E_{c} . \operatorname{contradict}(e, p)\right]$</td>
</tr>
<tr>
<td style="text-align: left;">Claim contradicted</td>
<td style="text-align: left;">$\exists p \in c .\left[\neg \exists e \in E_{c} . \operatorname{support}(e, p) \wedge \exists e \in E_{c} . \operatorname{contradict}(e, p)\right]$</td>
</tr>
<tr>
<td style="text-align: left;">Inconclusive (a)</td>
<td style="text-align: left;">$\exists p \in c .\left[\neg \exists e \in E_{c} . \operatorname{support}(e, p) \wedge \neg \exists e \in E_{c} . \operatorname{contradict}(e, p)\right]$</td>
</tr>
<tr>
<td style="text-align: left;">Inconclusive (b)</td>
<td style="text-align: left;">$\exists p \in c .\left[\exists e \in E_{c} . \operatorname{support}(e, p) \wedge \exists e \in E_{c} . \operatorname{contradict}(e, p)\right]$</td>
</tr>
</tbody>
</table>
<p>Table 5: Four scenarios that can happen in the claim verification step. support $(a, b)$ and contradict $(a, b)$ are two transitive predicates such that $\neg$ support $(a, b) \neq$ contradict $(a, b)$ and $\neg$ contradict $(a, b) \neq$ support $(a, b)$.</p>
<p>What is common in the two prompts is a sliding window for claim extraction. Each window has the format (context1 $=0-3$ sentence) <SOS>Sentence to be focused on<EOS> (context2 $=0-1$ sentence). The goal is to extract claims from the sentences marked by SOS and EOS while using the information in context1 and context2 to make the claims self-contained.</p>
<p>What is different in the two prompts is that for non-QA-type of inputs, we always prepend the first sentence of a paragraph to context1 if the paragraph is longer than five sentences; for QAtype of inputs, we always prepend the question to context1. This is based on the observation that, when answering a question or when an answer gets long, people might take the information in the question or previous sentences for granted and do not refer to an entity using its full name. Adding the question or the first sentence of a paragraph into context1 can help a model better recover time, location, and person references in a claim.</p>
<p>The prompts are given in Table 6 and Table 7</p>
<h2>D Human evaluation of claim extractions by our prompt and SAFE</h2>
<p>To verify the effectiveness of our proposed claim extraction method, we conducted a human evaluation of pair-wise comparison between claims ex-
tracted by our prompts and SAFE's. We hired three experienced data annotators on Upwork ${ }^{22}$.</p>
<p>To prepare the data for evaluation, we sampled 15 data points from each dataset in Table 1. We used the first four datasets as non-QA datasets and the others as QA datasets. Each data point was truncated to 300 white-space-separated words at the sentence boundary. Each annotator was asked to annotate the same set of 120 sampled data.</p>
<p>The evaluation was conducted on the opensource data labeling platform Label Studio (Tkachenko et al., 2020-2022). The task interface is given in Figure 4. Before the task begins, each annotator needs to read through the instructions of the task. ${ }^{23}$ We estimated the annotation task to take approximately two hours to complete. Therefore, each annotator was compensated at a rate of $\$ 15$ per hour.</p>
<p>Figure 5 depicts the human preference in each domain of data in Table 1. Among the 360 annotated data points, the claims extracted by SAFE are only preferred 26 times by the three annotators in total, among which 19 were chosen hesitatingly, as indicated by the light red color in Figure 5.</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Task Interface</h1>
<h2>Given the source text, which claim list better covers the verifiable content in the source text?</h2>
<p>I'm currently in 9 th grade and recently me and my friend who I'll call j, had a geometry test. I've never been great at math, but me and j had known each other for a while, and we normally work together. I have geometry fourth period ...</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Claim list 1:</th>
<th style="text-align: left;">Claim list 2:</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">I am currently in 9th grade.</td>
<td style="text-align: left;">No verifiable claims.</td>
</tr>
<tr>
<td style="text-align: left;">Recently, I had a geometry test.</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">My friend had a geometry test.</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">I refer to my friend as J.</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">I've never been great at math.</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">The narrator and the narrator's friend, who the narrator chooses to <br> call 'j', had known each other for a while.</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">The narrator and the narrator's friend, who the narrator calls J, <br> normally work together.</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Which claim list better covers the verifiable content of the source text?</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">$\square$ Claim list 1 $\square$ Claim list 2</td>
<td style="text-align: left;">Make your <br> Choice</td>
</tr>
<tr>
<td style="text-align: left;">Was it difficult to decide between the two lists (i.e., they are similarly good or bad)?</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">$\square$ Yes $\square$ No</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Please motivate your choice in 1 to 2 sentences.</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Figure 4: The interface design of the human evaluation described in Section 3.1 and Appendix D. The interface consists of four parts. Source text: the text from which claims are extracted. Claim lists: Two claim lists extracted by our prompt and SAFE respectively. The order of the two lists are randomized. Decisions: annotators indicate here which claim list is better and whether it is hard to choose between the two. Justification: annotators should briefly explain why they choose one list over the other.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Results of the pair-wise performance comparison between our one-step extraction prompts and SAFE's two-step extraction pipeline. The claims extracted by our prompts are overwhelmingly preferred by the annotators across all eight domains. The dark area in each bar indicates that the annotators strongly preferred one choice over the other. The light area represents slight preference. The numbers are aggregated over three annotators.</p>
<h2>E Human study on claim verification</h2>
<p>This appendix section provides supplementary details to Section 3.3.</p>
<h2>E. 1 Detailed examples of human verification</h2>
<p>In this section, we provide detailed examples from our human study on the claim verification task in Section 3.3. Table 8 presents the examples of
the annotation items whose claim was labeled as inconclusive by all annotators.</p>
<h2>E. 2 Reason of disagreement in human verification</h2>
<p>There are 9 items in the human study in Section 3.3 on which the annotators did not reach a full agreement. After inspecting these items, we conclude</p>
<p>4 sources of disagreement, listed in Table 9 with examples. First, an annotator made a mistake (e.g., misread a name). Second, there is disagreement in the interpretation of the claim and evidence (e.g., can in the claim vs. could in the evidence or an ambiguous referent). Third, the claim is complex and long, hence, is hard to verify. Fourth, the evidence indirectly supports the claim which means intermediate reasoning process is needed. one annotator have overlooked the connection between the claim and the search result.</p>
<h3>E. 3 Verifying/falsifying inconclusive claims is hard</h3>
<p>In Section 3.3, we presented the distribution of verification labels in Table 2. As many as $42.2 \%$ of the annotated items are labeled as inconclusive by our annotators. In order to understand whether the inconclusive cases can be verified/falsified by checking the full web page of the returned search results, we randomly picked 15 inconclusive cases and manually verified them. Results show that two claims are not specified enough to be verified, for example, (11).
(11) A group of archaeologists unearthed a cache of Roman weaponry near the ancient ruins of the Colosseum on a sweltering summer afternoon.</p>
<p>Only one claim in (12) can be verified the full web page. The search result snippets do not mention that Angular is maintained by Google but this is confirmed by a notice at the end of the web page.
(12) Google Angular supports dependency injection.
angular.io/guide/architecture-services
For the remaning 12 cases, we used Google search to find more evidence but only the claim in (13a) was weakly contradicted by a popular science article, which states the content in (13b). If "As a battery discharges" is describing the status change of a battery from not discharging to discharging, the article implies that the chemical reactions become active but not slowing down. However, if the claim is understood as "as a battery continues to discharge", the article does not mention anything about chemical reactions slowing down gradually.
(13) a. As a battery discharges, the chemical reactions inside the battery slow down.
b. When a battery is discharged, chemical reactions within the battery cells facilitate the movement of electrons from the negative terminal (anode) to the positive terminal (cathode) [...]. (link)</p>
<h2>F Details of fine-tuning open-source models for claim extraction</h2>
<p>In this section, we specify the details of fine-tuning open-source models for the claim extraction task.
Data We used two types of data for GPT-4 to extract claims, which are used to fine-tuned opensource models. The first type of data is the existing open-source data in Table 1. We sampled 100 data points from Scruples and 200 from the other datasets. The reason of sampling less data points from Scruples is that the majority of them is invariably subjective and yields the "No verifiable claim." output. Hence, they are not very helpful in teaching an open-source model how to extract claims from factual texts.</p>
<p>The second data type is our newly generated model responses. For this, we sampled 63 prompts from Biography and 80 from the other datasets listed in Table 1 (Usage $=$ Dev). To generate the responses, we prompted the first 12 LLMs in Table 14 with their default hyperparameters and the maximum token requirement was set to 1000. The LLMs are chosen in a way that we have both closeand open-source models as well as models in the same family with different sizes or versions.</p>
<p>After collecting and generating all the model long-form responses, we decompose them into claims with GPT-4 using the prompts in Appendix C. The temperature was set to 0 .</p>
<p>We formed the fine-tuning data in the following way. As the input, we used the prompt (if there is one) and the response text with one sentence being marked with $&lt;$ SOS $&gt;$ and $&lt;$ EOS $&gt;$. The output is the claims extracted from the marked sentence.</p>
<p>To get the final set of fine-tuning data, we randomly removed $80 \%$ of the data points whose marked sentence is shorter than 10 characters. These short marked sentences are usually the numbering of numbered lists. We also randomly dropped $50 \%$ data whose output is "No verifiable claim." In total, we got 99592 input-output pairs, among which 902 pairs have a short marked sentence, and 31819 pairs have "No verifiable claim." as the output. We took $95 \%, 4 \%$, and $1 \%$ of the dataset as the training, validation, and test splits.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Results of comparing GPT-4 and fine-tuned Mistral-7B-Instruct-v0.2 on the claim extraction task. Numbers are in percentage. The Cohen's $\kappa$ between the two annotators is 0.4320 . Mistral achieves a competitive result compared to GPT-4.</p>
<p>Fine-tuning We chose Llama3-8B-Instruct and Mistral-7B-Instruct-v0.2 as the base models (henceforth Llama3 and Mistral). Both were fine-tuned via Unsloth ${ }^{24}$ for two epochs using LoRA (Hu et al., 2022). Checkpoints were saved at each epoch and tested on the test set. We used string-based metrics for evaluation and selected Mistral fine-tuned for two epochs as the best checkpoint. ${ }^{25}$ We further evaluated this checkpoint manually.
Manual quality comparison To understand how good the performance of the fine-tuned model is compared to GPT-4, the first two authors did a pairwise comparison between the outputs from the two models on 300 test data points. After removing the data whose GPT-4 and Mistral outputs match exactly, there were 169 data points left for manual evaluation. Each annotator annotated the same data and were asked to choose which output was better or whether it was hard to choose between the two.</p>
<p>Figure 6 shows the percentage of each annotator's choices. The two authors fully agreed in 106 data points, achieving a Cohen's $\kappa=0.4320$ (moderate agreement, Landis and Koch, 1977). Given that the quality of both models' outputs is close with GPT-4 being slightly better, such a moderate agreement is expected.
Quality analysis GPT-4 and the fine-tuned Mistral perform similarly. In many cases, their outputs are identical with certain phrases being relocated in the sentences. However, there are cases where Mistral lost to GPT-4 because it misses small words and</p>
<p><sup id="fnref9:0"><a class="footnote-ref" href="#fn:0">1</a></sup>makes the extracted claims less specific. Occasionally, Mistral puts multiple pieces of information into one claim while GPT-4 breaks the information down into multiple claims. Concrete examples can be seen in Table 10. Besides these issues, in certain cases, Mistral does not refrain itself to the marked sentence in an input but also extracts claims that come after the span.</p>
<p>Overall, our fine-tuned Mistral model performs comparable to GPT-4. As an open-source model, it is also cost-efficient.</p>
<h2>G Details of fine-tuning open-source models for claim verification</h2>
<p>In this section, we offer details of fine-tuning the open-source models for the claim verification task, in addition to Section 4.1.</p>
<p>In order to select the best model for generating fine-tune data, we tested Mixtral-8×22-Instruct-v0.1, Claude-3-Opus, GPT-4, and GPT40 with the few-shot prompt in Table 12 on the 320 human annotated verification data in Section 3.3. We tried two types of verification task, one with supported and unsupported as the labels and one with supported, contradicted, and inconclusive as the labels. For a fair comparison between the model performance on the binary and ternary task, we convert the contradicted and inconclusive labels in the ternary task to unsupported. We then calculated the F1 scores on all 320 items as well as on supported and unsupported items separately. The results are in Table 11. Overall, GPT-4o with ternary labels has the most balanced performance on the supported and unsupported items. Hence, we generated data from GPT-4o for fine-tuning open-source models and converted the ternary labels to binary ones.
Data We sampled 10 prompts from the datasets listed in Table 1 (Usage $=$ Dev) and prompted the LLMs in Table 14 with their default hyperparameters and the maximum token requirement was set to 1024. The model responses are decomposed into claims with GPT-4 using the prompts in Appendix C. The temperature was set to 0 . Serper is then used to retrieve search results as described in Section 2.2. As the prompt, we use the binary prompt template in Table 12 without the few-shot examples. From the generated data, we randomly sampled 13403 data points, among which 9996 has the supported label and 3407 has</p>
<p>the unsupported label. We split the dataset into 85%, 3%, and 12% as the training, validation, and test splits.
Fine-tuning Similar to the fine-tuned claim extractor, we fine-tuned Llama3-8B-Instruct and Mistral-7B-Instruct-v0.2 via Unsloth for 5 epochs. Because the number of supported and unsupported items in the training dataset are imbalanced, we triplicated the unsupported data points. Checkpoints were saved at each epoch and tested on both the test dataset and the 320 human annotated set. The Llama3-8B-Instruct model fine-tuned for one epoch achieves the most balanced performance on supported and unsupported data points on the test and human data. Hence, we use this checkpoint for further experiments.</p>
<h2>H Details of data domains and studied LLMs</h2>
<p>This section gives the details of the datasets and LLMs in Section 4.2. Table 1 lists the datasets that are used for developing, test, and benchmark models on VERISCORE. Table 13 further expands the name and details of the FreshBooks dataset. For developing VERISCORE, we used the model generations from the first 12 models in Table 14. For benchmarking models on VERISCORE, we used all 16 models in Table 14.</p>
<h2>I Unsupported cases in VERISCORE outputs</h2>
<p>This appendix section provides more examples of the unsupported claims in Table 15 in complementary to Section 4.4.2.</p>
<h2>J VERISCORE on WritingPrompts and FreshQA</h2>
<p>In Section 4, we presented VERISCORE of 16 models on 6 domains of long-form model generation. In this section, we focus solely on model responses in the FreshQA and WritingPrompts datasets. As shown in Table 16, both domains yield very few verifiable facts. The median number for verifiable claims $(K)$ in FreshQA is four because the questions in general do not require long-form answers. WritingPrompts requires long-form generations but they are conditioned on fictional premises. Hence, the generations contain very few verifiable claims, resulting in $K=1$.</p>
<p>For the generations in WritingPromtps, the results in Table 16 show that the VERISCORE of
the models is very low, matching the expectation that there are few supported verifiable claims in a creative writing task. We examine the extracted claims from two models-Gemma-2b-it and GPT4 o. It turns out that the majority of the claims ( $82.61 \%$ for Gemma and $71.15 \%$ for GPT-4o) has the potential to be verified/falsified. The results of the models on WritingPrompts prove that the VERISCORE pipeline works effectively on fictional content although it occasionally extracts unverifiable claims.</p>
<p>For the generations in FreshQA, the results show that the Claude 3 models perform the best. However, upon careful examination of the outputs of Claude 3 Haiku and GPT-4o, we notice that GPT40 has a higher percentage of supported claim among all the extracted claims ( $74.70 \%$ ) compared to Claude 3 Haiku ( $72.71 \%$ ). The fact that GPT-4o generates shorter outputs than Claude 3 Haiku contributes to the lower final VERISCORE of GPT-4o. On average, Claude 3 Haiku has 5.5 claims per response, higher than $K=4$, but GPT-4o has only 3.39. After reading the claims extracted from both models in the FreshQA domain, we notice that GPT-4o tends to generate short and to-the-point answers while Claude 3 Haiku tends to generate longer answers, offering more auxiliary information. Within the Claude 3 model family, Claude 3 Haiku generates longer outputs than the other two and has more responses with 4 or more supported claims, resulting in a higher VERISCORE.</p>
<p>The results of FreshQA shows that, for a fair comparison, when calculating VERISCORE, the length of model responses should be taken into account. This can be done by forcing all the models to generate a similar length of outputs. However, this will not result in a setting of how end-users would use language models. Forcing models to generate longer responses than necessary can also elicit more infactual content, as noticed by Min et al. (2023) that later content in model responses tends to be less factual.</p>
<h2>K Detailed results of models' VERISCORE</h2>
<p>In this appendix section, we provide the breakdown of VERISCORE of the 16 models on all 8 domains in Table 16.</p>
<h1>Prompt for Extracting Verifiable Claims from Non-Question-Answering Type of Inputs</h1>
<p>You are trying to verify how factual a piece of text is. To do so, you need to break down a sentence and extract as many fine-grained facts mentioned in the sentence as possible. Each of these fine-grained facts should be verifiable against reliable external world knowledge (e.g., via Wikipedia). Any story, personal experiences, hypotheticals (e.g., "would be" or subjunctive), subjective statements (e.g., opinions), suggestions, advice, instructions, and other such content should not be included in the list. Biographical, historical, scientific, and other such texts are not personal experiences or stories. You should extract verifiable facts from them. Each fact should also be describing either one single event (e.g., "Nvidia is founded in 1993 in Sunnyvale, California, U.S.") or single state (e.g., " has existed for 161 years.") with necessary time and location information. Quotations should be extracted verbatim with the source when available. Listed references should be ignored.</p>
<p>Extract fine-grained facts from the sentence marked between <SOS> and <EOS>. You should focus on the named entities and numbers in the sentence and extract relevant information from the sentence. Other sentences are only context for you to recover pronouns, definite phrases (e.g., "the victims" or "the pope"), and so on. Each fact should be understandable on its own and require no additional context. This means that all entities must be referred to by name but not pronouns. Use the name of entities rather than definite noun phrases (e.g., 'the teacher') whenever possible. If a definite noun phrase is used, be sure to add modifiers (e.g., a embedded clause, a prepositional phrase, etc.). Each fact must be situated within relevant temporal and location whenever needed. Keep each fact to one sentence with zero or at most one embedded clause. You do not need to justify what you extract.</p>
<p>If there is no verifiable fact in the sentence, please write "No verifiable claim."
Here are some examples:
Text: The sweet potato or sweetpotato (Ipomoea batatas) is a dicotyledonous plant that belongs to the bindweed or morning glory family, Convolvulaceae. <SOS>Its large, starchy, sweet-tasting tuberous roots are used as a root vegetable.<EOS> The young shoots and leaves are sometimes eaten as greens.
Sentence to be focused on: Its large, starchy, sweet-tasting tuberous roots are used as a root vegetable.
Facts:</p>
<ul>
<li>Sweet potatoes' roots are large.</li>
<li>Sweet potatoes' roots are starchy.</li>
<li>Sweet potatoes' roots are sweet-tasting.</li>
<li>Sweet potatoes' roots are tuberous.</li>
<li>Sweet potatoes' roots are used as a root vegetable.</li>
</ul>
<p>Text: Garnett had spent well over a decade with the Minnesota Timberwolves, and while he stayed loyal to that team, he found little success there. <SOS>When he said "you can't get your youth back," he meant it - because from a human standpoint, had he been able to apply his talents somewhere else, NBA history might have been different.<EOS>
Sentence to be focused on: When he said "you can't get your youth back," he meant it - because from a human standpoint, had he been able to apply his talents somewhere else, NBA history might have been different.
Facts:</p>
<ul>
<li>Kevin Garnett said "you can't get your youth back."</li>
</ul>
<p>Text: I (27f) and my fiance "Leo" (27m) decided to let my FSIL "Maya" (32f) stay at our house because she needed space from her husband due to some relationship struggles they're having. Leo and I had gotten wedding cake samples from an expensive bakery specializing in wedding cakes. We planned to test them along with Maya after we finished up some other wedding plans yesterday. <SOS>However, when I came home from work to see Leo yelling at Maya, the box the samples came in wide open on the living room table, and Maya arguing with him.<EOS> I asked what was happening, and Leo angrily told me that while we were both at work, Maya had some friends over and they ended up eating almost all of our cake samples.
Sentence to be focused on: However, when I came home from work to see Leo yelling at Maya, the box the samples came in wide open on the living room table, and Maya arguing with him.
Facts:
No verifiable claim.
... <Total of 13 Examples> ...
Extract <em>verifiable atomic</em> facts.
Text: {sliding window}
Sentence to be focused on: {sentence}
Facts:
Table 6: Claim extraction prompt for non-question-answering type of inputs. The sliding window follows the template (context1 = 0-3 sentence) <SOS>Sentence to be focused on<EOS> (context2 = 0-1 sentence). If the paragraph from which the sentence is taken is longer than five sentences, the first sentence of the paragraph is always prepended before context1. Marked out content will be uncovered after the review process.</p>
<h1>Prompt for Extracting Verifiable Claims from Question-Answering Type of Inputs</h1>
<p>You are trying to verify how factual a response to a question or request is. To do so, you need to break down a sentence and extract as many fine-grained facts mentioned in the response. Each of these fine-grained facts should be verifiable against reliable external world knowledge (e.g., via Wikipedia). Any story, personal experiences, hypotheticals (e.g.,"would be" or subjunctive), subjective statements (e.g., opinions), suggestions, advice, instructions, and other such content should not be included in the list. Biographical, historical, scientific, and other such texts are not personal experiences or stories. You should extract verifiable facts from them. Each fact should also be describing either one single event (e.g., "Nvidia is founded in 1993 in Sunnyvale, California, U.S.") or single state (e.g., has existed for 161 years.") with necessary time and location information. Quotations should be extracted verbatim with the source when available. Listed references should be ignored. Extract fine-grained facts from the sentence between $&lt;$ SOS $&gt;$ and $&lt;$ EOS $&gt;$. You should focus on the named entities and numbers in the sentence and extract relevant information from the sentence. Do not extract claims from the question. The question and other sentences are only context for you to recover pronouns, definite phrases (e.g., "the victims" or "the pope"), and so on. Each fact should be understandable on its own and require no additional context. This means that you need to always related the extracted claims to the question. This also means that all entities must be referred to by name but not pronoun. Use the name of entities rather than definite noun phrases (e.g., 'the teacher') whenever possible. If a definite noun phrase is used, be sure to add modifiers (e.g., a embedded clause, a prepositional phrase, etc.). Each fact must be situated within relevant temporal and location whenever needed. Keep each fact to one sentence with zero or at most one embedded clause. You do not need to justify what you extract.
If there is no verifiable fact in the sentence, please write "No verifiable claim."
Here are some examples:
Question: What NASA programs would support our college in starting a robotics program?
Response: NASA has several programs that can support colleges in starting a robotics program. Here are a few:
<SOS>1. NASA Robotics Alliance Project (RAP): This program provides educational resources and support for robotics teams, including college-level teams, that are participating in NASA robotics competitions.<EOS>
2. NASA Minority University Research and Education Project (MUREP): This program provides funding and resources for colleges and universities with a significant minority student population to develop research and education programs in STEM fields, including robotics.
3. NASA's Robotics Education Project: This project provides robotics education materials and resources for educators, including college-level educators, to use in their classrooms.
4. NASA's Space Technology Mission Directorate (STMD): This directorate funds research and development in advanced technologies, including robotics, that can support NASA's mission to explore space.
Sentence to be focused on: 1. NASA Robotics Alliance Project (RAP): This program provides educational resources and support for robotics teams, including college-level teams, that are participating in NASA robotics competitions.
Facts:</p>
<ul>
<li>NASA has a program called NASA Robotics Alliance Project (RAP).</li>
<li>NASA Robotics Alliance Project provides educational resources for robotics teams.</li>
<li>NASA Robotics Alliance Project provides supports for robotics teams.</li>
<li>NASA Robotics Alliance Project provides supports for college-level teams that are participating in NASA robotics competitions.</li>
</ul>
<p>Question: How do trees know when to stop growing?
Thanks everyone i learned a lot more about trees. $\square$
Response: <SOS>Ah yes, tomatoes, this is a big problem with tomato plants.<EOS>
Sentence to be focused on: Ah yes, tomatoes, this is a big problem with tomato plants.
Facts:
No verifiable claim.
... <Total of 10 Examples $>$...
Extract <em>verifiable atomic</em> facts.
[sliding window]
Sentence to be focused on: {sentence}
Facts:
Table 7: Claim extraction prompt for question-answering type of inputs. The sliding window consists of the question and (context1 = 0-3 sentence) <SOS>Sentence to be focused on<EOS> (context2 = 0-1 sentence). Marked out content will be uncovered after the review process.</p>
<h1>Example Claim 1:</h1>
<p>A wooden spoon creates a small gap between the pot and the spoon.</p>
<h2>Example Claim 2:</h2>
<p>Martha had passed away.</p>
<h2>Example Claim 3:</h2>
<p>A systematic review on sex differences in the reinforcing effects of nicotine was published in the journal Nicotine \&amp; Tobacco Research in 2019.
A part of the claim is not mentioned in the evidence,
or no evidence confirms the relationship between the parts in a claim-Inconclusive (a) Case</p>
<h2>Reasoning:</h2>
<p>(1) Only the search result 7 hints that there might be a Persuasive Technology Lab at Stanford University.
(2) No evidence mentions this lab aims to create positive behavior change.</p>
<h2>Claim</h2>
<p>The intention of Stanford University's Persuasive Technology Lab was to create positive behavior change.</p>
<h2>Evidence</h2>
<p>Search result 1
Title: Behavior Design Lab - Stanford University
Content: Behavior Design is a new approach to understanding human behavior and how to design for behavior change. Based on the work of Dr. BJ Fogg , Behavior Design ...
Link: https://behaviordesign.stanford.edu/
Search result 2
Title: About Us - Behavior Design Lab - Stanford University
Content: Our lab's overall mission is this: Teach good people how human behavior works so they can create solutions that effectively increase health, boost happiness, ...
Link: https://behaviordesign.stanford.edu/about-us
Search result 3
Title: Building Habits: The Key to Lasting Behavior Change
Content: "Habits are easier to form than most people think," he says, "If you do it in the right way." As the founder and director of Stanford's Behavior ...
Link: https://www.gsb.stanford.edu/insights/building-habits-key-lasting-behavior-change
Search result 4
Title: The Ethical Use of Persuasive Technology - Behavior Design Lab
Content: While our research has moved on from persuasive technology to focus on designing for healthy behavior change, we believe it is important to continue to ...
Link: https://behaviordesign.stanford.edu/ethical-use-persuasive-technology
Search result 5
Title: Fiddling With Human Behavior - WIRED
Content: Researchers at Stanford are studying technology designed to persuade people to change the way they think or act.
Link: https://www.wired.com/2000/03/fiddling-with-human-behavior/
Search result 6
Title: BJ Fogg - Behavior Design Lab - Stanford University
Content: BJ wrote a seminal book, Persuasive Technology: Using Computers to Change What We Think and Do , about how computers can be designed to influence attitudes ...
Link: https://behaviordesign.stanford.edu/people/bj-fogg
Search result 7
Title: How Stanford Profits Off Addiction
Content: Back in 1998, one of Stanford's eccentric social scientists, B.J. Fogg, founded the Persuasive Technology Lab to research how tech products ...
Link: https://stanfordreview.org/how-stanford-profits-tech-addiction-social-media/
Search result 8
Title: Tech companies use "persuasive design" to get us hooked ... - Vox
Content: Big tech now employs mental health experts to use persuasive technology, a new field of research that looks at how computers can change the way ...
Link: https://www.vox.com/2018/8/8/17664580/persuasive-technology-psychology
Search result 9
Title: Stanford Behavior Design Lab - Wikipedia
Content: The Stanford Behavior Design Lab is a research organization advancing behavior change methods and models based at Stanford University. Founded in 1998 and ...
Link: https://en.wikipedia.org/wiki/Stanford_Behavior_Design_Lab
Search result 10
Title: How to create new good habits, according to Stanford ... - Quartz
Content: To create a real lifelong habit, the focus should be on training your brain to succeed at a small adjustments, then gaining confidence from that ...
Link: https://qz.com/877795/how-to-create-new-good-habits-according-to-stanford-psychologist-b-j-fogg
Table 8: Examples of the annotaton items whose claim was labeled as inconclusive by all three annotators.</p>
<p>Explanation: Two annotators chose the inconclusive label for this claim but one chose supported based on one search result as given below. This is an annotator mistake because the name Luis Guillermo Rivera is not mentioned in the evidence.
Claim: Luis Guillermo Rivera has written literary criticism.
Evidence:
Search result 5
Title: I Write with Words That Have Shadow but Don't Shelter
Content: Born in Tumeremo, Bolívar, in 1933, Venezuelan writer Guillermo Sucre is also an essayist, translator, literary critic, and educator. A ... Link: https://www.worldliteraturetoday.org/blog/poetry/i-write-words-have-shadow-dont-shelter-guillermo-sucre</p>
<h1>Ambiguity in the interpretation of the claim and evidence</h1>
<p>Explanation: Two annotators chosen the supported label but one chose inconclusive. The annotator's comment states:"I chose inconclusive as the claim is 'can be chosen' and all the results are that they potentially 'could' be chosen, not that they actually CAN."
Claim: Traits that can be chosen include eye color, hair color, intelligence, and athletic ability.
Evidence:
Search result 2
Title: [PDF] Sex Selection, Genetic Analysis, and Designer Babies
Content: In theory, parents could also select embryos on the basis of eye color, hair color, or any other genetic trait.
Link: https://med.nyu.edu/departments-institutes/population-health/divisions-sections-centers/medical-ethics/sites/default/files/medical-ethics-sex-selection-genetic-analysis.pdf</p>
<p>The claim contains an unclear referent.
Explanation: The annotators did not agree on this item at all. It is probably because there are multiple people with the name Jessica Barboza, making it hard to make a decision.
Claim: Jessica Barboza was born in São Paulo, Brazil.
Evidence:
Search result 1
Title: Jessica Barboza - Wikipedia
Content: Jessica Barboza. Born. Jessica Cristina Barboza Schmidt. (1987-08-14) 14 August 1987 (age 36). Maracaibo, Zulia, Venezuela. Height, $1.79 \mathrm{~m}(5 \mathrm{ft} 10+1 / 2 \mathrm{in})$.
Link: https://en.wikipedia.org/wiki/Jessica_Barboza
Search result 3
Title: Jessica Barboza - Age, Family, Bio | Famous Birthdays
Content: Style blogger and makeup guru known for her Peace and Vogue blog and YouTube channel. The blossoming beauty maven has gained a following of more than 550,000 ...
Link: https://www.famousbirthdays.com/people/jessica-barboza.html
Search result 5
Title: Jessica Barboza - Facebook
Content: Jessica Barboza ; Lives in $\square$ ; From São Paulo, Brazil ; In a relationship with $\qquad$ .
Link: https://www.facebook.com/ $\qquad$
The claim is hard to verify because it is long and complex.
Explanation: The annotators did not agree on this item at all. The claim contains multiple parts that are correlated to each other. However, it is also hard to further break the claim down to smaller claims.
Claim: Chuck Norris's victory in the 1968 World Full-Contact Karate Championships solidified his reputation as one of the best martial artists in the world.</p>
<p>Intermediate reasoning process is needed because the evidence might indirectly supports the claim.
Explanation: Two annotators chose the supported label and one chose inconclusive. It is possible to interprete he "safety objectives" in search result 8 as it includes "public health".
Claim: The disposal of radioactive waste is aimed at ensuring public health.
Evidence:
Search result 8
Title: PART 61—LICENSING REQUIREMENTS FOR LAND DISPOSAL ...
Content: (1) Disposal of radioactive waste in near-surface disposal facilities has the following safety objectives: protection of the general population from releases of ...
Link: https://www.nrc.gov/reading-rm/doc-collections/cfr/part061/full-text.html
Table 9: Example of the annotation items on which the annotators did not fully agree with each other. We mark out the private sensitive content.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{24}$ https://unsloth.ai/
${ }^{25}$ The string-based metrics and the scores are the following: exact match $=0.4317$, Rouge1 $=0.8243$, Rouge2 $=0.7576$, RougeL $=0.8009$, and CHRF++ $=74.6686$.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref9:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{14}$ The instruction “Generate a continuation of the following text. The continuation should be objective and factual” is prepended to the FreshBooks paragraphs.
${ }^{15}$ GPT-3.5-turbo-1106 is an exception because the model generates shorter responses than GPT-3.5-turbo-0613, which hurts the recall. On average, GPT-3.5-1106 generates 8.56 sentences per response and GPT-3.5-0613 generates 15.95.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
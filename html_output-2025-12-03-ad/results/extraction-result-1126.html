<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1126 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1126</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1126</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-11296818</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1301.6690v1.pdf" target="_blank">Model-Based Bayesian Exploration</a></p>
                <p><strong>Paper Abstract:</strong> Reinforcement learning systems are often concerned with balancing exploration of untested actions against exploitation of actions that are known to be good. The benefit of exploration can be estimated using the classical notion of Value of Information - the expected improvement in future decision quality arising from the information acquired by exploration. Estimating this quantity requires an assessment of the agent's uncertainty about its current value estimates for states. In this paper we investigate ways of representing and reasoning about this uncertainty in algorithms where the system attempts to learn a model of its environment. We explicitly represent uncertainty about the parameters of the model and build probability distributions over Q-values based on these. These distributions are used to compute a myopic approximation to the value of information for each action and hence to select the action that best balances exploration and exploitation.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1126.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1126.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayes-Exp (VPI agent)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-based Bayesian Exploration using Value of (Perfect) Information</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based reinforcement learning agent that maintains a Bayesian posterior over MDP transition/reward parameters, samples from that posterior to obtain Q-value distributions, and selects actions by adding a myopic Value-of-Perfect-Information (VPI) exploration bonus to expected Q-values to balance exploration vs. exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Model-based Bayesian Exploration (VPI agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Maintains a posterior over MDP parameters (Dirichlet or sparse-multinomial priors, assuming parameter independence), samples MDPs or Q-values from that posterior, computes Q* (via value iteration/prioritized sweeping) for samples, builds Q-value distributions, and selects actions by maximizing E[Q(s,a)] + VPI(s,a). Key components: Bayesian belief state over model parameters, sampling procedures, Q-value estimation, VPI calculation, and optional smoothing of Q-value samples (Gaussian/kernel).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>information gain maximization (myopic Value of Perfect Information / VPI)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>The agent updates a Bayesian posterior after each (s,a,r,t) tuple, samples (or repairs) MDPs from the posterior to obtain distributions over Q(s,a), computes the myopic expected gain (VPI) of learning each action's Q-value, and selects the action maximizing expected value plus VPI. It reweights samples after observations (importance sampling) and resamples when sample weights fall below a threshold. Smoothing (kernel/Gaussian) can be used to generalize from limited Q-value samples.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Custom discrete maze / trap domains (trap domain and larger maze domain)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown MDP dynamics (learned online); stochastic transitions (actions succeed with prob. 0.9 when clear, 0.1 perpendicular), discrete finite state space, episodic resets after flag collection, sparse negative 'trap' reward, finite action set (up/down/left/right). Not partially observable (standard MDP assumptions).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Reported domains: 'trap' domain with 18 states and 4 actions; 'maze' domain with 56 states and 4 actions; infinite-horizon discounted return objective (discount factor γ referenced but not numerically specified); episodes reset after completing flag→goal sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Qualitative: Bayesian VPI-based agents outperform prioritized sweeping early in learning (higher average future discounted reward over runs). Global-sampling variant attains best asymptotic/early performance among presented Bayesian variants but is computationally expensive. Exact numeric metrics not provided in text (evaluations reported as averaged discounted future reward over 10 runs; figures in paper show curves but no explicit numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baseline prioritized sweeping reported for comparison; Bayesian methods outperform it early. Exact baseline numeric values are not provided in text, only plotted comparisons; therefore numeric baseline performance is null.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Qualitative: Bayesian VPI methods explore more widely and reach better policies earlier (improved sample efficiency) compared to prioritized sweeping in tested maze/trap domains; most algorithms (except some smoothing variants) converge by ~1500 steps in reported runs. No precise sample counts to reach fixed performance thresholds are given.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Balanced via decision-theoretic VPI: action selection maximizes E[Q] + VPI(s,a), where VPI is the expected improvement in future decision quality if the true Q(s,a) were known; this creates an explicit, quantitative tradeoff between immediate reward and expected informational benefit of actions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Prioritized Sweeping (Moore & Atkeson 1993) as a baseline; Kearns & Singh theoretical approach discussed; internal comparisons across sampling strategies and smoothing (naive global sampling, importance sampling, sampling-with-repair, local sampling; Gaussian vs kernel smoothing).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Representing posterior distributions over MDP parameters and using them to compute Q-value distributions allows principled, quantitatively-driven exploration via VPI; this yields better early learning performance on trap/maze tasks than prioritized sweeping. Different approximation/sampling strategies trade off computation vs. accuracy: global sampling gives best performance but is computationally heavy, importance sampling is faster but may converge late or fail on some trials, sampling-with-repair provides a good compromise. Kernel (nonparametric) smoothing of Q-value samples performs better than Gaussian smoothing given skewed Q distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Computational cost: naive global sampling requires solving many full MDPs repeatedly (high CPU cost). Importance sampling can fail to represent posteriors well if proposal-target diverge and did not converge on all trials. Gaussian smoothing can misrepresent skewed Q-value distributions and slow convergence (continued exploration past 1500 steps). Local-sampling assumes independence between Q-value samples across states (an approximation that may be violated). Experiments are limited to relatively small, fully-observable discrete mazes; no continuous or partially-observable domains evaluated. The exploration criterion is myopic VPI (not globally optimal non-myopic information gathering).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1126.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1126.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Naive Global Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Naive sampling of full MDPs from the posterior (global sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that samples k full MDPs from the Bayesian posterior, solves each MDP to compute Q* per-sample, and uses the empirical Q-value samples to estimate means and VPI for action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Naive Global Sampling Bayesian agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Samples k complete MDP parameterizations independently from the current posterior, solves each MDP (value iteration / linear programming), collects Q*(s,a) samples across MDPs, estimates E[Q] and VPI for each action, and selects actions maximizing E[Q]+VPI. Resampling occurs as posterior changes significantly.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>information gain maximization (VPI) via full-model sampling</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts by updating posterior after each observation, drawing fresh sets of k MDP samples (or reusing/resampling when necessary), recomputing Q* per sample, and selecting actions using sample-derived VPI.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same maze/trap domains (trap: 18 states; maze: 56 states)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown discrete stochastic MDP dynamics; finite state/action spaces; trap with large negative reward; stochastic action outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>k full MDP solves per action-selection (k large enough to approximate distributions); reported domains: 18 and 56 states, 4 actions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Best empirical performance among Bayesian variants in the experiments (highest learning curves), especially early in learning; however, numeric results are provided only as plotted averaged discounted reward curves, not explicit numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>High sample efficiency in terms of interactions (learns good policies early), but computational sample (CPU) cost is high because each decision uses k full MDP solves; authors report computation about ~10x that of sampling-with-repair in their implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Direct VPI computation from full-model Q-samples yields explicit bootstrap of exploration when uncertainty is high; as posterior uncertainty shrinks, VPI terms shrink and behavior becomes greedy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against importance sampling, sampling-with-repair, local sampling, prioritized sweeping; kernel/Gaussian/no-smoothing variants evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Highest performance of Bayesian variants on maze/trap tasks but at substantial computational expense (~10x cost vs repair-based method). Works well when computational resources permit.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Computationally expensive (global MDP solves); impractical to run many times or on larger state spaces without further approximations; must resample frequently as posterior changes which increases runtime.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1126.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1126.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Importance Sampling (reweighting)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Importance sampling of MDPs with reweighting across steps</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reuses previously-sampled MDPs by reweighting them according to the updated posterior after each observation (importance sampling), avoiding re-solving new MDPs at every step until weights fall below threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Importance Sampling Bayesian agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Maintains a set of k sampled MDPs drawn from an earlier posterior, reweights each sample according to likelihood ratio under updated posterior after each new (s,a,r,t), computes weighted estimates of E[Q] and VPI, and resamples new MDPs when total sample weight drops below a threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>information gain maximization (VPI) using importance sampling</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts by updating sample weights after each experience according to Pr(M|new_posterior)/Pr(M|old_posterior); uses weighted Q-samples to compute VPI; triggers resampling when cumulative weight is too low to represent the current posterior.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same maze/trap domains</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Stochastic, discrete MDPs with unknown dynamics; same trap and maze characteristics.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Avoids repeated full MDP solves by reusing weighted samples; effective in small-to-moderate state spaces used in experiments (18, 56 states).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Faster runtime than naive global sampling (roughly ~2x faster according to authors) but empirically converged relatively late on the larger maze problem and did not converge on all trials in experiments; qualitative learning curves plotted in paper show delayed convergence compared to global sampling and sampling-with-repair.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Good interaction efficiency initially because samples are reused, but sample-representation can degrade as posterior changes, necessitating resampling; no explicit numeric sample-efficiency thresholds provided.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Same VPI-based tradeoff as other Bayesian variants; however efficacy depends on quality of weighted sample representation of posterior.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to naive global sampling, sampling-with-repair, local sampling, and prioritized sweeping.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Importance sampling can greatly reduce computation by reusing solved MDPs and reweighting them, but performance depends on similarity between sampling distribution and updated posterior; if they diverge, reweighting becomes ineffective and resampling is required. Empirically faster but sometimes less reliable convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Can fail when posterior shifts substantially (low effective sample size), causing poor representation of current beliefs and possibly failed convergence on some trials; needs thresholding and periodic resampling to remain effective.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1126.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1126.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sampling-with-Repair</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Global sampling with local repair (resample affected parameters & prioritized sweeping per sample)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An efficient approximation that keeps k sampled MDPs but, after each observed transition, re-samples only the affected local parameters for each sampled MDP and runs localized prioritized sweeping updates on each sample (repair), avoiding full re-solve of each MDP.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Sampling-with-Repair Bayesian agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Samples k MDPs initially; upon each observation (s,a,r,t) updates posterior for the local parameters (transition/reward for that s,a), re-samples those local parameters for each sampled MDP (repair), and runs prioritized sweeping locally within each MDP to update Q-values; uses resulting per-sample Q-values to compute E[Q] and VPI for action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>information gain maximization (VPI) using repaired sample MDPs and localized prioritized sweeping</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts by locally repairing sampled models to reflect evidence at specific (s,a) tuples, then propagating value updates via prioritized sweeping within each repaired sample; recomputes Q-sample distributions and VPI for action selection. Samples are reweighted/resampled only when needed.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same maze/trap domains</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown discrete stochastic MDPs with sparse transitions and traps; experiments on 18- and 56-state mazes with 4 actions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Targets moderate size domains where full re-solving k MDPs per step is expensive; reported computational cost ~1/10 that of naive global sampling in the authors' implementation (i.e., global sampling ~10x more expensive).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Strong empirical performance: nearly matches naive global sampling in learning quality while being significantly cheaper computationally; authors report it as a good compromise with reasonably fast convergence and reliable behavior across trials.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>High interaction sample efficiency similar to global sampling; significantly more CPU-efficient because updates are localized rather than full re-solves. No precise numeric sample counts provided.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Same VPI-driven tradeoff, but implemented more efficiently by keeping corrected samples up-to-date via local repair and prioritized sweeping.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared directly against naive global sampling, importance sampling, local sampling, and prioritized sweeping; smoothing variants also compared.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Sampling-with-repair achieves a favorable tradeoff: close-to-best learning performance of global sampling with substantially reduced computation (~one order of magnitude cheaper in reported implementation). Recommended as practical approach for model-based Bayesian exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Still requires maintaining k parallel model/value instances and doing prioritized sweeping per sample (computational overhead non-trivial); suitability for much larger state spaces or continuous domains not demonstrated; quality depends on the assumption that local repairs and localized sweeping are sufficient to approximate full re-solves.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1126.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1126.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Local Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Local Bellman-like sampling of Q-value distributions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that directly maintains and updates Q-value distributions per (s,a) using local Bellman-update style sampling: sample Q-values for successor states and transition/reward parameters and produce samples for q(s,a) without sampling full MDPs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Local Sampling Bayesian agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Maintains per-(s,a) approximations of Q-value distributions; performs Bellman-iteration-like updates by jointly sampling PT(s'|s,a), PR(r|s,a), and sampled q(s',a') from current per-(s',a') distributions to produce k samples of q(s,a); propagates updates (optionally with prioritized sweeping) based on which Q distributions are most affected.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>information gain maximization (VPI) using locally-updated Q-value distributions</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts by updating local Q-value distributions as observations update posteriors over local model parameters and by propagating these updates to predecessor Q distributions using prioritized-sweeping-like prioritization; selection uses E[Q]+VPI computed from these local distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same maze/trap domains</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown stochastic discrete MDPs with sparse transitions and trap states; finite action set; used on 18- and 56-state domains.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Avoids sampling full MDPs; relies on local sampling and iterative propagation; complexity depends on number of (s,a) distributions maintained and prioritized updates applied.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Proposed as computationally cheaper alternative; empirical evaluation limited in paper (authors state they are investigating local sampling's effectiveness further). The presented experiments emphasize global/repair/importance sampling more; no strong numeric performance claims provided for local sampling in main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Potentially high in CPU cost compared to global solves since only local samples and prioritized updates are done, but sample independence assumptions and propagation approximations can affect accuracy; no quantitative sample-efficiency numbers reported.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Same VPI framework applied to per-(s,a) Q distributions; assumes approximate independence between Q-value distributions across states to make local sampling tractable.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Discussed relative to global sampling, importance sampling, sampling-with-repair, and prioritized sweeping; not emphasized in main empirical comparisons in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Local sampling offers a conceptually attractive and more computationally efficient way to approximate Q-value distributions and VPI, but requires approximation assumptions (e.g., independence of Q-value samples across states) whose effects need empirical study; authors note they are investigating it further.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Key approximation (independence of Q-values across states) is generally false due to Bellman coupling; propagation of approximations may accumulate error; limited empirical validation in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Bayesian Q learning <em>(Rating: 2)</em></li>
                <li>Efficient bayesian parameter estimation in large discrete domains <em>(Rating: 2)</em></li>
                <li>Prioritized sweeping reinforcement learning with less data and less time <em>(Rating: 1)</em></li>
                <li>Generalized prioritized sweeping <em>(Rating: 1)</em></li>
                <li>Near-optimal performance for reinforcement learning in polynomial time <em>(Rating: 2)</em></li>
                <li>Information value theory <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1126",
    "paper_id": "paper-11296818",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "Bayes-Exp (VPI agent)",
            "name_full": "Model-based Bayesian Exploration using Value of (Perfect) Information",
            "brief_description": "A model-based reinforcement learning agent that maintains a Bayesian posterior over MDP transition/reward parameters, samples from that posterior to obtain Q-value distributions, and selects actions by adding a myopic Value-of-Perfect-Information (VPI) exploration bonus to expected Q-values to balance exploration vs. exploitation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Model-based Bayesian Exploration (VPI agent)",
            "agent_description": "Maintains a posterior over MDP parameters (Dirichlet or sparse-multinomial priors, assuming parameter independence), samples MDPs or Q-values from that posterior, computes Q* (via value iteration/prioritized sweeping) for samples, builds Q-value distributions, and selects actions by maximizing E[Q(s,a)] + VPI(s,a). Key components: Bayesian belief state over model parameters, sampling procedures, Q-value estimation, VPI calculation, and optional smoothing of Q-value samples (Gaussian/kernel).",
            "adaptive_design_method": "information gain maximization (myopic Value of Perfect Information / VPI)",
            "adaptation_strategy_description": "The agent updates a Bayesian posterior after each (s,a,r,t) tuple, samples (or repairs) MDPs from the posterior to obtain distributions over Q(s,a), computes the myopic expected gain (VPI) of learning each action's Q-value, and selects the action maximizing expected value plus VPI. It reweights samples after observations (importance sampling) and resamples when sample weights fall below a threshold. Smoothing (kernel/Gaussian) can be used to generalize from limited Q-value samples.",
            "environment_name": "Custom discrete maze / trap domains (trap domain and larger maze domain)",
            "environment_characteristics": "Unknown MDP dynamics (learned online); stochastic transitions (actions succeed with prob. 0.9 when clear, 0.1 perpendicular), discrete finite state space, episodic resets after flag collection, sparse negative 'trap' reward, finite action set (up/down/left/right). Not partially observable (standard MDP assumptions).",
            "environment_complexity": "Reported domains: 'trap' domain with 18 states and 4 actions; 'maze' domain with 56 states and 4 actions; infinite-horizon discounted return objective (discount factor γ referenced but not numerically specified); episodes reset after completing flag→goal sequence.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Qualitative: Bayesian VPI-based agents outperform prioritized sweeping early in learning (higher average future discounted reward over runs). Global-sampling variant attains best asymptotic/early performance among presented Bayesian variants but is computationally expensive. Exact numeric metrics not provided in text (evaluations reported as averaged discounted future reward over 10 runs; figures in paper show curves but no explicit numbers).",
            "performance_without_adaptation": "Baseline prioritized sweeping reported for comparison; Bayesian methods outperform it early. Exact baseline numeric values are not provided in text, only plotted comparisons; therefore numeric baseline performance is null.",
            "sample_efficiency": "Qualitative: Bayesian VPI methods explore more widely and reach better policies earlier (improved sample efficiency) compared to prioritized sweeping in tested maze/trap domains; most algorithms (except some smoothing variants) converge by ~1500 steps in reported runs. No precise sample counts to reach fixed performance thresholds are given.",
            "exploration_exploitation_tradeoff": "Balanced via decision-theoretic VPI: action selection maximizes E[Q] + VPI(s,a), where VPI is the expected improvement in future decision quality if the true Q(s,a) were known; this creates an explicit, quantitative tradeoff between immediate reward and expected informational benefit of actions.",
            "comparison_methods": "Prioritized Sweeping (Moore & Atkeson 1993) as a baseline; Kearns & Singh theoretical approach discussed; internal comparisons across sampling strategies and smoothing (naive global sampling, importance sampling, sampling-with-repair, local sampling; Gaussian vs kernel smoothing).",
            "key_results": "Representing posterior distributions over MDP parameters and using them to compute Q-value distributions allows principled, quantitatively-driven exploration via VPI; this yields better early learning performance on trap/maze tasks than prioritized sweeping. Different approximation/sampling strategies trade off computation vs. accuracy: global sampling gives best performance but is computationally heavy, importance sampling is faster but may converge late or fail on some trials, sampling-with-repair provides a good compromise. Kernel (nonparametric) smoothing of Q-value samples performs better than Gaussian smoothing given skewed Q distributions.",
            "limitations_or_failures": "Computational cost: naive global sampling requires solving many full MDPs repeatedly (high CPU cost). Importance sampling can fail to represent posteriors well if proposal-target diverge and did not converge on all trials. Gaussian smoothing can misrepresent skewed Q-value distributions and slow convergence (continued exploration past 1500 steps). Local-sampling assumes independence between Q-value samples across states (an approximation that may be violated). Experiments are limited to relatively small, fully-observable discrete mazes; no continuous or partially-observable domains evaluated. The exploration criterion is myopic VPI (not globally optimal non-myopic information gathering).",
            "uuid": "e1126.0"
        },
        {
            "name_short": "Naive Global Sampling",
            "name_full": "Naive sampling of full MDPs from the posterior (global sampling)",
            "brief_description": "An approach that samples k full MDPs from the Bayesian posterior, solves each MDP to compute Q* per-sample, and uses the empirical Q-value samples to estimate means and VPI for action selection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Naive Global Sampling Bayesian agent",
            "agent_description": "Samples k complete MDP parameterizations independently from the current posterior, solves each MDP (value iteration / linear programming), collects Q*(s,a) samples across MDPs, estimates E[Q] and VPI for each action, and selects actions maximizing E[Q]+VPI. Resampling occurs as posterior changes significantly.",
            "adaptive_design_method": "information gain maximization (VPI) via full-model sampling",
            "adaptation_strategy_description": "Adapts by updating posterior after each observation, drawing fresh sets of k MDP samples (or reusing/resampling when necessary), recomputing Q* per sample, and selecting actions using sample-derived VPI.",
            "environment_name": "Same maze/trap domains (trap: 18 states; maze: 56 states)",
            "environment_characteristics": "Unknown discrete stochastic MDP dynamics; finite state/action spaces; trap with large negative reward; stochastic action outcomes.",
            "environment_complexity": "k full MDP solves per action-selection (k large enough to approximate distributions); reported domains: 18 and 56 states, 4 actions.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Best empirical performance among Bayesian variants in the experiments (highest learning curves), especially early in learning; however, numeric results are provided only as plotted averaged discounted reward curves, not explicit numbers.",
            "performance_without_adaptation": null,
            "sample_efficiency": "High sample efficiency in terms of interactions (learns good policies early), but computational sample (CPU) cost is high because each decision uses k full MDP solves; authors report computation about ~10x that of sampling-with-repair in their implementation.",
            "exploration_exploitation_tradeoff": "Direct VPI computation from full-model Q-samples yields explicit bootstrap of exploration when uncertainty is high; as posterior uncertainty shrinks, VPI terms shrink and behavior becomes greedy.",
            "comparison_methods": "Compared against importance sampling, sampling-with-repair, local sampling, prioritized sweeping; kernel/Gaussian/no-smoothing variants evaluated.",
            "key_results": "Highest performance of Bayesian variants on maze/trap tasks but at substantial computational expense (~10x cost vs repair-based method). Works well when computational resources permit.",
            "limitations_or_failures": "Computationally expensive (global MDP solves); impractical to run many times or on larger state spaces without further approximations; must resample frequently as posterior changes which increases runtime.",
            "uuid": "e1126.1"
        },
        {
            "name_short": "Importance Sampling (reweighting)",
            "name_full": "Importance sampling of MDPs with reweighting across steps",
            "brief_description": "Reuses previously-sampled MDPs by reweighting them according to the updated posterior after each observation (importance sampling), avoiding re-solving new MDPs at every step until weights fall below threshold.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Importance Sampling Bayesian agent",
            "agent_description": "Maintains a set of k sampled MDPs drawn from an earlier posterior, reweights each sample according to likelihood ratio under updated posterior after each new (s,a,r,t), computes weighted estimates of E[Q] and VPI, and resamples new MDPs when total sample weight drops below a threshold.",
            "adaptive_design_method": "information gain maximization (VPI) using importance sampling",
            "adaptation_strategy_description": "Adapts by updating sample weights after each experience according to Pr(M|new_posterior)/Pr(M|old_posterior); uses weighted Q-samples to compute VPI; triggers resampling when cumulative weight is too low to represent the current posterior.",
            "environment_name": "Same maze/trap domains",
            "environment_characteristics": "Stochastic, discrete MDPs with unknown dynamics; same trap and maze characteristics.",
            "environment_complexity": "Avoids repeated full MDP solves by reusing weighted samples; effective in small-to-moderate state spaces used in experiments (18, 56 states).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Faster runtime than naive global sampling (roughly ~2x faster according to authors) but empirically converged relatively late on the larger maze problem and did not converge on all trials in experiments; qualitative learning curves plotted in paper show delayed convergence compared to global sampling and sampling-with-repair.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Good interaction efficiency initially because samples are reused, but sample-representation can degrade as posterior changes, necessitating resampling; no explicit numeric sample-efficiency thresholds provided.",
            "exploration_exploitation_tradeoff": "Same VPI-based tradeoff as other Bayesian variants; however efficacy depends on quality of weighted sample representation of posterior.",
            "comparison_methods": "Compared to naive global sampling, sampling-with-repair, local sampling, and prioritized sweeping.",
            "key_results": "Importance sampling can greatly reduce computation by reusing solved MDPs and reweighting them, but performance depends on similarity between sampling distribution and updated posterior; if they diverge, reweighting becomes ineffective and resampling is required. Empirically faster but sometimes less reliable convergence.",
            "limitations_or_failures": "Can fail when posterior shifts substantially (low effective sample size), causing poor representation of current beliefs and possibly failed convergence on some trials; needs thresholding and periodic resampling to remain effective.",
            "uuid": "e1126.2"
        },
        {
            "name_short": "Sampling-with-Repair",
            "name_full": "Global sampling with local repair (resample affected parameters & prioritized sweeping per sample)",
            "brief_description": "An efficient approximation that keeps k sampled MDPs but, after each observed transition, re-samples only the affected local parameters for each sampled MDP and runs localized prioritized sweeping updates on each sample (repair), avoiding full re-solve of each MDP.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Sampling-with-Repair Bayesian agent",
            "agent_description": "Samples k MDPs initially; upon each observation (s,a,r,t) updates posterior for the local parameters (transition/reward for that s,a), re-samples those local parameters for each sampled MDP (repair), and runs prioritized sweeping locally within each MDP to update Q-values; uses resulting per-sample Q-values to compute E[Q] and VPI for action selection.",
            "adaptive_design_method": "information gain maximization (VPI) using repaired sample MDPs and localized prioritized sweeping",
            "adaptation_strategy_description": "Adapts by locally repairing sampled models to reflect evidence at specific (s,a) tuples, then propagating value updates via prioritized sweeping within each repaired sample; recomputes Q-sample distributions and VPI for action selection. Samples are reweighted/resampled only when needed.",
            "environment_name": "Same maze/trap domains",
            "environment_characteristics": "Unknown discrete stochastic MDPs with sparse transitions and traps; experiments on 18- and 56-state mazes with 4 actions.",
            "environment_complexity": "Targets moderate size domains where full re-solving k MDPs per step is expensive; reported computational cost ~1/10 that of naive global sampling in the authors' implementation (i.e., global sampling ~10x more expensive).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Strong empirical performance: nearly matches naive global sampling in learning quality while being significantly cheaper computationally; authors report it as a good compromise with reasonably fast convergence and reliable behavior across trials.",
            "performance_without_adaptation": null,
            "sample_efficiency": "High interaction sample efficiency similar to global sampling; significantly more CPU-efficient because updates are localized rather than full re-solves. No precise numeric sample counts provided.",
            "exploration_exploitation_tradeoff": "Same VPI-driven tradeoff, but implemented more efficiently by keeping corrected samples up-to-date via local repair and prioritized sweeping.",
            "comparison_methods": "Compared directly against naive global sampling, importance sampling, local sampling, and prioritized sweeping; smoothing variants also compared.",
            "key_results": "Sampling-with-repair achieves a favorable tradeoff: close-to-best learning performance of global sampling with substantially reduced computation (~one order of magnitude cheaper in reported implementation). Recommended as practical approach for model-based Bayesian exploration.",
            "limitations_or_failures": "Still requires maintaining k parallel model/value instances and doing prioritized sweeping per sample (computational overhead non-trivial); suitability for much larger state spaces or continuous domains not demonstrated; quality depends on the assumption that local repairs and localized sweeping are sufficient to approximate full re-solves.",
            "uuid": "e1126.3"
        },
        {
            "name_short": "Local Sampling",
            "name_full": "Local Bellman-like sampling of Q-value distributions",
            "brief_description": "A method that directly maintains and updates Q-value distributions per (s,a) using local Bellman-update style sampling: sample Q-values for successor states and transition/reward parameters and produce samples for q(s,a) without sampling full MDPs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Local Sampling Bayesian agent",
            "agent_description": "Maintains per-(s,a) approximations of Q-value distributions; performs Bellman-iteration-like updates by jointly sampling PT(s'|s,a), PR(r|s,a), and sampled q(s',a') from current per-(s',a') distributions to produce k samples of q(s,a); propagates updates (optionally with prioritized sweeping) based on which Q distributions are most affected.",
            "adaptive_design_method": "information gain maximization (VPI) using locally-updated Q-value distributions",
            "adaptation_strategy_description": "Adapts by updating local Q-value distributions as observations update posteriors over local model parameters and by propagating these updates to predecessor Q distributions using prioritized-sweeping-like prioritization; selection uses E[Q]+VPI computed from these local distributions.",
            "environment_name": "Same maze/trap domains",
            "environment_characteristics": "Unknown stochastic discrete MDPs with sparse transitions and trap states; finite action set; used on 18- and 56-state domains.",
            "environment_complexity": "Avoids sampling full MDPs; relies on local sampling and iterative propagation; complexity depends on number of (s,a) distributions maintained and prioritized updates applied.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Proposed as computationally cheaper alternative; empirical evaluation limited in paper (authors state they are investigating local sampling's effectiveness further). The presented experiments emphasize global/repair/importance sampling more; no strong numeric performance claims provided for local sampling in main experiments.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Potentially high in CPU cost compared to global solves since only local samples and prioritized updates are done, but sample independence assumptions and propagation approximations can affect accuracy; no quantitative sample-efficiency numbers reported.",
            "exploration_exploitation_tradeoff": "Same VPI framework applied to per-(s,a) Q distributions; assumes approximate independence between Q-value distributions across states to make local sampling tractable.",
            "comparison_methods": "Discussed relative to global sampling, importance sampling, sampling-with-repair, and prioritized sweeping; not emphasized in main empirical comparisons in the paper.",
            "key_results": "Local sampling offers a conceptually attractive and more computationally efficient way to approximate Q-value distributions and VPI, but requires approximation assumptions (e.g., independence of Q-value samples across states) whose effects need empirical study; authors note they are investigating it further.",
            "limitations_or_failures": "Key approximation (independence of Q-values across states) is generally false due to Bellman coupling; propagation of approximations may accumulate error; limited empirical validation in this paper.",
            "uuid": "e1126.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Bayesian Q learning",
            "rating": 2,
            "sanitized_title": "bayesian_q_learning"
        },
        {
            "paper_title": "Efficient bayesian parameter estimation in large discrete domains",
            "rating": 2,
            "sanitized_title": "efficient_bayesian_parameter_estimation_in_large_discrete_domains"
        },
        {
            "paper_title": "Prioritized sweeping reinforcement learning with less data and less time",
            "rating": 1,
            "sanitized_title": "prioritized_sweeping_reinforcement_learning_with_less_data_and_less_time"
        },
        {
            "paper_title": "Generalized prioritized sweeping",
            "rating": 1,
            "sanitized_title": "generalized_prioritized_sweeping"
        },
        {
            "paper_title": "Near-optimal performance for reinforcement learning in polynomial time",
            "rating": 2,
            "sanitized_title": "nearoptimal_performance_for_reinforcement_learning_in_polynomial_time"
        },
        {
            "paper_title": "Information value theory",
            "rating": 1,
            "sanitized_title": "information_value_theory"
        }
    ],
    "cost": 0.0146185,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Model based Bayesian Exploration</p>
<p>Richard Dearden dearden@cs.ubc.ca 
Department of Computer Science
Institute of Computer Science
University of British Columbia Vancouver
V6T 1Z4BCCANADA</p>
<p>Nir Friedman 
Computer Science Division 387 Soda Hall
Hebrew University
91904JerusalemISRAEL</p>
<p>David Andre dandre@cs.berkeley.edu 
University of California Berkeley
94720-1776CAUSA</p>
<p>Model based Bayesian Exploration
150
Reinforcement learning systems are often concerned with balancing exploration of untested actions against exploitation of actions that are known to be good. The benefi t of exploration can be estimated using the classi cal notion of Value of Information-the expected im provement in future decision quality arising from the tnformation acquired by exploration. Estimating this quantity requires an assessment of the agent's uncer tainty about its current value estimates for states.In this paper we investigate ways to represent and rea son about this uncertainty in algorithms where the sys tem attempts to learn a model of its environment. We explicitly represent uncertainty about the parameters of the model and build probability distributions over Q values based on these. These distributions are used to compute a myopic approximation to the value of infor mation for each action and hence to select the action that best balances exploration and exploitation.</p>
<p>Introduction</p>
<p>Reinforcement learning addresses the problem of how an agent should learn to act in dynamic environments. This is an important learning paradigm for domains where the agent must consider sequences of actions to be made throughout its lifetime. The framework underlying much of reinforcement learning is that of Markov Decision Pro cesses (MOPs). These processes describe the effects of ac tions in a stochastic environment, and the possible rewards at various states of the environments. If we have an MOP we can compute the choice of actions that maximizes the expected future reward. The task in reinforcement learning is to achieve this level of performance when the underlying MOP is not known in advance.</p>
<p>A central debate in reinforcement learning is over the use of models . . Model-free approaches attempt to learn near optimal policies without explicitly estimating the dynamics of the surrounding environment. This is usually done by directly approximating a value function that measures the desirability of each environment state. On the other hand, model-based approaches attempt to estimate a model of the environment's dynamics and use it to compute an estimate of the expected value of actions in the environment.</p>
<p>A common argument for model-based approaches is that by learning a model the agent can avoid costly repetition of steps in the environment. Instead, the agent can use the model to learn the effects of its actions at various states.</p>
<p>This can lead to a significant reduction in the number of steps actually executed by the learner, since it can "learn" from simulated steps in the model (Sutton 1990).</p>
<p>Virtually all of the existing model-based approaches in the literature use simple estimation methods to learn the en vironment, and keep a point-estimate of the environment dynamics. Such estimates ignore the agent's uncertainty about various aspects of the environment's dynamics.</p>
<p>In this paper, we advocate a Bayesian approach to model based reinforcement learning. We show that under fairly reasonable assumptions we can represent the posterior dis tribution over possible models given our past experience. This is done with essentially the same cost as maintaining point estimates. Our methods thus allow us to continually update this distribution over possible models as we perform actions in the environment. By representing a distribution over possible models , we can quantify our uncertainty as to what are the best actions to perform. This gives us a handle on the exploitation vs. exploration problem. Roughly speaking, this problem in volves the dilemma of whether to explore-perform new actions that can lead us to uncharted territories-or to ex ploitperform actions that have the best performance ac cording to our current knowledge. Clearly, the uncertainty about our model and our expectations as to the range of pos sible results of actions play crucial roles in this problem.</p>
<p>In a precursor to this work, Dearden et a!. (1998) intro duce a Bayesian model-free approach in which uncertainty about the Q-values of actions is represented using probabil ity distributions. By explicitly reasoning using uncertainty about Q-values, they direct exploration specifically toward poorly known regions of the state space. Their approach is based on a decision-theoretic approach to action selec tion: the agent should choose actions based on the value of the information it can expect to learn by performing them (Howard 1966). Dearden  In this paper, we show how to use the posterior distri bution over possible models to estimate the distribution of possible Q-values, and then use these to select actions. This use of models allows us to avoid the problem faced by model-free exploration methods, such as the one used by Dearden et al., that neect to perform repeated actions to propagate values from one state to another. The main ques tion is how to estimate these Q-values from our distribu tion of possible models. We present several methods of stochastic sampling to approximate these Q-value distribu tions. We then evaluate the performance of the resulting Bayesian learning agents on test environments that are de signed to fool many exploration methods.</p>
<p>In Section 2 we briefly review the defi nition of MOPs and the definition of reinforcement learning problems. In Sec tion 3 we discuss a Bayesian approach for learning models.</p>
<p>In Section 4 we review the notion of Q-value distributions and the use of value of information for directing exploration and the notion. In Section 5 we propose several sampling methods for estimating Q-value distributions based on the uncertainty about the underlying model. In Section 6 we discuss several approaches of generalizing from the sam ples we get from the aforementioned methods, and how this generalization can improve our algorithms. In Section 7 we compare our methods to Prioritized Sweeping (Moore &amp; Atkeson 1993), a well known model-based reinforcement learning procedure.</p>
<p>Background</p>
<p>We assume the reader is familiar with the basic concepts of MOPs (see, e.g., (Kaelbling, Littman &amp; Moore 1996)).</p>
<p>We will use the following notation: An MOP is a 4-tuple, (S,A,J1T,PR) where Sis a set of states, A is a set of ac tions, PT ( sl:tt) is a transition model that captures the prob ability of reaching state t after we execute action a at state s, and PR(s.i:tr) is areward model that captures the proba bility of receiving reward rafter executing a at state s. For the reminder of this paper, we assume that possible rewards are a finite subset n of the real numbers.</p>
<p>In this paper, we focus on infi nite-horizon MOPs with a discount factor I· The agent's aim is to maximize the ex pected discounted total reward it receives. Equivalently, we can compute a optimal value function V<em> and a Q-function Q</em>.  </p>
<p>31ES</p>
<p>If the agent has access to v• or Q*, it can optimize its ex pected reward by choosing the action a at s that maximizes </p>
<p>Bayesian Model Learning</p>
<p>In this section we describe how to maintain a Bayesian pos terior distribution over MOPs given our experiences in the environment. At each step in the environment, we start at state s, choose an action a, and then observe a new state t and a reward r. We summarize our experience by a se quence of experience tuples (s, a, r, t).</p>
<p>A Bayesian approach to this learning problem is to main tain a belief state over the possible MOPs. Thus, a belief state Jl. defines a probability density P(M I Jl.). Given an experience tuple (s, a, r, t) we can compute the posterior belief state, which we denote fJ o (s, a, r, t), by Bayes rule: Thus, the prior distribution over the parameters of each lo cal probability term in the MOP is independent of the prior over the others. It turns out that this form is maintained as we incorporate evidence.
P(M I fJ o (s,
Proposition 3.1: lfthebeliefstate P(O I fJ) satisfies param eter independence, then P ( e I fJ o ( s, a, r, t)) also satisfies parameter independence.</p>
<p>As a consequence, the posterior after we incorporate an ar bitrarily long number of experience tuples also has the prod uct form of(!).</p>
<p>Parameter independence allows us to reformulate the learning problem as a collection of unrelated local learning problems. In each of these, we have to estimate a probabil ity distribution over all states or all rewards. The question is how to learn these distributions. We can use well-known Bayesian methods for learning standard distributions such as multinomials or Gaussian distributions (Degroot 1986).</p>
<p>1 The methods we describe are easily extend to other param eterizations. In particular, we can consider continuous distribu tions, e.g., Gaussians, over rewards. For clarity of discussion, we focus on multinomial distributions throughout the paper.</p>
<p>For the case of discrete multinomials, which we have assumed in our transition and reward models, we can use D�richlet prior _ s to represent Pr(e;,a ) and Pr(e�,al· These pnors are conjugate, and thus the posterior after each ob served experience tuple will also be a Dirichlet distribution.</p>
<p>In addition, Dirichlet distributions can be described using a small number of hyper-parameters. See Appendix A for a review of Dirichlet priors and their properties.</p>
<p>In the case of most MOPs studied in reinforcement learn ing, we expect the transition model to be sparse-there are only a few states that can result from a particular action at a particular state. Unfortunately, if the state space is large, learning with a Dirichlet prior can require many examples to recognize that most possible states are highly unlikely. This problem is addressed by a recent method of learn ing sparse-multinomial priors (Friedman &amp; Singer 1999).</p>
<p>Without going into details, the sparse-multinomial priors have the same general properties as Dirichlet priors, but as sume that the observed outcomes are from some small sub sets of the set of possible ones. The sparse Dirichlet priors make predictions as though only the observed outcomes are possible, except that they also assign to novel outcomes. In the MOP setting, a novel outcome is a transition to state t that was not reached from s previously by executing a. See Appendix A for a brief summary of sparse-multinomial pri ors and their properties.</p>
<p>For both the Dirichlet and its sparse-multinomial exten sion, we need to maintain the number of times, N(s-'!tt) , state t is observed after executing action a at state s, and similarly, N ( s-'!tr) for rewards. With the prior distributions over the parameters of the MOP, these counts define a poste rior distribution over MOPs. This representation allows us to both predict the probability of the next transition and re ward, and also to compute the probability of every possible MOP and to sample from the distribution of MOPs.</p>
<p>To summarize, we assumed parameter independence, and that for each prior in (I) we have either a Dirichlet or sparse multinomial prior. The consequence is that the posterior at each stage in the learning can be represented compactly.</p>
<p>This enables us to estimate a distribution over MOPs at each stage.</p>
<p>It is easy to extend this discussion for more compact pa rameterizations of the transition and reward models. For example, if each state is described by several attributes, we might use a Bayesian network to capture the MOP dynam ics. Such a structure requires fewer parameters and thus we can learn it with fewer examples. Nonetheless, much of the above discussion and conclusions about parameter independence and Dirichlet priors apply to these models (Heckerman 1998). To formally define the approach, we need to introduce some notation. We denote by q,,a a possible value of Q*(s, a) in some MDP. We treat these quantities as ran where, again, a 1 and a 2 are the actions with the best and second best expected values respectively. Since the agent does not know in advance what value will be revealed for q; a, we need to compute the expected gain given our prior b�liefs. Hence the expected value of perfect information about q,,a is:
00 VPI(s, a)= J Gains,a(x) Pr(q,,a = x)dx (2) -oo
The computation of this integral depends on how we repre sent our distributions over q,,a. We return to this issue be low. We see that the value of exploration estimate is used as a way of boosting the desirability of different actions. When the agent is confident of the estimated Q-values, the VPI of each action is close to 0, and the agent will always choose the action with the highest expected value.</p>
<p>Estimating Q-Value Distributions</p>
<p>How do we estimate the Q-value distributions? We now ex amine several methods of different complexity and bias. </p>
<p>Naive Sampling
VPI(s, a)� -L 1 . " w ; Gain, 0(q; ) . · W ' L...J p. , ,,a I JJ i
This approach is straightforward; however, it requires an efficient sampling procedure. Here again the assumptions we made about the priors helps us. If our prior has the form of (1), then we can sample each distribution (pT(s�t) or PR(s�r)) independently of the rest. Thus, the sampling problem reduces to sampling from "simple" posterior dis tributions. For Dirichlet priors there are known sampling methods. For the sparse-multinomials the problem is a bit more complex, but solvable. In Appendix A we describe both sampling methods.</p>
<p>Importance Sampling</p>
<p>An immediate problem with the naive sampling approach is that it requires several global computations (e.g., comput</p>
<p>ing value functions for MDPs) to evaluate each action made by the agent. This is clearly too expensive. One possible way of avoiding these repeated computations is to reuse the same sampled MDPs for several steps. To do so, we can use ideas from imponance sampling.</p>
<p>In importance sampling we want to a sample from Pr(M I p/) but for some reasons, we actually sample from Pr(M I ll)· We adjust the weight of each sample appro priately to correct for the difference between the sampling distribution (e.g., Pr(M I ll)) and the target distribution (e.g., Pr(M Ill ' )) :</p>
<p>; _ Pr(M ; Ill') ; w � , -Pr(Mi Ill) W w</p>
<p>We now use the weighted sum of samples to estimate the mean and the VPI of different actions. It is easy to verify that the weighted sample leads to correct prediction when we have a large number of samples. In practice, the success of importance sampling depends on the difference between the two distributions. If an MDP M has low probability ac cording to Pr(M Ill), then the probability of sampling it is small, even ifPr(M Ill ' ) is high.</p>
<p>Fortunately for us, the differences between the beliefs before and after observing an experience tuple are usually small. We can easily show that Of course, the original set of models we sampled be comes irrelevant as we learn more about the underlying MDP. We can use the total weight of the sampled MDPs to track how unlikely they are given the observations. Ini tially this weight is k. As we learn more it usually be comes smaller. When it becomes smaller than some thresh old kmin, we sample kkmin new MDPs from our current belief state, assigning each one weight 1 and thus bringing the total weight of the sample to k again. We then need only to solve the newly sampled MDPs.</p>
<p>To summarize, we sample k MDPs, solve them, and use the k Q-values to estimate properties of the Q-value distri bution. We re-weight the samples at each step to reflect our newly gained knowledge. Finally, we have an automatic method for detecting when new samples are required.</p>
<p>Global Sampling with Repair</p>
<p>The global sampling approach of the previous section has This suggests the following algorithm. Initially, we sam ple k MDPs from our prior belief state. At each step we:</p>
<p>• Observe an experience tuple ( s, a, r, t) • Update Pr(o; ,a ) by t, and Pr(ll� ,a ) by r.</p>
<p>• For each i = 1, ... , k, sample o;· , �, II�:� from the new Pr(ll; ,a ) and Pr(ll�, a ), respectively.</p>
<p>• For each i = 1, ... , k run a local instantiation of prior itized sweeping to update the Q-value function of M ; .</p>
<p>Thus, our approach is quite similar to standard model based learning with prioritized sweeping, but instead of run ning one instantiation of prioritized sweeping, we run k in stantiations in parallel, one for each sampled MDP. The re pair to the sampled MDPs ensures that they constitute a sample from the current belief state, and the local instantia tions of prioritized sweeping ensure that the Q-values com puted in each of these MDPs is a good approximation to the true value.</p>
<p>As with the other approaches we have described, after we invoke the k prioritized sweeping instances we use the k samples from each q,,a to select the next actions using VPI computations. Figure I shows a single run of learning where the actions selected were fixed and each of the three methods was used to estimate the Q-values of a state. Initially the means and variances are very high, but as the agent gains more experi ence, the means converge on the true value of the state, and the variances tend towards zero. These results suggest that the repair and importance sampling approaches both pro vide reasonable approximations to naive global sampling.</p>
<p>Local Sampling</p>
<p>Until now we have considered using global samples of MDPs. An alternative approach is to try to maintain for each ( s, a) an estimate of the Q-value distribution, and to update these distributions using. a local, Bellman-update like, propagation rule. To understand this approach, recall the Bellman equation: q,,a = E[pR(s . !+r)] + 1 L: PT(s.!+s') m'l'xq,•,a'· a 61ES 2 Generalized prioritized sweeping (Andre, Friedman &amp; Parr 1997) allows us to extend prioritized sweeping to these approx imate settings. When using approximate models or value func tions, one must address the problem of calculating the states on which to#�timate the priority.</p>
<p>Model based Bayesian Exploration</p>
<p>155</p>
<p>In our current setting, the terms q,',a' are random vari ables that depend on our current estimate of Q-value dis tributions. The probabilities PT ( s. !+s') are also random variables that depend on our posterior on o; ,a , and finally E[pR(s. !+r)] is also a random variable that depends on the posterior on ll� ,a · Thus, we can sample from q,,a, by jointly sampling from all of these distributions, i.e., q,',a' for all states, PT(s.!+s'), and PR(s.!+r)., and then computing the Q-value. If we repeat this sampling step k times, we get k samples from a single bellman iteration for q,,a.</p>
<p>Starting with our beliefs about the model and about the Q-value distribution of all states, we can sample from the distribution of q,,a. To make this procedure manageable, we assume that we can sample from each q,',a' indepen dently. This assumption does not hold in general MDPs, since the distribution of different Q-values are correlated (by the Bellman equation). However, we might hope that the exponential decay will weaken these dependencies.</p>
<p>We are now left with the question how to use the k sam ples from q, ,a. The simplest approach is to use the sam ples as a representation of our approximation of the distri bution of q,,a. We can compute the mean and VPI from a set of samples, as we did in the global sampling ap proach. Similarly, we can re-sample from this represen tation by randomly choosing one of the points. This re sults in a method that is similar to recent sampling methods that have been used successfully in monitoring complex dy namic processes (Kanazawa, Koller &amp; Russell 1995). This gives us a method for performing a Bellman-update on our Q-value distributions. To get a good estimate of these distributions we need to repeat these updates. Here we can use a prioritized sweeping like algorithm that performs updates based on an estimate of which Q-value distribution can be most affected by the updates to other Q-value distri butions.</p>
<p>Generalization and Smoothing</p>
<p>In the approaches described above we generated samples from the Q-value distributions, and effectively used a col lection of points to represent the approximation to the Q Value distribution. A possible problem with this represen tation approach is that we use a fairly simplistic representa tion to describe a complex distribution. This suggests that we should generalize from the k samples by using standard generalization methods. This is particularly important in the local sampling ap proach. Here we also use our representation of the Q-value distribution to propagate samples for other Q-value distri butions. Experience from monitoring tasks in stochastic processes suggest that introducing generalization can dras tically improve performance (Koller &amp; Fratkina 1998).</p>
<p>Perhaps the simplest approach to generalize from the k samples is to assume that the Q-value distribution has a par ticular parametric form, and then to fit the parameters to the samples. The first approach that comes to mind is fi t ting a Gaussian to the k samples. This captures the fi rst two " " ,-----,. ._.,., ,-----o:,.,= ,r.,c=:= -&gt; Glu .. liln 1\ppr DX -�n8 es11mat10n -M·-·· ""'
" "' ""' .i 005 ! 004 ""' "' J " " " . ' i &lt; " "' L-���mw����� .. -�� · .�-7--7 .
, -� ,��-�-� "'"' Figure 2: Samples, Gaussian approximation, and Kernel estimates of a Q-value distribution after 100, 300, and 700 steps of Naive global sampling on the same run as Figure 1.</p>
<p>moments of the sample, and allows simple generalization.</p>
<p>Unfortunately, because of the max() terms in the Bellman equations, we expect the Q-value distribution to be skewed to the positive direction. If this skew is strong, then fitting a Gaussian would be a poor generalization from the sample.</p>
<p>At the other end of the spectrum are non-parametric ap proaches. One of the simplest ones is K erne/ estimation (see for example (Bishop 1995)). In this approach, we ap proximate the distribution over Q ( s, a) by a sum of Gaus sians with a fixed variance, one for each sample. This ap proach can be effective if we are careful in choosing the variance parameter. A too small variance, will lead to a spiky distribution, a too large variance, will lead to an overly smooth and flat distribution. We use a simple rule for estimating the kernel width as a function of the mean (squared) distance between points.3</p>
<p>Of course, there are many other generalization methods we might consider using here, such as mixture distributions.</p>
<p>However, these two approaches provide us with initial ideas on the effect of generalization in this context.</p>
<p>We must also compute the VPI of a set of generalized dis tributions made up of Gaussians or kernel estimates. This is simply a matter of solving the integral given in Equation 2</p>
<p>3This rule is motivated by a leave-one-out cross-validation es timate of the kernel widths. Let q1, .•. , q" be the k samples. We want to find the kernel width 17 that maximizes the tenn 1(172) = 2:log(2:f(q'lq ' ,172))</p>
<p>;:#i where f(q'lq1, 17) is the Gaussian pdf with mean q1 and variance 172• Using Jensen's inequality, we have that where Pr(q,,a = x ) is computed from the generalized prob ability distribution for state s and action a. This integration can be simplified to a term where the main cost is an evalua tion of the cdf of a Gaussian distribution (e.g., see (Russell &amp; Wefald 1991). This function, however, is implemented in most language libraries (e.g., using the erf() function in the C-library), and thus can be done quite efficiently.    sweeping. This is due to their more cautious approach to the trap state. Although they are uncertain about it, they know that its value is probably bad, and hence do not explore it further after a small number of visits.</p>
<p>Figure 5 compares prioritized sweeping with our Q-value estimation techniques on the larger maze domain. As the graph shows, our techniques perform better than prioritize sweeping early in the learning process. They explore more widely initially, and do a better job of avoiding the trap state once they find it. Of the three techniques, global sampling performs best, although its computational requirements are considerable -about ten times as much as sampling with repair. Importance sampling runs about twice as fast as global sampling but converges relatively late on this prob lem, and did not converge on all trials. Figure 6 shows the relative performance of the three smoothing methods, again on the larger domain. To exag gerate the effects of smoothing, only 20 samples were used to produce this graph. Kernel estimation performs very well, while no smoothing failed to find the optimal (two flag) strategy on two out of ten runs. Gaussian approxima tion was slow to settle on a policy, it continued to make ex ploratory actions after 1500 steps while all the other algo rithms had converged by then.</p>
<p>We are currently investigating the performance of the al gorithm on both more complex maze domains and random MOPS, and also the effectiveness of the local sampling ap proach we have described.</p>
<p>Discussion</p>
<p>This paper makes two main contributions. First, we show how to maintain Bayesian belief states about MOPs. We show that this can be done in a simple manner by using ideas that appear in Bayesian learning of probabilistic mod els. Second, we discuss how to use the Bayesian belief state to choose actions in a way that balances exploration and ex ploitation. We adapt the value of information approach of Dearden et a!. (1998) to this model-based setup and show how to approximate the Q-value distributions needed for making these choices.</p>
<p>A recent approach to exploration that is related to our work is that of Kearns and Singh ( 1998). Their approach divides the set of states in to two groups. The known states are ones for which the learner is quite confident about the transition probabilities. That is, the learner believes that its estimate of the transition probabilities is close enough to the true distribution. All other states are considered un known states. In Kearns and Singh's proposal, the learner constructs a policy over the known states. This policy takes into account both exploitation and the possibility of find ing better rewards in unknown states (which are considered as highly-rewarding). When it finds itself in an unknown state, the agent chooses actions randomly. The algorithm proceeds in phases, after each one it reclassifies the states and recomputes the policy on the known states. Kearns and Singh's proposal is significant in that it is the first one for which we have polynomial guarantees on number of steps needed to get to a good policy. However, this algorithm was not implemented or tested, and it is not clear how fast it learns in real domains.</p>
<p>Our exploration strategy also keeps a record of how con fident we are in each state (i.e., Bayesian posterior), and also chooses actions based on their expected rewards (both known rewards, and possible exploration rewards). The main difference is that we do not commit to a binary classi fication of states, but instead choose actions in a way that takes into account the possible value of doing the explo ration. This leads to exploitation, even before we are ex tremely confident in the dynamics at every state in the "in teresting" parts of the domain.</p>
<p>There are several directions for future research. First, we are currently conduc;ing experiments on larger domains to show how our method scales up. We are also interested in applying it to more compact model representations (e.g., us ing dynamic Bayesian networks), and to problems with con tinuous state spaces.</p>
<p>Finally, the most challenging future direction is to deal with the actual value of information of an action rather than myopic estimates. This problem can stated as an MDP over belief states. However, this MDP is extremely large, and requires some approximations to find good policies quickly. Some of the ideas we introduced here, such as the re-weighting of sampled MDPs might allow us to address this computational task. a DOD National Defense Science and Engineering Grant.</p>
<p>A Dirichlet and Sparse-Multinomial Priors</p>
<p>Let X be a random variable that can take L possible values from a set E. Without loss of generality, let E = { I , ... L}.</p>
<p>We are given a training set D that contains the outcomes of N independent draws x 1 , ... , xN of X from an unknown multinomial distribution P*. The multinomial estimation problem is to find a good approximation for p•.</p>
<p>This problem can be stated as the problem of predicting the outcome xN+I given x 1 , ... , xN. Given a prior dis tribution over the possible multinomial distributions, the Bayesian estimate is: = J P(xN+I I IJ , �)P( IJ I x l ' ... ' x N ' �) diJ (3) where IJ = (IJ1, ... , IJL) is a vector that describes possible values of the (unknown) probabilities P<em>(l), ... , P</em>(L), and � is the "context" variable that denote all other assump tions about the domain.</p>
<p>The posterior probability of IJ can be rewritten as: P(!Jix 1 , ... ,xN,�) oc P(x 1 , ... ,xN IIJ,�)P(IJI&lt;)
P( IJ 1 �) II o{"•,(4)
where N; is the number of occurrences of the symbol i in the training data.</p>
<p>Dirichlet distributions are a parametric family that is conjugate to the multinomial distribution. That is, if the prior distribution is from this family, so is the posterior.</p>
<p>A Dirichlet prior for X is specified by hyper-parameters aq , .. . , aL, and has the form: P( IJ I 0 ()( II or·-I (2:8; =I and 8; � 0 for all i) i where the proportion depends on a normalizing constant that ensures that this is a legal density function (i.e., inte gral of P(O 1 �) over all parameter values is 1). Given a Dirichlet prior, the initial prediction for each value of X is It is easy to see that, if the prior is a Dirichlet prior with hyper-parameters a1, ... , aL, then the posterioris aDirich let with hyper-parameters a 1 + N 1 , ... , aL + N L. Thus, we get that the prediction for X N + 1 is ( N + l · I 1 N ) a;+ N; PX =t x, ... ,x .� ="' ( . N · ) · L.. j a J + J In some situations we would like to sample a vector IJ ac cording to the distribution P(B I �). This can be done us ing a simple procedure: Sample values y 1 , ... , Y L such that each y; � Gamma( a;, I) and then normalize to get a prob ability distribution, where Gamma( a, {3) is the Gamma dis tribution. Procedures for sampling from these distributions can be found in (Ripley 1987). Friedman and Singer (1999) introduce a structured prior that captures our uncertainty about the set of"feasible" val ues of X. Defi ne a random variable V that takes values from the set 2E of possible subsets of E. The intended se mantics for this variable, is that if we know the value of V, then B; &gt; 0 iff i E V.</p>
<p>Clearly, the hypothesis V = E' (for E' &lt;:;; E) is consis tent with training data only if E' contains all the indices i for which N; &gt; 0. We denote by E0 the set of observed sym bols. That is, E0 = {i: N; &gt; 0}, and we let k0 = IE01. Suppose we know the value of V. Given this assumption, we can define a Dirichlet prior over possible multinomial distributions (} if we use the same hyper-parameter a for each symbol in V. Formally, we define the prior: P(!JIV) ex II !Jf-1 (L= IJ ; = 1 and IJ; = 0 for all i � V) iEV Using Eq. (4), we have that: P(XN+ 1 = i 1 x 1 , ... ,x", V) = { b r* N jy (5) ifi E V otherwise (6) Now consider the case where we are uncertain about the actual set of feasible outcomes. We construct a two tiered prior over the values of V. We start with a prior over the size of V, and assume that all sets of the same cardinality have the same prior probability. We let the random variable S denote the cardinality of V. We assume that we are given a distribution P(S = k) fork = 1, ... , L. We defi ne the prior over sets to be P(V I S = k) = (�) -1 . This prior is a sparse-multinomial with parameters a and Pr(S = k).</p>
<p>Friedman and Singer show that how we can efficiently predict using this prior.  and r(x) = J000 tx-1 c1dt is the gamma function. Thus, "L k"a+ N C(D, L) = L.. k::k• ka+ N mk</p>
<p>Lk'�k· mk</p>
<p>We can think of C(D, L) as scaling factor that we apply to the Dirichlet prediction that assumes that we have seen all of the feasible symbols. The quantity 1 -C( D, L) is the probability mass assigned to novel (i.e., unseen) outcomes.</p>
<p>In some of the methods discussed above we need to sam ple a parameter vector from a sparse-multinomial prior. Probable parameter vectors according to such a prior are sparse, i.e., contain few non-zero entries. The choice of the non-zero entries among the outcomes that were not ob served is done with uniform probability. This presents a complication since each sample will depend on some unob served states. To "smooth" this behaviour we sample from the distributionover V" combined with the novel event. We sample a value of k from P(S = kiD). We then, sam ple from the Dirichlet distribution of dimension k where the fi rst k0 elements are assigned hyper-parameter a+ N; , and the rest are assigned hyper-parameter a. The sampled vec tor of probabilities describes the probability of outcomes in vo and additional k -k" events. We combine these latter probabilities to be the probability of the novel event.</p>
<p>et a!. propose a measure that bal ances the expected gains in performance from exploration -in the form of improved policies -with the expected cost of doing a potentially suboptimal action. This mea sure is computed from probability distributions over the Q values of actions.</p>
<p>These functions satisfy the Bellman equations: where V<em>(s) = maxQ</em>(s, a), a E.A Q<em>(s, a)= E ( • ) [ris, a]+ 1" PT(s.i:ts')V</em>(s').</p>
<p>.l...J</p>
<p>Thus, the Bayesian approach starts with some prior prob ability distribution over all possible MDPs (we assume that the sets of possible states, actions and rewards are delim ited in advance). As we gain experience, the approach fo cuses the mass of the posterior distribution on those MOPs in which the observed experience tuples are most probable.An immediate question is whether we can represent these prior and posterior distributions over an infinite number of MOPs. We show that this is possible by adopting re sults from Bayesian learning of probabilistic models, such as Bayesian networks(Heckerman 1998). Under carefully chosen assumptions, we can represent such priors and pos teriors in any of several compact manners. We discuss one such choice below.To formally represent our problem, we consider the pa rameterization of MOPs. The simplest parameterization istablebas�, where there are parameters e;,a,t and e:,a,r for the transitiOn and reward models. Thus, for each choice of s and a, the parameters e; ,a = { e; ,a,t : t E S} de fine a distribution over possible states, and the parameters e:,a = { e:,a,r : r E 'R.} define a distribution over possible rewards.1 We say that our prior satisfies parameter independence if it has the product form: Pr(O I fJ) = II II Pr(e;,a I fJ) Pr(O�.a I fJ). (I) a</p>
<p>Standard model-based learning methods maintain a point estimate of the model. These point estimates are often close to the mean prediction of the Bayesian method. However, these point estimates do not capture the uncertainty about the model. In this paper, we examine how knowledge of this uncertainty can be exploited to improve exploration. In a recent paper, Dearden et al. (1998) examined model free Bayesian reinforcement learning. Their approach builds on the notion of Q-value distributions. Recall, that Q* ( s, a) is the expected reward if we execute a at s and then continue with optimal selection of actions. Since dur ing learning we are uncertain about the model, there is a dis tribution over the Q-values at each pair (s, a). This distri bution is induced by the belief state over possible MDPs, and the Q-values for each of these MDPs. In the model free case, Dearden et al. propose an approach for estimat ing Q-value distributions without building a model. This approach makes several strong assumptions that are clearly violated in MDPs. In the next section, we show how we can use our representation of the posterior over models to give estimates of Q-value distributions. Before we do that, we briefly review how Dearden et al. use the Q-value distri butions for selecting actions, as we use this method in the current work. The approach of Dearden et al is based on the decision theoretic ideas of value ofinformation (Howard !966). The application of these ideas in this context is reminiscent of its use in tree search (Russell &amp; Wefald 1991 ), which can also be seen as a form of exploration. The idea is to balance the expected gains from exploration-in the form of improved policies-against the expected cost of doing a potentially suboptimal action.</p>
<p>The value of perfect information gives an upper bound on the myopic value of information for exploring action a. The expected cost incurred for this exploration is given by the difference between the value of a and the value of the cur rent best action, i.e., maxa• E[q,,a•] -E[q,,a]· This sug gests we choose the action that maximizes VPI(s,a)-(m'!-xE[q,,a•]-E [ q,,a]). a Clearly, this strategy is equivalent to choosing the action that maximizes: E[q,,a] + VPI(s, a).</p>
<p>Figure 1 :
1Proposition 5.1: W � o(s,a,r,t) Pr(M Ill o (s,a,r,t)) w Pr(M Ill) � Pr( (s, a, r, t) I M) Pr((s, a, r, t) Ill) Mean and variance of the Q-value distribution for a state, plotted as a function of time. Note that the means of each method converge to the true value of the state at the same time that the variances approach zero. The term Pr( (s, a, r, t) I M) is easily computed from M, and Pr( (s, a, r, t) Ill) can be easily computed based on our posteriors. Thus, we can easily re-weight the sampled mod els after each experience is recorded and use the weighted sum for choosing actions. Note that re-weighting of models is fast, and since we have already computed the Q-value for each pair (s, a) in each of the models, no additional compu tations are needed.</p>
<p>one serious deficiency. It involves computing global solu tions tc. MDPs which can be very expensive. Although we can reuse MDPs from previous steps, this approach still re quires us to sample new MDPs and solve them quite often. An alternative idea is to keep updating each of the sam pled MDPs. Recall that after observing an experience tuple (s, a, r, t), we only change the posterior over o; and gr . .,,a 6,a Thus, instead of re-weighting the sample M ; , we can up date, or repair, it by re-sampling o; , . and��� .•. If the orig-ina) sample M ; was sampled from Pr(M I J.l ) , then it eas ily follows that the repaired M i is sampled from Pr(M I J.l o (s, a, r , t)). Of course, once we modify M ; its Q-value function changes. However, all of these changes are consequences of the new values of the dynamics at ( s, a) . Thus, we can use prioritized sweeping to update the Q-value computed for M ; . This sweeping performs several Bellman updates to correct the values of states that are affected by the change in the model. 2</p>
<p>mnximizes I;, I:H'i log f(q ' I &lt;I, 172) is t d . where dis the average distance among samples: d = k(k � 1) 2: :E (q '-q ' ) 2 ' J:¢1</p>
<p>Figure 2 Figure 3
23shows the effects of Gaussian approximation and kernel estimation smoothing (using the computed ker nel width) on the sample values used to generate the Q distributions in Figure 1 for three different time steps. Early in the run Gaussian approximation produces a very poor ap proximation because the samples are quite widely spread and very skewed, while kernel estimation provides a much better approximation to the observed distribution. For this reason, we expect kernel estimation to perform better thanGaussian approximation for computing VPI. shows two domains of the type on which we have tested our algorithms. Each is a four action maze domain in which the agent begins at the point marked S and must collect the flag F and deliver it to the goal G. The agent re ceives a reward of 1 for each flag it collects and then moves to the goal state, and the problem is then reset. If the agent enters the square marked T (a trap) it receives a reward of -10. Each action(up, down, left, right)  succeeds with prob ability 0.9 if that direction is clear, and with probabilityO. l, moves the agent perpendicular to the desired direction. The "trap" domain has 18 states, the "maze" domain 56.</p>
<p>We evaluate the algorithms by computing the average (over 10 runs) future discounted reward received by the agent. We use this measure rather than the value of the learned policy because exploratory agents rarely actually follow either the greedy policy they have discovered or their current exploration policy for very long. For comparison we use prioritized sweeping (Moore &amp; Atkeson 1993) with the T bored parameter optimized for each problem.</p>
<p>Figure 4
4shows the performance of a representative sam ple of our algorithms on the trap domain. Unless they are based on a very small number of samples, all of the Bayesian exploration methods outperform prioritized</p>
<p>Figure 3 :Figure 4 :Figure 5 :Figure 6 :
3456The (a.) "trap" and (b.) larger maze domains. , Kerne18SIIrM!Ion smooth ing, Olrk:hlel pr10fS ····--· -1� aampDng, No amooth lng, Spll rM nUtlnomill prion ·········· _25 epair safT1'1 GaU111 1111 app roXimation smooth ln , Olrlchlel priori ----Discounted future reward received for the "trap" domain.·10 "12 o '------,:=coo:----�1 000 ----, , .=oo :------c ," 'ooo· Number of tteps Comparison of Q-value estimation techniques on the larger maze domain. 500 P!lori\Hd SwMpii'IQ Kernel Estimation · · ··----· No amoot hlng ... Gauss ian �oxln.. tiOI'I --The effects of smoothing techniques on perfor mance in the large maze domain.</p>
<p>Theorem A.l: (Friedman &amp; Singer 1999) Given a sparse multinomial prior, the probability of the.next symbol is P(xN+ 1 = i I D)= { k�t�JvC(D, L) D,L) = L ka+N P(k I D).</p>
<p>= P(S = k) (k-k?)! r(ka + N)</p>
<p>Given a model, we can compute Q<em> using a va riety of methods, including value iteration. In this method we repeatedly update an estimate Q of Q</em> by applying the Bellman equations to get new values of Q(s) for some (or all) of the states.Reinforcement learning procedures attempt to achieve an optimal policy when the agent does not know PT and PR·Model based Bayesian Exploration </p>
<p>151 </p>
<p>Q* (s, a) . Since we do not know the dynamics of the underlying MOP, </p>
<p>we cannot compute the Q-value function directly. However, </p>
<p>we can estimate it. In model-free approaches one usually es </p>
<p>timates Q by treating each step in the environment as a sam </p>
<p>ple from the underlying dynamics. These samples are then </p>
<p>used for performing updates of the Q-values based on the </p>
<p>Bellman equations. In model-based reinforcement learning </p>
<p>one usually directly estimates PT (s.i:tt) and PR(sl:tr). The </p>
<p>standard approach is then to act as though these approxima </p>
<p>tions are correct, compute Q*, and use it to choose actions. </p>
<p>A standard problem in learning is balancing between </p>
<p>planning (i.e., choosing a policy) and execution. Ideally, </p>
<p>the agent would compute the optimal value function for </p>
<p>its model of the environment each time it updates it. This </p>
<p>scheme is unrealistic since finding the optimal policy for </p>
<p>a given model is a non-trivial computational task. Fortu </p>
<p>nately, we can approximate this scheme if we notice thatthe </p>
<p>approximate model changes only slightly at each step. We </p>
<p>can hope that the value function from the previous model </p>
<p>can be easily "repaired" to reflect these changes. This ap </p>
<p>proach was pursued in the DYNA (Sutton 1990) frame </p>
<p>work, where after the execution of an action, the agent </p>
<p>updates its model of the environment, and then performs </p>
<p>some bounded number of value propagation steps to up </p>
<p>date its approximation of the value function. Each value </p>
<p>propagation step locally enforces the Bellman-equation by </p>
<p>setting V(s) +--maXae.A Q(s, a), where Q(s, a) 
E[PR(s.i:tr)] + 1 Ls'eS fiT(s.i:ts')V(s'), fiT(s.i:ts ' ) and 
P R(s.:;.r) are the agent's approximate model, and Vis the </p>
<p>agent's approximation of the value function. </p>
<p>This raises the question of which states should be up </p>
<p>dated. Prioritized Sweeping (Moore &amp; Atkeson 1993) is a </p>
<p>method that estimates to what extent states would change </p>
<p>their value as a consequence of new knowledge of the MDP </p>
<p>dynamics or previous value propagations. States are as </p>
<p>signed priorities based on the expected size of changes in </p>
<p>their values, and states with the highest priority are the ones </p>
<p>for which we perform value propagation. </p>
<p>dom variables that depend on our belief state. (For clarifica tion of the following discussion, we do not explicitly refer ence the belief state in the mathematical notation.) We now consider what can be gained by learning the true value q;,a of q,,a. How would this knowledge change the agent's fu ture rewards? Clearly, if this knowledge does not change the agent's policy, then future rewards would not change.Thus, the only interesting scenarios are those where the new knowledge does change the agent's policy. This can happen in two cases: (a) when the new knowledge shows that an action previously considered sub-optimal is revealed as the best choice (given the agent's beliefs about other actions), and (b) when the new knowledge indicates that an action that was previously considered best is actually inferior to other actions.For case (a), suppose that a1 is the best action; that is, E[q,,a,J ?: E[q,,a•] for all other actions a'. Moreover sup pose that the new knowledge indicates that a is a better ac tion; that is, q;,a &gt; E[q,,aJ Thus, we expect the agent to gain q;,a-E[q,,a,] by virtue of performing a instead of a*. case (b), suppose that a 1 is the action with the highest expected value and a 2 is the second-best action. If the new knowledge indicates that q,,a, &lt; E[q,,a,]. then the agent should perform a 2 instead of a1 and we expect it to gain E[q, , a,]q;,a,· Combining these arguments, the gain from learning the value of q;,a of q,,a is: if a= a 1 and q; ,a &lt; E[q,,a,] if a =f. a1 and q;, a &gt; E[q,,a,]    For Model based Bayesian Exploration </p>
<p>1 53 </p>
<p>otherwise </p>
<p>Initially these weights are all equal to I. I"V � L... ..t wp.q,, a · L...J i W JJ i Similarly, we can estimate the VPI by summing over the kPerhaps the simplest approach is to simulate the definition </p>
<p>of a Q-value distribution. Since there are an infinite number </p>
<p>of possible MDPs, we cannot afford to compute Q-values </p>
<p>for each. Instead, we sample k MDPs: M1, ... , Mk from </p>
<p>the distribution Pr(M I p.). We can solve each MOP us </p>
<p>ing standard techniques (e.g., value iteration or linear pro </p>
<p>gramming). For each state s and action a, we then have a </p>
<p>I 
I t 
. </p>
<p>t 
k </p>
<p>h 
; 
. h 
. 
IQ </p>
<p>samp e so u ton q ,,a , ... , q ,,a , w ere q,,a IS t e optima -</p>
<p>value, Q*(s, a), given the i'th MDP. From this sample we </p>
<p>can estimate properties of the Q-distribution. For general </p>
<p>if)', we denote the weight of each sample, given p., as w�. </p>
<p>Given these samples, we can estimate the mean Q-value </p>
<p>as </p>
<p>E[ ] � </p>
<p>1 </p>
<p>" ; ; 
q,,a MDPs: </p>
<p>AcknowledgementsWe are grateful for useful comments from Craig Boutilier and Stuart Russell. Richard Dearden was supported by a Killam Predoctoral fellowship and by IRIS Phase-III project "Dealing with Actions" (BAC). Some of this work was done while Nir Friedman was at U.C. Berkeley. Nir Friedman and David Andre were supported in part by ARO under the MURI program "Integrated Approach to Intelligent Systems", grant number DAAH04-96-l-0341, and by ONR under grant number N00014-97-1-0941. Nir Friedman was also supported through the generosity of the Michael Sacher Trust. David Andre was also supported by
Generalized priori tized sweeping. D Andre, N Friedman, R Parr, Advances in Neural information Process ing Systems·. 0Andre, D., Friedman, N. &amp; Parr, R. (1997), Generalized priori tized sweeping, in 'Advances in Neural information Process ing Systems·, Vol. I 0.</p>
<p>C M Bishop, Neural Networks for Pattern Recognition. OxfordOxford University PressBishop, C. M. (1995), Neural Networks for Pattern Recognition, Oxford University Press, Oxford.</p>
<p>Bayesian Q leaming. R Dearden, N Friedman, S Russell, Proceedings of the Fifteenth National Confer ence on Artificial intelligence (AAAI-98. the Fifteenth National Confer ence on Artificial intelligence (AAAI-98Dearden, R., Friedman, N. &amp; Russell, S. (1998), Bayesian Q leaming, in 'Proceedings of the Fifteenth National Confer ence on Artificial intelligence (AAAI-98)'.</p>
<p>M H Degroot, Proability and Statistics. Reading, MassAddison-Wesley2nd ednDegroot, M. H. (1986), Proability and Statistics, 2nd edn, Addison-Wesley, Reading, Mass.</p>
<p>Efficient bayesian parameter estimation in large discrete domains. N Friedman, Y Singer, Advances in Neu ral Information Processing Systems II. Cam bridge, MassMIT PressFriedman, N. &amp; Singer, Y. (1999), Efficient bayesian parameter estimation in large discrete domains, in 'Advances in Neu ral Information Processing Systems II', MIT Press, Cam bridge, Mass.</p>
<p>Learning in Graphical Models. D Heckerman, A tutorial on learning with Bayesian net works. M. I. JordanKluwer, Dordrecht, NetherlandsHeckerman, D. (1998), A tutorial on learning with Bayesian net works, in M. I. Jordan, ed., 'Learning in Graphical Models', Kluwer, Dordrecht, Netherlands.</p>
<p>Information value theory. R A Howard, IEEE Transac tions on Systems Science and Cybernetics SSC-2. Howard, R. A. ( 1966), 'Information value theory', IEEE Transac tions on Systems Science and Cybernetics SSC-2, 22-26.</p>
<p>Rein forcement learning: A survey. L P Kaelbling, M L Littman, A W Moore, Journal of Artificial intelli gence Research. 4Kaelbling, L. P., Littman, M. L. &amp; Moore, A. W. (1996), 'Rein forcement learning: A survey', Journal of Artificial intelli gence Research 4, 237-285.</p>
<p>Stochastic simula tion algorithms for dynamic probabilistic networks. K Kanazawa, D Koller, S Russell, Pro ceedings of the Eleventh Conference on Uncertainty in Arti ficial Intelligence (UAI-95. Morgan Kaufmann, MontrealKanazawa, K., Koller, D. &amp; Russell, S. (1995), Stochastic simula tion algorithms for dynamic probabilistic networks, in 'Pro ceedings of the Eleventh Conference on Uncertainty in Arti ficial Intelligence (UAI-95)', Morgan Kaufmann, Montreal.</p>
<p>Near-optimal performance for re inforcement learning in polynomial time. M Keams, S Singh, Proceedings of the Fifteenth Int. Conf. on Machine Learning. the Fifteenth Int. Conf. on Machine LearningMorgan KaufmannKeams, M. &amp; Singh, S. (1998), Near-optimal performance for re inforcement learning in polynomial time, in 'Proceedings of the Fifteenth Int. Conf. on Machine Learning', Morgan Kaufmann.</p>
<p>Using learning for approxima tion in stochastic processes. D Koller, R Fratkina, Proceedings of the Fifteenth International Conference on Machine Learning. the Fifteenth International Conference on Machine LearningSan Francisco, CalifMorgan KaufmannKoller, D. &amp; Fratkina, R. (1998), Using learning for approxima tion in stochastic processes, in 'Proceedings of the Fifteenth International Conference on Machine Learning', Morgan Kaufmann, San Francisco, Calif.</p>
<p>Prioritized sweeping reinforcement learning with less data and less time. A W Moore, C G Atkeson, Ma chine Learning. 13Moore, A. W. &amp; Atkeson, C. G. (1993), 'Prioritized sweeping reinforcement learning with less data and less time', Ma chine Learning 13, I 03-130.</p>
<p>B D Ripley, Stochastic Simulation. Wiley, NYRipley, B. D. (1987), Stochastic Simulation, Wiley, NY.</p>
<p>S J Russell, E H Wefald, Do the Right Thing: Studies in Limited Rationality. Cambridge, MassMIT PressRussell, S. J. &amp; Wefald, E. H. (1991), Do the Right Thing: Studies in Limited Rationality, MIT Press, Cambridge, Mass.</p>
<p>Integrated architectures for learning, plan ning, and reacting based on approximating dynamic pro gramming. R S Sutton, Proceedings of the Seventh Int. Conf. on Ma chine Learning. the Seventh Int. Conf. on Ma chine LearningMorgan KaufmannSutton, R. S. (1990), Integrated architectures for learning, plan ning, and reacting based on approximating dynamic pro gramming, in 'Proceedings of the Seventh Int. Conf. on Ma chine Learning', Morgan Kaufmann, pp. 216-224.</p>            </div>
        </div>

    </div>
</body>
</html>
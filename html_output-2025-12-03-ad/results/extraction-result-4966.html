<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4966 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4966</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4966</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-3c585441b4607b34f8bf4e352ed6e36753fe21ce</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3c585441b4607b34f8bf4e352ed6e36753fe21ce" target="_blank">Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Here it is shown that models which anecdotally have good reasoning performance over real-world tasks, have quantifiable lower gaps, motivating the open problem of building"gap 0"models.</p>
                <p><strong>Paper Abstract:</strong> We propose a framework for robust evaluation of reasoning capabilities of language models, using functional variants of benchmarks. Models that solve a reasoning test should exhibit no difference in performance over the static version of a problem compared to a snapshot of the functional variant. We have rewritten the relevant fragment of the MATH benchmark into its functional variant MATH(), with functionalization of other benchmarks to follow. When evaluating current state-of-the-art models over snapshots of MATH(), we find a reasoning gap -- the percentage difference between the static and functional accuracies. We find reasoning gaps from 58.35% to 80.31% among the state-of-the-art closed and open weights models that perform well on static benchmarks, with the caveat that the gaps are likely to be smaller with more sophisticated prompting strategies. Here we show that models which anecdotally have good reasoning performance over real-world tasks, have quantifiable lower gaps, motivating the open problem of building"gap 0"models. Code for evaluation and new evaluation datasets, three MATH() snapshots, are publicly available at https://github.com/consequentai/fneval/.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4966.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4966.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MATH()</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Functionalized MATH benchmark (MATH())</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A functional variant of the MATH benchmark where each static question is rewritten into deterministic generator+problem+solution functions; snapshots are instantiated by seeding the generators to create new problems that require the same reasoning but different surface data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MATH() (evaluation framework)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a language model — a benchmarking methodology that converts static QA items into three functions (problem(inputs), solution(inputs), inputs(rngs,seed)) to generate infinite 'snapshots' of tests that exercise the same reasoning procedure but with new data.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Functionalized MATH (MATH()) snapshots</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mathematical problems from the original MATH dataset rewritten as parameterized instances; requires symbolic and stepwise mathematical reasoning (arithmetic, algebra, calculus-level problems depending on item). Snapshots instantiate new numeric/ symbolic values but preserve the underlying reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Manual functionalization of static QA into generator/problem/solution functions; create dated snapshots (e.g., Oct/Nov/Dec-2023) by seeding generators, evaluate models on static + k snapshots, define 'reasoning gap' = percent decrease from static accuracy to functional (static+snapshots) accuracy; simple few-shot prompting and pass@1 used in current evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>The paper functionalized 41.2% (2060/5000) of MATH and released a Q1'24 snapshot (Oct/Nov/Dec-2023). No performance metric applies to the method itself; the method was used to report per-model static vs functional accuracies (see model entries).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Functionalization is manual and time-consuming; some problems are ungeneralizable/marked static; functionalization code is not publicly released (only snapshots); evaluation harness has output-matching limitations; large numeric instantiations may be unreasonable for models; functionalization might be too simple in rare cases enabling pattern matches.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Proposed as an improvement over static QA benchmarking to reduce contamination and memorization; compared in the paper against standard static MATH evaluation and shown to reveal large 'reasoning gaps' in SOTA models.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Snapshot ablation: 1,2,3-snapshot experiments show gap stabilizes with 3 snapshots (authors report three snapshots suffice to stabilize the reasoning gap). Coverage reported: 41.2% functionalized; plan to reach 100% in future work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4966.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4966.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-weight, API-accessed large language model from OpenAI evaluated on static MATH and functionalized MATH snapshots to quantify memorization vs reasoning using the reasoning gap metric.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source large language model accessed via OpenAI API; treated as a top-performing SOTA reasoning model in the paper's evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>MATH and MATH() (functionalized MATH snapshots)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mathematical problem solving across difficulty levels (prealgebra through graduate-level problems) requiring symbolic derivation and multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated with a simple few-shot prompting strategy and pass@1 on static MATH and on three MATH() snapshots (Oct/Nov/Dec-2023). Models were judged correct if outputs matched expected answers under the evaluation harness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Static MATH: 1299/5000 = 25.98% accuracy; Functional accuracy over 3-snapshots + static variant: 541/5000 = 10.82%; Reasoning gap = 58.35% (lowest gap among tested models). Of the 541 functional-correct items, 302 were genuinely solved across snapshots (6.04% of full test set) while 239 were marked ungeneralizable/static (4.78% of functional accuracy attributable to static items).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Large drop from static to functional indicates many static successes likely due to memorization/contamination; performance varies strongly with difficulty and subject, and many graduate-level functional instances are unsolved. Authors note that more sophisticated prompting (CoT/ToT/CoC), retrieval, or tool use may reduce gap but were not applied in this baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Lowest reasoning gap among evaluated models (58.35%). Performs better than OSS models and other closed/API models across difficulty levels and subjects; nonetheless still far from 'gap-0'.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Detailed per-problem analysis: GPT-4 solved 302 genuinely generalized problems across snapshots; authors categorized those by difficulty, subjective surprise, and 14 solution subtypes. Snapshot ablation shows results stabilize at 3 snapshots.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4966.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4966.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-weight OpenAI model accessed via API and evaluated on MATH and functionalized MATH to measure reasoning gap alongside other SOTA models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source, API-accessed GPT family model (predecessor to GPT-4) included in the paper's evaluation set of closed models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>MATH and MATH() snapshots</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mathematical reasoning tasks (same as above).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated with simple few-shot prompting and pass@1 on static and functional snapshots to compute reasoning gap.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Exact per-model numeric results not listed in text; the paper reports an overall reasoning gap range for SOTA models of 58.35%–80.31% (GPT-4 at lower end). Individual model gaps plotted in paper figures but not enumerated in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Susceptible to the same limitations as other closed models: drop in accuracy on functionalized snapshots, possible tool usage unknown, and sensitivity to prompting strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Compared as part of the closed-model cohort (OpenAI/Anthropic/Mistral). GPT-3.5 performs worse than GPT-4; specific numeric comparisons are in the paper's figures.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>No per-model ablation beyond snapshot-count stabilization reported for individual closed models in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4966.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4966.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude 2.1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 2.1 (Anthropic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Anthropic's Claude 2.1 closed-weight model accessed via API and evaluated on static and functionalized MATH to measure its reasoning gap.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude 2.1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source model from Anthropic; the paper notes some models (including Claude 2.1) tend to default to emitting chain-of-thought which can interfere with the evaluation harness.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>MATH and functionalized MATH snapshots</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mathematical problems requiring stepwise reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated via API with simple prompting and pass@1 across static and snapshot instantiations to compute functional accuracy and reasoning gap.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Exact per-model numbers are not provided in the main text; paper reports the aggregate SOTA gap range of 58.35%–80.31%. Figures include per-model gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Authors note Anthropic Claude 2.1 tends to output chain-of-thought by default, which can disrupt the few-shot formatting/answer-extraction used by the evaluation harness; tool usage in closed APIs is unknown and may affect outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Included in closed-model cohort; compared qualitatively in the paper's figures. GPT-4 had the lowest gap among the tested models.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>No specific ablations for Claude 2.1 beyond aggregate analyses; noted behavioral differences (default CoT verbosity) that affect evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4966.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4966.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral variants</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral Mixtral Medium and Mistral 7x8B MoE (incl. Instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mistral family models (Mixtral Medium and two 7x8B MoE variants) evaluated as part of closed/API SOTA models; included to sample diversity of architectures and training recipes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral Mixtral Medium, Mistral 7x8B MoE, Mistral 7x8B MoE Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Commercial/closed models from Mistral (paper lists mixtral medium and 7x8B MoE variants); architecture details are not expanded in the paper beyond model names.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>MATH and MATH() snapshots</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mathematical problem solving requiring multi-step reasoning, symbolic manipulation and arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated on static and functional MATH snapshots using simple few-shot prompts and pass@1; reasoning gap computed per model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Per-model numeric results not listed in text; included in the quoted aggregate reasoning gap range (58.35%–80.31%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Same limitations as other closed models: susceptibility to contamination, unknown tool usage, sensitivity to default output formatting and prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Part of closed-model comparison set; no per-model ranking in text, but visualized in figures.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>No specific ablation results per Mistral variant reported in text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4966.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4966.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA 2 70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 2 70B (Meta)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-weight (OSS) large language model included in evaluations as a representative OSS baseline; assessed on static and functional MATH snapshots.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA 2 70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source/model-weights-available transformer-based LLM (70B parameters) used as an OSS baseline in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>MATH and MATH() (functionalized snapshots)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same mathematical reasoning tasks drawn from MATH; tests span prealgebra to graduate-level problems.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated as OSS model with simple few-shot prompting and pass@1 on static and functional snapshots; per-model accuracies were aggregated for OSS cohort analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>OSS cohort static accuracies range reported as 1.86%–12.28%; OSS cohort functional accuracies drop to 0.52%–4.34%. The paper does not give per-model numeric values for LLaMA 2 70B in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Low absolute accuracies on functional snapshots; OSS models mainly solve easier subjects (prealgebra, algebra, intermediate algebra), and generally fail at more complex mathematics in functionalized forms.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>OSS models are substantially worse than top closed models (e.g., GPT-4) on both static and functional variants; only 19 problems were consistently solved across snapshots by a majority (≥5/10) of OSS models.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Aggregated OSS analysis shows drop from static to functional; no per-model ablation presented in text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4966.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4966.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WizardCoder Python 34B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WizardCoder Python 34B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source/code-specialized 34B model evaluated among OSS models on MATH and functionalized MATH snapshots.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>WizardCoder Python 34B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLM specialized/finetuned on code (34B parameters) included in the set of OSS models evaluated on functionalized MATH.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>34B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>MATH and MATH() snapshots</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mathematical problems requiring symbolic reasoning; code-specialized models might be expected to perform better on programmatic reasoning but were evaluated on math.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated with few-shot prompting and pass@1 on static and functional snapshots.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Per-model numbers not listed; OSS cohort ranges: static 1.86%–12.28%, functional 0.52%–4.34%.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Struggles on functionalized snapshots and higher-difficulty mathematical problems; performance concentrated on simpler arithmetic-style tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Part of OSS cohort that underperforms closed SOTA models like GPT-4 on the functionalized MATH benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>No specific ablation reported for WizardCoder in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4966.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4966.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yi 34B / Yi Chat 34B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Yi 34B and Yi Chat 34B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source 34B models (Yi and Yi Chat variants) included among OSS evaluated models on static and functionalized MATH snapshots.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Yi 34B / Yi Chat 34B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLMs at ~34B parameter scale included for breadth in OSS evaluations; paper does not expand architecture/training details.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>34B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>MATH and MATH()</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mathematical reasoning problems across multiple subjects and difficulty levels.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated with few-shot prompting and pass@1 over static and functional snapshots.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not individually enumerated; included in OSS cohort aggregated ranges (static 1.86%–12.28%, functional 0.52%–4.34%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Like other OSS models, mainly solve easier problems; poor generalization on functional snapshots.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Underperforms top closed models; only a small subset of problems are consistently solved across snapshots by OSS majority.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>No per-model ablation details provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4966.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4966.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StripedHypena Nous / Hessian 7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>StripedHypena Nous and Hessian 7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two 7B-scale OSS models evaluated as part of the OSS cohort on static and functionalized MATH to assess low-parameter open-weight model behaviour on strict reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>StripedHypena Nous, Hessian 7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-weight models at ~7B parameter scale included for diversity; paper does not provide further architecture/training specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>MATH and MATH() snapshots</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mathematical problems requiring multi-step reasoning; models expected to perform worse on harder problems.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated with few-shot prompting/pass@1 across static and functional snapshots.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Included within OSS cohort ranges: static 1.86%–12.28%, functional 0.52%–4.34%. No per-model numeric details in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Low absolute performance on functionalized tasks; mainly capable of simple calculations/properties-of-operators problems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Lower than large OSS and closed models; only a small set of easy problems were solved consistently by OSS majority.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>No detailed ablation for these 7B models reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4966.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4966.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen 7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen 7B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B OSS model included among evaluated OSS models on static and functionalized MATH snapshots.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen 7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-weight model (7B) included in the OSS evaluation cohort; paper lists it among evaluated models without further architecture details.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>MATH and MATH()</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mathematical multi-step reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluated with few-shot prompting and pass@1 on static and functional snapshots.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not individually enumerated; included in aggregate OSS ranges (static 1.86%–12.28%; functional 0.52%–4.34%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Poor generalization on functional snapshots; performance concentrated in easy math problems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Underperforms top closed models such as GPT-4; performance similar to other 7B OSS models in cohort.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>No specific ablation in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4966.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e4966.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OSS cohort (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open-source software (OSS) models ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregate of the open-weight models evaluated (LLaMA2-70B, WizardCoder-34B, Yi-34B/Chat, StripedHypena Nous/Hessian 7B, Qwen 7B) showing the OSS landscape's performance on functionalized MATH.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OSS models (aggregate)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Collection of open-weight LLMs of varying sizes (7B–70B) and specializations included to assess generalization on MATH().</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>MATH and functionalized MATH snapshots</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mathematical reasoning across difficulty levels; OSS models primarily succeed on simpler prealgebra/algebra items.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Same evaluation pipeline (few-shot prompts, pass@1) applied to each OSS model; aggregated statistics reported for coverage and consistency across snapshots.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Static accuracies across OSS models: 1.86%–12.28%; Functional accuracies (over snapshots) drop to 0.52%–4.34%. Only 19 problems were consistently solved across snapshots by a majority (≥5/10) of OSS models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Low absolute functional accuracy; solutions concentrated in simple calculations and operator-property problems; no medium/high-surprise problems solved consistently by OSS majority.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>OSS models perform substantially worse than the top closed models (e.g., GPT-4) in both static and functional evaluations; GPT-4 had lowest reasoning gap among all models tested.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Authors analyzed which subjects/levels OSS models solved (mainly level 1–2 prealgebra/algebra) and found only 19 problems had majority agreement across snapshots.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 2)</em></li>
                <li>Evaluating large language models trained on code <em>(Rating: 1)</em></li>
                <li>Proofnet: Autoformalizing and formally proving undergraduate-level mathematics <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4966",
    "paper_id": "paper-3c585441b4607b34f8bf4e352ed6e36753fe21ce",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "MATH()",
            "name_full": "Functionalized MATH benchmark (MATH())",
            "brief_description": "A functional variant of the MATH benchmark where each static question is rewritten into deterministic generator+problem+solution functions; snapshots are instantiated by seeding the generators to create new problems that require the same reasoning but different surface data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MATH() (evaluation framework)",
            "model_description": "Not a language model — a benchmarking methodology that converts static QA items into three functions (problem(inputs), solution(inputs), inputs(rngs,seed)) to generate infinite 'snapshots' of tests that exercise the same reasoning procedure but with new data.",
            "model_size": null,
            "logical_reasoning_task": "Functionalized MATH (MATH()) snapshots",
            "task_description": "Mathematical problems from the original MATH dataset rewritten as parameterized instances; requires symbolic and stepwise mathematical reasoning (arithmetic, algebra, calculus-level problems depending on item). Snapshots instantiate new numeric/ symbolic values but preserve the underlying reasoning steps.",
            "method_or_approach": "Manual functionalization of static QA into generator/problem/solution functions; create dated snapshots (e.g., Oct/Nov/Dec-2023) by seeding generators, evaluate models on static + k snapshots, define 'reasoning gap' = percent decrease from static accuracy to functional (static+snapshots) accuracy; simple few-shot prompting and pass@1 used in current evaluations.",
            "performance": "The paper functionalized 41.2% (2060/5000) of MATH and released a Q1'24 snapshot (Oct/Nov/Dec-2023). No performance metric applies to the method itself; the method was used to report per-model static vs functional accuracies (see model entries).",
            "limitations_or_failure_cases": "Functionalization is manual and time-consuming; some problems are ungeneralizable/marked static; functionalization code is not publicly released (only snapshots); evaluation harness has output-matching limitations; large numeric instantiations may be unreasonable for models; functionalization might be too simple in rare cases enabling pattern matches.",
            "comparison": "Proposed as an improvement over static QA benchmarking to reduce contamination and memorization; compared in the paper against standard static MATH evaluation and shown to reveal large 'reasoning gaps' in SOTA models.",
            "ablation_or_analysis_results": "Snapshot ablation: 1,2,3-snapshot experiments show gap stabilizes with 3 snapshots (authors report three snapshots suffice to stabilize the reasoning gap). Coverage reported: 41.2% functionalized; plan to reach 100% in future work.",
            "uuid": "e4966.0",
            "source_info": {
                "paper_title": "Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "Closed-weight, API-accessed large language model from OpenAI evaluated on static MATH and functionalized MATH snapshots to quantify memorization vs reasoning using the reasoning gap metric.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Closed-source large language model accessed via OpenAI API; treated as a top-performing SOTA reasoning model in the paper's evaluations.",
            "model_size": null,
            "logical_reasoning_task": "MATH and MATH() (functionalized MATH snapshots)",
            "task_description": "Mathematical problem solving across difficulty levels (prealgebra through graduate-level problems) requiring symbolic derivation and multi-step reasoning.",
            "method_or_approach": "Evaluated with a simple few-shot prompting strategy and pass@1 on static MATH and on three MATH() snapshots (Oct/Nov/Dec-2023). Models were judged correct if outputs matched expected answers under the evaluation harness.",
            "performance": "Static MATH: 1299/5000 = 25.98% accuracy; Functional accuracy over 3-snapshots + static variant: 541/5000 = 10.82%; Reasoning gap = 58.35% (lowest gap among tested models). Of the 541 functional-correct items, 302 were genuinely solved across snapshots (6.04% of full test set) while 239 were marked ungeneralizable/static (4.78% of functional accuracy attributable to static items).",
            "limitations_or_failure_cases": "Large drop from static to functional indicates many static successes likely due to memorization/contamination; performance varies strongly with difficulty and subject, and many graduate-level functional instances are unsolved. Authors note that more sophisticated prompting (CoT/ToT/CoC), retrieval, or tool use may reduce gap but were not applied in this baseline.",
            "comparison": "Lowest reasoning gap among evaluated models (58.35%). Performs better than OSS models and other closed/API models across difficulty levels and subjects; nonetheless still far from 'gap-0'.",
            "ablation_or_analysis_results": "Detailed per-problem analysis: GPT-4 solved 302 genuinely generalized problems across snapshots; authors categorized those by difficulty, subjective surprise, and 14 solution subtypes. Snapshot ablation shows results stabilize at 3 snapshots.",
            "uuid": "e4966.1",
            "source_info": {
                "paper_title": "Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5 (OpenAI)",
            "brief_description": "Closed-weight OpenAI model accessed via API and evaluated on MATH and functionalized MATH to measure reasoning gap alongside other SOTA models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "Closed-source, API-accessed GPT family model (predecessor to GPT-4) included in the paper's evaluation set of closed models.",
            "model_size": null,
            "logical_reasoning_task": "MATH and MATH() snapshots",
            "task_description": "Mathematical reasoning tasks (same as above).",
            "method_or_approach": "Evaluated with simple few-shot prompting and pass@1 on static and functional snapshots to compute reasoning gap.",
            "performance": "Exact per-model numeric results not listed in text; the paper reports an overall reasoning gap range for SOTA models of 58.35%–80.31% (GPT-4 at lower end). Individual model gaps plotted in paper figures but not enumerated in the main text.",
            "limitations_or_failure_cases": "Susceptible to the same limitations as other closed models: drop in accuracy on functionalized snapshots, possible tool usage unknown, and sensitivity to prompting strategy.",
            "comparison": "Compared as part of the closed-model cohort (OpenAI/Anthropic/Mistral). GPT-3.5 performs worse than GPT-4; specific numeric comparisons are in the paper's figures.",
            "ablation_or_analysis_results": "No per-model ablation beyond snapshot-count stabilization reported for individual closed models in main text.",
            "uuid": "e4966.2",
            "source_info": {
                "paper_title": "Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Claude 2.1",
            "name_full": "Claude 2.1 (Anthropic)",
            "brief_description": "Anthropic's Claude 2.1 closed-weight model accessed via API and evaluated on static and functionalized MATH to measure its reasoning gap.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude 2.1",
            "model_description": "Closed-source model from Anthropic; the paper notes some models (including Claude 2.1) tend to default to emitting chain-of-thought which can interfere with the evaluation harness.",
            "model_size": null,
            "logical_reasoning_task": "MATH and functionalized MATH snapshots",
            "task_description": "Mathematical problems requiring stepwise reasoning.",
            "method_or_approach": "Evaluated via API with simple prompting and pass@1 across static and snapshot instantiations to compute functional accuracy and reasoning gap.",
            "performance": "Exact per-model numbers are not provided in the main text; paper reports the aggregate SOTA gap range of 58.35%–80.31%. Figures include per-model gaps.",
            "limitations_or_failure_cases": "Authors note Anthropic Claude 2.1 tends to output chain-of-thought by default, which can disrupt the few-shot formatting/answer-extraction used by the evaluation harness; tool usage in closed APIs is unknown and may affect outcomes.",
            "comparison": "Included in closed-model cohort; compared qualitatively in the paper's figures. GPT-4 had the lowest gap among the tested models.",
            "ablation_or_analysis_results": "No specific ablations for Claude 2.1 beyond aggregate analyses; noted behavioral differences (default CoT verbosity) that affect evaluation.",
            "uuid": "e4966.3",
            "source_info": {
                "paper_title": "Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Mistral variants",
            "name_full": "Mistral Mixtral Medium and Mistral 7x8B MoE (incl. Instruct)",
            "brief_description": "Mistral family models (Mixtral Medium and two 7x8B MoE variants) evaluated as part of closed/API SOTA models; included to sample diversity of architectures and training recipes.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral Mixtral Medium, Mistral 7x8B MoE, Mistral 7x8B MoE Instruct",
            "model_description": "Commercial/closed models from Mistral (paper lists mixtral medium and 7x8B MoE variants); architecture details are not expanded in the paper beyond model names.",
            "model_size": null,
            "logical_reasoning_task": "MATH and MATH() snapshots",
            "task_description": "Mathematical problem solving requiring multi-step reasoning, symbolic manipulation and arithmetic.",
            "method_or_approach": "Evaluated on static and functional MATH snapshots using simple few-shot prompts and pass@1; reasoning gap computed per model.",
            "performance": "Per-model numeric results not listed in text; included in the quoted aggregate reasoning gap range (58.35%–80.31%).",
            "limitations_or_failure_cases": "Same limitations as other closed models: susceptibility to contamination, unknown tool usage, sensitivity to default output formatting and prompting.",
            "comparison": "Part of closed-model comparison set; no per-model ranking in text, but visualized in figures.",
            "ablation_or_analysis_results": "No specific ablation results per Mistral variant reported in text.",
            "uuid": "e4966.4",
            "source_info": {
                "paper_title": "Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLaMA 2 70B",
            "name_full": "LLaMA 2 70B (Meta)",
            "brief_description": "Open-weight (OSS) large language model included in evaluations as a representative OSS baseline; assessed on static and functional MATH snapshots.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA 2 70B",
            "model_description": "Open-source/model-weights-available transformer-based LLM (70B parameters) used as an OSS baseline in the paper's experiments.",
            "model_size": "70B",
            "logical_reasoning_task": "MATH and MATH() (functionalized snapshots)",
            "task_description": "Same mathematical reasoning tasks drawn from MATH; tests span prealgebra to graduate-level problems.",
            "method_or_approach": "Evaluated as OSS model with simple few-shot prompting and pass@1 on static and functional snapshots; per-model accuracies were aggregated for OSS cohort analyses.",
            "performance": "OSS cohort static accuracies range reported as 1.86%–12.28%; OSS cohort functional accuracies drop to 0.52%–4.34%. The paper does not give per-model numeric values for LLaMA 2 70B in the main text.",
            "limitations_or_failure_cases": "Low absolute accuracies on functional snapshots; OSS models mainly solve easier subjects (prealgebra, algebra, intermediate algebra), and generally fail at more complex mathematics in functionalized forms.",
            "comparison": "OSS models are substantially worse than top closed models (e.g., GPT-4) on both static and functional variants; only 19 problems were consistently solved across snapshots by a majority (≥5/10) of OSS models.",
            "ablation_or_analysis_results": "Aggregated OSS analysis shows drop from static to functional; no per-model ablation presented in text.",
            "uuid": "e4966.5",
            "source_info": {
                "paper_title": "Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "WizardCoder Python 34B",
            "name_full": "WizardCoder Python 34B",
            "brief_description": "An open-source/code-specialized 34B model evaluated among OSS models on MATH and functionalized MATH snapshots.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "WizardCoder Python 34B",
            "model_description": "Open-source LLM specialized/finetuned on code (34B parameters) included in the set of OSS models evaluated on functionalized MATH.",
            "model_size": "34B",
            "logical_reasoning_task": "MATH and MATH() snapshots",
            "task_description": "Mathematical problems requiring symbolic reasoning; code-specialized models might be expected to perform better on programmatic reasoning but were evaluated on math.",
            "method_or_approach": "Evaluated with few-shot prompting and pass@1 on static and functional snapshots.",
            "performance": "Per-model numbers not listed; OSS cohort ranges: static 1.86%–12.28%, functional 0.52%–4.34%.",
            "limitations_or_failure_cases": "Struggles on functionalized snapshots and higher-difficulty mathematical problems; performance concentrated on simpler arithmetic-style tasks.",
            "comparison": "Part of OSS cohort that underperforms closed SOTA models like GPT-4 on the functionalized MATH benchmark.",
            "ablation_or_analysis_results": "No specific ablation reported for WizardCoder in the paper.",
            "uuid": "e4966.6",
            "source_info": {
                "paper_title": "Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Yi 34B / Yi Chat 34B",
            "name_full": "Yi 34B and Yi Chat 34B",
            "brief_description": "Open-source 34B models (Yi and Yi Chat variants) included among OSS evaluated models on static and functionalized MATH snapshots.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Yi 34B / Yi Chat 34B",
            "model_description": "Open-source LLMs at ~34B parameter scale included for breadth in OSS evaluations; paper does not expand architecture/training details.",
            "model_size": "34B",
            "logical_reasoning_task": "MATH and MATH()",
            "task_description": "Mathematical reasoning problems across multiple subjects and difficulty levels.",
            "method_or_approach": "Evaluated with few-shot prompting and pass@1 over static and functional snapshots.",
            "performance": "Not individually enumerated; included in OSS cohort aggregated ranges (static 1.86%–12.28%, functional 0.52%–4.34%).",
            "limitations_or_failure_cases": "Like other OSS models, mainly solve easier problems; poor generalization on functional snapshots.",
            "comparison": "Underperforms top closed models; only a small subset of problems are consistently solved across snapshots by OSS majority.",
            "ablation_or_analysis_results": "No per-model ablation details provided.",
            "uuid": "e4966.7",
            "source_info": {
                "paper_title": "Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "StripedHypena Nous / Hessian 7B",
            "name_full": "StripedHypena Nous and Hessian 7B",
            "brief_description": "Two 7B-scale OSS models evaluated as part of the OSS cohort on static and functionalized MATH to assess low-parameter open-weight model behaviour on strict reasoning tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "StripedHypena Nous, Hessian 7B",
            "model_description": "Open-weight models at ~7B parameter scale included for diversity; paper does not provide further architecture/training specifics.",
            "model_size": "7B",
            "logical_reasoning_task": "MATH and MATH() snapshots",
            "task_description": "Mathematical problems requiring multi-step reasoning; models expected to perform worse on harder problems.",
            "method_or_approach": "Evaluated with few-shot prompting/pass@1 across static and functional snapshots.",
            "performance": "Included within OSS cohort ranges: static 1.86%–12.28%, functional 0.52%–4.34%. No per-model numeric details in main text.",
            "limitations_or_failure_cases": "Low absolute performance on functionalized tasks; mainly capable of simple calculations/properties-of-operators problems.",
            "comparison": "Lower than large OSS and closed models; only a small set of easy problems were solved consistently by OSS majority.",
            "ablation_or_analysis_results": "No detailed ablation for these 7B models reported.",
            "uuid": "e4966.8",
            "source_info": {
                "paper_title": "Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Qwen 7B",
            "name_full": "Qwen 7B",
            "brief_description": "A 7B OSS model included among evaluated OSS models on static and functionalized MATH snapshots.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen 7B",
            "model_description": "Open-weight model (7B) included in the OSS evaluation cohort; paper lists it among evaluated models without further architecture details.",
            "model_size": "7B",
            "logical_reasoning_task": "MATH and MATH()",
            "task_description": "Mathematical multi-step reasoning tasks.",
            "method_or_approach": "Evaluated with few-shot prompting and pass@1 on static and functional snapshots.",
            "performance": "Not individually enumerated; included in aggregate OSS ranges (static 1.86%–12.28%; functional 0.52%–4.34%).",
            "limitations_or_failure_cases": "Poor generalization on functional snapshots; performance concentrated in easy math problems.",
            "comparison": "Underperforms top closed models such as GPT-4; performance similar to other 7B OSS models in cohort.",
            "ablation_or_analysis_results": "No specific ablation in the main text.",
            "uuid": "e4966.9",
            "source_info": {
                "paper_title": "Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "OSS cohort (aggregate)",
            "name_full": "Open-source software (OSS) models ensemble",
            "brief_description": "Aggregate of the open-weight models evaluated (LLaMA2-70B, WizardCoder-34B, Yi-34B/Chat, StripedHypena Nous/Hessian 7B, Qwen 7B) showing the OSS landscape's performance on functionalized MATH.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "OSS models (aggregate)",
            "model_description": "Collection of open-weight LLMs of varying sizes (7B–70B) and specializations included to assess generalization on MATH().",
            "model_size": null,
            "logical_reasoning_task": "MATH and functionalized MATH snapshots",
            "task_description": "Mathematical reasoning across difficulty levels; OSS models primarily succeed on simpler prealgebra/algebra items.",
            "method_or_approach": "Same evaluation pipeline (few-shot prompts, pass@1) applied to each OSS model; aggregated statistics reported for coverage and consistency across snapshots.",
            "performance": "Static accuracies across OSS models: 1.86%–12.28%; Functional accuracies (over snapshots) drop to 0.52%–4.34%. Only 19 problems were consistently solved across snapshots by a majority (≥5/10) of OSS models.",
            "limitations_or_failure_cases": "Low absolute functional accuracy; solutions concentrated in simple calculations and operator-property problems; no medium/high-surprise problems solved consistently by OSS majority.",
            "comparison": "OSS models perform substantially worse than the top closed models (e.g., GPT-4) in both static and functional evaluations; GPT-4 had lowest reasoning gap among all models tested.",
            "ablation_or_analysis_results": "Authors analyzed which subjects/levels OSS models solved (mainly level 1–2 prealgebra/algebra) and found only 19 problems had majority agreement across snapshots.",
            "uuid": "e4966.10",
            "source_info": {
                "paper_title": "Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 2,
            "sanitized_title": "program_of_thoughts_prompting_disentangling_computation_from_reasoning_for_numerical_reasoning_tasks"
        },
        {
            "paper_title": "Evaluating large language models trained on code",
            "rating": 1,
            "sanitized_title": "evaluating_large_language_models_trained_on_code"
        },
        {
            "paper_title": "Proofnet: Autoformalizing and formally proving undergraduate-level mathematics",
            "rating": 2,
            "sanitized_title": "proofnet_autoformalizing_and_formally_proving_undergraduatelevel_mathematics"
        }
    ],
    "cost": 0.016746249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap</h1>
<p>Saurabh Srivastava,<br>Annarose M B, Anto P V, Shashank Menon, Ajay Sukumar, Adwaith Samod T, Alan Philipose, Stevin Prince, and Sooraj Thomas</p>
<p>Consequent AI</p>
<h2>1 Abstract</h2>
<p>We propose a framework for robust evaluation of reasoning capabilities of language models, using functional variants of benchmarks. Models that solve a reasoning test should exhibit no difference in performance over the static version of a problem compared to a snapshot of the functional variant. We have rewritten the relevant fragment of the MATH benchmark into its functional variant MATH(), with functionalization of other benchmarks to follow. When evaluating current state-of-the-art models over snapshots of MATH(), we find a reasoning gap - the percentage difference between the static and functional accuracies. We find reasoning gaps from $58.35 \%$ to $80.31 \%$ among the state-of-the-art closed and open weights models that perform well on static benchmarks, with the caveat that the gaps are likely to be smaller with more sophisticated prompting strategies. Here we show that models which anecdotally have good reasoning performance over real-world tasks, have quantifiable lower gaps, motivating the open problem of building "gap 0" models. Code for evaluation and new evaluation datasets, three MATH() snapshots, are publicly available at https://github.com/consequentai/fneval/.</p>
<h2>2 Introduction</h2>
<p>Accurately evaluating reasoning performance is critical to improving large language models (LLMs) beyond their current capabilities. The current standard for benchmarking fails to accurately measure reasoning of LLMs. An accurate test should consist of posing a question whose answer can be derived under the assumption of axioms for that domain. Reasoning systems (or humans) should be able to conclude the answer despite not having seen the question before. An ideal system would provide not only the answer but also each step of the derivation. Explicit elucidation of steps is ideal, but not critical. State-of-the-art language models exhibit some reasoning capabilities, but accurate evaluation is lagging. In particular, the capabilities of LLMs may be overestimated because they are tested using benchmarks</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>that are sufficient for language understanding (textual question, answer) but may not accurately measure reasoning.</p>
<p>When testing generalized problem-solving, the aim is to assess whether the tested system can effectively answer questions it has not encountered before. This is the opposite objective when compared to knowledge retrieval. Finding novel questions is challenging when the model has been trained on a snapshot of the internet. Even if datasets exist that are not currently on the internet, it is unreasonable to expect them to remain so, or that we will continue to develop novel datasets. Contamination-testing by checking for $k$-contiguous token sequences fails to account for various other forms in which leakage might occur, especially when the benchmark is a set of static text question-answers ("static QAs"). These may include i) accidental leakage occurring from paraphrasing or compressed representations in the training data or fragments of semantically similar statements; ii) not accounting for the provenance of data used in every stage of training or fine-tuning, which becomes increasingly difficult as more models are trained from existing high-quality starting weights; iii) sourcing synthetic training data from an existing model which could be contaminated, especially if it is closed-source and closed-data, iv) overfitting through preference tuning, which may in the extreme case cause reinforcement feedback allowing the model to evolve to discover correct static answers.</p>
<p>Checking against a static QA is only an indirect test of reasoning. Text questions and answers work well for humans as producing correct answers implies likely correct stepwise reasoning. The same does not apply to model outputs.</p>
<p>We propose an alternative approach that alleviates concerns with static QA testing, but also works seamlessly with existing evaluation harnesses. We propose rewriting each static QA's reasoning into code, whose inputs can be varied to create infinite versions that follow the same reasoning pattern, but the steps needed in each would be different. We call this the functional variant of the benchmark, and running each of those code fragments will create a snapshot that is a drop-in replacement into the existing static evaluation harness. The accuracy on these replacements, with the static variant included, might be lower than that on the static variant alone, which we quantify as the reasoning gap.</p>
<p>For domains where this can be done, employing functional variants would improve on static QA evaluation. We would convert a static benchmark Bench (e.g., MATH or GSM8K or HumanEval) into their functional variant Bench(rngs). The functional variants (denoted here as $\mathrm{QA}(r n g s)$ ) take as inputs a set of deterministic typed random generators $r n g s$ which output a pseudorandom typed value, e.g., natural int, positive or negative float, fractions, string, positive evens or odds, primes. The $r n g s$ when seeded make the instantiation deterministic, generating a snapshot of each test (denoted here as $\mathrm{QA}^{*}$ ) and of the entire benchmark.</p>
<p>Each $\mathrm{QA}^{<em>}$ text pair in this snapshot is different from the original static QA. However, answering $\mathrm{QA}^{</em>}$ requires the same reasoning used in the static QA. To have an unbiased accurate baseline, we created the functionalization manually by rewriting the reasoning of each test into code. Functionalization-a one-time timeconsuming task-results in infinite snapshot instantiations to test against.</p>
<p>Of the popular benchmarks, functionalization is easy for the subcategories of math (MATH [29] and GSM8K [20]), and code (HumanEval [12], MBPP [7], and Natural2Code [71]). The categories of commonsense reasoning (HellaSwag [89], Winogrande [59], PIQA [11], SIQA [60], OpenbookQA [50], ARC-Easy/Challenge [19],</p>
<p>CommonsenseQA [70]), world knowledge (NaturalQuestions [38], TriviaQA [36]), and reading comprehension (BoolQ [18], QuAC [16], DROP [22]) test English understanding and thus are not good candidates for functionalization. Subparts of the aggregated benchmarks (MMLU [28], HELM [43], BBH [69], and AGI Eval [94]) could be functionalized.</p>
<p>As of this work, we have functionalized $41.2 \%$ of the MATH benchmark, with the subset chosen so as to fully cover the static QA tests solved by 4 closed-weight and 9 open-weight state-of-the-art (SOTA) models. This allows us to provide complete metrics over all of MATH for these models, assuming a simple few-shot prompting strategy. We find a reasoning gap of $58.35 \%$ to $80.31 \%$ across these models. The value of the gap when using optimized prompting such as chain-of-thought (CoT [80]), tree-of-thought (ToT [85]), chain-of-code (CoC [41]), amongst others, could be lower and we will resolve that open question when we have built the $100 \%$ functionalized MATH().</p>
<p>In future work, we will present functionalized variants of GSM8K, HumanEval, and MBPP.</p>
<h1>2.1 Contributions</h1>
<p>This work presents the first steps towards reasoning evaluation with functional benchmarks:</p>
<ul>
<li>Framework for evaluation for models that have seen the text of the entire internet: We present a proposal for a long-term solution to the problem of evaluating models that are increasingly trained on all written knowledge. The proposed framework allows a benchmark to be instantiated to infinite snapshots, with a probabilistic guarantee that a new snapshot will not have existed before. For a static QA in the benchmark whose individual test captures a specific reasoning process, each snapshot QA* will follow the same process, but the test will be new. For a model to perform well on a set of snapshots, it will need to be able to perform step-by-step derivations to reach an answer, rather than simply recalling a static final answer.</li>
<li>Functional version of MATH, an important benchmark for reasoning evaluation, and its publicly available snapshots: Our framework is designed to allow the reworking of existing benchmarks on which models are already being tested without modification. We are in the process of functionalizing the entire MATH benchmark. The specific functionalization code will never be publicly accessible. Instead three snapshots will be released every quarter. The {Oct, Nov, Dec}-2023 snapshots are available at https://github.com/ consequentai/fneval/ along with the evaluation code. The repository will be kept up to date with the last quarter's snapshots.</li>
<li>
<p>Robust evaluations that are resistant to contamination: There are infinite snapshots of a functional benchmark, with the reasoning involved in each remaining identical. If a model consistently solves a problem across multiple snapshots (e.g., three snapshots) then there is a high likelihood it does so with proper reasoning, because it should be fairly difficult to get the answer right across many snapshots by chance.</p>
</li>
<li>
<p>Reasoning gap: We define the reasoning gap as the quantitative measure of the difference in accuracy when testing against static vs functional variants. The gap is a measure of reasoning vs memorization, with gap-0 being true reasoning and gap-100 being full memorization. The community should aim to build models that have the highest accuracy while maintaining a gap close to 0 . Minimizing the gap may serve as a training optimization objective that could lead to models that perform better at generalized reasoning.</p>
</li>
<li>Open problem of training gap-0 models: Here we show that even the current best models have large reasoning gaps, with the caveat that the reasoning gap is likely to be smaller with more sophisticated prompting strategies. Our current datasets, training, fine-tuning, and inference strategies might be nonoptimal for training gap-0 models. We leave it as an open problem to train gap-0 models.</li>
</ul>
<h1>3 Illustrative Examples</h1>
<p>We motivate the need for more accurate assessment of the reasoning capabilities of LLMs by examining specific cases in the MATH benchmark. Below and after, we use monotype font to indicate verbatim text from the benchmark. In particular, the benchmark problems include LaTeX formatting, which we leave as is to accurately display the model input. For all problems below, these static QAs are solved by current LLMs - i.e., the inferred answer matches the ground truth answer shown.</p>
<h2>Case 1 - Simple arithmetic</h2>
<div class="codehilite"><pre><span></span><code><span class="n">Question</span><span class="o">:</span><span class="w"> </span><span class="n">Compute</span><span class="w"> </span><span class="n">$</span><span class="o">\\</span><span class="n">left</span><span class="o">(-\\\</span><span class="n">sqrt</span><span class="o">{</span><span class="mi">5321</span><span class="o">}\\</span><span class="n">right</span><span class="o">)^</span><span class="mi">2</span><span class="n">$</span><span class="o">.</span>
<span class="n">Answer</span><span class="o">:</span><span class="w"> </span><span class="mi">5321</span>
</code></pre></div>

<p>Question: Round 15.49999999 to the nearest whole number.
Answer: 15
Solving problems such as these is in line with known capabilities of current models. We would expect such problems to be solvable in their functionalized form as well. We find this to be the case.</p>
<h2>Case 2 - Non-trivial arithmetic or use of numerical properties</h2>
<div class="codehilite"><pre><span></span><code>Question: $20!$ has 19 digits, the last 18 of which are
    432902008176640000. What is the first digit?
Answer: 2
</code></pre></div>

<p>The solution presented in the benchmark (for human reasoning) exploits a property of divibility by 9 to arrive at the answer. A model equipped with a calculator could explicitly compute 20 ! and arrive at the same answer. We do not expect the functionalized forms to be solvable by open weight models, for whom we can confirm no tools are used in the inference pipeline. We find this to be the case.</p>
<h1>Case 3 - Undergraduate-level mathematical insights and skills</h1>
<div class="codehilite"><pre><span></span><code><span class="n">Question</span><span class="o">:</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="n">value</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">$x$</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">give</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">minimum</span><span class="w"> </span><span class="n">value</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">$x</span><span class="o">^</span><span class="mi">2</span><span class="o">-</span><span class="mi">14</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">3</span><span class="n">$</span><span class="o">?</span>
<span class="n">Answer</span><span class="o">:</span><span class="w"> </span><span class="mi">7</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">Question</span><span class="o">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">equations</span><span class="w"> </span><span class="n">$x</span><span class="o">^</span><span class="mi">3</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">5</span><span class="n">x</span><span class="o">^</span><span class="mi">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">px</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">q</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="n">$</span><span class="w"> </span><span class="n">and</span>
<span class="w">    </span><span class="n">$x</span><span class="o">^</span><span class="mi">3</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">7</span><span class="n">x</span><span class="o">^</span><span class="mi">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">px</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">r</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="n">$</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="n">two</span><span class="w"> </span><span class="n">roots</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">common</span><span class="o">.</span>
<span class="w">    </span><span class="n">If</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">third</span><span class="w"> </span><span class="n">root</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="n">equation</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">represented</span>
<span class="w">    </span><span class="n">by</span><span class="w"> </span><span class="n">$x_1$</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">$x_2$</span><span class="w"> </span><span class="n">respectively</span><span class="o">,</span><span class="w"> </span><span class="n">compute</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">ordered</span>
<span class="w">    </span><span class="n">pair</span><span class="w"> </span><span class="n">$</span><span class="o">(</span><span class="n">x_1</span><span class="o">,</span><span class="n">x_2</span><span class="o">).</span><span class="n">$</span>
<span class="n">Answer</span><span class="o">:</span><span class="w"> </span><span class="o">(-</span><span class="mi">5</span><span class="o">,-</span><span class="mi">7</span><span class="o">)</span>
</code></pre></div>

<p>The first problem requires understanding when a quadratic equation is minimal. The second problem requires taking the difference of two polynomials, observing a cancellation, and using properties of roots of quadratic and cubic polynomials to arrive at the answer. Neither of these are trivial, and we only expect their functionalized forms to be solvable by top models that are capable of symbolic manipulation. We find that to be the case.</p>
<h2>Case 4 - Graduate-level mathematics</h2>
<div class="codehilite"><pre><span></span><code><span class="n">Question</span><span class="o">:</span><span class="w"> </span><span class="n">Euler</span><span class="w"> </span><span class="n">discovered</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">polynomial</span><span class="w"> </span><span class="n">$p</span><span class="o">(</span><span class="n">n</span><span class="o">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">n</span><span class="o">^</span><span class="mi">2</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">41</span><span class="n">$</span>
<span class="w">    </span><span class="n">yields</span><span class="w"> </span><span class="n">prime</span><span class="w"> </span><span class="n">numbers</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">many</span><span class="w"> </span><span class="n">small</span><span class="w"> </span><span class="n">positive</span><span class="w"> </span><span class="n">integer</span><span class="w"> </span><span class="n">values</span>
<span class="w">    </span><span class="n">of</span><span class="w"> </span><span class="n">$n$</span><span class="o">.</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">smallest</span><span class="w"> </span><span class="n">positive</span><span class="w"> </span><span class="n">integer</span><span class="w"> </span><span class="n">$n$</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">which</span>
<span class="w">    </span><span class="n">$p</span><span class="o">(</span><span class="n">n</span><span class="o">)</span><span class="n">$</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">$p</span><span class="o">(</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="o">)</span><span class="n">$</span><span class="w"> </span><span class="n">share</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">common</span><span class="w"> </span><span class="n">factor</span><span class="w"> </span><span class="n">greater</span><span class="w"> </span><span class="n">than</span><span class="w"> </span><span class="n">$1$</span><span class="o">?</span>
<span class="n">Answer</span><span class="o">:</span><span class="w"> </span><span class="mi">41</span>
</code></pre></div>

<p>While this problem shows solvable in its static QA form, the graduate-level reasoning required should be outside the planning capabilities of current models. We find that the functional forms are able distinguish such problems and no models are able to solve the functional snapshots.</p>
<h2>Case 5 - Problems where the ground truth answer is problematic.</h2>
<div class="codehilite"><pre><span></span><code><span class="n">Question</span><span class="o">:</span><span class="w"> </span><span class="n">Find</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">solutions</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">equation</span><span class="w"> </span><span class="n">$</span><span class="o">\\</span><span class="n">sqrt</span><span class="o">{</span><span class="mi">3</span><span class="n">x</span><span class="o">+</span><span class="mi">6</span><span class="o">}=</span><span class="n">x</span><span class="o">+</span><span class="mi">2</span><span class="n">$</span><span class="o">.</span><span class="w"> </span><span class="n">If</span>
<span class="w">    </span><span class="n">there</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">multiple</span><span class="w"> </span><span class="n">solutions</span><span class="o">,</span><span class="w"> </span><span class="n">order</span><span class="w"> </span><span class="n">them</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">least</span><span class="w"> </span><span class="n">to</span>
<span class="w">    </span><span class="n">greatest</span><span class="o">,</span><span class="w"> </span><span class="n">separated</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">comma</span><span class="o">(</span><span class="n">s</span><span class="o">).</span>
<span class="n">Answer</span><span class="o">:</span><span class="w"> </span><span class="mi">1</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">Question</span><span class="o">:</span><span class="w"> </span><span class="n">Two</span><span class="w"> </span><span class="n">concentric</span><span class="w"> </span><span class="n">circular</span><span class="w"> </span><span class="n">regions</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="n">radii</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">inch</span><span class="w"> </span><span class="n">and</span>
<span class="mi">10</span><span class="w"> </span><span class="n">inches</span><span class="o">.</span><span class="w"> </span><span class="n">What</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">area</span><span class="o">,</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">square</span><span class="w"> </span><span class="n">inches</span><span class="o">,</span><span class="w"> </span><span class="n">outside</span>
<span class="n">the</span><span class="w"> </span><span class="n">smaller</span><span class="w"> </span><span class="n">region</span><span class="o">,</span><span class="w"> </span><span class="n">but</span><span class="w"> </span><span class="n">inside</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">larger</span><span class="w"> </span><span class="n">region</span><span class="o">?</span><span class="w"> </span><span class="n">Express</span>
<span class="n">your</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">terms</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">$</span><span class="o">\\</span><span class="n">pi$</span><span class="o">.</span>
<span class="n">Answer</span><span class="o">:</span><span class="w"> </span><span class="mi">99</span><span class="o">\\</span><span class="n">pi</span><span class="w"> </span><span class="o">\\</span><span class="n">text</span><span class="o">\{\</span><span class="w"> </span><span class="n">Square</span><span class="w"> </span><span class="n">inches</span><span class="o">\}</span>
</code></pre></div>

<p>For the first question, the ground truth that matches the question is " $-2,1$ " and the stated answer is incorrect. For the second question, there is extra verbiage in the answer which is not asked for in the question. Model outputs that match the imprecise ground truth are concerning, and we do find models where they do.</p>
<p>When we test the functionalized, corrected, versions of these problems the matching dissappears.</p>
<p>These cases illustrate how a functional evaluation separates out various cases, and provides a more robust evaluation compared to static QAs.</p>
<h1>4 Approach</h1>
<p>Given a benchmark test written as a "question, answer" pair, we convert it to three functions "problem(inputs), solution(inputs), inputs(rngs, seed)". These functions are designed to have the property that arriving at answer using a well-reasoned derivation from the original question is the same that would yield answer' = solution(in) from question' $=$ problem(in) where in $=$ inputs(rngs, seed) for an arbitrarily chosen seed.</p>
<p>Building these functions requires human insight in: a) picking a set of symbolic inputs that sufficiently generalize both question and answer, b) encoding any implicit constraints on the inputs in the generator function and that the generator explores the input space as the seed is varied, c) translating the text of the specific question and answer, into a respective functions problem and solution that symbolically derive with the same steps as would have been followed in the derivation of answer from question.</p>
<p>Functional variants of a benchmark We call a benchmark, e.g., MATH, that has its test set converted from static "question, answer" form into functionals as above the functional variant, noted as MATH(). Each test in the functional variant cannot be directly used to evaluate, but instead it needs to be instantiated.</p>
<p>Snapshots We call an instantiation of each of the functionalized tests using a seed a snapshot of the benchmark. For our implementation, we use a hash of the month string, e.g., "Dec-2023", combined with a hash of the original text QA, as the seed. When instantiated, each test in the snapshot has the same reasoning as the original test but the steps to the answer will be different. We can run the evaluation scripts unmodified from the original benchmark over the snapshot.</p>
<p>Functional accuracy We define the functional accuracy as the accuracy of solving a test in all $k$ snapshots and the static variant. If a model arrives at an answer given a question, using proper reasoning, then its outcome should be identical over the static benchmark as well as each snapshot of the functional variant. Thus we define the functional accuracy of the model against that benchmark as the fraction of tests it gets correct over $k$-snapshots and the static variant.</p>
<p>Reasoning Gap The reasoning gap is the percent decrease in the accuracy between the static and functional variants.</p>
<p>Example of functionalizing a test Figure 1 shows an example text QA from the counting and probability subset of the MATH test dataset, that is solved in its static form by GPT4. While it is not a particularly complicated counting calculation, it does require more than grade school reasoning of realizing that the problem</p>
<div class="codehilite"><pre><span></span><code><span class="nv">Text</span><span class="w"> </span><span class="nv">problem</span>:
<span class="nv">The</span><span class="w"> </span><span class="nv">letters</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">word</span><span class="w"> </span><span class="s1">&#39;SIXTEEN&#39;</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">randomly</span><span class="w"> </span><span class="nv">arranged</span>.
<span class="nv">What</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">probability</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">two</span><span class="w"> </span><span class="nv">E</span><span class="err">&#39;s are not next to each other?</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="nv">Text</span><span class="w"> </span><span class="nv">solution</span>:
<span class="nv">The</span><span class="w"> </span><span class="nv">best</span><span class="w"> </span><span class="nv">way</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="k">do</span><span class="w"> </span><span class="nv">this</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">find</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">probability</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">the</span>
<span class="nv">two</span><span class="w"> </span><span class="nv">E</span><span class="err">&#39;s are next to each other. There are $\\dfrac{7!}{2}$\$ arrangements of the word SIXTEEN. If we want to find the number</span>
<span class="nv">of</span><span class="w"> </span><span class="nv">arrangements</span><span class="w"> </span><span class="nv">such</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">E</span><span class="err">&#39;s are next to each other, we find</span>
<span class="err">the number of arrangements of the six-letter word SIXT(EE)N</span>
<span class="ss">(</span><span class="nv">where</span><span class="w"> </span><span class="nv">we</span><span class="w"> </span><span class="nv">treat</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">two</span><span class="w"> </span><span class="nv">E</span><span class="err">&#39;s as a block), which is $6!$.</span>
<span class="err">So the probability that an arrangement of the word SIXTEEN</span>
<span class="nv">has</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">two</span><span class="w"> </span><span class="nv">E</span><span class="err">&#39;s next to each other is</span>
<span class="err">$\\dfrac{6!}{\frac{7!}{2}}=\backslash\backslash \operatorname{dfrac}{2}{7}$.</span>
<span class="nv">So</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">probability</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">two</span><span class="w"> </span><span class="nv">E</span><span class="s1">&#39;s aren&#39;</span><span class="nv">t</span><span class="w"> </span><span class="k">next</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">each</span><span class="w"> </span><span class="nv">other</span><span class="w"> </span><span class="nv">is</span>
<span class="mh">$1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span>\\<span class="nv">dfrac</span>{<span class="mi">2</span>}{<span class="mi">7</span>}<span class="w"> </span><span class="o">=</span><span class="w"> </span>\\<span class="nv">boxed</span>{\\<span class="nv">dfrac</span>{<span class="mi">5</span>}{<span class="mi">7</span>}}$.
</code></pre></div>

<p>Figure 1: Example of a text QA from the MATH test data (Counting and Probability, 714.json). Note that the evaluation suite only compares the correctness of the output text enclosed in " $\backslash \backslash$ boxed ${\ldots}$ " in this case the correct answer would be " $\backslash \backslash \operatorname{dfrac}{5}{7}$ " .
can be translated into its negated form, followed by canceling of major terms in the permutation. While it is not impossible that a model would do step-by-step reasoning of such form if it has seen the logic before, inventing that line of reasoning from first principles would be surprising. We convert it to a functional test as shown in Figure 2. Of the three snapshots we take of this, GPT4 solved $1 / 3$ of them.</p>
<p>Figure 3 shows an example text QA from the prealgebra subset of the MATH test dataset, also solved in its static form by GPT4. Figure 4 shows its corresponding functionalization. GPT4 reports "NO SOLUTION" for each of the snapshots, which it has been instructed to do if it does not know the answer, indicating that its answer to the static version was adding spurious accuracy. While hallucinations are not the core focus of this work, we also test how many times the models report NO SOLUTION, which they rarely do, and this is one of the rare occasions.</p>
<h1>5 Results and Discussion</h1>
<h3>5.1 Functionalizing MATH to MATH()</h3>
<p>We have functionalized $41.2 \%(2060 / 5000)$ of the MATH benchmark. This subset was chosen based on the portion of the benchmark that is solvable by a group of SOTA closed and OSS models, without prompting optimizations (e.g., CoT) and with pass@1. This choice makes the evaluation complete for the current set of models, because by definition tests that fail the static version do not count towards overall functional accuracy. The next release of the benchmark will provide $100 \%$ coverage, and also permit interpretable results under model-specific prompting optimization, and options for testing against pass@ $k$ or $\operatorname{maj} @ j$ for $k, j&gt;1$.</p>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">problem</span><span class="p">(</span><span class="n">word</span><span class="p">:</span><span class="w"> </span><span class="nb">str</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="n">prb</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">f</span><span class="s2">&quot;The letters of the word &#39;{word}&#39; are randomly arranged.&quot;</span>
<span class="w">        </span><span class="n">f</span><span class="s2">&quot;What is the probability that the two E&#39;s are not next to each other?&quot;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">prb</span>
<span class="n">def</span><span class="w"> </span><span class="n">solution</span><span class="p">(</span><span class="n">word</span><span class="p">:</span><span class="w"> </span><span class="nb">str</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="n">length</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
<span class="w">    </span><span class="c1"># The numerator is always 2 less than the length of the</span>
<span class="w">    </span><span class="c1"># random string - 2 being the length of the string &#39;EE&#39;</span>
<span class="w">    </span><span class="n">numerator</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">length</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">2</span>
<span class="w">    </span><span class="c1"># Resulting denominator will always be the length of the string</span>
<span class="w">    </span><span class="n">denominator</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">length</span>
<span class="w">    </span><span class="c1"># Simplifying the fraction</span>
<span class="w">    </span><span class="n">common_factor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">math</span><span class="o">.</span><span class="n">gcd</span><span class="p">(</span><span class="n">numerator</span><span class="p">,</span><span class="w"> </span><span class="n">denominator</span><span class="p">):</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">common_factor</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">1</span><span class="p">:</span>
<span class="w">        </span><span class="n">numerator</span><span class="w"> </span><span class="o">//=</span><span class="w"> </span><span class="n">common_factor</span>
<span class="w">        </span><span class="n">denominator</span><span class="w"> </span><span class="o">//=</span><span class="w"> </span><span class="n">common_factor</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">f</span><span class="s2">&quot;</span><span class="se">\\</span><span class="s2">dfrac{{{numerator}}}{{{{denominator}}}&quot;</span>
<span class="n">def</span><span class="w"> </span><span class="n">inputs</span><span class="p">(</span><span class="n">rngs</span><span class="p">,</span><span class="w"> </span><span class="nb">seed</span><span class="p">):</span>
<span class="w">    </span><span class="n">rngs</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="nb">seed</span><span class="p">)</span>
<span class="w">    </span><span class="c1"># The string is made up of three parts, pre + &#39;EE&#39; + post</span>
<span class="w">    </span><span class="c1"># We keep the string &#39;EE&#39; constant to align with original problem</span>
<span class="w">    </span><span class="c1"># Length of the generated word</span>
<span class="w">    </span><span class="n">length</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rngs</span><span class="o">.</span><span class="n">int_between</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">)</span>
<span class="w">    </span><span class="c1"># Generate a random index where the two &#39;E&#39;s will be placed</span>
<span class="w">    </span><span class="n">index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rngs</span><span class="o">.</span><span class="n">int_between</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span>
<span class="w">    </span><span class="c1"># A string containing all uppercase letters except &#39;E&#39;</span>
<span class="w">    </span><span class="n">excl_ee</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">string</span><span class="o">.</span><span class="n">ascii_uppercase</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;E&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="c1"># A lambda to build a random word with the given length</span>
<span class="w">    </span><span class="n">build</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">lambda</span><span class="w"> </span><span class="n">length</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">rngs</span><span class="o">.</span><span class="n">choose_from</span><span class="p">(</span><span class="n">excl_ee</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">_</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nb">range</span><span class="p">(</span><span class="n">length</span><span class="p">))</span>
<span class="w">    </span><span class="c1"># Construct the string with the three parts</span>
<span class="w">    </span><span class="n">pre</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">build</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
<span class="w">    </span><span class="n">post</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">build</span><span class="p">(</span><span class="n">length</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">index</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span>
<span class="w">    </span><span class="n">word</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pre</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="s2">&quot;EE&quot;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">post</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="p">{</span><span class="s2">&quot;word&quot;</span><span class="p">:</span><span class="w"> </span><span class="n">word</span><span class="p">}</span>
</code></pre></div>

<p>Figure 2: Functionalization of the text QA from Figure 1</p>
<div class="codehilite"><pre><span></span><code><span class="nv">Text</span><span class="w"> </span><span class="nv">problem</span>:
<span class="nv">Spinner</span><span class="w"> </span><span class="nv">I</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">divided</span><span class="w"> </span><span class="nv">into</span><span class="w"> </span><span class="nv">four</span><span class="w"> </span><span class="nv">equal</span><span class="w"> </span><span class="nv">sections</span><span class="w"> </span><span class="nv">labeled</span><span class="w"> </span><span class="mi">2</span>,<span class="w"> </span><span class="mi">3</span>,<span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="mi">5</span>.
<span class="nv">Spinner</span><span class="w"> </span><span class="nv">II</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">divided</span><span class="w"> </span><span class="nv">into</span><span class="w"> </span><span class="nv">five</span><span class="w"> </span><span class="nv">equal</span><span class="w"> </span><span class="nv">sections</span><span class="w"> </span><span class="nv">labeled</span><span class="w"> </span><span class="mi">1</span>,<span class="w"> </span><span class="mi">3</span>,<span class="w"> </span><span class="mi">5</span>,<span class="w"> </span><span class="mi">7</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="mi">9</span>.
<span class="k">If</span><span class="w"> </span><span class="nv">each</span><span class="w"> </span><span class="nv">spinner</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">spun</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">resulting</span><span class="w"> </span><span class="nv">numbers</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">multiplied</span>,
<span class="nv">what</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">probability</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">product</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">two</span><span class="o">-</span><span class="nv">digit</span><span class="w"> </span><span class="nv">even</span><span class="w"> </span><span class="nv">number</span>?
<span class="nv">Express</span><span class="w"> </span><span class="nv">your</span><span class="w"> </span><span class="nv">answer</span><span class="w"> </span><span class="nv">as</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">common</span><span class="w"> </span><span class="nv">fraction</span>.
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="nv">Text</span><span class="w"> </span><span class="nv">solution</span><span class="o">:</span>
<span class="nv">Let</span><span class="w"> </span><span class="nv">results</span><span class="w"> </span><span class="nv">be</span><span class="w"> </span><span class="nv">denoted</span><span class="w"> </span><span class="nv">by</span><span class="w"> </span><span class="ow">or</span><span class="nv">dered</span><span class="w"> </span><span class="nv">pairs</span><span class="w"> </span><span class="nv">where</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">first</span><span class="w"> </span><span class="nv">coordinate</span>
<span class="nv">corresponds</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="nv">Spinner</span><span class="w"> </span><span class="nv">I</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">second</span><span class="w"> </span><span class="nv">coordinate</span><span class="w"> </span><span class="nv">corresponds</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="nv">Spinner</span><span class="w"> </span><span class="nv">II</span><span class="o">.</span>
<span class="nv">Since</span><span class="w"> </span><span class="nv">all</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">section</span><span class="w"> </span><span class="nv">numbers</span><span class="w"> </span><span class="nv">on</span><span class="w"> </span><span class="nv">Spinner</span><span class="w"> </span><span class="nv">II</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">odd</span><span class="p">,</span><span class="w"> </span><span class="nv">Spinner</span><span class="w"> </span><span class="nv">I</span><span class="w"> </span><span class="nv">must</span><span class="w"> </span><span class="nv">given</span><span class="w"> </span><span class="nv">an</span>
<span class="nv">even</span><span class="w"> </span><span class="nv">number</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="ow">or</span><span class="nv">der</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">product</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="nv">be</span><span class="w"> </span><span class="nv">even</span><span class="o">.</span><span class="w"> </span><span class="nv">The</span><span class="w"> </span><span class="nv">results</span>
<span class="p">$(</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">)$,</span><span class="w"> </span><span class="p">$(</span><span class="mi">2</span><span class="p">,</span><span class="mi">7</span><span class="p">)$,</span><span class="w"> </span><span class="p">$(</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">)$,</span><span class="w"> </span><span class="p">$(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">)$,</span><span class="w"> </span><span class="p">$(</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">)$,</span><span class="w"> </span><span class="p">$(</span><span class="mi">4</span><span class="p">,</span><span class="mi">7</span><span class="p">)$,</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="p">$(</span><span class="mi">4</span><span class="p">,</span><span class="mi">9</span><span class="p">)$</span>
<span class="nv">are</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">ones</span><span class="w"> </span><span class="nv">whose</span><span class="w"> </span><span class="nv">products</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">two</span><span class="o">-</span><span class="nv">digit</span><span class="w"> </span><span class="nv">even</span><span class="w"> </span><span class="nv">numbers</span><span class="o">.</span>
<span class="nv">Since</span><span class="w"> </span><span class="nv">there</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="p">$</span><span class="mi">5</span>\\<span class="nv">times4</span><span class="o">=</span><span class="mi">20</span><span class="p">$</span><span class="w"> </span><span class="nv">equally</span><span class="w"> </span><span class="nv">likely</span><span class="w"> </span><span class="nv">results</span><span class="p">,</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">probability</span><span class="w"> </span><span class="nv">of</span>
<span class="nv">obtaining</span><span class="w"> </span><span class="nv">an</span><span class="w"> </span><span class="nv">even</span><span class="w"> </span><span class="nv">two</span><span class="o">-</span><span class="nv">digit</span><span class="w"> </span><span class="nv">product</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="p">$</span>\\<span class="nv">boxed</span><span class="p">{</span>\\<span class="nv">frac</span><span class="p">{</span><span class="mi">7</span><span class="p">}{</span><span class="mi">20</span><span class="p">}}</span><span class="o">.</span>
</code></pre></div>

<p>Figure 3: Example of a text QA from the MATH test data (Prealgebra, 1151.json). Note that the evaluation suite only compares the correctness of the output text enclosed in " $\backslash \backslash$ boxed ${\ldots}$ ", in this case the correct answer would be " $\backslash \backslash$ frac ${7}{20}$ ".</p>
<h1>5.2 Reasoning gap for major models</h1>
<p>Models evaluated We ran evaluations over SOTA reasoning models: The closed models accessed through APIs were OpenAI's GPT3.5 and GPT4 [54], Anthropic's Claude 2.1 [5], and Mistral's Mixtral Medium, 7x8B MoE and 7x8B MoE Instruct [33]. The OSS models included were LLaMA 2 70B [72], WizardCoder Python 34B [47], Yi 34B and Yi Chat 34B [2], StripedHypena Nous and Hessian 7B [57], Qwen 7B [9]. We chose this subset of models to sample a diversity of sizes, architectures, training data and recipes, and instruction and preference tuning.</p>
<p>Gap $=\mathbf{5 8 - 8 0 \%}$ for SOTA models Evaluated over the Q1'24 snapshot (consisting of Oct-2023, Nov-2023, Dec-2023), we find that the models have a reasoning gap varying between $58.35 \%$ and $80.31 \%$, as shown in Figure 5(a). Figure 5(b) shows the percentage of problems the model solved correctly that were evaluated using functional snapshots. While we attempted to functionalize every problem, some were not convertible, either because the problem was already as general as possible, or it was so specific that it did not permit any parameterization. Figure 5(c) shows the individual static accuracy vs functional accuracy.</p>
<p>Gap stabilization after three snapshots Figure 6 shows our analysis of gaps with three snapshots, two snapshots (three subsets chosen from Oct, Nov, Dec), and one snapshot (Oct, Nov, Dec taken individually). For two and one snapshot subsets of which there are three each, we take the mean as the representative, since we observed only minor variation in the accuracy. Raw data for all subsets is available in the associated repository for further examination. Figure 6(a) shows the gaps for all models, and Figure 6(b) shows GPT4's gap separately. We find that three snapshots suffice to stabilize the reasoning gap. Including further snapshots does not materially alter the gap. We take this to mean that tests that are solved by</p>
<div class="codehilite"><pre><span></span><code><span class="nx">def</span><span class="w"> </span><span class="nx">problem</span><span class="p">(</span><span class="nx">spinner1</span><span class="p">:</span><span class="w"> </span><span class="nx">list</span><span class="p">,</span><span class="w"> </span><span class="nx">spinner2</span><span class="p">:</span><span class="w"> </span><span class="nx">list</span><span class="p">,</span><span class="w"> </span><span class="nx">num_digits</span><span class="p">:</span><span class="w"> </span><span class="nx">int</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="nx">str</span><span class="p">:</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="nx">The</span><span class="w"> </span><span class="nx">helper</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">csp</span><span class="err">&#39;</span><span class="w"> </span><span class="nx">formats</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">list</span><span class="w"> </span><span class="nx">into</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">comma</span><span class="w"> </span><span class="nx">separated</span><span class="w"> </span><span class="nx">format</span><span class="w"> </span><span class="k">like</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">problem</span><span class="w"> </span><span class="nx">statement</span><span class="w"> </span><span class="err">&#39;</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="mi">5</span><span class="err">&#39;</span><span class="p">,</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">to_word</span><span class="w"> </span><span class="nx">creates</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">english</span><span class="w"> </span><span class="nx">word</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nx">f</span><span class="s">&quot;Spinner I is divided into four equal sections labeled {csp(spinner1)}. &quot;</span><span class="w"> </span>\
<span class="w">        </span><span class="nx">f</span><span class="s">&quot;Spinner II is divided into five equal sections labeled {csp(spinner2)}. &quot;</span><span class="w"> </span>\
<span class="w">        </span><span class="nx">f</span><span class="s">&quot;If each spinner is spun and the resulting numbers are multiplied, &quot;</span><span class="w"> </span>\
<span class="w">        </span><span class="nx">f</span><span class="s">&quot;what is the probability that the product is a {to_word(num_digits)}-digit &quot;</span><span class="w"> </span>\
<span class="w">        </span><span class="nx">f</span><span class="s">&quot;even number? Express your answer as a common fraction.&quot;</span>
<span class="nx">def</span><span class="w"> </span><span class="nx">solution</span><span class="p">(</span><span class="nx">spinner1</span><span class="p">:</span><span class="w"> </span><span class="nx">list</span><span class="p">,</span><span class="w"> </span><span class="nx">spinner2</span><span class="p">:</span><span class="w"> </span><span class="nx">list</span><span class="p">,</span><span class="w"> </span><span class="nx">num_digits</span><span class="p">:</span><span class="w"> </span><span class="nx">int</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="nx">str</span><span class="p">:</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="nx">The</span><span class="w"> </span><span class="nx">total</span><span class="w"> </span><span class="nx">possible</span><span class="w"> </span><span class="nx">permutations</span>
<span class="w">    </span><span class="nx">total_permutations</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">len</span><span class="p">(</span><span class="nx">spinner1</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nx">len</span><span class="p">(</span><span class="nx">spinner2</span><span class="p">)</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="nx">The</span><span class="w"> </span><span class="nx">result</span><span class="w"> </span><span class="nx">we</span><span class="w"> </span><span class="nx">need</span><span class="w"> </span><span class="k">where</span><span class="w"> </span><span class="nx">spinner</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">even</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">product</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">num_digits</span><span class="err">&#39;</span>
<span class="w">    </span><span class="nx">total_count</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">sum</span><span class="p">(</span>
<span class="w">        </span><span class="mi">1</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">spinner1</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="nx">j</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">spinner2</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nx">is_n_digit</span><span class="p">(</span><span class="nx">i</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nx">j</span><span class="p">,</span><span class="w"> </span><span class="nx">num_digits</span><span class="p">)</span>
<span class="w">    </span><span class="p">)</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="nx">Simplifying</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">fraction</span>
<span class="w">    </span><span class="nx">factor</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">math</span><span class="p">.</span><span class="nx">gcd</span><span class="p">(</span><span class="nx">total_count</span><span class="p">,</span><span class="w"> </span><span class="nx">total_permutations</span><span class="p">)</span>
<span class="w">    </span><span class="nx">numerator</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">total_count</span><span class="w"> </span><span class="c1">// factor</span>
<span class="w">    </span><span class="nx">denominator</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">total_permutations</span><span class="w"> </span><span class="c1">// factor</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nx">f</span><span class="s">&quot;&quot;&quot;\\frac{{{numerator}}}{{{{denominator}}}&quot;&quot;&quot;</span>
<span class="nx">def</span><span class="w"> </span><span class="nx">inputs</span><span class="p">(</span><span class="nx">rngs</span><span class="p">,</span><span class="w"> </span><span class="nx">seed</span><span class="p">):</span>
<span class="w">    </span><span class="nx">rngs</span><span class="p">.</span><span class="nx">set_seed</span><span class="p">(</span><span class="nx">seed</span><span class="p">)</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="nx">Generate</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">random</span><span class="w"> </span><span class="nx">target</span><span class="w"> </span><span class="nx">digit</span><span class="w"> </span><span class="nx">count</span>
<span class="w">    </span><span class="nx">digit_count</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">rngs</span><span class="p">.</span><span class="nx">even_between</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">10</span><span class="p">)</span>
<span class="w">    </span><span class="nx">one_num_count</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">digit_count</span><span class="w"> </span><span class="c1">// 2</span>
<span class="w">    </span><span class="nx">lower_limit</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="mi">10</span><span class="o">**</span><span class="w"> </span><span class="p">(</span><span class="nx">one_num_count</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span>
<span class="w">    </span><span class="nx">upper_limit</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="nx">one_num_count</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="nx">a</span><span class="p">,</span><span class="w"> </span><span class="nx">b</span><span class="p">,</span><span class="w"> </span><span class="nx">c</span><span class="p">,</span><span class="w"> </span><span class="nx">d</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">spinner</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">e</span><span class="p">,</span><span class="w"> </span><span class="nx">f</span><span class="p">,</span><span class="w"> </span><span class="nx">g</span><span class="p">,</span><span class="w"> </span><span class="nx">h</span><span class="p">,</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">spinner</span><span class="w"> </span><span class="mi">2</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="nx">Only</span><span class="w"> </span><span class="nx">numbers</span><span class="w"> </span><span class="nx">generated</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="nx">more</span><span class="w"> </span><span class="nx">than</span><span class="w"> </span><span class="nx">half</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">range</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">values</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="nx">will</span><span class="w"> </span><span class="nx">multiply</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">give</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">double</span><span class="w"> </span><span class="nx">digit</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">results</span><span class="p">.</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="nx">So</span><span class="w"> </span><span class="nx">d</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">f</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">fixed</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">having</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">double</span><span class="w"> </span><span class="nx">digit</span><span class="w"> </span><span class="nx">result</span><span class="p">.</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="nx">Rest</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">products</span><span class="w"> </span><span class="nx">will</span><span class="w"> </span><span class="nx">be</span><span class="w"> </span><span class="nx">random</span>
<span class="w">    </span><span class="nx">a</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">rngs</span><span class="p">.</span><span class="nx">int_between</span><span class="p">(</span><span class="nx">lower_limit</span><span class="p">,</span><span class="w"> </span><span class="nx">upper_limit</span><span class="p">)</span>
<span class="w">    </span><span class="nx">b</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">rngs</span><span class="p">.</span><span class="nx">int_between</span><span class="p">(</span><span class="nx">lower_limit</span><span class="p">,</span><span class="w"> </span><span class="nx">upper_limit</span><span class="p">)</span>
<span class="w">    </span><span class="nx">c</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">rngs</span><span class="p">.</span><span class="nx">natural_int</span><span class="p">(</span><span class="mi">100000</span><span class="p">)</span>
<span class="w">    </span><span class="nx">d</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">rngs</span><span class="p">.</span><span class="nx">even_between</span><span class="p">((</span><span class="nx">lower_limit</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">upper_limit</span><span class="p">)</span><span class="w"> </span><span class="c1">// 2, upper_limit)</span>
<span class="w">    </span><span class="nx">e</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">rngs</span><span class="p">.</span><span class="nx">odd_between</span><span class="p">(</span><span class="nx">lower_limit</span><span class="p">,</span><span class="w"> </span><span class="nx">upper_limit</span><span class="p">)</span>
<span class="w">    </span><span class="nx">f</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">rngs</span><span class="p">.</span><span class="nx">odd_between</span><span class="p">((</span><span class="nx">lower_limit</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">upper_limit</span><span class="p">)</span><span class="w"> </span><span class="c1">// 2, upper_limit)</span>
<span class="w">    </span><span class="nx">g</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">rngs</span><span class="p">.</span><span class="nx">odd_between</span><span class="p">(</span><span class="nx">lower_limit</span><span class="p">,</span><span class="w"> </span><span class="nx">upper_limit</span><span class="p">)</span>
<span class="w">    </span><span class="nx">h</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">rngs</span><span class="p">.</span><span class="nx">odd_between</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="nx">upper_limit</span><span class="p">)</span>
<span class="w">    </span><span class="nx">i</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">rngs</span><span class="p">.</span><span class="nx">odd_between</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="nx">upper_limit</span><span class="p">)</span>
<span class="w">    </span><span class="nx">spinner1</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="p">[</span><span class="nx">a</span><span class="p">,</span><span class="w"> </span><span class="nx">b</span><span class="p">,</span><span class="w"> </span><span class="nx">c</span><span class="p">,</span><span class="w"> </span><span class="nx">d</span><span class="p">]</span>
<span class="w">    </span><span class="nx">spinner2</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="p">[</span><span class="nx">e</span><span class="p">,</span><span class="w"> </span><span class="nx">f</span><span class="p">,</span><span class="w"> </span><span class="nx">g</span><span class="p">,</span><span class="w"> </span><span class="nx">h</span><span class="p">,</span><span class="w"> </span><span class="nx">i</span><span class="p">]</span>
<span class="err">#</span><span class="w"> </span><span class="nx">Adding</span><span class="w"> </span><span class="nx">more</span><span class="w"> </span><span class="nx">numbers</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">spinners</span>
<span class="k">for</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">range</span><span class="p">(</span><span class="nx">rngs</span><span class="p">.</span><span class="nx">int_between</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">)):</span>
<span class="w">    </span><span class="nx">spinner1</span><span class="p">.</span><span class="nx">append</span><span class="p">(</span><span class="nx">rngs</span><span class="p">.</span><span class="nx">int_between</span><span class="p">(</span><span class="nx">lower_limit</span><span class="p">,</span><span class="w"> </span><span class="nx">upper_limit</span><span class="p">))</span>
<span class="k">return</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;spinner1&quot;</span><span class="p">:</span><span class="w"> </span><span class="nx">spinner1</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;spinner2&quot;</span><span class="p">:</span><span class="w"> </span><span class="nx">spinner2</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;num_digits&quot;</span><span class="p">:</span><span class="w"> </span><span class="nx">digit_count</span><span class="p">}</span>
</code></pre></div>

<p>Figure 4: Functionalization of the text QA from Figure 3</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 5: (a) Reasoning gap (note: x-axis starts at $50 \%$ ), (b) Coverage: fraction of static QA that the model solves correctly that are tested functionally, (c) Static and functional accuracies.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 6: Gap starts at close to final value with a single snapshot, increases slightly with 2 snapshots, and essentially stabilizes at 3 snapshots.
the models consistently with $k=3$ are solved by reasoning about the problem. The likelihood of accidentally solving three separate snapshots, including the static variant, is miniscule.</p>
<p>Gap across difficulty levels in MATH() MATH has problems categorized into five levels. Figure 7 shows the gap against these levels. The levels in the test dataset roughly correspond to the difficulty of the problems, although our subjective assessment indicates that levels 1-3 are hard to discern as distinct from each other. We find that the reasoning gap expectedly increases with difficulty levels, suggesting that the models are capable of simpler reasoning at the lower difficulty levels and solve harder problems with more memorization (Figure 7(a)). The trend is similar for the top model (GPT4), except its gap is lower for each individual level (Figure 7(b)).</p>
<p>Gap across subject levels in MATH() Figure 8 shows the gap against subjects. Precalculus, prealgebra, and intermediate algebra are the easiest problems in</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 7: Gap across levels, aggregated across all models and for GPT4 separately.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 8: Gap across subjects, aggregated across all models and for GPT4 separately.</p>
<p>the dataset and the reasoning gap is expectedly lowest for those, indicating again that the models are solving problems under those subjects with proper reasoning (Figure 8(a)). Similar trends hold across the aggregated set of models and in the top model (GPT4), although GPT4's gap is lower across all subjects (Figure 8(b)).</p>
<h1>5.3 Analysis of problems solved across all snapshots, i.e., with proper reasoning</h1>
<p>Problems solved by the top model (GPT4) GPT4 solves 1299 static QA problems (accuracy $=25.98 \%$ ), and its functional accuracy with 3 -snapshots is $10.82 \%(541 / 5000)$, giving it the lowest reasoning gap of $58.35 \%$ amongst the models tested. Of these:</p>
<ul>
<li>239 were ungeneralizable, and their functional versions are static. This means that $4.78 \%$ of its functional accuracy can be attributed to being static.</li>
<li>302 had their functional snapshots solved, giving it a $6.04 \%$ accuracy on problems that indeed were different across snapshots.</li>
</ul>
<p>We manually categorized each of the 302 that were solved based on a) hardness as indicated by levels and subjects, b) our subjective surprise (low, medium, high) that they were solved based on the individual steps and process required to solve them, c) our subjective tag of the type of reasoning needed to solve.</p>
<p>Solved hardness, by levels: We find an expected dropoff across levels (Figure 9(a)), except for level 1. Human assessment from the team of mathematicians and computer scientists doing the functionalization, we find little difference between levels 1 and 2, and if we aggregate them then we find a monotonic drop in numbers solved.</p>
<p>Solved hardness, by subjects: For subjects (Figure 9(b)), prealgebra and algebra are outliers in being easier to solve. Most of the problems here are of the "grade school math" style problems where the task is to interpret the english description into a simple arithmetic equation. Their high solved counts here reinforce the commonly held perception that SOTA models are good at solving those problems.</p>
<p>224 low-, 63 medium-, 15 high-surprise problems solved: We find 2-4 problems with high surprise ( 15 in total) distributed across all levels (Figure 9(c)). It might be instructive to the community with access to the model and training dataset to understand the model's ability in solving these. The distribution of medium surprise problems follows an expected normal distribution.</p>
<p>14 solving strategies: We identify 14 subjective subtypes (Figure 9(d)) of solving strategies that would be needed for the 302 problems. Of these the extremes are worth noting. The highest count is for simple calculations by a significant margin, which aligns with capabilities, especially, if access to a calculator is in-built. The lowest counts are for figure interpretation ( 1 instance), trigonometry ( 1 instance), simplification insight ( 1 instance), and invent terms to simplify ( 3 instances). All these are worthy of individual investigation as they indicate surprising capabilities.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 9: 302 problems were solved by GPT4 across all snapshots.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 10: Problems solved by the majority $(\geq 5 / 10)$ of OSS models.</p>
<p>Appendix 11 lists a) two illustrative examples for each of the 14 reasoning subtypes, and b) the 15 specific high-surprise problems solved.</p>
<p>Problems solved by a majority of the OSS models The accuracy of OSS models varies between $1.86 \%-12.28 \%$ and it drops to $0.52 \%-4.34 \%$ for the functional snapshots. Only 19 problems are consistently solved across snapshots by the majority $(\geq 5 / 10)$ of models.</p>
<p>Solved hardness, by levels: As argued earlier, levels 1 and 2 are not substantially different, and we find that OSS models essentially solve these levels, except for one outlier in level 3 (Figure 10(a)).</p>
<p>Solved hardness, by subjects: OSS models solutions are limited to the easier subjects of prealgebra, algebra, intermediate algebra, and a single solution in geometry (Figure 10(b)).</p>
<p>19 low-surprise problems solved: There are no medium or high surprise problems consistently solved by the majority of the models (Figure 10(c)).</p>
<p>2 reasoning subtypes: The problems solved consistently were either simple calculations, or involving use of simple properties of operators (Figure 10(d)). Appendix 12 list a) 14 problems solved which were simple calculations, b) 5 problems which required properties of operators.</p>
<h1>6 Threats to the validity of results</h1>
<ul>
<li>More sophisticated prompting will likely change precise reasoning gap: Modelspecific prompting (e.g., Claude2.1 retrieval prompting [1]) or model-agnostic methods such as chain-of-thought [80], tree-of-thought [85], chain-of-code [41], chain-of-thought-decoding [78], tipping/threats/persuasion prompting to elicit more accurate output, are likely to reduce the reasoning gap. In fact, even if accuracy remains the same, the reduced reasoning gap may quantitatively validate the utility of these improvements.</li>
<li>Tool usage: For closed weights models accessed through APIs, it is unclear whether tool usage is part of the inference pipeline. We expect the use of tools, e.g., calculators or theorem provers, to reduce the reasoning gap. In future work, we will evaluate the change in gap with or without tool use with open weight models.</li>
<li>Default chain of thought, i.e., unable to format according to few-shot format: Some models seem to not follow instructions, or learn in-context with few shot examples, as others. In particular, we find Anthropic's Claude 2.1 and Mistral models tend to default output chain-of-thought steps, which does not work very well with the evaluation harness.</li>
<li>Scenarios where functional variant is not general enough: When we could not generalize a test, we marked it as "is static". This could happen at two extremes: i) when the problem is already in symbolic form or, ii) only permits a single formulation. These labelled cases are the reason for less than $100 \%$ coverage in Section 5.2. Aside from these cases, there might be scenarios where we functionalize but the generalization is simple enough that a pattern match against the original static QA would permit solving the functional snapshots. These cases should be rare.</li>
<li>Prompt injection in closed source models or non-determinism in closed source APIs: We do not have access to the additional system prompts being added by the closed source APIs for safety, which might cause reduced accuracy.</li>
<li>
<p>Big numbers: On rare occasions the random number functions might instantiate an excessively large number, e.g., with $10+$ digits, and it would be unreasonable for the model to compute that even if the reasoning used was correct.</p>
</li>
<li>
<p>Output matching: The evaluation code attempts to be liberal in equivalence checking for outputs, but has noticeable limitations. Cases we observe, and might be hard automatically check for, but would be judged as correct by a human evaluator include: a) $\mathrm{C}(21,7)$ and 9 choose 2 instead of their evaluated counterparts, b) expanded output expressions such as $5!=5 \times 4 \times 3 \times 2 \times 1$ and $33(22-12+1)=33 * 11=363$, c) extra formatting characters in output such as LaTeX math delimiters $\$ \backslash f r a c{1}{16} \$$, d) extra english verbiage such as $g(x)=3-2 f(x)$ " (for ground truth " $3-2 f(x)$ ") and
"The remainder when $\$ 2^{\wedge} 8 \$$ is divided by 5 is 1.".</p>
</li>
</ul>
<h1>7 Related Work</h1>
<p>Reasoning benchmarks Many popular benchmarks exist for testing reasoning, focussing on subdomains such abstract reasoning (ARC [17]), mathematical (MATH [29] and GSM8K [20]), code (HumanEval [12], MBPP [7], and Natural2Code [71]), commonsense and world knowledge (HellaSwag [89], Winogrande [59], PIQA [11], SIQA [60], OpenbookQA [50], ARC-Easy/Challenge [19], CommonsenseQA [70], NaturalQuestions [38], TriviaQA [36]), logical (LSAT [95]), or legal (LegalSupport [43]). Aggregated benchmarks collect many of these domains and attempt to give a comprehensive view of a generalized capabilities for foundational models (MMLU [28], HELM [43], BBH [69], and AGI Eval [94]). The aggregated benchmarks cover too much ground and might not be the best metrics for reasoning. There are concerns of contamination and overoptimization towards popular benchmarks leaving a gap between evaluated capabilities, and experienced capabilities over real-world tasks.</p>
<p>To alleviate that concern, there has been a push towards building new, and usually more difficult, text QA benchmarks for science and math (e.g., GPQA [58], ARB [61], JEE [6], CLRS [74], ProofNet [8]), agentic and world model reasoning (e.g., GAIA [49], WorldSense [10]) and code (e.g., CRUXEval [27], SWE-Bench [34], xCodeEval [37]). A recent survey [68] provides a comprehensive list of reasoning tasks by categories. We worry that without a systemic move away from text QAs, such benchmarks are prone to eventual leakage, and overestimation of solving capabilities of existing models.</p>
<p>Alternative benchmarking techniques are being proposed, e.g., through uncertainty quantification [87], ROSCOE step-by-step scoring [25], counterfactual facts [81].</p>
<p>We would suggest using our functionalization or the alternative benchmarking techniques, applied over existing popular or new text benchmarks as the way forward.</p>
<p>Techniques to improve reasoning in language models There is a vast body of ongoing work on improving reasoning capabilities of language models. Especially modifications to training data, or fine tuning recipes. We do not intend cover that literature. Instead, we mention techniques that might impact the reasoning gap during inference, assuming a given base model.</p>
<ul>
<li>Specialized prompting to elicit explicit reasoning: Techniques such as chain of thought [80], tree of thought [85], chain of code [41], chain of hindsight [46], program of thought [13], algorithmic skill prompting [97], progressive hint</li>
</ul>
<p>prompting [93] give models more scratch space to make the reasoning logic explicit, and arguably decompose the argument into simpler more tractable steps. They have been shown to improve accuracy, and we would conjecture they would reduce the gap. In future work we will explore how they change the gap.</p>
<ul>
<li>Augmentation: Delegating the last mile of the inference to a tool [26, 55] (calculator, web search [86], theorem prover, symbolic solver [21], planner [45], interpreter [24]) is bound to help reduce the reasoning gap if the model knows how to decompose the task into appropriate symbolic form.</li>
<li>Self-inspection or related post-processing to improve generalization: Various techniques have been proposed for using the core feedforward network as a unit module around with an inference pipeline can be built. These include using indirect reasoning [92], self-critique [3], divide-and-conquer [48], sampled math or code prompting [32], planner guided decoding [91], self-consistency [77], recursive code template improver [88] logic guide-driven inference [56], selfdebug [14], least-to-most prompting [96], self-discover to compose reasoning structures [98]. It is an open problem how much the accuracy improvements using these techniques translate to lowered reasoning gaps.</li>
</ul>
<p>Understanding the bounds of reasoning, generalization, and memorization in large language models Evaluations are the guides against which we build. They test the end artifact. If we need improved reasoning in these end artifacts, using reasoning-specific training data has shown to help-e.g., ORCA [52, 51], large models as reasoning teachers [30], textbook quality data [42], and code data [83]. Additionally, reasoning-specific fine-tuning algorithms help improve performance after pre-training-e.g., process supervision [44, 75], self-distilling context [66], feedback training for math using oracle teachers [65, 4], self-play finetuning for weak to strong learning [15], refusal-aware fine tuning to reduce hallucinations [90], improving reasoning by removing higher order components [63], and self-instruct [79].</p>
<p>At the other end of the spectrum are investigative analyses that improve our understanding. We now know it is possible to extract training data [53] so some memorization is inevitable, and that the notion of emergent properties might have been a result of choice of metrics [62]. Task contamination is an issue we need to be careful of [40]. Some encouraging results show evidence of multi-hop reasoning [84] while others demonstrating difficulty of such reasoning [39]. Evidence exists for transformer models identifying training data patterns very robustly, while performance degrading outside of distribution [82]. Reasoning might emerge from path-aggregation in the training data [76], and that asking for lengthier answers may improve reasoning [35]. Models are unlikely to know when they are violating formal rules and it is unclear whether they can self-correct [73, 31, 67], but with specific fine-tuning they might self-correct against harmful text [23], and that training on generated data might not be the best approach to preserve reasoning about outlier cases [64].</p>
<p>Given such evolving understanding, a proper scientific approach should innovate on evaluations based on the most recent understanding, so that progress stays stays aligned with real-world experience.</p>
<h1>8 Individual contributors and acknowledgments</h1>
<p>Annarose, Shashank, Anto, Ajay, Adwaith, Alan, and Stevin did the major work of converting each MATH test into code, and Sooraj oversaw their work. Saurabh conceptualized the functionalization and reasoning gap framework, implemented the evaluation code, and wrote the paper. Feedback from initial reviewers Kelly Iknayan, Ashish Agarwal, Prashast Srivastava, Henele Adams, and Soham Mazumdar, helped improve the manuscript.</p>
<h2>9 Future work</h2>
<p>This is part of an ongoing effort to build reasoning metrics that are robust against contamination, and a more accurate representation of model capabilities. In subsequent work, we will functionalize the entire MATH test suite of 5000 problems, and the 1000 GSM8K test problems. We will also use a similar strategy to functionalize the code benchmarks HumanEval, and MBPP.</p>
<p>Additionally, once the benchmarks are $100 \%$ functionalized, we will do studies on the effect of prompting and augmentation strategies (e.g., chain-of-thought, chain-of-code, tree-of-thought, tool usage) on the reasoning gap.</p>
<h2>10 Conclusion</h2>
<p>There is a disconnect between the high benchmark scores in reasoning and the observed below-average reasoning of state-of-the-art models. We propose the reasoning gap metric to quantify this difference, which motivates the open problem of building gap-0 models. Reasoning gaps can be evaluated using the framework of functionalized benchmarks, which are presented as a long term solution to evaluating reasoning on tasks that are known to be within the scope of current models, while presenting them versions they have not seen before. We have functionalized the relevant portion of the popular MATH benchmark, called MATH(), and in future work will extend this to other gold-standard reasoning benchmarks. We are releasing the evaluation code, and the Q1'24 snapshot of MATH(), and will continue to release new snapshots of functionalized benchmarks every quarter.</p>
<h2>References</h2>
<p>[1] Long context prompting for claude 2.1, 2023.
[2] 01.AI. Building the next generation of open-source and bilingual llms, 2023.
[3] Renat Aksitov, Sobhan Miryoosefi, Zonglin Li, Daliang Li, Sheila Babayan, Kavya Kopparapu, Zachary Fisher, Ruiqi Guo, Sushant Prakash, Pranesh Srinivasan, Manzil Zaheer, Felix Yu, and Sanjiv Kumar. Rest meets react: Selfimprovement for multi-step reasoning llm agent, 2023.
[4] Renat Aksitov, Sobhan Miryoosefi, Zonglin Li, Daliang Li, Sheila Babayan, Kavya Kopparapu, Zachary Fisher, Ruiqi Guo, Sushant Prakash, Pranesh Srinivasan, Manzil Zaheer, Felix Yu, and Sanjiv Kumar. Rest meets react: Selfimprovement for multi-step reasoning llm agent, 2023.</p>
<p>[5] Anthropic. Introducing claude 2.1, 2023.
[6] Daman Arora, Himanshu Gaurav Singh, and Mausam. Have llms advanced enough? a challenging problem solving benchmark for large language models, 2023.
[7] Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. CoRR, abs/2108.07732, 2021.
[8] Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W. Ayers, Dragomir Radev, and Jeremy Avigad. Proofnet: Autoformalizing and formally proving undergraduate-level mathematics, 2023.
[9] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.
[10] Youssef Benchekroun, Megi Dervishi, Mark Ibrahim, Jean-Baptiste Gaya, Xavier Martinet, Grégoire Mialon, Thomas Scialom, Emmanuel Dupoux, Dieuwke Hupkes, and Pascal Vincent. Worldsense: A synthetic benchmark for grounded reasoning in large language models, 2023.
[11] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning about physical commonsense in natural language. CoRR, abs/1911.11641, 2019.
[12] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel HerbertVoss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021.
[13] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks, 2023.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Corresponding author. Please send all correspondence to saurabh@consequent.ai.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
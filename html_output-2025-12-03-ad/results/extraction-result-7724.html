<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7724 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7724</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7724</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-144.html">extraction-schema-144</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-271544398</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.20906v5.pdf" target="_blank">Automated literature research and review-generation method based on large language models</a></p>
                <p><strong>Paper Abstract:</strong> Abstract Literature research, which is vital for scientific work, faces the challenge of surging information volumes that are exceeding researchers’ processing capabilities. This paper describes an automated review-generation method based on large language models (LLMs) to overcome efficiency bottlenecks and reduce cognitive load. Our statistically validated evaluation framework demonstrates that the generated reviews match or exceed manual quality, offering broad applicability across research fields without requiring user domain knowledge. Applied to propane dehydrogenation catalysts, our method demonstrated two aspects: first, generating comprehensive reviews from 343 articles spanning 35 topics; and, second, evaluating data-mining capabilities by using 1041 articles for experimental catalyst property analysis. Through multilayered quality control, we effectively mitigated the hallucinations of LLMs, with expert verification confirming accuracy and citation integrity, while demonstrating hallucination risks reduced to <0.5% with 95% confidence. The released software application enables one-click review generation, enhancing research productivity and literature-recommendation efficiency while facilitating broader scientific explorations.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7724.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7724.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automated Review Generation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated Review Generation Method Based on Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end, modular pipeline that uses LLMs to retrieve literature, formulate topics, extract knowledge (via repeated LLM queries and aggregation), perform data mining, and compose review text with DOI‑linked traceability and multi-level hallucination controls.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Automated Review Generation Method Based on Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Shican Wu et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2025</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Automated review generation method based on LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Combines API-based literature retrieval with LLM-driven topic formulation, multi-round LLM knowledge extraction (question generation, repeated extraction, self-consistency aggregation), RAG-like context injection, paragraph generation and scoring, and data mining (XML extraction + aggregation) to produce traceable, cited review articles automatically.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Full-text articles (segmented when exceeding context window), abstracts, titles, DOI metadata</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated review text (topic-structured paragraphs with citations), extracted structured data (XML tabular outputs), visualizations</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Retrieval-augmented context injection, task decomposition into extraction Q&A, repeated-query aggregation (self-consistency), chain-of-thought style evaluation prompts, format-enforced templates (XML), paragraph reranking</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude 2 (primary), Claude3.5 Sonnet (evaluation), Qwen2-72b-Instruct, Qwen2-7b-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Qwen2-7b: 7B; Qwen2-72b: 72B; Claude sizes not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Q1–Q3 chemistry journal articles (1980–2024) for PDH catalysts case study; institutional subscription corpus (proprietary) assembled by authors</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Human-expert comparisons (segmented review fragments), LLM-based scoring with intraclass correlation coefficient (ICC), Transitive Consistency Ratio (TCR), accuracy, false positive rate (95% CI), precision, recall, F1, consistency rate</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Generated reviews matched or exceeded manual quality in many comparisons (e.g., Claude3.5Sonnet average paragraph scores exceeded manual by ~23.6%); optimal-paragraph selection produced near-human or higher scores (up to ~130% of manual); manual sampling on 875 LLM outputs estimated hallucination probability <0.5% at 95% confidence in the knowledge-extraction stage; data-mining stage accuracy and false-positive rates reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Residual hallucination risk (esp. in data-mining numeric/unit extraction), dependence on model capability (smaller models underperform), reliance on available digital literature and Q1-focused initial retrieval choices in the demonstration, limited public release of full-text proprietary corpus, and potential evaluation biases when LLMs evaluate LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated literature research and review-generation method based on large language models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7724.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7724.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A paradigm that augments LLM generation with retrieved documents or context chunks to ground outputs in external text, reducing hallucinations and enabling up-to-date knowledge usage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for knowledgeintensive nlp tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Automated Review Generation Method Based on Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Shican Wu et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2025</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Retrieval-augmented generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Retrieves relevant documents to provide context to the LLM at generation time; used conceptually as part of the pipeline to inject literature evidence into LLM prompts and for verification/aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Retrieved document passages / full-text segments, metadata</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Context-grounded answers, summaries, extracted facts</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Context injection (retrieved passages), reranking and verification steps</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Authors note RAG can still produce errors if retrieved documents are semantically related but do not contain correct answers; RAG vs fine-tuning tradeoffs (RAG favored for lower initial cost and contextual new knowledge).</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated literature research and review-generation method based on large language models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7724.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7724.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaperQA / PaperQA2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaperQA (and PaperQA2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RAG-based agents for literature tasks that perform retrieval, question-answering, summarization, and contradiction detection over user-provided literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Paperqa: Retrieval-augmented generative agent for scientific research.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Automated Review Generation Method Based on Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Shican Wu et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2025</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>PaperQA (RAG-based literature agent)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses retrieval-augmented LLMs to answer queries and summarize scientific papers; PaperQA2 is an improved version noted for strong literature-related performance.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>User-provided literature (papers/abstracts/full texts)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>QA answers, summaries, contradiction detection, structured responses</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Retrieval-augmentation with LLM reranking and agent workflows</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Reported to surpass human expertise in some literature tasks (per authors' citation), evaluation details in original papers</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Cited as demonstrating excellent performance in retrieval, QA, summarization, and contradiction detection; PaperQA2 reported as an improved variant.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper notes these systems require user-provided literature, rely on Q&A interactions, or focus on specific points, which limits transferability to fully automated, end-to-end review generation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated literature research and review-generation method based on large language models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7724.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7724.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AcademicGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AcademicGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based system intended to provide comprehensive research support across literature-related tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Automated Review Generation Method Based on Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Shican Wu et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2025</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>AcademicGPT</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Described by the authors as a tool offering wide-ranging research assistance (summarization, question answering, etc.), cited among recent LLM-based literature tools.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Grouped with other prior systems that often require user-provided literature or interactive queries, limiting transferability to fully automated review generation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated literature research and review-generation method based on large language models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7724.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7724.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CuriousLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CuriousLLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-document question-answering system that improves traversal across documents using reasoning-based agents and knowledge-graph prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Automated Review Generation Method Based on Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Shican Wu et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2025</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>CuriousLLM</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Enhances multi-document QA through reasoning-infused knowledge-graph prompting and agent traversal strategies to collect and reason over evidence across documents.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Multiple documents / multi-document inputs</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Multi-document QA answers and aggregated reasoning traces</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Reasoning-based traversal agents and knowledge-graph prompting</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Mentioned as an advance but within the group of systems that remain reliant on user-provided inputs and interactive QA paradigms; transferability to full automated review pipelines is limited per authors' discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated literature research and review-generation method based on large language models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7724.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7724.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-AI Agent System</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-AI Agent System for Literature Review</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systems composed of multiple AI agents that jointly automate workflows from research-question generation to data extraction and synthesis across the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>System for systematic literature review using multiple ai agents: Concept and an empirical evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Automated Review Generation Method Based on Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Shican Wu et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2025</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Multi-AI agent literature review system</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Orchestrates multiple LLM-driven agents to perform end-to-end literature tasks including question generation, screening, extraction, and synthesis, aiming for full-process automation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>User-specified or retrieved literature corpus</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured literature syntheses, summaries, datasets</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Agent orchestration, retrieval, iterative refinement</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Cited as a recent advance; general limitations (e.g., reliance on correct retrieval, hallucination risk) are noted in the paper but not dissected specifically for each multi-agent system.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated literature research and review-generation method based on large language models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7724.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7724.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LitLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LitLLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A toolkit combining retrieval augmentation with LLM reranking to generate literature reviews from user-provided abstracts and papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Automated Review Generation Method Based on Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Shican Wu et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2025</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LitLLM</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Combines RAG with LLM-based reranking to produce higher-quality literature review outputs given user-supplied abstracts or documents.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>User-provided abstracts / papers</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated literature reviews / summaries</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>RAG with LLM reranking</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Described as requiring user-provided abstracts and thus less suited for fully automated retrieval-and-generation without user curation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated literature research and review-generation method based on large language models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7724.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7724.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLAssist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLAssist (Llassist)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Tools for automating literature screening and initial summarization stages of review preparation using LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Automated Review Generation Method Based on Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Shican Wu et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2025</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLAssist / Llassist</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Provides literature screening and assistance functionality leveraging LLMs to expedite parts of the systematic review workflow.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Abstracts, titles, and possibly full texts</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Screening decisions, extractive summaries</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Mentioned as focused tools for specific review sub-tasks rather than full end-to-end automated review generation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated literature research and review-generation method based on large language models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7724.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7724.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatCite</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatCite</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM agent approach that emulates human workflow guidance to produce comparative literature summaries with improved citation-grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Automated Review Generation Method Based on Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Shican Wu et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2025</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ChatCite</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses human-like workflow guidance for LLM agents to improve summary quality and citation accuracy in comparative literature summarization tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>User-provided literature and citation metadata</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Comparative summaries grounded with citations</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Human-workflow guided agent prompts</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Cited as an advance in improving citation-grounding but still part of the class of systems that require curated inputs or guided workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated literature research and review-generation method based on large language models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7724.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7724.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-consistency aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique that aggregates multiple independent LLM outputs for the same query, using majority/consensus to improve correctness and suppress stochastic hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Automated Review Generation Method Based on Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Shican Wu et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2025</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Self-consistency aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Runs multiple LLM queries for the same extraction question and aggregates repeated answers (majority or consistency checks) to reduce false positives (hallucinations) in extracted facts.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Repeated LLM responses to identical prompts over retrieved text</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Aggregated, higher-confidence extracted facts and answers</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Repeated-query aggregation, voting/consensus</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Measured improvements in aggregated accuracy and reduced false positive rates; binomial aggregation calculations used to justify repetition counts</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Authors used 5 repetitions empirically (also analyze 5/7/9 repetition theory); reported knowledge-extraction aggregated accuracy ~95.77% and estimated hallucination <0.5% (95% CI) via manual sampling in that stage.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Aggregation reduces but does not eliminate hallucinations and increases computation; assumes independence of errors across repetitions.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated literature research and review-generation method based on large language models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7724.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7724.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DAPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain-Adaptive Pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretraining/fine-tuning approach that further trains base LLMs on domain-specific corpora to improve domain knowledge and reduce domain-specific errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Don't stop pretraining: Adapt language models to domains and tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Automated Review Generation Method Based on Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Shican Wu et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2025</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Domain-adaptive pretraining (DAPT)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Fine-tunes the language model on a corpus from the target domain so that the model better handles domain-specific terminology and reduces extraction/conversion errors.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Domain-specific text corpora (in-domain publications)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Domain-adapted LLM weights enabling more accurate extractions and generation</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Authors chose not to fine-tune in order to preserve out-of-the-box multi-domain generalization; DAPT incurs additional cost but can mitigate unit/concept extraction errors in data mining.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated literature research and review-generation method based on large language models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7724.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e7724.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Page Rerank</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Page Rerank Algorithm (pairwise-to-absolute scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An algorithm used to convert pairwise relative comparisons into absolute scores on a fixed scale (0–10) for evaluation aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Automated Review Generation Method Based on Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Shican Wu et al.</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2025</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Page rerank algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Takes pairwise LLM comparison judgments and transforms them into calibrated absolute numeric scores for paragraph-level quality evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Pairwise comparison judgments between generated and reference paragraphs</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Absolute 0–10 quality scores</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Pairwise comparison prompts and reranking conversion</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Used as part of the dual-baseline evaluation framework together with ICC and TCR reliability tests</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Enabled conversion of LLM pairwise comparisons into absolute scores used to compare generated vs manual review paragraphs; contributed to reported model vs human comparative numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on reliable pairwise judgments; susceptible to evaluator bias if comparison judgments are noisy.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated literature research and review-generation method based on large language models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Paperqa: Retrieval-augmented generative agent for scientific research. <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledgeintensive nlp tasks. <em>(Rating: 2)</em></li>
                <li>Language agents achieve superhuman synthesis of scientific knowledge. <em>(Rating: 2)</em></li>
                <li>System for systematic literature review using multiple ai agents: Concept and an empirical evaluation. <em>(Rating: 2)</em></li>
                <li>LitLLM: A Toolkit for Scientific Literature Review. <em>(Rating: 1)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models. <em>(Rating: 1)</em></li>
                <li>Don't stop pretraining: Adapt language models to domains and tasks. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7724",
    "paper_id": "paper-271544398",
    "extraction_schema_id": "extraction-schema-144",
    "extracted_data": [
        {
            "name_short": "Automated Review Generation",
            "name_full": "Automated Review Generation Method Based on Large Language Models",
            "brief_description": "An end-to-end, modular pipeline that uses LLMs to retrieve literature, formulate topics, extract knowledge (via repeated LLM queries and aggregation), perform data mining, and compose review text with DOI‑linked traceability and multi-level hallucination controls.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Automated Review Generation Method Based on Large Language Models",
            "authors": "Shican Wu et al.",
            "year": 2025,
            "method_name": "Automated review generation method based on LLMs",
            "method_description": "Combines API-based literature retrieval with LLM-driven topic formulation, multi-round LLM knowledge extraction (question generation, repeated extraction, self-consistency aggregation), RAG-like context injection, paragraph generation and scoring, and data mining (XML extraction + aggregation) to produce traceable, cited review articles automatically.",
            "input_type": "Full-text articles (segmented when exceeding context window), abstracts, titles, DOI metadata",
            "output_type": "Generated review text (topic-structured paragraphs with citations), extracted structured data (XML tabular outputs), visualizations",
            "prompting_technique": "Retrieval-augmented context injection, task decomposition into extraction Q&A, repeated-query aggregation (self-consistency), chain-of-thought style evaluation prompts, format-enforced templates (XML), paragraph reranking",
            "model_name": "Claude 2 (primary), Claude3.5 Sonnet (evaluation), Qwen2-72b-Instruct, Qwen2-7b-Instruct",
            "model_size": "Qwen2-7b: 7B; Qwen2-72b: 72B; Claude sizes not specified in paper",
            "datasets_used": "Q1–Q3 chemistry journal articles (1980–2024) for PDH catalysts case study; institutional subscription corpus (proprietary) assembled by authors",
            "evaluation_metric": "Human-expert comparisons (segmented review fragments), LLM-based scoring with intraclass correlation coefficient (ICC), Transitive Consistency Ratio (TCR), accuracy, false positive rate (95% CI), precision, recall, F1, consistency rate",
            "reported_results": "Generated reviews matched or exceeded manual quality in many comparisons (e.g., Claude3.5Sonnet average paragraph scores exceeded manual by ~23.6%); optimal-paragraph selection produced near-human or higher scores (up to ~130% of manual); manual sampling on 875 LLM outputs estimated hallucination probability &lt;0.5% at 95% confidence in the knowledge-extraction stage; data-mining stage accuracy and false-positive rates reported in Table 1.",
            "limitations": "Residual hallucination risk (esp. in data-mining numeric/unit extraction), dependence on model capability (smaller models underperform), reliance on available digital literature and Q1-focused initial retrieval choices in the demonstration, limited public release of full-text proprietary corpus, and potential evaluation biases when LLMs evaluate LLM outputs.",
            "counterpoint": true,
            "uuid": "e7724.0",
            "source_info": {
                "paper_title": "Automated literature research and review-generation method based on large language models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation",
            "brief_description": "A paradigm that augments LLM generation with retrieved documents or context chunks to ground outputs in external text, reducing hallucinations and enabling up-to-date knowledge usage.",
            "citation_title": "Retrieval-augmented generation for knowledgeintensive nlp tasks.",
            "mention_or_use": "use",
            "paper_title": "Automated Review Generation Method Based on Large Language Models",
            "authors": "Shican Wu et al.",
            "year": 2025,
            "method_name": "Retrieval-augmented generation (RAG)",
            "method_description": "Retrieves relevant documents to provide context to the LLM at generation time; used conceptually as part of the pipeline to inject literature evidence into LLM prompts and for verification/aggregation.",
            "input_type": "Retrieved document passages / full-text segments, metadata",
            "output_type": "Context-grounded answers, summaries, extracted facts",
            "prompting_technique": "Context injection (retrieved passages), reranking and verification steps",
            "model_name": "",
            "model_size": "",
            "datasets_used": "",
            "evaluation_metric": "",
            "reported_results": "",
            "limitations": "Authors note RAG can still produce errors if retrieved documents are semantically related but do not contain correct answers; RAG vs fine-tuning tradeoffs (RAG favored for lower initial cost and contextual new knowledge).",
            "counterpoint": true,
            "uuid": "e7724.1",
            "source_info": {
                "paper_title": "Automated literature research and review-generation method based on large language models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "PaperQA / PaperQA2",
            "name_full": "PaperQA (and PaperQA2)",
            "brief_description": "RAG-based agents for literature tasks that perform retrieval, question-answering, summarization, and contradiction detection over user-provided literature.",
            "citation_title": "Paperqa: Retrieval-augmented generative agent for scientific research.",
            "mention_or_use": "mention",
            "paper_title": "Automated Review Generation Method Based on Large Language Models",
            "authors": "Shican Wu et al.",
            "year": 2025,
            "method_name": "PaperQA (RAG-based literature agent)",
            "method_description": "Uses retrieval-augmented LLMs to answer queries and summarize scientific papers; PaperQA2 is an improved version noted for strong literature-related performance.",
            "input_type": "User-provided literature (papers/abstracts/full texts)",
            "output_type": "QA answers, summaries, contradiction detection, structured responses",
            "prompting_technique": "Retrieval-augmentation with LLM reranking and agent workflows",
            "model_name": "",
            "model_size": "",
            "datasets_used": "",
            "evaluation_metric": "Reported to surpass human expertise in some literature tasks (per authors' citation), evaluation details in original papers",
            "reported_results": "Cited as demonstrating excellent performance in retrieval, QA, summarization, and contradiction detection; PaperQA2 reported as an improved variant.",
            "limitations": "Paper notes these systems require user-provided literature, rely on Q&A interactions, or focus on specific points, which limits transferability to fully automated, end-to-end review generation.",
            "counterpoint": true,
            "uuid": "e7724.2",
            "source_info": {
                "paper_title": "Automated literature research and review-generation method based on large language models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "AcademicGPT",
            "name_full": "AcademicGPT",
            "brief_description": "An LLM-based system intended to provide comprehensive research support across literature-related tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "Automated Review Generation Method Based on Large Language Models",
            "authors": "Shican Wu et al.",
            "year": 2025,
            "method_name": "AcademicGPT",
            "method_description": "Described by the authors as a tool offering wide-ranging research assistance (summarization, question answering, etc.), cited among recent LLM-based literature tools.",
            "input_type": "",
            "output_type": "",
            "prompting_technique": "",
            "model_name": "",
            "model_size": "",
            "datasets_used": "",
            "evaluation_metric": "",
            "reported_results": "",
            "limitations": "Grouped with other prior systems that often require user-provided literature or interactive queries, limiting transferability to fully automated review generation.",
            "counterpoint": true,
            "uuid": "e7724.3",
            "source_info": {
                "paper_title": "Automated literature research and review-generation method based on large language models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "CuriousLLM",
            "name_full": "CuriousLLM",
            "brief_description": "A multi-document question-answering system that improves traversal across documents using reasoning-based agents and knowledge-graph prompting.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "Automated Review Generation Method Based on Large Language Models",
            "authors": "Shican Wu et al.",
            "year": 2025,
            "method_name": "CuriousLLM",
            "method_description": "Enhances multi-document QA through reasoning-infused knowledge-graph prompting and agent traversal strategies to collect and reason over evidence across documents.",
            "input_type": "Multiple documents / multi-document inputs",
            "output_type": "Multi-document QA answers and aggregated reasoning traces",
            "prompting_technique": "Reasoning-based traversal agents and knowledge-graph prompting",
            "model_name": "",
            "model_size": "",
            "datasets_used": "",
            "evaluation_metric": "",
            "reported_results": "",
            "limitations": "Mentioned as an advance but within the group of systems that remain reliant on user-provided inputs and interactive QA paradigms; transferability to full automated review pipelines is limited per authors' discussion.",
            "counterpoint": true,
            "uuid": "e7724.4",
            "source_info": {
                "paper_title": "Automated literature research and review-generation method based on large language models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Multi-AI Agent System",
            "name_full": "Multi-AI Agent System for Literature Review",
            "brief_description": "Systems composed of multiple AI agents that jointly automate workflows from research-question generation to data extraction and synthesis across the literature.",
            "citation_title": "System for systematic literature review using multiple ai agents: Concept and an empirical evaluation.",
            "mention_or_use": "mention",
            "paper_title": "Automated Review Generation Method Based on Large Language Models",
            "authors": "Shican Wu et al.",
            "year": 2025,
            "method_name": "Multi-AI agent literature review system",
            "method_description": "Orchestrates multiple LLM-driven agents to perform end-to-end literature tasks including question generation, screening, extraction, and synthesis, aiming for full-process automation.",
            "input_type": "User-specified or retrieved literature corpus",
            "output_type": "Structured literature syntheses, summaries, datasets",
            "prompting_technique": "Agent orchestration, retrieval, iterative refinement",
            "model_name": "",
            "model_size": "",
            "datasets_used": "",
            "evaluation_metric": "",
            "reported_results": "",
            "limitations": "Cited as a recent advance; general limitations (e.g., reliance on correct retrieval, hallucination risk) are noted in the paper but not dissected specifically for each multi-agent system.",
            "counterpoint": null,
            "uuid": "e7724.5",
            "source_info": {
                "paper_title": "Automated literature research and review-generation method based on large language models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "LitLLM",
            "name_full": "LitLLM",
            "brief_description": "A toolkit combining retrieval augmentation with LLM reranking to generate literature reviews from user-provided abstracts and papers.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "Automated Review Generation Method Based on Large Language Models",
            "authors": "Shican Wu et al.",
            "year": 2025,
            "method_name": "LitLLM",
            "method_description": "Combines RAG with LLM-based reranking to produce higher-quality literature review outputs given user-supplied abstracts or documents.",
            "input_type": "User-provided abstracts / papers",
            "output_type": "Generated literature reviews / summaries",
            "prompting_technique": "RAG with LLM reranking",
            "model_name": "",
            "model_size": "",
            "datasets_used": "",
            "evaluation_metric": "",
            "reported_results": "",
            "limitations": "Described as requiring user-provided abstracts and thus less suited for fully automated retrieval-and-generation without user curation.",
            "counterpoint": true,
            "uuid": "e7724.6",
            "source_info": {
                "paper_title": "Automated literature research and review-generation method based on large language models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "LLAssist",
            "name_full": "LLAssist (Llassist)",
            "brief_description": "Tools for automating literature screening and initial summarization stages of review preparation using LLMs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "Automated Review Generation Method Based on Large Language Models",
            "authors": "Shican Wu et al.",
            "year": 2025,
            "method_name": "LLAssist / Llassist",
            "method_description": "Provides literature screening and assistance functionality leveraging LLMs to expedite parts of the systematic review workflow.",
            "input_type": "Abstracts, titles, and possibly full texts",
            "output_type": "Screening decisions, extractive summaries",
            "prompting_technique": "",
            "model_name": "",
            "model_size": "",
            "datasets_used": "",
            "evaluation_metric": "",
            "reported_results": "",
            "limitations": "Mentioned as focused tools for specific review sub-tasks rather than full end-to-end automated review generation.",
            "counterpoint": true,
            "uuid": "e7724.7",
            "source_info": {
                "paper_title": "Automated literature research and review-generation method based on large language models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "ChatCite",
            "name_full": "ChatCite",
            "brief_description": "An LLM agent approach that emulates human workflow guidance to produce comparative literature summaries with improved citation-grounding.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "Automated Review Generation Method Based on Large Language Models",
            "authors": "Shican Wu et al.",
            "year": 2025,
            "method_name": "ChatCite",
            "method_description": "Uses human-like workflow guidance for LLM agents to improve summary quality and citation accuracy in comparative literature summarization tasks.",
            "input_type": "User-provided literature and citation metadata",
            "output_type": "Comparative summaries grounded with citations",
            "prompting_technique": "Human-workflow guided agent prompts",
            "model_name": "",
            "model_size": "",
            "datasets_used": "",
            "evaluation_metric": "",
            "reported_results": "",
            "limitations": "Cited as an advance in improving citation-grounding but still part of the class of systems that require curated inputs or guided workflows.",
            "counterpoint": true,
            "uuid": "e7724.8",
            "source_info": {
                "paper_title": "Automated literature research and review-generation method based on large language models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Self-consistency",
            "name_full": "Self-consistency aggregation",
            "brief_description": "A technique that aggregates multiple independent LLM outputs for the same query, using majority/consensus to improve correctness and suppress stochastic hallucinations.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models.",
            "mention_or_use": "use",
            "paper_title": "Automated Review Generation Method Based on Large Language Models",
            "authors": "Shican Wu et al.",
            "year": 2025,
            "method_name": "Self-consistency aggregation",
            "method_description": "Runs multiple LLM queries for the same extraction question and aggregates repeated answers (majority or consistency checks) to reduce false positives (hallucinations) in extracted facts.",
            "input_type": "Repeated LLM responses to identical prompts over retrieved text",
            "output_type": "Aggregated, higher-confidence extracted facts and answers",
            "prompting_technique": "Repeated-query aggregation, voting/consensus",
            "model_name": "",
            "model_size": "",
            "datasets_used": "",
            "evaluation_metric": "Measured improvements in aggregated accuracy and reduced false positive rates; binomial aggregation calculations used to justify repetition counts",
            "reported_results": "Authors used 5 repetitions empirically (also analyze 5/7/9 repetition theory); reported knowledge-extraction aggregated accuracy ~95.77% and estimated hallucination &lt;0.5% (95% CI) via manual sampling in that stage.",
            "limitations": "Aggregation reduces but does not eliminate hallucinations and increases computation; assumes independence of errors across repetitions.",
            "counterpoint": false,
            "uuid": "e7724.9",
            "source_info": {
                "paper_title": "Automated literature research and review-generation method based on large language models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "DAPT",
            "name_full": "Domain-Adaptive Pretraining",
            "brief_description": "A pretraining/fine-tuning approach that further trains base LLMs on domain-specific corpora to improve domain knowledge and reduce domain-specific errors.",
            "citation_title": "Don't stop pretraining: Adapt language models to domains and tasks.",
            "mention_or_use": "mention",
            "paper_title": "Automated Review Generation Method Based on Large Language Models",
            "authors": "Shican Wu et al.",
            "year": 2025,
            "method_name": "Domain-adaptive pretraining (DAPT)",
            "method_description": "Fine-tunes the language model on a corpus from the target domain so that the model better handles domain-specific terminology and reduces extraction/conversion errors.",
            "input_type": "Domain-specific text corpora (in-domain publications)",
            "output_type": "Domain-adapted LLM weights enabling more accurate extractions and generation",
            "prompting_technique": "",
            "model_name": "",
            "model_size": "",
            "datasets_used": "",
            "evaluation_metric": "",
            "reported_results": "",
            "limitations": "Authors chose not to fine-tune in order to preserve out-of-the-box multi-domain generalization; DAPT incurs additional cost but can mitigate unit/concept extraction errors in data mining.",
            "counterpoint": null,
            "uuid": "e7724.10",
            "source_info": {
                "paper_title": "Automated literature research and review-generation method based on large language models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Page Rerank",
            "name_full": "Page Rerank Algorithm (pairwise-to-absolute scoring)",
            "brief_description": "An algorithm used to convert pairwise relative comparisons into absolute scores on a fixed scale (0–10) for evaluation aggregation.",
            "citation_title": "",
            "mention_or_use": "use",
            "paper_title": "Automated Review Generation Method Based on Large Language Models",
            "authors": "Shican Wu et al.",
            "year": 2025,
            "method_name": "Page rerank algorithm",
            "method_description": "Takes pairwise LLM comparison judgments and transforms them into calibrated absolute numeric scores for paragraph-level quality evaluation.",
            "input_type": "Pairwise comparison judgments between generated and reference paragraphs",
            "output_type": "Absolute 0–10 quality scores",
            "prompting_technique": "Pairwise comparison prompts and reranking conversion",
            "model_name": "",
            "model_size": "",
            "datasets_used": "",
            "evaluation_metric": "Used as part of the dual-baseline evaluation framework together with ICC and TCR reliability tests",
            "reported_results": "Enabled conversion of LLM pairwise comparisons into absolute scores used to compare generated vs manual review paragraphs; contributed to reported model vs human comparative numbers.",
            "limitations": "Relies on reliable pairwise judgments; susceptible to evaluator bias if comparison judgments are noisy.",
            "counterpoint": null,
            "uuid": "e7724.11",
            "source_info": {
                "paper_title": "Automated literature research and review-generation method based on large language models",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Paperqa: Retrieval-augmented generative agent for scientific research.",
            "rating": 2,
            "sanitized_title": "paperqa_retrievalaugmented_generative_agent_for_scientific_research"
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledgeintensive nlp tasks.",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Language agents achieve superhuman synthesis of scientific knowledge.",
            "rating": 2,
            "sanitized_title": "language_agents_achieve_superhuman_synthesis_of_scientific_knowledge"
        },
        {
            "paper_title": "System for systematic literature review using multiple ai agents: Concept and an empirical evaluation.",
            "rating": 2,
            "sanitized_title": "system_for_systematic_literature_review_using_multiple_ai_agents_concept_and_an_empirical_evaluation"
        },
        {
            "paper_title": "LitLLM: A Toolkit for Scientific Literature Review.",
            "rating": 1,
            "sanitized_title": "litllm_a_toolkit_for_scientific_literature_review"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models.",
            "rating": 1,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Don't stop pretraining: Adapt language models to domains and tasks.",
            "rating": 1,
            "sanitized_title": "dont_stop_pretraining_adapt_language_models_to_domains_and_tasks"
        }
    ],
    "cost": 0.01897525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Automated Review Generation Method Based on Large Language Models
1 May 2025</p>
<p>Shican Wu 
School of Chemical Engineering and Technology
Collaborative Innovation Center of Chemical Science and Engineering (Tianjin)
Key Laboratory for Green Chemical Technology of Ministry of Education
Tianjin University
300072TianjinChina</p>
<p>Xiao Ma 
School of Chemical Engineering and Technology
Collaborative Innovation Center of Chemical Science and Engineering (Tianjin)
Key Laboratory for Green Chemical Technology of Ministry of Education
Tianjin University
300072TianjinChina</p>
<p>Dehui Luo 
School of Chemical Engineering and Technology
Collaborative Innovation Center of Chemical Science and Engineering (Tianjin)
Key Laboratory for Green Chemical Technology of Ministry of Education
Tianjin University
300072TianjinChina</p>
<p>Lulu Li 
School of Chemical Engineering and Technology
Collaborative Innovation Center of Chemical Science and Engineering (Tianjin)
Key Laboratory for Green Chemical Technology of Ministry of Education
Tianjin University
300072TianjinChina</p>
<p>Xiangcheng Shi 
Joint School of National University of Singapore and Tianjin University
International Campus of Tianjin University
350207Binhai New City, Fuzhou, FujianChina</p>
<p>Xin Chang 
Joint School of National University of Singapore and Tianjin University
International Campus of Tianjin University
350207Binhai New City, Fuzhou, FujianChina</p>
<p>Xiaoyun Lin 
School of Chemical Engineering and Technology
Collaborative Innovation Center of Chemical Science and Engineering (Tianjin)
Key Laboratory for Green Chemical Technology of Ministry of Education
Tianjin University
300072TianjinChina</p>
<p>Ran Luo 
Joint School of National University of Singapore and Tianjin University
International Campus of Tianjin University
350207Binhai New City, Fuzhou, FujianChina</p>
<p>Chunlei Pei 
School of Chemical Engineering and Technology
Collaborative Innovation Center of Chemical Science and Engineering (Tianjin)
Key Laboratory for Green Chemical Technology of Ministry of Education
Tianjin University
300072TianjinChina</p>
<p>Zhejiang Institute of Tianjin University Ningbo
315201ZhejiangChina</p>
<p>Changying Du 
AIStrucX Technologies
No. 26, Information Road, Haidian District100000BeĳingChina</p>
<p>Zhi-Jian Zhao zjzhao@tju.edu.cn 
School of Chemical Engineering and Technology
Collaborative Innovation Center of Chemical Science and Engineering (Tianjin)
Key Laboratory for Green Chemical Technology of Ministry of Education
Tianjin University
300072TianjinChina</p>
<p>International Joint Laboratory of Low-carbon Chemical Engineering
300192TianjinChina</p>
<p>Jinlong Gong jlgong@tju.edu.cn 
School of Chemical Engineering and Technology
Collaborative Innovation Center of Chemical Science and Engineering (Tianjin)
Key Laboratory for Green Chemical Technology of Ministry of Education
Tianjin University
300072TianjinChina</p>
<p>Joint School of National University of Singapore and Tianjin University
International Campus of Tianjin University
350207Binhai New City, Fuzhou, FujianChina</p>
<p>Zhejiang Institute of Tianjin University Ningbo
315201ZhejiangChina</p>
<p>International Joint Laboratory of Low-carbon Chemical Engineering
300192TianjinChina</p>
<p>Haihe Laboratory of Sustainable Chemical Transformations
300192TianjinChina</p>
<p>National Industry-Education Platform of Energy Storage
Tianjin University
135 Yaguan Road300350TianjinChina</p>
<p>Tianjin Normal University
300387TianjinChina</p>
<p>Automated Review Generation Method Based on Large Language Models
1 May 20254DB21C7AD76586CF6B82D3C2D7169717arXiv:2407.20906v5[cs.CL]large language modelsautomated review generationliterature analysisscientific writing
Literature research, vital for scientific work, faces the challenge of surging information volumes exceeding researchers' processing capabilities.We present an automated review generation method based on large language models (LLMs) to overcome efficiency bottlenecks and reduce cognitive load.Our statistically validated evaluation framework demonstrates that the generated reviews match or exceed manual quality, offering broad applicability across research fields without requiring users' domain knowledge.Applied to propane dehydrogenation (PDH) catalysts, our method swiftly analyzed 343 articles, averaging seconds per article per LLM account, producing comprehensive reviews spanning 35 topics, with extended analysis of 1041 articles providing insights into catalysts' properties.Through multi-layered quality control, we effectively mitigated LLMs' hallucinations, with expert verification confirming accuracy and citation integrity while demonstrating hallucination risks reduced to below 0.5% with 95% confidence.Released Windows application enables one-click review generation, enhancing research productivity and literature recommendation efficiency while setting the stage for broader scientific explorations.</p>
<p>INTRODUCTION</p>
<p>Peer-reviewed academic literature functions as a critical medium for scientific knowledge dissemination, enabling researchers to advance human understanding through cumulative progress [1].The clarity and rigor of scientific language facilitates entity description, concept extraction, and consensus building, ensuring cognitive consistency between information senders and receivers.However, the exponential growth in publications has exceeded researcher' processing capacity [2,3,4], necessitating efficient tools for literature analysis and integration, thus avoiding redundant discoveries and broadening research perspectives.</p>
<p>Natural language processing (NLP), encompassing co-reference resolution, semantic analysis, etc. [5], powers literature comprehension.Since November 2022, Large Language Models like ChatGPT, as the latest NLP advancement, have exhibited unprecedented language understanding [6].Leading LLMs have surpassed human performance on various benchmarks including MMLU [7], which tests undergraduate knowledge, and GPQA Diamond [8], which assesses graduate-level reasoning, positioning them as potential "second brains" for efficient literature processing [6,9].Recent studies like PaperQA [10] and its improved version PaperQA2 [11], as retrieval-augmented generation (RAG [12]) agents, demonstrate excellent performance in literature-related tasks including retrieval, question-answering, summarization, and contradiction detection, surpassing human expertise in some aspects.AcademicGPT [13] provides comprehensive research support, while CuriousLLM [14] enhances multi-document questionanswering through reasoning-based traversal agents.However, these applications require user-provided literature, rely on question-answer interactions, or focus on specific points, limiting their transferability.</p>
<p>The review format effectively integrates literature information and generalizes across disciplinary fields, naturally leading to automated review generation research.However, early attempts encountered several limitations.They either reduced reviews to multi-document summarization [15] or depended on existing reviews and citation networks [16,17,18,19,20], which struggled with rapidly evolving fields and underestimated recent publications due to citation lag.Additional constraints included using only abstracts instead of full texts as input data [17,18,19,20] and employing either extractive summarization rather than integrated generation [16,17,18] or template-based generation [19], risking information loss and redundancy.Recent LLM-based solutions include: multi-AI agent systems [21] for full-process automation from research question generation to data extraction; LitLLM [22] combining RAG with LLM reranking to generate high-quality literature reviews based on user-provided abstracts; LLAssist [23] and related work [24] for literature screening; and ChatCite [25] improving summary quality through human-like workflows.These advances enhance automated review generation while enabling efficient academic research.</p>
<p>Based on the potential of LLMs, this study proposes an automated review generation method based on LLMs, builds an end-to-end data pipeline from literature retrieval to final review text generation.By leveraging information refinement and knowledge construction capabilities of LLMs, this method overcomes human cognitive limitations in single-threaded processing and memory capacity, reducing researchers' cognitive load while offering superior speed and scalability, thereby substantially conserving professional human resources.However, two critical challenges persist: the macro-level requirement for systematic quality evaluation and comparison with manual reviews, and the micro-level necessity to effectively mitigate LLM hallucinations.To address these challenges, we designed a dual-baseline automatic evaluation framework with rigorous statistical validation, alongside multi-level quality control strategies throughout the process.The distinctive feature of method lies in its adaptability to diverse disciplinary terminologies and knowledge structures without domain-specific training, facilitating both comprehensive field overviews for experienced researchers and accessible entry points for those lacking relevant background, opening up new possibilities for promoting interdisciplinary research and knowledge dissemination.This approach holds substantial scientific significance by enhancing literature processing efficiency, fostering knowledge discovery, and stimulating innovation.Its ability to promote interdisciplinary communication and knowledge integration positions it as a potential cornerstone of modern research infrastructure, accelerating scientific discovery and technological advancement across domains.</p>
<p>RESULTS AND DISCUSSION</p>
<p>Automated retrieval</p>
<p>Automated review generation fundamentally relies on retrieving and extracting scientific literature, with output quality dependent on input timeliness, quality, and breadth.To demonstrate cross-disciplinary generalization without human intervention, we conducted a case study on propane dehydrogenation (PDH) catalysts, searching chemistry and chemical engineering journals (1980-2024) ranked Q1 in the Chinese Academy of Sciences journal classification on Google Scholar through SerpAPI.</p>
<p>The automated retrieval yielded 1420 initial results from Google Scholar.To address the challenge of irrelevant or duplicate findings, we implemented a dual-level filtering process.The first level employed quick filtering of abstracts and titles to remove obviously irrelevant documents, as detailed in Method section, serving as a rapid but less precise narrowing method.The second level involved deeper LLM-based analysis of full texts, offering higher accuracy albeit at a slower pace.This coarse-to-fine screening method, reminiscent of high-throughput screening, enabled us to identify literature pertinent efficiently and accurately to our research.The initial screening shortlisted 343 articles as related to our topic.Subsequent LLM evaluation further confirmed 238 of these articles as relevant.</p>
<p>Implementation and analysis of one-click automated review generation</p>
<p>Using PDH catalysts as an example and building on the aforementioned automated retrieval, we have effectively produced high-quality, specialized review articles.Considering that the entire process is completely end-to-end without the need for human intervention, we believe that a single domain example is sufficient to demonstrate the applicability of this method.The main reason for limiting the journal range to Q1 journals is that although the impact factor of journals may not be closely related to the quality of articles, the lower limit of literature in Q1 journals that have passed strict peer review is relatively higher.Considering users lacking prior knowledge in the target domain, directly traversing Q1 chemistry journals provides an efficient starting point.We solemnly declare here that we are not encouraging users to only consider Q1 journals, but rather suggesting that in the initial stage, one can consider starting with Q1 journals, and for research on the entire field, all possibilities should be explored, which is also supported by our method.In the Windows GUI we provide, using Q1/Q2-3 journals is an optional button, allowing users to choose for themselves.For those with domain familiarity, the program allows the specification of a custom journal list to refine article selection.</p>
<p>We evaluated two topic construction strategies: based on existing reviews (9 topics, 35 questions, 125 citations) and direct LLM generation (12 topics, 12 questions, 43 citations).The examples showcased in subsequent sections and SI are based on outlines derived from existing reviews.The content has been manually checked by experts in the relevant field, with no errors in knowledge, correct referencing of cited literature, and a length and citation count that align with conventional review standards (see SI).The method's effectiveness stems from LLMs' human-level or superior language comprehension abilities, coupled with the injection of domain knowledge from retrieved literature through context window, thereby enabling generalization across all research fields.Beyond content accuracy, the method enables customizable research focus through supporting of adding specific questions and provides forward-looking insights with comprehensive understanding sections.To facilitate broad adoption, we developed an open-source Python3 GUI enabling one-click review generation without programming expertise or domain knowledge.</p>
<p>Evaluation of generated review quality</p>
<p>Research demonstrates LLMs excel in evaluation tasks, with GPT-4 surpassing both crowdsourced workers [26] and experts [27] in text annotation accuracy and reliability, and bias control for complex tasks requiring contextual knowledge reasoning [27].LLMs show comparable or superior performance to human annotation in persuasiveness, accuracy, and satisfaction [28].Studies confirm the capability of LLMs to evaluate other LLMs, with verification abilities improving faster than generation quality [29].GPT-4's evaluations exceeds 80% consistency with human reviewers [30] and exceeds 85% alignment in pairwise comparisons [31], reaching nearly 100% agreement when performance differences are significant [31].Self-verification benefits LLM performance [32], though biases issues [33], such as position bias [34], length bias [30], and self-bias [35], persist but can be reduced through proper design [36].</p>
<p>Given characteristic writing patterns of LLMs and potential human evaluation bias, this study employs LLM-based evaluation exclusively.Existing LLM-based LLM evaluation methods encompass scoring, comparison, selection judgment, and comprehensive description.This study introduces a dual-baseline review quality evaluation framework, to minimize potential LLM evaluation bias and quantitatively compare LLM-generated reviews with peer-reviewed expert-written content, validating reliability through statistical analysis.In our method, we segmented 14 published Q1 reviews into 89 fragments based on semantic content.Using extracted topics from these fragments and the literature cited in the original text, we generated comparative reviews using Qwen2-7b-Instruct, Qwen2-72b-Instruct, and Claude3.5Sonnet.This methodology enabled direct comparison between human experts and LLMs in writing reviews with identical literature background, establishing a rigorous benchmark for LLMs given their limitations in accessing both human accumulated domain expertise and pre-1970 undigitized literature.</p>
<p>In the evaluation process, we compared the performance of two models of different scales from the open-source Qwen2 series (Qwen2-7b-Instruct and Qwen2-72b-Instruct) and the closed-source model Claude3.5Sonnet(Fig. 1).Evaluation employed both selfscoring and uniform scoring by Qwen2-72b-Instruct.Intraclass Correlation Coefficient (ICC) tests and Transitive Consistency Ratio (TCR) analyses confirmed high reliability for Claude3.5Sonnet (Fig. 1(a,b,e,f)) and Qwen2-72b-Instruct (Fig. 1(c,d,e,f)), meeting human evaluation standards, while Qwen2-7b-Instruct's results necessitated substitution with Qwen2-72b-Instruct's scoring due to insufficient reliability.This phenomenon may stem from differences in capabilities between LLMs of different scales, specifically manifested in three dimensions: world knowledge, language understanding, and logical reasoning.We believe that small models differ significantly from large models in logical reasoning ability, while world knowledge has been supplemented through context-provided literature, and language ability differences are relatively small.For details on Qwen2-7b-Instruct's evaluation results, see SI. Model capability significantly impacts generation quality, while our method ensures a basic lower limit of generation quality.In repeated generation tests (9 times per model), average scores, taken as comprehensive performance of models, showed that Qwen2-7b-Instruct reached 43.94% of manual scoring, Qwen2-72b-Instruct reached 64.81% (Fig. 2(d,f)), and Claude3.5Sonnetexceeding manual scores by 23.63% (Fig. 2(c,f)), all significantly higher than the baseline level of direct generation.Optimal performance analysis, which taking the paragraph with the highest total score among those generated by each model as the optimal paragraph, revealed best-paragraph scores of 89.07%for Qwen2-7b-Instruct, 92.64% for Qwen2-72b-Instruct (Fig. 2(b,f)), and 130.79% for Claude3.5Sonnetrelative to manual scores (Fig. 2(a,f)).These results indicate multiple generations and selecting the best paragraph can improve performance of smaller models, while larger models maintain consistent quality.For optimal results, we recommend using larger models when possible; otherwise, multiple generations can enhance final performance in hardware-constrained scenarios.For details on Qwen2-7b-Instruct's evaluation results, see SI.The near-human scores of optimal paragraphs might reflect quality-related bias [37] because their closely approximate human-level quality could make LLMs' potential evaluation bias more prominent, while the distinctly inferior quality of direct generation remains easily distinguishable.Analysis of optimal paragraphs reveals high correlations between Qwen2-7B and Qwen2-72B across evaluation sub-items (0.926 for highest scores, 0.939 for scores relative to human benchmarks), reflecting the scaling law of LLM and suggesting potential for further improvements with model advancement (see SI for details).Our research validates the effectiveness of LLM-based automated review generation, with quality approaching or exceeding manual reviews.The effectiveness of method stems from general language processing and context learning capabilities of LLMs, rather than requiring specific domain expertise, suggests broad disciplinary applicability.While inheriting common biases and requiring readers' professional judgment, this method serves as a supportive rather than a replacement tool for human innovation, with open-source models showing comparable capabilities to closed-source alternatives.The approach demonstrates broad cross-disciplinary potential, promising to become an important tool for promoting academic innovation and knowledge dissemination, while the dual-baseline framework offers potential methodology for evaluating LLM agent workflows where manual data acquisition is costly.</p>
<p>Data mining and visual analysis</p>
<p>Catalysts play a vital role in chemical process optimization [38], with data mining enabling accelerated design through pattern recognition [39].Analyzing 839 PDH catalyst papers from a total of 1041 articles filtered by abstracts and titles in Q1-Q3 chemistry journals (1980-2024), our data mining module revealed comprehensive insights into catalysts' composition, structure, and performance.</p>
<p>For instance, publication analysis showed surging alloy research since 1995 and single-atom catalyst studies post-2015 (Fig. 3a), driven by advancements in structural composition (Fig. 3b).Performance analysis identified optimal promoter elements (Zn, Sn, La) (Fig. 3c) and support materials (alumina, zeolites) (Fig. 3d), while combination studies revealed superior performance in multi-metal systems, particularly Pt-based catalysts with Sn, Zn, In promoters (Fig. 3e).Moreover, impregnationprepared nanometallic catalysts demonstrated high conversion rates and selectivity, contrasting with single-atom alloys' high selectivity but lower conversion rates (Fig. 3f).</p>
<p>This comprehensive analysis reveals variable interactions and guides catalyst optimization, recommending Pt-based systems for selectivity and metal oxides for conversion rates, while highlighting the promise of single-atom and nanostructured catalysts.These findings not only establish performance benchmarks in PDH catalysis, but also demonstrate how our LLM-based methodology enables real-time scientific insight ex-traction, facilitating industrial-oriented catalyst design optimization.Complete data charts are available in the SI.</p>
<p>Hallucination mitigation</p>
<p>Unlike search engines, LLMs' process of understanding information and outputting it anew provides LLMs' creativity while inevitably accompanying the "hallucination" phenomenon, referring to false information generated by LLMs without sufficient evidence support or contextually inconsistent, off-input information responses [40].LLM hallucinations mainly originate from statistical biases in the training phase, noise in training data, and decision strategies when handling uncertain or multi-interpretable information during the alignment phase [40].Currently, there is no solution within the field [9,40,41], and research suggests that hallucinations are unavoidable [42,43].Especially in specialized sub-fields, LLMs greatly exacerbate the hallucination phenomenon due to extremely scarce data exposure.For example, tests in literature [44] show that even the most advanced GPT-4 only has a 73.3% accuracy rate when answering professional multiple-choice questions, which is far from sufficient for scientific research fields that strictly require correctness.In scientific research, this may lead to unrealistic academic conclusions and misleading research directions, often meaning great waste of time and material consumption [41].Therefore, effective hallucination mitigation is crucial for ensuring the scientific nature and reliability of automated review generation.</p>
<p>To address the challenge of hallucinations in LLMs, a high priority has been placed on the detection and prevention of such phenomena.In the entire automated review generation process, we adopted a multi-level filtering and verification quality control strategy, similar to the concept of retrieval-augmented generation (RAG) [12,45], to mitigate and correct hallucinations:</p>
<p>Prompt design and task decomposition.Firstly, we utilized strict and clear text summary guiding prompts, aimed at enhancing the scientific rationality of LLM's outputs and ensuring accuracy and reliability in its analysis and generation processes.Notably, the task of automated review generation aligns well with the strengths of LLMs-information extraction and text generation capabilities.LLMs can rapidly and accurately extract core information from a vast array of literature and integrate it into a coherent and rigorous review text.To enhance efficiency and quality, we deconstructed the core of the review writing process, namely literature reading and summarization, into a series of text summarization tasks.This approach is adopted because summaries generated by LLM significantly surpass manually crafted and fine-tuned model-generated summaries in terms of fluency, factual consistency, and flexibility [46].By establishing a list of questions, we directed the model to extract relevant content from the literature and respond based on this content, subsequently conducting a comprehensive analysis of all literature citations and responses.Ultimately, the LLM generates high-quality paragraphs closely related to the topic.Additionally, we employed a single-round, segmented generation strategy to avoid truncation limitations of approximately 8K output length.By reasonably segmenting long texts for generation, we not only ensured that the output was completed in a single conversational round but also provided finer parallel granularity to improve generation efficiency.In practice, we divided the 35 questions into 5 groups, ensuring that the generation results for each group could be successfully completed within the 8K limit of the LLM.This granularity avoids efficiency drops due to a high proportion of shared content and identical prompt frameworks, thereby enhancing processing speed while ensuring the quality of text generation.</p>
<p>Hallucination filtering and verification.To mitigate and rectify hallucinations, we employed a layered filtering and verification approach:</p>
<p>1.Text format filtering: Noting that hallucinations often disrupt text formatting, we applied a predefined XML format template to filter out disarrayed texts.</p>
<ol>
<li>
<p>DOI verification: DOIs, a combination of symbols and numbers lacking direct semantic linkage to context, present a challenge in generation and are prone to hallucinations.Yet, the precise reference nature of DOIs allows for verification.Through strict DOI verifications on generated content, we suppressed hallucinatory content from advancing further, ensuring each generated conclusion is traceable to its original source.</p>
</li>
<li>
<p>Relevance verification: Within the RAG system, documents related in semantics but lacking correct answers are particularly detrimental [47].We scrutinized each response in the knowledge extraction phase to ensure its relevance, eliminating off-topic answers with relevant keywords.</p>
</li>
<li>
<p>Self-consistency [48] verification: For text summarization, where a definitive correct answer exists, recognizing that the stochasticity of hallucinations means correct answers should recur more frequently across iterations, we employ aggregation from repeated queries to effectively suppress hallucinations.</p>
</li>
<li>
<p>Full data stream traceability mechanism: By using DOIs as key reference identifiers for each piece of generated content and mandating citations for every conclusion, we enable review readers to easily trace back to the original literature, supporting verification and deeper exploration in topics of interest.</p>
</li>
</ol>
<p>Effectiveness of hallucination mitigation.In evaluating the effectiveness of hallucination mitigation, we employed a confusion matrix to classify outcomes according to whether the LLM provided content and its pertinence to the original text, differentiating between two types of inaccuracies: false positives, which include fabricated or inconsistent information, and false negatives, referring to overlooked or partially extracted content.Our focus was primarily on reducing false positives, while adopting a relatively tolerant stance on false negatives.</p>
<p>Substantial progress was made in mitigating hallucinations.Specifically, in the paragraph generation part, which was achieved through 9 repetitions of 35-paragraph generation tasks, a total of 315 paragraphs that passed format checking and DOI checking were needed.Throughout the entire paragraph generation process, statistics show that LLM cumulatively performed 875 generations, of which only 36% of generation results passed after format and DOI list checks.In the analysis involving 343 topic-related literature, we divided 35 questions into 7 questions per segment for each literature, i.e., 5 segments per literature, totaling 1715 knowledge extractions.By conducting 5 repeated questions in each knowledge extraction, we obtained a total of 8575 answers, and finally aggregated 2783 effective information combinations after excluding answers unrelated to the literature and questions.Among these, up to 84.80% of the results were judged by LLM to have a 100% consistency rate when compared with the aggregated results (Table 1 and Fig. 4a), thus verifying the model's stability.For specific methods, see the Methods section.This method also provides a rough standard for judging the proportion of hallucinations, which can be used in the screening and evaluation of LLMs.</p>
<p>To assess the effectiveness of the knowledge extraction and data mining stages, we implemented a rigorous manual verification process.Specifically, 25 randomly selected articles from each stage were evaluated by a third-year PhD student specializing in PDH research.For the knowledge extraction stage, 35 segments per article were examined, totaling 875 data points.The data mining stage assessed 14 catalyst properties, including 5 direct answer repetitions and final generated results, encompassing 1750 and 350 data points respectively.We employed precise classification criteria for the evaluation.</p>
<p>In the knowledge extraction phase, true negatives (TN) were instances where the article did not address the guiding question and the LLM correctly identified it as irrelevant.True positives (TP) occurred when relevant topics were accurately extracted.False negatives (FN) were cases where relevant topics were incompletely extracted or incorrectly deemed irrelevant.False positives (FP) included irrelevant topics mistakenly identified as relevant or extractions that exceeded or deviated from the article's actual content.Similar criteria were applied to the data mining stage, with particular attention to unit conversion errors, which were classified as false positives even if numerical values were correct.Consistency comparisons were conducted using the Claude2 model through designed prompt templates, comparing pre-and post-aggregation texts and statistically analyzing the model's scoring results.Based on these evaluations, we calculated key metrics including accuracy, false positive rate (with 95% confidence intervals), precision, recall, F1 score, and consistency rate (Table 1).Confidence intervals for false positive rates were computed using Python3's statsmodels library.For detailed results and calculation methods, see SI.</p>
<p>It is crucial to emphasize that this manual verification step was conducted to demonstrate the method's effectiveness during the proof-of-concept phase and is not required in the actual automated review generation process.The detailed results are presented in Table 1.The data comparison underscores the efficacy of self-consistency verifications, revealing a substantial decrease in hallucinations, i.e., false positive content, while also compensating for some false negatives, where information was not fully extracted (Fig. 4b).In the knowledge extraction phase, critical for review content, our manual sampling found no fabricated conclusions by LLMs (Fig. 4a), attesting to our method's scientific integrity and reliability.From the sampling results, we are over 95% confident that the likelihood of hallucinations in this part is less than 0.5% (Table 1), which is also the source of our confidence that this method supports fully automated processes without manual intervention.Analysis of false positives in the post-aggregation data mining phase revealed hallucinations typically involved correct numerical extraction but with errors in units or definitions.False negatives mainly stemmed from LLMs' inability to comprehend highly abstract expressions, reflecting a general LLM's limited understanding of highly specialized scientific concepts.The incidence of hallucinations in knowledge extraction was significantly lower than in data mining, as answering questions did not involve converting units and concepts, thus avoiding the most challenging part of testing an LLM's grasp of scientific knowledge.Domain-specific models enhanced by domain-adaptive pretraining (DAPT) [49] are poised to mitigate this issue.Opting not to fine-tune LLMs for specific domains in this study prioritizes out-of-thebox functionality and multi-domain generalization, utilizing a general LLM as the base.Comparisons between RAG and fine-tuning effects in specific domains indicate that RAG sustains efficacy with contextually new knowledge and offers a significantly lower initial cost [50], aligning with our objective to support researchers' entry into diverse fields efficiently.</p>
<p>Considering the stringent accuracy requirements in research, increasing the number of repetitions can significantly reduce the probability of hallucinations appearing in aggregated results.Binomial probability calculations indicate that theoretically, a model with 79.09% accuracy yields aggregated prediction accuracies of 93.49%, 96.12%, and 97.64% after five, seven, and nine independent predictions, respectively, aligning with our sampling results (Table 1).Detailed sampling outcomes and calculations are available in the SI.We believe that 5 repetitions is an ideal empirical value, and users do not need to change this parameter when using it.On this foundation, every conclusive description in the generated reviews is supported by literature references and has been verified by relevant field researchers through tracing the cited literature, confirming that all literature references are correctly linked to the original publications and that the descriptions in the generated reviews correspond to those in the original publications.</p>
<p>This multi-layered strategy for hallucination control has built an effective verification system, ensuring the scientific integrity and reliability of the automated review generation.Furthermore, through a full data stream traceability mechanism, the authenticity and practicality of the content are further strengthened.This not only provides a secondary means of hallucination mitigation but also allows researchers to delve into original research papers for more precise and detailed academic information while accessing fast, automated research reviews.The strategy also implements a kind of literature recommendation mechanism.Since each content segment includes related DOIs, researchers can quickly locate specific original literature based on their interests and research needs, enabling deeper academic exploration.</p>
<p>While both our method and RAG utilize LLM's context learning ability, our approach fundamentally differs by achieving systematic knowledge reconstruction through multi-stage processing rather than simple retrieval combination.This method simulates the complete academic research process and produces coherent knowledge frame-works aligned with scholarly thinking, surpassing traditional RAG's question-answering paradigm through comprehensive quality control and hallucination mitigation mechanisms.</p>
<p>CONCLUSIONS</p>
<p>In this study, we introduce an innovative LLM-based automated review generation method, addressing two fundamental challenges in scientific research: improving literature review efficiency and mitigating LLM hallucination risks.Through proposing an evaluation framework that ensures the objectivity and reliability via statistical validation, and innovatively compared LLM-generated reviews with high-quality manual reviews, we demonstrate that our modular end-to-end approach produces reviews comparable to or exceeding human-written ones, while maintaining high reliability and traceability.Expert evaluation using PDH catalysts as a case study confirms the method's effectiveness: generated reviews are comparable to manual reviews in length and citations, show no hallucinations, and have impeccable reference accuracy.Statistical validation confirms the method's effectiveness in hallucination reduction, with testing on 875 LLM outputs from 25 random articles showing hallucination probability below 0.5% at 95% confidence.The quality assurance pipeline ensures robust data processing.Additionally, our advanced data mining module offers experienced users' in-depth field integrated perception, fully exploiting LLMs' analytical capabilities.Furthermore, an open-source user-friendly one-click program developed for Windows platforms significantly simplifies the review generation process.</p>
<p>The method's architecture offers significant advantages through its crossdisciplinary applicability without manual intervention or domain-specific knowledge injection.Its modular design enables component reuse for literature tracking, topic discovery, and dataset construction, while achieving cross-disciplinary generalization through LLM's inherent contextual adaptability.This means that by providing corresponding domain literature input, the method can generate high-quality reviews across various disciplines.Future development will focus on enhanced multimodal processing capabilities, automated scientific question generation and answering, personalized text generation, integration with existing academic tools, and domain-specific features for structured data analysis.</p>
<p>This advancement heralds a new era in human-machine academic collaboration, offering broad prospects for LLMs as writing assistance tools.While not intended to replace traditional manual reviews, our method serves as a powerful auxiliary tool for rapid domain overview and research hotspot identification, laying the foundation for in-depth analysis.Beyond its demonstrated excellence in chemistry, the method's technical framework exhibits remarkable cross-disciplinary applicability, potentially breaking down barriers between fields and catalyzing interdisciplinary innovation.By revolutionizing researcher-literature interaction and accelerating knowledge dissemination, this milestone advancement holds profound implications for knowledge base construction, literature recommendation, and structured academic writing, heralding a new era of scientific research productivity and interdisciplinary collaboration.</p>
<p>METHODS</p>
<p>Our method consists of four core components: literature search, topic formulation, knowledge extraction and review composition, along with a data mining module (Fig. 5a) and quality assessment framework (Fig. 5b).All prompt templates are available in SI and GitHub without requiring user adjustment.</p>
<p>Literature search</p>
<p>Literature retrieval begins with journal selection from journal classification tables, followed by an API-based keyword search and preliminary title/abstract filtering using a keyword list, with review-type literature marked separately (Fig. 5a, i).</p>
<p>Topic formulation</p>
<p>Review topics can be constructed either through direct LLM outline drafting or through LLM extraction and refinement of existing literature reviews (Fig. 5a, ii).After obtaining a list of topics, additional topics can be manually added and sorted as needed.This manual addition is not mandatory but provides an interface for advanced users to intervene if necessary.</p>
<p>Knowledge extraction</p>
<p>Based on the topic list, LLM generates extraction questions and conducts multiple rounds of information retrieval from each article.The LLM evaluates answer relevance to questions using structured prompts, where combinations of questions, LLM-aggregated relevant answers, and their corresponding citations constitute valid information combinations for subsequent processing.For literature exceeding the context window of LLM, the text is segmented into approximately equal parts, processed separately, and results are integrated during answer aggregation (Fig. 5a, iii).</p>
<p>Review composition</p>
<p>Extracted answers are associated with source DOIs and integrated into topicspecific paragraphs.Through multiple iterations and LLM scoring, optimal versions are selected to form the preliminary draft, followed by citation verification and format standardization.For answers exceeding context window length, LLM performs compression based on referenced texts and extracted answers until fitting within window limits (Fig. 5a, iv).</p>
<p>Data mining</p>
<p>The data mining module extends knowledge extraction (Fig. 5a, iii) capabilities for specific data extraction and aggregation, enables extraction of user-defined targets (e.g., catalyst types, compositions, performance metrics) from literature.The LLM performs multiple rounds of parsing and extraction in XML format, followed by result aggregation.The extracted data undergoes standardization and cleaning, with GPT4-generated code facilitating statistical analysis and visualization, requiring no programming expertise from users.</p>
<p>Quality Assessment</p>
<p>The evaluation framework employs dual baselines using manual Q1 journal reviews and direct LLM generation for quality assessment (Fig. 5b).High-quality reviews are semantically segmented, with corresponding content regenerated using our method and compared against direct LLM-generated content.Assessment utilizes chain-ofthought prompts across 27 scoring items in 9 categories, implementing cross-evaluation and repetition strategies to mitigate bias.The page rerank algorithm converts relative comparisons to absolute scores on a 0-10 scale, with framework reliability validated through intraclass correlation coefficient (ICC) tests and transitive consistency ratio (TCR) analyses.our data processing work, except for the evaluation section, was completed prior to November 21, 2023, utilizing the then-latest available Claude 2 API version [https://www.anthropic.com/news/claude-2,https://www.anthropic.com/news/claude-2-1].Anthropic did not publish specific minor version numbers within the Claude 2 series, only distinguishing between Claude 2, Claude 2.1, and the subsequent Claude 3 series.Our proposed framework demonstrates good adaptability, with overall effectiveness increasing as the performance of the underlying model improves.This characteristic has been amply demonstrated in our evaluation work, indicating that the framework's efficacy is not strictly dependent on any particular model version.The use of different LLMs in the Evaluation of generated review quality section was primarily to assess the performance of the latest and most powerful open-source and closed-source models (as of September 2024) under the method described in this paper.</p>
<p>Our published graphical user interface (GUI) leverages certain APIs for functionality, which, due to legal and regulatory requirements, necessitate that users provide their own API keys.This requirement is detailed in the documentation accompanying the code repository to assist users in setting up and utilizing the GUI effectively.</p>
<p>Figure 1 :
1
Figure 1: Reliability verification results of the dual-baseline review quality assess-</p>
<p>Figure 2 :
2
Figure 2: Quality assessment results of automatically generated reviews.Heat map of the percentage difference in scores of review paragraphs generated by this method relative to human scores, red to green showing -100% to +100% range, higher values indicate better performance, values truncated to ±100% range, values exceeding are recorded as -100% and +100%: a, Highest scoring paragraph of Claude3.5 Sonnet model; b, Highest scoring paragraph of Qwen2-72b-Instruct model; c, Average paragraph score of Claude3.5 Sonnet model; d, Average paragraph score of Qwen2-72b-Instruct model.e, Histogram of percentage differences in scores relative to human scores for highest scoring paragraphs, average paragraph scores, and directly generated paragraph scores without going through this method for Claude3.5 Sonnet model and Qwen2-72b-Instruct model, colors ranging from dark to light representing Claude3.5</p>
<p>Figure 3 :
3
Figure 3: Example of visual analysis results.Line charts for annual publication numbers: a, different catalyst types; b, Performance enhancement sources.Radar charts for peak performance of single factors, with selectivity (black) and stability (purple) scales: c, Promoter elements; d, Support materials.Bubble charts for dual-variable correlations, show selectivity (color depth), conversion rate (bubble size), and stability (bubble edge thickness), aiming for high selectivity, conversion rate, and stability.Data includes only those with selectivity ≥85%, conversion rate ≥45%, stability ≥1h: e, Active site element-composition element; f, Alloy structure type-preparation method.Complete data charts are available in the SI.</p>
<p>4 :
4
Effectiveness of hallucination mitigation.a, Consistency as determined by LLMs between direct LLM responses and aggregated results during the knowledge extraction phase, where blue represents 100% consistency and orange less than 100%.b, Distribution of manual sampling results for direct LLM responses and aggregated outcomes during the data mining phase, with TP (True Positive), TN (True Negative), FP (False Positive), FN (False Negative)</p>
<p>a b 5 :
5
a, Flowchart of the automated review generation method based on large language models.It includes four modules: i) literature search, ii) topic formulation, iii) knowledge extraction, iv) review composition, as well as an additional data mining module.b, Flowchart of the quality assessment framework for review generation based on large language models.</p>
<p>Table 1 :
1
Comparison of results before and after self-consistency aggregation
StageData PointsAccu-racyFalse Positive95%CI of FPRPreci-sionRecallF1 ScoreConsist -encyRateKnowledge Extraction (Aggregated)87595.77% 0.000%0.000% -0.485%100.0% 57.47% 72.99% 84.80%Data Mining (Direct Response)175079.09% 35.34%31.45% -39.42%84.14% 85.68% 84.90%86.60%12.20%Data Mining35093.71% 18.75%-93.28% 98.43% 95.79%(Aggregated)27.70%</p>
<p>Online database Abstracts Correlation test highly correlated papers Local database LLM Direct Review Topics Creation LLM Literature-based Review Topics Creation LLM for Literature Answer Extraction LLM for Aggregating Literature Answers LLM for Paragraph Construction LLM for Full-text Refinement LLM for Paragraph Scoring
Key-wordsAPI1. Automated Literature Search2. Automated Topic Formulation4. Automated Review Composition3. Automated Knowledge ExtractionSegment peer-Extract uniquereviewed, published expert reviewstopics for each segment using LLMLLM-basedValidate reliability via ICC and transitivity consistency testsComparative Assessment Framework for Review GenerationRegenerate reviews based on original cited literature by LLMQualityPairwise compareConvert scores to 0-10 scale using Page Rerankgenerated reviews, original reviews, anddirect LLM output
CONFLICT OF INTEREST STATEMENTThe authors declare no competing interests.ACKNOWLEDGMENTWe acknowledge the Natural Science Foundation of China (No. 22121004), the Haihe Laboratory of Sustainable Chemical Transformations, the Program of Introducing Talents of Discipline to Universities (BP0618007) and the XPLORER PRIZE for financial support.We also acknowledge generous computing resources at High Performance Computing Center of Tianjin University.AUTHOR CONTRIBUTIONSJ.G and Z.Z.conceived and supervised the project.S.W. and X.M. designed the research and developed the program.S.W. led the manuscript preparation.D.L. performed manual verification of hallucination.L.L., X.S., and X.C. advanced the integration of computational models, while X.L., R.L., C.P. and C.D. advanced the catalytic science.All authors contributed to writing and revising the manuscript.DATA AVAILABILITYOur study leverages a dataset compiled from scientific literature acquired through our institution's subscription.Due to copyright considerations, the dataset itself cannot be made publicly available.However, we ensure that our research's integrity and reproducibility do not rely on direct access to these proprietary documents.Instead, we provide extensive documentation on the dataset's structure, the criteria used for literature selection, and the analysis methods applied, enabling interested researchers to reconstruct a similar dataset from publicly available resources or their institutional subscriptions.Furthermore, to facilitate a deeper understanding of our research process and promote further exploration and innovation, we have made all intermediate data, excluding the copyrighted full-text articles, publicly available on GitHub [https://github.com/TJU-ECAT-AI/AutomaticReviewGenerationData].This repository includes the prompts used in our study and the corresponding responses generated by the large language model.By sharing these resources, we aim to provide valuable insights into our methodology and encourage other researchers to build upon our work, advancing the field of natural language processing and its applications in scientific literature analysis.CODE AVAILABILITYThe custom code developed for this research is central to our conclusions and is made available to ensure transparency and reproducibility of our results.The codebase, including all relevant custom scripts and mathematical algorithms, has been opensourced under the Apache 2.0 license and is accessible via our GitHub repository at [https://github.com/TJU-ECAT-AI/AutomaticReviewGeneration].We encourage users to review the license for any usage restrictions that may apply.As stated in the text, all LLMs invoked in this article are Claude 2, except for the Evaluation of generated review quality section which uses Claude 3.5 Sonnet, Qwen2-72b-Instruct, and Qwen2-7b-Instruct.It is important to note thatADDITIONAL INFORMATIONSupplementary information is available for this paper.Correspondence and requests for materials should be addressed to J.G.
Literature reviews: modern methods for investigating scientific and technological knowledge. Apc Ermel, D P Lacerda, Miw Morandi, L Gauss, 2021Springer Nature</p>
<p>Free online availability substantially increases a paper's impact. Nature. S Lawrence, May 31 2001411521</p>
<p>Speed reading: scientists are struggling to make sense of the expanding scientific literature. Corie Lok asks whether computational tools can do the hard work for them. C Lok, Nature. 46372802010</p>
<p>Recent Advances in Lead Chemisorption for Perovskite Solar Cells. P F Wu, F Zhang, Oct 202228Transactions of Tianjin University</p>
<p>Natural language processing: state of the art, current trends and challenges. D Khurana, A Koli, K Khatter, S Singh, Multimed Tools Appl. 8232023</p>
<p>Summary of chatgpt-related research and perspective towards the future of large language models. Y Liu, T Han, S Ma, Meta-Radiology. 1000172023</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, arXiv:2009033002020arXiv preprint</p>
<p>D Rein, B L Hou, A C Stickland, arXiv:231112022A graduate-level google-proof q&amp;a benchmark. 2023arXiv preprint</p>
<p>The future of chemistry is language. A D White, Nature Reviews Chemistry. 77Jul 2023</p>
<p>Paperqa: Retrieval-augmented generative agent for scientific research. J Lála, O 'donoghue, O Shtedritski, A Cox, S Rodriques, S G White, A D , arXiv:2312075592023arXiv preprint</p>
<p>Language agents achieve superhuman synthesis of scientific knowledge. M D Skarlinski, S Cox, J M Laurent, arXiv:2409137402024arXiv preprint</p>
<p>Retrieval-augmented generation for knowledgeintensive nlp tasks. P Lewis, E Perez, A Piktus, Advances in Neural Information Processing Systems. 332020</p>
<p>S Wei, X Xu, X Qi, arXiv:231112315Empowering Academic Research. 2023arXiv preprint</p>
<p>Z Yang, Z Zhu, Curiousllm, arXiv:240409077Elevating Multi-Document QA with Reasoning-Infused Knowledge Graph Prompting. 2024arXiv preprint</p>
<p>Multi-document summarization via deep learning techniques: A survey. C Ma, W E Zhang, M Guo, H Wang, Q Z Sheng, Acm Comput Surv. 5552022</p>
<p>Automatic generation of reviews of scientific papers. A Nikiforovskaya, N Kapralov, A Vlasova, O Shpynov, A Shpilman, IEEE2020</p>
<p>Using citations to generate surveys of scientific paradigms. S Mohammad, B Dorr, M Egan, 2009</p>
<p>Towards multi-document summarization of scientific articles: making interesting comparisons with SciSumm. N Agarwal, R S Reddy, G Kiran, C Rose, 2011</p>
<p>Deconstructing human literature reviews-a framework for multi-document summarization. K Jaidka, C Khoo, J-C Na, 2013</p>
<p>SciReviewGen: A Large-scale Dataset for Automatic Literature Review Generation. T Kasanishi, M Isonuma, J Mori, I Sakata, arXiv:2305151862023arXiv preprint</p>
<p>System for systematic literature review using multiple ai agents: Concept and an empirical evaluation. A M Sami, Z Rasheed, K-K Kemell, arXiv:2403083992024arXiv preprint</p>
<p>S Agarwal, I H Laradji, L Charlin, C Pal, Litllm, arXiv:240201788A Toolkit for Scientific Literature Review. 2024arXiv preprint</p>
<p>C Y Haryanto, Llassist, arXiv:240713993Simple Tools for Automating Literature Review Using Large Language Models. 2024arXiv preprint</p>
<p>Cutting Through the Clutter: The Potential of LLMs for Efficient Filtration in Systematic Literature Reviews. L Joos, D A Keim, M T Fischer, arXiv:2407106522024arXiv preprint</p>
<p>Y Li, L Chen, A Liu, K Yu, Wen L Chatcite, arXiv:240302574LLM Agent with Human Workflow Guidance for Comparative Literature Summary. 2024arXiv preprint</p>
<p>ChatGPT outperforms crowd workers for textannotation tasks. F Gilardi, M Alizadeh, M Kubli, P Natl Acad Sci. 12030e2305016120Jul 25 2023</p>
<p>Chatgpt-4 outperforms experts and crowd workers in annotating political twitter messages with zero-shot learning. P Törnberg, arXiv:2304065882023arXiv preprint</p>
<p>Large Language Models as Evaluators for Recommendation Explanations. X Zhang, Y Li, J Wang, 2024</p>
<p>Language models (mostly) know what they know. S Kadavath, T Conerly, A Askell, arXiv:2207052212022arXiv preprint</p>
<p>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. L M Zheng, W L Chiang, Y Sheng, Adv Neur In. 362023</p>
<p>Benchmarking foundation models with language-modelas-an-examiner. Y Bai, J Ying, Y Cao, Advances in Neural Information Processing Systems. 362024</p>
<p>Critic: Large language models can self-correct with tool-interactive critiquing. Z Gou, Z Shao, Y Gong, arXiv:2305117382023arXiv preprint</p>
<p>Llm-based nlg evaluation: Current status and challenges. M Gao, X Hu, J Ruan, X Pu, Wan X , arXiv:2402013832024arXiv preprint</p>
<p>Large language models are not fair evaluators. P Wang, L Li, L Chen, arXiv:2305179262023arXiv preprint</p>
<p>Y Liu, D Iter, Y Xu, S Wang, R Xu, C Zhu, G-Eval, arXiv:230316634Nlg evaluation using gpt-4 with better human alignment. 2023arXiv preprint</p>
<p>Split and merge: Aligning position biases in large language model based evaluators. Z Li, C Wang, P Ma, arXiv:2310014322023arXiv preprint</p>
<p>Large language models are not yet human-level evaluators for abstractive summarization. C Shen, L Cheng, X-P Nguyen, Y You, L Bing, arXiv:2305130912023arXiv preprint</p>
<p>Progress in Processes and Catalysts for Dehydrogenation of Cyclohexanol to Cyclohexanone. J Gong, S X Hou, Y Wang, X B Ma, Jun 202329Transactions of Tianjin University</p>
<p>Data-Driven Design of Single-Atom Electrocatalysts with Intrinsic Descriptors for Carbon Dioxide Reduction Reaction. X Y Lin, S Y Zhen, X H Wang, Oct 202430Transactions of Tianjin University</p>
<p>Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models. Y Zhang, Y Li, L Cui, arXiv:2309012192023arXiv preprint</p>
<p>GPT-4 is here: what scientists think. K Sanderson, Nature. 6157954Mar 30 2023</p>
<p>Hallucination is inevitable: An innate limitation of large language models. Z Xu, S Jain, M Kankanhalli, arXiv:2401118172024arXiv preprint</p>
<p>Calibrated language models must hallucinate. A T Kalai, S S Vempala, 2024</p>
<p>A comparative study of open-source large language models, gpt-4 and claude 2: Multiple-choice test taking in nephrology. S Wu, M Koo, L Blum, arXiv:2308047092023arXiv preprint</p>
<p>Large language models should be used as scientific reasoning engines, not knowledge databases. D Truhn, J S Reis-Filho, J N Kather, Nature Medicine. 2023</p>
<p>Summarization is (almost) dead. X Pu, M Gao, Wan X , arXiv:2309095582023arXiv preprint</p>
<p>The power of noise: Redefining retrieval for rag systems. F Cuconasu, G Trappolini, F Siciliano, 2024</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, arXiv:2203111712022arXiv preprint</p>
<p>Don't stop pretraining: Adapt language models to domains and tasks. S Gururangan, A Marasović, S Swayamdipta, arXiv:2004109642020arXiv preprint</p>
<p>RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture. A Gupta, A Shirgaonkar, Balaguer Adl, arXiv:2401084062024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1990 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1990</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1990</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-46.html">extraction-schema-46</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <p><strong>Paper ID:</strong> paper-279075311</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.00844v1.pdf" target="_blank">LLM Cannot Discover Causality, and Should Be Restricted to Non-Decisional Support in Causal Discovery</a></p>
                <p><strong>Paper Abstract:</strong> This paper critically re-evaluates LLMs' role in causal discovery and argues against their direct involvement in determining causal relationships. We demonstrate that LLMs' autoregressive, correlation-driven modeling inherently lacks the theoretical grounding for causal reasoning and introduces unreliability when used as priors in causal discovery algorithms. Through empirical studies, we expose the limitations of existing LLM-based methods and reveal that deliberate prompt engineering (e.g., injecting ground-truth knowledge) could overstate their performance, helping to explain the consistently favorable results reported in much of the current literature. Based on these findings, we strictly confined LLMs' role to a non-decisional auxiliary capacity: LLMs should not participate in determining the existence or directionality of causal relationships, but can assist the search process for causal graphs (e.g., LLM-based heuristic search). Experiments across various settings confirm that, by strictly isolating LLMs from causal decision-making, LLM-guided heuristic search can accelerate the convergence and outperform both traditional and LLM-based methods in causal structure learning. We conclude with a call for the community to shift focus from naively applying LLMs to developing specialized models and training method that respect the core principles of causal discovery.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1990.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1990.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-guided Evolutionary Operators (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-assisted Evolutionary Operators using Tree-of-Thoughts prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that replaces random crossover and mutation in evolutionary causal-structure search with proposals generated by LLMs (GPT-4, GPT-3.5, Claude-3) using a Tree-of-Thoughts prompting strategy to suggest edge additions/removals and crossover points; retained classical scoring (BIC/BDeu) for final selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-assisted Evolutionary Operators</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>LLM-based (learned, data-driven)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>LLMs (GPT-4, GPT-3.5, Claude-3) are prompted (Tree-of-Thoughts style) with the current DAG(s), optimization objectives (e.g., increase diversity or refine local structures), and contextual examples; the model outputs mutation and crossover proposals (which edges to add/remove or crossover points). These proposals replace or guide traditional random operators during evolutionary optimization; selection still uses tournament selection and classical scoring functions for fitness.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper; authors note GPT-4 has a 'larger training corpus' and broader background knowledge but provide no dataset size, domain breakdown, or pretraining details.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Causal-structure learning on bnlearn real-world Bayesian network datasets (10 datasets across small to extra-large scales: e.g., Asia, Sachs, Insurance, Water, Alarm, Barley, Hepar II, Win95PTS, Pathfinder, Andes); optimization measured under budgets of fitness evaluations (200, 400, 600).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>EO1 (uniform crossover + bit-flip mutation), EO2 (parent-based crossover + bit-flip mutation); classical evolutionary search with same selection and scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td>Qualitative: GPT-4-based operators yield the best F1 across different fitness-evaluation budgets (200/400/600) as shown in Figure 8; GPT-3.5 and Claude-3 achieve performance comparable to traditional EO1/EO2. Exact numeric F1 values are shown in Figure 8 but not tabulated in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td>Qualitative: EO1 and EO2 provide comparable baseline F1; outperformed by GPT-4-assisted operators under the tested budgets (see Figure 8). Exact numeric values are presented in Figure 8 but not explicitly listed in text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Partially reported: LLM-assisted operators perform relatively well on medium/large bnlearn datasets; however LLMs' performance (as operators) depends on LLM capability and degrades on more complex/higher-dimension problems indirectly (paper shows overall LLM causal tasks degrade with complexity). No explicit OOD benchmark for operators was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td>Paper documents multiple LLM biases that affect operator suggestions: sensitivity to prompt wording, word-order and redundant textual context, and numerical precision encoding; these biases imply operators can inherit training-distribution patterns and be skewed by prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td>Not reported quantitatively. The paper does not provide wall-clock/time/memory/inference-cost comparisons between invoking LLMs for operators and running traditional operators.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td>Not compared; only general-purpose LLMs (GPT variants, Claude, Llama, Gemini, Falcon) are evaluated and authors note differences in recall/precision likely due to pretraining scale/corpus but provide no direct controlled comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td>Not formally characterized; authors argue LLM proposals can help escape local optima by leveraging world knowledge, effectively shifting search trajectories, but no coverage metrics of hypothesis space were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td>Proposed but not fully tested: authors describe potential dynamic adjustment where the LLM assesses stagnation and suggests parameter/strategy changes (mutation probability, step size) — presented as a potential extension rather than evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>LLM proposals can be unstable and influenced by prompt quality; they may suggest incorrect edges due to hallucination, are sensitive to redundant/contextual information, and can degrade when tokenized numerical data distort signal. Also, reliance on LLMs as decision-makers would undermine theoretical guarantees—here they are used only as proposers.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>When confined to a non-decisional role, LLMs can serve as effective learned operators that accelerate convergence in heuristic/evolutionary causal search: GPT-4 in particular outperforms traditional random crossover/mutation under fixed evaluation budgets by providing targeted proposals informed by broad world knowledge; however, benefits depend on LLM capability and prompt design, and computational costs and theoretical guarantees were not quantified.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1990.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1990.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based Initialization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-guided Initial Population Initialization for Causal Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using LLMs to prune improbable variable pairs and generate a sparse initial DAG/search space, reducing complexity before heuristic or evolutionary causal-structure search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-based Initialization</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>LLM-based (learned, data-driven) used for search-space pruning</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>LLMs (GPT-4, Claude-3, GPT-3.5) are prompted with variable information and background knowledge to score potential relationships; pairs deemed unlikely are pruned from the search space before standard CDA/evolutionary search, producing an initial population biased to sparser and more plausible DAGs.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not reported; models are pre-trained LLMs with unspecified corpora. The paper notes differences in effectiveness when variable names are complex.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Causal-structure learning on bnlearn datasets (10 networks of varying scale).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>CI-Based initialization (conditional-independence pruning) and MI-Based initialization (mutual-information-based pruning), and combined CI-MI.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td>Qualitative: On medium and large bnlearn datasets, LLM-based initialization 'significantly outperforms' CI- and MI-based methods in F1 (Figure 7). On small datasets CI-based initialization performed best. Exact F1 numbers are plotted in Figure 7 but not exhaustively tabulated in-text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td>CI-Based performs best on small datasets; MI-Based and CI-MI vary by dataset. Numeric comparisons appear in Figure 7 only.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not directly evaluated; authors note LLM-based pruning helps on larger networks by leveraging prior knowledge, but complex/ambiguous variable names can limit LLM effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td>Prompt quality, background knowledge completeness and variable naming affect pruning quality; when variable names are complex, LLMs' pruning can be less effective, indicating sensitivity to training/representation patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td>Not reported. The cost of querying LLMs for initialization versus running CI/MI tests is not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td>Indirect: LLM-based pruning reduces the initial hypothesis/search space (sparser DAGs) but no formal measure of retained coverage or false-pruning rates is provided in text beyond plotted F1 outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Potential false pruning of true edges if LLM priors are incorrect; sensitivity to prompt quality and variable name ambiguity; manual prompt refinement can improve outcomes but at human cost.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>LLM-informed initialization can speed search and improve performance on larger causal-structure problems by reducing search complexity, but on small networks classical CI-based pruning may be superior; the approach is sensitive to prompt and variable representation and thus cannot be treated as reliably correct priors.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1990.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1990.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Soft-Constraint Priors (LLM→Score)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embedding LLM-generated priors as soft constraints in score-based causal discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid method that converts LLM judgments into soft prior scores σ(G; λ) and adds them to classical scoring functions (e.g., BIC, BDeu), aiming to bias search toward LLM-suggested structures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-derived Soft Priors in Score-based CDAs</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>Hybrid (LLM-based priors combined with traditional score-based CDA)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>LLM outputs (causal judgments) are converted into priors λ and a scoring term σ(G; λ) which is added to the data-driven score σ(G; D) to produce σ(G; D, λ) = σ(G; D) + σ(G; λ). This modifies the objective used by score-based search algorithms to favor structures consistent with the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>N/A for scoring function itself; LLM provenance (GPT-4) as above not specified in detail.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Global causal graph discovery on bnlearn datasets (small networks) and controlled experiments on 14 small bnlearn datasets in Appendix E for manipulability analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Traditional score-based CDAs without LLM priors (using BIC/BDeu/MDL scores), and soft-constrained versions with high-quality vs low-quality prompts for LLM priors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td>Mixed: Some prior studies claim empirical improvements; in the paper's experiments with GPT-4 on 14 small bnlearn datasets, soft-constrained scoring schemes 'consistently struggle to filter out errors' from low-quality prompts and improve only when prompts are manually refined (Appendix E, Figure 9). No single numeric aggregate performance is presented in-text for soft priors vs baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td>Traditional scoring without LLM priors maintains theoretical guarantees; empirical baseline performance varies by dataset (plotted in appendices). The paper argues cases where soft priors 'coincidentally' align numerically with data may show improved empirical scores but lack theoretical soundness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td>When LLM priors are high-quality (manually curated prompts), soft-constrained methods can show improved empirical performance on small networks; with low-quality prompts, erroneous priors are largely retained and degrade reliability. Exact performance depends strongly on prompt quality (Appendix E, Figure 9).</td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not tested; authors show sensitivity to prompt-engineered injection of ground-truth causing inflated results—indicating poor generalization to truly unknown causal problems.</td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td>Strong evidence: prompt-engineering and injection of ground-truth into prompts substantially alter performance; LLM priors reflect training and corpus biases and can introduce erroneous, non-decomposable global constraints that violate scoring assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td>Not reported quantitatively. The paper critiques mathematical incompatibility and scaling mismatch between σ(G; D) and σ(G; λ) rather than cost.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td>Authors argue adding σ(G; λ) can change the effective hypothesis space and break decomposability and local score consistency, but no empirical coverage metrics are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Key failure modes: (1) mathematical incompatibility in adding priors to data scores leading to scale mismatch; (2) breakage of decomposability and score-local-consistency making many optimization algorithms inapplicable; (3) retention of incorrect priors under low-quality prompts; (4) vulnerability to prompt injection of ground-truth and experimental manipulability.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>Embedding LLM outputs as soft priors into traditional scoring functions lacks rigorous probabilistic grounding and can undermine theoretical properties (decomposability, local consistency), producing empirically variable results heavily dependent on prompt quality; consequently LLMs should not be used as decision-making priors but may be limited to non-decisional assistive roles.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evolutionary computation in the era of large language model: Survey and roadmap <em>(Rating: 2)</em></li>
                <li>Efficient causal graph discovery using large language models <em>(Rating: 2)</em></li>
                <li>Using gpt-4 to guide causal machine learning <em>(Rating: 2)</em></li>
                <li>Large language models are effective priors for causal graph discovery <em>(Rating: 2)</em></li>
                <li>Mitigating prior errors in causal structure learning: Towards llm driven prior knowledge <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1990",
    "paper_id": "paper-279075311",
    "extraction_schema_id": "extraction-schema-46",
    "extracted_data": [
        {
            "name_short": "LLM-guided Evolutionary Operators (ToT)",
            "name_full": "LLM-assisted Evolutionary Operators using Tree-of-Thoughts prompting",
            "brief_description": "An approach that replaces random crossover and mutation in evolutionary causal-structure search with proposals generated by LLMs (GPT-4, GPT-3.5, Claude-3) using a Tree-of-Thoughts prompting strategy to suggest edge additions/removals and crossover points; retained classical scoring (BIC/BDeu) for final selection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LLM-assisted Evolutionary Operators",
            "operator_type": "LLM-based (learned, data-driven)",
            "operator_description": "LLMs (GPT-4, GPT-3.5, Claude-3) are prompted (Tree-of-Thoughts style) with the current DAG(s), optimization objectives (e.g., increase diversity or refine local structures), and contextual examples; the model outputs mutation and crossover proposals (which edges to add/remove or crossover points). These proposals replace or guide traditional random operators during evolutionary optimization; selection still uses tournament selection and classical scoring functions for fitness.",
            "training_data_description": "Not specified in this paper; authors note GPT-4 has a 'larger training corpus' and broader background knowledge but provide no dataset size, domain breakdown, or pretraining details.",
            "domain_or_benchmark": "Causal-structure learning on bnlearn real-world Bayesian network datasets (10 datasets across small to extra-large scales: e.g., Asia, Sachs, Insurance, Water, Alarm, Barley, Hepar II, Win95PTS, Pathfinder, Andes); optimization measured under budgets of fitness evaluations (200, 400, 600).",
            "comparison_baseline": "EO1 (uniform crossover + bit-flip mutation), EO2 (parent-based crossover + bit-flip mutation); classical evolutionary search with same selection and scoring.",
            "performance_learned_operator": "Qualitative: GPT-4-based operators yield the best F1 across different fitness-evaluation budgets (200/400/600) as shown in Figure 8; GPT-3.5 and Claude-3 achieve performance comparable to traditional EO1/EO2. Exact numeric F1 values are shown in Figure 8 but not tabulated in the main text.",
            "performance_traditional_operator": "Qualitative: EO1 and EO2 provide comparable baseline F1; outperformed by GPT-4-assisted operators under the tested budgets (see Figure 8). Exact numeric values are presented in Figure 8 but not explicitly listed in text.",
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": "Partially reported: LLM-assisted operators perform relatively well on medium/large bnlearn datasets; however LLMs' performance (as operators) depends on LLM capability and degrades on more complex/higher-dimension problems indirectly (paper shows overall LLM causal tasks degrade with complexity). No explicit OOD benchmark for operators was reported.",
            "training_bias_evidence": "Paper documents multiple LLM biases that affect operator suggestions: sensitivity to prompt wording, word-order and redundant textual context, and numerical precision encoding; these biases imply operators can inherit training-distribution patterns and be skewed by prompt engineering.",
            "computational_cost_comparison": "Not reported quantitatively. The paper does not provide wall-clock/time/memory/inference-cost comparisons between invoking LLMs for operators and running traditional operators.",
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": "Not compared; only general-purpose LLMs (GPT variants, Claude, Llama, Gemini, Falcon) are evaluated and authors note differences in recall/precision likely due to pretraining scale/corpus but provide no direct controlled comparison.",
            "ablation_study_results": null,
            "hypothesis_space_characterization": "Not formally characterized; authors argue LLM proposals can help escape local optima by leveraging world knowledge, effectively shifting search trajectories, but no coverage metrics of hypothesis space were reported.",
            "adaptation_during_evolution": "Proposed but not fully tested: authors describe potential dynamic adjustment where the LLM assesses stagnation and suggests parameter/strategy changes (mutation probability, step size) — presented as a potential extension rather than evaluated.",
            "failure_modes": "LLM proposals can be unstable and influenced by prompt quality; they may suggest incorrect edges due to hallucination, are sensitive to redundant/contextual information, and can degrade when tokenized numerical data distort signal. Also, reliance on LLMs as decision-makers would undermine theoretical guarantees—here they are used only as proposers.",
            "key_findings_for_theory": "When confined to a non-decisional role, LLMs can serve as effective learned operators that accelerate convergence in heuristic/evolutionary causal search: GPT-4 in particular outperforms traditional random crossover/mutation under fixed evaluation budgets by providing targeted proposals informed by broad world knowledge; however, benefits depend on LLM capability and prompt design, and computational costs and theoretical guarantees were not quantified.",
            "uuid": "e1990.0"
        },
        {
            "name_short": "LLM-based Initialization",
            "name_full": "LLM-guided Initial Population Initialization for Causal Search",
            "brief_description": "Using LLMs to prune improbable variable pairs and generate a sparse initial DAG/search space, reducing complexity before heuristic or evolutionary causal-structure search.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LLM-based Initialization",
            "operator_type": "LLM-based (learned, data-driven) used for search-space pruning",
            "operator_description": "LLMs (GPT-4, Claude-3, GPT-3.5) are prompted with variable information and background knowledge to score potential relationships; pairs deemed unlikely are pruned from the search space before standard CDA/evolutionary search, producing an initial population biased to sparser and more plausible DAGs.",
            "training_data_description": "Not reported; models are pre-trained LLMs with unspecified corpora. The paper notes differences in effectiveness when variable names are complex.",
            "domain_or_benchmark": "Causal-structure learning on bnlearn datasets (10 networks of varying scale).",
            "comparison_baseline": "CI-Based initialization (conditional-independence pruning) and MI-Based initialization (mutual-information-based pruning), and combined CI-MI.",
            "performance_learned_operator": "Qualitative: On medium and large bnlearn datasets, LLM-based initialization 'significantly outperforms' CI- and MI-based methods in F1 (Figure 7). On small datasets CI-based initialization performed best. Exact F1 numbers are plotted in Figure 7 but not exhaustively tabulated in-text.",
            "performance_traditional_operator": "CI-Based performs best on small datasets; MI-Based and CI-MI vary by dataset. Numeric comparisons appear in Figure 7 only.",
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": "Not directly evaluated; authors note LLM-based pruning helps on larger networks by leveraging prior knowledge, but complex/ambiguous variable names can limit LLM effectiveness.",
            "training_bias_evidence": "Prompt quality, background knowledge completeness and variable naming affect pruning quality; when variable names are complex, LLMs' pruning can be less effective, indicating sensitivity to training/representation patterns.",
            "computational_cost_comparison": "Not reported. The cost of querying LLMs for initialization versus running CI/MI tests is not quantified.",
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": null,
            "ablation_study_results": null,
            "hypothesis_space_characterization": "Indirect: LLM-based pruning reduces the initial hypothesis/search space (sparser DAGs) but no formal measure of retained coverage or false-pruning rates is provided in text beyond plotted F1 outcomes.",
            "adaptation_during_evolution": null,
            "failure_modes": "Potential false pruning of true edges if LLM priors are incorrect; sensitivity to prompt quality and variable name ambiguity; manual prompt refinement can improve outcomes but at human cost.",
            "key_findings_for_theory": "LLM-informed initialization can speed search and improve performance on larger causal-structure problems by reducing search complexity, but on small networks classical CI-based pruning may be superior; the approach is sensitive to prompt and variable representation and thus cannot be treated as reliably correct priors.",
            "uuid": "e1990.1"
        },
        {
            "name_short": "Soft-Constraint Priors (LLM→Score)",
            "name_full": "Embedding LLM-generated priors as soft constraints in score-based causal discovery",
            "brief_description": "A hybrid method that converts LLM judgments into soft prior scores σ(G; λ) and adds them to classical scoring functions (e.g., BIC, BDeu), aiming to bias search toward LLM-suggested structures.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LLM-derived Soft Priors in Score-based CDAs",
            "operator_type": "Hybrid (LLM-based priors combined with traditional score-based CDA)",
            "operator_description": "LLM outputs (causal judgments) are converted into priors λ and a scoring term σ(G; λ) which is added to the data-driven score σ(G; D) to produce σ(G; D, λ) = σ(G; D) + σ(G; λ). This modifies the objective used by score-based search algorithms to favor structures consistent with the LLM.",
            "training_data_description": "N/A for scoring function itself; LLM provenance (GPT-4) as above not specified in detail.",
            "domain_or_benchmark": "Global causal graph discovery on bnlearn datasets (small networks) and controlled experiments on 14 small bnlearn datasets in Appendix E for manipulability analysis.",
            "comparison_baseline": "Traditional score-based CDAs without LLM priors (using BIC/BDeu/MDL scores), and soft-constrained versions with high-quality vs low-quality prompts for LLM priors.",
            "performance_learned_operator": "Mixed: Some prior studies claim empirical improvements; in the paper's experiments with GPT-4 on 14 small bnlearn datasets, soft-constrained scoring schemes 'consistently struggle to filter out errors' from low-quality prompts and improve only when prompts are manually refined (Appendix E, Figure 9). No single numeric aggregate performance is presented in-text for soft priors vs baseline.",
            "performance_traditional_operator": "Traditional scoring without LLM priors maintains theoretical guarantees; empirical baseline performance varies by dataset (plotted in appendices). The paper argues cases where soft priors 'coincidentally' align numerically with data may show improved empirical scores but lack theoretical soundness.",
            "performance_hybrid_operator": "When LLM priors are high-quality (manually curated prompts), soft-constrained methods can show improved empirical performance on small networks; with low-quality prompts, erroneous priors are largely retained and degrade reliability. Exact performance depends strongly on prompt quality (Appendix E, Figure 9).",
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": "Not tested; authors show sensitivity to prompt-engineered injection of ground-truth causing inflated results—indicating poor generalization to truly unknown causal problems.",
            "training_bias_evidence": "Strong evidence: prompt-engineering and injection of ground-truth into prompts substantially alter performance; LLM priors reflect training and corpus biases and can introduce erroneous, non-decomposable global constraints that violate scoring assumptions.",
            "computational_cost_comparison": "Not reported quantitatively. The paper critiques mathematical incompatibility and scaling mismatch between σ(G; D) and σ(G; λ) rather than cost.",
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": null,
            "ablation_study_results": null,
            "hypothesis_space_characterization": "Authors argue adding σ(G; λ) can change the effective hypothesis space and break decomposability and local score consistency, but no empirical coverage metrics are provided.",
            "adaptation_during_evolution": null,
            "failure_modes": "Key failure modes: (1) mathematical incompatibility in adding priors to data scores leading to scale mismatch; (2) breakage of decomposability and score-local-consistency making many optimization algorithms inapplicable; (3) retention of incorrect priors under low-quality prompts; (4) vulnerability to prompt injection of ground-truth and experimental manipulability.",
            "key_findings_for_theory": "Embedding LLM outputs as soft priors into traditional scoring functions lacks rigorous probabilistic grounding and can undermine theoretical properties (decomposability, local consistency), producing empirically variable results heavily dependent on prompt quality; consequently LLMs should not be used as decision-making priors but may be limited to non-decisional assistive roles.",
            "uuid": "e1990.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evolutionary computation in the era of large language model: Survey and roadmap",
            "rating": 2
        },
        {
            "paper_title": "Efficient causal graph discovery using large language models",
            "rating": 2
        },
        {
            "paper_title": "Using gpt-4 to guide causal machine learning",
            "rating": 2
        },
        {
            "paper_title": "Large language models are effective priors for causal graph discovery",
            "rating": 2
        },
        {
            "paper_title": "Mitigating prior errors in causal structure learning: Towards llm driven prior knowledge",
            "rating": 2
        }
    ],
    "cost": 0.014988999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LLM Cannot Discover Causality, and Should Be Restricted to Non-Decisional Support in Causal Discovery
1 Jun 2025</p>
<p>Xingyu Wu xingy.wu@polyu.edu.hk 
The Hong Kong Polytechnic University Hong Kong SAR
China</p>
<p>Kui Yu yukui@hfut.edu.cn 
Hefei University of Technology Hefei
China</p>
<p>Jibin Wu jibin.wu@polyu.edu.hk 
The Hong Kong Polytechnic University Hong Kong SAR
China</p>
<p>Kay Chen Tan kctan@polyu.edu.hk 
The Hong Kong Polytechnic University Hong Kong SAR
China</p>
<p>LLM Cannot Discover Causality, and Should Be Restricted to Non-Decisional Support in Causal Discovery
1 Jun 2025F074D512CDD9761AF697982D582A965BarXiv:2506.00844v1[cs.LG]
This paper critically re-evaluates LLMs' role in causal discovery and argues against their direct involvement in determining causal relationships.We demonstrate that LLMs' autoregressive, correlation-driven modeling inherently lacks the theoretical grounding for causal reasoning and introduces unreliability when used as priors in causal discovery algorithms.Through empirical studies, we expose the limitations of existing LLM-based methods and reveal that deliberate prompt engineering (e.g., injecting ground-truth knowledge) could overstate their performance, helping to explain the consistently favorable results reported in much of the current literature.Based on these findings, we strictly confined LLMs' role to a non-decisional auxiliary capacity: LLMs should not participate in determining the existence or directionality of causal relationships, but can assist the search process for causal graphs (e.g., LLM-based heuristic search).Experiments across various settings confirm that, by strictly isolating LLMs from causal decision-making, LLM-guided heuristic search can accelerate the convergence and outperform both traditional and LLM-based methods in causal structure learning.We conclude with a call for the community to shift focus from naively applying LLMs to developing specialized models and training method that respect the core principles of causal discovery.</p>
<p>Introduction</p>
<p>Causal discovery aims to uncover underlying causal relationships from observational data [1].Traditional causal discovery algorithms (CDAs) construct causal structures by systematically leveraging conditional independence tests, optimization techniques, or functional causal models.These methods integrate theoretical principles from graphical models, information theory, and structural causal modeling to ensure interpretability and maintain theoretical guarantees in causal discovery [2].Recently, large language models (LLMs) have emerged as a potential alternative for causal discovery [3].Many studies have explored the application of LLMs in this domain, leveraging their linguistic knowledge and common-sense reasoning to identify causal relationships.These efforts have sparked significant interest, particularly in investigating whether LLMs can assist in identifying causality from text and structured data.Figure 1: This logical framework illustrates the focus and main positions of Sections 2 to 6. Building on Section 2's comprehensive review of existing methods, Sections 3 and 4 conduct a thorough critique of current algorithms, whether using LLMs as hard knowledge or soft knowledge.Then, Section 5 defines the scope of LLMs' role in causal discovery.Section 6 further reveals the manipulability of experimental results in existing methods, which explains why they achieve good performance.</p>
<p>LLMs have achieved notable performance in causal discovery, particularly in identifying pairwise causal relationships from text [4,5].Existing studies have leveraged variable information [6] and background knowledge [7] to enhance LLMs' ability to identify causality.These studies not only highlight LLMs' strengths in recognizing pairwise causality but also explore hybrid approaches that integrate LLMs with traditional CDAs to enhance performance [8].For instance, researchers have utilized LLMs to initialize causal structures [9] or refine CDA-generated graphs [10].Most studies demonstrate promising results, suggesting that LLMs hold significant potential in causal discovery.However, these efforts primarily focus on empirical evaluations, leaving open questions about whether LLMs truly possess causal reasoning abilities.Some researchers have raised concerns that LLMs may struggle with complex causal tasks [7] and are prone to generating hallucinated causal relations, i.e., assigning causality where none exists [11].Given the instability of LLMs, several mitigation strategies have been proposed.These include providing observational data [12], employing LLMs as conditional independence test operators [13], and incorporating LLM-generated causal judgments into classical scoring functions as priors [5].While these improvements claim to enhance reliability to some extent, they do not fully resolve the fundamental limitations of LLMs in causal discovery.</p>
<p>Different from most existing studies, we argue that the role of LLMs in causal discovery tasks is limited.Through the following critical and compensatory perspectives, we clearly define the current scope of LLMs' applicable functions:</p>
<ol>
<li>Critical Position: LLMs cannot identify causality, and their outputs should not directly or indirectly determine the existence or directionality of causal relationships.This includes:</li>
</ol>
<p>• Direct Result Determination: LLMs should not independently conclude whether a causal relationship exists (Section 3).• Indirect Influence on Results: Causal relationships identified by LLMs should not be embedded as "prior knowledge", whether as hard constraints or soft penalties, into the decision-making processes of CDAs (Section 4).</p>
<p>Compensatory Position: Without influencing causal decision-making, LLMs can assist</p>
<p>CDAs by improving the search process for causal graphs, thereby accelerating the convergence toward optimal causal structures (Section 5).</p>
<p>To this end, this paper investigates the limitations of LLMs in causal discovery from two critical perspectives: (i) the intrinsic limitations of LLMs to infer causality, and (ii) the challenges arising from their integration with classical CDAs.As shown in Figure 1, Section 3 first explains how LLMs' autoregressive modeling based on word order [14] inherently differs from the probability decomposition in structural causal models [15], leading to a lack of theoretical grounding for causal reasoning.Through a series of empirical studies, we systematically analyze the impact of several key factors-such as word order, redundancy, entity positioning, and numerical precision-on LLMs' causal inference.Results reveal that LLM performance is highly sensitive to prompt design and text quality, and merely supplying observational data does not guarantee improved reliability.Unless causal relationships are stated with exceptional clarity and minimal noise, LLMs often fail to reach consistent or correct conclusions.Given these reliability concerns, Section 4 further investigates the interaction between LLMs and traditional CDAs, analyzing different collaboration paradigms and their limitations.We focus on how using LLM-derived causal relationships as (hard/soft) prior knowledge in CDAs [5] compromises algorithmic reliability and theoretical guarantees, regardless of whether the method is score-based or constraint-based.Moreover, in Section 6, we demonstrate that the performance of LLM-based methods can be artificially inflated via prompt engineering, for instance, by injecting ground-truth into the input.</p>
<p>Building on these analyses, we advocate for a principled delineation of LLMs' functional boundaries in causal discovery: LLMs should not determine the existence or directionality of causal relationships, but can serve as auxiliary agents that guide the causal graph search process.Section 5 introduces a novel paradigm where LLMs assist in heuristic search-such as through initialization, mutation guidance, or cycle resolution-while preserving the decision-making authority of theoretically sound CDAs.By leveraging LLMs' general world knowledge [16] in a non-decisional role, this framework helps CDAs escape local optima and converge faster to globally optimal structures.Crucially, the evaluation of causal structures remains grounded in statistical analysis of observational data via established scoring functions or independence tests, which is fundamentally different from existing LLM-based causal discovery methods.Comprehensive experiments across varying network scales confirm that when LLMs are appropriately confined to assistive roles, they can significantly enhance search efficiency and accuracy compared to both traditional CDAs and existing LLM-based methods.Thus, our results underscore the value of LLMs as heuristic facilitators-not causal reasoners-in the pursuit of reliable causal discovery.</p>
<p>In summary, this paper presents a critical re-evaluation of the role of LLMs in causal discovery, challenging the prevailing optimism surrounding their application.We call upon the research community to adopt a more principled and cautious stance when incorporating LLMs into causal tasks, ensuring that empirical convenience does not come at the cost of epistemic soundness.The path forward lies not in overstating the reasoning capabilities of unmodified general-purpose LLMs; instead, the future of LLM-based causal discovery depends on rethinking model design and training objectives to align with the structural and statistical principles of causality.</p>
<p>Characteristics of LLM-based Causal Discovery (Detailed in Appendix A)</p>
<p>Causal discovery, a fundamental task in causal inference [1], aims to uncover causal relationships from observational data, with traditional methods like constraint-based and score-based approaches [17] relying on statistical and mathematical frameworks.Recent advancements in LLMs have introduced new opportunities, as LLM-based methods leverage linguistic knowledge, common sense, and domain insights to complement traditional techniques.Existing approaches, summarized in Table 1 in Appendix A, can be categorized along three dimensions: (1) Pairwise Causality and Global Causal Graph: Methods focus on either inferring pairwise causal directions/existence [3,11,4,18] or learning full causal graphs by integrating LLMs with data-driven CDAs [8,19]; (2) Prompt Design: Strategies include incorporating variable information [6], background knowledge [7,13], observational data [12], and CDA results [20] to guide LLMs; (3) Collaboration Paradigms: Hybrid approaches involve using LLMs for initialization/prior [5,21,9], post-processing [8,22,10], iterative collaboration [20,19,23], or modular integration [13,24] with traditional CDAs.In Appendix A, we detail the comprehensive overview of existing LLM-based causal discovery methods.</p>
<p>3 Fundamental Limitations of LLMs in Identifying Causality</p>
<p>Identify Causality in Textual Data</p>
<p>We commence by elucidating that the autoregressive paradigm of LLMs, which is predicated on word order [14], is not conductive for discerning causality.LLMs model the associations between words through the attention mechanism.Fundamentally, this amounts to modeling the conditional probability contingent on the preceding context.Consider an input word sequence x = (x 1 , x 2 , . . ., x T ) fed into the LLM, where T denotes the sequence length.The attention mechanism empowers the model, when generating or predicting each word x i , to incorporate the information from all prior words x 1 , x 2 , . . ., x i−1 (or words within a specific window).Consequently, the LLM is effectively modeling the following distribution:
P (x) = P (x 1 ) • P (x 2 | x 1 ) • • • P (x T | x 1 , x 2 , . . . , x T −1 )(1)
By the chain rule, Eq. ( 1) holds true for any sequence x.From the perspective of probability decomposition, this word order-based decomposition is significantly different from the decomposition based on causal graphs in structural causal models [15].For a causal structure represented by a directed acyclic graph (DAG), the structural causal model decomposes its probability distribution as:
P (X 1 , X 2 , . . . , X n ) = n i=1 P (X i |pa(X i ))(2)
where pa(X i ) is the set of parent nodes of X i .This decomposition method in Eq. ( 2) can accurately reflect the conditional dependence and independence relationships between variables, which cannot be captured by the word order-based modeling paradigm in Eq. ( 1), because when determining the generation distribution at a certain position, all previous words are considered.For example, in a text, there may be a causal relationship: "Because it rained (X), the ground is wet (Z), causing people to walk carefully (Y )".When the LLM generates the probability of Y , it will consider the word X, even though in the causal structure, given Z, X and Y are conditionally independent (causal chain X → Z → Y ).On the other hand, the sequential order of words may lead to confusion in parsing causal relationships.For example, in a text, there are two events A and B that are causally independent, but A comes before B in the text order.The LLM will be affected by A when estimating P (B), rather than estimating according to their true causal relationship.</p>
<p>Therefore, the modeling approach of LLMs is not oriented towards causal relationships, but rather more towards modeling the correlations between words in the text.At the same time, the variable relationships constructed by LLMs contain a large amount of redundant structure, which is essentially different from the sparse characteristics of causal graphs.From this perspective, there is no theoretical basis for LLMs to identify causal relationships, and the causal relationships obtained by LLMs from texts are not robust, as shown in the following position.Position 3.1.When LLMs identify causal relationships from text, they are at least affected by the following factors: (1) The word order in the text and whether it follows the order of causal words that frequently appear in the LLM's training corpus; (2) The amount of redundant entities and redundant information in the text; (3) The distance between the entities whose causal relationships are to be analyzed in the text.</p>
<p>Factor Analyses and Empirical Studies: As shown in Position 3.1, LLMs' ability to infer causal relationships is influenced by multiple textual factors, including word order, the presence of redundant entities, and the textual distance between causally related entities.To systematically investigate these limitations, we conducted a series of empirical analyses examining the impact of these factors on mainstream LLMs' causal identification performance.The experimental results, along with detailed discussions on how these factors affect LLMs' reasoning processes, are presented in Appendix B.</p>
<p>Identify Causality in Observational Data</p>
<p>The causal relationships inferred by LLMs from textual information are often influenced by subjectivity, ambiguity, and incompleteness inherent in textual data.In response, recent studies have explored using LLMs to analyze observational data directly for causal discovery [12].Unlike textual information, observational data provides an objective and detailed record of real-world phenomena, offering richer and more accurate information.By allowing LLMs to analyze observational data, it is possible to compensate for the limitations of textual sources, enabling the models to explore causal relationships from multiple dimensions.Researchers aim for LLMs to infer causality more effectively by examining correlations and covariances among variables in the data.However, we observe that even when supplied with observational data, the architecture and operational principles of LLMs may still limit their capacity to understand and accurately infer causal relationships.Unlike traditional CDAs [25], which directly manipulate numerical data through arithmetic operations (e.g., addition and multiplication), LLMs process data as tokenized strings [26].Numerical data encoded as tokens can distort original numerical features, hindering LLMs' ability to grasp true causal mechanisms.</p>
<p>This highlights inherent limitations in LLMs when processing numerical data for causal discovery, as proposed in the following Position: Position 3.2.Unlike traditional CDAs, LLMs lack intrinsic mechanisms to exploit numerical features.LLMs fail to effectively utilize high-precision numerical data for inferring causal patterns.Even with low-precision data, their causal discovery capabilities are constrained.</p>
<p>Empirical Studies: To systematically evaluate these limitations, we conduct a series of controlled experiments assessing the sensitivity of LLMs to numerical precision, their ability to recognize correlations, and their performance on benchmark causal discovery tasks.The detailed experimental setup, results, and further discussions on these limitations are presented in Appendix C.</p>
<p>Risks of Integrating LLMs with CDAs</p>
<p>Since the causal relationships derived from LLMs in textual and observational data are unreliable, recent studies have begun to explore the integration of LLMs with traditional CDAs [27,8].This section will discuss the issues that arise in the interaction between LLMs and these CDAs.In Section 2, we discussed four ways in which LLMs can be integrated with CDAs.The second method, which treats the LLM as a posterior, essentially places the decision-making process under the control of the LLM.This approach does not involve mutual correction or guidance between the CDA and the LLM; instead, it narrows the decision space to some extent (e.g., by using equivalence classes derived from the algorithm).The CDA relies on the LLM to resolve issues that it cannot handle, but fundamentally, it places full trust in the LLM's decisions in this space and does not involve a collaboration between the algorithm and the LLM.The issues faced by this approach are the same as those discussed in the previous section, and therefore, we will not explore them further in this section.</p>
<p>Research of Reliable Prior Knowledge: Other than Type (2), other types mentioned in Section 2 all involve using the results from LLMs as priors for CDAs.This is currently the mainstream paradigm for combining LLMs with traditional CDAs.In fact, the use of prior knowledge in causal discovery was studied even before the advent of LLMs.However, these prior knowledge studies typically assume reliable background knowledge [28][29][30][31].In causal discovery research, there is a significant difference between reliable and unreliable prior knowledge.Reliable prior knowledge is typically based on rigorous research, theories, or empirical evidence, and it is characterized by a high degree of certainty and accuracy.For example, background knowledge defined in [28] specifies directed edges that are either prohibited or required, which can directly guide the construction and orientation of the graph in an algorithm.However, as discussed in Section 3, LLMs clearly fall under the category of unreliable prior knowledge.Due to the quality of textual data and model limitations, LLMs contain numerous errors and uncertainties, making it difficult to accurately assess their reliability.</p>
<p>Reliable prior knowledge is strongly consistent with causal models and can effectively assist in their construction and inference.Some existing studies use reliable background knowledge to narrow the search space for causal relationships, improving inference accuracy and efficiency [28][29][30][31].On the other hand, unreliable prior knowledge may not align with the true causal relationships.A potential risk is that if the LLM misjudges the causal relationships, the algorithm may search in the wrong direction, making it harder to find the correct causal relationships.Furthermore, the uncertainty in LLM-generated knowledge leads to instability in causal discovery results, affecting model reproducibility and making it difficult to provide stable and reliable causal inferences in practical applications.We present the following viewpoint and analyze the issues faced by constraintbased methods and score-based methods when embedding LLM priors.Position 4.1.When using causal relationships identified by LLMs as priors in CDAs, the unreliable prior knowledge provided by LLMs undermines the reliability and theoretical guarantees of these algorithms.</p>
<p>Analysis of Score-based Methods: A significant body of research [5,19,32,27] embeds LLM results as prior knowledge within classical scoring functions.Due to data limitations and the uncertainty of prior knowledge, it may not always be realistic to strictly adhere to all constraints.Therefore, soft constraint methods have been proposed.The core idea is to introduce a fault tolerance mechanism when applying constraints, allowing some flexibility to accommodate potential conflicts between data and prior knowledge.This is achieved by modifying the scoring function, incorporating the prior constraints in the form of Bayesian priors, rewarding networks that satisfy specific structural constraints, and penalizing those that do not.According to Bayes' theorem, the relationship between causal structure (G), observed data (D), and prior constraints (λ) can be expressed as:
P (G|D, λ) = P (D|G, λ)P (G|λ) P (D|λ)(3)
Studies such as [5,19,32,27] rewrite the causal structure scoring function as follows:
σ(G; D) → σ(G; D, λ) = σ(G; D) + σ(G; λ)(4
) Here, σ(G; D) is the scoring function used in causal structure learning, such as the BIC or BDeu score, while σ(G; λ) is computed based on the prior constraints λ, considering the probability of specific structures.</p>
<p>The first problem with this type of method is the direct addition of σ(G; D) and σ(G; λ), while prior information and data are in different probability spaces.Specifically, σ(G; D) reflects the log-likelihood of the data, which is calculated under the sample data D. σ(G; λ) reflects the prior probability of the causal structure, which is independent of the data and is defined by the LLM.Here, λ is a soft-constraint converted from the results generated by the LLM according to certain preset rules, and it is usually not a strict probability distribution.This means that the direct addition of σ(G; D) and σ(G; λ) lacks mathematical correctness.In addition, the scales of the two are not compatible.Without proper normalization, the scales of the data-scoring term and the prior-scoring term may be extremely mismatched, causing one term to dominate the final scoring function.Taking the BIC score as an example:
σ BIC (G; D) = log P (D|G) − k 2 log N (5)
where k is the number of free parameters of the model, and the sample size N significantly affects the order of magnitude of the score value.σ(G; λ) which is independent of N , if not properly scaled, its impact may be drowned out by the data-scoring term, or conversely, it may have an unreasonable dominant effect on the final score.Taking the MDL score as another example:
σ MDL (G; D) = L(D|G) + L(G)(6)
where L(D|G) is the encoding length of the data given the network G, and L(G) is the encoding length of the network structure (structural complexity penalty term).They are usually measured in bits, and may not even match the dimension of the prior term σ(G; λ).Examples of non-matching dimensions also include cross-validation based [33] and structure entropy based scoring functions [34].Moreover, some scoring functions already have priors, and the direct introduction of σ(G; λ) conflicts with the existing priors.For example, the BDeu score by default uses the Dirichlet uniform prior and assumes prior independence of all variables [35], which is clearly contrary to the recognition results of the LLM.There is already a hyper -parameter in BDeu to control the influence of data on the posterior.After introducing σ(G; λ), it is even more necessary to balance the influence of the two priors and the data, which cannot be solved by simply adding σ(G; λ).</p>
<p>In short, if σ(G; D) in existing studies uses classical causal network scoring functions, they do not directly calculate log P (D|G), but calculate some other quantity related to log P (D|G).These quantities are usually approximations or positively correlated with the degree of data fitting, and at the same time, they carry other priors and penalty terms.And σ(G; λ) is the probability modeled for a specific structure according to the output of the LLM, and then the corresponding score is obtained, which does not depend on the specific statistical characteristics of the data.The consequence of direct addition is that the two parts of the scoring function are superimposed under different dimensions, which may lead to an unreasonable trade -off between data fitting and prior knowledge in the model.Eventually, σ(G; D) and σ(G; λ) produce scores of different scales in different probability spaces, and direct summation is very likely to cause one term to have too much influence on the overall scoring function, thus distorting the final result of causal structure learning.</p>
<p>The second problem of this type of methods is that introducing the LLM as a prior will undermine the theoretical basis and basic properties of causal network scoring function, such as Decomposablity and Score Local Consistency [36], which makes certain optimization algorithms inapplicable.In traditional causal discovery settings, a decomposable scoring function allows the overall causal structure score to be broken down into the sum of local scores for each variable and its parent set, i.e.,
σ(G; D) = n i=1 σ(v i , pa(v i ); D)(7)
This property makes it easier to compute and understand the scoring function, and also provides the foundation for many efficient causal structure learning algorithms, as these algorithms can independently evaluate and optimize each local structure.However, after introducing the soft constraint scoring function σ(G; D, λ) = σ(G; D) + σ(G; λ), for σ(G; λ), which is computed based on the prior constraints λ, the calculation sometimes depends on global features of the entire causal structure G rather than being naturally decomposable into local variables and their parent sets as in σ(G; D).For example, prior constraints may impose complex relationships among multiple variables, which cannot be easily decomposed into individual constraints for each variable.This leads to the inapplicability of many optimization algorithms designed based on decomposability.</p>
<p>On the other hand, the introduction of prior constraints σ(G; λ) in the soft constraint scoring function disrupts Score Local Consistency.Prior knowledge is often based on external assumptions or empirical observations, which may not always be accurate or applicable.When the local structure of the causal graph changes, the changes in σ(G; λ) may not align with the actual relationships between variables.For example, prior knowledge may erroneously emphasize certain causal relationships between variables, and when the local structure changes to better reflect the data characteristics, the prior constraints may still assign high scores to the incorrect structure, or give unreasonably low scores to the newly proposed structure.This undermines the ability of the soft constraint scoring function to accurately reflect the validity of local structural changes.These issues break the theoretical foundations of the scoring function in practical applications.</p>
<p>Analysis of Constraint-Based Methods:</p>
<p>The core of constraint-based methods is conditional independence testing.The simplest way to embed LLMs is through textual prompts, asking the LLM to determine conditional dependencies and independencies between variables [13,24], which corresponds to the fourth method discussed in Section 2. The main issues with this approach have already been analyzed in Section 3. Another way to incorporate unreliable priors is by considering prior information in the computation of the statistical measures for conditional independence tests.Some studies have made related attempts in causal feature selection, a downstream task in causal discovery.For discrete variables, constraint-based methods typically use the G 2 -test to determine the conditional dependence and independence between variables.Research such as [37][38][39] constructs different prior terms p based on background knowledge and uses p to adjust the computation of the G 2 -statistic [40], embedding the prior into the hypothesis testing process.The expression is:
G 2 (X, Y |Z) − p &gt; χ 2 α,f(8)
where p is calculated based on prior knowledge, α is the significance level of the hypothesis test, and f is the degrees of freedom of the G 2 (X, Y |Z) statistic.</p>
<p>However, although intuitively these studies seem to expand or shrink the rejection region through the term p, directly comparing G 2 (X, Y |Z) − p with χ 2 α,f is not reasonable.This is because the G 2statistic has a specific asymptotic distribution theory, and subtracting a value could distort its original statistical properties.The new statistic may no longer asymptotically follow a chi-squared distribution, thus invalidating hypothesis tests based on the chi-squared distribution.Even if G 2 (X, Y |Z) − p can be transformed to follow a chi-squared distribution, the related statistical properties and critical values, such as the degrees of freedom f , need to be derived anew.Therefore, unreliable priors, when introduced into conditional independence testing, will also break the theoretical guarantees.</p>
<p>Defining the Functional Boundaries of LLMs in Collaboration with CDAs</p>
<p>Functional Boundaries Illustration</p>
<p>Despite the limitations of LLMs in reliably identifying causal relationships, this does not render them entirely useless for causal discovery tasks.Through extensive pretraining on vast textual data, LLMs accumulate substantial world knowledge, encompassing various domain-specific concepts, facts, and relationships.This extensive knowledge can provide intuitive guidance for causal discovery by facilitating more efficient exploration of causal structures.However, their role must be strictly confined to a non-decisional auxiliary capacity.The core principle is: Position 5.1.In the collaboration between CDAs and LLMs, LLMs should not participate in determining the existence or directionality of causal relationships but can influence the search procedure for causal structures, thereby expediting the optimization process.</p>
<p>Specifically: (1) Prohibited Actions: LLM outputs must not serve as the final criterion for causal structures, such as directly deciding whether an edge exists between variables, the direction of edges, or acting as the core weight in scoring functions to influence decisions.(2) Permitted Actions: LLMs can be used for intermediate steps like initializing the search space, guiding mutation directions in evolutionary algorithms, or assisting with cycle detection.However, the final causal structure must rely entirely on reliable techniques, e.g., CDA scoring functions or constraint tests.</p>
<p>Case Study: Use LLM to Guide Heuristic Search in CDAs</p>
<p>To substantiate Position 5.1, we present a concrete case study in this section: employing LLMs to guide heuristic search.Heuristic search is a commonly used strategy in causal discovery [41], including methods such as hill climbing and evolutionary algorithms [42].These techniques leverage heuristic information to enhance search efficiency.Traditional heuristic search methods often rely on random exploration during iterative optimization, which is inefficient and prone to local optima.In this section, we replace random search with LLM-driven targeted search while retaining classical scoring functions (e.g., BIC and BDeu scores) as optimization objectives.By integrating LLMs' knowledge and reasoning capabilities into the heuristic search process, search algorithms can intelligently select search directions, thereby accelerating convergence to the optimal solution.We incorporate LLMs into heuristic search from three perspectives:</p>
<p>(1) LLM-Based Initial Population Initialization: Before initiating heuristic search, LLMs analyze the causal structure space preliminarily.Given a causal learning dataset, variable information and their background knowledge are input into the LLM, which assesses their potential relationships.Variable pairs deemed highly unlikely to have causal relationships by the LLM are pruned from the search space at the initialization stage, significantly reducing the complexity.This well-informed initialization allows heuristic search to commence from a relatively accurate and sparse DAG, enhancing initial search efficiency.</p>
<p>(2) LLM-Guided Evolutionary Optimization: LLM is used to guide population evolution, as crossover and mutation operations.During mutation, given the current causal structure, LLMs receive detailed structural information along with mutation objectives (e.g., increasing diversity or refining local structures) and generate plausible mutation proposals.Unlike traditional random mutations, LLMs suggest where to add or remove edges based on their domain knowledge.Similarly, during crossover, LLMs evaluate two parent causal structures and recommend reasonable crossover points and strategies, ensuring offspring inherit desirable traits while exploring new structural spaces.</p>
<p>(3) Cycle Detection and Resolution: During heuristic search, generated causal structures may contain cycles, violating the acyclic constraint of DAGs.We employ LLMs for cycle detection and resolution.Once a new causal structure is formed and cycles are detected, its information is input into the LLM, where node and edge relationships are analyzed.LLMs suggest edges to remove or adjust based on their understanding of variable dependencies, ensuring the resulting causal structure remains a valid DAG.</p>
<p>We present the experimental results on bnlearn datasets of different scales in Appendix D, demonstrating the performance advantages of LLM-guided causal discovery.We also introduce other potential extensions of LLM-assisted evolutionary optimization for CDA in Appendix D, including the dynamic adjustment of search strategies, parameters, and uses accumulated search information to predict regions with a higher chance of having optimal solutions.</p>
<p>Alternative Views Based on "Manipulability of Experimental Results"</p>
<p>Actually, most of the studies listed in Appendix A support the application of LLM in causal discovery.These studies typically prove their points from the perspective of empirical research on small-scale networks (with the number of nodes less than 100), and indicate that LLMs can provide relatively precise causal relationship identifications [4,3,43].Some studies also support the collaboration of LLM and traditional CDA, and claim that even when LLMs yield incorrect results, score-based methods driven by soft constraints can still guide the CDA toward producing correct outputs [19,5,27].These findings appear to contradict the claim made in our position that "LLMs cannot discover causality."Next, we first demonstrate why existing studies can still obtain favorable experimental results even though LLMs cannot identify causal relationships.Subsequently, we conduct a comprehensive experimental comparison among existing LLM-based causal discovery methods, traditional CDAs, and our proposed LLM-based heuristic search, highlighting the differences between these approaches.</p>
<p>Inflated Performance Outcomes via Prompt Engineering: As analyzed in Section 3, LLMs' causal identification accuracy heavily relies on prompt-provided textual information, especially explicit background knowledge about variable relationships.Carefully designed prompts can artificially inflate performance-for instance, when prompts explicitly state known causal links (e.g., "Gene X causes Disease Y" from medical literature), LLMs merely retrieve preexisting knowledge rather than discover novel causality.This setup bypasses genuine causal inference, as the goal of causal discovery is to uncover unknown relationships, not replicate known ones.While LLMs can parrot causal statements under such conditions, their conclusions lack reliability, being neither consistently correct nor theoretically grounded.In global causal network discovery, incorporating LLM-generated priors into scoring functions (Section 4) exhibits a similar paradox: flawed methodologies yield empirically improved results.This stems from two factors: (1) Soft-constrained scoring functions, though theoretically unsound, may coincidentally align numerically when prior and data terms are scaled similarly; (2) In experiments (e.g., on 'bnlearn' datasets), researchers often leverage groundtruth knowledge in prompts to manually engineer favorable outcomes, particularly on small-scale networks where prompt refinement is feasible.</p>
<p>To validate our argument, we experimented with soft-constrained scoring functions using GPT-4 on 14 small datasets from 'bnlearn' in Appendix E. We tested two prompt types: (1) High-quality prompts, manually curated and refined to ensure accurate causal statements with minimal redundancy;</p>
<p>(2) General prompts (Low-quality), sourced from online repositories like Wikipedia, containing redundant and potentially incorrect causal information (see Figure 7).Results show that despite varying levels of incorrect prior knowledge, soft-constrained scoring functions consistently struggle to filter out errors.Under low-quality prompts, most erroneous priors from LLMs persist.While improving prompt quality reduces errors, this improvement stems from manual refinement rather than the effectiveness of soft constraints, highlighting their limitations.</p>
<p>Conclusion: A Call to the Community</p>
<p>This paper challenges the prevailing enthusiasm for using LLMs in causal discovery, revealing fundamental limitations in their ability to identify causal relationships and the risks of integrating them into CDAs.However, our critique does not dismiss LLMs entirely.We propose a cautious, theorypreserving role for LLMs as heuristic guides in causal structure search.Based on the discussion, we present the call to the community:</p>
<p>A Summary of Existing Studies for LLM-based Causal Discovery</p>
<p>Causal discovery is a fundamental task in causal inference [1], aiming to uncover underlying causal relationships from observational data.Traditional causal discovery methods, including constraint-based and score-based approaches [17], are grounded in statistical analysis and mathematical reasoning, providing rigorous and interpretable frameworks for inferring causal structures.While these methods have been widely applied across various domains, recent advancements in LLMs have introduced new opportunities for causal discovery.Unlike purely data-driven approaches, LLM-based methods can leverage rich linguistic knowledge, common sense reasoning, and domain-specific insights to complement traditional techniques.To provide a comprehensive overview, we summarize existing methods in Table 1.Based on these studies, we introduce current approaches along three key dimensions: the form of causal relationships considered, the design of prompts, and the integration strategies between LLMs and traditional algorithms.</p>
<p>Pairwise Causality and Global Causal Graph: Existing LLM-based causal discovery methods can be broadly categorized into two main groups: those focused on pairwise causal relationships and those aimed at learning the full causal graph.The key distinction between these two categories lies in their scope and technical approach.The first category of LLM-based methods concentrates on inferring the causal direction between a pair of variables (A←B or A→B) or determining the existence of a causal relationship [3,11].These approaches leverage the language understanding and reasoning capabilities of LLMs to make judgments about the causal connections between individual variable pairs.The key idea is to prompt the LLM with the variable names, potentially along with some contextual information, and have the model output its assessment of the causal relationship.Some studies have shown that LLMs like GPT-4 can outperform traditional CDAs in some tasks [4], but other studies also recognized the limitations that LLMs may simply repeat patterns embedded in their training data, rather than engaging in genuine causal reasoning [18].In contrast, the second category of LLM-based methods aims to uncover the complete causal graph, involving all the variables in the system [8,19].These approaches go beyond pairwise relationships and seek to model the overall causal structure.Typically, these methods combine the language understanding of LLMs with data-driven CDAs, where the LLMs are used to generate causal insights or knowledge that can then be integrated with numerical reasoning techniques.The LLMs' ability to reason about the contextual information and leverage domain knowledge plays a crucial role in these full graph discovery tasks, which are generally more complex than the pairwise setting.</p>
<p>Prompt Design for Identifying Causality: The design of the prompt plays a critical role in guiding LLMs' causal reasoning capabilities in both pairwise and full causal graph discovery tasks.</p>
<p>Researchers have explored incorporating various types of information into the prompts to provide the necessary context and guidance for the LLMs: (1) Variable information [6]: By explicitly specifying the variables involved, the LLMs can better understand the entities and their potential causal relationships.(2) Background knowledge [7]: This background knowledge can range from simple problem descriptions to more detailed information from scientific literature [13].The rationale behind this is to equip the LLMs with relevant domain expertise and contextual understanding, which can provide them with information and context they may not have encountered during the pre-training stage.</p>
<p>(3) Observational data [12]: By exposing the LLMs to the actual data, either in the form of raw data or summarized statistics, the models can leverage the empirical information to inform their causal reasoning, potentially leading to more grounded and reliable conclusions.( 4) Results of CDA [20]: In the case of full causal graph discovery, researchers have also explored including intermediate causal discovery results in the prompts.This approach aims to guide the LLMs' reasoning by providing them with partial causal insights, which the models can then use to refine and expand the overall causal structure.The underlying idea is to leverage the LLMs' language understanding capabilities to integrate these intermediate results and generate a more comprehensive causal graph.</p>
<p>Collaboration Paradigm between LLMs and CDAs: Recent studies have explored hybrid approaches that integrate LLMs with traditional CDAs.These hybrid techniques seek to leverage the complementary strengths of both LLMs and established numerical techniques.Based on the different ways these two components are combined, these hybrid methods can be broadly categorized into four main types: (1) Initialization or Prior: LLMs are used to generate initial causal insights or hypotheses, which are then used to initialize or provide a prior for the traditional CDAs.This allows the algorithms to start from a more informed state [5,21,9].(2) Post-processing: The traditional CDAs are first applied, and the results are then further refined or expanded by the LLMs.The LLMs can leverage their language understanding and reasoning capabilities to build upon the outputs of the traditional methods [8,22,10].(3) Iterative Collaboration: The LLMs and traditional algorithms are used in an iterative, back-and-forth manner.The models can exchange information, with the LLMs providing contextual knowledge or insights that guide the traditional algorithms, and the algorithms offering numerical reasoning to validate or refine the LLMs' outputs [20,19,23].( 4) Modular Integration: LLMs can be used to implement specific modules or components within the traditional CDAs, such as scoring functions or conditional independence tests.This allows the LLMs to contribute their strengths to the overall causal inference process [13,24].This appendix provides a systematic analysis and empirical studies of the causality identification capabilities of LLMs, further validating and quantifying the key influencing factors discussed in Section 3. Specifically, we designed multiple experiments to examine the impact of word order, redundant information, and the distance between causal entities on the causal reasoning ability of LLMs.</p>
<p>Recall</p>
<p>Explicit -Cause First Explicit -Effect First Implicit -Cause First Implicit -Effect First First, it is essential to ensure that the causal relationships in the context are clear and familiar.Starting from the autoregressive mechanism, the representation form of causal relationships and the word order in the prompt should be similar to the causal patterns that frequently appear in the training corpus.The example of "rain" mentioned in Section 3 can intuitively reflect the motivation for this requirement.To verify the impact of word order in the text on the ability of LLMs to identify causal relationships between entities, we tested the performance of several mainstream LLMs, including GPT-3.5, GPT-4o, Claude 3.5, Llama 3.1, Gemini 1.5, and Falcon-40B, using different sentence patterns and word orders.Specifically, we extensively collected test examples from large -scale general -purpose corpora (such as Wikipedia, news corpora, etc.) and corpora in specific fields (such as medical and scientific literature).The expressions of causal relationships are divided into two categories.One category consists of words that explicitly indicate causal relationships, such as:</p>
<p>Because, Since, Therefore, Consequently, Thus, Hence, As a result, Due to, Owing to, Resulting in, For this reason, So, Caused by, Cause -Effect.</p>
<p>The other category consists of words that implicitly suggest causal relationships, such as:</p>
<p>Trigger, Lead to, Bring about, Give rise to, Imply, Influence, Promote, Encourage, Contribute to, Pave the way for, Stem from, Derived from, Necessitate.</p>
<p>Based on these causal words, we constructed a series of text sets containing causal relationships.</p>
<p>For each text, we rewrote it to generate two versions: one with the cause preceding the effect and the other with the effect preceding the cause.Meanwhile, we added redundant information and intermediate events to increase the complexity of the task.We also detected the detection accuracy of LLMs for true and false causal relationships, and Figure 2 shows the precision and recall of each LLM under different conditions.Through testing with different sentence patterns and word orders, we observed several notable trends.Firstly, LLM generally perform better on texts with explicit causal relationships compared to texts with implicit causal relationships, which indicates that LLMs can more effectively identify and understand causal relationships when dealing with clear causal words.Secondly, when we adjusted the order of entities, that is, placing the effect before the cause, the performance of some LLMs significantly decreased, especially in texts with implicit causal relationships.This result shows that LLMs are sensitive to the order of word and may rely on common causal patterns in training for reasoning.Therefore, the structure of the text plays a crucial role in the model's understanding process.Finally, by comprehensively analyzing the performance of various models, we found that models with higher recall often have lower precision.This phenomenon indicates that these LLMs have certain inclinations.In some cases, such as GPT-4o, it is more inclined to judge "there is a causal relationship", while in other models, such as Falcon-40B, it is more inclined to judge "there is no causal relationship".This inclination, derived from the training data, may reflect the inherent biases of the models in causal reasoning, reminding us to be cautious when interpreting their discrimination results in practical applications.</p>
<p>B.2 Analysis of Factor (2) in Position 3.1</p>
<p>In addition to the special requirements for word order, there should not be too many entities or redundant information in the text, which can be understood from the perspective of information entropy.In causal learning, we are concerned with the mutual information I(x c ; x e ) between the cause and the effect, where x c and x e represent the sets of words related to the cause and the effect, respectively.Consider the conditional mutual information:
I(x c ; x e |x z ) = H(x c |x z ) + H(x e |x z ) − H(x c , x e |x z )(9)
where x z represents other information in the text.Excessive entities and redundant information make P (x c , x e |x z ) more dispersed, resulting in an increase in the joint entropy H(x c , x e |x z ), forcing entities that are far apart in the text to tend to have no causal relationship.At the same time, redundant entities make the conditional entropies H(x c |x z ), H(x e |x z ) and H(x c , x e |x z ) more complex, making it difficult for the model to accurately grasp the mutual information between the cause and the effect given these interfering information, thus increasing the difficulty of understanding causal relationships from the text.</p>
<p>To investigate the impact of redundant entities on the ability of LLMs to recognize causal relationships in text, we continued to test several widely popular LLMs, including GPT-3.5, GPT-4o, Claude 3.5, Llama 3.1, Gemini 1.5, and Falcon-40B.First, we identified a series of simple causal relationship examples as the basic texts, with the form such as "Because (Entity 1), so (Entity 2)".Each basic text clearly expressed a causal relationship and was easy to understand.Different types of basic texts covered multiple fields such as life, nature, and society to ensure the comprehensiveness of the experiment.Then, we added redundant information to each basic text.The main positions for adding redundant information included the following four types:</p>
<p>• Redundancy before the first entity: Adding redundant text or entities before the first entity of the basic text.</p>
<p>• Redundancy between two entities: Inserting redundant content between the first and the second entities of the basic text.</p>
<p>• Redundancy after the second entity: Adding redundant parts after the second entity of the basic text.</p>
<p>• Redundancy at both ends: Adding redundant information before the first entity and after the second entity of the basic text simultaneously, but not between the two entities.</p>
<p>According to the above designed method, a large number of test texts with different redundant positions and different redundancy measures were generated.In the comparison, similar numbers of words and the same number of entities were added at different redundant positions.For each selected LLM, the texts in the test set were successively input into the model, and the recognition results of the causal relationships in the text by the model were obtained.When calling the model, the input format and questioning method were kept consistent.The causal relationship recognition results output by the model were compared with the pre-annotated correct causal relationships.For each test text, it was recorded whether the model correctly recognized the causal relationship and whether it wrongly recognized a non-existent causal relationship.The recall and precision rates of each model under different redundant positions were calculated separately, and the results are shown in Figure 3.</p>
<p>Overall, when redundant information was added separately in the front and back of the causal text, the impact on the precision and recall rates of each model was not significant.However, when comparing the front and back positions, adding redundant information before the first entity had a relatively greater impact on the model performance than adding it after the second entity.When redundant information was added simultaneously at the front and back of the causal text, the precision and recall rates of each model decreased to a certain extent.This, to some extent, reflects that LLMs tend to pay more attention to the information at the beginning and end of the text when processing it.When the key causal information is in the middle of the text, it is easily ignored by the model due to the interference of redundant information at the front and back, thus affecting its accurate recognition of causal relationships.Adding redundant information between the two entities had a great impact on the ability of each model to recognize causal relationships.This is because the redundant information at this position widens the distance in the text space between the two causally related entities that were originally closely associated.LLMs usually process text based on the autoregressive mechanism of word order, and this mechanism is significantly affected when the text-based association between entities is weakened.These results fully demonstrate that the ability of LLMs to recognize causal relationships from text is affected by redundant information in the text.This, from the side, reflects that the process of LLMs recognizing causal relationships is not really about excavating the internal causal mechanisms between entities in the text, but rather more about summarizing the surface information of the text.When processing text, LLMs rely on the word order and information distribution presented in the text.When redundant information changes the text structure and information layout, the model's judgment of causal relationships will be interfered with, making it difficult to accurately grasp the true causal logic behind the text.</p>
<p>B.3 Analysis of Factor (3) in Position 3.1</p>
<p>From Figure 3, we discover that adding redundant information between the two entities had a great negative impact, which aligns with the Factor (3) in Position 3.1.Due to the technical characteristics of word embedding techniques and the attention mechanism, words that are adjacent in text positions usually have stronger semantic and syntactic associations.Therefore, the embedding vectors corresponding to tokens that are far apart and have no obvious semantic associations tend to be orthogonal (according to the properties of high-dimensional vectors), and correspondingly, the attention scores will be lower.On the other hand, from the perspective of information transfer, when calculating the degree of attention of one word to other words, the information carried by words that are far apart will have a smaller contribution weight to the current word after Softmax normalization.If the entities to be analyzed are far apart in the text, the LLM will have difficulty capturing the causal relationship between the two entities.To verify the impact of "entity distance" under different measurement methods on the aforementioned LLMs' ability to identify causal relationships, we adopt two ways to measure the distance between two entities, based on the number of words and the number of entities respectively:</p>
<p>• Based on the number of words: For each position where redundancy is added, redundant texts with word counts in the ranges of 0 -10, 10 -25, 25 -50, 50 -100, and 100 -150 are added respectively, thereby generating different versions of test texts.</p>
<p>• Based on the number of entities: For each position where redundancy is added, redundant entities with the number of entities in the ranges of 1 -2, 3 -4, 5 -6, 7 -8, and 9 -10 are added.</p>
<p>We select a variety of common causal relationship scenarios, such as weather and traffic, disease and symptoms, behavior and consequences, etc.In each scenario, two entities of the core causal relationship are determined.For example, in the weather-and-traffic scenario, "sudden heavy snowfall" and "traffic congestion" are taken as the causal entities.Subsequently, different numbers of words and other entities are added between the two core entities to control the lexical distance.The added content is used to describe intermediate events, background information, irrelevant details, etc., constructing text samples with different lexical distances.For each LLM, the texts in the test set are successively input into the model, and the model's recognition results of the causal relationships in the text are obtained.The causal -relationship recognition results output by the model are compared with the pre -annotated correct causal relationships, and the recall and precision rates of each model under different lexical distances are calculated.The experimental results are shown in Figure 4.</p>
<p>From the results, it is not difficult to find that as the number of redundant words and redundant entities between the two entities increases, both the precision and recall of LLMs in identifying causal relationships gradually decline.Among them, GPT-4o has a continuous and significant advantage in recall compared with other LLMs, but its precision is similar to that of other LLMs.The decline in recall means that some distant causal relationships cannot be recognized.We find that as the number of redundant entities increases, the identification of causal relationships by LLMs gradually becomes random, and the results of multiple experiments are also unstable.On the other hand, the decline in precision means that more and more irrelevant entities are recognized as causal relationships, and we observe that these irrelevant entities are mostly distributed near the other entity.All these reflect that the entity distance has a significant impact on LLMs' identification of causal relationships.In fact, for the requirements in ( 2) and (3), an example can be used to show the impact of text on the LLM.For example, consider the following text:</p>
<p>Last night, there was a sudden heavy snowfall (Snow).The sanitation workers went out early in the morning to clear the snow on the main roads (Clean).The bus company temporarily adjusted some bus routes (Bus) to deal with possible road safety problems.Due to the slippery roads, pedestrians walked carefully (Pedestrian), and there was traffic congestion during the morning rush hour today (Traffic Jam).</p>
<p>As shown in Figure 5, because the autoregressive model reasons based on sequential dependence, the LLM sees the model in Figure 5(a).But in fact, the node Snow is the direct cause of the node Traffic Jam (as shown in Figure 5(b)).However, due to the many redundant events in the middle and the fact that Snow and Traffic Jam are far apart in the text, the LLM has difficulty capturing this complex actual causal relationship and is likely to regard intermediate events (such as Pedestrian) as the cause of Traffic Jam.</p>
<p>C Analysis and Empirical Studies of Position 3.2</p>
<p>This appendix investigates the effects of numerical precision on LLMs' ability to understand correlation and causation in data and evaluates their performance using causal benchmarks.</p>
<p>We begin by assessing the sensitivity of LLMs to numerical precision in the simplest task of recognizing correlations.Classic three-variable causal graphs are employed, with predefined causal mechanisms and distributions.Consider three variables X, Y , and Z, connected in typical causal structures such as X → Y → Z, X ← Y → Z, and X → Y ← Z. Data samples are simulated based on these causal graphs using Monte Carlo methods.Each variable's distribution is determined according to its causal relationships.For instance, if X is exogenous, it is sampled from a normal distribution N (µ 1 , σ
1 ). If Y is influenced by X, it is generated as Y = f (X) + ϵ 1 , where ϵ 1 ∼ N (0, σ2
2 ).Similarly, Z is generated based on its causal relationships with X and Y .A total of 100 samples are generated, each containing full observations of the three variables.</p>
<p>To explore the effect of numerical precision, we vary the level of precision in the observational data, retaining 1 to 8 significant digits through rounding.This simulates observational data of varying precision.These datasets are then formatted as prompts suitable for LLM input and tested on GPT-3.5, GPT-4o, Claude 3.5, Llama 3.1, Gemini 1.5, and Falcon-40B.The task requires models to identify correlations and conditional dependencies among variables, and their outputs are evaluated using accuracy metrics.</p>
<p>As shown in Figure 6, increasing numerical precision does not improve LLM performance as seen with traditional algorithms.Instead, LLMs exhibit a slight performance improvement at low precision (e.g., 1-2 significant digits) but degrade as precision increases.Ultimately, all LLMs achieve an F1 score near 0.5 at higher precision levels, indicating a performance equivalent to random guessing.This suggests that LLMs struggle to interpret high-precision numerical data.When numbers are encoded as tokens, the encoding process likely distorts critical numerical features, leading to information loss or misrepresentation.At lower precision levels, essential features are less affected by encoding, allowing models to leverage their language understanding and pattern recognition capabilities to identify causal relationships.However, as precision increases, the tokenized representation introduces excessive detail, hindering the extraction of key causal features.To further evaluate LLMs' ability to infer causality from observational data, we utilize benchmark datasets from the bnlearn community [70].These datasets span various domains, including healthcare, finance, and environmental sciences, and feature varying scales.For this experiment, we select 14 small-scale datasets with fewer than 100 nodes, as listed in Table C.These datasets are well-suited for LLMs given their prompt length constraints.Observational samples from these datasets, which reflect variable relationships, are formatted as prompts and tested on GPT-3.5 and GPT-4, the two best-performing models from the previous experiment.</p>
<p>Performance is measured using two metrics: the F1 score and Structural Hamming Distance (SHD) [71].The F1 score captures the trade-off between precision and recall in identifying causal relationships.SHD measures the discrepancy between the inferred causal structure and the ground truth, accounting for added, reversed, and missing edges.Lower SHD values indicate more accurate graph structures.Experimental results, summarized in Table C, show that GPT-4 significantly outperforms GPT-3.5.However, GPT-3.5 only succeeds in identifying causal relationships in the simplest datasets, such as the Earthquake dataset, where observational data is beneficial.For higher-dimensional datasets, even GPT-3.5 struggles to extract meaningful causal information.Both models show declining performance with increasing dataset complexity.As dataset size grows, F1 scores decrease, indicating reduced precision and recall, while SHD values rise, reflecting greater structural differences from the ground truth.This trend underscores LLMs' limited capacity to extract causal relationships from observational data.</p>
<p>D Experimental Evaluation for LLM-guided Causal Discovery</p>
<p>We conducted experiments to evaluate the effectiveness of LLMs as initialization tools and evolutionary operators in causal learning, examining their impact on algorithmic performance.We utilized 10 real-world causal network datasets from the causal community benchmark bnlearn, which offer well-defined variables and reliable prior knowledge.These datasets vary in scale: small (e.g., Asia, Sachs), medium (e.g., Insurance, Water, Alarm, Barley), large (e.g., Hepar II, Win95PTS), and extra-large (e.g., Pathfinder, Andes).Each dataset contains 1000 samples, and each experiment was independently run 30 times to compute the mean results for robustness.</p>
<p>Initial Population Initialization:</p>
<p>We compared LLM-based initialization with two classical methods, i.e., Conditional independence-based method (CI-Based), which removes independent edges based on conditional independence tests to reduce the search space, and Mutual information-based method (MI-Based), which prunes the initial search space using mutual information as a relevance measure.We employed the top three performing LLMs from Section 3 (GPT-4, Claude-3, GPT-3.5) to assist in search space reduction.Variable information and background knowledge were formatted into specific prompts and input into the LLM to obtain relationship scores.</p>
<p>Figure 7 presents the experimental results.On small datasets, the CI-based method performs best due to its accurate reduction of the search space.However, on medium and large datasets, LLM-based search space reduction significantly outperforms other strategies.In some datasets, LLM-based performance is comparable to a combined CI-MI method, likely due to complex variable names limiting LLMs' effectiveness.This indicates that LLMs exhibit substantial potential in handling large-scale datasets by leveraging extensive prior knowledge to eliminate irrelevant variables and reduce search complexity, thereby providing a more efficient starting point for causal learning.</p>
<p>LLM-based Evolutionary Operators: In the evolutionary operation phase, we compare LLMassisted evolution with traditional evolutionary methods.Specifically, we consider two traditional approaches: (1) EO1, where the crossover operation employs a conventional uniform crossover, and mutation is performed using standard bit-flipping; (2) EO2, where the crossover stage adopts a traditional parent-based crossover, while mutation remains standard bit-flipping.For the LLM-assisted evolutionary process, we adopt the Tree of Thoughts (ToT) prompting strategy, which provides the LLM with optimization task information, contextual examples, and expected output formats to guide crossover and mutation operations.Selection is performed using tournament selection.In contrast, EO1 and EO2 follow their respective predefined evolutionary operations.</p>
<p>To evaluate the effectiveness of these approaches, we conduct experiments under different numbers of fitness evaluations (200, 400, 600), observing the convergence behavior of each method.As illustrated in Figure 8, among all LLM-based methods, GPT-4 exhibits the best performance across different fitness evaluation budgets.This advantage can be attributed to its larger training corpus, broader background knowledge, and stronger reasoning capabilities.Although the performance of GPT-3.5 and Claude 3 is slightly inferior to that of GPT-4, they achieve results comparable to the traditional evolutionary operation methods EO1 and EO2.This validates the effectiveness of LLM-assisted evolutionary operations, demonstrating that well-trained LLMs, with their extensive pretraining on world knowledge, can match or even surpass traditional evolutionary crossover and mutation strategies.These findings suggest that incorporating LLMs as evolutionary operators can enhance the search efficiency and accuracy of causal learning algorithms, enabling faster discovery of optimal causal structures.</p>
<p>Potential Extensions of LLM-Assisted Evolutionary Optimization: Beyond the aforementioned approaches, LLMs offer several additional possibilities for assisting causal structure optimization.One promising direction is the dynamic adjustment of search strategies.As the search progresses, the LLM can be utilized to dynamically assess the search state and adjust the strategy accordingly.</p>
<p>If the LLM detects that the search algorithm has stagnated in a particular region-i.e., multiple consecutive iterations yield no significant improvement-it can leverage its understanding of the current search space to suggest modifying the step size or adjusting evolutionary parameters such as mutation probability and crossover rate.These adjustments enable the search algorithm to escape local optima and continue exploring more promising solutions.Furthermore, the LLM can utilize accumulated search information to predict regions with a higher likelihood of containing optimal solutions.By guiding the search algorithm toward these regions, the overall search efficiency can be further enhanced.This capability highlights the potential of LLMs not only as passive evolutionary operators but also as active agents in shaping adaptive and intelligent search strategies.</p>
<p>E Empirical Studies for Alternative Views</p>
<p>Manipulability of Experimental Results: To validate our argument, we conducted a series of experiments on soft-constrained scoring functions using GPT-4.Specifically, we selected 14 small datasets from 'bnlearn' and tested two types of prompts on each dataset.The first type, highquality background knowledge prompts, underwent rigorous manual curation, thorough verification, and careful refinement to eliminate redundant information.These prompts contained direct causal statements and ensured close textual proximity between variables, often explicitly stating causal relationships.The second type, general background knowledge prompts, sourced information from online repositories such as Wikipedia, contained substantial redundant information, and lacked guarantees regarding the correctness of causal knowledge, as illustrated in Figure 7.</p>
<p>Experimental results clearly demonstrate that, despite substantial differences in the proportion of incorrect prior knowledge under the two prompt settings, the proportion of incorrect prior knowledge rejected by the soft-constrained scoring function remains consistently low.This finding indicates that soft-constrained scoring functions are highly limited in their ability to filter out incorrect priors.Under low-quality prompts, most erroneous priors generated by LLMs are retained.However, by improving prompt quality, the likelihood of LLMs generating incorrect priors decreases, consequently reducing the proportion of errors undetected by the soft constraints.Importantly, these reductions in errors do not stem from the capability of the soft constraints themselves but rather from the manual enhancement of textual quality at an additional cost.</p>
<p>Low-quality Prompt</p>
<p>3 :Section 4 :Section 5 :Section 6 :
3456
You are an expert in [Domain].Currently, you are studying the causal relationship between variables A and B, where: [A and B's Variable Information]; [A and B's Background knowledge].The Observational Data are as follows: LLM only influences initialization and search process, while the existence and directionality of causality are ultimately determined by the score function in CDA.Assist Causal Graph Search, But Not Influence Decision-Making of Causality Section Critique of using LLMs for direct causality identification (i.e., hard knowledge), focusing on inherent flaws in LLMs' causal reasoning Critique of integrating LLMs as prior knowledge (i.e., soft knowledge) into CDAs, addressing issues in their collaboration with CDAs.Defining the scope of LLMs' capabilities in causal discovery tasks Section 2: A comprehensive review of existing LLM-based causal discovery methods, laying the foundation for the comprehensive critique in this paper.Reveal the manipulability of experimental results in LLM-based causal discovery and explain why existing literature reports consistently favorable results.</p>
<p>Figure 2 :
2
Figure 2: The precision and recall of different LLMs under impact of causal relationship expression.</p>
<p>PrecisionFigure 3 :
3
Figure 3: The precision and recall of different LLMs under impact of redundant expression.</p>
<p>Figure 4 :
4
Figure 4: The precision and recall of different LLMs under impact of redundant expression.</p>
<p>sequential dependence in autoregressive LLM.(b) Actual Causal DAG.</p>
<p>Figure 5 :
5
Figure 5: An example of Factors (2) and (3) in Position 3.1.</p>
<p>Figure 6 :
6
Figure 6: The F1 Score of different LLMs under different significant figures.</p>
<p>Figure 7 :
7
Figure 7: The F1 Score of different initialization methods on 10 bnlearn datasets.</p>
<p>Figure 8 :
8
Figure 8: The F1 Score of different evolutionary operators on 10 bnlearn datasets.</p>
<p>Figure 9 :
9
Figure 9: The wrong prior knowledge proportion and rejected prior knowledge proportion on different datasets under low-quality and high-quality prompts.</p>
<p>1 .
1
Exercise Caution with LLM-Based Causal Discovery Methods: Practitioners must recognize that current LLM-based algorithms for causal relationship identification lack theoretical guarantees and may produce unreliable results.2.Re-Evaluate Experimental Designs: Many existing studies demonstrating "successful" LLMdriven causal discovery may inadvertently benefit from prompt engineering or information leakage (e.g., ground-truth knowledge in prompts).The community should adopt transparent, unbiased evaluation frameworks that exclude such artifacts, focusing on scenarios where causal relationships are genuinely unknown.3.Prioritize Theoretical Rigor in Method Development: Integrating LLMs as priors or decision components in CDAs should not circumvent the field's foundational requirement for theoretical soundness.If LLMs are to be used, their role should be strictly confined to non-decisional tasks (e.g., search acceleration) to preserve CDA guarantees.Researchers should resist the temptation to justify LLM use with ad-hoc rationales that overlook their fundamental limitations.</p>
<ol>
<li>Invest in LLM Architecture and Training for Causal Reasoning: The community should explore how causal discovery can benefit from LLMs from the perspectives of LLM architecture and training process.It is not advisable to force general-purpose LLMs to identify causal relationships through prompt engineering without making any modifications to LLMs as in existing methods.</li>
</ol>
<p>Table 1 :
1
Overview of Causal Discovery Research Based on LLM (Part 1)
ResearchFormofPrompt ContentHow to Combine with Causal Dis-MaximumIdentifiedcovery Algorithm (CDA)DAG in Ex-Causalityperiment[44]PairwiseVariable Information,N/A2Background Knowledge[45]PairwiseVariable Information,N/A4Causal Statement amongVariables[7]PairwiseVariable Information,N/AN/ABackground Knowledge[46]PairwiseVariable Information,N/A6Background Knowledge[3]Global DAG Variable InformationN/A12[8]Global DAG Variable InformationUse CDA to find Markov equiva-37lence classes and use LLM to obtainthe DAG[5]Global DAG Variable InformationTake the LLM's results as prior and48incorporate into a score-based CDA[32]Global DAG Variable InformationTake the LLM's results as prior and37incorporate into a score-based CDA[11]PairwiseVariable Information,N/A6Background Knowledge[20]Global DAG Variable Information,Initialize with LLM, then obtain a15Background Knowledge,DAG using CDA, and finally refineCDA Resultsthe DAG using LLM[47]PairwiseVariable Information,N/AN/ACausal Statement amongVariables[22]Global DAG Variable InformationUse CDA to obtain an initial DAGN/Aand use LLM to refine the DAG[19]Global DAG Variable Information,Iteratively use the CDA and LLM84Background Knowledge[43]PairwiseVariable Information,N/A4Causal Statement amongVariables[4]PairwiseVariable CorrelationN/A6[12]PairwiseVariable Information,N/A12Observation Data[48]Global DAG Variable Information,N/A221Variable Correlation[23]Global DAG Variable Information,Iteratively use the CDA and LLM11CDA Results[49]PairwiseVariable Information,N/AN/ABackground Knowledge(Scientific Publication)[13]Global DAG Variable Information,Use a constraint-based CDA and11Background Knowledgeleverages LLMs to perform condi-(Scientific Publication)tional independence queries[50]PairwiseVariable Information,N/A25Observation Data, Back-ground Knowledge[18]PairwiseVariable Information,N/A2Background Knowledge[10]Global DAG Initial DAG, Variable In-Use CDA to obtain an initial DAG221formation, Backgroundand use LLM to refine the DAGKnowledge</p>
<p>Table 2 :
2
Overview of Causal Discovery Research Based on LLM (Part 2)
ResearchFormofPrompt ContentHow to Combine with Causal Dis-MaximumIdentifiedcovery Algorithm (CDA)DAG in Ex-Causalityperiment[21]Global DAG Initial DAG, Variable In-Take the LLM's results as prior and27formationincorporate into a score-based CDA[9]Global DAG Variable Information,Take the LLM's results as prior and18Background Knowledgethen use CDA[24]Global DAG Variable Information,Use a constraint-based CDA and11Background Knowledgeleverages LLMs to perform condi-tional independence queries[51]Global DAG Variable Information,N/AN/ABackground Knowledge,Observation Data[6]Global DAG Variable InformationN/A56[52]Global DAG Variable InformationN/A9[53]Global DAG Variable Information,Use LLM to obtain an initial DAG38Background Knowledgeand use a score-based CDA to refinethe DAG[54]PairwiseVariable Information,N/A9Background Knowledge,CDA Results[55]Global DAG Variable Information,Take the LLM's results as prior and70CDA Resultsthen use a CDA[27]Global DAG Variable Information,Take the LLM's results as prior and37Background Knowledgethen use a CDA[56]Global DAG Variable Information,N/A100Background Knowledge[57]Global DAG Variable InformationTake the LLM's results as prior and413incorporate into a score-based CDA[58]Global DAG Variable Information,Take the LLM's results as prior and22Background Knowledgethen use CDA[59]PairwiseVariable Information,N/AN/ABackground Knowledge[60]Global DAG Variable Information,Take the LLM's results as prior and5Background Knowledge,then use CDAStructured Data[61]Global DAG Variable Information,Use LLM to guide the selection of37Background Knowledgeintervention targets in active CDA.[62]Global DAG Variable Information,Take the LLM's results as prior and48Background Knowledgeincorporate into a score-based CDA[63]PairwiseVariable Information,N/AN/ABackground Knowledge[64]Global DAG Variable Information,N/A221Background Knowledge[65]Global DAG Variable Information,Use a constraint-based CDA and11Background Knowledgeleverages LLMs to perform condi-tional independence queries[66]Pairwise andVariable Information,N/A8Global DAGObservation Data, Back-ground Knowledge[67]Global DAG Variable Information,N/A100Background Knowledge(Scientific Publication)[68]Global DAG Textual DataN/A40[69]Global DAG Variable Information,N/AN/ABackground KnowledgeB Analysis and Empirical Studies of Position 3.1</p>
<p>Table 3 :
3
The impact of the observational data.
DatasetNode numberGPT-3.5GPT-4oF1F1 DSHDSHD DF1F1 DSHDSHD DCancer50.5673 0.54497.48.60.6468 0.67126.86.4Earthquake50.7202 0.78134.43.60.6485 0.73193.45.6Survey60.5788Fail7.2Fail0.6024 0.54226.57.97Asia80.5787 0.514818.424.20.5794 0.598818.221.8Sachs110.4450Fail54.6Fail0.4836 0.466328.552.92Child200.5018Fail62.1Fail0.4782 0.476683.684.1Insurance270.4709Fail256Fail0.5505 0.5849175167.4Water320.4855Fail68Fail0.4834 0.411968.6159.8Mildew350.4540Fail361.6Fail0.5452 0.5255 136.4149.5Alarm370.4740Fail347Fail0.4705 0.5126 406.4232.1Barley480.4862Fail437.2Fail0.4979 0.4765 312.4 145.41Hailfinder560.3985Fail1178.8Fail0.3620 0.2959 1420.6 1865.2Hepar II700.4797Fail1299Fail0.5023 0.4219 1172.2 1275.4Win95PTS760.4856Fail932.6Fail0.4912 0.3250 912.4 994.07</p>
<p>Review of causal discovery methods based on graphical models. Clark Glymour, Kun Zhang, Peter Spirtes, Frontiers in genetics. 105242019</p>
<p>A survey on causal discovery: theory and practice. Alessio Zanga, Elif Ozkirimli, Fabio Stella, International Journal of Approximate Reasoning. 1512022</p>
<p>Emre Kıcıman, Robert Ness, Amit Sharma, Chenhao Tan, arXiv:2305.00050Causal reasoning and large language models: Opening a new frontier for causality. 2023arXiv preprint</p>
<p>Can large language models infer causation from correlation?. Zhijing Jin, Jiarui Liu, Spencer Zhiheng, Mrinmaya Poff, Rada Sachan, Mona T Mihalcea, Bernhard Diab, Schölkopf, Proceedings of the 12th International Conference on Learning Representations. the 12th International Conference on Learning Representations2024</p>
<p>From query tools to causal architects: Harnessing large language models for advanced causal discovery from data. Taiyu Ban, Lyvzhou Chen, Xiangyu Wang, Huanhuan Chen, arXiv:2306.169022023arXiv preprint</p>
<p>Using gpt-4 to guide causal machine learning. Neville K Anthony C Constantinou, Alessio Kitson, Zanga, Expert Systems with Applications. 1261202024</p>
<p>Cheng Zhang, Stefan Bauer, Paul Bennett, Jiangfeng Gao, Wenbo Gong, Agrin Hilmkil, Joel Jennings, Chao Ma, Tom Minka, arXiv:2304.05524Nick Pawlowski, et al. Understanding causality with large language models: Feasibility and opportunities. 2023arXiv preprint</p>
<p>Causal discovery with language models as imperfect experts. Stephanie Long, Alexandre Piché, Valentina Zantedeschi, Tibor Schuster, Alexandre Drouin, ICML 2023 Workshop on Structured Probabilistic Inference {\&amp;} Generative Modeling. 2023</p>
<p>Applying large language models for causal structure learning in non small cell lung cancer. Narmada Naik, Ayush Khandelwal, Mohit Joshi, Madhusudan Atre, Hollis Wright, Kavya Kannan, Scott Hill, Giridhar Mamidipudi, Ganapati Srinivasa, Carlo Bifulco, 2024 IEEE 12th International Conference on Healthcare Informatics (ICHI). IEEE2024</p>
<p>Alcm: Autonomous llm-augmented causal discovery framework. Elahe Khatibi, Mahyar Abbasian, Zhongqi Yang, Iman Azimi, Amir, Rahmani, arXiv:2405.017442024arXiv preprint</p>
<p>Causal parrots: Large language models may talk causality but are not causal. Matej Zečević, Moritz Willig, Devendra Singh Dhami, Kristian Kersting, Transactions on Machine Learning Research. 2023</p>
<p>Is knowledge all large language models needed for causal reasoning?. Hengrui Cai, Shengjie Liu, Rui Song, arXiv:2401.001392024arXiv preprint</p>
<p>Causal graph discovery with retrieval-augmented generation based large language models. Yuzhe Zhang, Yipeng Zhang, Yidong Gan, Lina Yao, Chen Wang, arXiv:2402.153012024arXiv preprint</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>Foundations of structural causal models with cycles and latent variables. Stephan Bongers, Patrick Forré, Jonas Peters, Joris M Mooij, The Annals of Statistics. 4952021</p>
<p>Evolutionary computation in the era of large language model: Survey and roadmap. Xingyu Wu, Sheng-Hao Wu, Jibin Wu, Liang Feng, Kay Chen, Tan , IEEE Transactions on Evolutionary Computation. 2024</p>
<p>D'ya like dags? a survey on structure learning and causal discovery. J Matthew, Necati Vowels, Richard Cihan Camgoz, Bowden, ACM Computing Surveys. 5542022</p>
<p>Unveiling causal reasoning in large language models: Reality or mirage?. Haoang Chi, He Li, Wenjing Yang, Feng Liu, Long Lan, Xiaoguang Ren, Tongliang Liu, Bo Han, Proceedings of the 38th Annual Conference on Neural Information Processing Systems. the 38th Annual Conference on Neural Information Processing Systems2024</p>
<p>Causal structure learning supervised by large language model. Taiyu Ban, Lyuzhou Chen, Derui Lyu, Xiangyu Wang, Huanhuan Chen, arXiv:2311.116892023arXiv preprint</p>
<p>Causal modelling agents: Causal graph discovery through synergising metadata-and data-driven reasoning. Ahmed Abdulaal, Nina Montana-Brown, Tiantian He, Ayodeji Ijishakin, Ivana Drobnjak, C Daniel, Daniel C Castro, Alexander, Proceedings of the 12th International Conference on Learning Representations. the 12th International Conference on Learning Representations2024</p>
<p>Large language models are effective priors for causal graph discovery. Victor-Alexandru Darvariu, Stephen Hailes, Mirco Musolesi, arXiv:2405.135512024arXiv preprint</p>
<p>Marrying llms with domain expert validation for causal graph generation. Alessandro Castelnovo, Riccardo Crupi, Fabio Mercorio, Mario Mezzanzanica, Daniele Potertì, Daniele Regoli, Proceedings of the 3rd Italian Workshop on Artificial Intelligence and Applications for Business and Industries co-located with 22nd International Conference of the Italian Association for Artificial Intelligence. the 3rd Italian Workshop on Artificial Intelligence and Applications for Business and Industries co-located with 22nd International Conference of the Italian Association for Artificial IntelligenceCEUR-WS20233650</p>
<p>Integrating large language models in causal discovery: A statistical causal approach. Masayuki Takayama, Tadahisa Okuda, Thong Pham, Tatsuyoshi Ikenoue, Shingo Fukuma, Shohei Shimizu, Akiyoshi Sannai, arXiv:2402.014542024arXiv preprint</p>
<p>Vasileios Sitokonstantinou, and Gustau Camps-Valls. Large language models for constrained-based causal discovery. Kai-Hendrik Cohrs, Gherardo Varando, Emiliano Diaz, arXiv:2406.073782024arXiv preprint</p>
<p>Comprehensive review and empirical evaluation of causal discovery algorithms for numerical data. Wenjin Niu, Zijun Gao, Liyan Song, Lingbo Li, arXiv:2407.130542024arXiv preprint</p>
<p>Toward a theory of tokenization in llms. Nived Rajaraman, Jiantao Jiao, Kannan Ramchandran, arXiv:2404.083352024arXiv preprint</p>
<p>Llmdriven causal discovery via harmonized prior. Taiyu Ban, Lyuzhou Chen, Derui Lyu, Xiangyu Wang, Qinrui Zhu, Huanhuan Chen, IEEE Transactions on Knowledge and Data Engineering. 2025</p>
<p>Causal inference and causal explanation with background knowledge. Christopher Meek, Proceedings of the Eleventh conference on Uncertainty in artificial intelligence. the Eleventh conference on Uncertainty in artificial intelligence1995</p>
<p>On the completeness of causal discovery in the presence of latent confounding with tiered background knowledge. Andrews, Spirtes, Cooper, Proeedings of the International Workshop on Artificial Intelligence and Statistics. 2020</p>
<p>Sound and complete causal identification with latent variables given local background knowledge. Tian-Zuo Wang, Tian Qin, Zhi-Hua Zhou, Artificial Intelligence. 3221039642023</p>
<p>Tian-Zuo Wang, Lue Tao, Zhi-Hua Zhou, arXiv:2407.15259New rules for causal identification with background knowledge. 2024arXiv preprint</p>
<p>Lyuzhou Chen, Taiyu Ban, Xiangyu Wang, Derui Lyu, Huanhuan Chen, arXiv:2306.07032Mitigating prior errors in causal structure learning: Towards llm driven prior knowledge. 2023arXiv preprint</p>
<p>Generalized score functions for causal discovery. Biwei Huang, Kun Zhang, Yizhu Lin, Bernhard Schölkopf, Clark Glymour, Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining. the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining2018</p>
<p>Entropy-based pruning for learning bayesian networks using bic. Mauro Cassio P De Campos, Giorgio Scanagatta, Marco Corani, Zaffalon, Artificial Intelligence. 2602018</p>
<p>A theoretical analysis of the bdeu scores in bayesian network structure learning. Joe Suzuki, Behaviormetrika. 442017</p>
<p>Optimal structure identification with greedy search. David Maxwell, Chickering , Journal of machine learning research. 3112002</p>
<p>Informative priors for markov blanket discovery. Adam Pocock, Mikel Lujan, Gavin Brown, Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics. the Fifteenth International Conference on Artificial Intelligence and StatisticsPMLR2012</p>
<p>Domain knowledge-enhanced variable selection for biomedical data analysis. Xingyu Wu, Zhenchao Tao, Bingbing Jiang, Tianhao Wu, Xin Wang, Huanhuan Chen, Information Sciences. 6062022</p>
<p>Feature selection in the data stream based on incremental markov boundary learning. Xingyu Wu, Bingbing Jiang, Xiangyu Wang, Taiyu Ban, Huanhuan Chen, IEEE Transactions on Neural Networks and Learning Systems. 34102023</p>
<p>Probabilistic reasoning in intelligent systems: networks of plausible inference. Judea Pearl, 2014Elsevier</p>
<p>A review on evolutionary algorithms in bayesian network learning and inference tasks. Pedro Larranaga, Hossein Karshenas, Concha Bielza, Roberto Santana, Information Sciences. 2332013</p>
<p>A genetic algorithm for causal discovery based on structural causal model. Zhengyin Chen, Kun Liu, Wenpin Jiao, Proceedings of the CAAI International Conference on Artificial Intelligence. the CAAI International Conference on Artificial IntelligenceSpringer2022</p>
<p>Cladder: assessing causal reasoning in language models. Zhijing Jin, Yuen Chen, Felix Leeb, Luigi Gresele, Ojasv Kamal, Zhiheng Lyu, Kevin Blin, Fernando Gonzalez, Max Kleiman-Weiner, Mrinmaya Sachan, Proceedings of the 37th International Conference on Neural Information Processing Systems. the 37th International Conference on Neural Information Processing Systems2023</p>
<p>Can large language models distinguish cause from effect?. Zhijing Lyu Zhiheng, Rada Jin, Mrinmaya Mihalcea, Bernhard Sachan, Schölkopf, UAI 2022 Workshop on Causal Representation Learning. 2022</p>
<p>Can large language models build causal graphs?. Stephanie Long, Tibor Schuster, Alexandre Piché, NeurIPS 2022 Workshop on Causality for Real-world Impact. 2022</p>
<p>Probing for correlations of causal facts: Large language models and causality. Moritz Willig, Matej Zečević, Devendra Singh Dhami, Kristian Kersting, 2023</p>
<p>Is chatgpt a good causal reasoner? a comprehensive evaluation. Jinglong Gao, Xiao Ding, Bing Qin, Ting Liu, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Efficient causal graph discovery using large language models. Thomas Jiralerspong, Xiaoyin Chen, Yash More, Vedant Shah, Yoshua Bengio, ICLR 2024 Workshop: How Far Are We From AGI. 2024</p>
<p>Automating psychological hypothesis generation with ai: Large language models meet causal graph. Song Tong, Kai Mao, Zhen Huang, Yukun Zhao, Kaiping Peng, arXiv:2402.144242024arXiv preprint</p>
<p>Are llms capable of data-based statistical and causal reasoning? benchmarking advanced quantitative reasoning with data. Xiao Liu, Zirui Wu, Xueqing Wu, Pan Lu, Kai-Wei Chang, Yansong Feng, Findings of the 62nd Annual Meeting of the Association for Computational Linguistics. 2024</p>
<p>Multi-agent causal discovery using large language models. Xin Hao Duong Le, Zhang Xia, Chen, arXiv:2407.150732024arXiv preprint</p>
<p>Sirui Chen, Bo Peng, Meiqi Chen, Ruiqi Wang, Mengying Xu, Xingyu Zeng, Rui Zhao, Shengjie Zhao, Yu Qiao, Chaochao Lu, arXiv:2405.00622Causal evaluation of language models. 2024arXiv preprint</p>
<p>Realtcd: Temporal causal discovery from interventional data with large language model. Peiwen Li, Xin Wang, Zeyang Zhang, Yuan Meng, Fang Shen, Yue Li, Jialong Wang, Yang Li, Wenwu Zhu, Proceedings of the 33rd ACM International Conference on Information and Knowledge Management. the 33rd ACM International Conference on Information and Knowledge Management2024</p>
<p>Causalchat: Interactive causal model development and refinement using large language models. Yanming Zhang, Akshith Kota, Eric Papenhausen, Klaus Mueller, arXiv:2410.141462024arXiv preprint</p>
<p>. Shiv Kampani, David Hidary, Constantijn Van Der, Martin Poel, Brenda Ganahl, Miao, arXiv:2410.211412024Llm-initialized differentiable causal discovery. arXiv preprint</p>
<p>Structured knowledge-based causal discovery: Agentic streams of thought. Information Processing &amp; Management. Sven Meier, Pratik Narendra Raut, Felix Mahr, Nils Thielen, Jörg Franke, Florian Risch, 202562104202</p>
<p>Largescale hierarchical causal discovery via weak prior knowledge. Xiangyu Wang, Taiyu Ban, Lyuzhou Chen, Derui Lyu, Qinrui Zhu, Huanhuan Chen, IEEE Transactions on Knowledge and Data Engineering. 2025</p>
<p>Causal order: The key to leveraging imperfect experts in causal inference. Aniket Vashishtha, Gowtham Reddy Abbavaram, Abhinav Kumar, Saketh Bachu, Amit Vineeth N Balasubramanian, Sharma, 2025</p>
<p>Dr. eci: Infusing large language models with causal knowledge for decomposed reasoning in event causality identification. Ruichu Cai, Shengyin Yu, Jiahao Zhang, Wei Chen, Boyan Xu, Keli Zhang, Proceedings of the 31st International Conference on Computational Linguistics. the 31st International Conference on Computational Linguistics2025</p>
<p>Causaleval: Towards better causal reasoning in language models. Longxuan Yu, Delin Chen, Siheng Xiong, Qingyang Wu, Dawei Li, Zhikai Chen, Xiaoze Liu, Liangming Pan, Proceedings of the 2025 Conference of the Nations of the Americas Chapter. Long Papers. the 2025 Conference of the Nations of the Americas Chapterthe Association for Computational Linguistics20251</p>
<p>Can large language models help experimental design for causal discovery. Junyi Li, Yongqiang Chen, Chenxi Liu, Qianyi Cai, Tongliang Liu, Bo Han, Kun Zhang, Hui Xiong, arXiv:2503.011392025arXiv preprint</p>
<p>Integrating large language model for improved causal discovery. Taiyu Ban, Lyuzhou Chen, Derui Lyu, Xiangyu Wang, Qinrui Zhu, Qiang Tu, Huanhuan Chen, IEEE Transactions on Artificial Intelligence. 2025</p>
<p>Kulfi framework: Knowledge utilization for optimizing large language models for financial causal reasoning. Neelesh Kumar Shukla, Sandeep Singh, Prabhat Kumar Prabhakar, Sakthivel Thangaraj, Weiyi Sun, Viji Prasanna Venkatesan, Krishnamurthy, Proceedings of the Joint Workshop of the 9th Financial Technology and Natural Language Processing (FinNLP), the 6th Financial Narrative Processing (FNP), and the 1st Workshop on Large Language Models for Finance and Legal (LLMFinLegal). the Joint Workshop of the 9th Financial Technology and Natural Language Processing (FinNLP), the 6th Financial Narrative Processing (FNP), and the 1st Workshop on Large Language Models for Finance and Legal (LLMFinLegal)2025</p>
<p>Fairness-driven llm-based causal discovery with active learning and dynamic scoring. Khadija Zanna, Akane Sano, arXiv:2503.175692025arXiv preprint</p>
<p>Vasileios Sitokonstantinou, Gherardo Varando, and Gustau Camps-Valls. Large language models for causal hypothesis generation in science. Kai-Hendrik Cohrs, Emiliano Diaz, Machine Learning: Science and Technology. 61130012025</p>
<p>Can llms leverage observational data? towards data-driven causal discovery with llms. Yuni Susanti, Michael Färber, arXiv:2504.109362025arXiv preprint</p>
<p>Large language models for zero-shot inference of causal structures in biology. Izzy Newsham, Luka Kovačević, Richard Moulange, Nan Rosemary Ke, Sach Mukherjee, arXiv:2503.043472025arXiv preprint</p>
<p>Beyond llms: A linguistic approach to causal graph generation from narrative texts. Zehan Li, Ruhua Pan, Xinyu Pi, arXiv:2504.074592025arXiv preprint</p>
<p>On comparing llm-generated causal networks with a rulebased approach. J Solat, Sajjad Sheikh, Haider, Asian Conference on Intelligent Information and Database Systems. Springer2025</p>
<p>Learning bayesian networks with the bnlearn r package. Marco Scutari, Journal of Statistical Software. 35i032010</p>
<p>Hamming distance metric learning. Mohammad Norouzi, David J Fleet, Ruslan Salakhutdinov, Proceedings of the 25th International Conference on Neural Information Processing Systems. the 25th International Conference on Neural Information Processing Systems20121</p>            </div>
        </div>

    </div>
</body>
</html>
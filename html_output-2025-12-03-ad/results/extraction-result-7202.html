<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7202 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7202</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7202</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-a4c40532e68728fbeab5d9415f6ad8e9530db360</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a4c40532e68728fbeab5d9415f6ad8e9530db360" target="_blank">The WebNLG Challenge: Generating Text from RDF Data</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Natural Language Generation</p>
                <p><strong>Paper TL;DR:</strong> The microplanning task is introduced, data preparation, evaluation methodology, participant results and a brief description of the participating systems are provided.</p>
                <p><strong>Paper Abstract:</strong> The WebNLG challenge consists in mapping sets of RDF triples to text. It provides a common benchmark on which to train, evaluate and compare “microplanners”, i.e. generation systems that verbalise a given content by making a range of complex interacting choices including referring expression generation, aggregation, lexicalisation, surface realisation and sentence segmentation. In this paper, we introduce the microplanning task, describe data preparation, introduce our evaluation methodology, analyse participant results and provide a brief description of the participating systems.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7202.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7202.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RDF-triple linearisation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linearisation of RDF triple sets into token sequences</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Representation that serialises a set of RDF triples into a flat token sequence (a sequence of subject-predicate-object tuples), optionally separated by special tuple tokens and optionally delexicalised, so it can be used as input to sequence-to-sequence language models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The WebNLG Challenge: Generating Text from RDF Data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>RDF triple linearisation (tuple-separated)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each RDF triple (s p o) is converted to a linear token span and triples are concatenated into a single sequence. Some systems insert explicit tuple-separator special tokens between triples; others order triples to reflect discourse. Linearisation may be applied on raw lexical items or after delexicalisation (entity placeholders) and may use subword segmentation on tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>simple linearisation (serialize triples in some order) with optional tuple-separator tokens; ordering sometimes chosen to maintain discourse order but no single canonical traversal algorithm is specified.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation (RDF-to-text)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenNMT (baseline), Nematus (ADAPTCentre), Edinburgh NMT variant (UTilburg-NMT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used with attention-based encoder-decoder architectures: baseline used OpenNMT (two-layer bidirectional LSTM encoder-decoder, hidden size 500, trained 13 epochs); ADAPTCentre used Nematus (attention-based NMT with subword units); UTilburg-NMT used an Edinburgh NMT submission style (attention-based encoder-decoder).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, TER</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Baseline (OpenNMT, delex+linearisation) BLEU=33.24, METEOR=0.23, TER=0.61 (global). UTilburg-NMT (linearised delex input) BLEU=34.60 (global). ADAPTCentre (linearisation + tuple separators + subword) BLEU=31.06 (global).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Linearisation enables direct use of sequence-to-sequence models and benefits from standard NMT toolkits; when combined with delexicalisation it reduces sparsity and allows reasonable performance (baseline). Ordering triples to preserve discourse can help generation quality but is not standardized.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Flat linearisation may lose explicit graph structure (edge/node relationships) beyond local token order; performance depends on chosen ordering and on delexicalisation; no guaranteed lossless encoding unless special schemes are used. Discourse ordering is heuristic rather than canonical. Average token length per graph not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to graph-structured linguistic representations (e.g., PredArg graphs used by UPF-FORGE), linearisation is simpler and directly compatible with seq2seq models but can underperform on unseen properties where structured or template-based approaches adapt better; linearised+delex systems performed well on seen categories but template/grammar approaches were superior on unseen categories in this challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The WebNLG Challenge: Generating Text from RDF Data', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7202.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7202.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Delexicalisation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Delexicalised entity placeholders / ENTITY-ID representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preprocessing representation that replaces entity surface forms in the input (and correspondingly in outputs) by placeholders (e.g., category names, ENTITY-ID or property-based placeholders) to reduce lexical sparsity and enable models to learn generation patterns independent of specific entity strings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The WebNLG Challenge: Generating Text from RDF Data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Delexicalised RDF (entity/placeholders, ENTITY-ID, Wikipedia IDs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Entities or arguments are replaced by placeholders before training (e.g., subject replaced by its category name C, object replaced by PROPERTY placeholder, or entities replaced by unique ENTITY-ID or annotated with Wikipedia ID). Replacement is typically done by exact matching between text and input; some systems append DBpedia types to identifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token-based, lossy (with relexicalisation step)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Exact-match substitution of entity strings with placeholders (category tokens or ENTITY-ID/Wikipedia-ID), sometimes enriched by appending entity types; used both on source linearisation and target texts (delexicalised targets).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation (RDF-to-text) with delexicalised training</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Moses SMT (UTilburg-SMT), OpenNMT (baseline), other attention-based NMTs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>UTilburg-SMT used Moses SMT trained on delexicalised data with MGIZA alignments and a 6-gram language model; baseline used an LSTM encoder-decoder on delexicalised data; UMelbourne used a standard attention encoder-decoder with entities replaced by ENTITY-ID and DBpedia types appended.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, TER</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>UTilburg-SMT (delex + Wikipedia IDs) BLEU=44.28 (global), METEOR=0.38, TER=0.53; UMelbourne (delex+enrichment) BLEU=45.13 (global). Baseline (delex+OpenNMT) BLEU=33.24 (global).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Delexicalisation substantially reduces vocabulary sparsity and allows ML models (SMT/NMT) to learn more general mapping patterns; it enabled high BLEU scores for SMT/NMT on seen categories. It simplifies the learning problem by separating content planning from lexical realization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Lossy with respect to surface lexical and morphological realization; requires a relexicalisation step which can fail if exact matching is not available (example: 'noodles' in target was not substituted because of no exact match). Delexicalisation hides morphological inflection and lexical choices, and requires additional resources/rules for correct re-insertion and agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Alternative strategies include subword segmentation (ADAPTCentre) which avoids delexicalisation; however, in this challenge delexicalised systems (esp. UMelbourne, UTilburg-SMT) achieved higher BLEU on seen data and SMT with delex+IDs was among top performers. Template/grammar approaches (UPF-FORGE) performed better on unseen categories, showing that delexicalisation + ML is strong for seen data but less robust for unseen properties.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The WebNLG Challenge: Generating Text from RDF Data', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7202.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7202.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PredArg graph / FORGe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Predicate-Argument templates aggregated into a PredArg graph and realised by the FORGe generator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A linguistically-motivated graph representation built from manually-defined predicate-argument templates for DBpedia properties; these PredArg structures are aggregated into a PredArg graph that is fed to a grammar-based generator (FORGe) for surface realisation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The WebNLG Challenge: Generating Text from RDF Data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>PredArg templates / PredArg graph (input to FORGe)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For each DBpedia property, hand-crafted PredArg templates map triples to predicate-argument structures with DBpedia-specific and linguistic features; PredArg structures are combined/aggregated into a PredArg graph which is used by the FORGe grammar-based generator to produce text.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>graph / hierarchical (linguistic structure)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Template-driven mapping from triples to PredArg structures followed by aggregation into a PredArg graph; this is a linguistic/structured encoding rather than a simple flat serialization.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation (grammar/template-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FORGe (grammar-based generator) as used by UPF-FORGE</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>FORGe is a grammar-based generator that takes predicate-argument graph input and performs sentence planning and surface realisation based on manually defined PredArg templates for DBpedia properties (UPF-FORGE system).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, TER</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>UPF-FORGE overall BLEU=38.65 (global), METEOR=0.39 (global); on unseen categories UPF-FORGE ranked 1st with BLEU=35.70 and METEOR=0.37.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Because templates were manually defined per property, the approach adapted well to unseen properties/categories in the unseen test set, yielding top performance on unseen data. The representation supports richer linguistic variation (reflected by high METEOR) and is robust when training data lacks property coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on manual creation of PredArg templates (labour-intensive) and DBpedia-specific knowledge; scalability to many domains/properties requires significant manual effort. It is not a learned representation so it lacks the data-driven adaptability of NMT in well-covered domains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to linearised/delexicalised NMT and SMT, PredArg+FORGe performed better on unseen categories and produced outputs with higher METEOR (suggesting more varied lexical/morphological realization), while NMT/SMT often performed better on seen categories in BLEU. PredArg requires manual engineering but generalizes to unseen properties more readily.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The WebNLG Challenge: Generating Text from RDF Data', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7202.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7202.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Subword tokenisation (BPE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Subword segmentation (e.g., BPE) used instead of delexicalisation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Token-level subword segmentation applied to input/output tokens to address rare words and sparsity without delexicalising entities, used by at least one participating NMT system.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The WebNLG Challenge: Generating Text from RDF Data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Subword representation (BPE / subword units)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Instead of replacing entities with placeholders, tokens are segmented into subword units (byte-pair encoding or similar) so that rare or unseen entity surface forms can be partially represented by shared subword pieces; the RDF input is linearised and tokenised into subwords.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token-based (subword), sequential</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Linearise triples then apply subword segmentation to tokens (no delexicalisation); tuple separation tokens may still be used.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation (subword-tokenised input/output)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Nematus (ADAPTCentre)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ADAPTCentre used Nematus with subword segmentation (no delexicalisation) and inserted tuple separation special tokens in the linearised input.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, TER</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ADAPTCentre overall BLEU=31.06 (global). On unseen categories ADAPTCentre performed poorly (BLEU around 10.53 on unseen subset and TER high), indicating problems generalising to unseen properties.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Subword tokenisation was intended to reduce sparsity for rare entity names, avoiding the need for delexicalisation; however, in this challenge the subword approach (without delexicalisation) did not perform well on unseen categories, suggesting it did not adequately address unseen property lexicalisation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Does not explicitly encode entity types or property identity; while it reduces vocabulary size, it may not provide the inductive bias needed to lexicalise unseen properties or to enforce correct re-use of entity mentions in multi-sentence outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to delexicalisation, subword segmentation avoided a separate relexicalisation step but underperformed on unseen categories in this evaluation. Delexicalised SMT/NMT systems and template-based systems were more robust in the WebNLG results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The WebNLG Challenge: Generating Text from RDF Data', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Creating training corpora for nlg micro-planners. <em>(Rating: 2)</em></li>
                <li>Building rdf content for data-to-text generation <em>(Rating: 2)</em></li>
                <li>Split and rephrase <em>(Rating: 2)</em></li>
                <li>FORGe at SemEval-2017 task 9: Deep sentence generation based on a sequence of graph transducers <em>(Rating: 2)</em></li>
                <li>Multi-domain neural network language generation for spoken dialogue systems <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7202",
    "paper_id": "paper-a4c40532e68728fbeab5d9415f6ad8e9530db360",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "RDF-triple linearisation",
            "name_full": "Linearisation of RDF triple sets into token sequences",
            "brief_description": "Representation that serialises a set of RDF triples into a flat token sequence (a sequence of subject-predicate-object tuples), optionally separated by special tuple tokens and optionally delexicalised, so it can be used as input to sequence-to-sequence language models.",
            "citation_title": "The WebNLG Challenge: Generating Text from RDF Data",
            "mention_or_use": "use",
            "representation_name": "RDF triple linearisation (tuple-separated)",
            "representation_description": "Each RDF triple (s p o) is converted to a linear token span and triples are concatenated into a single sequence. Some systems insert explicit tuple-separator special tokens between triples; others order triples to reflect discourse. Linearisation may be applied on raw lexical items or after delexicalisation (entity placeholders) and may use subword segmentation on tokens.",
            "representation_type": "sequential, token-based",
            "encoding_method": "simple linearisation (serialize triples in some order) with optional tuple-separator tokens; ordering sometimes chosen to maintain discourse order but no single canonical traversal algorithm is specified.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WebNLG",
            "task_name": "graph-to-text generation (RDF-to-text)",
            "model_name": "OpenNMT (baseline), Nematus (ADAPTCentre), Edinburgh NMT variant (UTilburg-NMT)",
            "model_description": "Used with attention-based encoder-decoder architectures: baseline used OpenNMT (two-layer bidirectional LSTM encoder-decoder, hidden size 500, trained 13 epochs); ADAPTCentre used Nematus (attention-based NMT with subword units); UTilburg-NMT used an Edinburgh NMT submission style (attention-based encoder-decoder).",
            "performance_metric": "BLEU, METEOR, TER",
            "performance_value": "Baseline (OpenNMT, delex+linearisation) BLEU=33.24, METEOR=0.23, TER=0.61 (global). UTilburg-NMT (linearised delex input) BLEU=34.60 (global). ADAPTCentre (linearisation + tuple separators + subword) BLEU=31.06 (global).",
            "impact_on_training": "Linearisation enables direct use of sequence-to-sequence models and benefits from standard NMT toolkits; when combined with delexicalisation it reduces sparsity and allows reasonable performance (baseline). Ordering triples to preserve discourse can help generation quality but is not standardized.",
            "limitations": "Flat linearisation may lose explicit graph structure (edge/node relationships) beyond local token order; performance depends on chosen ordering and on delexicalisation; no guaranteed lossless encoding unless special schemes are used. Discourse ordering is heuristic rather than canonical. Average token length per graph not reported.",
            "comparison_with_other": "Compared to graph-structured linguistic representations (e.g., PredArg graphs used by UPF-FORGE), linearisation is simpler and directly compatible with seq2seq models but can underperform on unseen properties where structured or template-based approaches adapt better; linearised+delex systems performed well on seen categories but template/grammar approaches were superior on unseen categories in this challenge.",
            "uuid": "e7202.0",
            "source_info": {
                "paper_title": "The WebNLG Challenge: Generating Text from RDF Data",
                "publication_date_yy_mm": "2017-09"
            }
        },
        {
            "name_short": "Delexicalisation",
            "name_full": "Delexicalised entity placeholders / ENTITY-ID representations",
            "brief_description": "A preprocessing representation that replaces entity surface forms in the input (and correspondingly in outputs) by placeholders (e.g., category names, ENTITY-ID or property-based placeholders) to reduce lexical sparsity and enable models to learn generation patterns independent of specific entity strings.",
            "citation_title": "The WebNLG Challenge: Generating Text from RDF Data",
            "mention_or_use": "use",
            "representation_name": "Delexicalised RDF (entity/placeholders, ENTITY-ID, Wikipedia IDs)",
            "representation_description": "Entities or arguments are replaced by placeholders before training (e.g., subject replaced by its category name C, object replaced by PROPERTY placeholder, or entities replaced by unique ENTITY-ID or annotated with Wikipedia ID). Replacement is typically done by exact matching between text and input; some systems append DBpedia types to identifiers.",
            "representation_type": "token-based, lossy (with relexicalisation step)",
            "encoding_method": "Exact-match substitution of entity strings with placeholders (category tokens or ENTITY-ID/Wikipedia-ID), sometimes enriched by appending entity types; used both on source linearisation and target texts (delexicalised targets).",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WebNLG",
            "task_name": "graph-to-text generation (RDF-to-text) with delexicalised training",
            "model_name": "Moses SMT (UTilburg-SMT), OpenNMT (baseline), other attention-based NMTs",
            "model_description": "UTilburg-SMT used Moses SMT trained on delexicalised data with MGIZA alignments and a 6-gram language model; baseline used an LSTM encoder-decoder on delexicalised data; UMelbourne used a standard attention encoder-decoder with entities replaced by ENTITY-ID and DBpedia types appended.",
            "performance_metric": "BLEU, METEOR, TER",
            "performance_value": "UTilburg-SMT (delex + Wikipedia IDs) BLEU=44.28 (global), METEOR=0.38, TER=0.53; UMelbourne (delex+enrichment) BLEU=45.13 (global). Baseline (delex+OpenNMT) BLEU=33.24 (global).",
            "impact_on_training": "Delexicalisation substantially reduces vocabulary sparsity and allows ML models (SMT/NMT) to learn more general mapping patterns; it enabled high BLEU scores for SMT/NMT on seen categories. It simplifies the learning problem by separating content planning from lexical realization.",
            "limitations": "Lossy with respect to surface lexical and morphological realization; requires a relexicalisation step which can fail if exact matching is not available (example: 'noodles' in target was not substituted because of no exact match). Delexicalisation hides morphological inflection and lexical choices, and requires additional resources/rules for correct re-insertion and agreement.",
            "comparison_with_other": "Alternative strategies include subword segmentation (ADAPTCentre) which avoids delexicalisation; however, in this challenge delexicalised systems (esp. UMelbourne, UTilburg-SMT) achieved higher BLEU on seen data and SMT with delex+IDs was among top performers. Template/grammar approaches (UPF-FORGE) performed better on unseen categories, showing that delexicalisation + ML is strong for seen data but less robust for unseen properties.",
            "uuid": "e7202.1",
            "source_info": {
                "paper_title": "The WebNLG Challenge: Generating Text from RDF Data",
                "publication_date_yy_mm": "2017-09"
            }
        },
        {
            "name_short": "PredArg graph / FORGe",
            "name_full": "Predicate-Argument templates aggregated into a PredArg graph and realised by the FORGe generator",
            "brief_description": "A linguistically-motivated graph representation built from manually-defined predicate-argument templates for DBpedia properties; these PredArg structures are aggregated into a PredArg graph that is fed to a grammar-based generator (FORGe) for surface realisation.",
            "citation_title": "The WebNLG Challenge: Generating Text from RDF Data",
            "mention_or_use": "use",
            "representation_name": "PredArg templates / PredArg graph (input to FORGe)",
            "representation_description": "For each DBpedia property, hand-crafted PredArg templates map triples to predicate-argument structures with DBpedia-specific and linguistic features; PredArg structures are combined/aggregated into a PredArg graph which is used by the FORGe grammar-based generator to produce text.",
            "representation_type": "graph / hierarchical (linguistic structure)",
            "encoding_method": "Template-driven mapping from triples to PredArg structures followed by aggregation into a PredArg graph; this is a linguistic/structured encoding rather than a simple flat serialization.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WebNLG",
            "task_name": "graph-to-text generation (grammar/template-based)",
            "model_name": "FORGe (grammar-based generator) as used by UPF-FORGE",
            "model_description": "FORGe is a grammar-based generator that takes predicate-argument graph input and performs sentence planning and surface realisation based on manually defined PredArg templates for DBpedia properties (UPF-FORGE system).",
            "performance_metric": "BLEU, METEOR, TER",
            "performance_value": "UPF-FORGE overall BLEU=38.65 (global), METEOR=0.39 (global); on unseen categories UPF-FORGE ranked 1st with BLEU=35.70 and METEOR=0.37.",
            "impact_on_training": "Because templates were manually defined per property, the approach adapted well to unseen properties/categories in the unseen test set, yielding top performance on unseen data. The representation supports richer linguistic variation (reflected by high METEOR) and is robust when training data lacks property coverage.",
            "limitations": "Relies on manual creation of PredArg templates (labour-intensive) and DBpedia-specific knowledge; scalability to many domains/properties requires significant manual effort. It is not a learned representation so it lacks the data-driven adaptability of NMT in well-covered domains.",
            "comparison_with_other": "Compared to linearised/delexicalised NMT and SMT, PredArg+FORGe performed better on unseen categories and produced outputs with higher METEOR (suggesting more varied lexical/morphological realization), while NMT/SMT often performed better on seen categories in BLEU. PredArg requires manual engineering but generalizes to unseen properties more readily.",
            "uuid": "e7202.2",
            "source_info": {
                "paper_title": "The WebNLG Challenge: Generating Text from RDF Data",
                "publication_date_yy_mm": "2017-09"
            }
        },
        {
            "name_short": "Subword tokenisation (BPE)",
            "name_full": "Subword segmentation (e.g., BPE) used instead of delexicalisation",
            "brief_description": "Token-level subword segmentation applied to input/output tokens to address rare words and sparsity without delexicalising entities, used by at least one participating NMT system.",
            "citation_title": "The WebNLG Challenge: Generating Text from RDF Data",
            "mention_or_use": "use",
            "representation_name": "Subword representation (BPE / subword units)",
            "representation_description": "Instead of replacing entities with placeholders, tokens are segmented into subword units (byte-pair encoding or similar) so that rare or unseen entity surface forms can be partially represented by shared subword pieces; the RDF input is linearised and tokenised into subwords.",
            "representation_type": "token-based (subword), sequential",
            "encoding_method": "Linearise triples then apply subword segmentation to tokens (no delexicalisation); tuple separation tokens may still be used.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WebNLG",
            "task_name": "graph-to-text generation (subword-tokenised input/output)",
            "model_name": "Nematus (ADAPTCentre)",
            "model_description": "ADAPTCentre used Nematus with subword segmentation (no delexicalisation) and inserted tuple separation special tokens in the linearised input.",
            "performance_metric": "BLEU, METEOR, TER",
            "performance_value": "ADAPTCentre overall BLEU=31.06 (global). On unseen categories ADAPTCentre performed poorly (BLEU around 10.53 on unseen subset and TER high), indicating problems generalising to unseen properties.",
            "impact_on_training": "Subword tokenisation was intended to reduce sparsity for rare entity names, avoiding the need for delexicalisation; however, in this challenge the subword approach (without delexicalisation) did not perform well on unseen categories, suggesting it did not adequately address unseen property lexicalisation.",
            "limitations": "Does not explicitly encode entity types or property identity; while it reduces vocabulary size, it may not provide the inductive bias needed to lexicalise unseen properties or to enforce correct re-use of entity mentions in multi-sentence outputs.",
            "comparison_with_other": "Compared to delexicalisation, subword segmentation avoided a separate relexicalisation step but underperformed on unseen categories in this evaluation. Delexicalised SMT/NMT systems and template-based systems were more robust in the WebNLG results.",
            "uuid": "e7202.3",
            "source_info": {
                "paper_title": "The WebNLG Challenge: Generating Text from RDF Data",
                "publication_date_yy_mm": "2017-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Creating training corpora for nlg micro-planners.",
            "rating": 2
        },
        {
            "paper_title": "Building rdf content for data-to-text generation",
            "rating": 2
        },
        {
            "paper_title": "Split and rephrase",
            "rating": 2
        },
        {
            "paper_title": "FORGe at SemEval-2017 task 9: Deep sentence generation based on a sequence of graph transducers",
            "rating": 2
        },
        {
            "paper_title": "Multi-domain neural network language generation for spoken dialogue systems",
            "rating": 1
        }
    ],
    "cost": 0.014355249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The WebNLG Challenge: Generating Text from RDF Data</h1>
<p>Claire Gardent Anastasia Shimorina<br>CNRS, LORIA, UMR 7503<br>Vandoeuvre-lès-Nancy, F-54500, France<br>claire.gardent@loria.fr<br>anastasia.shimorina@loria.fr</p>
<p>Shashi Narayan Laura Perez-Beltrachini School of Informatics, University of Edinburgh 10 Crichton Street, Edinburgh, EH8 9AB, UK shashi.narayan@ed.ac.uk lperez@ed.ac.uk</p>
<h4>Abstract</h4>
<p>The WebNLG challenge consists in mapping sets of RDF triples to text. It provides a common benchmark on which to train, evaluate and compare "microplanners", i.e. generation systems that verbalise a given content by making a range of complex interacting choices including referring expression generation, aggregation, lexicalisation, surface realisation and sentence segmentation. In this paper, we introduce the microplanning task, describe data preparation, introduce our evaluation methodology, analyse participant results and provide a brief description of the participating systems.</p>
<h2>1 Introduction</h2>
<p>Previous Natural Language Generation (NLG) challenges have focused on surface realisation (Banik et al., 2013; Belz et al., 2011), referring expression generation (Belz and Gatt, 2007; Gatt et al., 2008; Gatt et al., 2009; Belz et al., 2008; Belz et al., 2009; Belz et al., 2010) and content selection (BouayadAgha et al., 2013).</p>
<p>In contrast, the WebNLG challenge focuses on microplanning, that subtask of NLG which consists in mapping a given content to a text verbalising this content. Microplanning is a complex choice problem involving several subtasks referred to in the literature as referring expression generation, aggregation, lexicalisation, surface realisation and sentence segmentation. For instance, given the WebNLG data unit shown in (1a), generating the text in (1b) involves choosing to lexicalise the JOHN.E.BLAHA
entity only once (referring expression generation), lexicalising the occupation property as the phrase worked as (lexicalisation), using PP coordination to avoid repeating the word born (aggregation) and verbalising the three triples by a single complex sentence including an apposition, a PP coordination and a transitive verb construction (sentence segmentation and surface realisation).
(1) a. Data: (John.E.BlaHa birthDate 1942_08_26) (John.E.BlaHa birthPlace San_Antonio) (John.E.BlaHa occupation Fighter_pilot)
b. Text: John E Blaha, born in San Antonio on 1942-08-26, worked as a fighter pilot</p>
<h2>2 Data</h2>
<p>As illustrated by the above example, the WebNLG dataset was designed to exercise the ability of NLG systems to handle the whole range of microplanning operations and their interactions. It was created using a content selection procedure specifically designed to enhance data and text variety (Perez-Beltrachini et al., 2016). In (Gardent et al., 2017), we compared a dataset created using the WebNLG process with existing benchmarks in particular, (Wen et al., 2016)'s dataset (RNNLG) which was produced using a similar process. In what follows, we give various statistics about the WebNLG dataset using the RNNLG dataset as a reference point.</p>
<p>Size. The WebNLG dataset consists of 25,298 (data,text) pairs and 9,674 distinct data units. The data units are sets of RDF triples extracted from DBPedia and the texts are sequences of one or more sentences verbalising these data units.</p>
<p>Lexicalisation. As illustrated by the examples in (2), different properties can induce different lexical forms (a property might be lexicalised as a verb, a relational noun, a preposition or an adjective). Therefore, the larger the number of properties, the more likely the data is to allow for a wider range of lexicalisation patterns.
(2) X title $\mathrm{Y} \Rightarrow X$ served as $Y$</p>
<p>X Nationality $\mathrm{Y} \Rightarrow X$ 's nationality is $Y$
Relational noun
X country $\mathrm{Y} \Rightarrow X$ is in $Y \quad$ Preposition
X nationality USA $\Rightarrow X$ is American Adjective
To promote diverse lexicalisation patterns, we extracted data from 15 DBPedia categories (Astronaut, University, Monument, Building, ComicsCharacter, Food, Airport, SportsTeam, WrittenWork, Athlete, Artist, City, MeanOfTransportation, CelestialBody, Politician) resulting in a set of 373 distinct RDF properties (more than three times the number of properties contained in the RNNLG dataset). The corrected type token ratio $\left(\right.$ CTTR $^{1}$ ) and the number of word types is roughly twice as large in theWebNLG dataset than in RNNLG.</p>
<p>Surface Realisation. To increase syntactic variety, we use a content selection procedure which extracts data units of various shapes. The intuition is that different input shapes may induce distinct linguistic constructions. This is illustrated in Figure 2. Typically, while triples sharing a subject (SIBLING configuration) are likely to induce a VP or a sentence coordination, a CHAIN configuration (where the object of one triple is the subject of the other) will more naturally give rise to object relative clauses or participials.</p>
<p>Another factor impacting syntactic variation is the set of properties (input patterns) cooccuring in a given input. This is illustrated by the examples in (3) where two inputs of the same length ( 3 triples hence 3 properties) result in text with different syntax. That is, a larger number of input patterns is more likely to induce texts with greater syntactic variety. By extracting data units from a large number of distinct domains (DBPedia categories), we seeked to produce a large number of distinct input patterns.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>A was born in E. She worked as a D. A was born in $E$ and worked as a $D$.
(3) a. location-country-startDate
$\Rightarrow$ Passive-Apposition-Active
108 St. Georges Terrace is located in Perth, Australia. Its construction began in 1981.
b. BIRTHPLACE-ALMA MATER-SELECTION
$\Rightarrow$ Passive-VP coordination
William Anders was born in British Hong Kong, graduated from AFIT in 1962, and joined NASA in 1963.</p>
<p>As shown in Table 3, the WebNLG dataset contains twice as many distinct input patterns and ten times more input shapes than the RNNLG dataset. It is also less redundant with a ratio between number of inputs and number of input patterns of 2.34 against 10.31 for RNNLG.</p>
<p>Aggregation, Sentence Segmentation and Referring Expression Generation. Finally, the need for aggregation, sentence segmentation and referring expression generation mainly arise when texts contains more than one sentence. As Table 3 shows, although data units are overall smaller in the WebNLG dataset than in RNNLG, the WebNLG dataset has a higher number of texts containing more than one sentence and contains texts of longer length.</p>
<h2>3 Participating Systems</h2>
<p>The WebNLG challenge received eight submissions from six participating teams: the ADAPT Centre, Ireland (ADAPTCentre), the University of Melbourne, Australia (UMelbourne), Peking University, China (PKUWriter), Tilburg University, The Netherlands (UTilburg), University of Information Technology, VNU-HCM, Vietnam (UIT-VNU-</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">WebNLG</th>
<th style="text-align: right;">RNNLG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Size</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;"># data-text pairs</td>
<td style="text-align: right;">25,298</td>
<td style="text-align: right;">30,842</td>
</tr>
<tr>
<td style="text-align: left;"># distinct inputs</td>
<td style="text-align: right;">9,674</td>
<td style="text-align: right;">22,225</td>
</tr>
<tr>
<td style="text-align: left;">Lexicalisation</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;"># properties</td>
<td style="text-align: right;">373</td>
<td style="text-align: right;">108</td>
</tr>
<tr>
<td style="text-align: left;"># domains</td>
<td style="text-align: right;">15</td>
<td style="text-align: right;">4</td>
</tr>
<tr>
<td style="text-align: left;"># CTTR</td>
<td style="text-align: right;">6.51</td>
<td style="text-align: right;">3.42</td>
</tr>
<tr>
<td style="text-align: left;"># Words (Type)</td>
<td style="text-align: right;">6,547</td>
<td style="text-align: right;">3,524</td>
</tr>
<tr>
<td style="text-align: left;">Syntactic Variety</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;"># input patterns</td>
<td style="text-align: right;">4,129</td>
<td style="text-align: right;">2,155</td>
</tr>
<tr>
<td style="text-align: left;"># input / # input patterns</td>
<td style="text-align: right;">2.34</td>
<td style="text-align: right;">10.31</td>
</tr>
<tr>
<td style="text-align: left;"># input shapes</td>
<td style="text-align: right;">62</td>
<td style="text-align: right;">6</td>
</tr>
<tr>
<td style="text-align: left;">Aggregation, GRE, Segmentation</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;"># input with 1 or 2 triples</td>
<td style="text-align: right;">11,111</td>
<td style="text-align: right;">4,087</td>
</tr>
<tr>
<td style="text-align: left;"># input with 3 or 4 triples</td>
<td style="text-align: right;">8,172</td>
<td style="text-align: right;">6,690</td>
</tr>
<tr>
<td style="text-align: left;"># input with 5 to 7 triples</td>
<td style="text-align: right;">6,015</td>
<td style="text-align: right;">20,065</td>
</tr>
<tr>
<td style="text-align: left;"># text with 1 sentence</td>
<td style="text-align: right;">16,740</td>
<td style="text-align: right;">24,234</td>
</tr>
<tr>
<td style="text-align: left;"># text with 2 sentences</td>
<td style="text-align: right;">6,798</td>
<td style="text-align: right;">5,729</td>
</tr>
<tr>
<td style="text-align: left;"># text with $\geq 3$ sentences</td>
<td style="text-align: right;">1,760</td>
<td style="text-align: right;">879</td>
</tr>
<tr>
<td style="text-align: left;"># words/text (avg/min/max)</td>
<td style="text-align: right;">$22.69 / 4 / 80$</td>
<td style="text-align: right;">$18.37 / 1 / 76$</td>
</tr>
</tbody>
</table>
<p>Table 1: Some Statistics about the WebNLG Dataset</p>
<p>HCM) and Universitat Pompeu Fabra, Barcelona, Spain (UPF-FORGE). Each team submitted outputs from a single system except UTILBURG who submitted outputs from three different systems. As a result, there were nine systems in total: eight participating systems and our baseline (BASELINE) system. These can be grouped into three categories: pipeline systems, statistical machine translation (SMT) and neural machine translation (NMT) systems. Table 3 shows the system categorisations.</p>
<p>Pipeline Systems. Three submissions used a template or grammar-based pipeline framework with some NLG module: UTILBURG-PIPELINE, UIT-VNU-HCM and UPF-FORGE.</p>
<p>The first two systems, UTILBURG-PIPELINE and UIT-VNU-HCM, extracted rules or templates from the training data for surface realisation, whereas the third system, UPF-FORGE, used the FORGe grammar (Mille et al., 2017).</p>
<p>UTILBURG-PIPELINE extracted rules mapping a triple (or a triple set) to a text observed in the training data; both the triple and the associated text were delexicalised. Given a RDF triple set to generate</p>
<table>
<thead>
<tr>
<th style="text-align: center;">System ID</th>
<th style="text-align: center;">Institution</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">PIPELINE Systems</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">UTILBURG-SMT</td>
<td style="text-align: center;">Tilburg University</td>
</tr>
<tr>
<td style="text-align: center;">UIT-VNU-HCM</td>
<td style="text-align: center;">University of Information Technology</td>
</tr>
<tr>
<td style="text-align: center;">UPF-FORGE</td>
<td style="text-align: center;">Universitat Pompeu Fabra</td>
</tr>
<tr>
<td style="text-align: center;">SMT Systems</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">UTILBURG-SMT</td>
<td style="text-align: center;">Tilburg University</td>
</tr>
<tr>
<td style="text-align: center;">NMT Systems</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ADAPTCENTRE</td>
<td style="text-align: center;">ADAPT Centre, Ireland</td>
</tr>
<tr>
<td style="text-align: center;">UMELBOURNE</td>
<td style="text-align: center;">University of Melbourne</td>
</tr>
<tr>
<td style="text-align: center;">UTILBURG-NMT</td>
<td style="text-align: center;">Tilburg University</td>
</tr>
<tr>
<td style="text-align: center;">PKUWRITER</td>
<td style="text-align: center;">Peking University</td>
</tr>
<tr>
<td style="text-align: center;">BASELINE</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 2: Categorisation of participating systems.
from, UTILBURG-PIPELINE first ordered triples to maintain discourse order. Extracted rules were then applied to generate a delexicalised text. Missing entities were added using a referring expression generation module (Castro Ferreira et al., 2016). Finally, a 6 -gram language model trained on the Gigaword corpus was used to rank the system output.</p>
<p>UIT-VNU-HCM did not resort to delexicalisation in their rules. Instead of using the text to extract templates, it used the typed-dependency structure of the text to facilitate rule extraction from the training data. In addition, at run time, WordNet was used to estimate similarity between predicates in the test and train sets.</p>
<p>UPF-FORGE mostly focused on sentence planning with predicate-argument (PredArg) templates. For each of the DBPedia properties found in the training and evaluation data, they manually defined PredArg templates encoding various DBPediaspecific and linguistic features. Given a RDF triple set to generate from, PredArg templates were used to convert these triples to PredArg structures and to further aggregate them to form a PredArg graph structure. The FORGe generator took this linguistic PredArg structure as input and generated a text.</p>
<p>SMT Systems. UTilburg-SMT was the only system which used the statistical machine translation framework. It was trained on the WebNLG dataset using the Moses toolkit (Koehn et al., 2007). The dataset was pre-processed whereby each entity in the input and each corresponding referring expression in the output were delexicalised and annotated with the entity Wikipedia ID. The alignments from the training set were obtained using MGIZA and model weights were tuned using 60batch MIRA with BLEU as the evaluation metric. Similar to UTilburg-Pipeline, the system used a 6-gram language model trained on the Gigaword corpus using KenLM.</p>
<p>NMT Systems. Four systems (ADAPTCentre, UMElbourne, UTilburg-NMT and PKUWRIter) build upon the attention-based encoder-decoder architecture proposed in (Bahdanau et al., 2014). Most of them make use of existing NMT frameworks. There are however important differences among systems with respect to both the concrete architecture and the sequence representations they use.</p>
<p>ADAPTCentre makes use of the Nematus (Sennrich et al., 2017) system. They opt for subword representations rather than delexicalisation to deal with rare words and sparsity. They linearise the input sequence and insert tuple separation special tokens.</p>
<p>UMelbourne does a combined delexicalisation procedure and enrichment of the input sequence. Entities are delexicalised using an entity identifier (ENTITY-ID). When available, the DBPedia type of the entity is appended. An n-gram search is used to assure the most accurate target sequence delexicalisation. They use a standard encoder-decoder with attention model.</p>
<p>UTilburg-NMT is based on the Edinburgh Neural Machine Translation submission for the 2016 machine translation shared task (WMT 2016). The target sequences are the delexicalised texts (cf. UTilburg-PiPEline) and the input sequences are the linearisation of the delexicalised input set of triples. The REG module from their pipeline system is used to post-process the decoder outputs.</p>
<p>The PKUWRITER system relies upon two extra mechanisms, namely a ranking module and an extra Reinforcement Learning (RL) training objective. It uses an ensemble of attention-based encoderdecoder models based on the TensorFlow seq2seq API in addition to the baseline ( 7 models in total). They propose an output ranking module to choose the best verbalisation among those output by the generation models. The ranker is trained on supervised data generated automatically. Input triple sets are paired with verbalisations produced by each of the generation models. Then, each pair is associated with a quality score, i.e. the BLEU score of the verbalisation and the reference. Word and sentence level features are extracted to train the ranker. The generation models and ranker are trained on different data partitions. The RL objective encourages the generation of output texts which include subjects occurring in the input RDF triples. In addition, PKUWRITER uses a set of hand-crafted rules to handle input cases where the model fails.</p>
<h2>4 Evaluation Methodology</h2>
<p>The WebNLG challenge includes both an automatic and a human-based evaluation. Due to time constraints, only the results of the automatic evaluation are presented in this paper. The results of the humanbased evaluation will be provided on the WebNLG website ${ }^{2}$ in October 2017.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>4.1 Automatic Evaluation</h3>
<p>Three automatic metrics were used to evaluate the participating systems:</p>
<ul>
<li>BLEU-4 ${ }^{3}$ (Papineni et al., 2002). BLEU scores were computed using up to three references.</li>
<li>METEOR (v1.5) ${ }^{4}$ (Denkowski and Lavie, 2014);</li>
<li>$\mathrm{TER}^{5}$ (Snover et al., 2006).</li>
</ul>
<p>For statistical significance testing, we followed the bootstrapping algorithm described in (Koehn and Monz, 2006).</p>
<p>To assess the ability of the participating systems to generalise to out of domain data, the test dataset consists of two sets of roughly equal size: a test set containing inputs created for entities belonging to DBpedia categories that were seen in the training data (Astronaut, University, Monument, Building, ComicsCharacter, Food, Airport, SportsTeam, City, and WrittenWork), and a test set containing inputs extracted for entities belonging to 5 unseen categories (Athlete, Artist, MeanOfTransportation, CelestialBody, Politician). We call the first type of data seen categories, the second, unseen categories. Correspondingly, we report results for 3 datasets: the seen category dataset, the unseen category dataset and the total test set made of both the seen and the unseen category datasets.</p>
<p>Table 3 gives more detailed statistics about the number of properties, objects and subject entities occurring in each test set.</p>
<ul>
<li>$|$ Test $\mid$ is the number of distinct properties, subjects and objects in the test set;</li>
<li>$|$ Test $\cap T n D v \mid$ is the number of distinct properties, subjects and objects which are in the test set and were seen in the training or the development set;</li>
<li>$|$ Test $\backslash T n D v \mid$ is the number of distinct properties, subjects and objects which occur in the</li>
</ul>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>test set, but not in the training and development set.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Seen</th>
<th style="text-align: center;">Unseen</th>
<th style="text-align: center;">All</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Prop.</td>
<td style="text-align: center;">$\mid$ Test $\mid$</td>
<td style="text-align: center;">188</td>
<td style="text-align: center;">159</td>
<td style="text-align: center;">300</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mid$ Test $\cap$ TnDv $\mid$</td>
<td style="text-align: center;">188</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">192</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mid$ Test $\backslash T n D v \mid$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">108</td>
<td style="text-align: center;">108</td>
</tr>
<tr>
<td style="text-align: center;">Obj.</td>
<td style="text-align: center;">$\mid$ Test $\mid$</td>
<td style="text-align: center;">1033</td>
<td style="text-align: center;">898</td>
<td style="text-align: center;">1888</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mid$ Test $\cap$ TnDv $\mid$</td>
<td style="text-align: center;">1011</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">1025</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mid$ Test $\backslash T n D v \mid$</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">841</td>
<td style="text-align: center;">863</td>
</tr>
<tr>
<td style="text-align: center;">Subj.</td>
<td style="text-align: center;">$\mid$ Test $\mid$</td>
<td style="text-align: center;">343</td>
<td style="text-align: center;">238</td>
<td style="text-align: center;">575</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mid$ Test $\cap$ TnDv $\mid$</td>
<td style="text-align: center;">342</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">342</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mid$ Test $\backslash T n D v \mid$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">232</td>
<td style="text-align: center;">233</td>
</tr>
</tbody>
</table>
<p>Table 3: Test data statistics on properties, objects and subjects for seen, unseen and all datasets.</p>
<p>While in the seen test data (first column) almost all triple elements are present in the training and development sets, in the unseen test data (second column) the vast majority of subjects, objects, and, more importantly, properties (which need to be lexicalised) has not been seen in the training and development data.</p>
<p>Participants were requested to submit tokenised and lowercased texts. To ensure consistency between submissions, we pre-processed the submitted results one more time to double-check that those requirements were fullfilled. As teams used different strategies of tokenisation, we had to modify submissions using our own scripts. In particular, all punctuation signs were separated from alphanumeric sequences (e.g. a two-token group 65.6 feet was modified to a four-token 65.6 feet). Moreover, we converted both references and submission outputs to the ASCII character set.</p>
<h3>4.2 Baseline System</h3>
<p>We developed a baseline system using neural networks and delexicalisation. Before training, we preprocess the data by linearising triples, performing tokenisation and delexicalisation using exact matching.</p>
<p>While delexicalising, we make the following replacements:</p>
<ul>
<li>
<p>given a triple of the form ( $s p o$ ) where $s$ is of the category $C$ for which the triple set has been produced (e.g., Alan_Bean for the category Astronaut), we replace $s$ by $C$.</p>
</li>
<li>
<p>given a triple of the form $s p o$, we replace $o$ by p. E.g., (s country Indonesia) becomes (s country COUNTRY). The replacements were made using the exact match and as a result not all the entities were replaced.</p>
</li>
</ul>
<p>Examples 4 and 5 show a (data,text) pair before and after delexicalisation. Note that noodles was not substituted by the corresponding entity category in the target text (because there is no exact match with the Noodle object in the input). Table 4 shows the number of distinct tokens occurring in the original and delexicalised data.
(4) a. Set of triples: (INDONESIA LEADERNAME Jusuf.KALLA) (BAKSO INGREDIENT NOODLE) (BAKSO COUNTRY INDONESIA)
b. Text: Bakso is a food containing noodles; it is found in Indonesia where Jusuf Kalla is the leader.
(5) a. Source: (COUNTRY LEADERNAME LEADERNAME) (FOOD INGREDIENT INGREDIENT) (FOOD COUNTRY COUNTRY)
b. Target: FOOD is a food containing noodles; it is found in COUNTRY where LEADERNAME is the leader.</p>
<p>On this delexicalised data-to-text corpus, we trained a vanilla sequence-to-sequence model with attention mechanism using the OpenNMT toolkit (Klein et al., 2017) with default parameters for training and decoding. The network consists of a twolayered bidirectional encoder-decoder model with LSTM units. We use a batch size of 64 and a starting learning rate of 1.0. The size of the hidden layers is 500. The network was trained for 13 epochs with a stochastic gradient descent optimisation method and a dropout probability of 0.3 . We used the entire vocabulary for the baseline due to its rather small size.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Original</th>
<th style="text-align: center;">Delexicalised</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Source</td>
<td style="text-align: center;">2703</td>
<td style="text-align: center;">1300</td>
</tr>
<tr>
<td style="text-align: left;">Target</td>
<td style="text-align: center;">5374</td>
<td style="text-align: center;">5013</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: center;">8077</td>
<td style="text-align: center;">6313</td>
</tr>
</tbody>
</table>
<p>Table 4: Vocabulary size in tokens.
After training we relexicalised sentences with corresponding entities if of course their counterparts are
present in generated output. The performance of the baseline is shown in Tables 5, 6, 7 along with other teams' results.</p>
<h2>5 Results</h2>
<p>We briefly discuss the automatic scores distinguishing between results on the whole dataset, on data extracted from previously unseen categories and on data extracted from seen categories.</p>
<p>Global Scores. Table 5 shows the global results that is, results on the whole test set. Horizontal lines group together systems for which the difference in scores is not statistically significant. The names of the teams are coloured according to system type: neural-based systems are in red, pipeline systems in blue, and SMT systems in light grey.</p>
<p>Most systems (6 out of 8 ) outperform the baseline, four of them obtaining scores well above it. In terms of BLEU and TER scores, the first four systems include systems of each type (neural, SMT-based and pipelines).</p>
<p>While BLEU and METEOR yield almost identical rankings, METEOR does not, suggesting that the systems handle synonyms and morphological variation differently. In particular, the fact that UPFFORGE ranks first under the METEOR score suggests that it often generates text that differs from the references because of synonymic or morphological variation.</p>
<p>Scores on Seen Categories. For data extracted from DBPedia categories that were seen in the training data, machine learning based systems (neural and SMT) mostly outperform rule-based systems. In particular, in terms of BLEU and TER scores, the three pipeline systems are at the low end of the ranking. Again though, the METEOR scores show a much higher ranking (3rd rather than 6th) for the UPF-FORGE systems.</p>
<p>Scores on Unseen Categories. On unseen categories, the UPF-FORGE systems ranks first as the system could quickly be adapted to handle properties that had not been seen in the training data. The ranking of the other systems is more or less unchanged with the exception of the ADAPTCENTRE system. This neural system does not use delexicalisation and the subword approach that was adopted</p>
<table>
<thead>
<tr>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Melbourne 45.13</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Tilb-SMT 44.28</td>
</tr>
<tr>
<td style="text-align: center;">$3-4$</td>
<td style="text-align: center;">PKUWritER 39.88</td>
</tr>
<tr>
<td style="text-align: center;">$3-4$</td>
<td style="text-align: center;">UPF-FORGE 38.65</td>
</tr>
<tr>
<td style="text-align: center;">$5-6$</td>
<td style="text-align: center;">Tilb-PiPELine 35.29</td>
</tr>
<tr>
<td style="text-align: center;">$5-6$</td>
<td style="text-align: center;">Tilb-NMT 34.60</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">BASELINE 33.24</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">ADAPT 31.06</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">UIT-VNU 7.07</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">TER</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Melbourne 0.47</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Tilb-SMT 0.53</td>
</tr>
<tr>
<td style="text-align: center;">$3-4$</td>
<td style="text-align: center;">PKUWritER 0.55</td>
</tr>
<tr>
<td style="text-align: center;">$3-5$</td>
<td style="text-align: center;">UPF-FORGE 0.55</td>
</tr>
<tr>
<td style="text-align: center;">$4-5$</td>
<td style="text-align: center;">Tilb-PiPELine 0.56</td>
</tr>
<tr>
<td style="text-align: center;">$6-7$</td>
<td style="text-align: center;">Tilb-NMT 0.60</td>
</tr>
<tr>
<td style="text-align: center;">$6-7$</td>
<td style="text-align: center;">BASELINE 0.61</td>
</tr>
<tr>
<td style="text-align: center;">$8-9$</td>
<td style="text-align: center;">UIT-VNU 0.82</td>
</tr>
<tr>
<td style="text-align: center;">$8-9$</td>
<td style="text-align: center;">ADAPT 0.84</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">METEOR</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">UPF-FORGE 0.39</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Tilb-SMT 0.38</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Melbourne 0.37</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Tilb-NMT 0.34</td>
</tr>
<tr>
<td style="text-align: center;">$5-6$</td>
<td style="text-align: center;">ADAPT 0.31</td>
</tr>
<tr>
<td style="text-align: center;">$5-7$</td>
<td style="text-align: center;">PKUWritER 0.31</td>
</tr>
<tr>
<td style="text-align: center;">$6-7$</td>
<td style="text-align: center;">Tilb-PiPELine 0.30</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">BASELINE 0.23</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">UIT-VNU 0.09</td>
</tr>
</tbody>
</table>
<p>Table 5: Results for all categories. Lines between systems indicate a difference in scores which is statistically significant ( $p&lt;$ 0.05 ). A colour for a team name indicates a type of the system used (NMT, SMT, Pipeline).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">ADAPT</td>
</tr>
<tr>
<td style="text-align: center;">$2-3$</td>
<td style="text-align: center;">Melbourne 54.52</td>
</tr>
<tr>
<td style="text-align: center;">$2-4$</td>
<td style="text-align: center;">Tilb-SMT 54.29</td>
</tr>
<tr>
<td style="text-align: center;">$3-4$</td>
<td style="text-align: center;">BASELINE 52.39</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">PKUWritER 51.23</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Tilb-PiPELine 44.34</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">Tilb-NMT 43.28</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">UPF-FORGE 40.88</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">UIT-VNU 19.87</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">TER</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">ADAPT 0.37</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Melbourne 0.40</td>
</tr>
<tr>
<td style="text-align: center;">$3-4$</td>
<td style="text-align: center;">BASELINE 0.44</td>
</tr>
<tr>
<td style="text-align: center;">$3-4$</td>
<td style="text-align: center;">PKUWritER 0.45</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Tilb-SMT 0.47</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Tilb-PiPELine 0.48</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">Tilb-NMT 0.51</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">UPF-FORGE 0.55</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">UIT-VNU 0.78</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">METEOR</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">ADAPT 0.44</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Tilb-SMT 0.42</td>
</tr>
<tr>
<td style="text-align: center;">$3-4$</td>
<td style="text-align: center;">Melbourne 0.41</td>
</tr>
<tr>
<td style="text-align: center;">$3-4$</td>
<td style="text-align: center;">UPF-FORGE 0.40</td>
</tr>
<tr>
<td style="text-align: center;">$5-6$</td>
<td style="text-align: center;">Tilb-NMT 0.38</td>
</tr>
<tr>
<td style="text-align: center;">$5-8$</td>
<td style="text-align: center;">Tilb-PiPELine 0.38</td>
</tr>
<tr>
<td style="text-align: center;">$6-8$</td>
<td style="text-align: center;">PKUWritER 0.37</td>
</tr>
<tr>
<td style="text-align: center;">$6-8$</td>
<td style="text-align: center;">BASELINE 0.37</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">UIT-VNU 0.15</td>
</tr>
</tbody>
</table>
<p>Table 6: Results for seen categories.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">UPF-FORGE 35.70</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Melbourne 33.27</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Tilb-SMT 29.88</td>
</tr>
<tr>
<td style="text-align: center;">$4-5$</td>
<td style="text-align: center;">PKUWritER 25.36</td>
</tr>
<tr>
<td style="text-align: center;">$4-5$</td>
<td style="text-align: center;">Tilb-NMT 25.12</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Tilb-PiPELine 20.65</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">ADAPT 10.53</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">BASELINE 06.13</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">UIT-VNU 0.11</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">TER</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">UPF-FORGE 0.55</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Melbourne 0.55</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Tilb-SMT 0.61</td>
</tr>
<tr>
<td style="text-align: center;">$4-5$</td>
<td style="text-align: center;">Tilb-PiPELine 0.65</td>
</tr>
<tr>
<td style="text-align: center;">$4-5$</td>
<td style="text-align: center;">PKUWritER 0.67</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Tilb-NMT 0.72</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">BASELINE 0.80</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">UIT-VNU 0.87</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">ADAPT 1.4</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">METEOR</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">UPF-FORGE 0.37</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Tilb-SMT 0.33</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Melbourne 0.33</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Tilb-NMT 0.31</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">PKUWritER 0.24</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">Tilb-PiPELine 0.21</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">ADAPT 0.19</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">BASELINE 0.07</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">UIT-VNU 0.03</td>
</tr>
</tbody>
</table>
<p>Table 7: Results for unseen categories.</p>
<p>S John Clancy is a labour politican who leads Birmingham, where architect John Madin, who designed 103 Colmore Row, was born.
$\mathbf{M}<em>{S} \quad{$ Birmingham|LeaderName|John_Clancy</em>(Labour_politician),
John_Madin|birthPlace|Birmingham,
103_Colmore_Row|architect|John_Madin $}$
$\mathbf{T}<em T__1="T_{1">{1} \quad$ Labour politician, John Clancy is the leader of Birmingham.
$\mathbf{M}</em> \quad{$ Birmingham|LeaderName|John_Clancy_(Labour_politician) $}$
$\mathbf{T}}<em T__2="T_{2">{2} \quad$ John Madin was born in Birmingham.
$\mathbf{M}</em> \quad{$ John_Madin|birthPlace|Birmingham $}$
$\mathbf{T}}<em T__3="T_{3">{3} \quad$ He was the architect of 103 Colmore Row.
$\mathbf{M}</em> \quad{$ 103_Colmore_Row|architect|John_Madin $}$
Figure 1: An example pair out of the Split-and-Rephrase Dataset. $\mathbf{S}$ is a single complex sentence with meaning $\mathbf{M}}<em 1="1">{S} . \mathbf{T}</em>}, \mathbf{T<em 1="1">{1}, \mathbf{T}</em>}$ form a text of three simple sentences whose joint meaning $\mathbf{M<em 1="1">{T</em>}} \cup \mathbf{M<em 2="2">{T</em>}} \cup \mathbf{M<em 3="3">{T</em>$.}}$ is the same as the meaning $\mathbf{M}_{S}$ of the corresponding single complex sentence $\mathbf{S</p>
<p>to handle unseen data does not seem to work well.</p>
<h2>6 Conclusion</h2>
<p>The WebNLG challenge was novel in that it was the first challenge to provide a benchmark on which to evaluate and compare microplanners. Despite a tight schedule (we released the training data in April for a submission in August), it generated a high level of interest among the NLG community: 62 groups from 18 countries ${ }^{6}$ downloaded the data, 6 groups submitted 8 systems and 3 groups developped a system but did not submit.</p>
<p>The training data for the WebNLG 2017 challenge is available on the WebNLG website ${ }^{7}$ and evaluation on the test data can be run by the organisers on demand. A larger dataset consisting of 40,049 (data, text) pairs, 15,095 distinct data input and 15 DBpedia categories is also available. Both datasets are under the creative common licence "CC Attribution-Noncommercial-Share Alike 4.0 International license". We hope that these resources will enable a long and fruitful strand of research on microplanning.</p>
<p>The usefulness of the WebNLG dataset reaches far beyond the WebNLG challenge. It can be used for instance to train a semantic parser which would convert a sentence into a set of RDF triples. It can also be used to derive new datasets for related tasks. Thus in (Narayan et al., 2017), we show how to derive from the WebNLG dataset, a dataset for sentence simplification which we call the Split-andRephrase dataset. In this dataset, each pair consists of (i) a single, complex sentence with its meaning representation in terms of RDF triples and (ii) a sequence of at least two sentences and their corresponding sets of RDF triples whereby these sets form a partition on the set of RDF triples associated with the input complex sentence. In other words, the Split-and-Rephrase dataset associates a complex sentence with a sequence of at least two sentences whose meaning is the same as that of the complex sentence. As explained in (Narayan et al., 2017), this dataset was created using the meaning represen-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>tations (sets of RDF triples) as pivot. The Split-and-Rephrase dataset consists of 1,100,166 pairs of the form $\left\langle\left(M_{C}, T_{C}\right),\left{\left(M_{1}, T_{1}\right) \ldots\left(M_{n}, T_{n}\right)\right}\right\rangle$ where $T_{C}$ is a complex sentence and $T_{1} \ldots T_{n}$ is a sequence of texts with semantics $M_{1}, \ldots M_{n}$ expressing the same content $M_{C}$ as $T_{C}$. Figure 1 shows an example pair. It was used to train four neural systems and the associated meaning representations were shown to improve performance.</p>
<p>In the future, we are planning to build a multilingual resource in which the English text present in the WebNLG dataset will be translated into French, Russian and Maltese. In this way, morphological variation can be explored which is an interesting avenue of research in particular for neural systems which have a limited ability to handle unseen input: how well will these systems be able to handle the generation of morphologically rich languages ?</p>
<p>The analysis of the participants results presented in this paper will be complemented in an arxiv report by the results of a human-based evaluation. Using human judgements obtained through crowdsourcing, this human evaluation will assess the system results on three criteria, namely fluency, grammaticality and appropriateness (does the text correctly verbalise the input data?). We will also provide a more in depth analysis of the participant results on data extracted from different categories and data of various length.</p>
<h2>Acknowledgments</h2>
<p>The research presented in this paper has been supported by the following grants and projects: "WebNLG", Project ANR-14-CE24-0033 of the French National Research Agency and "SUMMA", H2020 project No. 688139.</p>
<h2>References</h2>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. In Proceedings of ICLR-2015 (abs/1409.0473).
Eva Banik, Claire Gardent, and Eric Kow. 2013. The kbgen challenge. In the 14th European Workshop on Natural Language Generation (ENLG), pages 94-97.
Anja Belz and Albert Gatt. 2007. The attribute selection for gre challenge: Overview and evaluation results.</p>
<p>Proceedings of UCNLG+ MT: Language Generation and Machine Translation, pages 75-83.
Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt. 2008. The grec challenge: Overview and evaluation results.
Anja Belz, Eric Kow, and Jette Viethen. 2009. The grec named entity generation challenge 2009: overview and evaluation results. In Proceedings of the 2009 Workshop on Language Generation and Summarisation, pages 88-98. Association for Computational Linguistics.
Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt. 2010. Generating referring expressions in context: The grec task evaluation challenges. In Empirical methods in natural language generation, pages 294327. Springer.</p>
<p>Anja Belz, Michael White, Dominic Espinosa, Eric Kow, Deirdre Hogan, and Amanda Stent. 2011. The first surface realisation shared task: Overview and evaluation results. In Proceedings of the 13th European Workshop on Natural Language Generation, ENLG '11, pages 217-226, Stroudsburg, PA, USA. Association for Computational Linguistics.
Nadjet Bouayad-Agha, Gerard Casamayor, Leo Wanner, and Chris Mellish. 2013. Overview of the first content selection challenge from open semantic web data. In ENLG, pages 98-102.
J. B. Carroll. 1964. Language and thought. NJ: PrenticeHall. Englewood Cliffs.
Thiago Castro Ferreira, Emiel Krahmer, and Sander Wubben. 2016. Towards more variation in text generation: Developing and evaluating variation models for choice of referential form. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 568-577.
Michael Denkowski and Alon Lavie. 2014. Meteor universal: Language specific translation evaluation for any target language. In Proceedings of the EACL 2014 Workshop on Statistical Machine Translation.
Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. Creating training corpora for nlg micro-planners. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 179-188.
Albert Gatt, Anja Belz, and Eric Kow. 2008. The tuna challenge 2008: Overview and evaluation results. In Proceedings of the Fifth International Natural Language Generation Conference, pages 198-206. Association for Computational Linguistics.
Albert Gatt, Anja Belz, and Eric Kow. 2009. The tunareg challenge 2009: Overview and evaluation results. In Proceedings of the 12th European Workshop on</p>
<p>Natural Language Generation, pages 174-182. Association for Computational Linguistics.
G. Klein, Y. Kim, Y. Deng, J. Senellart, and A. M. Rush. 2017. OpenNMT: Open-Source Toolkit for Neural Machine Translation. ArXiv e-prints.
Philipp Koehn and Christof Monz. 2006. Manual and automatic evaluation of machine translation between european languages. In Proceedings of the Workshop on Statistical Machine Translation, StatMT '06, pages 102-121.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondřej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 177180.</p>
<p>Xiaofei Lu. 2008. Automatic measurement of syntactic complexity using the revised developmental level scale. In FLAIRS Conference, pages 153-158.
Simon Mille, Roberto Carlini, Alicia Burga, and Leo Wanner. 2017. FORGe at SemEval-2017 task 9: Deep sentence generation based on a sequence of graph transducers. In Proceedings of SemEval-2017, pages 917-920.
Shashi Narayan, Claire Gardent, Shay B. Cohen, and Anastasia Shimorina. 2017. Split and rephrase. In Proceedings of EMNLP.
Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311-318. Association for Computational Linguistics.
Laura Perez-Beltrachini and Claire Gardent. 2017. Analysing data-to-text generation benchmarks. In Proceedings of the tenth International Natural Language Generation Conference, INLG.
Laura Perez-Beltrachini, Rania Sayed, and Claire Gardent. 2016. Building rdf content for data-to-text generation. In COLING, pages 1493-1502.
Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexandra Birch-Mayne, Barry Haddow, Julian Hitschler, Marcin Junczys-Dowmunt, Samuel Laubli, Antonio Miceli Barone, Jozef Mokry, and Maria Nadejde. 2017. Nematus: A toolkit for neural machine translation. In Proceedings of the EACL 2017 Software Demonstrations, pages 65-68, 4.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation.</p>
<p>In Proceedings of association for machine translation in the Americas, volume 200.
Tsung-Hsien Wen, Milica Gašić, Nikola Mrkšić, Lina M. Rojas-Barahona, Pei-Hao Su, David Vandyke, and Steve Young. 2016. Multi-domain neural network language generation for spoken dialogue systems. In Proceedings of NAACL-HLT.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ Australia, Canada, China, Croatie, France, Germany, India, Iran, Ireland, Italy, Netherlands, Norway, Poland, Spain, Tunisia, UK, USA, Vietnam
${ }^{7}$ http://talcl.loria.fr/webnlg/stories/ challenge.html&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
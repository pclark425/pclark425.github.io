<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8812 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8812</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8812</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-cb15c1c51e8a7da42d5b2ebac955bf1cd9dd4022</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cb15c1c51e8a7da42d5b2ebac955bf1cd9dd4022" target="_blank">Text Generation from Knowledge Graphs with Graph Transformers</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work addresses the problem of generating coherent multi-sentence texts from the output of an information extraction system, and in particular a knowledge graph by introducing a novel graph transforming encoder which can leverage the relational structure of such knowledge graphs without imposing linearization or hierarchical constraints.</p>
                <p><strong>Paper Abstract:</strong> Generating texts which express complex ideas spanning multiple sentences requires a structured representation of their content (document plan), but these representations are prohibitively expensive to manually produce. In this work, we address the problem of generating coherent multi-sentence texts from the output of an information extraction system, and in particular a knowledge graph. Graphical knowledge representations are ubiquitous in computing, but pose a significant challenge for text generation techniques due to their non-hierarchical nature, collapsing of long-distance dependencies, and structural variety. We introduce a novel graph transforming encoder which can leverage the relational structure of such knowledge graphs without imposing linearization or hierarchical constraints. Incorporated into an encoder-decoder setup, we provide an end-to-end trainable system for graph-to-text generation that we apply to the domain of scientific text. Automatic and human evaluations show that our technique produces more informative texts which exhibit better document structure than competitive encoder-decoder methods.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8812.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8812.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphWriter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphWriter (Graph Transformer encoder + copy decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end graph-to-text encoder-decoder introduced in this paper that encodes knowledge graphs with a transformer-style graph encoder and decodes with an attention-based RNN decoder with a copy mechanism to generate multi-sentence scientific abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph Transformer + Copy decoding (GraphWriter)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Represent input as a connected, unlabeled graph (entities, relation-nodes, global node); embed entity phrases with a BiRNN, learn relation-node embeddings, then contextualize vertex embeddings with a multi-headed transformer-style attention over each vertex's graph neighborhood (stacked blocks with LayerNorm and FFN). Decode with an RNN attention decoder that mixes vocabulary prediction and a pointer-style copy distribution over graph vertices and title tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs produced by an IE system (entities, coreference-collapsed nodes, labeled relations converted to relation-nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Collapse coref chains to canonical mentions; convert each labeled edge into two relation vertices (forward/backward) connected to entity vertices, then add a global vertex that links to all entity vertices to ensure connectivity; entity phrases embedded by BiRNN; the resulting connected unlabeled graph is fed to the Graph Transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Multi-sentence scientific abstract generation (text generation conditioned on title and the knowledge graph)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>BLEU 14.3 ± 1.01; METEOR 18.8 ± 0.28 (test set averages reported in Table 2). Human evaluation: GraphWriter chosen 'Best' 24% vs Rewriter 12% and Human-authored 64% (Table 3). Pairwise human judgments vs EntityWriter: Structure win 63%, Informativeness win 43%, Grammar win 63%, Overall win 63% (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Outperforms GAT (BLEU 12.2, METEOR 17.2), EntityWriter (BLEU 10.38, METEOR 16.53), and Rewriter (BLEU 1.05, METEOR 8.38). Authors report that GraphWriter's global contextualization yields higher automatic metrics and better human-judged document structure and informativeness than GAT and entity-only baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Preserves relation information via relation-nodes while producing a connected input graph; global node and transformer-style global contextualization enable propagation beyond local neighborhoods, improving document structure, informativeness, and grammar; end-to-end trainable with copy mechanism for entity fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Still misses many input entities in output (coverage issue), and repetition remains an issue requiring post-processing; relatively complex encoder compared to simpler baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Authors report ~40% of entities in the input knowledge graphs are not mentioned in generated text (low coverage). About 18% of generated sentences contain repetitions (repeated sentences or clauses) that required a post-processing pruning step. Some canonicalization of entity mentions causes disfluencies compared to gold abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Text Generation from Knowledge Graphs with Graph Transformers', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8812.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8812.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Edge-to-node bipartite conversion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unlabeled connected bipartite graph conversion (edge-to-node, with global node)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preprocessing representation that converts labeled, possibly disconnected knowledge graphs into connected unlabeled graphs by turning each labeled relation edge into relation-vertices (forward and backward) and adding a global context node linking all entities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph-to-Sequence Learning using Gated Graph Neural Networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Edge-to-node bipartite conversion with global node</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Transform each labeled directed edge into two new vertices representing forward and reverse relation directions; connect relation-vertices to the respective entity vertices; add a single global vertex connected to all entity vertices; collapse coreferent entity mentions first. This yields a connected unlabeled directed graph that preserves original relation labels via relation-vertices.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs derived from information extraction (entities + labeled relations; initially possibly disconnected)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Collapse coreference chains, then for each labeled edge create two relation nodes (forward/backward) and connect them to the entity nodes; add a global vertex connected to all entities to ensure connectivity; produce adjacency matrix for the resulting unlabeled graph.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Used as encoder input representation for graph-to-text generation (scientific abstract generation) and compatible with attention-based graph encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No standalone metric reported for conversion itself; used as input to GraphWriter which achieves BLEU 14.3 ± 1.01 and METEOR 18.8 ± 0.28.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Procedure follows Beck et al. (2018); compared implicitly against linearization/sequence encodings (which do not explicitly model graph structure). Authors argue this preserves relation information without imposing a linear order.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Preserves labeled relation information while producing connected unlabeled graph inputs suitable for attention-based encoders; avoids lossy linearization and supports global-context mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Introduces additional relation vertices (increases graph size); requires learning embeddings for relation-nodes (two per relation direction), which may increase parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No explicit failure modes for conversion itself are reported; downstream coverage and repetition issues in generated text indicate conversion alone does not ensure all entities will be verbalized.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Text Generation from Knowledge Graphs with Graph Transformers', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8812.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8812.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Transformer encoder (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-style graph encoder that extends graph attention by enabling more global contextualization via multi-headed self-attention restricted to a node's graph neighborhood, stacked transformer blocks with LayerNorm and FFN.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Transformer-style graph encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Input vertex embeddings (entities, relation-nodes, global node) are updated via N-headed self-attention over each node's neighbors (attention normalized across neighbors), concatenation across heads, residual connections, LayerNorm, and FFN blocks; stacking L such blocks propagates information beyond immediate neighbors.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (entities and relation-nodes, connected via bipartite conversion)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Operates on the prepared connected unlabeled graph (edge-to-node conversion); vertex embeddings are computed (entity phrase via BiRNN, relation embeddings learned) and then contextualized via neighborhood-restricted transformer attention blocks.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (scientific abstracts).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>As part of GraphWriter: BLEU 14.3 ± 1.01, METEOR 18.8 ± 0.28; GraphTransformer-based model outperforms the GAT baseline (BLEU 12.2 ± 0.44, METEOR 17.2 ± 0.63).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared directly to Graph Attention Network (GAT) as encoder: Graph Transformer performs better on automatic metrics and human evaluations, attributed to its better global contextualization vs GAT which limits updates to adjacent nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Captures both local neighborhood information and global graph patterns via stacked attention blocks; enables parallel computation (like sequence Transformer) and richer vertex updates than GAT's neighbor-only attention.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Increased modeling complexity and computational cost compared to simpler GAT; requires multiple stacked blocks and several attention heads (authors use L=6, heads=4).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Although improved over GAT, still exhibits coverage (40% entities not verbalized) and repetition problems in outputs; the encoder alone does not solve generation coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Text Generation from Knowledge Graphs with Graph Transformers', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8812.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8812.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GAT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Attention Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neighborhood-attention-based graph encoder that computes node representations by attending over immediate graph neighbors; used as a baseline encoder in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph Attention Networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph Attention Network (neighbor-attention)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Compute hidden node representations by applying multi-headed attention where each node attends only to its adjacent neighbors; uses learned per-head linear projections and attention weights normalized over neighbors.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs; here applied to the converted unlabeled connected knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Used with the same edge-to-node converted connected graph as GraphWriter (entity and relation-nodes with global node).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (scientific abstracts) as encoder in an encoder-decoder pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>BLEU 12.2 ± 0.44; METEOR 17.2 ± 0.63 (test set averages, Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Outperformed by GraphWriter (Graph Transformer) on BLEU and METEOR; authors argue GAT's limitation—updates restricted to adjacent nodes—reduces its effectiveness compared to Graph Transformer's additional global contextualization.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Directly models neighborhood structure using attention and is less constrained than convolutional GCNs; simpler than full transformer-style global attention.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Node updates limited to adjacent neighbors which can hinder propagation of long-distance information; lower automatic metrics and human-judged structure than GraphWriter in this task.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Produces more repetition and less coherent document-level structure than GraphWriter in experiments; all GraphWriter runs outperformed all GAT runs indicating consistent gap.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Text Generation from Knowledge Graphs with Graph Transformers', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8812.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8812.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EntityWriter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EntityWriter (entity-list encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that encodes only the list of extracted entities plus the title, ignoring relations; used to measure the value of structured relations in the input.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Unstructured entity list representation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Input consists only of extracted entity phrases and the title; entities are available as a collection (no relational structure) and encoded for the decoder, allowing copying but without graph relational context.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Flat list of entities (no explicit graph edges)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Canonicalize and collapse coreferent mentions; provide only the set/list of entity phrase embeddings along with the title encoding to the decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Scientific abstract generation (same dataset and task as GraphWriter).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>BLEU 10.38; METEOR 16.53 (Table 2). Human pairwise judgments vs GraphWriter: GraphWriter won 63% on structure, 63% grammar, 43% informativeness (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Underperforms GraphWriter and GAT, demonstrating that including relations and graph structure improves generation quality and document structure.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simpler input representation; still provides useful topical anchors (entities) for generation and is easier to prepare than full graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Lacks relational/contextual structure leading to incoherent composition of entities (copies entities into unreasonable contexts), worse document structure and grammar.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Tends to copy entities without coherent linking; human judges prefer GraphWriter substantially over EntityWriter on structure and grammar.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Text Generation from Knowledge Graphs with Graph Transformers', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8812.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8812.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InferEntityWriter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InferEntityWriter (title → inferred entities → EntityWriter)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-step pipeline that first predicts likely entities from a title by embedding titles and entities into a shared space and retrieving nearest entities, then uses an entity-only generator to produce abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Inferred-entity list representation (title-conditioned retrieval + entity-list generation)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Learn shared embeddings for titles and entities by minimizing cosine distance with negative sampling; at test time, embed a title, retrieve the K=12 nearest entities, and feed this inferred entity list to an EntityWriter-style generator.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Inferred entity lists (no relations) derived from title through vector similarity</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Map titles and entities to a shared dense vector space via training with cosine-distance objective and negative samples; select nearest K entities per title as the input collection.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Abstract generation from title when no extracted knowledge is available.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>BLEU 3.60; METEOR 12.2 (Table 6). Compared to Rewriter (BLEU 1.05, METEOR 8.38), InferEntityWriter improves metrics but remains far below GraphWriter.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Performs better than the title-only Rewriter baseline but worse than knowledge-aware models that use extracted graph relations. Demonstrates that inferring entities from title is helpful but insufficient to match full knowledge-based generation.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Allows some knowledge injection when no IE output is available; improves over pure title-only generation.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Inferred entities may be noisy or incomplete; lacks relational structure so generation quality remains limited.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Still low BLEU and METEOR compared to graph-aware models; inferred entity list cannot capture the structural relations necessary for coherent multi-sentence abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Text Generation from Knowledge Graphs with Graph Transformers', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8812.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8812.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rewriter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Paper Abstract Writing through Editing Mechanism (Rewriter)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A title-only iterative rewriting model (Wang et al., 2018) used as a baseline that repeatedly rewrites drafts to produce an abstract using only the paper title as input.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Paper Abstract Writing through Editing Mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Title-only iterative rewrite representation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Condition generation solely on the title; employ a gated rewriter network to iteratively produce and refine draft outputs in multiple sequence-to-sequence editing steps, without any explicit extracted knowledge input.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>No graph (title-only serial sequence input)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No graph conversion; model uses title sequence as input and performs multi-step sequence-to-sequence rewriting to generate the abstract.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Scientific abstract generation from title only.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>BLEU 1.05; METEOR 8.38 (Table 2). Human evaluation: Rewriter chosen 'Best' 12% and 'Worst' 64% in a 50-sample human test (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Performs much worse than knowledge-informed models (GraphWriter, GAT, EntityWriter); authors state that title-only models tend to be fluent but fail to relate strongly to the factual input and jump across topics.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Operates without any extracted knowledge; can produce fluent and grammatical text in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Poor informativeness and topic coherence with respect to the article title; fails to use specific entities/relations and often jumps between unrelated topics.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Human annotators rated it 'Worst' 64% of time in a comparison including GraphWriter and human-authored abstracts; typical failure is topic drift and lack of factual grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Text Generation from Knowledge Graphs with Graph Transformers', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8812.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8812.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linearized AMR / seq2seq</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linearized AMR sequence encoding (Neural AMR: sequence-to-sequence models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A representation that linearizes graph structures (like AMR) into sequences which are then modeled with sequence-to-sequence architectures rather than directly modeling graph topology.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural AMR: Sequence-to-Sequence Models for Parsing and Generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph linearization / sequence serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert a graph (e.g., AMR) into a linear sequence representation (serialization) and train sequence-to-sequence models to map between linearized graphs and text; does not explicitly model graph edges or neighborhood structure.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR semantic graphs and similar rooted, dense graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Custom linearization/serialization of graph nodes and labeled edges into a token sequence suitable for seq2seq models (e.g., bracketed traversal/ordering heuristics).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation and parsing (graph-text mapping).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No metrics reported for linearization in this paper; cited as a prior method where pretraining on large noisy parses improves AMR generation in Konstas et al. (2017).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Authors note that Konstas et al. (2017) rely on linearization and sequence encoding rather than direct graph modeling; linearization can work for AMR (which is denser and rooted) but authors argue it's less appropriate for their sparse, unrooted IE knowledge graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simplicity and compatibility with powerful sequence models; benefits from large-scale pretraining on sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Does not directly model graph structure, may lose topological information, and may be less suitable for graphs that are sparse, unrooted, or structurally diverse (like IE KGs used here).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Authors argue linearization was not ideal for their IE-derived KGs (which are sparser and unrooted) and can collapse long-distance dependencies or lose structural variety.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Text Generation from Knowledge Graphs with Graph Transformers', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8812.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8812.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph convolutional encoders</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Graph Convolutional Encoders for Structured Data to Text Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Graph convolutional network (GCN) based encoders applied directly to input graphs to produce node representations for graph-to-text generation tasks (referenced prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep Graph Convolutional Encoders for Structured Data to Text Generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph Convolutional Network (GCN) encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Apply graph convolution layers (spectral or message-passing style) to propagate and aggregate neighbor information into node embeddings; these encodings are used by decoders for text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs and other structured input graphs for data-to-text tasks</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Operate directly on nodes and labeled edges (often requires label usage) using convolutional message passing across graph adjacency.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (AMR-to-text and related structured data-to-text tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No direct metrics reported in this paper; cited as prior successful approach for AMR-based generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Authors position their Graph Transformer and GAT as descendants/improvements over GCNs; graph convolutions limit flexibility and GAT/Transformer-style attention provide more modeling power.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Directly encodes graph structure, effective for dense/label-rich graphs like AMR.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Convolutional approaches may impose constraints (e.g., on connectivity or edge-label usage) and can be less flexible than attention-based methods; may struggle with label-sparse knowledge graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Authors claim GCNs and gated models make heavy use of label information, which is sparser in their IE graphs, making such methods less suitable for the AGENDA KGs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Text Generation from Knowledge Graphs with Graph Transformers', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8812.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8812.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gated Graph Neural Networks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-to-Sequence Learning using Gated Graph Neural Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A gated graph neural network (GGNN) approach for graph-to-sequence learning that can be used for encoding graphs by message passing with gating (cited and referenced in conversion procedure).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph-to-Sequence Learning using Gated Graph Neural Networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Gated Graph Neural Network encoding + edge-to-node conversions (as used in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Use gated recurrent updates over graph nodes where each node's state is updated by gated aggregation of neighbor messages; Beck et al. (2018) used edge-to-node conversions similar to the conversion applied in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR and other structured graphs for graph-to-sequence tasks</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Often works on graphs where edges may be converted to intermediate nodes to handle labeled relations; uses gated message passing across timesteps.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (AMR-to-text, structured data-to-text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No metrics reported here; referenced as methodology that inspired this paper's graph conversion.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Authors state they followed a similar edge-to-node conversion procedure to Beck et al. (2018) but preferred attention-based encoders (GAT/Graph Transformer) because their IE graphs are label-sparse and attention-based models impose fewer constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>GGNNs can effectively model sequential message passing and labeled edges using gating mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Heavily uses edge label information (which is sparser in IE graphs), and may be less suited to the AGENDA graphs compared to attention-based encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not directly evaluated in this paper; authors argue GGNN-style reliance on labels and gated updates makes them less appropriate for sparse labeled KGs in their dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Text Generation from Knowledge Graphs with Graph Transformers', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8812.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8812.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph LSTM (Song et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A Graph-to-Sequence Model for AMR-to-Text Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-LSTM approach that represents each vertex as a gated combination of connected vertices and labeled edges for information propagation; referenced as a related model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Graph-to-Sequence Model for AMR-to-Text Generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph LSTM (gated vertex updates using neighbors and edge labels)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>At each timestep, a vertex state is updated by a gated combination of the representations of connected vertices and the labeled edges between them, effectively propagating information via recurrent updates.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs and other labeled graphs for text generation</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Operate on labeled edges directly using gated recurrent updates across graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation and related graph-to-sequence tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No metrics provided in this paper; cited as prior AMR work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Authors contrast graph-LSTM/gated models with attention-based models, noting gated models make heavy use of label information and may be less suitable for their label-sparse KGs.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Models propagation over multiple timesteps and can incorporate rich edge-label information via gating.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires sufficient labeled edge information; may be less effective in sparse-label scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not evaluated directly here; authors note potential mismatch to their IE KGs which are label-sparse and disconnected.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Text Generation from Knowledge Graphs with Graph Transformers', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8812.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e8812.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pointer-generator / copy mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pointer-generator network (copy mechanism)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid decoder mechanism that mixes generation from a fixed vocabulary with copying tokens from the input, implemented here similarly to See et al. (2017).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Get to the Point: Summarization with Pointer-Generator Networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Pointer-generator (mixed copy and vocabulary distribution)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>At each decoding timestep compute a scalar p (copy probability) from decoder hidden state and context; final token distribution is p * attention-over-input-tokens + (1-p) * softmax(vocabulary distribution). Here attention-over-input includes graph vertices and title tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Any input with token-level entities/title tokens (applied here to knowledge-graph vertex tokens and title sequence)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Decoder attends to graph-contextualized vertex encodings and title encodings; uses attention scores to form copy distribution over entities and title tokens, combined with vocabulary distribution via gating p.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Text generation (scientific abstract generation) with ability to reproduce entity mentions from the graph.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Component of GraphWriter (BLEU 14.3 ± 1.01, METEOR 18.8 ± 0.28); no isolated metrics for copy mechanism alone.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Adopts the methodology of See et al. (2017) for pointer-generator summarization; authors use it to improve entity fidelity compared to pure vocabulary-only decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Allows faithful reproduction of multi-word entity mentions from the input graph; helps ground generated text in input entities.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Copying alone does not guarantee coverage of all entities (40% of entities still omitted) and can lead to incoherent insertions if not properly contextualized.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Despite using copy mechanism, many entities are not verbalized; copying can also insert entities into unnatural contexts in entity-only models (EntityWriter).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Text Generation from Knowledge Graphs with Graph Transformers', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Graph Attention Networks <em>(Rating: 2)</em></li>
                <li>Graph-to-Sequence Learning using Gated Graph Neural Networks <em>(Rating: 2)</em></li>
                <li>Neural AMR: Sequence-to-Sequence Models for Parsing and Generation <em>(Rating: 2)</em></li>
                <li>Deep Graph Convolutional Encoders for Structured Data to Text Generation <em>(Rating: 2)</em></li>
                <li>A Graph-to-Sequence Model for AMR-to-Text Generation <em>(Rating: 2)</em></li>
                <li>Get to the Point: Summarization with Pointer-Generator Networks <em>(Rating: 2)</em></li>
                <li>Semi-Supervised Classification with Graph Convolutional Networks <em>(Rating: 1)</em></li>
                <li>Paper Abstract Writing through Editing Mechanism <em>(Rating: 2)</em></li>
                <li>Neural Text Generation from Structured Data with Application to the Biography Domain <em>(Rating: 1)</em></li>
                <li>Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8812",
    "paper_id": "paper-cb15c1c51e8a7da42d5b2ebac955bf1cd9dd4022",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "GraphWriter",
            "name_full": "GraphWriter (Graph Transformer encoder + copy decoder)",
            "brief_description": "An end-to-end graph-to-text encoder-decoder introduced in this paper that encodes knowledge graphs with a transformer-style graph encoder and decodes with an attention-based RNN decoder with a copy mechanism to generate multi-sentence scientific abstracts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Graph Transformer + Copy decoding (GraphWriter)",
            "representation_description": "Represent input as a connected, unlabeled graph (entities, relation-nodes, global node); embed entity phrases with a BiRNN, learn relation-node embeddings, then contextualize vertex embeddings with a multi-headed transformer-style attention over each vertex's graph neighborhood (stacked blocks with LayerNorm and FFN). Decode with an RNN attention decoder that mixes vocabulary prediction and a pointer-style copy distribution over graph vertices and title tokens.",
            "graph_type": "Knowledge graphs produced by an IE system (entities, coreference-collapsed nodes, labeled relations converted to relation-nodes)",
            "conversion_method": "Collapse coref chains to canonical mentions; convert each labeled edge into two relation vertices (forward/backward) connected to entity vertices, then add a global vertex that links to all entity vertices to ensure connectivity; entity phrases embedded by BiRNN; the resulting connected unlabeled graph is fed to the Graph Transformer.",
            "downstream_task": "Multi-sentence scientific abstract generation (text generation conditioned on title and the knowledge graph)",
            "performance_metrics": "BLEU 14.3 ± 1.01; METEOR 18.8 ± 0.28 (test set averages reported in Table 2). Human evaluation: GraphWriter chosen 'Best' 24% vs Rewriter 12% and Human-authored 64% (Table 3). Pairwise human judgments vs EntityWriter: Structure win 63%, Informativeness win 43%, Grammar win 63%, Overall win 63% (Table 4).",
            "comparison_to_others": "Outperforms GAT (BLEU 12.2, METEOR 17.2), EntityWriter (BLEU 10.38, METEOR 16.53), and Rewriter (BLEU 1.05, METEOR 8.38). Authors report that GraphWriter's global contextualization yields higher automatic metrics and better human-judged document structure and informativeness than GAT and entity-only baselines.",
            "advantages": "Preserves relation information via relation-nodes while producing a connected input graph; global node and transformer-style global contextualization enable propagation beyond local neighborhoods, improving document structure, informativeness, and grammar; end-to-end trainable with copy mechanism for entity fidelity.",
            "disadvantages": "Still misses many input entities in output (coverage issue), and repetition remains an issue requiring post-processing; relatively complex encoder compared to simpler baselines.",
            "failure_cases": "Authors report ~40% of entities in the input knowledge graphs are not mentioned in generated text (low coverage). About 18% of generated sentences contain repetitions (repeated sentences or clauses) that required a post-processing pruning step. Some canonicalization of entity mentions causes disfluencies compared to gold abstracts.",
            "uuid": "e8812.0",
            "source_info": {
                "paper_title": "Text Generation from Knowledge Graphs with Graph Transformers",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Edge-to-node bipartite conversion",
            "name_full": "Unlabeled connected bipartite graph conversion (edge-to-node, with global node)",
            "brief_description": "A preprocessing representation that converts labeled, possibly disconnected knowledge graphs into connected unlabeled graphs by turning each labeled relation edge into relation-vertices (forward and backward) and adding a global context node linking all entities.",
            "citation_title": "Graph-to-Sequence Learning using Gated Graph Neural Networks",
            "mention_or_use": "use",
            "representation_name": "Edge-to-node bipartite conversion with global node",
            "representation_description": "Transform each labeled directed edge into two new vertices representing forward and reverse relation directions; connect relation-vertices to the respective entity vertices; add a single global vertex connected to all entity vertices; collapse coreferent entity mentions first. This yields a connected unlabeled directed graph that preserves original relation labels via relation-vertices.",
            "graph_type": "Knowledge graphs derived from information extraction (entities + labeled relations; initially possibly disconnected)",
            "conversion_method": "Collapse coreference chains, then for each labeled edge create two relation nodes (forward/backward) and connect them to the entity nodes; add a global vertex connected to all entities to ensure connectivity; produce adjacency matrix for the resulting unlabeled graph.",
            "downstream_task": "Used as encoder input representation for graph-to-text generation (scientific abstract generation) and compatible with attention-based graph encoders.",
            "performance_metrics": "No standalone metric reported for conversion itself; used as input to GraphWriter which achieves BLEU 14.3 ± 1.01 and METEOR 18.8 ± 0.28.",
            "comparison_to_others": "Procedure follows Beck et al. (2018); compared implicitly against linearization/sequence encodings (which do not explicitly model graph structure). Authors argue this preserves relation information without imposing a linear order.",
            "advantages": "Preserves labeled relation information while producing connected unlabeled graph inputs suitable for attention-based encoders; avoids lossy linearization and supports global-context mechanisms.",
            "disadvantages": "Introduces additional relation vertices (increases graph size); requires learning embeddings for relation-nodes (two per relation direction), which may increase parameters.",
            "failure_cases": "No explicit failure modes for conversion itself are reported; downstream coverage and repetition issues in generated text indicate conversion alone does not ensure all entities will be verbalized.",
            "uuid": "e8812.1",
            "source_info": {
                "paper_title": "Text Generation from Knowledge Graphs with Graph Transformers",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Graph Transformer",
            "name_full": "Graph Transformer encoder (this paper)",
            "brief_description": "A transformer-style graph encoder that extends graph attention by enabling more global contextualization via multi-headed self-attention restricted to a node's graph neighborhood, stacked transformer blocks with LayerNorm and FFN.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Transformer-style graph encoding",
            "representation_description": "Input vertex embeddings (entities, relation-nodes, global node) are updated via N-headed self-attention over each node's neighbors (attention normalized across neighbors), concatenation across heads, residual connections, LayerNorm, and FFN blocks; stacking L such blocks propagates information beyond immediate neighbors.",
            "graph_type": "Knowledge graphs (entities and relation-nodes, connected via bipartite conversion)",
            "conversion_method": "Operates on the prepared connected unlabeled graph (edge-to-node conversion); vertex embeddings are computed (entity phrase via BiRNN, relation embeddings learned) and then contextualized via neighborhood-restricted transformer attention blocks.",
            "downstream_task": "Graph-to-text generation (scientific abstracts).",
            "performance_metrics": "As part of GraphWriter: BLEU 14.3 ± 1.01, METEOR 18.8 ± 0.28; GraphTransformer-based model outperforms the GAT baseline (BLEU 12.2 ± 0.44, METEOR 17.2 ± 0.63).",
            "comparison_to_others": "Compared directly to Graph Attention Network (GAT) as encoder: Graph Transformer performs better on automatic metrics and human evaluations, attributed to its better global contextualization vs GAT which limits updates to adjacent nodes.",
            "advantages": "Captures both local neighborhood information and global graph patterns via stacked attention blocks; enables parallel computation (like sequence Transformer) and richer vertex updates than GAT's neighbor-only attention.",
            "disadvantages": "Increased modeling complexity and computational cost compared to simpler GAT; requires multiple stacked blocks and several attention heads (authors use L=6, heads=4).",
            "failure_cases": "Although improved over GAT, still exhibits coverage (40% entities not verbalized) and repetition problems in outputs; the encoder alone does not solve generation coverage.",
            "uuid": "e8812.2",
            "source_info": {
                "paper_title": "Text Generation from Knowledge Graphs with Graph Transformers",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "GAT",
            "name_full": "Graph Attention Network",
            "brief_description": "A neighborhood-attention-based graph encoder that computes node representations by attending over immediate graph neighbors; used as a baseline encoder in experiments.",
            "citation_title": "Graph Attention Networks",
            "mention_or_use": "use",
            "representation_name": "Graph Attention Network (neighbor-attention)",
            "representation_description": "Compute hidden node representations by applying multi-headed attention where each node attends only to its adjacent neighbors; uses learned per-head linear projections and attention weights normalized over neighbors.",
            "graph_type": "General graphs; here applied to the converted unlabeled connected knowledge graphs",
            "conversion_method": "Used with the same edge-to-node converted connected graph as GraphWriter (entity and relation-nodes with global node).",
            "downstream_task": "Graph-to-text generation (scientific abstracts) as encoder in an encoder-decoder pipeline.",
            "performance_metrics": "BLEU 12.2 ± 0.44; METEOR 17.2 ± 0.63 (test set averages, Table 2).",
            "comparison_to_others": "Outperformed by GraphWriter (Graph Transformer) on BLEU and METEOR; authors argue GAT's limitation—updates restricted to adjacent nodes—reduces its effectiveness compared to Graph Transformer's additional global contextualization.",
            "advantages": "Directly models neighborhood structure using attention and is less constrained than convolutional GCNs; simpler than full transformer-style global attention.",
            "disadvantages": "Node updates limited to adjacent neighbors which can hinder propagation of long-distance information; lower automatic metrics and human-judged structure than GraphWriter in this task.",
            "failure_cases": "Produces more repetition and less coherent document-level structure than GraphWriter in experiments; all GraphWriter runs outperformed all GAT runs indicating consistent gap.",
            "uuid": "e8812.3",
            "source_info": {
                "paper_title": "Text Generation from Knowledge Graphs with Graph Transformers",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "EntityWriter",
            "name_full": "EntityWriter (entity-list encoder)",
            "brief_description": "A baseline that encodes only the list of extracted entities plus the title, ignoring relations; used to measure the value of structured relations in the input.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Unstructured entity list representation",
            "representation_description": "Input consists only of extracted entity phrases and the title; entities are available as a collection (no relational structure) and encoded for the decoder, allowing copying but without graph relational context.",
            "graph_type": "Flat list of entities (no explicit graph edges)",
            "conversion_method": "Canonicalize and collapse coreferent mentions; provide only the set/list of entity phrase embeddings along with the title encoding to the decoder.",
            "downstream_task": "Scientific abstract generation (same dataset and task as GraphWriter).",
            "performance_metrics": "BLEU 10.38; METEOR 16.53 (Table 2). Human pairwise judgments vs GraphWriter: GraphWriter won 63% on structure, 63% grammar, 43% informativeness (Table 4).",
            "comparison_to_others": "Underperforms GraphWriter and GAT, demonstrating that including relations and graph structure improves generation quality and document structure.",
            "advantages": "Simpler input representation; still provides useful topical anchors (entities) for generation and is easier to prepare than full graphs.",
            "disadvantages": "Lacks relational/contextual structure leading to incoherent composition of entities (copies entities into unreasonable contexts), worse document structure and grammar.",
            "failure_cases": "Tends to copy entities without coherent linking; human judges prefer GraphWriter substantially over EntityWriter on structure and grammar.",
            "uuid": "e8812.4",
            "source_info": {
                "paper_title": "Text Generation from Knowledge Graphs with Graph Transformers",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "InferEntityWriter",
            "name_full": "InferEntityWriter (title → inferred entities → EntityWriter)",
            "brief_description": "A two-step pipeline that first predicts likely entities from a title by embedding titles and entities into a shared space and retrieving nearest entities, then uses an entity-only generator to produce abstracts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Inferred-entity list representation (title-conditioned retrieval + entity-list generation)",
            "representation_description": "Learn shared embeddings for titles and entities by minimizing cosine distance with negative sampling; at test time, embed a title, retrieve the K=12 nearest entities, and feed this inferred entity list to an EntityWriter-style generator.",
            "graph_type": "Inferred entity lists (no relations) derived from title through vector similarity",
            "conversion_method": "Map titles and entities to a shared dense vector space via training with cosine-distance objective and negative samples; select nearest K entities per title as the input collection.",
            "downstream_task": "Abstract generation from title when no extracted knowledge is available.",
            "performance_metrics": "BLEU 3.60; METEOR 12.2 (Table 6). Compared to Rewriter (BLEU 1.05, METEOR 8.38), InferEntityWriter improves metrics but remains far below GraphWriter.",
            "comparison_to_others": "Performs better than the title-only Rewriter baseline but worse than knowledge-aware models that use extracted graph relations. Demonstrates that inferring entities from title is helpful but insufficient to match full knowledge-based generation.",
            "advantages": "Allows some knowledge injection when no IE output is available; improves over pure title-only generation.",
            "disadvantages": "Inferred entities may be noisy or incomplete; lacks relational structure so generation quality remains limited.",
            "failure_cases": "Still low BLEU and METEOR compared to graph-aware models; inferred entity list cannot capture the structural relations necessary for coherent multi-sentence abstracts.",
            "uuid": "e8812.5",
            "source_info": {
                "paper_title": "Text Generation from Knowledge Graphs with Graph Transformers",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Rewriter",
            "name_full": "Paper Abstract Writing through Editing Mechanism (Rewriter)",
            "brief_description": "A title-only iterative rewriting model (Wang et al., 2018) used as a baseline that repeatedly rewrites drafts to produce an abstract using only the paper title as input.",
            "citation_title": "Paper Abstract Writing through Editing Mechanism",
            "mention_or_use": "use",
            "representation_name": "Title-only iterative rewrite representation",
            "representation_description": "Condition generation solely on the title; employ a gated rewriter network to iteratively produce and refine draft outputs in multiple sequence-to-sequence editing steps, without any explicit extracted knowledge input.",
            "graph_type": "No graph (title-only serial sequence input)",
            "conversion_method": "No graph conversion; model uses title sequence as input and performs multi-step sequence-to-sequence rewriting to generate the abstract.",
            "downstream_task": "Scientific abstract generation from title only.",
            "performance_metrics": "BLEU 1.05; METEOR 8.38 (Table 2). Human evaluation: Rewriter chosen 'Best' 12% and 'Worst' 64% in a 50-sample human test (Table 3).",
            "comparison_to_others": "Performs much worse than knowledge-informed models (GraphWriter, GAT, EntityWriter); authors state that title-only models tend to be fluent but fail to relate strongly to the factual input and jump across topics.",
            "advantages": "Operates without any extracted knowledge; can produce fluent and grammatical text in some cases.",
            "disadvantages": "Poor informativeness and topic coherence with respect to the article title; fails to use specific entities/relations and often jumps between unrelated topics.",
            "failure_cases": "Human annotators rated it 'Worst' 64% of time in a comparison including GraphWriter and human-authored abstracts; typical failure is topic drift and lack of factual grounding.",
            "uuid": "e8812.6",
            "source_info": {
                "paper_title": "Text Generation from Knowledge Graphs with Graph Transformers",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Linearized AMR / seq2seq",
            "name_full": "Linearized AMR sequence encoding (Neural AMR: sequence-to-sequence models)",
            "brief_description": "A representation that linearizes graph structures (like AMR) into sequences which are then modeled with sequence-to-sequence architectures rather than directly modeling graph topology.",
            "citation_title": "Neural AMR: Sequence-to-Sequence Models for Parsing and Generation",
            "mention_or_use": "mention",
            "representation_name": "Graph linearization / sequence serialization",
            "representation_description": "Convert a graph (e.g., AMR) into a linear sequence representation (serialization) and train sequence-to-sequence models to map between linearized graphs and text; does not explicitly model graph edges or neighborhood structure.",
            "graph_type": "AMR semantic graphs and similar rooted, dense graphs",
            "conversion_method": "Custom linearization/serialization of graph nodes and labeled edges into a token sequence suitable for seq2seq models (e.g., bracketed traversal/ordering heuristics).",
            "downstream_task": "AMR-to-text generation and parsing (graph-text mapping).",
            "performance_metrics": "No metrics reported for linearization in this paper; cited as a prior method where pretraining on large noisy parses improves AMR generation in Konstas et al. (2017).",
            "comparison_to_others": "Authors note that Konstas et al. (2017) rely on linearization and sequence encoding rather than direct graph modeling; linearization can work for AMR (which is denser and rooted) but authors argue it's less appropriate for their sparse, unrooted IE knowledge graphs.",
            "advantages": "Simplicity and compatibility with powerful sequence models; benefits from large-scale pretraining on sequences.",
            "disadvantages": "Does not directly model graph structure, may lose topological information, and may be less suitable for graphs that are sparse, unrooted, or structurally diverse (like IE KGs used here).",
            "failure_cases": "Authors argue linearization was not ideal for their IE-derived KGs (which are sparser and unrooted) and can collapse long-distance dependencies or lose structural variety.",
            "uuid": "e8812.7",
            "source_info": {
                "paper_title": "Text Generation from Knowledge Graphs with Graph Transformers",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Graph convolutional encoders",
            "name_full": "Deep Graph Convolutional Encoders for Structured Data to Text Generation",
            "brief_description": "Graph convolutional network (GCN) based encoders applied directly to input graphs to produce node representations for graph-to-text generation tasks (referenced prior work).",
            "citation_title": "Deep Graph Convolutional Encoders for Structured Data to Text Generation",
            "mention_or_use": "mention",
            "representation_name": "Graph Convolutional Network (GCN) encoding",
            "representation_description": "Apply graph convolution layers (spectral or message-passing style) to propagate and aggregate neighbor information into node embeddings; these encodings are used by decoders for text generation.",
            "graph_type": "AMR graphs and other structured input graphs for data-to-text tasks",
            "conversion_method": "Operate directly on nodes and labeled edges (often requires label usage) using convolutional message passing across graph adjacency.",
            "downstream_task": "Graph-to-text generation (AMR-to-text and related structured data-to-text tasks).",
            "performance_metrics": "No direct metrics reported in this paper; cited as prior successful approach for AMR-based generation.",
            "comparison_to_others": "Authors position their Graph Transformer and GAT as descendants/improvements over GCNs; graph convolutions limit flexibility and GAT/Transformer-style attention provide more modeling power.",
            "advantages": "Directly encodes graph structure, effective for dense/label-rich graphs like AMR.",
            "disadvantages": "Convolutional approaches may impose constraints (e.g., on connectivity or edge-label usage) and can be less flexible than attention-based methods; may struggle with label-sparse knowledge graphs.",
            "failure_cases": "Authors claim GCNs and gated models make heavy use of label information, which is sparser in their IE graphs, making such methods less suitable for the AGENDA KGs.",
            "uuid": "e8812.8",
            "source_info": {
                "paper_title": "Text Generation from Knowledge Graphs with Graph Transformers",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Gated Graph Neural Networks",
            "name_full": "Graph-to-Sequence Learning using Gated Graph Neural Networks",
            "brief_description": "A gated graph neural network (GGNN) approach for graph-to-sequence learning that can be used for encoding graphs by message passing with gating (cited and referenced in conversion procedure).",
            "citation_title": "Graph-to-Sequence Learning using Gated Graph Neural Networks",
            "mention_or_use": "mention",
            "representation_name": "Gated Graph Neural Network encoding + edge-to-node conversions (as used in related work)",
            "representation_description": "Use gated recurrent updates over graph nodes where each node's state is updated by gated aggregation of neighbor messages; Beck et al. (2018) used edge-to-node conversions similar to the conversion applied in this paper.",
            "graph_type": "AMR and other structured graphs for graph-to-sequence tasks",
            "conversion_method": "Often works on graphs where edges may be converted to intermediate nodes to handle labeled relations; uses gated message passing across timesteps.",
            "downstream_task": "Graph-to-text generation (AMR-to-text, structured data-to-text).",
            "performance_metrics": "No metrics reported here; referenced as methodology that inspired this paper's graph conversion.",
            "comparison_to_others": "Authors state they followed a similar edge-to-node conversion procedure to Beck et al. (2018) but preferred attention-based encoders (GAT/Graph Transformer) because their IE graphs are label-sparse and attention-based models impose fewer constraints.",
            "advantages": "GGNNs can effectively model sequential message passing and labeled edges using gating mechanisms.",
            "disadvantages": "Heavily uses edge label information (which is sparser in IE graphs), and may be less suited to the AGENDA graphs compared to attention-based encoders.",
            "failure_cases": "Not directly evaluated in this paper; authors argue GGNN-style reliance on labels and gated updates makes them less appropriate for sparse labeled KGs in their dataset.",
            "uuid": "e8812.9",
            "source_info": {
                "paper_title": "Text Generation from Knowledge Graphs with Graph Transformers",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Graph LSTM (Song et al.)",
            "name_full": "A Graph-to-Sequence Model for AMR-to-Text Generation",
            "brief_description": "A graph-LSTM approach that represents each vertex as a gated combination of connected vertices and labeled edges for information propagation; referenced as a related model.",
            "citation_title": "A Graph-to-Sequence Model for AMR-to-Text Generation",
            "mention_or_use": "mention",
            "representation_name": "Graph LSTM (gated vertex updates using neighbors and edge labels)",
            "representation_description": "At each timestep, a vertex state is updated by a gated combination of the representations of connected vertices and the labeled edges between them, effectively propagating information via recurrent updates.",
            "graph_type": "AMR graphs and other labeled graphs for text generation",
            "conversion_method": "Operate on labeled edges directly using gated recurrent updates across graph structure.",
            "downstream_task": "AMR-to-text generation and related graph-to-sequence tasks.",
            "performance_metrics": "No metrics provided in this paper; cited as prior AMR work.",
            "comparison_to_others": "Authors contrast graph-LSTM/gated models with attention-based models, noting gated models make heavy use of label information and may be less suitable for their label-sparse KGs.",
            "advantages": "Models propagation over multiple timesteps and can incorporate rich edge-label information via gating.",
            "disadvantages": "Requires sufficient labeled edge information; may be less effective in sparse-label scenarios.",
            "failure_cases": "Not evaluated directly here; authors note potential mismatch to their IE KGs which are label-sparse and disconnected.",
            "uuid": "e8812.10",
            "source_info": {
                "paper_title": "Text Generation from Knowledge Graphs with Graph Transformers",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Pointer-generator / copy mechanism",
            "name_full": "Pointer-generator network (copy mechanism)",
            "brief_description": "A hybrid decoder mechanism that mixes generation from a fixed vocabulary with copying tokens from the input, implemented here similarly to See et al. (2017).",
            "citation_title": "Get to the Point: Summarization with Pointer-Generator Networks",
            "mention_or_use": "use",
            "representation_name": "Pointer-generator (mixed copy and vocabulary distribution)",
            "representation_description": "At each decoding timestep compute a scalar p (copy probability) from decoder hidden state and context; final token distribution is p * attention-over-input-tokens + (1-p) * softmax(vocabulary distribution). Here attention-over-input includes graph vertices and title tokens.",
            "graph_type": "Any input with token-level entities/title tokens (applied here to knowledge-graph vertex tokens and title sequence)",
            "conversion_method": "Decoder attends to graph-contextualized vertex encodings and title encodings; uses attention scores to form copy distribution over entities and title tokens, combined with vocabulary distribution via gating p.",
            "downstream_task": "Text generation (scientific abstract generation) with ability to reproduce entity mentions from the graph.",
            "performance_metrics": "Component of GraphWriter (BLEU 14.3 ± 1.01, METEOR 18.8 ± 0.28); no isolated metrics for copy mechanism alone.",
            "comparison_to_others": "Adopts the methodology of See et al. (2017) for pointer-generator summarization; authors use it to improve entity fidelity compared to pure vocabulary-only decoding.",
            "advantages": "Allows faithful reproduction of multi-word entity mentions from the input graph; helps ground generated text in input entities.",
            "disadvantages": "Copying alone does not guarantee coverage of all entities (40% of entities still omitted) and can lead to incoherent insertions if not properly contextualized.",
            "failure_cases": "Despite using copy mechanism, many entities are not verbalized; copying can also insert entities into unnatural contexts in entity-only models (EntityWriter).",
            "uuid": "e8812.11",
            "source_info": {
                "paper_title": "Text Generation from Knowledge Graphs with Graph Transformers",
                "publication_date_yy_mm": "2019-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Graph Attention Networks",
            "rating": 2,
            "sanitized_title": "graph_attention_networks"
        },
        {
            "paper_title": "Graph-to-Sequence Learning using Gated Graph Neural Networks",
            "rating": 2,
            "sanitized_title": "graphtosequence_learning_using_gated_graph_neural_networks"
        },
        {
            "paper_title": "Neural AMR: Sequence-to-Sequence Models for Parsing and Generation",
            "rating": 2,
            "sanitized_title": "neural_amr_sequencetosequence_models_for_parsing_and_generation"
        },
        {
            "paper_title": "Deep Graph Convolutional Encoders for Structured Data to Text Generation",
            "rating": 2,
            "sanitized_title": "deep_graph_convolutional_encoders_for_structured_data_to_text_generation"
        },
        {
            "paper_title": "A Graph-to-Sequence Model for AMR-to-Text Generation",
            "rating": 2,
            "sanitized_title": "a_graphtosequence_model_for_amrtotext_generation"
        },
        {
            "paper_title": "Get to the Point: Summarization with Pointer-Generator Networks",
            "rating": 2,
            "sanitized_title": "get_to_the_point_summarization_with_pointergenerator_networks"
        },
        {
            "paper_title": "Semi-Supervised Classification with Graph Convolutional Networks",
            "rating": 1,
            "sanitized_title": "semisupervised_classification_with_graph_convolutional_networks"
        },
        {
            "paper_title": "Paper Abstract Writing through Editing Mechanism",
            "rating": 2,
            "sanitized_title": "paper_abstract_writing_through_editing_mechanism"
        },
        {
            "paper_title": "Neural Text Generation from Structured Data with Application to the Biography Domain",
            "rating": 1,
            "sanitized_title": "neural_text_generation_from_structured_data_with_application_to_the_biography_domain"
        },
        {
            "paper_title": "Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction",
            "rating": 2,
            "sanitized_title": "multitask_identification_of_entities_relations_and_coreference_for_scientific_knowledge_graph_construction"
        }
    ],
    "cost": 0.020251250000000002,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Text Generation from Knowledge Graphs with Graph Transformers</h1>
<p>Rik Koncel-Kedziorski ${ }^{1}$, Dhanush Bekal ${ }^{1}$, Yi Luan ${ }^{1}$, Mirella Lapata ${ }^{2}$, and Hannaneh Hajishirzi ${ }^{1,3}$<br>${ }^{1}$ University of Washington<br>{kedzior, dhanush,luanyi, hannaneh}@uw.edu<br>${ }^{2}$ University of Edinburgh<br>mlap@inf.ed.ac.uk<br>${ }^{3}$ Allen Institute for Artificial Intelligence</p>
<h4>Abstract</h4>
<p>Generating texts which express complex ideas spanning multiple sentences requires a structured representation of their content (document plan), but these representations are prohibitively expensive to manually produce. In this work, we address the problem of generating coherent multi-sentence texts from the output of an information extraction system, and in particular a knowledge graph. Graphical knowledge representations are ubiquitous in computing, but pose a significant challenge for text generation techniques due to their non-hierarchical nature, collapsing of longdistance dependencies, and structural variety. We introduce a novel graph transforming encoder which can leverage the relational structure of such knowledge graphs without imposing linearization or hierarchical constraints. Incorporated into an encoder-decoder setup, we provide an end-to-end trainable system for graph-to-text generation that we apply to the domain of scientific text. Automatic and human evaluations show that our technique produces more informative texts which exhibit better document structure than competitive encoder-decoder methods. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Increases in computing power and model capacity have made it possible to generate mostlygrammatical sentence-length strings of natural language text. However, generating several sentences related to a topic and which display overall coherence and discourse-relatedness is an open challenge. The difficulties are compounded in domains of interest such as scientific writing. Here the variety of possible topics is great (e.g. topics as diverse as driving, writing poetry, and picking stocks are all referenced in one subfield of</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A scientific text showing the annotations of an information extraction system and the corresponding graphical representation. Coreference annotations shown in color. Our model learns to generate texts from automatically extracted knowledge using a graph encoder decoder setup.
one scientific discipline). Additionally, there are strong constraints on document structure, as scientific communication requires carefully ordered explanations of processes and phenomena.</p>
<p>Many researchers have sought to address these issues by working with structured inputs. Data-totext generation models (Konstas and Lapata, 2013; Lebret et al., 2016; Wiseman et al., 2017; Puduppully et al., 2019) condition text generation on table-structured inputs. Tabular input representations provide more guidance for producing longer texts, but are only available for limited domains as they are assembled at great expense by manual annotation processes.</p>
<p>The current work explores the possibility of using information extraction (IE) systems to automatically provide context for generating longer texts (Figure 1). Robust IE systems are available and have support over a large variety of textual domains, and often provide rich annotations of relationships that extend beyond the scope of</p>
<p>a single sentence. But due to their automatic nature, they also introduce challenges for generation such as erroneous annotations, structural variety, and significant abstraction of surface textual features (such as grammatical relations or predicateargument structure).</p>
<p>To effect our study, we use a collection of abstracts from a corpus of scientific articles (Ammar et al., 2018). We extract entity, coreference, and relation annotations for each abstract with a state-of-the-art information extraction system (Luan et al., 2018), and represent the annotations as a knowledge graph which collapses co-referential entities. An example of a text and graph are shown in Figure 1. We use these graph/text pairs to train a novel attention-based encoder-decoder model for knowledge-graph-to-text generation. Our model, GraphWriter, extends the successful Transformer for text encoding (Vaswani et al., 2017) to graphstructured inputs, building on the recent Graph Attention Network architecture (Veličković et al., 2018). The result is a powerful, general model for graph encoding which can incorporate global structural information when contextualizing vertices in their local neighborhoods.</p>
<p>The main contributions of this work include:</p>
<ol>
<li>We propose a new graph transformer encoder that applies the successful sequence transformer to graph structured inputs.</li>
<li>We show how IE output can be formed as a connected unlabeled graph for use in attention-based encoders.</li>
<li>We provide a large dataset of knowledgegraphs paired with scientific texts for further study.
Through detailed automatic and human evaluations, we demonstrate that automatically extracted knowledge can be used for multi-sentence text generation. We further show that structuring and encoding this knowledge as a graph leads to improved generation performance compared to other encoder-decoder setups. Finally, we show that GraphWriter's transformer-style encoder is more effective than Graph Attention Networks on the knowledge-graph-to-text task.</li>
</ol>
<h2>2 Related Work</h2>
<p>Our work falls under the larger scope of concept-to-text generation. Barzilay and Lapata (2005) introduced a collective content selection model for generating summaries of football games from ta-
bles of game statistics. Liang et al. (2009) jointly learn to segment and align text with records, reducing the supervision needed for learning. Kim and Mooney (2010) improve this technique by learning a semantic parse to logical forms. Konstas and Lapata (2013) focus on the generation objective, jointly learning planning and generating using a rhetorical (RST) grammar induction approach.</p>
<p>These earlier works often focused on smaller record generation datasets such as WeatherGov and RoboCup, but recently Mei et al. (2016) showed how neural models can achieve strong results on these standards, prompting researchers to investigate more challenging domains such as ours.</p>
<p>Lebret et al. (2016) tackles the task of generating the first sentence of a Wikipedia entry from the associated infobox. They provide a large dataset of such entries and a language model conditioned on tables. Our work focuses on a multi-sentence task where relations can extend beyond sentence boundaries.</p>
<p>Wiseman et al. (2017) study the difficulty of applying neural models to the data-to-text task. They introduce a large dataset where a text summary of a basketball game is paired with two tables of relevant statistics and show that neural models struggle to compete with template based methods over this data. We propose generating from graphs rather than tables, and show that graphs can be effectively encoded to capture both local and global structure in the input.</p>
<p>We show that modeling knowledge as a graph improves generation results, connecting our work to other graph-to-text tasks such as generating from Abstract Meaning Representation (AMR) graphs. Konstas et al. (2017) provide the first neural model for this task, and show that pretraining on a large dataset of noisy automatic parses can improve results. However, they do not directly model the graph structure, relying on linearization and sequence encoding instead. Current works improve this through more sophisticated graph encoding techniques. Marcheggiani and Perez-Beltrachini (2018) encode input graphs directly using a graph convolution encoder (Kipf and Welling, 2017). Our model extends the graph attention networks of Veličković et al. (2018), a direct descendant of the convolutional approach which offers more modeling power and has been</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Title</th>
<th style="text-align: center;">Abstract</th>
<th style="text-align: center;">KG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Vocab</td>
<td style="text-align: center;">29 K</td>
<td style="text-align: center;">77 K</td>
<td style="text-align: center;">54 K</td>
</tr>
<tr>
<td style="text-align: left;">Tokens</td>
<td style="text-align: center;">413 K</td>
<td style="text-align: center;">5.8 M</td>
<td style="text-align: center;">1.2 M</td>
</tr>
<tr>
<td style="text-align: left;">Entities</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">518 K</td>
</tr>
<tr>
<td style="text-align: left;">Avg Length</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">141.2</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Avg #Vertices</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">12.42</td>
</tr>
<tr>
<td style="text-align: left;">Avg #Edges</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">4.43</td>
</tr>
</tbody>
</table>
<p>Table 1: Data statistics of our AGENDA dataset. Averages are computed per instance.
shown to improve performance. Song et al. (2018) uses a graph LSTM model to effect information propagation. At each timestep, a vertex is represented by a gated combination of the vertices to which it is connected and the labeled edges connecting them. Beck et al. (2018) use a similar gated graph neural network. Both of these gated models make heavy use of label information, which is much sparser in our knowledge graphs than in AMR. Generally, AMR graphs are denser, rooted, and connected, whereas the knowledge our model works with lacks these characteristics. For this reason, we focus on attention-based models such as Veličković et al. (2018), which impose fewer constraints on their input.</p>
<p>Finally, our work is related to Wang et al. (2018) who offer a method for generating scientific abstracts from titles. Their model uses a gated rewriter network to write and revise several draft outputs in several sequence-to-sequence steps. While we operate in the same general domain as this work, our task setup is ultimately different due to the use of extracted information as input. We argue that our setup improves the task defined in Wang et al. (2018), and our more general model can be applied across tasks and domains.</p>
<h2>3 The AGENDA Dataset</h2>
<p>We consider the problem of generating a text from automatically extracted information (knowledge). IE systems can produce high quality knowledge for a variety of domains, synthesizing information from across sentence and even document boundaries. Generating coherent text from knowledge requires a model which considers global characteristics of the knowledge as well as local characteristics of each entity. This feature of the task motivates our use of graphs for representing knowledge, where neighborhoods localize important information and paths through the graph build con-
nections between distant nodes through intermediate ones. An example knowledge graph can be seen in Figure 1.</p>
<p>We formulate our problem as follows: given the title of a scientific article and a knowledge graph constructed by an automatic information extraction system, the goal is to generate an abstract that a) is appropriate for the given title and b) expresses the content of the knowledge graph in natural language text. To evaluate how well a model accomplishes this goal, we introduce the Abstract GENERation DAtaset (AGENDA), a dataset of knowledge graphs paired with scientific abstracts. Our dataset consists of 40 k paper titles and abstracts from the Semantic Scholar Corpus taken from the proceedings of 12 top AI conferences (Ammar et al., 2018).</p>
<p>For each abstract, we create a knowledge graph in two steps. First, we apply the SciIE system of Luan et al. (2018), a state-of-the-art sciencedomain information extraction system. This system provides named entity recognition for scientific terms, with entity types Task, Method, Metric, Material, or Other Scientific Term. The model also produces co-reference annotations as well as seven relations that can obtain between different entities (Compare, Used-for, Feature-of, Hyponymof, Evaluate-for, and Conjunction). For example, in Figure 1, the node labeled "SemEval 2011 Task 11" is of type 'Task', "HMM Models" is of type 'Model', and there is a 'Evaluate-For' relation showing that the models are evaluated on the task.</p>
<p>We form these annotations into knowledge graphs. We collapse co-referential entities into a single node associated with the longest mention (on the assumption that these will be the most informative). We then connect nodes to one another using the relation annotations, treating these as labeled edges in the graph. The result is a possibly unconnected graph representation of the SciIE annotations for a given abstract.</p>
<p>Statistics of the AGENDA dataset are available in Table 1. We split the AGENDA dataset into 38,720 training, 1000 validation, and 1000 test datapoints. We offer standardized data splits to facilitate comparison.</p>
<h2>4 Model</h2>
<p>Following most work on neural generation we adopt an encoder-decoder architecture, shown in</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Converting disconnected labeled graph to connected unlabeled graph for use in attention-based encoder. $v_{i}$ refer to vertices, $R_{i j}$ to relations, and $G$ is a global context node.</p>
<p>Figure 3, which we call GraphWriter. The input to GraphWriter is a title and a knowledge graph which are encoded respectively with a bidirectional recurrent neural network and a novel Graph Transformer architecture (to be discussed in Section 4.1). At each decoder time step, we attend on encodings of the knowledge graph and document title using the decoder hidden state $\mathbf{h}<em t="t">{t} \in \mathbb{R}^{d}$. The resulting vectors are used to select output $w</em>$ either from the decoder's vocabulary or by copying an entity from the knowledge graph. Details of our decoding process are described in Section 4.2. The model is trained end-to-end to minimize the negative log likelihood of the mixed copy and vocabulary probability distribution and the human authored text.</p>
<h3>4.1 Encoder</h3>
<p>The AGENDA dataset contains a knowledge graph for each datapoint, but our model requires unlabeled, connected graphs as input. To encode knowledge graphs with this model, we restructure each graph as an unlabeled connected graph, preserving label information by the method described below and sketched in Figure 2.</p>
<p>Graph Preparation We convert each graph to an unlabeled connected bipartite graphs following a similar procedure to Beck et al. (2018). In this process, each labeled edge is replaced with two vertices: one representing the forward direction of the relation and one representing the reverse. These new vertices are then connected to the entity vertices so that the directionality of the former edge is maintained. This restructures the original knowledge graph as an unlabeled directed graph where all vertices correspond to entities and relations in the SciIE annotations without loss of infor-
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: GraphWriter Model Overview
mation. To promote information flow between disconnected parts of the graph, we add a global vertex which connects all entity vertices. This global vertex will be used to initialize the decoder, analogously to the final encoder hidden state in a traditional sequence to sequence model. The final result of these restructuring operations is a connected, unlabeled graph $G=(V, E)$, where $V$ is a list of entities, relations, and a global node and $E$ is an adjacency matrix describing the directed edges.</p>
<p>Graph Transformer Our model is most similar to the Graph Attention Network (GAT) of Veličković et al. (2018), which computes the hidden representations of each node in a graph by attending over its neighbors following a selfattention strategy. The use of self-attention in GAT addresses the shortcomings of prior methods based on graph convolutions (Defferrard et al., 2016; Kipf and Welling, 2017), but limits vertex updates to information from adjacent nodes. Our model allows for a more global contextualization of each vertex through the use of a transformerstyle architecture. The recently proposed Transformer (Vaswani et al., 2017) addresses the inherent sequential computation shortcoming of recurrent neural networks, enabling efficient and paralleled computation by invoking a self-attention mechanism for global context modeling. These models have shown promising results in a variety of text processing tasks (Radford et al., 2018).</p>
<p>Our Graph Transformer encoder starts with self-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Graph Transformer
attention of local neighborhoods of vertices; the key difference with GAT is that our model includes additional mechanisms for capturing global context. This additional modeling power allows the Graph Transformer to better articulate how a vertex should be updated given the content of its neighbors, as well as to learn global patterns of graph structure relevant to the model's objective.</p>
<p>Specifically, $V$ is embedded in a dense continuous space by the embedding process described at the end of this section, resulting in matrix $\mathbf{V}^{0}=$ $\left[\mathbf{v}<em i="i">{i}\right], \mathbf{v}</em>} \in \mathbb{R}^{d}$ which will serve as input to the graph transformer model shown in Figure 4. Each vertex representation $\mathbf{v<em i="i">{i}$ is contextualized by attending over the other vertices to which $v</em>$ is connected in $G$. We use an $N$-headed self attention setup, where $N$ independent attentions are calculated and concatenated before a residual connection is applied:</p>
<p>$$
\begin{aligned}
\hat{\mathbf{v}}<em i="i">{i} &amp; =\mathbf{v}</em>+\big|<em _in="\in" _mathcal_N="\mathcal{N" j="j">{\substack{N \
n=1}}^{*} \sum</em><em i="i" j="j">{i}} \alpha</em>}^{n} \mathbf{W<em j="j">{V}^{n} \mathbf{v}</em> \
\alpha_{i j}^{n} &amp; =a^{n}\left(\mathbf{v}<em j="j">{i}, \mathbf{v}</em>\right)
\end{aligned}
$$</p>
<p>Here, $|$ denotes the concatenation of the $N$ attention heads, $\mathcal{N}<em i="i">{i}$ denotes the neighborhood of $v</em>$ are attention mechanisms parameterized per head. In this work, we use attention functions of the following form:}$ in $G, \mathbf{W}_{V}^{n} \in \mathbb{R}^{d \times d}$, and where $a^{n</p>
<p>$$
a\left(\mathbf{q}<em j="j">{i}, \mathbf{k}</em>}\right)=\frac{\exp \left(\left(\mathbf{W<em j="j">{K} \mathbf{k}</em>}\right)^{\top} \mathbf{W<em i="i">{Q} \mathbf{q}</em>}\right)}{\sum_{z \in \mathcal{N<em K="K">{i}} \exp \left(\left(\mathbf{W}</em>} \mathbf{k<em Q="Q">{z}\right)^{\top} \mathbf{W}</em>
$$} \mathbf{q}_{i}\right)</p>
<p>Each $a$ learns independent transformations $\mathbf{W}<em K="K">{Q}, \mathbf{W}</em>$, following Vaswani et al. (2017).} \in \mathbb{R}^{d \times d}$ of $\mathbf{q}$ and $\mathbf{k}$ respectively, and the resulting product is normalized across all connected edges. To reduce the tendency of these dot products to impede gradient flow, we scale them by $\frac{1}{\sqrt{d}</p>
<p>The Graph Transformer then augments these multi-headed attention layers with block networks. Each block applies the following transformations:</p>
<p>$$
\begin{aligned}
&amp; \hat{\mathbf{v}}<em i="i">{i}=\operatorname{LayerNorm}\left(\mathbf{v}</em>}^{\prime}+\operatorname{LayerNorm}\left(\hat{\mathbf{v}<em i="i">{i}\right)\right) \
&amp; \mathbf{v}</em>\right)\right)
\end{aligned}
$$}^{\prime}=\operatorname{FFN}\left(\operatorname{LayerNorm}\left(\hat{\mathbf{v}}_{i</p>
<p>Where $\operatorname{FFN}(\mathbf{x})$ is a two layer feedforward network with a non-linear transformation $f$ between layers i.e. $f\left(\mathbf{x} \mathbf{W}<em 1="1">{1}+b</em>}\right) \mathbf{W<em 2="2">{2}+b</em>$.</p>
<p>Stacking multiple blocks allows information to propagate through the graph. Blocks are stacked $L$ times, with the output of layer $l-1$ taken as the input to layer $l$, so that $\mathbf{v}<em i="i">{i}^{l}=\hat{\mathbf{v}}</em>\right]$ represent entities, relations, and the global node contextualized by their relationships in the graph structure. We refer to the resulting encodings as graph contextualized vertex encodings.}^{l-1}$. The resulting vertex encodings $\mathbf{V}^{L}=\left[\mathbf{v}_{i}^{L</p>
<p>Embedding Vertices, Encoding Title As stated above, the vertices of our graph correspond to entities and relations from the SciIE annotations. Because each relation is represented as both a forward- and backward-looking vertex, we learn two embeddings per relation as well as an initial embedding for the global node. Entities correspond to scientific terms which are often multi-word expressions. To produce a single $d$ dimensional embedding per phrase, we use the last hidden state of a bidirectional RNN run over embeddings of each word in the entity phrase, i.e. $\operatorname{BiRNN}\left(\mathbf{x}<em m="m">{1} \ldots \mathbf{x}</em>$ of $d$-dimensional vectors representing each vertex in $V$.}\right)$ for dense embeddings $\mathbf{x}$ and phrase length $m$. The output of our embedding step is a collection $\mathbf{V}^{0</p>
<p>The title input is also a short string, and so we encode it with another BiRNN to produce $\mathbf{T}=$ $\operatorname{BiRNN}\left(x_{1}^{\prime} \ldots x_{m}^{\prime}\right)$ for title word embedding $\mathbf{x}^{\prime}$.</p>
<h3>4.2 Decoder</h3>
<p>We decode with an attention-based decoder with a copy mechanism for copying input from the knowledge graph and title. At each decoding timestep $t$ we use decoder hidden state $\mathbf{h}<em g="g">{t}$ to compute context vectors $\mathbf{c}</em>$ for the graph and}$ and $\mathbf{c}_{s</p>
<p>title sequence respectively. $\mathbf{c}<em t="t">{g}$ is computed using multi-headed attention contextualized by $\mathbf{h}</em>$ :</p>
<p>$$
\begin{aligned}
&amp; \mathbf{c}<em t="t">{g}=\mathbf{h}</em>}+\bigoplus_{n=1}^{N} \sum_{j \in V} \alpha_{j}^{n} \mathbf{W<em j="j">{G}^{n} \mathbf{v}^{\mathbf{L}}{ }</em> \
&amp; \alpha_{j}=a\left(\mathbf{h}<em j="j">{t}, \mathbf{v}</em>\right)
\end{aligned}
$$}^{\mathbf{L}</p>
<p>for $a$ as described in Equation (1) by attending over the graph contextualized encodings $\mathbf{V}^{L} . \mathbf{c}<em t="t">{s}$ is computed similarly, attending over the title encoding $\mathbf{T}$. We then construct the final context vector by concatenation, $\mathbf{c}</em>}=\left[\mathbf{c<em s="s">{g} | \mathbf{c}</em>}\right]$. We use an input-feeding decoder (Luong et al., 2015) where both $\mathbf{h<em t="t">{t}$ and $\mathbf{c}</em>$ are passed as input to the next RNN timestep.</p>
<p>We compute a probability $p$ of copying from the input using $\mathbf{h}<em t="t">{t}$ and $\mathbf{c}</em>$ in a fashion similar to See et al. (2017), that is:</p>
<p>$$
p=\sigma\left(\mathbf{W}<em t="t">{\text {copy }}\left[\mathbf{h}</em>} | \mathbf{c<em _copy="{copy" _text="\text">{t}\right]+b</em>\right)
$$}</p>
<p>The final next-token probability distribution is:</p>
<p>$$
p * \alpha^{\text {copy }}+(1-p) * \alpha^{\text {vocab }}
$$</p>
<p>Where the probability distribution $\alpha^{\text {copy }}$ over entities and input tokens is computed as $\alpha_{j}^{\text {copy }}=$ $a\left(\left[\mathbf{h}<em t="t">{t} | \mathbf{c}</em>}\right], \mathbf{x<em j="j">{j}\right)$ for $\mathbf{x}</em>} \in \mathbf{V} | \mathbf{T}$. The remaining $1-p$ probability is given to $\alpha^{\text {vocab }}$, which is calculated by scaling $\left[\mathbf{h<em t="t">{t} | \mathbf{c}</em>\right]$ to the vocabulary size and taking a softmax.</p>
<h2>5 Experiments</h2>
<p>Evaluation Metrics We evaluate using a combination of human and automatic evaluations. For human evaluation, participants were asked to compare abstracts generated by various models and those written by the authors of the scientific articles. We used Best-Worst Scaling (BWS; (Louviere and Woodworth, 1991; Louviere et al., 2015)), a less labor-intensive alternative to paired comparisons that has been shown to produce more reliable results than rating scales (Kiritchenko and Mohammad, 2016). Participants were presented with two or three abstracts and asked to decide which one was better and which one was worse in order of grammar and fluency (is the abstract written in well-formed English?), coherence (does the abstract have an introduction, state the problem or task, describe a solution, and discuss evaluations or results?), and informativeness (does the abstract relate to the provided title and make use
of appropriate scientific terms?). We provided examples of good and bad abstracts and explain how they succeed or fail to meet the defined criteria.</p>
<p>Because our dataset is scientific in nature, evaluations must be done by experts and we can only collect a limited number of these high quality datapoints. ${ }^{2}$ The study was conducted by 15 experts (i.e. computer science students) who were familiar with the abstract writing task and the content of the abstracts they judged. To supplement this, we also provide automatic metrics. We use BLEU (Papineni et al., 2002), an n-gram overlap measure popular in text generation tasks, and METEOR (Denkowski and Lavie, 2014), a machine translation with paraphrase and language-specific considerations.</p>
<p>Comparisons We compare our GraphWriter against several strong baselines. In GAT, we replace our Graph Transformer encoder with a Graph Attention Network of (Veličković et al., 2018). This encoder consists of PReLU activations stacked between 6 self-attention layers. To determine the usefulness of including graph relations, we compare to a model which uses only entities and title (EntityWriter). Finally, we compare with the gated rewriter model of Wang et al. (2018) (Rewriter). This model uses only the document title to iteratively rewrite drafts of its output. ${ }^{3}$</p>
<p>Implementation Details Our models are trained end-to-end to minimize the negative joint $\log$ likelihood of the target text vocabulary and the copied entity indices. We use SGD optimization with momentum (Qian, 1999) and "warm restarts", a cyclical regiment that reduces the learning rate from 0.25 to 0.05 over the course of 5 epochs, then resets for the following epoch. Models are trained for 15 epochs with early stopping (Prechelt, 1998) based on the validation loss, with most models stopping between 8 and 13 epochs. We use singlelayer LSTMs (Hochreiter and Schmidhuber, 1997) as recurrent networks. We use dropout (Srivastava et al., 2014) in self attention layers set to 0.3. Hidden states and embedding dimensions are fixed at 500 and attentions learn 500 dimen-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">METEOR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GraphWriter</td>
<td style="text-align: center;">$\mathbf{1 4 . 3} \pm 1.01$</td>
<td style="text-align: center;">$\mathbf{1 8 . 8} \pm 0.28$</td>
</tr>
<tr>
<td style="text-align: left;">GAT</td>
<td style="text-align: center;">$12.2 \pm 0.44$</td>
<td style="text-align: center;">$17.2 \pm 0.63$</td>
</tr>
<tr>
<td style="text-align: left;">EntityWriter</td>
<td style="text-align: center;">10.38</td>
<td style="text-align: center;">16.53</td>
</tr>
<tr>
<td style="text-align: left;">Rewriter</td>
<td style="text-align: center;">1.05</td>
<td style="text-align: center;">8.38</td>
</tr>
</tbody>
</table>
<p>Table 2: Automatic Evaluations of Generation Systems.
sional projections. In Block layers, the feedforward network has an intermediate size of 2000, and we use a PReLU activation function (He et al., 2015). GraphWriter and GAT use $L=6$ layers. The number of attention heads is set to 4 . In all models, for both inputs and output, we replace words occurring fewer than 5 times with $&lt;$ unk $&gt;$ tokens. In each abstract, we replace all mentions in a coreference chain in the abstract with the canonical mention used in the graph. We decode with beam search (Graves, 2012; Sutskever et al., 2014) with a beam size of 4. A post-processing step deletes repeated sentences and repeated coordinated clauses.</p>
<h3>5.1 Results</h3>
<p>A comparison of all systems in terms of automatic metrics is shown in Table 2. Our GraphWriter model outperforms other methods. We see that models which leverage title, entities, and relations (GraphWriter and GAT) outperform models which use less information (EntityWriter and Rewriter).</p>
<p>We see that GraphWriter outperforms GAT across metrics, indicating that the global contextualization provided by GraphWriter improves generation. To verify the performance gap between GraphWriter and GAT, we report the average test metrics for 4 training runs of each model along with their variances. We see that the variance of the different models is non-overlapping, and in fact all training runs of GraphWriter outperformed all runs of GAT on these metrics.</p>
<p>Does Knowledge Help? To evaluate the value of knowledge in the generation task we compare our GraphWriter model to a model which does not generate from knowledge. We provide expert annotators with 50 randomly-selected paper titles from the test set and ask them for a single judgment according to the criteria described in Section 5. We pair each paper title with the generated abstracts produced by GraphWriter (a knowledgeinformed modes), Rewriter (a knowledge-agnostic model), and the gold abstract (with canonicalized</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Best</th>
<th style="text-align: center;">Worst</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Rewriter (No knowledge)</td>
<td style="text-align: center;">$12 \%$</td>
<td style="text-align: center;">$64 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GraphWriter (Knowledge)</td>
<td style="text-align: center;">$24 \%$</td>
<td style="text-align: center;">$36 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Human Authored</td>
<td style="text-align: center;">$64 \%$</td>
<td style="text-align: center;">$0 \%$</td>
</tr>
</tbody>
</table>
<p>Table 3: Does knowledge improve generation? Human evaluations of best and worst abstract.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Win</th>
<th style="text-align: center;">Lose</th>
<th style="text-align: center;">Tie</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Structure</td>
<td style="text-align: center;">$63 \%$</td>
<td style="text-align: center;">$17 \%$</td>
<td style="text-align: center;">$20 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Informativeness</td>
<td style="text-align: center;">$43 \%$</td>
<td style="text-align: center;">$23 \%$</td>
<td style="text-align: center;">$33 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Grammar</td>
<td style="text-align: center;">$63 \%$</td>
<td style="text-align: center;">$23 \%$</td>
<td style="text-align: center;">$13 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Overall</td>
<td style="text-align: center;">$63 \%$</td>
<td style="text-align: center;">$17 \%$</td>
<td style="text-align: center;">$20 \%$</td>
</tr>
</tbody>
</table>
<p>Table 4: Human Judgments of GraphWriter and EntityWriter models.
coreferential mentions).
Results of this comparison can be seen in Table 3. We see that GraphWriter is selected as "Best" more often than Rewriter, and is less often selected as "Worst", attesting to the value of including knowledge in the text generation process. We see that sometimes generated texts are preferred to human authored text, which is due in part to the disfluencies introduced by canonicalization of entity mentions.</p>
<p>To further understand the advantages of using knowledge graphs, we provide a more detailed comparison of the GraphWriter and EntityWriter models. We select 30 additional test datapoints and ask experts to provide per-criterion judgments of the outputs of the two systems. Since both models make use of extracted entities, we show this list along with the title for each datapoint, and modify the description of Informativeness to include "making use of the provided entities". Results of this evaluation are shown in Table 4. Here we see that including structured knowledge in the form of a graph improves abstract generation compared to generating from an unstructured collection of entities. The largest gains are made in terms of document structure and grammar, indicating that the structure of the input knowledge is being translated into the surface form.</p>
<p>Generating from Title The Rewriter model (Wang et al., 2018) considers the task of generating an abstract with only the paper's title as input. We compare against this model because it is among the first end-to-end systems to attempt to write scientific abstracts. However, the task setup used in Wang et al. (2018) differs significantly from the task introduced in this work. In order</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Title</th>
<th style="text-align: left;">Block and Group Regularized Sparse Modeling for Dictionary Learning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Knowledge</td>
<td style="text-align: left;">(dictionary learning, CONJUNCTION, sparse coding) ; (optimization problems, USED-FOR, dictionary <br> learning) ; (optimization problems, USED-FOR, sparse coding)...</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">GraphWriter</th>
<th style="text-align: left;">Sparse representations have recently been shown to be effective in many optimization problems. However, <br> existing dictionary learning methods are limited in the number of dictionary blocks, which can be expensive <br> to obtain. In this paper, we propose a novel approach to dictionary learning based on sparse coding ...</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GAT</td>
<td style="text-align: left;">In this paper, we consider the problem of dictionary learning in well-known datasets. In particular, we <br> consider the problem of dictionary learning, where the goal is to find a set of dictionary blocks that maximize <br> the likelihood of a given set of dictionary blocks ...</td>
</tr>
<tr>
<td style="text-align: left;">EntityWriter</td>
<td style="text-align: left;">We propose a novel dictionary learning framework for reconstructed block/group sparse coding schemes. <br> The dictionary learning framework is based on the descent, which is a block structure of the group structure <br> ...</td>
</tr>
<tr>
<td style="text-align: left;">Rewriter</td>
<td style="text-align: left;">This paper presents a new approach to the k-means of the algorithm. The proposed approach is based on the <br> basis of the stationarity algorithm. The algorithm is based on the fact that the number of bits is a constant <br> of the base of the base of the input ...</td>
</tr>
<tr>
<td style="text-align: left;">Gold</td>
<td style="text-align: left;">This paper proposes a dictionary learning framework that combines the proposed block/group (BGSC) or <br> reconstructed block/group (R-BGSC) sparse coding schemes with the novel Intra-block Coherence Suppres- <br> sion Dictionary Learning algorithm. An important and distinguishing feature of the proposed framework is <br> that all dictionary blocks are trained simultaneously ...</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">Title</th>
<th style="text-align: left;">Image Interpolation with Directionlets</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Knowledge</td>
<td style="text-align: left;">(directionally adaptive image interpolation USED-FOR edge information) ; (numeric and visual quality, <br> HYPONYM-OF, directionally adaptive image interpolation) ; (directionlets, EVALUATE-FOR, multiple- <br> direction wavelet transform) ...</td>
</tr>
</tbody>
</table>
<p>GraphWriter In this paper, we propose a novel directionally adaptive image interpolation based on the multiple-direction
wavelet transform, called directionlets, which can be used as a directionlets to improve the numeric and
visual quality of the directionally adaptive image interpolation ...
GAT In this paper, we propose a novel directionally adaptive image interpolation, called directionally adaptive
image interpolation, for directionally adaptive image interpolation, which is based on the multiple-direction
wavelet transform ...
EntityWriter We present a novel directionally adaptive image interpolation for numeric and visual quality. The wavelet
transform is based on the wavelet transform between the low-resolution image and the interpolated image.
The high-resolution image is represented by a wavelet transform ...
Rewriter We present a new method for finding topic-specific data sets. The key technical contributions of our ap-
proach is to be a function of the terrestrial distributed memory. The key idea is to be a function of the page
that seeks to be ranked the buckets of the data. The basic idea is a new tool for the embedded space ...
Gold We present a novel directionally adaptive image interpolation based on a multiple-direction wavelet trans-
form, called directionlets. The directionally adaptive image interpolation uses directionlets to efficiently
capture directional features and to extract edge information along different directions from the low-
resolution image ...</p>
<p>Table 5: Example outputs of various systems versus Gold.
to make a fair comparison, we construct a variant of our model which is only provided with a title as input. We develop a model that predicts entities from the title, and then uses our knowledge-aware model to generate the abstract. For this comparison we use the EntityWriter model with a collection of entities inferred from the title alone (InferEntityWriter).</p>
<p>To infer relevant entities, we learn to embed titles and entities extracted from the corresponding abstract in a shared dense vector space by minimizing their cosine distance. We use negative sampling to provide definition to this vector space. At test time, we use the title embedding to infer the $K=12$ closest entities to feed into the InferEntityWriter model. Results are shown in Table 6, which shows that InferEntityWriter achieves bet-</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">BLEU</th>
<th style="text-align: left;">METEOR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Rewriter</td>
<td style="text-align: left;">1.05</td>
<td style="text-align: left;">8.38</td>
</tr>
<tr>
<td style="text-align: left;">InferEntityWriter</td>
<td style="text-align: left;">3.60</td>
<td style="text-align: left;">12.2</td>
</tr>
</tbody>
</table>
<p>Table 6: Comparison of generation without knowledge and with Inferred Knowledge (InferEntityWriter)
ter results than Rewriter, indicating that the intermediate entity prediction step is helpful in abstract generation.</p>
<h3>5.2 Analysis</h3>
<p>Table 5 shows examples of various system outputs for a particular test instance. We see that GraphWriter makes use of more entities from the input, arranged with more articulated textual context. It demonstrates less repetition than GAT. Both GraphWriter and GAT show much better coher-</p>
<p>ence than EntityWriter, which copies entities from the input into unreasonable contexts. Rewriter, while fluent and grammatical, jumps from topic to topic, failing to relate as strongly to the input as the knowledge-aware models.</p>
<p>To determine the shortcomings of our model, we calculate rough error statistics over the outputs of the GraphWriter on the test set. We notice that $40 \%$ of entities in the knowledge graphs do not appear in the generated text. Future work should address this coverage problem, perhaps through modifications to the inference procedure or a coverage loss (Tu et al., 2016) modified to the specifics of this task. We find that $18 \%$ of all sentences generated by our model repeat sentences or clauses and are subjected to the post-processing pruning mentioned in Section 5. While this step is a simple solution to improve generated outputs, a more advanced solution is required.</p>
<h2>6 Conclusion</h2>
<p>We have studied the problem of generating multisentence text from the output of automatic information extraction systems, and have shown that incorporating knowledge as graphs improves performance. We introduced GraphWriter, featuring a new attention model for graph encoding, and demonstrated its utility through human and automatic evaluation compared to strong baselines. Lastly, we provide a new resource for the generation community, the AGENDA dataset of abstracts and knowledge. Future work could address the problem of repetition and entity coverage in the generated texts.</p>
<h2>Acknowledgments</h2>
<p>This research was supported by the Office of Naval Research under the MURI grant N00014-18-12670, NSF (IIS 1616112, III 1703166), Allen Distinguished Investigator Award, Samsung GRO and gifts from Allen Institute for AI, Google, Amazon, and Bloomberg. We gratefully acknowledge the support of the European Research Council (Lapata; award number 681760). We also thank the anonymous reviewers and the UW-NLP group for their helpful comments.</p>
<h2>References</h2>
<p>Waleed Ammar, Dirk Groeneveld, Chandra Bhagavatula, Iz Beltagy, Miles Crawford, Doug Downey, Ja-
son Dunkelberger, Ahmed Elgohary, Sergey Feldman, Vu Ha, Rodney Kinney, Sebastian Kohlmeier, Kyle Lo, Tyler Murray, Hsu-Han Ooi, Matthew Peters, Joanna Power, Sam Skjonsberg, Lucy Lu Wang, Chris Wilhelm, Zheng Yuan, Madeleine van Zuylen, and Oren Etzioni. 2018. Construction of the Literature Graph in Semantic Scholar. In NAACL.</p>
<p>Regina Barzilay and Mirella Lapata. 2005. Collective Content Selection for Concept-to-Text Generation. In EMNLP, pages 331-338. Association for Computational Linguistics.</p>
<p>Daniel Edward Robert Beck, Gholamreza Haffari, and Trevor Cohn. 2018. Graph-to-Sequence Learning using Gated Graph Neural Networks. In ACL.</p>
<p>Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering. In NeurIPS.</p>
<p>Michael J. Denkowski and Alon Lavie. 2014. Meteor Universal: Language Specific Translation Evaluation for Any Target Language. In Workshop on Statistical Machine Translation.</p>
<p>Alex Graves. 2012. Sequence Transduction with Recurrent Neural Networks. arXiv preprint arXiv:1211.3711.</p>
<p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. In Proceedings of the IEEE international conference on computer vision, pages 1026-1034.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-Term Memory. Neural Comput., 9(8):17351780 .</p>
<p>Joohyun Kim and Raymond J Mooney. 2010. Generative Alignment and Semantic Parsing for Learning from Ambiguous Supervision. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 543-551.</p>
<p>Thomas N. Kipf and Max Welling. 2017. SemiSupervised Classification with Graph Convolutional Networks. In ICLR.</p>
<p>Svetlana Kiritchenko and Saif Mohammad. 2016. Capturing Reliable Fine-Grained Sentiment Associations by Crowdsourcing and Best-Worst Scaling. In NAACL-HLT.</p>
<p>Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, and Luke S. Zettlemoyer. 2017. Neural AMR: Sequence-to-Sequence Models for Parsing and Generation. In $A C L$.</p>
<p>Ioannis Konstas and Mirella Lapata. 2013. Inducing Document Plans for Concept-to-Text Generation. In EMNLP, pages 1503-1514.</p>
<p>Rémi Lebret, David Grangier, and Michael Auli. 2016. Neural Text Generation from Structured Data with Application to the Biography Domain. In EMNLP.</p>
<p>Percy Liang, Michael I. Jordan, and Dan Klein. 2009. Learning Semantic Correspondences with Less Supervision. In ACL/AFNLP, pages 91-99.</p>
<p>Jordan J Louviere, Terry N Flynn, and Anthony Alfred John Marley. 2015. Best-Worst Scaling: Theory, Methods and Applications. Cambridge University Press.</p>
<p>Jordan J Louviere and George G Woodworth. 1991. Best-Worst Scaling: A Model for the Largest Difference Judgments. University of Alberta: Working Paper.</p>
<p>Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018. Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction. In EMNLP, pages 3219-3232.</p>
<p>Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective Approaches to Attentionbased Neural Machine Translation. In EMNLP.</p>
<p>Diego Marcheggiani and Laura Perez-Beltrachini. 2018. Deep Graph Convolutional Encoders for Structured Data to Text Generation. INLG.</p>
<p>Hongyuan Mei, Mohit Bansal, and Matthew R Walter. 2016. What to talk about and how? Selective Generation using LSTMs with Coarse-to-Fine Alignment. In NAACL-HLT, pages 720-730.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In $A C L$.</p>
<p>Lutz Prechelt. 1998. Early Stopping - but when? In Neural Networks: Tricks of the Trade, pages 55-69, London, UK, UK. Springer-Verlag.</p>
<p>Ratish Puduppully, Li Dong, and Mirella Lapata. 2019. Data-to-Text Generation with Content Selection and Planning. In $A A A I$.</p>
<p>Ning Qian. 1999. On the momentum term in gradient descent learning algorithms. Neural networks : the official journal of the International Neural Network Society, 12 1:145-151.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving Language Understanding by Generative Pre-Training. Accessed at https://s3-us-west-2.amazonaws. com/openai-assets/research-covers/ language-unsupervised/language_ understanding_paper.pdf.</p>
<p>Abigail See, Peter J Liu, and Christopher D Manning. 2017. Get to the Point: Summarization with Pointer-Generator Networks. arXiv preprint arXiv:1704.04368.</p>
<p>Linfeng Song, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2018. A Graph-to-Sequence Model for AMR-to-Text Generation. In ACL.</p>
<p>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A Simple Way to Prevent Neural Networks from Overfitting. J. Mach. Learn. Res., 15(1):1929-1958.</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to Sequence Learning with Neural Networks. In NeurIPS, pages 3104-3112.</p>
<p>Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, and Hang Li. 2016. Modeling Coverage for Neural Machine Translation. arXiv preprint arXiv:1601.04811.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All You Need. In NeurIPS, pages 5998-6008.</p>
<p>Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2018. Graph Attention Networks. In ICLR.</p>
<p>Qingyun Wang, Zhihao Zhou, Lifu Huang, Spencer Whitehead, Boliang Zhang, Heng Ji, and Kevin Knight. 2018. Paper Abstract Writing through Editing Mechanism. In Proceedings of NAACL-HLT.</p>
<p>Sam Wiseman, Stuart M Shieber, and Alexander M Rush. 2017. Challenges in Data-to-document Generation. In EMNLP.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Attempts to crowd source this evaluation failed.
${ }^{3}$ Due to the larger size and greater variety of our dataset and accompanying vocabularies compared to theirs, we were unable to train this model with the reported batch size of 240. We use batch size 24 instead, which is partially responsible for the lower performance.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
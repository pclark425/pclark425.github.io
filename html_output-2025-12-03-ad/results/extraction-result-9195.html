<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9195 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9195</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9195</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-269983662</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.14766v1.pdf" target="_blank">Evaluating Large Language Models for Public Health Classification and Extraction Tasks</a></p>
                <p><strong>Paper Abstract:</strong> Advances in Large Language Models (LLMs) have led to significant interest in their potential to support human experts across a range of domains, including public health. In this work we present automated evaluations of LLMs for public health tasks involving the classification and extraction of free text. We combine six externally annotated datasets with seven new internally annotated datasets to evaluate LLMs for processing text related to: health burden, epidemiological risk factors, and public health interventions. We initially evaluate five open-weight LLMs (7-70 billion parameters) across all tasks using zero-shot in-context learning. We find that Llama-3-70B-Instruct is the highest performing model, achieving the best results on 15/17 tasks (using micro-F1 scores). We see significant variation across tasks with all open-weight LLMs scoring below 60% micro-F1 on some challenging tasks, such as Contact Classification, while all LLMs achieve greater than 80% micro-F1 on others, such as GI Illness Classification. For a subset of 12 tasks, we also evaluate GPT-4 and find comparable results to Llama-3-70B-Instruct, which scores equally or outperforms GPT-4 on 6 of the 12 tasks. Overall, based on these initial results we find promising signs that LLMs may be useful tools for public health experts to extract information from a wide variety of free text sources, and support public health surveillance, research, and interventions.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9195.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9195.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 70-billion-parameter open-weight instruction-tuned language model used as the top-performing open-weight model in this paper's public‑health NLU evaluations across classification and extraction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating Large Language Models for Public Health Classification and Extraction Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-weight instruction-tuned LLM with ~70 billion parameters, run in FP16 where feasible, context window ~8,192 tokens in the authors' internal HPC deployment; evaluated as an off-the-shelf in‑context learner (zero-shot and few-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Public health (subdomains: burden, epidemiological risk factors, interventions)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based simulation of expert classification and structured extraction across 17 public‑health tasks (examples: disease mention extraction from PubMed abstracts, GI illness classification and symptom/food extraction from social media reviews, ICD-10 description infection classification, contact-type classification from questionnaire-style free text, guidance recommendation detection, BioDex adverse-drug-event extraction, MMLU subsets for domain knowledge).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Primary: micro‑F1 (headline measure); also macro‑F1. Extraction tasks evaluated by exact string matching with postprocessing/normalisation for some tasks (e.g., mapping disease mentions to MeSH/OMIM codes, country/food lookup tables).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Reported as the highest-scoring open-weight model, achieving the best micro‑F1 on 15/17 tasks in these evaluations. Performance varies by task: examples called out by the authors include >80% micro‑F1 on easier tasks (e.g., Gastrointestinal Illness Classification, Country Disambiguation, Food Extraction, ICD-10 description classification) and <60% micro‑F1 on harder zero‑shot tasks (notably Contact Classification, BioDex Drugs Extraction, MMLU Virology). (Paper reports specific per-task micro‑F1s in Table 3; overall summary above is as stated in the text.)</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Model size (scaling from 8B -> 70B produced >10 percentage point gains on multiple tasks), prompt engineering (zero‑shot vs few‑shot; few‑shot provided >15 pp gains on some hard tasks), output-format fragility (models failing to produce requested format e.g., JSON), long-document handling/context-window limits, task complexity (nuanced label definitions, need to apply protocol definitions), dataset/annotation alignment (mismatch between prompt definitions and original annotation protocols), model implementation details (quantisation, generation config), and pretraining/fine-tuning data coverage (sensitivity to sexual-content-avoidance in pretraining affecting contact‑related text).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against four other internal open-weight models (Mistral-7B, Llama-3-8B-Instruct, Flan-T5-xxl, Stable-Beluga-2) and GPT-4 (private) on a 12-task subset; Llama-3-70B-Instruct scored best among open-weight models on 15/17 tasks and scored equally or outperformed GPT-4 on 6 of the 12 tasks where GPT-4 was evaluated, i.e., comparable to GPT-4 across the subset.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Poor zero‑shot performance on tasks requiring long-document extraction (BioDex Drugs Extraction), and on tasks that require strict application of user‑supplied protocols or nuanced causal distinctions (Contact Classification, Health Causal Claims) without few‑shot examples; fragile output formatting (invalid JSON or explanations instead of labels) leading to underestimation of capabilities; sensitive to prompt wording (smaller models more fragile); extraction span-size errors (too many/too large spans).</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors recommend using few‑shot examples (and Chain‑of‑Thought where appropriate) and more complex prompting pipelines to raise performance on hard tasks, considering task‑specific fine‑tuning for production use, developing and embedding rigorous domain annotation protocols into prompts, post‑processing and output validation to mitigate format fragility, in‑context validation and human‑in‑the‑loop checks before deployment, and contextual evaluation on target datasets because benchmark applicability can vary.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models for Public Health Classification and Extraction Tasks', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9195.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9195.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (generator & evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (gpt-4-turbo-2024-04-09 via OpenAI API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A private, high-performing LLM (accessed via OpenAI API) used both to generate synthetic questionnaire-style data for Contact Type Classification and as an evaluated classifier/extractor on a 12-task subset for comparison to open-weight models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating Large Language Models for Public Health Classification and Extraction Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4-turbo-2024-04-09 (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Private multimodal-capable LLM (OpenAI GPT-4 family); in the authors' comparison the deployed GPT-4 token context reported as large (table shows context up to 128,000 tokens for this variant); accessed via OpenAI API for selected tasks and for synthetic data generation.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Public health (epidemiology/contact tracing) and general biomedical NLU (disease/drug extraction, causal claim detection, domain MMLU QA subsets).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>1) Text simulation: generation of an entirely synthetic dataset of questionnaire-style contact descriptions for Contact Type Classification (GPT-4 prompted to generate instance text and assign labels); 2) Text-based simulation/evaluation: zero‑shot/few‑shot classification and extraction on public‑health tasks (12-task subset) including causal-claim classification, MMLU genetics/virology/nutrition subsets, PubMedQA, BioDex drug extraction, guidance recommendation detection, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>For classification/extraction: micro‑F1 and macro‑F1 (paper's headline metric micro‑F1); for synthetic-data generation the synthetic labels were validated via two independent human annotators (>90% agreement reported) and discrepancies reconciled.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>When evaluated on the 12-task subset GPT-4 'performs well' and was the highest-scoring model on 6 of 12 tasks; it particularly outperformed open-weight models on Health Causal Claims Classification and MMLU Genetics. The authors also report that no model (including GPT-4) exceeded 60% micro‑F1 in zero‑shot on particularly hard tasks (BioDex Drugs Extraction, Contact Classification, MMLU Virology). For the synthetic data generation, GPT-4 outputs were validated by two human reviewers with >90% agreement after reconciliation.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Prompt design (zero‑ vs few‑shot; few‑shot and CoT can substantially improve results), context window / input truncation (long documents truncated for BioDex, limiting available evidence), output formatting and parsing reliability, alignment of prompt-defined label semantics with dataset annotation protocols, and inherent task difficulty/domain knowledge density.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly to multiple internal open-weight models (including Llama-3-70B-Instruct); GPT-4 was top on half the evaluated subset but overall performed comparably to Llama-3-70B (Llama-3-70B equalled or outperformed GPT‑4 on the remaining tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Failed to reach acceptable accuracy in zero‑shot for challenging long‑document extraction and nuanced protocol‑driven classification tasks; synthetic data generation required human validation (GPT-4 labels were cross-checked by two human reviewers) indicating that generation alone is not a guaranteed gold standard; truncation of input abstracts/papers reduces available evidence for ADE extraction and question‑answering tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Use few‑shot prompting and more sophisticated pipelines (CoT, output validation/correction) to improve performance, validate synthetically generated data with human reviewers, consider fine‑tuning or bespoke pipelines for long document extraction, and ensure annotation protocols are explicit in prompts to align model outputs with ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models for Public Health Classification and Extraction Tasks', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9195.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9195.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioDEX (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Biodex: Large-scale biomedical adverse drug event extraction for real-world pharmacovigilance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An external biomedical extraction benchmark/dataset (BioDEX) referenced in the paper where prior work evaluated GPT-3.5 and GPT-4 for extracting adverse drug event (ADE) attributes from PubMed articles, serving as an example of LLMs applied as text-based extractive simulators in pharmacovigilance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Biodex: Large-scale biomedical adverse drug event extraction for real-world pharmacovigilance</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 / GPT-4 (as reported in cited prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>In the cited BioDEX study, few-shot prompting was used with GPT‑3.5 and GPT‑4 on truncated paper inputs (abstracts used to fit context windows); the models were applied to extract core ADE report attributes from biomedical text.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Pharmacovigilance / biomedical adverse drug event extraction</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Extract structured Adverse Drug Event attributes (patient sex, seriousness, reactions, drugs) from PubMed articles (long-document biomedical text).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>F1 score on extraction of core ADE attributes (exact or task-appropriate matching as defined by BioDEX evaluation protocols).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Prior work (as summarised in this paper) reports an overall F1 of approximately 0.5 for GPT-3.5/GPT-4 with few‑shot prompting on the BioDEX ADE attribute extraction task — substantially below expert level.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Long-input documents requiring truncation (context window limits), use of abstracts instead of full text, prompt construction and few‑shot example choice, and the difficulty of extracting correct spans/attribute boundaries from long biomedical narratives.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to expert annotations / benchmark ground truth; reported to be below expert level (F1 ≈ 0.5), indicating room for improvement relative to task‑specific systems or expert human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Context-window truncation reduces evidence available to the model, leading to missed or spurious extractions; few‑shot prompts on abstracts may omit key information present in full text; extraction from long documents remains a consistent failure mode.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Highlights the need for strategies to handle long documents (chunking, retrieval + read pipelines), better prompt construction, and possibly model fine‑tuning or specialized extraction pipelines to reach expert‑level performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models for Public Health Classification and Extraction Tasks', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9195.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9195.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Disease classification studies (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited literature where GPT‑4 and related LLMs were applied to medical/disease classification tasks (examples include disease classification from EHRs and public health tweets), showing high variability across tasks and improvements with few‑shot or CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 / GPT-3.5 (as reported in cited prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Private models (GPT‑4/GPT‑3.5) evaluated in prior studies on clinical and public‑health classification tasks, typically used in zero‑ and few‑shot prompting regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Clinical medicine / public health surveillance (disease classification from EHRs, social media)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Single‑label disease classification from electronic health records or social media posts (simulate human clinical labeling/triage).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>F1 score (per‑disease F1 reported), sometimes macro/micro F1 depending on study.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>As summarised in this paper: examples include GPT‑4 zero‑shot F1 between ~0.75 and 0.96 across 5 diseases in one cited study, and F1 ranges from ~0.35 to 0.8 across tasks in another cited social‑media study — demonstrating high variance by task.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Prompting (zero vs few‑shot, CoT), task definition and annotation protocols, domain and dataset differences (EHR vs social media), and the inherent variability of social media language.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to smaller LLMs and fine‑tuned baselines in cited studies; GPT‑4 generally outperformed GPT‑3.5 in the cited evaluations but performance varied considerably by task.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High variance across tasks; some tasks remain challenging particularly where annotation protocols are not explicit or where social media language is adversarial/ambiguous; few‑shot and CoT can materially change results.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Cited studies and this paper jointly recommend careful prompt design (few‑shot/CoT), domain‑specific evaluation, and the possible use of fine‑tuned or task‑specific models where high reliability is required.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models for Public Health Classification and Extraction Tasks', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9195.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9195.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rare disease extraction (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Identifying and extracting rare disease phenotypes with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited evaluation of LLMs (including ChatGPT) on extraction of rare disease mentions, signs and symptoms showing substantial variability across label types and improvements with few‑shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Identifying and extracting rare disease phenotypes with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (GPT-3.5) and others (as reported in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs applied to a rare‑disease extraction corpus (RareDis) with zero‑ and few‑shot prompting; extraction evaluated with relaxed matching in the cited study.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Rare disease phenotyping / biomedical information extraction</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Extract rare diseases, signs and symptoms from clinical/biomedical text (simulate clinical phenotyping and information extraction workflows).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>F1 score (reported with relaxed/partial matching for extraction span variability).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Reported (as summarised in this paper) ChatGPT zero‑shot relaxed matching F1 scores overall ~0.41–0.47, with substantial variation by label type (e.g., symptoms F1 ~0.15, rare disease mentions F1 ~0.76); few‑shot prompting generally improved performance.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Complexity and granularity of labels, exact vs relaxed matching evaluation methodology, availability of few‑shot exemplars, and label definition clarity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to human‑annotated ground truth from RareDis; models performed well on some label types and poorly on others, highlighting heterogeneity of capability across extraction categories.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Low performance on fine‑grained labels (e.g., symptoms) with zero‑shot prompting; evaluation sensitive to exact matching criteria leading to worse apparent performance for complex labels.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Using few‑shot prompting and relaxed matching helps; careful design of evaluation metrics and annotation protocols is necessary to fairly assess LLM extraction performance on complex biomedical labels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models for Public Health Classification and Extraction Tasks', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Biodex: Large-scale biomedical adverse drug event extraction for real-world pharmacovigilance <em>(Rating: 2)</em></li>
                <li>Identifying and extracting rare disease phenotypes with large language models <em>(Rating: 2)</em></li>
                <li>Rarebench (Can LLMs serve as rare diseases specialists?) <em>(Rating: 2)</em></li>
                <li>Evaluating the chatgpt family of models for biomedical reasoning and classification <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9195",
    "paper_id": "paper-269983662",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "Llama-3-70B",
            "name_full": "Llama-3-70B-Instruct",
            "brief_description": "A 70-billion-parameter open-weight instruction-tuned language model used as the top-performing open-weight model in this paper's public‑health NLU evaluations across classification and extraction tasks.",
            "citation_title": "Evaluating Large Language Models for Public Health Classification and Extraction Tasks",
            "mention_or_use": "use",
            "model_name": "Llama-3-70B-Instruct",
            "model_description": "Open-weight instruction-tuned LLM with ~70 billion parameters, run in FP16 where feasible, context window ~8,192 tokens in the authors' internal HPC deployment; evaluated as an off-the-shelf in‑context learner (zero-shot and few-shot).",
            "scientific_subdomain": "Public health (subdomains: burden, epidemiological risk factors, interventions)",
            "simulation_task": "Text-based simulation of expert classification and structured extraction across 17 public‑health tasks (examples: disease mention extraction from PubMed abstracts, GI illness classification and symptom/food extraction from social media reviews, ICD-10 description infection classification, contact-type classification from questionnaire-style free text, guidance recommendation detection, BioDex adverse-drug-event extraction, MMLU subsets for domain knowledge).",
            "evaluation_metric": "Primary: micro‑F1 (headline measure); also macro‑F1. Extraction tasks evaluated by exact string matching with postprocessing/normalisation for some tasks (e.g., mapping disease mentions to MeSH/OMIM codes, country/food lookup tables).",
            "simulation_accuracy": "Reported as the highest-scoring open-weight model, achieving the best micro‑F1 on 15/17 tasks in these evaluations. Performance varies by task: examples called out by the authors include &gt;80% micro‑F1 on easier tasks (e.g., Gastrointestinal Illness Classification, Country Disambiguation, Food Extraction, ICD-10 description classification) and &lt;60% micro‑F1 on harder zero‑shot tasks (notably Contact Classification, BioDex Drugs Extraction, MMLU Virology). (Paper reports specific per-task micro‑F1s in Table 3; overall summary above is as stated in the text.)",
            "factors_affecting_accuracy": "Model size (scaling from 8B -&gt; 70B produced &gt;10 percentage point gains on multiple tasks), prompt engineering (zero‑shot vs few‑shot; few‑shot provided &gt;15 pp gains on some hard tasks), output-format fragility (models failing to produce requested format e.g., JSON), long-document handling/context-window limits, task complexity (nuanced label definitions, need to apply protocol definitions), dataset/annotation alignment (mismatch between prompt definitions and original annotation protocols), model implementation details (quantisation, generation config), and pretraining/fine-tuning data coverage (sensitivity to sexual-content-avoidance in pretraining affecting contact‑related text).",
            "comparison_baseline": "Compared against four other internal open-weight models (Mistral-7B, Llama-3-8B-Instruct, Flan-T5-xxl, Stable-Beluga-2) and GPT-4 (private) on a 12-task subset; Llama-3-70B-Instruct scored best among open-weight models on 15/17 tasks and scored equally or outperformed GPT-4 on 6 of the 12 tasks where GPT-4 was evaluated, i.e., comparable to GPT-4 across the subset.",
            "limitations_or_failure_cases": "Poor zero‑shot performance on tasks requiring long-document extraction (BioDex Drugs Extraction), and on tasks that require strict application of user‑supplied protocols or nuanced causal distinctions (Contact Classification, Health Causal Claims) without few‑shot examples; fragile output formatting (invalid JSON or explanations instead of labels) leading to underestimation of capabilities; sensitive to prompt wording (smaller models more fragile); extraction span-size errors (too many/too large spans).",
            "author_recommendations_or_insights": "Authors recommend using few‑shot examples (and Chain‑of‑Thought where appropriate) and more complex prompting pipelines to raise performance on hard tasks, considering task‑specific fine‑tuning for production use, developing and embedding rigorous domain annotation protocols into prompts, post‑processing and output validation to mitigate format fragility, in‑context validation and human‑in‑the‑loop checks before deployment, and contextual evaluation on target datasets because benchmark applicability can vary.",
            "uuid": "e9195.0",
            "source_info": {
                "paper_title": "Evaluating Large Language Models for Public Health Classification and Extraction Tasks",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT-4 (generator & evaluator)",
            "name_full": "GPT-4 (gpt-4-turbo-2024-04-09 via OpenAI API)",
            "brief_description": "A private, high-performing LLM (accessed via OpenAI API) used both to generate synthetic questionnaire-style data for Contact Type Classification and as an evaluated classifier/extractor on a 12-task subset for comparison to open-weight models.",
            "citation_title": "Evaluating Large Language Models for Public Health Classification and Extraction Tasks",
            "mention_or_use": "use",
            "model_name": "gpt-4-turbo-2024-04-09 (GPT-4)",
            "model_description": "Private multimodal-capable LLM (OpenAI GPT-4 family); in the authors' comparison the deployed GPT-4 token context reported as large (table shows context up to 128,000 tokens for this variant); accessed via OpenAI API for selected tasks and for synthetic data generation.",
            "scientific_subdomain": "Public health (epidemiology/contact tracing) and general biomedical NLU (disease/drug extraction, causal claim detection, domain MMLU QA subsets).",
            "simulation_task": "1) Text simulation: generation of an entirely synthetic dataset of questionnaire-style contact descriptions for Contact Type Classification (GPT-4 prompted to generate instance text and assign labels); 2) Text-based simulation/evaluation: zero‑shot/few‑shot classification and extraction on public‑health tasks (12-task subset) including causal-claim classification, MMLU genetics/virology/nutrition subsets, PubMedQA, BioDex drug extraction, guidance recommendation detection, etc.",
            "evaluation_metric": "For classification/extraction: micro‑F1 and macro‑F1 (paper's headline metric micro‑F1); for synthetic-data generation the synthetic labels were validated via two independent human annotators (&gt;90% agreement reported) and discrepancies reconciled.",
            "simulation_accuracy": "When evaluated on the 12-task subset GPT-4 'performs well' and was the highest-scoring model on 6 of 12 tasks; it particularly outperformed open-weight models on Health Causal Claims Classification and MMLU Genetics. The authors also report that no model (including GPT-4) exceeded 60% micro‑F1 in zero‑shot on particularly hard tasks (BioDex Drugs Extraction, Contact Classification, MMLU Virology). For the synthetic data generation, GPT-4 outputs were validated by two human reviewers with &gt;90% agreement after reconciliation.",
            "factors_affecting_accuracy": "Prompt design (zero‑ vs few‑shot; few‑shot and CoT can substantially improve results), context window / input truncation (long documents truncated for BioDex, limiting available evidence), output formatting and parsing reliability, alignment of prompt-defined label semantics with dataset annotation protocols, and inherent task difficulty/domain knowledge density.",
            "comparison_baseline": "Compared directly to multiple internal open-weight models (including Llama-3-70B-Instruct); GPT-4 was top on half the evaluated subset but overall performed comparably to Llama-3-70B (Llama-3-70B equalled or outperformed GPT‑4 on the remaining tasks).",
            "limitations_or_failure_cases": "Failed to reach acceptable accuracy in zero‑shot for challenging long‑document extraction and nuanced protocol‑driven classification tasks; synthetic data generation required human validation (GPT-4 labels were cross-checked by two human reviewers) indicating that generation alone is not a guaranteed gold standard; truncation of input abstracts/papers reduces available evidence for ADE extraction and question‑answering tasks.",
            "author_recommendations_or_insights": "Use few‑shot prompting and more sophisticated pipelines (CoT, output validation/correction) to improve performance, validate synthetically generated data with human reviewers, consider fine‑tuning or bespoke pipelines for long document extraction, and ensure annotation protocols are explicit in prompts to align model outputs with ground truth.",
            "uuid": "e9195.1",
            "source_info": {
                "paper_title": "Evaluating Large Language Models for Public Health Classification and Extraction Tasks",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "BioDEX (prior work)",
            "name_full": "Biodex: Large-scale biomedical adverse drug event extraction for real-world pharmacovigilance",
            "brief_description": "An external biomedical extraction benchmark/dataset (BioDEX) referenced in the paper where prior work evaluated GPT-3.5 and GPT-4 for extracting adverse drug event (ADE) attributes from PubMed articles, serving as an example of LLMs applied as text-based extractive simulators in pharmacovigilance.",
            "citation_title": "Biodex: Large-scale biomedical adverse drug event extraction for real-world pharmacovigilance",
            "mention_or_use": "mention",
            "model_name": "GPT-3.5 / GPT-4 (as reported in cited prior work)",
            "model_description": "In the cited BioDEX study, few-shot prompting was used with GPT‑3.5 and GPT‑4 on truncated paper inputs (abstracts used to fit context windows); the models were applied to extract core ADE report attributes from biomedical text.",
            "scientific_subdomain": "Pharmacovigilance / biomedical adverse drug event extraction",
            "simulation_task": "Extract structured Adverse Drug Event attributes (patient sex, seriousness, reactions, drugs) from PubMed articles (long-document biomedical text).",
            "evaluation_metric": "F1 score on extraction of core ADE attributes (exact or task-appropriate matching as defined by BioDEX evaluation protocols).",
            "simulation_accuracy": "Prior work (as summarised in this paper) reports an overall F1 of approximately 0.5 for GPT-3.5/GPT-4 with few‑shot prompting on the BioDEX ADE attribute extraction task — substantially below expert level.",
            "factors_affecting_accuracy": "Long-input documents requiring truncation (context window limits), use of abstracts instead of full text, prompt construction and few‑shot example choice, and the difficulty of extracting correct spans/attribute boundaries from long biomedical narratives.",
            "comparison_baseline": "Compared to expert annotations / benchmark ground truth; reported to be below expert level (F1 ≈ 0.5), indicating room for improvement relative to task‑specific systems or expert human performance.",
            "limitations_or_failure_cases": "Context-window truncation reduces evidence available to the model, leading to missed or spurious extractions; few‑shot prompts on abstracts may omit key information present in full text; extraction from long documents remains a consistent failure mode.",
            "author_recommendations_or_insights": "Highlights the need for strategies to handle long documents (chunking, retrieval + read pipelines), better prompt construction, and possibly model fine‑tuning or specialized extraction pipelines to reach expert‑level performance.",
            "uuid": "e9195.2",
            "source_info": {
                "paper_title": "Evaluating Large Language Models for Public Health Classification and Extraction Tasks",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Disease classification studies (prior work)",
            "name_full": "",
            "brief_description": "Cited literature where GPT‑4 and related LLMs were applied to medical/disease classification tasks (examples include disease classification from EHRs and public health tweets), showing high variability across tasks and improvements with few‑shot or CoT prompting.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-4 / GPT-3.5 (as reported in cited prior work)",
            "model_description": "Private models (GPT‑4/GPT‑3.5) evaluated in prior studies on clinical and public‑health classification tasks, typically used in zero‑ and few‑shot prompting regimes.",
            "scientific_subdomain": "Clinical medicine / public health surveillance (disease classification from EHRs, social media)",
            "simulation_task": "Single‑label disease classification from electronic health records or social media posts (simulate human clinical labeling/triage).",
            "evaluation_metric": "F1 score (per‑disease F1 reported), sometimes macro/micro F1 depending on study.",
            "simulation_accuracy": "As summarised in this paper: examples include GPT‑4 zero‑shot F1 between ~0.75 and 0.96 across 5 diseases in one cited study, and F1 ranges from ~0.35 to 0.8 across tasks in another cited social‑media study — demonstrating high variance by task.",
            "factors_affecting_accuracy": "Prompting (zero vs few‑shot, CoT), task definition and annotation protocols, domain and dataset differences (EHR vs social media), and the inherent variability of social media language.",
            "comparison_baseline": "Compared to smaller LLMs and fine‑tuned baselines in cited studies; GPT‑4 generally outperformed GPT‑3.5 in the cited evaluations but performance varied considerably by task.",
            "limitations_or_failure_cases": "High variance across tasks; some tasks remain challenging particularly where annotation protocols are not explicit or where social media language is adversarial/ambiguous; few‑shot and CoT can materially change results.",
            "author_recommendations_or_insights": "Cited studies and this paper jointly recommend careful prompt design (few‑shot/CoT), domain‑specific evaluation, and the possible use of fine‑tuned or task‑specific models where high reliability is required.",
            "uuid": "e9195.3",
            "source_info": {
                "paper_title": "Evaluating Large Language Models for Public Health Classification and Extraction Tasks",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Rare disease extraction (prior work)",
            "name_full": "Identifying and extracting rare disease phenotypes with large language models",
            "brief_description": "A cited evaluation of LLMs (including ChatGPT) on extraction of rare disease mentions, signs and symptoms showing substantial variability across label types and improvements with few‑shot prompting.",
            "citation_title": "Identifying and extracting rare disease phenotypes with large language models",
            "mention_or_use": "mention",
            "model_name": "ChatGPT (GPT-3.5) and others (as reported in cited work)",
            "model_description": "LLMs applied to a rare‑disease extraction corpus (RareDis) with zero‑ and few‑shot prompting; extraction evaluated with relaxed matching in the cited study.",
            "scientific_subdomain": "Rare disease phenotyping / biomedical information extraction",
            "simulation_task": "Extract rare diseases, signs and symptoms from clinical/biomedical text (simulate clinical phenotyping and information extraction workflows).",
            "evaluation_metric": "F1 score (reported with relaxed/partial matching for extraction span variability).",
            "simulation_accuracy": "Reported (as summarised in this paper) ChatGPT zero‑shot relaxed matching F1 scores overall ~0.41–0.47, with substantial variation by label type (e.g., symptoms F1 ~0.15, rare disease mentions F1 ~0.76); few‑shot prompting generally improved performance.",
            "factors_affecting_accuracy": "Complexity and granularity of labels, exact vs relaxed matching evaluation methodology, availability of few‑shot exemplars, and label definition clarity.",
            "comparison_baseline": "Compared to human‑annotated ground truth from RareDis; models performed well on some label types and poorly on others, highlighting heterogeneity of capability across extraction categories.",
            "limitations_or_failure_cases": "Low performance on fine‑grained labels (e.g., symptoms) with zero‑shot prompting; evaluation sensitive to exact matching criteria leading to worse apparent performance for complex labels.",
            "author_recommendations_or_insights": "Using few‑shot prompting and relaxed matching helps; careful design of evaluation metrics and annotation protocols is necessary to fairly assess LLM extraction performance on complex biomedical labels.",
            "uuid": "e9195.4",
            "source_info": {
                "paper_title": "Evaluating Large Language Models for Public Health Classification and Extraction Tasks",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Biodex: Large-scale biomedical adverse drug event extraction for real-world pharmacovigilance",
            "rating": 2,
            "sanitized_title": "biodex_largescale_biomedical_adverse_drug_event_extraction_for_realworld_pharmacovigilance"
        },
        {
            "paper_title": "Identifying and extracting rare disease phenotypes with large language models",
            "rating": 2,
            "sanitized_title": "identifying_and_extracting_rare_disease_phenotypes_with_large_language_models"
        },
        {
            "paper_title": "Rarebench (Can LLMs serve as rare diseases specialists?)",
            "rating": 2,
            "sanitized_title": "rarebench_can_llms_serve_as_rare_diseases_specialists"
        },
        {
            "paper_title": "Evaluating the chatgpt family of models for biomedical reasoning and classification",
            "rating": 1,
            "sanitized_title": "evaluating_the_chatgpt_family_of_models_for_biomedical_reasoning_and_classification"
        }
    ],
    "cost": 0.022046249999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating Large Language Models for Public Health Classification and Extraction Tasks
23 May 2024</p>
<p>Joshua Harris joshua.harris@ukhsa.gov.uk 
Health Security Agency
UK</p>
<p>Timothy Laurence 
Health Security Agency
UK</p>
<p>Leo Loman 
Health Security Agency
UK</p>
<p>Fan Grayson 
Health Security Agency
UK</p>
<p>Toby Nonnenmacher 
Health Security Agency
UK</p>
<p>Harry Long 
Health Security Agency
UK</p>
<p>Loes Walsgriffith 
Health Security Agency
UK</p>
<p>Amy Douglas 
Health Security Agency
UK</p>
<p>Holly Fountain 
Health Security Agency
UK</p>
<p>Stelios Georgiou 
Health Security Agency
UK</p>
<p>Jo Hardstaff 
Health Security Agency
UK</p>
<p>Kathryn Hopkins 
Health Security Agency
UK</p>
<p>Y-Ling Chi 
Health Security Agency
UK</p>
<p>Galena Kuyumdzhieva 
Health Security Agency
UK</p>
<p>Lesley Larkin 
Health Security Agency
UK</p>
<p>Samuel Collins 
Health Security Agency
UK</p>
<p>Hamish Mohammed 
Health Security Agency
UK</p>
<p>Thomas Finnie 
Health Security Agency
UK</p>
<p>Luke Hounsome 
Health Security Agency
UK</p>
<p>Steven Riley 
Health Security Agency
UK</p>
<p>Evaluating Large Language Models for Public Health Classification and Extraction Tasks
23 May 2024CB8309D418E4B44602F841E68705DCD2arXiv:2405.14766v1[cs.CL]
Advances in Large Language Models (LLMs) have led to significant interest in their potential to support human experts across a range of domains, including public health.In this work we present automated evaluations of LLMs for public health tasks involving the classification and extraction of free text.We combine six externally annotated datasets with seven new internally annotated datasets to evaluate LLMs for processing text related to: health burden, epidemiological risk factors, and public health interventions.We initially evaluate five open-weight LLMs (7-70 billion parameters) across all tasks using zero-shot in-context learning.We find that Llama-3-70B-Instruct is the highest performing model, achieving the best results on 15/17 tasks (using micro-F1 scores).We see significant variation across tasks with all open-weight LLMs scoring below 60% micro-F1 on some challenging tasks, such as Contact Classification, while all LLMs achieve greater than 80% micro-F1 on others, such as GI Illness Classification.For a subset of 12 tasks, we also evaluate GPT-4 and find comparable results to Llama-3-70B-Instruct, which scores equally or outperforms GPT-4 on 6 of the 12 tasks.Overall, based on these initial results we find promising signs that LLMs may be useful tools for public health experts to extract information from a wide variety of free text sources, and support public health surveillance, research, and interventions.</p>
<p>Introduction</p>
<p>There have been rapid improvements in the ability of Large Language Models (LLMs) 1to perform a broad range of text processing tasks [3,4,5].This has led to significant interest in applying them to support experts across a range of domains [6], including public health [7,8,9].</p>
<p>Previous work has often demonstrated considerable variation in how different LLMs perform on a given task, and that a given LLM's performance is highly dependent on the type and nature of the task involved [10,11,12,13,14].Therefore, to understand the potential effective application of LLMs within public health an important prerequisite is developing domain specific evaluations that are representative of the tasks, free text, and knowledge that human experts regularly process.Here, our overarching aim is to understand what factors determine variations in performance, whether that be: the LLM, the nature of the task, the type of free text, or the specific implementation details.</p>
<p>We evaluate LLMs across a wide range of public health tasks and free text.In this initial work, using the definitions provided by Chang et al. [15], we focus on Automated Evaluation (excluding LLM-as-a-Judge [16]) of Natural Language Understanding (NLU) tasks (e.g classification and inference).We also focus solely on evaluating LLM in-context learning (prompting) approaches.</p>
<p>These initial evaluations enable us to start assessing LLMs for potential use in public health in an automated way, including: (1) continually assessing new and existing private and open-weight LLMs for their potential applicability to public health, (2) identifying specific areas of public health where LLMs could potentially be applied, (3) providing a baseline for fine-tuning public health specific LLMs in the future.</p>
<p>We see this work as an important first step to understanding the potential of LLMs to perform public health free text processing tasks.Further research is needed to investigate long form Natural Language Generation (NLG) [15] tasks, with evaluation by public health experts, as well as in-depth studies on specific use-cases and potential issues such bias.</p>
<p>Methods</p>
<p>Evaluation of LLMs is a broad and rapidly growing field of research [15] ranging from very general assessments of capabilities or intelligence [3] to very task specific performance results [17] (Fig. 4).We focus on domain and task specific evaluations of LLMs within public health and provide a review of the relevant literature in Sec. 6.</p>
<p>The appropriate evaluation methodology largely depends on three factors: level of generality (see Sec. 6.1.1),types of task (see Sec. 6.1.2),and outcome of interest (see Sec. 6.1.3).For our initial evaluations of LLMs within public health, we aim to provide a high level assessment of LLM performance over a broad range of tasks, models, and sub-domains.To enable this we focus on classification and extraction NLU tasks that can be assessed using automated evaluation approaches.By collecting representative internal data and annotations in collaboration with public health experts, we aim to also provide early evidence of potential task specific performance in individual areas.Specifically we use:</p>
<ol>
<li>7 New Annotated Datasets -We collect and manually annotate seven datasets with public health specific annotations using a combination of internal, synthetic, and external free text sources.</li>
</ol>
<p>2.</p>
<p>Existing Datasets and Literature -We identify and include six evaluation datasets from existing work that are applicable to public health.To inform and develop our public health evaluations we also review the literature on relevant evaluations in related domains, such as Medicine.We provide a detailed overview of these external results and datasets in Sec.6.2 and Sec.2.1.</p>
<ol>
<li>17 Public Health Specific Evaluation Tasks -In total we bring together 17 classification and extraction evaluation tasks across three sub-domains of public health, see Fig. 1.</li>
</ol>
<p>5 Open-weight LLMs Evaluated and GPT-4 Comparisons -We deploy and assess five</p>
<p>open-weight models ranging from 7-70 billion parameters across all tasks.For a subset of 12 tasks, we also evaluate GPT-4 in order to understand how open-weight model performance compares to one of the highest performing private models on our public health tasks.</p>
<p>Public Health Evaluation Tasks and Datasets</p>
<p>Public health is a broad field, intersecting with many different fields and issues in addition to conditions treated within healthcare settings [18,19,20,21].Therefore, to ensure our evaluations are representative, the 17 classification and extraction tasks we bring together target a commensurately broad range of tasks within three key sub-domains of public health: burden (2.1.1),risk factors (2.1.2),and interventions (2.1.3),as shown in Fig. 2.</p>
<p>The range of data used in our evaluation is also broad, because potentially relevant health information is found in a diverse range of sources: academic literature, electronic health records, public health guidance, social media, news articles, and questionnaire responses [22].Details of all the tasks, datasets, and annotations are shown in Table 1.</p>
<p>Burden</p>
<p>Public health aims to mitigate adverse health outcomes in the population, which requires gathering information on health burden such as reports of symptoms, injuries, cases, morbidity, or mortality [23].Systematic data collection on burden is critical for developing evidence-based public health measures [24,25,26].We use the following tasks to evaluate LLMs in this sub-domain:</p>
<ol>
<li>NCBI Disease Extraction: To evaluate an LLM's ability to identify and extract diseases from free text, we use the NCBI disease corpus [27] of PubMed article abstracts annotated with the diseases mentioned and their associated MeSH (Medical Subject Headings) and OMIM (genes and genetic disorders) codes.This task involves prompting the LLM to extract a structured comma separated list of diseases from the free text.In order to relieve some of the issues observed in the literature around exact matching of output strings [28,29,30], we first map all extracted disease mentions to their respective codes and assess performance on the de-duplicated set of MeSH and OMIM codes.</li>
</ol>
<p>Gastrointestinal Illness Classification:</p>
<p>To evaluate an LLM's ability to identify potential illness or disease within non-technical social media free text, we use the Yelp Open Dataset [31] of restaurant reviews.To annotate the dataset, we first filter to those reviews that contain at least one of a comprehensive list of GI illness related keywords.We then take a random sample of approximately 3000 restaurant reviews and manually annotate (Sec.7) whether they refer to an instance of possible GI illness using an agreed epidemiological protocol.</p>
<p>The LLM is prompted to provide a binary classification of GI illness ("yes" or "no") for each review.This is an adversarial task as all reviews manually annotated as "no" do contain at least one keyword associated with possible GI illness.</p>
<p>Gastrointestinal Illness Symptom Extraction:</p>
<p>To evaluate an LLM's ability to extract possible symptoms from non-technical social media free text, we use the same annotated Yelp review dataset as in 2. but filter to only those annotated as referring to possible GI illness.We then annotate (Sec.7) these reviews with all symptoms mentioned within the free text.The LLM is then prompted to extract all symptoms as a structured comma separated list.</p>
<p>ICD-10 Description Classification:</p>
<p>In order to evaluate an LLM's ability to identify infections and conditions attributed to infections, we use descriptions of abnormal findings, signs of illness and symptoms from the International Statistical Classification of Diseases and Related Health Problems (ICD) [32] classification system.Using a protocol, two research analysts with relevant expertise separately annotate ICD-10 Version:2019 descriptions (Sec.7) with whether they directly refer to an infection or to a disease with a primarily infectious aetiology.We use a balanced sample of infection and non-infection disease descriptions.The LLM is then prompted to provide a binary classification of whether an ICD-10 code description relates to an infection, using the description and a summary of the classification protocol.</p>
<p>News Headline Classification:</p>
<p>We evaluate an LLM's ability to identify references to infectious diseases within non-technical free text using a manually annotated (Sec.7) dataset of news headlines with possible references to avian influenza collected from the GDELT Project [33].The LLM is prompted with a set of 5 news headlines and asked to provide a structured JSON output with its classifications.A secondary purpose of this task is to evaluate the LLM's ability to generate correctly formatted JSON strings consistently.</p>
<p>MMLU Virology:</p>
<p>To evaluate an LLM's basic knowledge of virology, we use the virology subset of the MMLU benchmark [34] (Sec.6.2.3) noting that these questions also cover wider topics related to epidemiology.The LLM is prompted to provide the answer to multiple choice questions.</p>
<p>Risk Factors</p>
<p>Epidemiological risk factors are environmental, behavioral, or biological factors that increase a person's likelihood of developing disease or injury [35].Risk factor surveillance is essential for quantifying these risks and developing evidence-based interventions to reduce them [36].</p>
<p>Addressing risk factors directly may provide a more effective strategy than treating diseases once they arise [37,38].We use the following tasks to evaluate LLMs in this sub-domain:</p>
<ol>
<li>Contact Type Classification: A key challenge during outbreak and pandemic response is often rapidly implementing and scaling contact tracing [39].One important aspect of this is identifying the type of contact that has occurred in order to assess the risk of onward transmission.To evaluate an LLM's ability to identify contact types from representative free text, we generate and manually annotate (Sec.7) an entirely new synthetic dataset created using GPT-4 via the OpenAI API [40], designed to reflect the style, content and structure of the answers provided within the enhanced surveillance questionnaires for contacts submitted during the mpox outbreak response [41].We prompt the LLM to classify the type of contact based on an epidemiological protocol.</li>
</ol>
<p>This task is challenging for two reasons.First, it requires the LLM to apply a detailed protocol provided within the prompt, rather than drawing on existing knowledge provided during pre-training.Second, this particular dataset was chosen because the text often contains discussion of sexual activity, which is an important risk factor for certain infections [42].However, many LLM pre-training [43] and fine-tuning datasets [44] are designed to avoid text about sexual activity and so evaluating performance on this type of free text is essential if using LLMs for certain disease areas in public health.</p>
<ol>
<li>Country Disambiguation: Different pathogens are endemic to different regions of the world [35].As such, understanding the risk profile of an individual often requires understanding their recent travel history or previous countries where they have lived.</li>
</ol>
<p>Identifying geographies within free text is often an important task to help determine an individual's risk of infection or other exposure [45].To evaluate an LLM's knowledge and understanding of geographic locations, we use a manually annotated dataset (Sec.7) of anonymised free text responses from GP registration form place of birth fields where the location cannot be identified using existing automated matching.The main reasons matches fail is people supply place names within countries (without reporting the country) and typographical errors.The LLM is prompted to either disambiguate the country the free text refers to or identify it as unknown.The LLM response is then post-processed to a standardised list of countries using the country-converter Python package [46].</p>
<p>Food Extraction:</p>
<p>To evaluate an LLM's ability to generate structured data on risk factors from social media free text, we use the same filtered annotated Yelp review dataset as in Gastrointestinal Illness Symptom Extraction (3.).We then manually annotate these reviews with all the foods mentioned within the free text.The LLM is prompted to extract all references to food or meals as a structured comma separated list.Foods are very challenging to disambiguate, so we use a large lookup table of foods based on the FoodEx 2 database [47] to disambiguate the foods the LLM extracts into a list of 27 potential labels that are relevant to public health food borne illness investigation.</p>
<p>MMLU Genetics:</p>
<p>To evaluate an LLM's basic knowledge of genetics, we use the Medical Genetics subset of the MMLU benchmark [34] (Sec.6.2.3).The LLM is prompted to provide the answer to multiple choice questions on a range of topics within medical genetics.</p>
<p>MMLU Nutrition:</p>
<p>Similarly, to evaluate an LLM's basic knowledge of nutrition, we use the Nutrition subset of the MMLU benchmark [34] (Sec.6.2.3).The LLM is prompted to provide the answer to multiple choice questions on a range of topics within nutrition.</p>
<p>Interventions</p>
<p>Public health interventions can take many forms [21] [48].One common way to classify them is into pharmaceutical and non-pharmaceutical interventions [49].Public health guidance is one of the primary ways public health interventions can be communicated and implemented.Therefore, we introduce four initial guidance related evaluations of increasing difficulty to assess LLMs.We also evaluate LLMs for processing text related to pharmaceutical interventions and biomedical reasoning.</p>
<p>Guidance Topic Classification:</p>
<p>To evaluate an LLM's ability to identify the relevant topics to which a piece of guidance relates, we used 331 UKHSA guidance publication summary pages from the gov.uk web page.The text was extracted and manually categorised (Sec.7) into one of eight broad health topics based on which team wrote the guidance (e.g.Radiation, Sexually Transmitted Infections, etc.).The LLM is prompted with the summary page and the list of possible health topics and asked to return the single health topic that is most suitable for the text.</p>
<p>Guidance Recommendation Classification:</p>
<p>Recommendations are a crucial component of public health guidance.Demonstrating an LLM's ability to identify recommendations is an essential prerequisite to other processing of public health guidance.To evaluate this we use UKHSA publications on gov.uk.Each publication is split into subsections (chunks) and we manually annotate a random sample of 489 chunks of text, with one of two labels: 1 (contains recommendations -defined as a statement containing a clear, specific actionable instruction, request, or advice in the event of a given public health scenario), or 0 (does not contain any recommendations).The LLM is prompted with the text chunk and asked to answer "yes" or "no" (corresponding to class 1 or 0) to whether it contains any recommendations.</p>
<p>Health Advice Classification:</p>
<p>To further evaluate an LLM's ability to understand recommendations, we adopt the HealthAdvice dataset from Chen et al. [50] (Sec.6.2.2), which includes 10,845 manually annotated sentences from the abstract and discussion sections of PubMed articles.Each sentence has one of three labels: 0 (no advice), 1 (weak advice -statement hints that a behaviour or practice may require changing, or that an alternative approach for existing clinical or medical practice may be required), 2 (strong advice -statement makes a clear and straightforward recommendation for a change in behaviour or practice) [51].The LLM is prompted with the sentence and asked to assign the sentence a label of 0, 1, or 2, corresponding to the levels of health advice described.This extends 2. to a different corpus (biomedical literature as opposed to public health guidance), shorter text (sentences rather than chunks), and advice slightly differing in definition from recommendation.</p>
<p>Health Causal Claims Classification:</p>
<p>The final guidance-related task we consider is evaluating an LLM's understanding of the types of claims that can be found within guidance and broader biomedical text.To do this, we use the Causal-Relation dataset [50] (Sec.6.2.2), which consists of annotated biomedical text from PubMed article conclusions.Each sentence is labelled as one of the following classes: 0 (no relationship), 1 (correlational relationship -association between variables are described, but causation is not explicitly stated), 2 (conditional causal relationship -suggestion that one variable directly changes the other, with an element of doubt), or 3 (direct causal -explicit statement that one variable directly changes the other) [52].The LLM is prompted with the sentence and asked to assign a label of 0, 1, 2, or 3, based on the relationship described.</p>
<p>BioDex Drugs Extraction:</p>
<p>To evaluate an LLM's ability to extract information on pharmaceutical interventions we use 1,558 randomly sampled entries from the BioDex dataset [29] (Sec.6.2.2) of annotated Adverse Drug Event reports and corresponding PubMed articles.To account for different model context windows we first chunk each article into sets of contiguous paragraphs of no more than 6000 characters.The LLM is prompted to extracted the drugs involved in the ADE from each of the raw PubMed article free text chunks.</p>
<ol>
<li>PubMedQA: Text regarding guidance and interventions often includes supporting evidence and relevant academic findings.Therefore, we also evaluate an LLM's ability to understand and reason about biomedical evidence.To evaluate an LLM on this task we use the PubMedQA [53] benchmark in the "reasoning-required setting".The LLM is prompted with a PubMed abstract (excluding any concluding sections) and asked to answer the question ("yes", "no", or "maybe") that is posed in the title of the article.The LLM's performance is evaluated against expert human annotator answers.</li>
</ol>
<p>Summary</p>
<p>In constructing these evaluations we have balanced general public health text processing (such as drugs and disease extraction) with specific public health tasks (such as contact and guidance classification).We also combine external evaluation datasets for comparability and diversity of text with internal datasets that are more representative of public health specific tasks and free text.</p>
<p>Evaluation Methodology</p>
<p>Our aim is to provide a consistent assessment of LLMs across tasks, rather than attempt to achieve the highest possible performance for any given model and task combination.Therefore, in this paper we focus on implementing simple but standardised prompting, postprocessing, and sampling across all models.As discussed in Sec.</p>
<p>Large Language Models</p>
<p>In our initial evaluations we assess five open-weight LLMs ranging from 7 billion to 70 billion parameters (including the Llama-2 and 3, Mistral, and Flan-T5 base models), see Table 2. To run the evaluations, we have developed an internal LLM API on UKHSA High Performance Computing (HPC) resources [7] using models from the HuggingFace repository [58] and open-source packages such as transformers [59] and vLLM [60].This enables us to securely use datasets where data governance and security mean they are not authorised to leave UKHSA systems.It also allows us to control implementation details, such as model quantisation, prompt templates, model versions, and generation configurations, that have been shown to have potentially significant impacts on performance [61], allowing for comparable and reproducible results.</p>
<p>In addition to evaluating internally hosted models, we also access one of the highest performing private models, GPT-4 3 , via the OpenAI API [40].While we do not run GPT-4 for all evaluations primarily due to data restrictions, we provide results for GPT-4 on a subset of 12 tasks across the three areas for comparison.</p>
<p>Prompting</p>
<p>For all tasks, we report zero-shot prompting results.Additionally, for a subset of more complex tasks we also investigate few-shot prompting results.</p>
<p>We use the prompt templates provided by the model authors where available.This leads to some variation in prompt content due to the availability of system prompts in some templates.However, we aim to keep the informational content consistent across prompts as far as possible.</p>
<p>Sampling</p>
<p>For all tasks, we use greedy decoding in order to generate reproducible results.</p>
<p>Dataset Splits</p>
<p>We use a 20-80 validation-test split for all tasks, where only the validation set is used for prompt development; this avoids overfitting the prompt to the test set, which we use to quantify performance in this paper.</p>
<p>Evaluation Metrics</p>
<p>In this paper we primarily report the micro and macro F1 scores for each task.The F1 score is calculated as the harmonic mean of precision 4 (positive predictive value) and recall 5  (sensitivity).The micro-F1 score is calculated by weighting each label according to its frequency in the dataset, while the macro-F1 score is calculated weighting each label equally.</p>
<p>It is also important to note that for single label classification tasks the micro-F1 score is equivalent to accuracy.</p>
<p>Our headline measure of performance is micro-F1, as we are assessing raw performance rather than accounting for potentially heterogeneous public health significance of different labels.</p>
<p>For extraction tasks, we use exact matching of output strings.We treat extracted outputs not found in the ground-truth label set as valid (i.e included in result calculations) but incorrect classifications (because they are not one of the ground-truth label options).</p>
<p>For classification tasks, we also use exact matching but with simple post-processing to clean outputs (e.g we would convert "Rule 1" -&gt; "rule 1", and "1." -&gt; 1).</p>
<p>However, for some tasks with large numbers of possible labels and where it is feasible, we implement more advanced post-processing to standardise outputs, such as for Country Disambiguation, NCBI Disease Extraction, and Food Extraction.</p>
<p>Results</p>
<p>We primarily discuss results for internally hosted open-weight models as they can be evaluated for all tasks.We compare these results with GPT-4's performance on a subset of tasks in Sec.3.4.</p>
<p>Model Results</p>
<p>Table 3 shows the performance of models on different tasks.Llama Flan-T5-xxl, Llama-3-8B-Instruct or Mistral-7B-Instruct-v0.2 were each the worst performing model on at least one task.The authors note that smaller models appear to have outputs that are more fragile with respect to the exact wording of the prompt.This means that particularly low performance on a given task may be more related to a failure of the model to understand the prompter's intentions, rather than the model not being capable of performing the task with optimised prompts.</p>
<p>On some tasks the difference in performance between models is very considerable.For instance, scaling parameter size from the Llama-3-8B-Instruct to the Llama-3-70B-Instruct leads to greater than 10 percentage point increases in micro-F1 on tasks such as MMLU Genetics, MMLU Nutrition, Health Advice, and Causal Relation classification.Similarly, moving from a Llama-2-70B base model to a Llama-3-70B base model also considerably improves performance on the same tasks.</p>
<p>Task Results</p>
<p>We find significant variability in performance across tasks.For tasks like ICD-10 Description Classification all of the models classify text with a high level of accuracy, with the minimum micro-F1 score achieved 92%.Similarly, for other tasks like Gastrointestinal Illness Classifi-cation, Country Disambiguation, and Food Extraction, all models achieve micro-F1 scores above 80%.</p>
<p>However, some tasks still appear highly challenging for the LLMs evaluated, with no LLM getting over 60% micro-F1 on any of BioDex Drugs Extraction, Contact Classification and MMLU Virology question answering using zero-shot prompts.</p>
<p>Few-shot Prompting</p>
<p>To investigate the potential impact of using more advanced prompting techniques on challenging tasks, we evaluate the impact of few-shot prompting on the hardest internal and external classification tasks.</p>
<p>For the hardest of our internally annotated tasks, Contact Classification, we use a 10-shot prompt illustrating how to apply the protocol in difficult edge cases.We find substantial improvements across all models, see Fig. 3.With the exception of Flan-T5-xxl, all other models see greater than 15 percentage point increases in micro-F1 scores over the zero-shot setting.Similarly, for the hardest of the externally annotated classification tasks, Health Causal Claims Classification, we use a 7-shot prompt demonstrating the application of the definitions to example sentences (see Fig. 6).Again with the notable exception of Flan-T5-xxl (which performs strongly in the zero-shot setting), we also see a generally greater than 15 percentage point increase in micro-F1 scores over the zero-shot baseline across models, see Fig. 3. Large improvements in performance from few-shot prompting on this task were also found for GPT-4 and GPT-3.5 in the original work by Chen et al. [50].</p>
<p>The relative performance of models also changes between the zero-shot and few-shot settings for the two tasks.For example, Llama-3-8B-Instruct is the lowest performing model with a zero-shot prompt but outperforms the Flan-T5-xxl and Mistral-7B-Instruct-v0.2 models when using few-shot prompting.</p>
<p>An interesting feature of both of these classification tasks, and potentially why there are significant gains from few-shot prompting, is that they require the LLM to make nuanced distinctions between complex labels that are defined by the user within the prompt.This type of task is particularly relevant for public health given the regular use of specific definitions and protocols that often change over time.</p>
<p>GPT-4 Comparison</p>
<p>Finally, to understand how current open-weight models compare on extraction and classification tasks to one of the highest performing private models, we evaluate GPT-4 on 12 of the 17 tasks.</p>
<p>We find GPT-4 performs well across all tasks and is the highest scoring model on 6 of the 12. GPT-4 particularly outperforms open-weight models on Health Causal Claims Classification and MMLU Genetics.However, looking across all tasks, GPT-4 performs comparably to the most recent Llama-3-70B-Instruct open-weight model, which scores equally or outperforms GPT-4 on the remaining 6 tasks.</p>
<p>Discussion</p>
<p>We see research developing automated evaluations of LLMs on representative public health free text, tasks, and knowledge as crucial for future successful deployment in real world use cases.This work introduces a set of these evaluations for Natural Language Understanding tasks in order to provide an initial assessment of both LLMs' applicability in general and to compare the performance of individual LLMs.</p>
<p>Overall, we find LLMs of all sizes perform strongly on the simpler public health classification tasks across all three areas (burden, risk factors, and interventions) and types of free text (academic, news, social media, and questionnaires).This is a promising sign that LLMs may already be useful tools for processing some public health text to support with real world tasks.It also demonstrates that LLMs have considerable domain-specific information about a range of public health topics, without which they could not achieve these results.</p>
<p>This promising performance suggests that public health professionals and LLM specialists should explore the potential benefits these models can offer in controlled deployments on simpler tasks.We see significant opportunities for LLMs to systematically structure free text, converting vital public health information embedded in text into novel structured datasets.Additionally, in situations where the sheer volume of data renders manual review impractical, LLMs could provide a scalable solution, especially during events like pandemics where data volumes grow exponentially along with case counts.Furthermore, LLMs could be employed to enhance the quality assurance of existing manual annotation processes, as a particularly low-risk way to incorporate this technology.</p>
<p>However, for a number of public health tasks where very specific knowledge (e.g MMLU Virology or Contact Type Classification) is required, or where it requires extracting structured data from long pieces of free text (e.g BioDex Drugs Extraction) LLMs perform poorly at all model sizes in the zero-shot setting.Therefore, continued model improvement and assessment may be needed before LLMs can reliably support human experts on advanced public health text processing tasks.</p>
<p>These evaluations also highlight a number of specific considerations for LLMs within public health:</p>
<p>Gains from advanced prompting.For some of the hardest tasks in the evaluation, such as Contact Type Classification and Health Causal Claims Classification, we find few-shot prompting significantly improves performance.The literature [62,50,63] also suggests potential further gains from Chain-of-Thought (CoT) prompting techniques.Therefore, more complex LLM pipelines may improve the reliability of results even on these more challenging tasks.</p>
<p>Weaker long form extraction performance.The weakest performance across LLMs is observed on long document extraction tasks, such as BioDex Drugs Extraction.A particular challenge of these tasks is the LLM identifying the correct span of text to extract along with applying the correct definition of the target.We find this often results in the LLM extracting too many spans or the LLM extracting the incorrect sized span for the ground-truth label.</p>
<p>Variable benchmark applicability.We find some public LLM benchmarks, which use data that often was not explicitly designed for public health evaluation, have some limitations.</p>
<p>A particular common issue is the lack of a well defined annotation protocol (often because the data was annotated for a different purpose).This means a prompt may lead an LLM to generate labels following a different "definition" of the labels or tasks than those used when the data was originally annotated.This may lead to LLMs that are potentially capable of performing a task generating incorrect labels.While reviewing outputs of LLMs we also noted some potentially anomalous ground truth labels in some datasets, most notably MMLU Virology.</p>
<p>Output fragility.Anecdotally, we observe weak performance is often due in part to either the LLM failing to generate the requested output format (e.g providing an explanation instead of simply "yes" or "no") or output structure (e.g outputting an invalid JSON).For example, Flan-5-xxl performs poorly on News Headline Classification largely due to it not being able to generate consistently well-formatted JSON outputs, rather than incorrect classifications.These issues can often be solved via more advanced prompting or post processing [28] and so our evaluations likely underestimate what could be achieved with bespoke pipelines.</p>
<p>Best open-weight models are increasingly comparable to private models.We find the latest Llama-3-70b model performs comparably to GPT-4 on the 12 tasks assessed.This is one initial indication that the latest open-weight models are becoming competitive with private models for these types of classification and extraction tasks in public health.</p>
<p>Task specific fine-tuning approaches.Whilst this paper focuses on evaluating LLM in-context learning capabilities, it is important to note that literature finds that for many applications specific fine-tuned models often perform competitively [29,50,64].</p>
<p>Benefits of domain specific annotation protocols.Methodologically, a crucial part of successful LLM NLU evaluations is having rigorous and consistently applied annotations, particularly in complex and subjective areas such as health.Epidemiologists have significant expertise in dealing with these issues within public health.We have found drawing on this expertise to develop detailed definitions and protocols for our evaluation datasets to be valuable.We find this approach to codifying information has been helpful both to align experts providing manual annotation, and also often incorporating these definitions into the LLM prompts leads to outputs that more closely match the prompter's intention.Descriptions of our annotation approaches can be found in Sec.7 and we will share more details of the protocols developed in these projects in use-case specific public health papers.</p>
<p>Conclusion</p>
<p>Whilst the general capability of LLMs has grown rapidly [3,4,5], assessing their performance on public health specific knowledge, tasks, and free text is an important prerequisite for successful real-world applications.In this work we take a first step towards evaluating and understanding the applicability of LLMs to public health via a broad range of automated LLM evaluations on representative tasks and anonymised or synthetic free-text.</p>
<p>Our initial results suggest that LLMs may already be useful tools to support public health experts extract information from a wide variety of free text sources.This ability in turn can potentially support and scale public health surveillance, intervention, and research activities.</p>
<p>We note that while some LLMs can process public health related text with a relatively high degree of accuracy, there is variable performance between models and across tasks.Appropriate validation is essential for any task, because even highly capable models can generate labels or extract data that do not match the prompt author's intentions.Some real-world applications are also likely to need assessments of context specific risks for a given LLM, task, and dataset combination, as well as approaches to testing known limitations such as output fragility or bias.</p>
<p>Future research is needed particularly to understand LLMs' applicability to more complex long form public health generation tasks, as well as evaluate the performance of fine-tuned domain or task specific LLMs.We also aim to continue extending public health automated evaluations for novel types of tasks and data, as well as new LLMs.</p>
<p>Ethical Considerations</p>
<p>To avoid potential detrimental impacts of text processing on individuals, we employ only public, synthetic, or anonymous data, with some datasets spanning multiple categories.We also prioritise research on less sensitive data sources and lower risk tasks, especially in comparison to others in healthcare settings.</p>
<p>It is important to mitigate potential risks within public health text processing in order to avoid errors causing harm to individuals or communities.In this paper we have provided initial results to assist researchers in understanding potential LLM performance on public health tasks.This assessment is important, because it helps set out the potential opportunities to use LLMs to generate public health insight and support public health action; however, future application should be done with sufficient mitigation measures in place.</p>
<p>In terms of mitigation, for real-world deployments it is imperative that any LLM-based software be evaluated within its specific context.The results reported herein are not sufficient to endorse the deployment of software for these public health tasks, without in-context assessments.For further discussion of these issues and other wider evaluation that is important to consider for real-world deployments see Sec. 6.1.3.</p>
<p>Appendix: Literature on Domain Specific Evaluations</p>
<p>Evaluation of LLMs is a broad and rapidly growing field of research [15] ranging from very general assessments of capabilities or intelligence to very task specific performance results (Fig. 4).</p>
<p>However, while researchers have developed evaluations for related fields, there is limited literature assessing open-weight LLMs in a standardised way across a variety of public health specific tasks and data.</p>
<p>Figure 4: LLM Evaluation Spectrum.In this paper we focus on a combination of domain and task specific LLM evaluations within public health in order to inform our understanding of where LLMs may be successfully deployed within the field.The area marked by the (*) -denotes how our evaluations compare to others.</p>
<p>Approaches to Domain Specific LLM Evaluations</p>
<p>Domain specific evaluations of LLMs can take a number of forms depending on several factors: (1) level of generality, (2) types of task, and (3) outcome of interest.</p>
<p>Level of Generality</p>
<p>Domain specific evaluations often lie on a spectrum, from assessing the potential applicability of an LLM for a domain in general (e.g Medicine [5]) to assessing an LLM on a single specific task within a domain (e.g diagnosing neuro-ophthalmic diseases [65]).</p>
<p>General domain evaluations often draw on existing human assessments and exams.For example, in Medicine using questions from (or in the style of) the US Medical Licensing Examination (USMLE) [66,67,68,69], in Law the US Bar Exam [70,71], or using human exams for given subjects [4,72,73,74].However, bespoke domain evaluations for LLMs are increasingly being developed (often including aspects of the human assessments), such as LawBench (Legal) [13], MultiMedQA (Medical) [68], FinBen (Financial) [12], the Financial Language Understanding Evaluation (FLUE) benchmarks (Financial) [75], and ChemLLMBench (Chemistry) [76].</p>
<p>In contrast, the more specific domain evaluations in the literature, which target either sub-fields or specific tasks, have often involved collecting and manually annotating new evaluation datasets (or modified / filtered existing domain specific data).In Medicine this has commonly involved collecting data and evaluating LLMs for specific sub-fields, including, Neuro-ophthalmology (diagnosis) [65], Bariatric surgery (QA) [17], Osteoarthritis (case management) [77], Dementia (diagnosis) [78], and Genetics (QA) [79].In Law, specific evaluations have generally been carried out for given tasks or abilities, including, generating explanations for legal terms [80], statutory reasoning [81], and legal entailment [82].</p>
<p>The chosen level of generality for an evaluation is primarily determined by the overall aim.More general domain evaluations are most useful for understanding broad capabilities, developing or fine-tuning new LLMs, and prioritising existing LLMs for further assessment.More task specific domain evaluations are generally crucial when considering deploying LLMs for real world use cases or applying LLMs to unseen datasets and tasks (such as private organisational data).</p>
<p>Types of Task</p>
<p>In the literature, the approach adopted for a domain specific LLM evaluation is also significantly influenced by the type of task (or tasks) involved.As discussed by Chang et al. [15], a key factor is whether the task primarily involves classification or inference (Natural Language Understanding -NLU) or whether it involves generating free text (Natural Language Generation -NLG).</p>
<p>Evaluation of LLMs for tasks focusing on classification or the extraction of structured data (NLU) utilises similar metrics and approaches to traditional data science.This involves collecting a representative dataset of free text, annotating with ground truth labels, and then evaluating the performance of the LLM using metrics such as accuracy, recall, precision, and F1 scores [29,28,30].</p>
<p>In contrast, LLM tasks that generate unstructured free text (NLG), such as summarisation, have been shown to be hard to evaluate with traditional automated methods [83,84,15,85], such as ROUGE [86].This has led many NLG task evaluations to adopt human evaluation approaches, particularly in domain specific or risk-averse fields, such as Medicine [68,5,87,85].This usually involves human experts reading the LLM outputs and scoring them on absolute (rate out of 10) or relative (which response is better) metrics for a given criteria.</p>
<p>However, human evaluation of NLG tasks brings a number of challenges, including: (1) cost, as expert annotation is time-consuming and experts' time is valuable, (2) subjectivity, as there may be inconsistency in annotations between experts, and (3) scalability, as human evaluations must be repeated manually for every model (whereas annotated data can be used to evaluate all models).These issues, combined with the increasing capabilities of the state of the art LLMs, have led researchers to investigate replacing manual human review of generation tasks, with automated review by different, usually more capable LLMs [16,88,89].Whilst these approaches have seen some success [16,89] it remains unclear which domains or tasks LLMs are "qualified" to evaluate.This is a particular issue for risk-averse sectors that involve specialised knowledge, such as public health.</p>
<p>Outcome of Interest</p>
<p>The appropriate methodology for domain specific evaluations is also strongly influenced by the exact outcome researchers are interested in assessing.For NLU tasks, the potential outcomes of interest can range from those focused on test set performance (e.g accuracy), which we look at in this paper, to broader considerations such as bias and robustness.</p>
<p>For automated evaluations the literature largely uses performance metrics, with a particular focus on F1 scores [62,64,30,50,28].For specific tasks these high level metrics are often supplemented with further class specific analysis [30].</p>
<p>Some of the other key outcomes of interest for fields such as public health are assessments of software for bias and fairness, where this bias could be caused by bias in the LLM or other places in the input data or software.General assessments of LLM bias are often carried out during LLM training and benchmarking [44,90,91,92,93].For specific NLU tasks, further bias evaluations often involve analysing label specific results to identify whether the predictions or errors deviate significantly based on individual characteristics or group characteristics [94,15,95].</p>
<p>Evaluating bias and fairness is crucial for understanding the potential risks when deploying LLMs for real-world use cases, particularly those that involve text about certain communities.In a UK context, protected characteristics are enshrined in law as characteristics on which people cannot be discriminated against [96].Specifically for public health, evaluations may also need to consider wider health equity frameworks, such as CORE20PLUS [97].</p>
<p>Evaluations of bias and fairness can generally be most effectively carried out using the contents of the free-text (focusing on the specific protected characteristics that are relevant), task (assessing the risk of bias in the output), and deployment process (evaluating the software as a whole and how a human expert in the loop could mitigate or perpetuate risks) that would be used in production.</p>
<p>Summary</p>
<p>Overall, our aim with this work is to provide initial assessments of LLMs within public health.To cover the broadest range of models and areas, we focus on automated domain specific evaluations across a range of relevant NLU tasks, see Fig. 5.</p>
<p>Existing Evaluations of LLMs in Public Health and Related Fields</p>
<p>The literature contains a number of existing domain specific NLU LLM evaluations that are relevant for public health, some of which we adopt relevant sub-sets of within our evaluations.</p>
<p>Disease and Health Issue Classification and Extraction</p>
<p>Classifying and extracting potential diseases and health issues from free text is an important task within public health analysis and has significant overlap with the Medical domain.</p>
<p>Zhang et al. [62] evaluate ChatGPT (GPT-3.5)and GPT-4 at the single label task of disease classification from electronic health records (EHRs).GPT-4 with zero-shot prompting was found to achieve between 0.75 and 0.96 F1 score across the 5 diseases and outperformed GPT-3.5 on 4 of the 5 diseases.Sensitivity analysis suggested few-shot with Chain of Thought (CoT) prompting improved GPT-4's performance.</p>
<p>Similarly, Guo et al. [64] evaluate GPT-4 and GPT-3.5 (along with fine-tuned classifiers) across six single label health information classification tasks using manually annotated Twitter (now called X) data and data from the Social Media Mining for Health Applications (SMM4H) datasets.As found in other work, GPT-4 outperforms GPT-3.5 using zero-shot prompting but with a high variance across tasks, with F1 scores ranging from 0.35 and 0.8.</p>
<p>Finally, due to the importance of novel pathogens within public health, the literature dealing with rare diseases (where information in the LLM pre-training data is likely to be more limited) is particularly interesting for LLM evaluation.</p>
<p>Chen et al. [10] introduce RareBench to evaluate LLMs' (including GPT-4, GPT-3.5, Gemini, Llama-2-7b and Mistral-1-7b) ability to perform rare disease phenotype extraction, screening, and diagnosis.GPT-4 is found to have the highest performance across all tasks using zeroshot prompting while the smaller 7b parameter open-weight Llama-2 and Mistral-1 models have the lowest scores.</p>
<p>Shyr et al. [30] evaluate ChatGPT for extracting rare diseases, signs and symptoms using the the RareDis corpus [98].ChatGPT with zero-shot prompting and relaxed matching achieves overall F1 scores of 0.41 to 0.47, but with significant variation across types from 0.15 (symptoms) to 0.76 (rare diseases).Few-shot prompting was generally found to lead to better performance.The complexity of the labels meant that evaluation approaches requiring exact matching of outputs and labels had significantly worse results.</p>
<p>Drug and Intervention Extraction</p>
<p>Extracting relevant Pharmaceutical interventions from free text is another common task in public health free text processing.</p>
<p>D'Oosterlinck et al. [29] propose BioDEX, a dataset of PubMed articles about Adverse Drug Events (ADE) and their corresponding ground truth ADE extractions.BioDEX is used to evaluate GPT-3.5 and GPT-4 LLMs with few-shot prompting, this achieves an overall F1 score of approximately 0.5 at extracting the 4 core ADE report attributes (patient sex, serious event, reactions and drugs), currently well below expert level.Due to the length of the papers the few-shot prompts are constructed using abstracts, and the input paper is truncated to fit the context window, potentially limiting the information available to the model.</p>
<p>Agrawal et al. [28] add new annotations to the Clinical Acronym Sense Inventory (CASI) dataset [99] to evaluate GPT-3 for the task of extracting medical interventions from clinical notes and medical acronym disambiguation.Combining GPT-3 with a resolver to form structured outputs along with zero-shot and one-shot prompting is shown to have fairly strong results across tasks, generally achieving F1 scores &gt;0.6.However, as in other studies, the authors note the difficulty of generating exact matches for complex ground truth labels.</p>
<p>Similarly, Bisercic et al. [63] evaluate InstructGPT for the task of generating structured JSON data from medical reports within their TEMED-LLM approach.Using medical reports on patient treatments, psychologist notes, strokes, hepatitis, and heart disease, InstructGPT with one-shot and CoT prompting combined with additional output validation (e.g correcting JSON formatting errors) achieves over 90% accuracy for 4 / 5 tasks and outperforms one-shot prompting.</p>
<p>Finally, identifying and understanding relevant medical and health recommendations is a key capability for interpreting public health guidance.Related to this, Chen et al. [50] evaluate GPT-4 and GPT-3.5 on single label Biomedical classification tasks using the HealthAdvice [52] and CausalRelation [51] datasets.The HealthAdvice dataset is used to evaluate the LLMs' ability to identify whether a given sentence contains no, weak, or strong health advice.The CausalRelation reasoning evaluation tests the LLMs' ability to identify casual and correlation claims in PubMed conclusion sentences.Testing a range of prompts from zero-shot to few-shot with CoT, GPT-4 is shown to slightly outperform GPT-3.5 with 0.65-0.77macro-F1 scores across the tasks using the highest performing prompt (few-shot with CoT).</p>
<p>Relevant Sub-sets of General Evaluations</p>
<p>In addition to these domain specific evaluations in the literature, it is also important to note that there are subsets of general LLM evaluations that are relevant for public health (although detailed breakdowns of subset results often are not reported).</p>
<p>The Massive Multitask Language Understanding (MMLU) benchmark [34] has subsets covering general medical and scientific knowledge.More importantly, it also contains subsets relating to virology, genetics, and nutrition, that are particularly relevant for some public health sub-fields.</p>
<p>Also relevant for public health is the PubMedQA [53] Biomedical question answering benchmark now included in the broader MultiMedQA [68] medical benchmark.The labelled subset of PubMedQA consists of 1000 PubMed articles that have questions in the title, combined with the manually annotated answer (yes, no, maybe) to the question based on a review of the abstract (with and without concluding sections).This is particularly relevant for assessing the ability of LLMs to infer conclusions to scientific questions based on a summary of the evidence.</p>
<p>More recently, the Massive Multi-discipline Multimodal Understanding and Reasoning (MMMU) [100] benchmark has a specific subset for public health.However, due to the multi-modal nature of the benchmark it currently sits outside the scope of this paper.</p>
<p>Appendix: Annotations</p>
<p>Seven of the datasets we use to evaluate LLM performance are datasets annotated in collaboration with public health specialists in the relevant sub-fields of public health.Given the comprehensive nature of this evaluation, the detail of the exact approach to manual annotation is left for future public health specific papers.However, this appendix provides a summary of the level of rigour used to ensure high quality annotations that would be reproducible by other human experts (in order to give LLMs a chance at producing similar results).User-generated online restaurant reviews contain highly variable information, making it hard to apply a protocol.However, the considerable degree of consistency between human reviewers gives more confidence.</p>
<p>Gastrointestinal Illness Symptom Extraction</p>
<p>Bespoke for LLM evaluation A forthcoming publication will set out the methodology in more detail.A detailed protocol was honed by several public health specialists.The protocol was then applied by a reviewer involved in the GI Classification task, uncertain classifications were shared with a second reviewer to check.</p>
<p>User-generated online restaurant reviews contain highly variable information, making it hard to apply a protocol.However, the considerable degree of consistency between human reviewers gives more confidence.</p>
<p>ICD-10 Description Classification</p>
<p>Preexisting organisational A forthcoming publication will set out the methodology in more detail.The assignment of annotations is based on: public health experts in the a given disease area, review of the literature, and other pre-existing labels by public health experts.Some of the labels in this dataset are assigned trivially, where a description clearly relates to an infection.Other labels, where a disease is labelled infection related, because the disease has an infectious aetiology can be considerably harder.Considerable validation with subject matter experts and triangulation between approaches leads to considerably higher confidence.</p>
<p>News Headline Classification</p>
<p>Bespoke for LLM evaluation</p>
<p>A detailed protocol was honed by several public health specialists.A single reviewer then assigned labels based on the protocol, spot checking was done by another reviewer who confirmed a high degree of agreement.</p>
<p>The headlines in this work are relatively easy to annotate.However, using a single reviewer with spot checking is a less rigorous process than the double blind review used for some of the other datasets.Table 4: Manual Annotation.Summary of the approach to manual annotation for each task.</p>
<p>Dataset</p>
<p>Origin</p>
<p>Annotation approach Annotation confidence</p>
<p>Contact Type Classification</p>
<p>Bespoke for LLM evaluation A detailed protocol was honed by several public health specialists.The data was generated by GPT-4, which was prompted to generate text and assign a label on generation.Two independent reviewers then also manually annotated their label.Over 90% agreement was confirmed and any discrepancies reconciled.</p>
<p>The independent annotation by two human experts and GPT-4, where discrepancies were resolved, leads to high confidence that the protocol (also provided to the LLM in the prompt) was followed.</p>
<p>Country Disambiguation</p>
<p>Preexisting organisational</p>
<p>Country labels were manually assigned to ambiguous free text (that could not be matched to a country through key word searches) by single reviewers, with spot checking by a second reviewer.The approach to assigning labels also involved reviewers using search engines to try to ascertain the country of sub-country geographical units mentioned in the free text.</p>
<p>Some of the labels are inherently uncertain (for instance, the text only refers to a location that exists in several countries, or a spelling mistake like Nigeri means Nigeria and Niger are both possible countries).However, spot checking reveals a low error rate, with disagreements in labels largely relating to inherent uncertainty.</p>
<p>Food Extraction Bespoke for LLM evaluation</p>
<p>A forthcoming publication will set out the methodology in more detail.A detailed protocol was honed by several public health specialists.The protocol was then applied by a reviewer involved in the GI Classification task, uncertain classifications were shared with a second reviewer to check.</p>
<p>User-generated online restaurant reviews contain highly variable information, making it hard to apply a protocol.However, the considerable degree of consistency between human reviewers gives more confidence.</p>
<p>Guidance Topic Classification</p>
<p>Preexisting organisational</p>
<p>The Topic labels were assigned based on which team in UKHSA wrote the piece of guidance.Pieces of guidance were included only if teams have a clear remit in a specific sub-field of public health (e.g.Sexual Transmitted Infections) rather than an organisational remit that would be less relevant to LLMs (e.g.Health Protection Operations).</p>
<p>Teams in UKHSA only publish guidance within their team remit, so there is a very high degree of confidence in the labels assigned.Some pieces of guidance are more uncertain where they related to topics that fall at the intersection of two teams' remits.</p>
<p>Guidance Recommendation Classification</p>
<p>Bespoke for LLM evaluation A detailed protocol was honed by several public health specialists.The protocol was applied by at least two reviewers for a sample of 100 examples.&gt;90% agreement was confirmed before a single reviewer was assigned the remaining labels with the option to send uncertain labels to a second reviewer.</p>
<p>Recommendations are an abstract concept, which makes them more challenging to define.However, the detailed protocol and considerable inter-reviewer agreement leads to more confidence that the annotation is correct.</p>
<p>Table 5: Manual Annotation continued.Summary of the approach to manual annotation for each task 8 Appendix: Example Prompts</p>
<p>Figure 1 :
1
Figure 1: Public Health Large Language Model (LLM) Evaluation Areas [Number of Evaluations] and Task Evaluation Micro-F1 Scores by Model.(Left) We divide public health free text processing into three sub-domains: (1) burden, such as reports of disease symptoms, cases, morbidity, or mortality; (2) risk factors, such as environmental, behavioural, or biological contributors; (3) interventions, pharmaceutical and non-pharmaceutical.(Right) Evaluation results (micro-F1 scores) for five open-weight LLMs across the 17 tasks using zero-shot prompting.</p>
<p>Figure 2 :
2
Figure 2: Evaluation Tasks by Public Health Area.A summary of different tasks which we use to evaluate the LLMs, grouped by public health area.See 2.1.1,2.1.2,and 2.1.3for full descriptions.</p>
<ul>
<li>3 -
3
70B-Instruct is the highest scoring open-weight model (or equal highest) across 15 of the 17 tasks according to micro-F1.The only tasks where it does not have the highest micro-F1 are News Headline Classification and PubMedQA where it performs marginally less well than the highest performing open-weight model.</li>
</ul>
<p>Figure 3 :
3
Figure 3: Comparison of zero-shot and few-shot prompting on challenging tasks.We compare the baseline zero shot prompt to a 10-shot prompt for Contact Classification (Left) and a 7-shot prompt for Health Causal Claims Classification (Right).</p>
<p>Figure 5 :
5
Figure 5: Overview of Evaluations.We focus on automated evaluations of classification and extraction tasks.</p>
<p>Figure 6 :
6
Figure 6: Example Few Shot Prompt for Health Causal Claims Classification.This is an example of the 7 shot prompt used in the Health Causal Claims Classification task structured with the Stable-Beluga-2 prompt template.Few-shot examples taken from Yu et al. [51].</p>
<p>Figure 7 :
7
Figure 7: Example Zero Shot Prompt for NCBI Disease Extraction.This is an example of the zero shot prompt used in the NCBI Disease Extraction task structured with the Stable-Beluga-2 prompt template.</p>
<p>Table 1 :
1
Overview of Public Health Evaluation Tasks.In order to capture a broad range of free text, the 17 tasks we use draw on 13 distinct datasets from internal and external sources."Text length" refers to the average number of characters in the free text (excluding the prompt template and question)."Public" refers to whether the annotations are available online.
Task NameDatasetTask TypeTest Set SizeText TypeText LenExample LabelsPublicRows Labels(Avg char)NCBI Disease ExtractionNCBI Disease Cor-Extraction475907Academic1276["non-hereditaryYespus(sporadic) breastcancer", "br...Gastro-intestinal Illness Classifi-Yelp Open Dataset Classification 24562456Social Media635"Yes"NocationGastro-intestinal Illness Symp-Yelp Open DatasetExtraction400461Social Media464["nausea"]Notom ExtractionICD-10 Description Classifica-ICD-10Classification 22262226Academic36"Yes"NotionNews Headline ClassificationGDELTClassification353353News Articles70"Yes"NoMMLU VirologyMMLUClassification152152Multiple Choice-"C"YesContact Type ClassificationSynthetic Question-Classification254254Questionnaire86"Rule 2"NonairesCountry DisambiguationGPRegistrationClassification 80008000Questionnaire14"Brazil"NoFormsFood ExtractionYelp Open DatasetExtraction400602Social Media464["fish", "fruit"]NoMMLU GeneticsMMLUClassification9393Multiple Choice-"C"YesMMLU NutritionMMLUClassification276276Multiple Choice-"C"YesGuidance Topic ClassificationUKHSAClassification265265Guidance456"4"NoGuidance Recommendation Clas-UKHSAClassification392392Guidance794"Yes"NosificationHealth Advice ClassificationHealthAdviceClassification 86768676Academic144"2"YesHealth Causal Claims Classifica-CausalRelationClassification 24482448Academic125"1"YestionPubMedQAPubMedQAClassification800800Academic1330"Yes"YesBioDex Drugs ExtractionBioDexExtraction12474429Academicc.6000["flucloxacillin",Yes"midazolam"]</p>
<p>Pharmaceutical interventions predominantly use medical technologies to prevent, diagnose or treat disease.Vaccination is one particularly common public health pharmaceutical intervention.Public health non-pharmaceutical interventions generally aim to reduce behavioural risk factors and so mitigate various forms of disease.These include nutrition advice or infection control through hand hygiene.</p>
<p>Table 2 :
2
3, this means our zero-shot prompt2evaluations should provide a reasonable lower bound for each model's performance, with potentially significant gains possible from more advanced pipelines.Overview of Large Language Models Evaluated.We focus on evaluating opensource/weight LLMs that we host internally to ensure comparability of results, as well as for data protection.We run all models in FP16 precision where feasible.MoE = Mixture of Experts (MoE) architecture.We include the latest GPT-4 model for comparison on a subset of tasks.
Model NameBase ModelPrecisionModel SizeAuthorMoEHostParams (bn) Context (toks)Flan-T5-xxl [54]Flan-T5-xxlFP16112048GoogleNoInternal APIMistral-7B-Instruct-v0.2 [55]Mistral-7BFP16732768MistralNoInternal APIStable-Beluga-2 [56]Llama-2-70B [44]FP16704096Stability-AI + MetaNoInternal APILlama-3-70B-Instruct [57]Llama-3-70BFP16708192MetaNoInternal APILlama-3-8B-Instruct [57]Llama-3-8BFP1688192MetaNoInternal APIgpt-4-turbo-2024-04-09GPT-4--128000OpenAI-OpenAI API</p>
<p>Table 3 :
3
Zero-shot Results.Bold indicates the highest open-weight model micro-F1 score on a given task.Underlined indicates the highest micro-F1 score across all models.Mean Task Rank is calculated as the rank of the open-weight model on each task averaged over all tasks.Models ordered by number of parameters.
Mistral-7BLlama-3-8BFlan-T5-xxlStable-Beluga-2 Llama-3-70BGPT-4macro micro macro micro macro micro macromicro macro micro macro microTask NameNCBI Disease Extraction0.460.630.620.80.480.720.620.820.660.840.660.83GI Classification0.850.860.820.820.880.890.890.90.920.920.820.84MMLU Virology0.280.410.540.550.430.430.530.530.570.570.590.59GI Symptom Extraction0.550.580.820.870.710.760.870.90.870.910.860.89ICD-10 Description Classification0.940.940.930.930.920.920.940.940.950.95--News Headline Classification0.900.900.950.950.300.440.950.950.920.93--Guidance Topic Classification0.650.730.760.870.920.940.900.910.930.94--BioDex Drugs Extraction0.050.250.100.320.090.330.080.30.110.330.10.33Health Advice0.410.750.460.680.640.770.470.660.450.780.640.83Guidance Recommendations0.510.750.820.830.830.830.820.820.870.870.850.85Health Causal Claims0.160.380.180.260.400.50.280.350.340.510.480.64PubMedQA0.010.330.570.740.530.760.430.730.460.750.630.75Contact Classification0.190.240.240.230.290.280.400.460.420.48--Country Disambiguation0.790.820.730.860.780.830.830.860.760.92--Food Extraction0.830.870.820.870.770.830.830.880.830.880.830.89MMLU Genetics0.330.590.680.70.580.590.660.680.920.910.960.96MMLU Nutrition0.480.600.680.680.600.60.710.710.850.850.880.88Mean Task Rank4.294.183.123.243.243.292.592.881.651.18--
4True Positives divided by the total number of True Positives and False Positives5True Positives divided by the total number of True Positives and False Negatives</p>
<p>For background, Zhao et al.[1] and Kaddour et al.[2] are recent surveys on LLMs and their applications.
The number of "shots" refers to how many example question-answer pairs are provided in the prompt, in addition to the question being asked.
We use gpt-4-turbo-2024-04-09, the latest GPT-4 model available via the API at the time of evaluation.
AcknowledgementsThis work was enabled by UKHSA HPC Cloud &amp; DevOps Technology colleagues developing and maintaining internal HPC resources.We would also like to thank our UKHSA colleagues in Clinical and Public Health for their support and expertise in developing this work.
Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Yifan Dong, Chen Du, Yushuo Yang, Zhipeng Chen, Jinhao Chen, Ruiyang Jiang, Yifan Ren, Xinyu Li, Zikang Tang, Peiyu Liu, Jian-Yun Liu, Ji-Rong Nie, Wen, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, Robert Mchardy, arXiv:2307.10169Challenges and applications of large language models. 2023arXiv preprint</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>arXiv:2303.08774OpenAI. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Towards expert-level medical question answering with large language models. Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, arXiv:2305.096172023arXiv preprint</p>
<p>Tyna Eloundou, Sam Manning, Pamela Mishkin, Daniel Rock, arXiv:2303.10130Gpts are gpts: An early look at the labor market impact potential of large language models. 2023arXiv preprint</p>
<p>Ukhsa advisory board -artificial intelligence discovery exercise. 09/05/20242023UKHSA Advisory Board: Artificial Intelligence Discovery Exercise</p>
<p>15/05/2024CDC. Artificial Intelligence and Machine Learning | Technologies | CDC -cdc.gov, 2023. Artificial Intelligence and Machine Learning: Applying Advanced Tools for Public Health. </p>
<p>Artificial intelligence in public health: Challenges and opportunities for public health made possible by advances in natural language processing. Oliver Baclic, Matthew Tunis, Kelsey Young, Coraline Doan, Howard Swerdfeger, Justin Schonfeld, Canada Communicable Disease Report. 4661612020</p>
<p>Xuanzhong Chen, Xiaohao Mao, Qihan Guo, Lun Wang, Shuyang Zhang, Ting Chen, Rarebench, arXiv:2402.06341Can llms serve as rare diseases specialists?. 2024arXiv preprint</p>
<p>Benchmarking large language models on answering and explaining challenging medical questions. Hanjie Chen, Zhouxiang Fang, Yash Singla, Mark Dredze, arXiv:2402.180602024arXiv preprint</p>
<p>Qianqian Xie, Weiguang Han, Zhengyu Chen, Ruoyu Xiang, Xiao Zhang, Yueru He, Mengxi Xiao, Dong Li, Yongfu Dai, Duanyu Feng, Yijing Xu, Haoqiang Kang, Ziyan Kuang, Chenhan Yuan, Kailai Yang, Zheheng Luo, Tianlin Zhang, Zhiwei Liu, Guojun Xiong, Zhiyang Deng, Yuechen Jiang, Zhiyuan Yao, Haohang Li, Yangyang Yu, Gang Hu, Jiajia Huang, Xiao-Yang Liu, Alejandro Lopez-Lira, Benyou Wang, Yanzhao Lai, Hao Wang, Min Peng, Sophia Ananiadou, Jimin Huang, arXiv:2402.12659The finben: An holistic financial benchmark for large language models. 2024arXiv preprint</p>
<p>Zhiwei Fei, Xiaoyu Shen, Dawei Zhu, Fengzhe Zhou, Zhuo Han, Songyang Zhang, Kai Chen, Zongwen Shen, Jidong Ge, arXiv:2309.16289Lawbench: Benchmarking legal knowledge of large language models. 2023arXiv preprint</p>
<p>HELM: A reproducible and transparent framework for evaluating foundation models. 2024Stanford UniversityHolistic evaluation of langauge models results page</p>
<p>Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S Yu, Qiang Yang, Xing Xie, arXiv:2307.03109A survey on evaluation of large language models. 2023arXiv preprint</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P Xing, Hao Zhang, Joseph E Gonzalez, arXiv:2306.05685and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. 2023arXiv preprint</p>
<p>Assessing the accuracy of responses by the language model chatgpt to questions regarding bariatric surgery. Yee Jamil S Samaan, Nithya Hui Yeo, Lauren Rajeev, Stuart Hawley, Wee Abel, Han Ng, Nitin Srinivasan, Justin Park, Miguel Burch, Rabindra Watson, Obesity surgery. 3362023</p>
<p>The untilled fields of public health. C-Ea Winslow, Science. 511306. 1920</p>
<p>Policies and strategies to promote social equity. Health Institute of Future Studies. Göran Dahlgren, Margaret Whitehead, 1991Stockholm</p>
<p>The concepts and principles of equity and health. Margaret Whitehead, International journal of health services. 2231992</p>
<p>Peter G David A Ross, Richard H Smith, Morrow, Types of Intervention and Their Development. Oxford University Press2015</p>
<p>Policy implications of big data in the health sector. Effy Vayena, Joan Dzenowagis, John S Brownstein, Aziz Sheikh, Bulletin of the World Health Organization. 961662018</p>
<p>Global burden of 369 diseases and injuries in 204 countries and territories, 1990-2019: a systematic analysis for the global burden of disease study. Theo Vos, Stephen S Lim, Cristiana Abbafati, Kaja M Abbas, Mohammad Abbasi, Mitra Abbasifard, Mohsen Abbasi-Kangevari, Hedayat Abbastabar, Foad Abd-Allah, Ahmed Abdelalim, The lancet. 3962019. 10258. 2020</p>
<p>Public health surveillance: a tool for targeting and monitoring interventions. Peter Nsubuga, Mark E White, Stephen B Thacker, Mark A Anderson, Stephen B Blount, Claire V Broome, Tom M Chiller, Victoria Espitia, Rubina Imtiaz, Dan Sosin, 2011</p>
<p>Action plan for the prevention and control of noncommunicable diseases in the who european region. 2016-2025, 2016WHO</p>
<p>Political declaration of the third high-level meeting of the general assembly on the prevention and control of non-communicable diseases. Assembly Un General, Resolution adopted by the General Assembly October. 2018</p>
<p>Ncbi disease corpus: a resource for disease name recognition and concept normalization. Rezarta Islamaj Dogan, Robert Leaman, Zhiyong Lu, Journal of biomedical informatics. 472014</p>
<p>Large language models are few-shot clinical information extractors. Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim, David Sontag, arXiv:2205.126892022arXiv preprint</p>
<p>Biodex: Large-scale biomedical adverse drug event extraction for real-world pharmacovigilance. D' Karel, François Oosterlinck, Johannes Remy, Thomas Deleu, Chris Demeester, Klim Develder, Aneiss Zaporojets, Simon Ghodsi, Jack Ellershaw, Christopher Collins, Potts, arXiv:2305.133952023arXiv preprint</p>
<p>Identifying and extracting rare disease phenotypes with large language models. Cathy Shyr, Yan Hu, Paul A Harris, Hua Xu, arXiv:2306.126562023arXiv preprint</p>
<p>Yelp Open Dataset -An all-purpose dataset for learning. Yelp, 2023Yelp open dataset</p>
<p>International Statistical Classification of Diseases and Related Health Problems 10th Revision. 2019GenevaInternational Classification of Diseases Tenth Revision (ICD-10)</p>
<p>GDELT. Gdelt project. GDELT Project -Watching Our World Unfold. </p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, 2021</p>
<p>Global burden of 87 risk factors in 204 countries and territories, 1990-2019: a systematic analysis for the global burden of disease study. Christopher, Murray, Peng Aleksandr Y Aravkin, Cristiana Zheng, Kaja M Abbafati, Mohsen Abbas, Foad Abbasi-Kangevari, Ahmed Abd-Allah, Mohammad Abdelalim, Ibrahim Abdollahi, Abdollahpour, The lancet. 3962019. 10258. 2020</p>
<p>National environmental public health tracking program: bridging the information gap. Judith R Michael A Mcgeehin, Amanda Sue Qualters, Niskar, 2004Environmental Health Perspectives112</p>
<p>Global action plan for the prevention and control of noncommunicable diseases 2013-2020. 2013102</p>
<p>Is an ounce of prevention worth a pound of cure? a cross-sectional study of the impact of english public health grant on mortality and morbidity. Stephen Martin, James Lomas, Karl Claxton, BMJ open. 1010e0364112020</p>
<p>on the covid-19 pandemic in the uk. Chris Whitty, Gregor Smith, Frank Atherton, Michael Mcbride, Patrick Vallance, Jenny Harries, Stephen Powis, Jonathan Van-Tam, Nicola Steedman, Graham Ellis, Marion Bain, Lourda Geoghegan, Naresh Chada, Chris Jones, Aidan Fowler, Thomas Waite, 17/04/20242022Technical reportTechnical report on the COVID-19 pandemic in the UK</p>
<p>. Openai, 2024OpenAI developer platform</p>
<p>Mpox: contact tracing -Classification of contacts and follow-up advice for non-HCID strains of mpox. UKHSA. 2023Mpox: contact tracing</p>
<p>Prevalence, risk factors, and uptake of interventions for sexually transmitted infections in britain: findings from the national surveys of sexual attitudes and lifestyles (natsal). The Lancet. Pam Sonnenberg, Soazig Clifton, Simon Beddows, Nigel Field, Kate Soldan, Clare Tanton, Catherine H Mercer, Filomeno Coelho Da, Sarah Silva, Andrew J Alexander, Copas, 2013382</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, 2023</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, 2023Aurelien RodriguezAngela Fan, Melanie Kambadur; Robert Stojnic, Sergey Edunovand Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Application of natural language processing algorithms for extracting information from news articles in event-based surveillance. Victoria Ng, Erin E Rees, Jingcheng Niu, Abdelhamid Zaghool, Homeira Ghiasbeglou, Adrian Verster, Canada Communicable Disease Report= Releve des Maladies Transmissibles au Canada. 4662020</p>
<p>The country converter coco -a python package for converting country names between different classification schemes. Konstantin Stadler, 10.21105/joss.00332Journal of Open Source Software. 2163322017</p>
<p>The food classification and description system foodex 2 (revision 2). European Food Safety Authority (EFSA). Wiley Online Library2015Technical report</p>
<p>The international classification of health interventions: an 'epistemic hub'for use in public health. Nicola Fortune, Richard Madden, Therese Riley, Stephanie Short, Health promotion international. 3662021</p>
<p>Systematic review of empirical studies comparing the effectiveness of non-pharmaceutical interventions against covid-19. Alba Mendez-Brito, Charbel El Bcheraoui, Francisco Pozo-Martin, Journal of Infection. 8332021</p>
<p>Evaluating the chatgpt family of models for biomedical reasoning and classification. Shan Chen, Yingya Li, Sheng Lu, Hoang Van, J W L Hugo, Guergana K Aerts, Danielle S Savova, Bitterman, 10.1093/jamia/ocad256Journal of the American Medical Informatics Association. 1527-974X314January 2024</p>
<p>Detecting causal language use in science findings. Bei Yu, Yingya Li, Jun Wang, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)2019</p>
<p>Detecting health advice in medical research literature. Yingya Li, Jun Wang, Bei Yu, Proceedings of EMNLP'2021. EMNLP'20212021</p>
<p>Pubmedqa: A dataset for biomedical research question answering. Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, Xinghua Lu, 2019</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Alex Chowdhery, Marie Castro-Ros, Kevin Pellat, Dasha Robinson, Sharan Valter, Gaurav Narang, Adams Mishra, Vincent Yu, Yanping Zhao, Andrew Huang, Hongkun Dai, Slav Yu, Ed H Petrov, Jeff Chi, Jacob Dean, Adam Devlin, Denny Roberts, Quoc V Zhou, Jason Le, Wei, 2022</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Mistral 7b. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023</p>
<p>Stable beluga models. Dakota Mahan, Ryan Carlow, Louis Castricato, Nathan Cooper, Christian Laforte, 2024. StableBeluga2</p>
<p>Llama 3 model card. A I , Meta , Llama 3 Model Card. 2024</p>
<p>Hugging face model respository. 2024Hugging Face. Hugging Face</p>
<p>Huggingface's transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Xu, Mariama Drame, Quentin Lhoest, and Alexander M. Rush2020Teven Le Scao, Sylvain Gugger</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, Ion Stoica, Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. the ACM SIGOPS 29th Symposium on Operating Systems Principles2023</p>
<p>How good are low-bit quantized llama3 models? an empirical study. Wei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng, Chengtao Lv, Hong Chen, Jie Luo, Xiaojuan Qi, Xianglong Liu, Michele Magno, 2024</p>
<p>Jingqing Zhang, Kai Sun, Akshay Jagadeesh, Mahta Ghahfarokhi, Deepa Gupta, Ashok Gupta, Vibhor Gupta, Yike Guo, The potential and pitfalls of using a large language model such as chatgpt or gpt-4 as a clinical assistant. 2023</p>
<p>Interpretable medical diagnostics with structured data extraction by large language models. Aleksa Bisercic, Mladen Nikolic, Mihaela Van Der Schaar, Boris Delibasic, Pietro Lio, Andrija Petrovic, 2023</p>
<p>Evaluating large language models for health-related text classification tasks with public social media data. Yuting Guo, Anthony Ovadje, Mohammed Ali Al-Garadi, Abeed Sarker, 2024</p>
<p>Yeganeh Madadi, Mohammad Delsoz, Priscilla A Lao, Joseph W Fong, Malik Y Hollingsworth, Siamak Kahook, Yousefi, Chatgpt assisting diagnosis of neuro-ophthalmology diseases based on case reports. 2023</p>
<p>Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models. Tiffany H Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, Maria Madriaga, Rimel Aggabao, Giezel Diaz-Candido, James Maningo, Victor Tseng, 10.1371/journal.pdig.0000198PLOS Digital Health. 222023</p>
<p>Can large language models reason about medical questions?. Christoffer Egeberg Valentin Liévin, Ole Hother, Winther, arXiv:2207.081432022arXiv preprint</p>
<p>Large language models encode clinical knowledge. Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Scharli, Aakanksha Chowdhery, Philip Mansfield, Greg S Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan2022Blaise Aguera y Arcas, Dale Webster,</p>
<p>Meditron-70b: Scaling medical pretraining for large language models. Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, Alexandre Sallinen, Alireza Sakhaeirad, Vinitra Swamy, Igor Krawczuk, Deniz Bayazit, Axel Marmet, Syrielle Montariol, Mary-Anne Hartley, Martin Jaggi, and Antoine Bosselut2023</p>
<p>Michael Bommarito, I I , Daniel Martin Katz, arXiv:2212.14402Gpt takes the bar exam. 2022arXiv preprint</p>
<p>. Martin Daniel, Michael James Katz, Shang Bommarito, Pablo Gao, Arredondo, 20234389233Gpt-4 passes the bar exam. Available at SSRN</p>
<p>Trialling a large language model (chatgpt) in general practice with the applied knowledge test: Observational study demonstrating opportunities and limitations in primary care. Arun James Thirunavukarasu, Refaat Hassan, Shathar Mahmood, Rohan Sanghera, Kara Barzangi, Mohanned El Mukashfi, Sachin Shah, 10.2196/46599JMIR Medical Education. 2369-37629e46599Apr 2023</p>
<p>Gpt as knowledge worker: A zero-shot evaluation of (ai) cpa capabilities. Jillian Bommarito, Michael Bommarito, Daniel Martin Katz, Jessica Katz, arXiv:2301.044082023arXiv preprint</p>
<p>. Jonathan H Choi, Kristin E Hickman, Amy Monahan, Daniel Schwarcz, 2023Chatgpt goes to law school. Available at SSRN</p>
<p>When flue meets flang: Benchmarks and large pre-trained language model for financial domain. Raj Sanjay Shah, Kunal Chawla, Dheeraj Eidnani, Agam Shah, Wendi Du, Sudheer Chava, Natraj Raman, Charese Smiley, Jiaao Chen, Diyi Yang, 2022</p>
<p>What can large language models do in chemistry?. Taicheng Guo, Kehan Guo, Bozhao Nan, Zhenwen Liang, Zhichun Guo, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, 2023a comprehensive benchmark on eight tasks</p>
<p>Evaluating and enhancing large language models performance in domain-specific medicine: Osteoarthritis management with docoa. Xi Chen, Mingke You, Li Wang, Weizhi Liu, Yu Fu, Jie Xu, Shaoting Zhang, Gang Chen, Kang Li, Jian Li, 2024</p>
<p>Can llms like gpt-4 outperform traditional ai tools in dementia diagnosis? maybe. Zhuo Wang, Rongzhen Li, Bowen Dong, Jie Wang, Xiuxing Li, Ning Liu, Chenhui Mao, Wei Zhang, Liling Dong, Jing Gao, Jianyong Wang, 2023but not today</p>
<p>Analysis of large-language model versus human performance for genetics questions. Dat Duong, Benjamin D Solomon, European Journal of Human Genetics. 2023</p>
<p>Explaining legal concepts with augmented large language models. Jaromir Savelka, Kevin D Ashley, Morgan A Gray, Hannes Westermann, Huihui Xu, 2023gpt-4</p>
<p>Andrew Blair-Stanek, Nils Holzenberger, Benjamin Van Durme, arXiv:2302.06100Can gpt-3 perform statutory reasoning?. 2023arXiv preprint</p>
<p>Legal prompting: Teaching a language model to think like a lawyer. Fangyi Yu, Lee Quartey, Frank Schilder, arXiv:2212.013262022arXiv preprint</p>
<p>Evaluating large language models on medical evidence summarization. Liyan Tang, Zhaoyi Sun, Betina Idnay, Jordan G Nestor, Ali Soroush, Pierre A Elias, Ziyang Xu, Ying Ding, Greg Durrett, Justin F Rousseau, Digital Medicine. 611582023</p>
<p>Tanya Goyal, Junyi , Jessy Li, Greg Durrett, arXiv:2209.12356News summarization and evaluation in the era of gpt-3. 2022arXiv preprint</p>
<p>Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. Qingyu Chen, Jingcheng Du, Yan Hu, Kuttichi Vipina, Xueqing Keloth, Kalpana Peng, Rui Raja, Zhiyong Zhang, Hua Lu, Xu, 2024</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>Summarizing, simplifying, and synthesizing medical evidence using gpt-3 (with varying success). Chantal Shaib, Millicent L Li, Sebastian Joseph, Iain J Marshall, Junyi , Jessy Li, Byron C Wallace, arXiv:2305.062992023arXiv preprint</p>
<p>Benchmarking foundation models with language-model-as-an-examiner. Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, Lei Hou, 2023</p>
<p>Can large language models be an alternative to human evaluations?. Cheng- , Han Chiang, Hung Yi, Lee , 2023</p>
<p>BBQ: A hand-built bias benchmark for question answering. Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, Samuel Bowman, 10.18653/v1/2022.findings-acl.165Findings of the Association for Computational Linguistics: ACL 2022. Preslav Smaranda Muresan, Aline Nakov, Villavicencio, Dublin, IrelandAssociation for Computational LinguisticsMay 2022</p>
<p>Claude 3 model family. Introducing the next generation of Claude. Anthropic, </p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>Gemini: A family of highly capable multimodal models. Gemini Team, 2024</p>
<p>Measuring and mitigating unintended bias in text classification. Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, Lucy Vasserman, 2018</p>
<p>Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, Nesreen K Ahmed, arXiv:2309.00770Bias and fairness in large language models: A survey. 2024arXiv preprint</p>
<p>Equality act. Uk Government, 2010. 2010Equality Act</p>
<p>UKHSA Advisory Board: UKHSA's approach to delivering health equity for health security. 09/05/20242023Ukhsa's approach to delivering health equity for health security</p>
<p>Claudia Martínez-Demiguel, Isabel Segura-Bedmar, Esteban Chacón-Solano, Sara Guerrero-Aspizua, arXiv:2108.01204The raredis corpus: a corpus annotated with rare diseases, their signs and symptoms. 2021arXiv preprint</p>
<p>A sense inventory for clinical abbreviations and acronyms created using clinical notes and medical dictionary resources. Sungrim Moon, Serguei Pakhomov, Nathan Liu, James O Ryan, Genevieve B Melton, Journal of the American Medical Informatics Association. 2122014</p>
<p>Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, arXiv:2311.165022023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
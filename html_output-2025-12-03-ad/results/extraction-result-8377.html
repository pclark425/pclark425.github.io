<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8377 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8377</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8377</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-45ece6f3b0a319dba60c20b3013b5161dd49c58b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/45ece6f3b0a319dba60c20b3013b5161dd49c58b" target="_blank">Linear algebra with transformers</a></p>
                <p><strong>Paper Venue:</strong> Trans. Mach. Learn. Res.</p>
                <p><strong>Paper TL;DR:</strong> Nine problems of linear algebra are studied, from basic matrix operations to eigenvalue decomposition and inversion, and four encoding schemes to represent real numbers are introduced.</p>
                <p><strong>Paper Abstract:</strong> Transformers can learn to perform numerical computations from examples only. I study nine problems of linear algebra, from basic matrix operations to eigenvalue decomposition and inversion, and introduce and discuss four encoding schemes to represent real numbers. On all problems, transformers trained on sets of random matrices achieve high accuracies (over 90%). The models are robust to noise, and can generalize out of their training distribution. In particular, models trained to predict Laplace-distributed eigenvalues generalize to different classes of matrices: Wigner matrices or matrices with positive eigenvalues. The reverse is not true.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8377.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8377.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer-LAWT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequence-to-sequence Transformer trained for Linear Algebra with Transformers (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Encoder-decoder transformers trained from examples to perform numerical linear-algebra tasks (transposition, addition, multiplication, eigen/singular value decomposition, inversion) using supervised cross-entropy on sequences encoding matrices; achieves high accuracy on many tasks with small models (up to 6 layers, 10–50M parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (sequence-to-sequence, LAWT experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard encoder-decoder transformer (Vaswani et al.) with 512 hidden dim, 8 attention heads, up to 6 layers (experiments also with up to 24 decoder layers), trained with Adam (lr=1e-4, 10k warmup, cosine schedule), batch size 64; models range from ~10M to tens of millions of parameters; asymmetric variants (e.g. 6-layer encoder / 1-layer decoder or 1/6) and variations in head count (8,10,12) were evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Numerical linear-algebra computations: matrix transposition, addition, matrix-vector and matrix-matrix multiplication, eigenvalue prediction, eigenvector decomposition, singular values and SVD, and matrix inversion (on floating-point inputs rounded to 3 significant digits).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Learns algorithms implicitly from examples; relies on sequence encodings of floating-point numbers (see encodings P10/P1000/B1999/FP15) and attention-based sequence processing; no explicit symbolic arithmetic modules—models learn to (i) predict eigenvalues independently of eigenvectors, (ii) produce unit-norm vectors, and (iii) approximate orthogonality, indicating decomposition of sub-tasks internally.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Behavioral probing and diagnostics rather than internal neural probes: evaluation of decoded outputs under L1 tolerance thresholds; measuring derived properties of predictions (e.g. unit norms of eigenvectors, dot products between eigenvectors, condition numbers of predicted matrices), experiments with noisy inputs, variable-size training, training-data distribution interventions (mixtures of eigenvalue distributions), model-size scaling and asymmetric architectures; retraining experiments to test memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>High task-dependent accuracy reported as percent of test examples where decoded matrix P meets ||P-O|| < τ||O|| (L1 norm). Highlights: transposition ~99–100% exact; addition ~99% at 1% tolerance (up to 99.5% at 0.5% for B1999 on 15x15); matrix-vector and matrix-matrix multiplication ≈99.9% at 1% for 5x5 and 10x10 (P1000/P10); eigenvalues: 100% at 5% and ~99% at 2% for 5x5 and 8x8; eigenvectors: best ≈93–97% at 5% (5x5) with asymmetric FP15/P1000 model, but much lower at 0.5–1%; inversion (5x5): best ≈90% at 5% (asymmetric FP15 encoder / P1000 decoder, 12/8 heads); SVD: 100% at 5% for 4x4 singular values, full SVD up to ~98.9% at 5% for small matrices. Detailed accuracy depends on encoding, depth, and tolerance; training sample sizes range from millions to hundreds of millions for harder tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Observed failure modes include: (1) eigenvector decomposition failures concentrated on enforcing strict orthogonality (predicted eigenvalues and unit norms are correct, but non-orthogonal eigenvectors cause failures); (2) inversion failures concentrated on ill-conditioned input matrices (high condition number); (3) poor out-of-distribution generalization when training distribution is narrow (Wigner-only training fails on positive or non-iid eigenvalue distributions); (4) difficulty scaling in fixed-dimension training to larger matrices (e.g. 10x10) unless using variable-size training or larger models; (5) encoding/sequence-length limitations: long positional encodings (P10) make training harder for large matrices; (6) robustness to input noise fails when noise magnitude approaches or exceeds tolerance.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical diagnostics: (a) eigenvalues are predicted with near-100% accuracy even when decomposition fails, indicating model factors the task into subproblems; (b) predicted eigenvector columns have unit norm in ~99.9% of cases, dot-products of successive eigenvectors within [-0.05,0.05] in ~93.6% cases—showing approximate orthogonality; (c) condition numbers of predicted eigenvector matrices H correlate with success: >98% of correct predictions have cond(H) < 1.035 while >98% of failures have cond(H) >1.04; (d) inversion success correlates with input condition number: 98% of correct inverses have condition number <51.5, 98% of failures >51.5; (e) retraining on different sizes requires far fewer examples than training from scratch, arguing against simple memorization; (f) training on mixtures of data distributions (Laplace, Gaussian, uniform) produces models that generalize broadly, demonstrating the role of training distribution in learned algorithmic behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Counterpoints and limitations reported: (i) models fail to extrapolate far outside the training distribution when trained on narrow (single) distributions (e.g. Wigner-only); (ii) very large matrices (beyond ~50x50) are infeasible with quadratic attention given sequence lengths—scaling limitation; (iii) FP15 compact encoding reduces sequence length but increases vocabulary and training difficulty; (iv) some tasks (SVD, eigenvectors, inversion) need asymmetric deeper encoders or decoders and/or more heads; (v) training sample requirements can be huge for fixed-size larger matrices (hundreds of millions of examples needed for some 10x10 tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Linear algebra with transformers', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8377.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8377.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NumberEncodings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Number encoding schemes: P10, P1000, B1999, FP15</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Four explicit token encodings of floating-point numbers used to represent matrix coefficients as input/output sequences: P10 (base-10 positional, 5 tokens per number), P1000 (base-1000 positional, 3 tokens), B1999 (balanced base with sign/mantissa combined, 2 tokens), and FP15 (15-bit-like single-token floating point, compact but large vocabulary).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (this paper) with different numeric encodings</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same seq2seq transformer architectures as above; experiments compare training efficiency and accuracy across encodings and tasks; FP15 uses large vocabulary (~30k tokens) to represent numbers as single tokens; P1000 often best trade-off for moderate sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>All linear-algebra arithmetic tasks studied: addition, multiplication, eigenvalue/eigenvector decomposition, SVD, inversion.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Encodings shape internal processing: long positional encodings (P10/P1000) embed digit/exponent structure that can be exploited (e.g. sign/exponent tokens allow coarse comparisons), while compact single-token FP15 shortens sequences but requires larger embeddings and different learning dynamics; asymmetry (FP15 encoder + P1000 decoder) helps hardest tasks by compact input and informative output tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Ablative-style comparisons across identical architectures but different encodings; training curves and sample complexity comparisons; investigation of asymmetric encoder/decoder encoding combinations (FP15/P1000).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Observed effects: P1000 generally outperforms P10 and B1999 for moderate-size tasks; FP15 excels when sequence length is limiting and in encoder role for hard tasks; concrete examples: eigenvectors best result achieved with FP15 encoder + P1000 decoder (93.5% at 5% for 5x5), inversion best with 6-layer FP15 encoder + 1-layer P1000 decoder (90% at 5%). P10 suffers for long sequences; B1999 seldom best.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Long encodings (P10/P1000) produce very long input sequences for larger matrices causing training difficulty and memory/attention bottlenecks; FP15's large vocabulary increases training sample complexity and can be harder to train with noise; rounding precision (3 significant digits used) is a trade-off to keep FP15 vocabulary manageable.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Direct empirical comparisons: (i) P1000 more sample-efficient than P10 on multiplication and eigenvalue tasks; (ii) FP15 encoder + P1000 decoder outperforms pure FP15 or P1000 on eigenvectors and inversion; (iii) experiments in Appendix C show modest effect of numeric precision on accuracy but larger impact on required training examples for higher digit precision.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>No single encoding is best for all tasks: FP15 reduces sequence length but increases vocabulary size and training difficulty; P10 yields interpretable digit tokens but sequences grow too long for larger matrices; choice depends on matrix size, model capacity, and task complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Linear algebra with transformers', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8377.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8377.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FailureDiagnostics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Behavioral diagnostics and failure predictors (orthogonality and condition number analyses)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Quantitative diagnostic suite applied to model outputs to localize failure modes: measure eigenvalue accuracy, unit norms, pairwise dot-products (orthogonality), condition numbers of predicted eigenvector matrices H, and condition number of input matrices for inversion failures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer-LAWT (diagnostics applied to trained models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same transformer models; diagnostics are applied post-hoc to decoded matrix outputs on held-out test sets (10k examples) to classify failures and to derive predictive indicators of success/failure.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Eigenvector decomposition and matrix inversion primarily, but diagnostics are applicable to other numeric outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Not a neural probe; uses algebraic properties (orthogonality, unit norm, condition number) of the mathematical outputs to infer which sub-tasks the model has learned vs failed to learn (e.g., eigenvalue prediction vs orthogonalization).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Behavioral probing via computed metrics on predictions: check ||Q^T I Q - D|| < τ||D|| for eigenvectors; compute norms of columns of H, dot products between eigenvectors, cond(H); for inversion compute ||P I - Id|| and also direct distance to true I^{-1}; correlate success with input condition numbers; use these metrics to predict success without rerunning model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Key diagnostic findings: eigenvalues predicted 100% at 5% tolerance and 99.4% at 0.5% in many settings; predicted eigenvector column norms within 5% of 1 in 99.9% of examples; dot-products within [-0.05,0.05] in 93.6% of cases; cond(H)<1.035 for 98% of correct predictions while 98% of failures have cond(H)>1.04; inversion: 98% of correct predictions have input condition number <51.5, while 98% of failures >51.5.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Failures are concentrated on (a) lacking strict orthogonality in eigenvectors despite correct eigenvalues and unit norms, and (b) ill-conditioned matrices causing inversion failure and amplified rounding error; these failures are not random hallucinations but structured approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Strong empirical correlations between the algebraic diagnostics and prediction success provide evidence that the model learns eigenvalue estimation and unit-norm vector generation but struggles with orthogonalization; similarly for inversion, empirical correlation with input condition number supports the hypothesis that mathematical ill-conditioning—not architecture—drives failures.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Diagnostics are behavioral and do not reveal internal neuron-level algorithmic steps; they cannot directly show how attention or FFN weights implement arithmetic, only that outputs obey or violate mathematical constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Linear algebra with transformers', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8377.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8377.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DataInterventions_OOD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Training-distribution interventions for out-of-distribution generalization and noise robustness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systematic experiments modifying training data distributions (mixtures of Wigner matrices with varying std; mixes of Wigner with Gaussian, Laplace, positive-eigenvalue matrices; training on Laplace-only or mixtures) and noise-injection experiments to study generalization and robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer-LAWT (data-intervention experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformers as above trained on varied synthetic ensembles of random symmetric matrices; noise injected into inputs with Gaussian noise levels 0.01σ, 0.02σ, 0.05σ relative to matrix coefficient std σ.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Primarily eigenvalue prediction and matrix addition under noisy inputs; also general implications for other tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Shows that the model's learned mapping depends strongly on training data distribution; models appear to learn distribution-dependent shortcuts or inductive biases (e.g., predicting largest eigenvalues from coefficient statistics) when trained narrowly, but can learn more robust algorithms when trained on diverse or edge-case distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Train/test on different eigenvalue distributions and standard deviations; train on mixtures; inject Gaussian noise in inputs during training and test; measure accuracy at several tolerances (0.5–5%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Out-of-distribution results (eigenvalues, 2% tolerance): baseline trained on Wigner A=10 gets ~100% on matched Wigner but very low on positive/uniform/Laplace (0–26%); training on mixtures (Wigner with A∈[1,100]) improves generalization to many stds but not positive; training on Wigner+Laplace or Laplace-only yields high accuracy (~95–100%) across test distributions including positive; noise robustness: addition tasks robust when noise < tolerance (100% at 5% tolerance for 0.01σ/0.02σ noise), but degrade to ~0–40% when noise ≈ tolerance; eigenvalues robust to higher noise (0.05σ) with ~99% at 5% tolerance in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Narrow training distributions produce brittle models that fail on matrix ensembles with different eigenvalue statistics (e.g., positive-definite). Noise above tolerance causes accuracy collapse. Variable-size training helps scaling but requires architecture or dataset adjustments.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical: models trained on Laplace or mixed ensembles generalize to Wigner matrices and other distributions, whereas Wigner-only models do not generalize to positive eigenvalue matrices; noise experiments show expected dependency of linear operations on noise magnitude and non-linear amplification/attenuation for eigenvalue problems; retraining experiments (appendix) show faster convergence indicating learned algorithmic components rather than memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Generalization improvements require careful, sometimes counter-intuitive training-distribution choices; making training distribution more diverse can harm performance on specific held-in distributions (e.g., adding positive matrices harms Wigner performance unless mixtures are selected appropriately).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Linear algebra with transformers', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural GPUs learn algorithms <em>(Rating: 2)</em></li>
                <li>Investigating the limitations of transformers with simple arithmetic tasks <em>(Rating: 2)</em></li>
                <li>Neural arithmetic logic units <em>(Rating: 2)</em></li>
                <li>Analysing mathematical reasoning abilities of neural models <em>(Rating: 2)</em></li>
                <li>Deep learning for symbolic mathematics <em>(Rating: 1)</em></li>
                <li>Neural networks for computing eigenvalues and eigenvectors <em>(Rating: 2)</em></li>
                <li>Learning numeracy: Binary arithmetic with neural turing machines <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8377",
    "paper_id": "paper-45ece6f3b0a319dba60c20b3013b5161dd49c58b",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "Transformer-LAWT",
            "name_full": "Sequence-to-sequence Transformer trained for Linear Algebra with Transformers (this paper)",
            "brief_description": "Encoder-decoder transformers trained from examples to perform numerical linear-algebra tasks (transposition, addition, multiplication, eigen/singular value decomposition, inversion) using supervised cross-entropy on sequences encoding matrices; achieves high accuracy on many tasks with small models (up to 6 layers, 10–50M parameters).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer (sequence-to-sequence, LAWT experiments)",
            "model_description": "Standard encoder-decoder transformer (Vaswani et al.) with 512 hidden dim, 8 attention heads, up to 6 layers (experiments also with up to 24 decoder layers), trained with Adam (lr=1e-4, 10k warmup, cosine schedule), batch size 64; models range from ~10M to tens of millions of parameters; asymmetric variants (e.g. 6-layer encoder / 1-layer decoder or 1/6) and variations in head count (8,10,12) were evaluated.",
            "arithmetic_task_type": "Numerical linear-algebra computations: matrix transposition, addition, matrix-vector and matrix-matrix multiplication, eigenvalue prediction, eigenvector decomposition, singular values and SVD, and matrix inversion (on floating-point inputs rounded to 3 significant digits).",
            "mechanism_or_representation": "Learns algorithms implicitly from examples; relies on sequence encodings of floating-point numbers (see encodings P10/P1000/B1999/FP15) and attention-based sequence processing; no explicit symbolic arithmetic modules—models learn to (i) predict eigenvalues independently of eigenvectors, (ii) produce unit-norm vectors, and (iii) approximate orthogonality, indicating decomposition of sub-tasks internally.",
            "probing_or_intervention_method": "Behavioral probing and diagnostics rather than internal neural probes: evaluation of decoded outputs under L1 tolerance thresholds; measuring derived properties of predictions (e.g. unit norms of eigenvectors, dot products between eigenvectors, condition numbers of predicted matrices), experiments with noisy inputs, variable-size training, training-data distribution interventions (mixtures of eigenvalue distributions), model-size scaling and asymmetric architectures; retraining experiments to test memorization.",
            "performance_metrics": "High task-dependent accuracy reported as percent of test examples where decoded matrix P meets ||P-O|| &lt; τ||O|| (L1 norm). Highlights: transposition ~99–100% exact; addition ~99% at 1% tolerance (up to 99.5% at 0.5% for B1999 on 15x15); matrix-vector and matrix-matrix multiplication ≈99.9% at 1% for 5x5 and 10x10 (P1000/P10); eigenvalues: 100% at 5% and ~99% at 2% for 5x5 and 8x8; eigenvectors: best ≈93–97% at 5% (5x5) with asymmetric FP15/P1000 model, but much lower at 0.5–1%; inversion (5x5): best ≈90% at 5% (asymmetric FP15 encoder / P1000 decoder, 12/8 heads); SVD: 100% at 5% for 4x4 singular values, full SVD up to ~98.9% at 5% for small matrices. Detailed accuracy depends on encoding, depth, and tolerance; training sample sizes range from millions to hundreds of millions for harder tasks.",
            "error_types_or_failure_modes": "Observed failure modes include: (1) eigenvector decomposition failures concentrated on enforcing strict orthogonality (predicted eigenvalues and unit norms are correct, but non-orthogonal eigenvectors cause failures); (2) inversion failures concentrated on ill-conditioned input matrices (high condition number); (3) poor out-of-distribution generalization when training distribution is narrow (Wigner-only training fails on positive or non-iid eigenvalue distributions); (4) difficulty scaling in fixed-dimension training to larger matrices (e.g. 10x10) unless using variable-size training or larger models; (5) encoding/sequence-length limitations: long positional encodings (P10) make training harder for large matrices; (6) robustness to input noise fails when noise magnitude approaches or exceeds tolerance.",
            "evidence_for_mechanism": "Empirical diagnostics: (a) eigenvalues are predicted with near-100% accuracy even when decomposition fails, indicating model factors the task into subproblems; (b) predicted eigenvector columns have unit norm in ~99.9% of cases, dot-products of successive eigenvectors within [-0.05,0.05] in ~93.6% cases—showing approximate orthogonality; (c) condition numbers of predicted eigenvector matrices H correlate with success: &gt;98% of correct predictions have cond(H) &lt; 1.035 while &gt;98% of failures have cond(H) &gt;1.04; (d) inversion success correlates with input condition number: 98% of correct inverses have condition number &lt;51.5, 98% of failures &gt;51.5; (e) retraining on different sizes requires far fewer examples than training from scratch, arguing against simple memorization; (f) training on mixtures of data distributions (Laplace, Gaussian, uniform) produces models that generalize broadly, demonstrating the role of training distribution in learned algorithmic behavior.",
            "counterexamples_or_challenges": "Counterpoints and limitations reported: (i) models fail to extrapolate far outside the training distribution when trained on narrow (single) distributions (e.g. Wigner-only); (ii) very large matrices (beyond ~50x50) are infeasible with quadratic attention given sequence lengths—scaling limitation; (iii) FP15 compact encoding reduces sequence length but increases vocabulary and training difficulty; (iv) some tasks (SVD, eigenvectors, inversion) need asymmetric deeper encoders or decoders and/or more heads; (v) training sample requirements can be huge for fixed-size larger matrices (hundreds of millions of examples needed for some 10x10 tasks).",
            "uuid": "e8377.0",
            "source_info": {
                "paper_title": "Linear algebra with transformers",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "NumberEncodings",
            "name_full": "Number encoding schemes: P10, P1000, B1999, FP15",
            "brief_description": "Four explicit token encodings of floating-point numbers used to represent matrix coefficients as input/output sequences: P10 (base-10 positional, 5 tokens per number), P1000 (base-1000 positional, 3 tokens), B1999 (balanced base with sign/mantissa combined, 2 tokens), and FP15 (15-bit-like single-token floating point, compact but large vocabulary).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer (this paper) with different numeric encodings",
            "model_description": "Same seq2seq transformer architectures as above; experiments compare training efficiency and accuracy across encodings and tasks; FP15 uses large vocabulary (~30k tokens) to represent numbers as single tokens; P1000 often best trade-off for moderate sizes.",
            "arithmetic_task_type": "All linear-algebra arithmetic tasks studied: addition, multiplication, eigenvalue/eigenvector decomposition, SVD, inversion.",
            "mechanism_or_representation": "Encodings shape internal processing: long positional encodings (P10/P1000) embed digit/exponent structure that can be exploited (e.g. sign/exponent tokens allow coarse comparisons), while compact single-token FP15 shortens sequences but requires larger embeddings and different learning dynamics; asymmetry (FP15 encoder + P1000 decoder) helps hardest tasks by compact input and informative output tokens.",
            "probing_or_intervention_method": "Ablative-style comparisons across identical architectures but different encodings; training curves and sample complexity comparisons; investigation of asymmetric encoder/decoder encoding combinations (FP15/P1000).",
            "performance_metrics": "Observed effects: P1000 generally outperforms P10 and B1999 for moderate-size tasks; FP15 excels when sequence length is limiting and in encoder role for hard tasks; concrete examples: eigenvectors best result achieved with FP15 encoder + P1000 decoder (93.5% at 5% for 5x5), inversion best with 6-layer FP15 encoder + 1-layer P1000 decoder (90% at 5%). P10 suffers for long sequences; B1999 seldom best.",
            "error_types_or_failure_modes": "Long encodings (P10/P1000) produce very long input sequences for larger matrices causing training difficulty and memory/attention bottlenecks; FP15's large vocabulary increases training sample complexity and can be harder to train with noise; rounding precision (3 significant digits used) is a trade-off to keep FP15 vocabulary manageable.",
            "evidence_for_mechanism": "Direct empirical comparisons: (i) P1000 more sample-efficient than P10 on multiplication and eigenvalue tasks; (ii) FP15 encoder + P1000 decoder outperforms pure FP15 or P1000 on eigenvectors and inversion; (iii) experiments in Appendix C show modest effect of numeric precision on accuracy but larger impact on required training examples for higher digit precision.",
            "counterexamples_or_challenges": "No single encoding is best for all tasks: FP15 reduces sequence length but increases vocabulary size and training difficulty; P10 yields interpretable digit tokens but sequences grow too long for larger matrices; choice depends on matrix size, model capacity, and task complexity.",
            "uuid": "e8377.1",
            "source_info": {
                "paper_title": "Linear algebra with transformers",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "FailureDiagnostics",
            "name_full": "Behavioral diagnostics and failure predictors (orthogonality and condition number analyses)",
            "brief_description": "Quantitative diagnostic suite applied to model outputs to localize failure modes: measure eigenvalue accuracy, unit norms, pairwise dot-products (orthogonality), condition numbers of predicted eigenvector matrices H, and condition number of input matrices for inversion failures.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer-LAWT (diagnostics applied to trained models)",
            "model_description": "Same transformer models; diagnostics are applied post-hoc to decoded matrix outputs on held-out test sets (10k examples) to classify failures and to derive predictive indicators of success/failure.",
            "arithmetic_task_type": "Eigenvector decomposition and matrix inversion primarily, but diagnostics are applicable to other numeric outputs.",
            "mechanism_or_representation": "Not a neural probe; uses algebraic properties (orthogonality, unit norm, condition number) of the mathematical outputs to infer which sub-tasks the model has learned vs failed to learn (e.g., eigenvalue prediction vs orthogonalization).",
            "probing_or_intervention_method": "Behavioral probing via computed metrics on predictions: check ||Q^T I Q - D|| &lt; τ||D|| for eigenvectors; compute norms of columns of H, dot products between eigenvectors, cond(H); for inversion compute ||P I - Id|| and also direct distance to true I^{-1}; correlate success with input condition numbers; use these metrics to predict success without rerunning model.",
            "performance_metrics": "Key diagnostic findings: eigenvalues predicted 100% at 5% tolerance and 99.4% at 0.5% in many settings; predicted eigenvector column norms within 5% of 1 in 99.9% of examples; dot-products within [-0.05,0.05] in 93.6% of cases; cond(H)&lt;1.035 for 98% of correct predictions while 98% of failures have cond(H)&gt;1.04; inversion: 98% of correct predictions have input condition number &lt;51.5, while 98% of failures &gt;51.5.",
            "error_types_or_failure_modes": "Failures are concentrated on (a) lacking strict orthogonality in eigenvectors despite correct eigenvalues and unit norms, and (b) ill-conditioned matrices causing inversion failure and amplified rounding error; these failures are not random hallucinations but structured approximations.",
            "evidence_for_mechanism": "Strong empirical correlations between the algebraic diagnostics and prediction success provide evidence that the model learns eigenvalue estimation and unit-norm vector generation but struggles with orthogonalization; similarly for inversion, empirical correlation with input condition number supports the hypothesis that mathematical ill-conditioning—not architecture—drives failures.",
            "counterexamples_or_challenges": "Diagnostics are behavioral and do not reveal internal neuron-level algorithmic steps; they cannot directly show how attention or FFN weights implement arithmetic, only that outputs obey or violate mathematical constraints.",
            "uuid": "e8377.2",
            "source_info": {
                "paper_title": "Linear algebra with transformers",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "DataInterventions_OOD",
            "name_full": "Training-distribution interventions for out-of-distribution generalization and noise robustness",
            "brief_description": "Systematic experiments modifying training data distributions (mixtures of Wigner matrices with varying std; mixes of Wigner with Gaussian, Laplace, positive-eigenvalue matrices; training on Laplace-only or mixtures) and noise-injection experiments to study generalization and robustness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer-LAWT (data-intervention experiments)",
            "model_description": "Transformers as above trained on varied synthetic ensembles of random symmetric matrices; noise injected into inputs with Gaussian noise levels 0.01σ, 0.02σ, 0.05σ relative to matrix coefficient std σ.",
            "arithmetic_task_type": "Primarily eigenvalue prediction and matrix addition under noisy inputs; also general implications for other tasks.",
            "mechanism_or_representation": "Shows that the model's learned mapping depends strongly on training data distribution; models appear to learn distribution-dependent shortcuts or inductive biases (e.g., predicting largest eigenvalues from coefficient statistics) when trained narrowly, but can learn more robust algorithms when trained on diverse or edge-case distributions.",
            "probing_or_intervention_method": "Train/test on different eigenvalue distributions and standard deviations; train on mixtures; inject Gaussian noise in inputs during training and test; measure accuracy at several tolerances (0.5–5%).",
            "performance_metrics": "Out-of-distribution results (eigenvalues, 2% tolerance): baseline trained on Wigner A=10 gets ~100% on matched Wigner but very low on positive/uniform/Laplace (0–26%); training on mixtures (Wigner with A∈[1,100]) improves generalization to many stds but not positive; training on Wigner+Laplace or Laplace-only yields high accuracy (~95–100%) across test distributions including positive; noise robustness: addition tasks robust when noise &lt; tolerance (100% at 5% tolerance for 0.01σ/0.02σ noise), but degrade to ~0–40% when noise ≈ tolerance; eigenvalues robust to higher noise (0.05σ) with ~99% at 5% tolerance in many settings.",
            "error_types_or_failure_modes": "Narrow training distributions produce brittle models that fail on matrix ensembles with different eigenvalue statistics (e.g., positive-definite). Noise above tolerance causes accuracy collapse. Variable-size training helps scaling but requires architecture or dataset adjustments.",
            "evidence_for_mechanism": "Empirical: models trained on Laplace or mixed ensembles generalize to Wigner matrices and other distributions, whereas Wigner-only models do not generalize to positive eigenvalue matrices; noise experiments show expected dependency of linear operations on noise magnitude and non-linear amplification/attenuation for eigenvalue problems; retraining experiments (appendix) show faster convergence indicating learned algorithmic components rather than memorization.",
            "counterexamples_or_challenges": "Generalization improvements require careful, sometimes counter-intuitive training-distribution choices; making training distribution more diverse can harm performance on specific held-in distributions (e.g., adding positive matrices harms Wigner performance unless mixtures are selected appropriately).",
            "uuid": "e8377.3",
            "source_info": {
                "paper_title": "Linear algebra with transformers",
                "publication_date_yy_mm": "2021-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural GPUs learn algorithms",
            "rating": 2
        },
        {
            "paper_title": "Investigating the limitations of transformers with simple arithmetic tasks",
            "rating": 2
        },
        {
            "paper_title": "Neural arithmetic logic units",
            "rating": 2
        },
        {
            "paper_title": "Analysing mathematical reasoning abilities of neural models",
            "rating": 2
        },
        {
            "paper_title": "Deep learning for symbolic mathematics",
            "rating": 1
        },
        {
            "paper_title": "Neural networks for computing eigenvalues and eigenvectors",
            "rating": 2
        },
        {
            "paper_title": "Learning numeracy: Binary arithmetic with neural turing machines",
            "rating": 1
        }
    ],
    "cost": 0.01594475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Linear algebra with transformers</h1>
<p>François Charton Meta AI<br>fcharton@meta.com</p>
<h4>Abstract</h4>
<p>Transformers can learn to perform numerical computations from examples only. I study nine problems of linear algebra, from basic matrix operations to eigenvalue decomposition and inversion, and introduce and discuss four encoding schemes to represent real numbers. On all problems, transformers trained on sets of random matrices achieve high accuracies (over $90 \%$ ). The models are robust to noise, and can generalize out of their training distribution. In particular, models trained to predict Laplace-distributed eigenvalues generalize to different classes of matrices: Wigner matrices or matrices with positive eigenvalues. The reverse is not true.</p>
<h2>1 Introduction</h2>
<p>Since their introduction for machine translation by Vaswani et al. (2017), transformers were applied to a wide range of problems, from text generation (Radford et al., 2018; 2019) to image processing (Carion et al., 2020) and speech recognition (Dong et al., 2018), where they now achieve state-of-the-art performance (Dosovitskiy et al., 2021; Wang et al., 2020b). Transformers have also been proposed for problems of symbolic mathematics, like integration (Lample \&amp; Charton, 2019), theorem proving (Polu \&amp; Sutskever, 2020), formal logic (Hahn et al., 2021), SAT solving (Shi et al., 2021), symbolic regression (Biggio et al., 2021) and dynamical systems Charton et al. (2020). In these works, transformers perform symbolic computations, i.e. manipulate abstract mathematical symbols.</p>
<p>Beyond symbol manipulation, mathematics also involve numerical calculations (e.g. arithmetic, numerical solutions of equations). On these tasks, experiments with transformers and other sequence models have been disappointing. Basic arithmetic operations, like multiplication or modulus, prove very difficult to learn (Kaiser \&amp; Sutskever, 2015; Palamas, 2017), and models struggle with generalization out of their training distribution (Nogueira et al., 2021). It could even be shown (Shalev-Shwartz et al., 2017) that some arithmetic tasks cannot be solved using gradient descent. Such results might severely restrict the applicability of transformers in science. Most practical problems of mathematics mix symbolic and numerical computations. If transformers "cannot compute", their use in science is very limited.</p>
<p>In this paper, I investigate the capability of transformers to learn to perform numerical computations with high accuracy. I focus on nine problems of linear algebra, from basic operations on dense matrices to inversion, eigen and singular value decomposition. I show that small transformers can be trained, from examples only, to compute approximate solutions (up to a few percents of the $L^{1}$ norm) with more than $90 \%$ accuracy (over $99 \%$ in most cases). I propose and discuss four encodings to represent real numbers, and train small sequence to sequence transformers (up to 6 layers, 10 to 50 million trainable parameters) from generated datasets of random matrices. I investigate different architectures, in particular asymmetric configurations where the encoder or decoder has only one layer. Finally, I show that the models are robust to noisy data, and that they can generalize out of their training distribution if special attention is paid to training data generation.</p>
<p>Caveat. This paper does not advocate replacing existing linear algebra algorithms with transformer-based implementations. Numerical packages are faster, more accurate, and scale better. My motivation is twofold: better understand the capabilities and limitations of transformers in mathematics, and investigate their potential use as tools for the emerging field of AI for science.</p>
<p>In applications to mathematics, while previous research has shown that transformers struggle with basic arithmetic, I demonstrate that they can learn complex computations, like eigenvalue decomposition. I also show that leveraging the theory of random matrices can help understand the mechanisms of out-of-domain generalization, a known limitation of transformers in mathematics (Welleck et al., 2021), and a difficult problem because of the lack of metrics over problem space.</p>
<p>Beyond mathematics, transformers are fast becoming the "default model" for many deep learning applications. Their potential use as end to end tools for AI for Science has received a lot of attention recently. I believe that demonstrating that transformers can handle some of the computational building blocks of many scientific problems, like the problems of linear algebra discussed here, is a pre-requisite to their wider generalization.</p>
<p>The source code for the model and experiments is available at github.com/facebookresearch/LAWT.</p>
<h1>2 Problems and datasets</h1>
<p>Let $M$ and $N$ be $m \times n$ real matrices and $V \in \mathbb{R}^{m}$. This paper considers nine problems of linear algebra:</p>
<ul>
<li>matrix transposition: find $M^{T}$, a $n \times m$ matrix,</li>
<li>matrix addition: find $M+N$, a $m \times n$ matrix,</li>
<li>matrix-vector multiplication: find $M^{T} V$, in $\mathbb{R}^{n}$,</li>
<li>matrix multiplication: find $M^{T} N$, a $n \times n$ matrix,</li>
<li>eigenvalues: $M$ symmetric, find its $n$ (real) eigenvalues, sorted in descending order,</li>
<li>eigenvectors: $M$ symmetric, find $D$ diagonal and $Q$ orthogonal such that $Q M Q^{T}=D$, set as a $(n+1) \times n$ matrix, with (sorted) eigenvalues in its first row,</li>
<li>singular values: find the $n$ eigenvalues of $M^{T} M$, sorted in descending order,</li>
<li>singular value decomposition: find orthogonal $U, V$ and diagonal $S$ such that $S=U M V$, set as a $(m+n+1) \times \min (m, n)$ matrix,</li>
<li>inversion: $M$ square and invertible, find its inverse $P$, such that $M P=P M=I d$.</li>
</ul>
<p>These problems range from operations on single coefficients of the matrices (transposition and addition), to computations over rows and columns, involving several arithmetic operations (multiplication), and complex nonlinear transformations involving the whole matrix (decompositions and inversion).</p>
<p>For each problem, the training data is generated by sampling random input matrices $I$ (see section 2.2), and computing the output $O$ with a linear algebra package (NumPy linalg). All coefficients in $I$ and $O$ are set in base ten floating-point representation, and rounded to three significant digits in the mantissa. If a problem has several input or output matrices, they are concatenated into one (for instance, the two $m \times n$ operands of the addition task are concatenated into one $m \times 2 n$ matrix $I$ ).</p>
<h3>2.1 Encoding matrices as sequences</h3>
<p>The input and output to all problems studied here are matrices. Transformers process sequences of tokens. To encode a $m \times n$ matrix as a sequence, its dimensions are encoded as two symbolic tokens ( Vm and Vn ), and its $m n$ coefficients are then enumerated and encoded. I propose four encoding schemes for matrix coefficients (set in scientific notation with three significant digits): P10, P1000, B1999, and FP15.</p>
<p>Base 10 positional encoding (P10) represents numbers as sequences of five tokens : one sign token (+ or -), 3 digits (from 0 to 9 ) for the mantissa, and a symbolic token (from E-100 to E+100) for the exponent. For instance, 3.14 is represented as $314.10^{-2}$, and encoded as $[+, 3,1,4, \mathrm{E}-2]$.</p>
<p>Base 1000 positional encoding (P1000) provides a more compact representation. The mantissa is encoded as a single token (from 0 to 999) and a number is represented as the triplet (sign, mantissa, exponent).</p>
<p>Balanced base 1999 (B1999) encodes the sign and mantissa as a single token (from -999 to 999).
15 bit floating point (FP15) encodes a floating point number $x=m 10^{b}$ as a single token FPm/b.
Table 1 provides examples for the four encodings. More information can be found in Appendix A.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Encoding</th>
<th style="text-align: center;">3.14</th>
<th style="text-align: center;">$-6.02 .10^{23}$</th>
<th style="text-align: center;">Tokens / coefficient</th>
<th style="text-align: center;">Size of vocabulary</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">P10</td>
<td style="text-align: center;">$[+, 3,1,4, E-2]$</td>
<td style="text-align: center;">$[-, 6,0,2, E 21]$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">210</td>
</tr>
<tr>
<td style="text-align: left;">P1000</td>
<td style="text-align: center;">$[+, 314, E-2]$</td>
<td style="text-align: center;">$[-, 602, E 21]$</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1100</td>
</tr>
<tr>
<td style="text-align: left;">B1999</td>
<td style="text-align: center;">$[314, E-2]$</td>
<td style="text-align: center;">$[-602, E 21]$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2000</td>
</tr>
<tr>
<td style="text-align: left;">FP15</td>
<td style="text-align: center;">$[\mathrm{FP} 314 /-2]$</td>
<td style="text-align: center;">$[\mathrm{FP}-602 / 21]$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">30000</td>
</tr>
</tbody>
</table>
<p>Table 1: Four encodings for matrix coefficients.</p>
<p>Choosing an encoding is a trade-off. Long encodings (P10, P1000) use a small vocabulary, and embed knowledge about numbers that the model can use (e.g. that numbers can be crudely compared from their signs and exponents only, that addition and multiplication can be learned by memorizing small tables). Compact encodings use a larger vocabulary (harder to learn) but result in shorter sequences that facilitate training with transformers. In P10, a $20 \times 20$ matrix is a sequence of 2002 tokens, close to the practical limit of transformers with quadratic attention. In FP15, it is only 402 tokens long.</p>
<p>The decision to round matrix coefficients to three significant digits is mainly motivated by the need to keep FP15 vocabulary at sizes that small transformers can learn without pre-training. Experiments about the impact of number precision can be found in appendix C.</p>
<h1>2.2 Random matrix generation</h1>
<p>In most experiments, training and test data are random matrices with coefficients uniformly distributed in $[-A, A]$ (with $A=10$ ). When symmetric, these matrices are known as Wigner matrices. Their eigenvalues have a centered distribution with standard deviation $\sigma=A \sqrt{n / 3}$ (see Mehta (2004) and Appendix H) that converges as $n$ grows to the semi-circle law $p(\lambda)=\sqrt{4 \sigma^{2}-\lambda^{2}} /\left(2 \pi \sigma^{2}\right)$. If the coefficients follow a gaussian distribution, the associated eigenvectors are uniformly distributed over the unit sphere.</p>
<p>In section 4.4, while investigating out-of-distribution generalization, I will need to generate random symmetric matrices with specific eigenvalue distributions (i.e. classes of random matrices with non-independent coefficients). To this effect, I sample random symmetric matrices $M$ with gaussian coefficients, and compute their eigenvalue decomposition $M=P D P^{T}$, with $P$ an orthogonal matrix of eigenvectors (uniformly distributed over the unit sphere because the coefficients are gaussian). Replacing $D$, the diagonal matrix of eigenvalues of $M$, with a diagonal $D^{\prime}$ sampled from a different distribution, and recomputing $M^{\prime}=P D^{\prime} P^{T}$, yields a symmetric matrix (since $P$ is orthogonal) with eigenvalues following the desired distribution, and eigenvectors uniformly distributed over the unit sphere.</p>
<h2>3 Models and experimental settings</h2>
<p>Models and training. All models use the transformer architecture from Vaswani et al. (2017): an encoder and a decoder connected by cross-attention. Models have 512 dimensions, 8 attention heads and up to 6 layers (experiments with larger models can be found in Appendix D.3). Training is supervised, minimizes the cross-entropy between model predictions and correct solutions, and uses the Adam optimiser (Kingma \&amp; Ba, 2014) with a learning rate of $10^{-4}$, a linear warm-up phase of 10,000 steps and cosine scheduling (Loshchilov \&amp; Hutter, 2016). Training data is generated on the fly in batches of 64 . All models are trained on an internal cluster, using NVIDIA Volta GPU with 32GB memory. Basic operations on matrices and eigenvalues train on 1 GPU in less than a day (from a few hours for transposition and addition, to a day for multiplication and eigenvalues). Eigenvectors, SVD and inversion train on 4 GPU, and take from 3 days to a week.</p>
<p>Evaluation. At the end of every epoch (300,000 examples), a random test set (10,000 examples) is generated and model accuracy is evaluated. A predicted sequence is a correct solution to the problem $(I, O)(I$ and $O$ the input and output matrices) if it can be decoded as a valid matrix $P$ and approximates the correct solution to a given tolerance $\tau$. In most problems, I check that $P$ verifies $|P-O|&lt;\tau|O|$. When computing eigenvectors, I verify that the predicted solution $(Q, D)$ can reconstruct the input matrix, $\left|Q I Q^{T}-D\right|&lt;\tau|D|$. For singular value decomposition, I check that $|U I V-S|&lt;\tau|S|$, and for matrix inversion, that $|P I-I d|&lt;\tau|I d|=\tau$.</p>
<p>The $L^{1}$ norm, $|A|=\sum_{i, j}\left|a_{i, j}\right|$, for $A=\left(a_{i, j}\right)$, is used in all experiments. With other norms, like $L^{2}$ or $L^{\infty}$, the error is weighted in favor of correct predictions of the largest coefficients in the solution. For eigenvalue and singular value prediction, this amounts to finding the largest values, a different and easier problem. More discussion and comparisons between norms can be found in Appendix B.</p>
<p>Numerical tolerance. All results are provided with tolerance $\tau$ between 0.5 and $5 \%$. Since coefficients are rounded to three significant digits, $0.5 \%$ is the best we can achieve when computations are subject to rounding error. As computations become more complex, error accumulates, and larger values of $\tau$ should be considered. I consider $\tau=0 \%$ for transposition, $\tau=1 \%$ for basic matrix operations (addition and multiplication), and $\tau=2$ or $5 \%$ for non linear operations (decomposition, inversion).</p>
<p>Problem size. All experiments are performed on dense matrices. In most cases, I focus on $5 \times 5$ matrices (or rectangular matrices with as many coefficients: e.g. $6 \times 4,2 \times 13$ ), and scale to larger dimensions, from $8 \times 8$ to $15 \times 15$, and datasets of matrices with variable dimensions (e.g. $5 \times 5$ to $15 \times 15$ ). In this paper, the emphasis is on problems that can be solved by small transformers (up to 6 layers). I discuss scaling and larger models in Appendix D.3.</p>
<h1>4 Experiments and results</h1>
<p>This section presents experimental results for the nine problems considered. I compare encodings for different matrix sizes and tolerance levels, using the best choice of hyperparameters for each problem (i.e. the smallest architecture that can achieve high accuracy). I also show that our models are robust to noise in the training data. Learning curves and experiments with model size can be found in Appendix D, alternative architectures in Appendix E. 1 (LSTM and GRU) and E. 2 (universal transformers), and additional tasks (re-training, joint training) in Appendix F.</p>
<h3>4.1 Transposition</h3>
<p>Learning to transpose a matrix amounts to learning a permutation of its elements. For a square matrix, all cycles in the permutation have length 1 or 2 . Longer cycles may appear in rectangular matrices. This task involves no arithmetic operations: tokens in the input sequence are merely copied to different positions in the output. This paper investigates two cases. In the fixed-dimension case, all matrices in the dataset have the same dimensions and only one permutation must be learned. In the variable-dimension case, the dataset includes matrices of different formats, and several permutations must be learned (one per matrix format). In these experiments, models have one layer, 256 dimensions and 8 attention heads, and use the four encodings.</p>
<p>After training, all models achieve $99 \%$ exact accuracy ( $0 \%$ tolerance) for fixed-size matrices with dimensions up to $30 \times 30$. This holds for all encodings and input and output sequence lengths up to 2000 tokens. The variable-size case proves more difficult, because the model must learn many different permutations. Still, the model achieve $99 \%$ accuracy on matrices with 5 to 15 dimensions, and $96 \%$ for matrices with 5 to 20 dimensions. Table 2 summarizes the results.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Fixed dimensions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Variable dimensions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Square</td>
<td style="text-align: center;">Rectangular</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Square</td>
<td style="text-align: center;">Rectangular</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">5 x 5</td>
<td style="text-align: center;">10 x 10</td>
<td style="text-align: center;">20 x 20</td>
<td style="text-align: center;">30 x 30</td>
<td style="text-align: center;">5 x 6</td>
<td style="text-align: center;">7 x 8</td>
<td style="text-align: center;">9 x 11</td>
<td style="text-align: center;">$5-15$</td>
<td style="text-align: center;">$5-20$</td>
<td style="text-align: center;">$5-15$</td>
<td style="text-align: center;">$5-20$</td>
</tr>
<tr>
<td style="text-align: left;">P10</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">P1000</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">B1999</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">96.6</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">91.4</td>
</tr>
<tr>
<td style="text-align: left;">FP15</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">96.1</td>
</tr>
</tbody>
</table>
<p>Table 2: Exact prediction of matrix transposition for different matrix dimensions. Transformers with one layer, 256 dimensions and 8 attention heads.</p>
<h1>4.2 Addition</h1>
<p>To add two $m \times n$ matrices, the model must learn the correspondence between input and output positions and the algorithm for adding two numbers in scientific notation. Then, it must apply the algorithm to $m n$ pairs of coefficients. In these experiments, models have one or two layers, 8 attention heads and 512 dimensions.</p>
<p>All models achieve $99 \%$ accuracy at $1 \%$ tolerance ( $98 \%$ at $0.5 \%$ ) on sums of fixed-size matrices with dimensions up to $10 \times 10$, for all four encodings. B1999 models achieve $99.5 \%$ accuracy at $0.5 \%$ tolerance for $15 \times 15$ matrices and $87.9 \%$ accuracy at $1 \%$ tolerance on $20 \times 20$ matrices. As dimensions increase, models using long encodings ( P 1000 and P 10 ) become more difficult to train as their input sequences grow longer. For instance, adding two $15 \times 15$ matrices involves 450 coefficients, an input of 1352 tokens in P1000 and 2252 in P10.</p>
<p>On variable-size matrices, models achieve $99.5 \%$ accuracy at $1 \%$ tolerance for dimensions up to 10 , with 2-layer transformers using the B1999 encoding. Their accuracy drops to 48 and $37 \%$ for square and rectangular matrices with 5 to 15 dimensions. This can be mitigated by increasing the depth of the decoder: models with one layer in the encoder and 6 in the decoder achieve 77 and $87 \%$ accuracy. Table 3 summarizes these results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Size <br> Layers</th>
<th style="text-align: center;">Fixed dimensions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Variable dimensions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">5x5</td>
<td style="text-align: center;">6x4</td>
<td style="text-align: center;">3x8</td>
<td style="text-align: center;">10x10</td>
<td style="text-align: center;">15x15</td>
<td style="text-align: center;">$20 \times 20$</td>
<td style="text-align: center;">Square</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rectangular</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5-10</td>
<td style="text-align: center;">5-15</td>
<td style="text-align: center;">5-15</td>
<td style="text-align: center;">5-10</td>
<td style="text-align: center;">5-15</td>
<td style="text-align: center;">5-15</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2/2</td>
<td style="text-align: center;">2/2</td>
<td style="text-align: center;">2/2</td>
<td style="text-align: center;">2/2</td>
<td style="text-align: center;">2/2</td>
<td style="text-align: center;">$1 / 1$</td>
<td style="text-align: center;">2/2</td>
<td style="text-align: center;">$1 / 1$</td>
<td style="text-align: center;">$1 / 6$</td>
<td style="text-align: center;">$2 / 2$</td>
<td style="text-align: center;">$2 / 2$</td>
<td style="text-align: center;">$1 / 6$</td>
</tr>
<tr>
<td style="text-align: center;">$5 \%$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">99.4</td>
</tr>
<tr>
<td style="text-align: center;">$2 \%$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">88.1</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">94.9</td>
</tr>
<tr>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">87.9</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">47.9</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">86.8</td>
</tr>
<tr>
<td style="text-align: center;">$0.5 \%$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">98.1</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">48.8</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">99.1</td>
<td style="text-align: center;">29.7</td>
<td style="text-align: center;">80.1</td>
</tr>
</tbody>
</table>
<p>Table 3: Accuracies of matrix sums, for different tolerances. B1999 encoding, 512 dimension and 8 attention heads.</p>
<h3>4.3 Multiplication</h3>
<p>Multiplication of a matrix $M$ of dimension $m \times n$ by a vector $V \in \mathbb{R}^{n}$ amounts to computing $m$ dot products between $V$ and the lines of $M$. Each calculation features $n$ multiplications and $n-1$ additions, and involves one row in the matrix and all coefficients in the vector. The model must now learn two operations: add and multiply. Experiments with models with 1 and 2 layers show that high accuracy can only be achieved with the P10 or P1000 encoding, with P1000 performing better on average. The number of layers, on the other hand, makes little difference.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">P10</th>
<th style="text-align: center;">P1000</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">P1000</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Variable 5-10 (P1000)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$5 \times 5$</td>
<td style="text-align: center;">$5 \times 5$</td>
<td style="text-align: center;">$10 \times 10$</td>
<td style="text-align: center;">$14 \times 2$</td>
<td style="text-align: center;">$9 \times 3$</td>
<td style="text-align: center;">$4 \times 6$</td>
<td style="text-align: center;">$2 \times 10$</td>
<td style="text-align: center;">Square</td>
<td style="text-align: center;">Rectangular</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Tolerance</td>
<td style="text-align: center;">$2 / 2$ layers</td>
<td style="text-align: center;">$2 / 2$</td>
<td style="text-align: center;">$2 / 2$</td>
<td style="text-align: center;">$1 / 1$</td>
<td style="text-align: center;">$1 / 1$</td>
<td style="text-align: center;">$2 / 2$</td>
<td style="text-align: center;">$2 / 2$</td>
<td style="text-align: center;">$4 / 4$</td>
<td style="text-align: center;">$2 / 2$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$5 \%$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$2 \%$</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">98.5</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">98.7</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">99.2</td>
<td style="text-align: center;">60.1</td>
<td style="text-align: center;">20.1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$0.5 \%$</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">98.1</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">98.6</td>
<td style="text-align: center;">94.5</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 4: Accuracies of matrix-vector products, for different tolerances. All model have 512 dimensions and 8 heads.</p>
<p>On this task, models achieve $99.9 \%$ accuracy at $1 \%$ tolerance for $5 \times 5$ and $10 \times 10$ square matrices, and $99 \%$ for rectangular matrices with about 30 coefficients. The variable-size case proves much harder. Models achieve non-trivial results: $60 \%$ accuracy with $1 \%$ tolerance for square matrices, but larger models are needed for high accuracy. Table 4 summarizes the results.</p>
<p>Multiplication of matrices $M$ and $P$ is a scaled-up version of matrix-vector multiplication, now performed for every column in matrix $P$. As above, high accuracy is only achieved with the P10 and P1000 encoding.</p>
<table>
<thead>
<tr>
<th></th>
<th>Square matrices</th>
<th></th>
<th>Rectangular matrices</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>$5 \times 5$</td>
<td>$5 \times 5$</td>
<td>$2 \times 13$</td>
<td>$2 \times 12$</td>
<td>$3 \times 8$</td>
<td>$4 \times 6$</td>
<td>$6 \times 4$</td>
<td>$8 \times 3$</td>
<td>$12 \times 2$</td>
<td>$13 \times 2$</td>
</tr>
<tr>
<td>Tolerance</td>
<td>P10 2/2 layers</td>
<td>$1 / 4$</td>
<td>$4 / 4$</td>
<td>$4 / 4$</td>
<td>$2 / 6$</td>
<td>$1 / 4$</td>
<td>$1 / 6$</td>
<td>$1 / 6$</td>
<td>$1 / 6$</td>
<td>$1 / 4$</td>
</tr>
<tr>
<td>$5 \%$</td>
<td>100</td>
<td>100</td>
<td>100</td>
<td>100</td>
<td>100</td>
<td>100</td>
<td>100</td>
<td>100</td>
<td>100</td>
<td>99.9</td>
</tr>
<tr>
<td>$2 \%$</td>
<td>100</td>
<td>100</td>
<td>100</td>
<td>100</td>
<td>100</td>
<td>100</td>
<td>100</td>
<td>100</td>
<td>99.7</td>
<td>99.8</td>
</tr>
<tr>
<td>$1 \%$</td>
<td>99.8</td>
<td>100</td>
<td>99.9</td>
<td>100</td>
<td>100</td>
<td>99.9</td>
<td>100</td>
<td>99.9</td>
<td>99.3</td>
<td>99.8</td>
</tr>
<tr>
<td>$0.5 \%$</td>
<td>64.5</td>
<td>99.9</td>
<td>97.1</td>
<td>98.5</td>
<td>99.6</td>
<td>99.7</td>
<td>99.5</td>
<td>99.5</td>
<td>99.0</td>
<td>99.8</td>
</tr>
</tbody>
</table>
<p>Table 5: Accuracy of matrix multiplication, for different tolerances. Fixed-size matrices with 24-26 coefficients. All encodings are P1000 unless specified. Models have 512 dimensions and 8 attention heads.</p>
<p>Models achieve $99 \%$ accuracy at $1 \%$ tolerance for $5 \times 5$ square matrices and rectangular matrices of comparable dimensions (see Table 5). Performance is the same as matrix-vector multiplication, a simpler task. However, matrix multiplication needs deeper models (especially decoders), and more training time.</p>
<h1>4.4 Eigenvalues</h1>
<p>Compared to basic operations on matrices, computing the eigenvalues of symmetric matrices is a much harder problem, non-linear and typically solved by iterative algorithms. Deeper models, with 4 or 6 layers, are used in this task. They achieve $100 \%$ accuracy at $5 \%$ tolerance, and $99 \%$ at $2 \%$, for $5 \times 5$ and $8 \times 8$ matrices. High accuracy is achieved with all four encodings, but P1000 proves more efficient with $8 \times 8$ matrices.</p>
<p>On fixed-size datasets, scaling to larger problems proves difficult. It takes 360 million examples for our best models to reach $25 \%$ accuracy on $10 \times 10$ matrices. As a comparison, 40 million examples are required to train $5 \times 5$ models to $99 \%$ accuracy, and 60 million for $8 \times 8$ models. This limitation can be overcome by training on variable-size datasets, achieving $100 \%$ accuracy at $5 \%$ tolerance, and 100,100 and $76 \%$ at $2 \%$, for sets of $5-10,5-15$ and $5-20$ matrices. Table 6 summarizes the results.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Fixed dimensions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Variable dimensions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$5 \times 5$</td>
<td style="text-align: center;">$5 \times 5$</td>
<td style="text-align: center;">$5 \times 5$</td>
<td style="text-align: center;">$5 \times 5$</td>
<td style="text-align: center;">$8 \times 8$</td>
<td style="text-align: center;">$8 \times 8$</td>
<td style="text-align: center;">$10 \times 10$</td>
<td style="text-align: center;">$5-10$</td>
<td style="text-align: center;">$5-15$</td>
<td style="text-align: center;">$5-20$</td>
</tr>
<tr>
<td style="text-align: left;">Encoding</td>
<td style="text-align: center;">P10</td>
<td style="text-align: center;">P1000</td>
<td style="text-align: center;">B1999</td>
<td style="text-align: center;">FP15</td>
<td style="text-align: center;">P1000</td>
<td style="text-align: center;">FP15</td>
<td style="text-align: center;">FP15</td>
<td style="text-align: center;">FP15</td>
<td style="text-align: center;">FP15</td>
<td style="text-align: center;">FP15</td>
</tr>
<tr>
<td style="text-align: left;">Layers</td>
<td style="text-align: center;">$6 / 6$</td>
<td style="text-align: center;">$4 / 1$</td>
<td style="text-align: center;">$6 / 6$</td>
<td style="text-align: center;">$6 / 1$</td>
<td style="text-align: center;">$6 / 1$</td>
<td style="text-align: center;">$1 / 6$</td>
<td style="text-align: center;">$1 / 6$</td>
<td style="text-align: center;">$4 / 4$</td>
<td style="text-align: center;">$6 / 6$</td>
<td style="text-align: center;">$4 / 4$</td>
</tr>
<tr>
<td style="text-align: left;">$5 \%$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">25.3</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;">$2 \%$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.2</td>
<td style="text-align: center;">97.7</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">75.5</td>
</tr>
<tr>
<td style="text-align: left;">$1 \%$</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">98.5</td>
<td style="text-align: center;">98.6</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">84.7</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">94.3</td>
<td style="text-align: center;">45.3</td>
</tr>
<tr>
<td style="text-align: left;">$0.5 \%$</td>
<td style="text-align: center;">93.7</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">91.8</td>
<td style="text-align: center;">31.1</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">22.5</td>
</tr>
</tbody>
</table>
<p>Table 6: Accuracy of eigenvalues for different tolerances and dimensions. All models have 512 dimensions and 8 attention heads, except the $10 \times 10$ model, which has 510 and 12 .</p>
<p>Larger models. In the fixed-dimension case, 6-layer models are limited to $8 \times 8$ matrices. Experiments with deeper models show that they can solve larger problems. For instance, 12-layer transformers can compute the eigenvalues of $12 \times 12$ matrices. Large models also need less examples to train to high accuracy. Table 7 summarizes these results. Detailed results are in Appendix D. 3</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Sample size (millions)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Layers</td>
<td style="text-align: center;">$1 / 8$</td>
<td style="text-align: center;">$1 / 12$</td>
<td style="text-align: center;">$1 / 24$</td>
<td style="text-align: center;">$1 / 8$</td>
<td style="text-align: center;">$1 / 12$</td>
<td style="text-align: center;">$1 / 24$</td>
</tr>
<tr>
<td style="text-align: left;">$8 \times 8$ matrices</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">11.4</td>
<td style="text-align: center;">11.1</td>
</tr>
<tr>
<td style="text-align: left;">$10 \times 10$ matrices</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">85.2</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">32.1</td>
</tr>
<tr>
<td style="text-align: left;">$12 \times 12$ matrices</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">97</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">99.3</td>
</tr>
</tbody>
</table>
<p>Table 7: Eigenvalues, larger models. Accuracy to $5 \%$ tolerance, and sample size to reach $99 \%$ accuracy.</p>
<h1>4.5 Eigenvectors</h1>
<p>In this task, the model predicts both the eigenvalues and an associated orthogonal matrix of eigenvectors. Models using the P10 and P1000 encoding achieve 97 and $94 \%$ accuracy at $5 \%$ tolerance for $5 \times 5$ matrices. P1000 models also reach $82 \%$ accuracy on $6 \times 6$ matrices. Whereas FP15 models only reach $52 \%$ accuracy, an asymmetric model, coupling a 6 -layer FP15 encoder and a 1-layer P1000 decoder, achieves $94 \%$ accuracy at $5 \%$ and 87 at $2 \%$, the best result on this task. Table 8 summarizes these results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">$5 \times 5$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$6 \times 6$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P10</td>
<td style="text-align: center;">P1000</td>
<td style="text-align: center;">FP15</td>
<td style="text-align: center;">FP15/P1000</td>
<td style="text-align: center;">P1000</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$4 / 4$ layers</td>
<td style="text-align: center;">$6 / 6$</td>
<td style="text-align: center;">$1 / 6$</td>
<td style="text-align: center;">$6 / 1$</td>
<td style="text-align: center;">$6 / 1$</td>
</tr>
<tr>
<td style="text-align: center;">$5 \%$</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">93.5</td>
<td style="text-align: center;">81.5</td>
</tr>
<tr>
<td style="text-align: center;">$2 \%$</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">67.2</td>
</tr>
<tr>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">11.0</td>
</tr>
<tr>
<td style="text-align: center;">$0.5 \%$</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">11.8</td>
<td style="text-align: center;">0.1</td>
</tr>
</tbody>
</table>
<p>Table 8: Accuracies of eigenvectors, for different tolerances and depths (512 dimensions, 8 heads).</p>
<p>Analysis of failure cases. On this task, models achieve significantly less than $100 \%$ accuracy. This makes it possible to investigate failure cases. For this analysis, I use the trained FP15/P1000 6/1 layer model from table 8 ( $93 \%$ accuracy), generate a new test sample of 10000 problems, predict solutions, and evaluate performance on various metrics. On this new test set, the model achieves $91.1 \%$ accuracy with $5 \%$ tolerance, and $82.1,45.2$ and $1.4 \%$ at 2,1 and 0.5 tolerance.</p>
<p>First, one notes that almost all model predictions (9999 out of 10000) are well-formed matrices: the model produces no meaningless output, such as incorrect encoding of numbers, or matrices with the wrong number of elements. This is consistently observed in all experiments: output syntax is learned to near-perfection at the beginning of training. Also, accuracy increases with tolerance: $95.6 \%$ accuracy at $25 \%$ tolerance. This suggests that, even when they fail to predict the correct solution, models do not hallucinate irrelevant predictions (as was reported on natural language tasks). Instead, they predict syntactically correct solutions that often turn out to be "rough approximations" of the correct answer.</p>
<p>When computing eigenvectors, the model predicts two matrices, a diagonal matrix of eigenvalues $D$ and an orthogonal matrix of eigenvectors $H$. Accuracy is measured as the $L^{1}$ distance between $H^{T} I H$ ( $I$ the input matrix) and $D$, i.e. how well $H$ diagonalizes the input into $D$. A different metric, the distance between $H D H^{T}$ and $I$, could be used instead, which is associated to a related, but weaker, problem: finding approximations to $I$ of the form $H D H^{T}$ ( $D$ diagonal). With this metric, the model achieves slightly higher accuracy: $95.7 \%$ at $5 \%$ tolerance ( $92.9,88.4$ and 73.0 at 2,1 and $0.5 \%$ ). This confirms our previous observation that, in many failure cases, the model predicts solutions that are "somehow relevant" to eigen decomposition (here, solutions to the weak problem).</p>
<p>We also know from theory that $D$ should contain the eigenvalues of the input matrices, and that $H$ should be orthogonal (all lines and columns orthogonal, with unit norm). Translating these properties into metrics can help us understand failure cases, and the relevance of incorrect model predictions.</p>
<p>First, eigenvalues are always correctly predicted: the corresponding accuracy is $100 \%$ at $5 \%$ tolerance, and $99.4 \%$ at $0.5 \%$. The (easier) sub-task of eigenvalue prediction has been learned by the model. Second, all the norms of the columns of $H$ are within $5 \%$ of 1 in $99.9 \%$ of the test examples (within $1 \%$ in $99.2 \%$ ). All predicted eigenvectors have unit norm. This indicates that failed model predictions actually succeed in computing the eigenvalues, and a set of unit vectors. In other words, those two properties of eigen decomposition have been learned by the model, in the sense that they are respected even in incorrect predictions.</p>
<p>To measure the orthogonality of the eigenvectors, I compute the dot products of successive eigenvectors, which should be all be 0 . On the test set, all dot products are within -0.05 and 0.05 in 93.6 of the cases (and within -0.01 and 0.01 in 85.1 ). This suggests that when the model fails, it proveds the correct eigenvalues, and unit eigenvectors, but fails to make the "eigenvectors" strictly orthogonal. This observation suggests a criterion for</p>
<p>predicting model accuracy: we can test the orthogonality of predicted $H$ by measuring its condition number (the ratio of its largest and smallest singular values), which should be one if $H$ is orthogonal, and larger if it is not. In fact, in $98 \%$ of correct predictions, the predicted $H$ has a condition number smaller than 1.035 . For $98 \%$ of failures, the condition number of $H$ is larger than 1.04 .</p>
<p>To summarize, when trained on eigen decomposition, the model learns the easier sub-task of predicting eigenvalues, with $100 \%$ accuracy. It also learns to preserve theoretical properties of the result, like the unit norm of eigenvectors, and their (approximate) orthogonality. All failures concentrate on one specific sub-task: orthogonalizing the eigenvectors. This allows us to derive an accurate predictor of model failure: the condition number of the predicted matrix $H$.</p>
<h1>4.6 Inversion</h1>
<p>Computing the inverses of $5 \times 5$ matrices proves the hardest task so far. Models using the P10 and P1000 encodings, with 6-layer encoders and 1-layer decoders and 8 attention heads, achieve 74 and $80 \%$ accuracy at $5 \%$ tolerance. Adding more heads in the encoder bring no gain in accuracy, but makes training faster: 8 -head models need 250 millions examples to train to $75 \%$ accuracy, 10 and 12 -head models only 120 . As in the previous task, asymmetric models achieve the best results. $90 \%$ accuracy at $5 \%$ tolerance can be reached with a 6-layer FP15 encoder with 12 attention heads, and a 1-layer P1000 decoder with 8 heads.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Tolerance</th>
<th style="text-align: center;">P10</th>
<th style="text-align: center;">P1000</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">FP15/P1000</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">8/8 heads</td>
<td style="text-align: center;">8/8 heads</td>
<td style="text-align: center;">10/8 heads</td>
<td style="text-align: center;">12/8 heads</td>
<td style="text-align: center;">10/4 heads</td>
<td style="text-align: center;">12/8 heads</td>
</tr>
<tr>
<td style="text-align: center;">$5 \%$</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">90.0</td>
</tr>
<tr>
<td style="text-align: center;">$2 \%$</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">61.7</td>
<td style="text-align: center;">52.5</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">81.8</td>
</tr>
<tr>
<td style="text-align: center;">$1 \%$</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">30.1</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">55.5</td>
<td style="text-align: center;">60.0</td>
</tr>
<tr>
<td style="text-align: center;">$0.5 \%$</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">24.7</td>
</tr>
</tbody>
</table>
<p>Table 9: 5x5 matrix inversion. All models have 512 dimension and 6/1 layers, except P1000 10 heads, which has $6 / 6$.</p>
<p>Analysis of failure cases. I proceed as in section 4.5, and use the 6/1 layer, 12/8 heads, FP15/P1000 model from table 9 ( $90 \%$ accuracy). Model accuracy on the new test set is $89.6 \%$ at $5 \%$ tolerance, and 81.7, 59.1 and 23.7 at 2,1 and $0.5 \%$ tolerance. As previously, all 10000 model predictions are well-formed matrices, and accuracy increases with tolerance: $96.6 \%$ at $25 \%$. Again, even when the model fails, it provides a "relevant" bad approximation of the solution, instead of hallucinating an unrelated guess.
For this task, the accuracy metric is the distance between $P I$ ( $P$ the predicted matrix, $I$ the input) and identity (in $L^{1}$ norm). This measures that the inverse has indeed been found. However, since the inverse is unique, we might as well use the distance between $P$ and $I^{-1}$ (i.e. the distance the model is minimizing during training). On this alternative metric, the model achieves $98.2 \%$ accuracy at $5 \%$ tolerance, and 96.0 , 92.3 and $84.5 \%$ at 2,1 and 0.5 tolerance. In this metric, all failure cases are bad approximations: the model achieves $99.5 \%$ accuracy at $25 \%$ tolerance.</p>
<p>This suggests that most model failures happen because the approximation of the $I^{-1}$ predicted by the model is not a "good inverse" of $I$, in the sense that $P I$ is not close to identity. Theory tells us this happens when the condition number of the input matrix (the ratio of largest to smallest singular values) is large. Indeed, $98 \%$ of correct predictions correspond to matrices with condition number below 51.5. On the other hand, $98 \%$ of failures are matrices with condition numbers larger than 51.5. The condition number of the input matrix proves to be a very accurate predictor of model success.</p>
<p>These results provide a complete explanation of model failures for this task. They indicate that failures are not due to the architecture or learning technique, but to the mathematical limitations of the computation of matrix inverses, which apply to every numerical algorithm. They also indicate that failures are concentrated on a small class of problems, and can be predicted in advance (without running the model, in this case).</p>
<p>They also suggest two directions for improvement. First, we could oversample ill-conditioned matrices in the training set, in a manner of curriculum learning. Second, since ill-conditioning amplifies the effect of rounding and approximate computations, training with increased precision should improve accuracy.</p>
<h1>4.7 Singular value decomposition (SVD)</h1>
<p>For symmetric matrices, singular value and eigenvalue decompositions are related: the singular values of a symmetric matrix are the square roots of the absolute values of its eigenvalues, and the vectors are the same. Yet, this task proves more difficult than computing the eigenvectors. Models achieve 100 accuracy at $5 \%$ tolerance, and $86.7 \%$ at $1 \%$ when predicting the singular values of $4 \times 4$ symmetric matrices. For the full decomposition, models achieve 98.9 and $75.3 \%$ accuracy. However, the SVD of $5 \times 5$ matrices could not be predicted using transformers with up to 6 layers, and using the P10 or P1000 encoding. Table 10 summarizes these results, on models with 512 dimensions and 8 attention heads.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Singular values</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Singular vectors</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">P10 2/2 layers</td>
<td style="text-align: center;">P1000 4/4 layers</td>
<td style="text-align: center;">P10 1/6 layers</td>
<td style="text-align: center;">P1000 6/6 layers</td>
</tr>
<tr>
<td style="text-align: left;">$5 \%$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">98.9</td>
</tr>
<tr>
<td style="text-align: left;">$2 \%$</td>
<td style="text-align: center;">98.5</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">95.7</td>
</tr>
<tr>
<td style="text-align: left;">$1 \%$</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">75.3</td>
</tr>
<tr>
<td style="text-align: left;">$0.5 \%$</td>
<td style="text-align: center;">41.1</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">6.3</td>
</tr>
</tbody>
</table>
<p>Table 10: Accuracies of SVD for $4 \times 4$ matrices.</p>
<h3>4.8 Experiments with noisy data</h3>
<p>Because experimental data is often noisy, robustness to noise is a key feature of efficient models. In this section, I investigate model behavior in the presence of random error when computing the sum and eigenvalues of $5 \times 5$ matrices. A random gaussian error is added to all coefficients of the input matrices in the train and test sets, for three levels of noise: standard deviation equal to 1,2 and $5 \%$ of the standard deviation of the random matrix coefficients ( $\sigma=5.77$ for uniform coefficients in $[-10,10]$ ). For a linear operation like addition, one expects the model to predict correct results so long tolerance $\tau$ is larger than error. For non-linear computations like eigenvalues, expected outcomes are unclear, as errors may be amplified by non-linearities or reduced by concentration laws.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">Addition</th>
<th style="text-align: right;"></th>
<th style="text-align: right;">Eigenvalues</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Encoding</td>
<td style="text-align: right;">B1999</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">FP15</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">P1000</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Dimension</td>
<td style="text-align: right;">256</td>
<td style="text-align: right;">512</td>
<td style="text-align: right;">512</td>
<td style="text-align: right;">1024</td>
<td style="text-align: right;">512</td>
<td style="text-align: right;">1024</td>
</tr>
<tr>
<td style="text-align: left;">$5 \%$ tolerance</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">$0.01 \sigma$ error</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">6.1</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">100</td>
</tr>
<tr>
<td style="text-align: left;">$0.02 \sigma$</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">100</td>
</tr>
<tr>
<td style="text-align: left;">$0.05 \sigma$</td>
<td style="text-align: right;">41.5</td>
<td style="text-align: right;">41.2</td>
<td style="text-align: right;">99.1</td>
<td style="text-align: right;">99.3</td>
<td style="text-align: right;">99.3</td>
<td style="text-align: right;">99.0</td>
</tr>
<tr>
<td style="text-align: left;">$2 \%$ tolerance</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">$0.01 \sigma$ error</td>
<td style="text-align: right;">99.8</td>
<td style="text-align: right;">99.9</td>
<td style="text-align: right;">0.7</td>
<td style="text-align: right;">99.8</td>
<td style="text-align: right;">99.3</td>
<td style="text-align: right;">99.6</td>
</tr>
<tr>
<td style="text-align: left;">$0.02 \sigma$</td>
<td style="text-align: right;">43.7</td>
<td style="text-align: right;">44.2</td>
<td style="text-align: right;">97.0</td>
<td style="text-align: right;">97.1</td>
<td style="text-align: right;">97.3</td>
<td style="text-align: right;">97.9</td>
</tr>
<tr>
<td style="text-align: left;">$0.05 \sigma$</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">37.9</td>
<td style="text-align: right;">38.4</td>
<td style="text-align: right;">40.1</td>
<td style="text-align: right;">37.3</td>
</tr>
<tr>
<td style="text-align: left;">$1 \%$ tolerance</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">$0.01 \sigma$ error</td>
<td style="text-align: right;">39.8</td>
<td style="text-align: right;">41.7</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: right;">82.1</td>
<td style="text-align: right;">79.7</td>
<td style="text-align: right;">83.8</td>
</tr>
<tr>
<td style="text-align: left;">$0.02 \sigma$</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: right;">47.8</td>
<td style="text-align: right;">51.3</td>
<td style="text-align: right;">46.2</td>
<td style="text-align: right;">47.5</td>
</tr>
<tr>
<td style="text-align: left;">$0.05 \sigma$</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">3.8</td>
<td style="text-align: right;">4.2</td>
<td style="text-align: right;">4.1</td>
<td style="text-align: right;">3.8</td>
</tr>
</tbody>
</table>
<p>Table 11: Accuracy with noisy data, for different error levels and tolerances ( $5 \times 5$ matrices).
Addition. Training on noisy data causes no loss in accuracy in the models, so long the ratio between the standard deviation of noise and that of the coefficients is lower than tolerance. Within $5 \%$ tolerance, models</p>
<p>trained with $0.01 \sigma$ and $0.02 \sigma$ noise reach $100 \%$ accuracy, as do models trained with trained with $0.01 \sigma$ noise at $2 \%$ tolerance. Accuracy drops to about $40 \%$ when error levels are approximately equal to tolerance, and to zero once error exceeds tolerance. Model size and encoding have no impact on robustness (see Table 11, 2-layer, 8-head models and Table 27 in Appendix F.3).</p>
<p>Eigenvalues. Models trained with the P1000 encoding prove more robust to noise when computing eigenvalues than when calculating sums. For instance, they achieve $99 \%$ accuracy at $5 \%$ tolerance with noise equal to $0.05 \sigma$, vs only $41 \%$ for addition. As before, model size has no impact on robustness. However, FP15 models prove more difficult to train on noisy data than P1000 (see Table 11 and Table 28 in Appendix F. 3 for additional results, models have 4 layers and 8 heads).</p>
<h1>5 Out-of-domain generalization</h1>
<p>So far, model accuracy was measured on test sets of matrices generated with the same procedure as the training set. In this section, I investigate accuracies on test sets with different distributions. I focus on one task: predicting the eigenvalues of symmetric matrices (with tolerance $2 \%$ ).</p>
<p>Wigner matrices. All models are trained on datasets of random symmetric real matrices, with independent and identically distributed (iid) coefficients sampled from a uniform distribution over $[-A, A]$. These are known as Wigner matrices (see 2.2), and constitute a very common class of random matrices. Yet, matrices with different eigenvalue distributions (and non iid coefficients) appear in important problems. For instance, statistical covariance matrices have all their eigenvalues positive, and the adjacency matrices of scale-free and other non-Erdos-Renyi graphs have centered but non semi-circle distributions of eigenvalues (Preciado \&amp; Rahimian, 2017). We now investigate how models trained on Wigner matrices perform on test sets of matrices with different distributions.</p>
<p>Testing on different distributions. Matrix coefficients in the training set are sampled from $\mathcal{U}[-10,10]$, with standard deviation $\sigma_{t r}=5.77$. First, I consider test sets of Wigner matrices with different standard deviation $\sigma_{t s t}$. Models achieve high accuracy ( $96 \%$ at $2 \%$ tolerance) so long $0.6 \sigma_{t r}&lt;\sigma_{t s t}&lt;\sigma_{t r}$. Out of this range, model accuracy drops: to $54 \%$ for $0.4 \sigma_{t r}$, to $26 \%$ for $1.1 \sigma_{t r}$, to $2 \%$ for $1.3 \sigma_{t r}$ and to $0 \%$ for $0.2 \sigma_{t r}$. Then, the model is tested on sets of matrices with different eigenvalue distributions: positive, uniform, Gaussian and Laplace (generated as per section 2.2), with standard deviation $\sigma_{t r}$ and $0.6 \sigma_{t r}$. With $\sigma_{t s t}=\sigma_{t r}$, the model achieves $26 \%$ accuracy for Laplace, 25 for Gaussian, 19 for uniform, and 0 for positive. With $\sigma_{t s t}=0.6 \sigma_{t r}$, model accuracy is slightly higher, $28,44,60$ and $0 \%$ respectively, but remains low overall. Matrices with positive eigenvalues cannot be predicted at all. These results are summarized in line 1 of Table 12. These results confirm previous observations (Welleck et al., 2021): transformers only generalize to a narrow neighborhood around their training distribution.</p>
<p>Training on different distributions. A common approach to improving out-of-distribution accuracy is to make the training set more diverse. Models trained from a mixture of Wigner matrices with different standard deviation $(A \in[1,100]$, line 2 of Table 12) generalize to Wigner matrices of all standard deviation (which are no longer out-of-distribution), and achieve better performances on the uniform, Gaussian and Laplace test set. But they do not generalize to positive matrices. A model trained on a mixture of Wigner and positive eigenvalues (line 3 of Table 12) can predict positive eigenvalues (now in-domain), but its performance degrades on all other test sets.</p>
<p>Training on mixtures of Wigner and Gaussian eigenvalues, or Wigner and Laplace eigenvalues (lines 4 and 5 of Table 12), achieves high accuracies over all test sets, including the out-of-distribution sets: uniform and positive eigenvalues, and Wigner with low or high standard deviations.</p>
<p>Finally, models trained on matrices with Laplace eigenvalues only, or a mixture of uniform, Gaussian and Laplace eigenvalues (all non-Wigner matrices) achieve $95 \%$ accuracy over all test sets (lines 6 and 7 of Table 12). These results confirm that out-of-distribution generalization is possible, if attention is paid to the training data distribution. They also suggest that Wigner matrices, the default model for random matrices, is not the best choice for training transformers: models trained on Wigner matrices do not generalize out of distribution, whereas models trained on non-Wigner matrices, with non-iid coefficients, do generalize to Wigner matrices.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Train set distribution</th>
<th style="text-align: center;">Test set eigenvalue distribution</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Wigner</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Positive</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Uniform</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Gaussian</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Laplace</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\sigma_{t s t} / \sigma_{t r}$</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Wigner, $\mathrm{A}=10$ (baseline)</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">44</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">26</td>
</tr>
<tr>
<td style="text-align: center;">Wigner, $A \in[1,100]$</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">97</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">65</td>
<td style="text-align: center;">59</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">53</td>
</tr>
<tr>
<td style="text-align: center;">Wigner - Positive</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">88</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: center;">Wigner - Gaussian</td>
<td style="text-align: center;">88</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">93</td>
<td style="text-align: center;">97</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">90</td>
</tr>
<tr>
<td style="text-align: center;">Wigner - Laplace</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">99</td>
</tr>
<tr>
<td style="text-align: center;">Laplace</td>
<td style="text-align: center;">95</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">97</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">94</td>
<td style="text-align: center;">96</td>
</tr>
<tr>
<td style="text-align: center;">Gaussian-Uniform-Laplace</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">97</td>
<td style="text-align: center;">99</td>
</tr>
</tbody>
</table>
<p>Table 12: Out-of-distribution eigenvalue accuracy (tolerance 2\%) for different training distributions. All models have 512 dimensions and 8 attention heads, and use the P1000 encoding.</p>
<h1>6 Related work</h1>
<p>Neural networks for linear algebra. Neural networks that can compute eigenvalues and eigenvectors have been proposed since the early 1990s (Samardzija \&amp; Waterland, 1991; Cichocki \&amp; Unbehauen, 1992; Oja, 1992; Yi et al., 2004), and are still an active field of research (Tang \&amp; Li, 2010; Finol et al., 2019). They leverage the Universal Approximation Theorem (Cybenko, 1989; Hornik, 1991), which states that, under weak conditions on their activation functions, neural networks can approximate any continuous mapping - in this case, the mapping between a matrix and its eigenvalues or vectors. In these works, the network represents a differential equations involving matrix coefficients, which features the eigenvalues in its solution (Brockett, 1991). The matrix to decompose is encoded in the input, and prediction errors are back-propagated until a solution to the differential equation is found, from which eigenvalues can be recovered. Note that these models compute their solutions during training, and must be retrained every time a new matrix is to be processed. Similar techniques have been proposed for other problems of linear algebra (Wang, 1993a;b; Zhang et al., 2008).</p>
<p>Arithmetic with neural networks. Neural networks for binary addition and multiplication have been proposed since the 1990s (Siu \&amp; Roychowdhury, 1992). Since 2015, recurrent architectures have been used, from LSTM (Kalchbrenner et al., 2015) to RNN (Zaremba et al., 2015), Neural Turing Machines (Castellini, 2019) and Neural GPU (Kaiser \&amp; Sutskever, 2015). All authors note that sequential models struggle to generalize out of their training distribution (i.e. to larger numbers), and that their architectures only perform satisfactorily on binary numbers. Neural Arithmetic Logic Units (NALU (Trask et al., 2018) were introduced as a solution to the generalization problem. They can perform exact additions, substractions, multiplications and divisions by constraining the weights of a linear network to remain close to 0,1 or -1 . NALU (and Neural GPU) can extrapolate to numbers far larger than those they were trained on, and could serve as building blocks for larger models. The use of language models for arithmetic and problem solving has been studied by Saxton et al. (2019). Palamas (2017) experiments with modular arithmetic.Nogueira et al. (2021) investigates the limitations of transformers.</p>
<p>Transformers for mathematics. Early applications of transformers to mathematics focused on symbolic computation. Lample \&amp; Charton (2019) used transformers to compute symbolic integrals and solve differential equations. Davis (2019) and Welleck et al. (2021) discuss the limits of their approach, especially with respect to out-of-distribution generalization. Transformers have also been applied to theorem proving (Polu \&amp; Sutskever, 2020; Han et al., 2021), temporal logic (Hahn et al., 2021), and have been proposed as a replacement for the genetic algorithms used in symbolic regression (Biggio et al., 2021; d'Ascoli et al., 2022). In more numerical applications, Charton et al. (2020) use them to predict the numerical properties of differential systems, and Dersy et al. (2022) to simplify formulas involving polylogarithms.</p>
<p>With the advent of large language models (Bommasani et al., 2021), a new line of research focuses on informal mathematics: solving problems of mathematics written in natural language, as a language task (Griffith \&amp; Kalita, 2021; Meng \&amp; Rumshisky, 2019; Cobbe et al., 2021). Lewkowycz et al. (2022) show that a very large</p>
<p>(540 billion parameters) pre-trained transformer can be retrained on a large math corpus to solve grade and high school problems of mathematics. Welleck et al. (2022) apply similar techniques to theorem proving.</p>
<p>Other architectures for mathematics. Graph Neural Networks Scarselli et al. (2009) have been widely used in scientific applications of AI, because of their capacity to integrate problem or domain-specific inductive biases into the network structure. They have been applied to a wide range of mathematical problems, from dynamical systems (Iakovlev et al. 2020) to combinatorial optimization (Cappart et al. 2021) and knot theory (Davies et al. 2021). Vinyals et al. (2015) proposed pointer networks to solve combinatorial problems. Blalock &amp; Guttag (2021) use machine learning techniques to improve existing algorithms for matrix multiplication, in the specific case where one fixed matrix should be multiplied by many others.</p>
<h2>7 Discussion</h2>
<h3>Encodings and architecture</h3>
<p>Our best results are achieved using the P1000 and FP15 encodings. For most problems, P10 is dominated by the more economical P1000, and B1999 never finds its use, between the more compact FP15 and the more efficient P1000. P1000 emerges as a good choice for problems of moderate size, and FP15 when sequences grow long. For the hardest problems, eigenvectors and inversion, asymmetric encodings, FP15 in the encoder and P1000 in the decoder, achieve the best results. I believe that the longer and meaningful P1000 output representation provide better error feedback to the model, facilitating learning, while the FP15 encoding provides a compact representation of the input, which is easier to train.</p>
<p>Experiments also showcase the efficacy of asymmetric architectures, with one layer in either the encoder or decoder. Whether the encoder or the decoder should be shallow is unclear: on the eigenvalue and eigenvector tasks, the 6/1 and 1/6 architecture seem equally efficient. Finally, increasing the number of attention heads seems to help. Most transformers architectures (from Vaswani to BERT) maintain a dimension/head ratio of 64, increased to 96 or more in very large models like GPT-3. For eigenvalue and inversion, using 10 or 12 heads with dimension 512, i.e. a dimension/head ratio between 40 and 50, improves model accuracy.</p>
<h3>Model limitations, scaling to large dimensions</h3>
<p>Most experiments feature dense matrices with 5 to 10 dimensions. Experiments with eigenvalues suggest that larger problems can be solved by training from samples of matrices of variable size, or by using larger models. However, scaling to larger dense matrices will be limited by the length of the sequences a transformer can handle. For quadratic attention models (i.e. most current transformer architectures), sequence length can hardly exceed a few thousand tokens, and the methods proposed in this paper could probably not scale beyond 50 × 50 matrices. Experimenting with transformers with linear or log-linear attention (Zaheer et al. 2021; Wang et al. 2020a; Vyas et al. 2020; Child et al. 2019) is a natural extension of this work. Problems of larger dimension usually feature sparse matrices, and therefore are out of the scope of this work. Extension to sparse matrices constitutes a future research direction.</p>
<h3>Out-of-distribution experiments</h3>
<p>These are our most significant results. They prove that transformers trained on random data can generalize to a wide range of test distributions, provided their training data distribution is chosen with care. Selecting a training distribution can be counter-intuitive. In our experiments, Wigner matrices are the “obvious” random model, but “special” matrices (with non-iid coefficients and Laplace eigenvalues) produce models that better generalize, notably on Wigner matrices. This matches the intuitive idea that we learn more from edge cases than averages.</p>
<h3>Result verification</h3>
<p>One common criticism of deep learning models is that they provide no guarantee on the correctness of their output. This limitation does not apply here, as the model achieves 100% accuracy on basic matrix operations and eigenvalue calculations, and our analysis of failure cases propose a mitigation for the harder problems of eigenvectors and matrix inversion.</p>
<h3>Do the models memoize?</h3>
<p>Transformers are often accused of using the large capacity of their feed-forward networks to memorize training examples, and interpolating between them at inference. Three observations lead me to believe that this is not the case here. First, in section 4.4, the eigenvalues of matrices with more than 9 dimensions cannot be learned from a training set where all matrices have the same size. However, training on a mixture of matrices from 5 × 5 to 20 × 20 allows all dimensions to be learned. If memoization happened, a training set with just one dimension would be easier to train than a mixture. Second, in</p>
<p>appendix F.1, retraining a model on matrices of a different dimension takes significantly less examples than training it from scratch. In a memoization setting, there would be little benefit to retraining. Finally, the results on out-of-domain generalization seem to rule out interpolation. A model trained on matrices with Laplace distributed eigenvalues (which can be positive or negative) will generalize to positive definite matrices, a different ensemble, with very little overlap (almost no Laplace matrix is positive).</p>
<p>Comparison with numerical packages. Given the practical importance of linear algebra, optimized numerical libraries exist for most programming languages and environment. Since my models run in Python, I compare them with Numpy.</p>
<p>Calculating the eigenvalues of a $5 \times 5$ matrix takes 0.5 millisecond on a trained $4 / 1$ layer transformer running pyTorch on a single GPU machine. Matrix inversion takes 1 ms with a $6 / 1$ layer transformer, running pyTorch on a single GPU machine. On the same machine, the optimized algorithms in Numpy (linalg.eigval, and linalg.inv) are faster : 0.07 millisecond for eigenvalues and 0.04 ms for inversion. However, the current code was not designed for speed. An optimized version of the models might achieve inference speeds comparable to those of numerical packages. Note, however, that the memory footprint of a transformer would be considerably larger).</p>
<p>For these two tasks, the algorithms implemented in Numpy and other packages have asymptotic complexity $O\left(n^{3}\right)\left(O\left(n^{2.37}\right)\right.$ for the best known bounds) for $n \times n$ matrices. The attention mechanism of the transformers used in this paper is quadratic in the length of the sequence $\left(O\left(n^{2}\right)\right)$, which makes it $O\left(n^{4}\right)$. Linear attention models could reduce complexity to $O\left(n^{2}\right)$, lower than known algorithms, but the memory requirement of transformers would offset this advantage for large $n$. As stated in the introduction, there is no clear advantage to replace existing algorithms with transformers.</p>
<h1>8 Conclusion.</h1>
<p>I have shown that transformers can learn to perform numerical computations from examples only. I also proved that they can generalize out of domain when their training distribution is carefully selected. This suggests that applications of transformers to mathematics are not limited to symbolic computation, and can cover a broader range of scientific problems. I believe that these results pave the way for wider use of transformers in science.</p>
<h1>References</h1>
<p>Luca Biggio, Tommaso Bendinelli, Alexander Neitz, Aurelien Lucchi, and Giambattista Parascandolo. Neural symbolic regression that scales. arXiv preprint arXiv:2106.06427, 2021.</p>
<p>Davis Blalock and John Guttag. Multiplying matrices without multiplying. arXiv preprint arXiv:2106.10860, 2021 .</p>
<p>Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramèr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the opportunities and risks of foundation models, 2021.</p>
<p>Roger W Brockett. Dynamical systems that sort lists, diagonalize matrices, and solve linear programming problems. Linear Algebra and its applications, 146:79-91, 1991.</p>
<p>Quentin Cappart, Didier Chételat, Elias Khalil, Andrea Lodi, Christopher Morris, and Petar Veličković. Combinatorial optimization and reasoning with graph neural networks, 2021.</p>
<p>Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. arXiv preprint arXiv:2005.12872, 2020.</p>
<p>Jacopo Castellini. Learning numeracy: Binary arithmetic with neural turing machines. arXiv preprint arXiv:1904.02478, 2019.</p>
<p>François Charton, Amaury Hayat, and Guillaume Lample. Learning advanced mathematical computations from examples. arXiv preprint arXiv:2006.06462, 2020.</p>
<p>Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.</p>
<p>Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.</p>
<p>Andrzej Cichocki and Rolf Unbehauen. Neural networks for computing eigenvalues and eigenvectors. Biological Cybernetics, 68(2):155-164, 1992.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Róbert Csordás, Kazuki Irie, and Jürgen Schmidhuber. The neural data router: Adaptive control flow in transformers improves systematic generalization. arXiv preprint arXiv:2110.07732, 2021.</p>
<p>George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and systems, 2(4):303-314, 1989.</p>
<p>Stéphane d’Ascoli, Pierre-Alexandre Kamienny, Guillaume Lample, and François Charton. Deep symbolic regression for recurrent sequences, 2022.</p>
<p>A Davies, P Velickovic, L Buesing, S Blackwell, D Zheng, N Tomasev, R Tanburn, P Battaglia, C Blundell, A Juhasz, et al. Advancing mathematics by guiding human intuition with ai. Nature, 2021.</p>
<p>Ernest Davis. The use of deep learning for symbolic integration: A review of (lample and charton, 2019). arXiv preprint arxiv:1912.05752, 2019.</p>
<p>Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018.</p>
<p>Aurélien Dersy, Matthew D. Schwartz, and Xiaoyuan Zhang. Simplifying polylogarithms with machine learning, 2022.</p>
<p>Linhao Dong, Shuang Xu, and Bo Xu. Speech-transformer: A no-recurrence sequence-to-sequence model for speech recognition. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5884-5888, 2018.</p>
<p>Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2021.</p>
<p>David Finol, Yan Lu, Vijay Mahadevan, and Ankit Srivastava. Deep convolutional neural networks for eigenvalue problems in mechanics. International Journal for Numerical Methods in Engineering, 118(5): $258-275,2019$.</p>
<p>Alex Graves. Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983, 2016 .</p>
<p>Kaden Griffith and Jugal Kalita. Solving arithmetic word problems with transformers and preprocessing of problem text. arXiv preprint arXiv:2106.00893, 2021.</p>
<p>Christopher Hahn, Frederik Schmitt, Jens U. Kreber, Markus N. Rabe, and Bernd Finkbeiner. Teaching temporal logics to neural networks. arXiv preprint arXiv:2003.04218, 2021.</p>
<p>Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward W. Ayers, and Stanislas Polu. Proof artifact co-training for theorem proving with language models. arXiv preprint arxiv:2102.06203, 2021.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780, 1997 .</p>
<p>Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4(2):251-257, 1991.</p>
<p>Valerii Iakovlev, Markus Heinonen, and Harri Lähdesmäki. Learning continuous-time pdes from sparse data with graph neural networks, 2020.</p>
<p>Łukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. arXiv preprint arXiv:1511.08228, 2015.
Nal Kalchbrenner, Ivo Danihelka, and Alex Graves. Grid long short-term memory. arXiv preprint arxiv:1507.01526, 2015.</p>
<p>Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>Donald E. Knuth. The Art of Computer Programming, Volume 2: Seminumerical Algorithms. AddisonWesley, third edition, 1997.</p>
<p>Guillaume Lample and François Charton. Deep learning for symbolic mathematics. arXiv preprint arXiv:1912.01412, 2019.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models, 2022.</p>
<p>Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016.</p>
<p>Madan Lal Mehta. Random Matrices. Academic Press, 3rd edition, 2004.
Yuanliang Meng and Anna Rumshisky. Solving math word problems with double-decoder transformer. arXiv preprint arXiv:1908.10924, 2019.</p>
<p>Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. Investigating the limitations of transformers with simple arithmetic tasks. arXiv preprint arXiv:2102.13019, 2021.</p>
<p>Erkki Oja. Principal components, minor components, and linear neural networks. Neural networks, 5(6): $927-935,1992$.</p>
<p>Theodoros Palamas. Investigating the ability of neural networks to learn simple modular arithmetic. 2017.
Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving. arXiv preprint arXiv:2009.03393, 2020.</p>
<p>Victor M. Preciado and M. Amin Rahimian. Moment-based spectral analysis of random graphs with given expected degrees. arXiv preprint arXiv:1512.03489, 2017.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.</p>
<p>Nikola Samardzija and RL Waterland. A neural network for computing eigenvectors and eigenvalues. Biological Cybernetics, 65(4):211-214, 1991.</p>
<p>David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. arXiv preprint arXiv:1904.01557, 2019.</p>
<p>Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2009.</p>
<p>Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah. Failures of gradient-based deep learning. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 3067-3075. PMLR, 06-11 Aug 2017.</p>
<p>Feng Shi, Chonghan Lee, Mohammad Khairul Bashar, Nikhil Shukla, Song-Chun Zhu, and Vijaykrishnan Narayanan. Transformer-based machine learning for fast sat solvers and logic synthesis. arXiv preprint arXiv:2107.07116, 2021.</p>
<p>Kai-Yeung Siu and Vwani Roychowdhury. Optimal depth neural networks for multiplication and related problems. In S. Hanson, J. Cowan, and C. Giles (eds.), Advances in Neural Information Processing Systems, volume 5. Morgan-Kaufmann, 1992.</p>
<p>Ying Tang and Jianping Li. Another neural network based approach for computing eigenvalues and eigenvectors of real skew-symmetric matrices. Computers \&amp; Mathematics with Applications, 60(5):1385-1392, 2010.</p>
<p>Andrew Trask, Felix Hill, Scott Reed, Jack Rae, Chris Dyer, and Phil Blunsom. Neural arithmetic logic units. arXiv preprint arXiv:1808.00508, 2018.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 6000-6010, 2017.</p>
<p>Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks, 2015.
Apoorv Vyas, Angelos Katharopoulos, and François Fleuret. Fast transformers with clustered attention. arXiv preprint arXiv:2007.04825, 2020.</p>
<p>Jun Wang. A recurrent neural network for real-time matrix inversion. Applied Mathematics and Computation, $55(1): 89-100,1993 a$.</p>
<p>Jun Wang. Recurrent neural networks for solving linear matrix equations. Computers \&amp; Mathematics with Applications, 26(9):23-34, 1993b.</p>
<p>Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020a.</p>
<p>Yongqiang Wang, Abdelrahman Mohamed, Due Le, Chunxi Liu, Alex Xiao, Jay Mahadeokar, Hongzhao Huang, Andros Tjandra, Xiaohui Zhang, Frank Zhang, and et al. Transformer-based acoustic modeling for hybrid speech recognition. 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), May 2020b.</p>
<p>Sean Welleck, Peter West, Jize Cao, and Yejin Choi. Symbolic brittleness in sequence models: on systematic generalization in symbolic mathematics. arXiv preprint arXiv:2109.13986, 2021.</p>
<p>Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, and Yejin Choi. Naturalprover: Grounded mathematical proof generation with language models, 2022.</p>
<p>Zhang Yi, Yan Fu, and Hua Jin Tang. Neural networks based approach for computing eigenvectors and eigenvalues of symmetric matrix. Computers \&amp; Mathematics with Applications, 47(8-9):1155-1164, 2004.</p>
<p>Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences. arXiv preprint arXiv:2007.14062, 2021.</p>
<p>Wojciech Zaremba, Tomas Mikolov, Armand Joulin, and Rob Fergus. Learning simple algorithms from examples, 2015.</p>
<p>Yunong Zhang, Weimu Ma, and Binghuang Cai. From zhang neural network to newton iteration for matrix inversion. IEEE Transactions on Circuits and Systems I: Regular Papers, 56(7):1405-1415, 2008.</p>
<h1>A Number encodings</h1>
<p>Let $x$ be a non-zero real number, it can be represented uniquely as $x=s . m .10^{e}$, with $s \in{-1,1}$, $m \in[100,1000[$ and $e \in \mathbb{Z}$. Rounding $m$ to the nearest integer $n$ (and potentially adjusting for round-up to 1000), we get the base ten, floating-point representation of $x$, with three significant digits:</p>
<p>$$
x \approx s . m .10^{e},(s, m, e) \in{-1,1} \times{100, \ldots, 999} \times \mathbb{Z}
$$</p>
<p>By convention, 0 is encoded as $+0.10^{0}$. All encodings are possible representations of the triplets $(s, m, e)$. In this paper, $e$ is restricted to $[-100,100]$, and $m$ to $[100,999]$.
In base N positional encoding, $s$ (the sign) and $e$ (the exponent) are encoded as unique tokens: + or - for $s$, and one token from E-100 to E100 for $e$. The mantissa, $m$, is encoded as the representation of $m$ in base N (e.g. binary representation if $N=2$, decimal representation if $N=10$ ), a sequence of $\left\lceil\log <em N="N">{N}(1000)\right\rceil$ tokens from 0 to N-1. Overall, a number will be encoded as a sequence of $\left\lceil\log </em>(1000)\right\rceil+2$ tokens, from a vocabulary of $202+N$ tokens.
For instance, $x=e^{\pi} \approx 23.14069$, will be represented by $+231.10^{-1}$, and encoded in P10 (base 10 positional) as the sequence $[+, 2,3,1, \mathrm{E}-1]$, and in P1000 (base 1000 positional) as $[+, 231, \mathrm{E}-1] . x=-0.5$ will be represented as $-500.10^{-3}$, and encoded in P10 as $[-, 5,0,0, \mathrm{E}-3]$, and in P1000 as $[-, 500, \mathrm{E}-3]$. Other bases N could be considered, as well as different bases for the exponent, and different lengths for the mantissa. This paper uses two positional encodings: P10, which encodes numbers rounded to three significant digits, with absolute value in $\left[10^{-100}, 10^{101}\right]$, as sequences of 5 tokens, using a vocabulary of 213 tokens ( 10 digits, 2 signs, and 202 values of the exponent), and P1000 which encodes numbers as sequences of 3 tokens, with a vocabulary of 1104 .</p>
<p>Balanced base $2 a+1$ uses digits between $-a$ and $a$ (Knuth, 1997). For instance, in balanced base 11, digits range from -5 to 5 . An every day example of a balanced base can be found in the way we state the hour as "twenty to two", or "twenty past two". Setting $a$ to 999 defines B1999, which encodes the sign and mantissa as a single token between -999 and 999, and the exponent as in P10 and P1000. Numbers are encoded on two tokens, with a vocabulary of 2004.</p>
<p>For an even more compact representation, floating point numbers can be encoded as unique tokens, by rewriting any number as $x=m 10^{b}$, with $m \in[-999,999], b \in[-(p+2) / 2,(p+2) / 2]$ and $p+2=0,[2]$, and representing it as the unique token $\mathrm{FPm}, \mathrm{b}$. This allows to represent numbers with 3 significant digits and a dynamic range of $10^{p+2}$, using a vocabulary of $1800(p+3)$ tokens. Setting $p=14$, this paper introduces FP15, which encodes numbers as unique tokens with a vocabulary of 30,000 .</p>
<h2>B $L^{1}, L^{2}$ and $L^{\infty}$ norms for evaluation</h2>
<p>The accuracy of trained models is evaluated by decoding their predictions and verifying that they approximate the correct solution up to a fixed tolerance $\tau$. In the general case, if the model predicts a sequence $S_{P}$, and the solution of the problem is $O$, the prediction is considered to be correct if $S_{P}$ can be decoded into a matrix $P$ and</p>
<p>$$
|P-O|&lt;\tau|O|
$$</p>
<p>For eigenvalue decomposition, the solution is correct if it can be decomposed as a pair $(Q, D)$ that verifies: $\left|Q^{T} I Q-D\right|&lt;\tau|D|$ ( $I$ the input matrix), i.e. that $Q$ diagonalizes $I$ into $D$. For singular value decomposition, the solution must verify $|U I V-S|&lt;\tau|S|$, and for matrix inversion $|P I-I d|&lt;\tau|I d|=\tau$. The matrix norm $L^{1}:|A|=\sum_{i, j}\left|a_{i, j}\right|$, for $A=\left(a_{i, j}\right)$ is used throughout this paper. This section discusses its advantage over two other possible norms: $L^{2}\left(|A|=\sum_{i, j} a_{i, j}^{2}\right)$, and $L^{\infty}\left(|A|=\max <em i_="i," j="j">{i, j}\left|a</em>\right|\right)$.
Using $L^{1}$ norm in equation 1 amounts to comparing the average absolute error on the predicted coefficients $(P-O)$ to the average absolute value of coefficients of $O . L^{2}$ compares the squared values and errors, and $L^{\infty}$ the largest absolute error to the largest coefficient in $|O|$. Compared to $L^{1}, L^{2}$ and $L^{\infty}$ (Max) emphasize large absolute errors, and large absolute coefficients of $O$. The impact of the norm varies from one problem to another. Figure 1 presents learning curves using the three norms for our best models, on different problems.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Learning accuracies for different problems measured with norms $L^{1}, L^{2}$ and $L^{\infty}$ (Max).</p>
<p>For basic arithmetic operations (transposition, addition, multiplication), there is little difference between $L^{1}$ and $L^{2}$ accuracies, and no reason to prefer one over the other for model evaluation. $L^{\infty}$ provides a stricter criterion for accuracy, but it has little practical impact.</p>
<p>For eigenvalue and singular value problems, $L^{2}$ accuracies reach a high value early during training, long before the model begins to learn according to the other norms. This is due to the fact that the eigenvalues of Wigner matrices tend to be regularly spaced over the interval $[-2 \sigma, 2 \sigma]$ $(\sigma=\sqrt{n} s$ with $s$ the standard deviation of coefficients and $n$ the dimension of the matrix). This means that the model can predict the largest absolute eigenvalues from the distribution of the coefficients, which can be computed from the dataset. For this reason, $L^{2}$ accuracy is not a good evaluation metric for the eigenvalue or singular value problem. This is particularly clear in the $10 \times 10$ case: transformers struggle with such matrices, and $L^{1}$ and $L^{\infty}$</p>
<p>accuracies remain very low even after a thousand epochs ( 300 million examples), but $L^{2}$ accuracy is close to $100 \%$ since the beginning of training.</p>
<p>A similar phenomenon takes place for eigenvector calculations: $L^{2}$ and $L^{\infty}$ accuracy rise steeply, long before the model begins to learn according to the $L^{1}$ norm. In this task, the model predicts both the eigenvalues and the coefficients of the matrix of eigenvectors $Q$. Because $Q$ is orthogonal, its coefficients will usually have small absolute values, compared to those of eigenvalues. As training goes on, the largest eigenvalue is first predicted, which causes the rise in the $L^{2}$ curve, then other eigenvalues are, which cause the rise in the $L^{\infty}$ curve, and finally the eigenvectors are correctly predicted, which is depicted in the (much slower) rise of the $L^{1}$ curve. Again, using $L^{2}$ or $L^{\infty}$ amounts to evaluating an easier problem (computing eigenvalues) than the one we are currently solving (eigen decomposition). These observations motivate the choice of $L^{1}$ as our evaluation norm.</p>
<h1>C Impact of number precision</h1>
<p>In all experiments, matrix coefficients are rounded to three significant digits. Three-digit precision was selected in order to keep the size of the FP15 vocabulary manageable. With three-digit precision, FP15 uses a vocabulary of 30000 words, with four digits, it would use 300000 words, and would be difficult to train on the small transformers this paper focuses on.</p>
<p>In this section, I investigate the impact of number precision on $10 \times 10$ matrix addition and $5 \times 5$ eigenvalue computation. Random matrices are rounded to two, three and four significant digits, using the P100, P1000 and P10000 encoding (i.e. numbers encoded on three tokens, mantissa in base 100, 1000 and 10000). I train transformers with 512 dimensions, 8 attention heads, and $2 / 2$ layers (addition) or $4 / 1$ layers (eigenvalues). Results for $10,5,2,1,0.5$ and $0.1 \%$ tolerance are presented in tables 13 and 14 .</p>
<p>On the addition task, rounding precision has no impact for tolerances larger than $1 \%$ : all models achieve close to $100 \%$ accuracy. At 0.5 tolerance, models trained with 2-digit precision are penalised by rounding error, but there is no significant difference between 3 and 4-digit precision. However, 4-digit models need significantly more examples to train: whereas 2 and 3-digit models achieve $99 \%$ accuracy at $5 \%$ tolerance after 5 million examples, a 4-digit model need 21 millions to reach $99 \%$ accuracy.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Tolerance</th>
<th style="text-align: center;">10</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">0.5</th>
<th style="text-align: center;">0.1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">2-digit precision, P100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">60.8</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">3-digit precision, P1000</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">98.1</td>
<td style="text-align: center;">97.2</td>
<td style="text-align: center;">66.4</td>
</tr>
<tr>
<td style="text-align: left;">4-digit precision, P10000</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">17.3</td>
</tr>
</tbody>
</table>
<p>Table 13: Accuracy of $10 \times 10$ matrix addition, for different precision and tolerance, after training on 30 million examples.</p>
<p>For eigenvalues, all models achieve $100 \%$ accuracy at $5 \%$ tolerance. At lower tolerance ( 1 or $0.5 \%$ ), accuracy increases with precision. On this task, learning speed is comparable for all models: 2, 3 and 4-digit models achieve $99 \%$ accuracy (with $5 \%$ tolerance) after 9,8 and 7 million examples respectively. Overall, number precision has a marginal effect on accuracy.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Tolerance</th>
<th style="text-align: center;">10</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">0.5</th>
<th style="text-align: center;">0.1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">2-digit precision, P100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">94.3</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">24.1</td>
</tr>
<tr>
<td style="text-align: left;">3-digit precision, P1000</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">98.2</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">9.8</td>
</tr>
<tr>
<td style="text-align: left;">4-digit precision, P10000</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">1.4</td>
</tr>
</tbody>
</table>
<p>Table 14: Accuracy of $5 \times 5$ eigenvalue calculation, for different precision and tolerance, after training on 60 million examples.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Learning curves for different problems. All problems except addition use $5 \times 5$ matrices. All models have 512 dimensions and $8 / 8$ heads (except when mentioned in the legend). Inversion models have $6 / 1$ layers. Epochs correspond to 300,000 training examples. Test loss is cross-entropy.</p>            </div>
        </div>

    </div>
</body>
</html>
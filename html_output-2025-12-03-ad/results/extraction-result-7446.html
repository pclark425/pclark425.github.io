<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7446 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7446</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7446</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-267938333</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.15833v1.pdf" target="_blank">Prompt Perturbation Consistency Learning for Robust Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have demonstrated impressive performance on a number of natural language processing tasks, such as question answering and text summarization. However, their performance on sequence labeling tasks such as intent classification and slot filling (IC-SF), which is a central component in personal assistant systems, lags significantly behind discriminative models. Furthermore, there is a lack of substantive research on robustness of LLMs to various perturbations in the input prompts. The contributions of this paper are three-fold. First, we show that fine-tuning sufficiently large LLMs can produce IC-SF performance comparable to discriminative models. Next, we systematically analyze the performance deterioration of those fine-tuned models due to three distinct yet relevant types of input perturbations - oronyms, synonyms, and paraphrasing. Finally, we propose an efficient mitigation approach, Prompt Perturbation Consistency Learning (PPCL), which works by regularizing the divergence between losses from clean and perturbed samples. Our experiments show that PPCL can recover on an average 59% and 69% of the performance drop for IC and SF tasks, respectively. Furthermore, PPCL beats data augmentation approach while using ten times fewer augmented data samples.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7446.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7446.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sentinel-based structured prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sentinel-based structured prompt format (structured fields + sentinel tokens for slot labeling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt style that organizes input and output into a structured template (domain, intent, slots, arguments) and uses sentinel markers to align tokens with slot labels, reducing redundant repetition and improving token-to-slot association during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-7b (instruction fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA family decoder-only transformer, instruction fine-tuned (supervised fine-tuning) to generate structured IC-SF outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Intent Classification and Slot Filling (IC-SF) on MASSIVE</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given an utterance, generate domain, intent, and token-level slot labels (structured hypothesis); evaluate intent accuracy and slot F1.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Instruction-style generation: input utterance with a structured target output template; outputs include sentinel markers mapping tokens to slot tags.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Structured layout (intent in domain, slots with arguments), use of sentinel tokens for tokens/indices so targets omit repeating input tokens; supervised instruction fine-tuning with paired Prompt(X,Y).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Intent accuracy; Slot F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>88.01% intent accuracy; 80.45 F1 (slot)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Simple + Tag: 88.68% intent accuracy; 72.91 F1 (slot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Slot F1 +7.54% absolute improvement versus Simple+Tag; Intent accuracy −0.67% absolute (negligible)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Instruction fine-tuning (SFT) on in-domain training data, sentinel-based structured prompt used at training and inference.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Perturbation Consistency Learning for Robust Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7446.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7446.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sentinel-based (simple) vs Tag-only SF formats</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sentinel-based slot-format (simple prompt) compared to tag-only slot-format</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Slot-filling prompt variants where 'tag-only' requires the model to output token-aligned labels implicitly, while 'sentinel-based' uses explicit sentinel indices to associate tokens with labels; sentinel-based formats lead to substantially higher slot-F1 across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-7b (instruction fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer fine-tuned to generate slot labels as text sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Intent Classification and Slot Filling (IC-SF) on ATIS / SNIPS / MASSIVE</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate intent and token-level slot labels; measure intent accuracy and slot F1.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Simple prompt formats with different SF target styles: (1) Tag-only (output sequence of tags), (2) Sentinel + Tag (output tag labels keyed by sentinel token indices), (3) Extractive Sentinel + Tag (only output sentinel-tag pairs for relevant tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style (slot-label representation)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Comparisons across datasets use the same input utterance prompt but vary the target representation; no few-shot examples. Examples: Tag-only (e.g., 'Slots: Other Other time date ...'), Sentinel + Tag (e.g., '<4>time <5>time ...').</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Slot F1 (primary), Intent accuracy (secondary)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ATIS: Sentinel (Simple + Sentinel Tag) => 98.21% intent, 94.26 F1 (slot); SNIPS: 98.14% intent, 94.51 F1 (slot); MASSIVE: 87.51% intent, 75.36 F1 (slot) for Simple+SentinelTag</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Tag-only (Simple + Tag) => ATIS: 98.43% intent, 86.04 F1; SNIPS: 97.85% intent, 89.11 F1; MASSIVE: 88.68% intent, 72.91 F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Slot F1 improvements: ATIS +8.22% absolute; SNIPS +5.40% absolute; MASSIVE +2.45% absolute (sentinel vs tag-only)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Supervised instruction fine-tuning; same training data; sentinel tokens added to training targets to simplify token-tracking during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Perturbation Consistency Learning for Robust Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7446.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7446.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Structured prompt (field-oriented)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structured prompt format (intent in domain, slots with arguments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt layout that explicitly lists intent in domain and slots with their arguments (a more organized target representation than a simple sequential list), facilitating structured output generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-7b (instruction fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer, instruction fine-tuned to map input utterances to structured templates.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>IC-SF on MASSIVE (and other NLU datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate domain/intent and slot-argument mappings for utterances; evaluate with intent accuracy and slot F1.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Structured prompt: explicitly labeled fields (Utterance, Intent in Domain, Slots with Arguments) for targets.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style (structured vs flat)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Targets present intent and slots in a human-readable, labeled schema (reduces ambiguity in decoding compared to flat tag lists); used for instruction fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Intent accuracy; Slot F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>MASSIVE (Structured + Sentinel): 88.01% intent, 80.45 F1 (slot) (best slot F1 among structured variants)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Simple + Tag: 88.68% intent, 72.91 F1 (slot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Slot F1 +7.54% absolute (versus Simple + Tag baseline when sentinel included in structured form vs simple tag-only baseline), intent practically unchanged</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Instruction fine-tuning with structured target templates; sentinel markers optionally used to improve token alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Perturbation Consistency Learning for Robust Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7446.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7446.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot vs Few-shot prompting (GPT3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot and Few-shot prompting with GPT-3.5 (10-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of GPT-3.5 performance when given no examples (zero-shot) versus 10 in-context examples (few-shot) shows non-trivial gains from providing examples but overall performance is still much lower than SFT LLMs on IC-SF.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3.5 series (API), large decoder-only model evaluated with zero-shot and few-shot in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified (GPT-3.5 family)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>IC-SF on MASSIVE (intent accuracy reported)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predict intent label from utterance using zero-shot or few-shot in-context prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>In-context prompting: zero-shot (no examples) and few-shot (10 examples) natural-language prompts; no supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style (few-shot vs zero-shot / in-context learning)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Few-shot used 10 exemplar demonstrations in the prompt; ordering and example selection not exhaustively varied in this paper (standard few-shot protocol).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Intent accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Zero-shot: 60.39% intent accuracy; Few-shot (10 examples): 67.18% intent accuracy (MASSIVE)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+6.79% absolute improvement when moving from zero-shot to 10-shot (GPT-3.5 on MASSIVE)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>API-based zero/few-shot evaluation (10 examples in few-shot); no fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Perturbation Consistency Learning for Robust Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7446.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7446.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Oronym perturbation impact</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Oronym (phonetic/homophonic) perturbation effect on prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Replacing words/phrases with phonetically similar but possibly semantically different tokens (oronym) substantially degrades LLM performance on IC-SF tasks, especially slot filling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-7b (instruction fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer fine-tuned for IC-SF generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>IC-SF on MASSIVE</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate effect of meaning-preserving or lightly altered (phonetic) perturbations on intent accuracy and slot F1.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Input utterance textual perturbation: replace up to 3 words with phonetically similar alternatives (using pronouncing corpus); prompts otherwise unchanged.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>input perturbation / prompt presentation</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Oronyms generated with NLTK pronouncing corpus; stop words/key labels not substituted; filtered with BERTScore >= 0.85 for semantic similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Performance Drop Rate (PDR) computed relative to clean test set; also raw intent accuracy and slot F1 pre/post perturbation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Clean: 89.18% intent, 79.35 F1 (slot) → Perturbed (oronym): 74.31% intent, 47.01 F1 (slot)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Clean: 89.18% intent, 79.35 F1 (slot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>IC-PDR = 16.68% relative drop; SF-PDR = 40.75% relative drop (LLaMA-7b+SFT on MASSIVE)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Supervised fine-tuned LLaMA-7b; perturb up to 3 tokens; semantic filtering (BERTScore >= 0.85).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Perturbation Consistency Learning for Robust Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7446.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7446.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Synonym perturbation impact</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Synonym replacement perturbation effect on prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Replacing content words with synonyms (WordNet-based) causes measurable declines in IC-SF performance, with varying magnitude across intent and slot tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-7b (instruction fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer fine-tuned for generation of structured IC-SF outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>IC-SF on MASSIVE</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assess robustness to synonym replacements in user utterances.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Input perturbation via WordNet synonym substitution (up to 3 words), with BERTScore filtering (>= 0.85). Prompts unchanged.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>input perturbation / prompt presentation</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Synonym replacement avoids substituting key stop words and label tokens; maintains utterance length and ordering when possible.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Performance Drop Rate (PDR); raw intent accuracy and slot F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Clean: 89.23% intent, 80.75 F1 (slot) → Perturbed (synonym): 76.79% intent, 72.90 F1 (slot)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Clean: 89.23% intent, 80.75 F1 (slot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>IC-PDR = 13.94% relative drop; SF-PDR = 9.72% relative drop (LLaMA-7b+SFT on MASSIVE)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Supervised fine-tuned LLaMA-7b; WordNet synonyms, up to 3 token changes, semantic filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Perturbation Consistency Learning for Robust Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7446.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7446.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Paraphrase perturbation impact</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Paraphrase perturbation (LLM-generated) effect on prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paraphrasing the input utterance (high-quality LLM paraphrases, filtered for semantic similarity) causes moderate-to-large drops in IC and SF performance depending on the model and dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-7b (instruction fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer fine-tuned via instruction fine-tuning to map utterances to structured outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>IC-SF on MASSIVE</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Measure robustness when utterances are rephrased while preserving meaning (paraphrasing).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Input paraphrases generated by a paraphrase-specialized LLM and filtered by BERTScore >= 0.85; targets aligned via automatic slot label projection when length/order changes.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>input perturbation / prompt presentation</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Paraphrases can change length and word order; for paraphrase L_JS loss averaged distributions used in PPCL (paper-specific detail).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Performance Drop Rate (PDR); raw intent accuracy and slot F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Clean: 82.78% intent, 81.58 F1 (slot) → Perturbed (paraphrase): 80.21% intent, 68.41 F1 (slot) (LLaMA-7b+SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Clean: 82.78% intent, 81.58 F1 (slot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>IC-PDR = 8.62% relative drop; SF-PDR = 16.14% relative drop (LLaMA-7b+SFT on MASSIVE)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Supervised fine-tuned LLaMA-7b; paraphrases generated and filtered via BERTScore >= 0.85; automatic slot-label projection for paraphrases.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Perturbation Consistency Learning for Robust Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7446.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7446.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PPCL (Prompt Perturbation Consistency Learning)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt Perturbation Consistency Learning (PPCL): JS divergence regularization between clean and perturbed outputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training objective augmentation that adds a Jensen-Shannon divergence term between output token probability distributions for clean and perturbed inputs, combined with supervised losses on both, to enforce consistent predictions and improve robustness to input perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-7b (instruction fine-tuned + PPCL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA-7b fine-tuned first with supervised instruction SFT, then further fine-tuned with PPCL objective combining cross-entropy on clean/perturbed samples and JS divergence regularizer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>IC-SF on MASSIVE (mitigation of oronym/synonym/paraphrase perturbation effects)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improve robustness so that model produces consistent structured outputs for clean and perturbed versions of the same utterance.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Training-time augmentation + consistency regularization: supply paired (clean, perturbed) inputs and minimize CE(clean)+CE(perturbed)+λ * average JS(output distributions).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>training objective / prompt presentation (paired clean/perturbed)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>JS divergence computed per-token (or averaged output distribution for paraphrases); loss weights (λ1, λ2, λ3) tuned per dataset; uses one perturbed sample per clean by default (small augmentation size).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Performance Drop Rate (PDR) after mitigation and listed recovery percentage</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Oronyms (MASSIVE, LLaMA-7b): PPCL yields IC-PDR 8.74% and SF-PDR 15.41% after mitigation</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline (Lc only) Oronyms: IC-PDR 16.67%, SF-PDR 40.75% (LLaMA-7b+SFT without PPCL)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>IC-PDR reduced by 7.93 percentage points (47% of drop recovered); SF-PDR reduced by 25.34 percentage points (62% of drop recovered). Reported average recovery across perturbations: 59% IC recovery, 69% SF recovery.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Fine-tune SFT model for 2 additional epochs with paired clean and perturbed inputs; default augmentation: one perturbed sample per clean; compared with multi-sample augmentation (10x more samples).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Perturbation Consistency Learning for Robust Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Do prompt-based models really understand the meaning of their prompts? <em>(Rating: 2)</em></li>
                <li>Rethinking the role of demonstrations: What makes in-context learning work? <em>(Rating: 2)</em></li>
                <li>Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. <em>(Rating: 2)</em></li>
                <li>On robustness of prompt-based semantic parsing with large pre-trained language model: An empirical study on codex <em>(Rating: 2)</em></li>
                <li>The unreliability of explanations in few-shot prompting for textual reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7446",
    "paper_id": "paper-267938333",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "Sentinel-based structured prompt",
            "name_full": "Sentinel-based structured prompt format (structured fields + sentinel tokens for slot labeling)",
            "brief_description": "A prompt style that organizes input and output into a structured template (domain, intent, slots, arguments) and uses sentinel markers to align tokens with slot labels, reducing redundant repetition and improving token-to-slot association during generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-7b (instruction fine-tuned)",
            "model_description": "LLaMA family decoder-only transformer, instruction fine-tuned (supervised fine-tuning) to generate structured IC-SF outputs.",
            "model_size": "7B",
            "task_name": "Intent Classification and Slot Filling (IC-SF) on MASSIVE",
            "task_description": "Given an utterance, generate domain, intent, and token-level slot labels (structured hypothesis); evaluate intent accuracy and slot F1.",
            "problem_format": "Instruction-style generation: input utterance with a structured target output template; outputs include sentinel markers mapping tokens to slot tags.",
            "format_category": "prompt style",
            "format_details": "Structured layout (intent in domain, slots with arguments), use of sentinel tokens for tokens/indices so targets omit repeating input tokens; supervised instruction fine-tuning with paired Prompt(X,Y).",
            "performance_metric": "Intent accuracy; Slot F1",
            "performance_value": "88.01% intent accuracy; 80.45 F1 (slot)",
            "baseline_performance": "Simple + Tag: 88.68% intent accuracy; 72.91 F1 (slot)",
            "performance_change": "Slot F1 +7.54% absolute improvement versus Simple+Tag; Intent accuracy −0.67% absolute (negligible)",
            "experimental_setting": "Instruction fine-tuning (SFT) on in-domain training data, sentinel-based structured prompt used at training and inference.",
            "statistical_significance": null,
            "uuid": "e7446.0",
            "source_info": {
                "paper_title": "Prompt Perturbation Consistency Learning for Robust Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Sentinel-based (simple) vs Tag-only SF formats",
            "name_full": "Sentinel-based slot-format (simple prompt) compared to tag-only slot-format",
            "brief_description": "Slot-filling prompt variants where 'tag-only' requires the model to output token-aligned labels implicitly, while 'sentinel-based' uses explicit sentinel indices to associate tokens with labels; sentinel-based formats lead to substantially higher slot-F1 across datasets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-7b (instruction fine-tuned)",
            "model_description": "Decoder-only transformer fine-tuned to generate slot labels as text sequences.",
            "model_size": "7B",
            "task_name": "Intent Classification and Slot Filling (IC-SF) on ATIS / SNIPS / MASSIVE",
            "task_description": "Generate intent and token-level slot labels; measure intent accuracy and slot F1.",
            "problem_format": "Simple prompt formats with different SF target styles: (1) Tag-only (output sequence of tags), (2) Sentinel + Tag (output tag labels keyed by sentinel token indices), (3) Extractive Sentinel + Tag (only output sentinel-tag pairs for relevant tokens).",
            "format_category": "prompt style (slot-label representation)",
            "format_details": "Comparisons across datasets use the same input utterance prompt but vary the target representation; no few-shot examples. Examples: Tag-only (e.g., 'Slots: Other Other time date ...'), Sentinel + Tag (e.g., '&lt;4&gt;time &lt;5&gt;time ...').",
            "performance_metric": "Slot F1 (primary), Intent accuracy (secondary)",
            "performance_value": "ATIS: Sentinel (Simple + Sentinel Tag) =&gt; 98.21% intent, 94.26 F1 (slot); SNIPS: 98.14% intent, 94.51 F1 (slot); MASSIVE: 87.51% intent, 75.36 F1 (slot) for Simple+SentinelTag",
            "baseline_performance": "Tag-only (Simple + Tag) =&gt; ATIS: 98.43% intent, 86.04 F1; SNIPS: 97.85% intent, 89.11 F1; MASSIVE: 88.68% intent, 72.91 F1",
            "performance_change": "Slot F1 improvements: ATIS +8.22% absolute; SNIPS +5.40% absolute; MASSIVE +2.45% absolute (sentinel vs tag-only)",
            "experimental_setting": "Supervised instruction fine-tuning; same training data; sentinel tokens added to training targets to simplify token-tracking during generation.",
            "statistical_significance": null,
            "uuid": "e7446.1",
            "source_info": {
                "paper_title": "Prompt Perturbation Consistency Learning for Robust Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Structured prompt (field-oriented)",
            "name_full": "Structured prompt format (intent in domain, slots with arguments)",
            "brief_description": "A prompt layout that explicitly lists intent in domain and slots with their arguments (a more organized target representation than a simple sequential list), facilitating structured output generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-7b (instruction fine-tuned)",
            "model_description": "Decoder-only transformer, instruction fine-tuned to map input utterances to structured templates.",
            "model_size": "7B",
            "task_name": "IC-SF on MASSIVE (and other NLU datasets)",
            "task_description": "Generate domain/intent and slot-argument mappings for utterances; evaluate with intent accuracy and slot F1.",
            "problem_format": "Structured prompt: explicitly labeled fields (Utterance, Intent in Domain, Slots with Arguments) for targets.",
            "format_category": "prompt style (structured vs flat)",
            "format_details": "Targets present intent and slots in a human-readable, labeled schema (reduces ambiguity in decoding compared to flat tag lists); used for instruction fine-tuning.",
            "performance_metric": "Intent accuracy; Slot F1",
            "performance_value": "MASSIVE (Structured + Sentinel): 88.01% intent, 80.45 F1 (slot) (best slot F1 among structured variants)",
            "baseline_performance": "Simple + Tag: 88.68% intent, 72.91 F1 (slot)",
            "performance_change": "Slot F1 +7.54% absolute (versus Simple + Tag baseline when sentinel included in structured form vs simple tag-only baseline), intent practically unchanged",
            "experimental_setting": "Instruction fine-tuning with structured target templates; sentinel markers optionally used to improve token alignment.",
            "statistical_significance": null,
            "uuid": "e7446.2",
            "source_info": {
                "paper_title": "Prompt Perturbation Consistency Learning for Robust Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Zero-shot vs Few-shot prompting (GPT3.5)",
            "name_full": "Zero-shot and Few-shot prompting with GPT-3.5 (10-shot)",
            "brief_description": "Comparison of GPT-3.5 performance when given no examples (zero-shot) versus 10 in-context examples (few-shot) shows non-trivial gains from providing examples but overall performance is still much lower than SFT LLMs on IC-SF.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "OpenAI GPT-3.5 series (API), large decoder-only model evaluated with zero-shot and few-shot in-context examples.",
            "model_size": "not specified (GPT-3.5 family)",
            "task_name": "IC-SF on MASSIVE (intent accuracy reported)",
            "task_description": "Predict intent label from utterance using zero-shot or few-shot in-context prompting.",
            "problem_format": "In-context prompting: zero-shot (no examples) and few-shot (10 examples) natural-language prompts; no supervised fine-tuning.",
            "format_category": "prompt style (few-shot vs zero-shot / in-context learning)",
            "format_details": "Few-shot used 10 exemplar demonstrations in the prompt; ordering and example selection not exhaustively varied in this paper (standard few-shot protocol).",
            "performance_metric": "Intent accuracy",
            "performance_value": "Zero-shot: 60.39% intent accuracy; Few-shot (10 examples): 67.18% intent accuracy (MASSIVE)",
            "baseline_performance": null,
            "performance_change": "+6.79% absolute improvement when moving from zero-shot to 10-shot (GPT-3.5 on MASSIVE)",
            "experimental_setting": "API-based zero/few-shot evaluation (10 examples in few-shot); no fine-tuning.",
            "statistical_significance": null,
            "uuid": "e7446.3",
            "source_info": {
                "paper_title": "Prompt Perturbation Consistency Learning for Robust Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Oronym perturbation impact",
            "name_full": "Oronym (phonetic/homophonic) perturbation effect on prompting",
            "brief_description": "Replacing words/phrases with phonetically similar but possibly semantically different tokens (oronym) substantially degrades LLM performance on IC-SF tasks, especially slot filling.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-7b (instruction fine-tuned)",
            "model_description": "Decoder-only transformer fine-tuned for IC-SF generation.",
            "model_size": "7B",
            "task_name": "IC-SF on MASSIVE",
            "task_description": "Evaluate effect of meaning-preserving or lightly altered (phonetic) perturbations on intent accuracy and slot F1.",
            "problem_format": "Input utterance textual perturbation: replace up to 3 words with phonetically similar alternatives (using pronouncing corpus); prompts otherwise unchanged.",
            "format_category": "input perturbation / prompt presentation",
            "format_details": "Oronyms generated with NLTK pronouncing corpus; stop words/key labels not substituted; filtered with BERTScore &gt;= 0.85 for semantic similarity.",
            "performance_metric": "Performance Drop Rate (PDR) computed relative to clean test set; also raw intent accuracy and slot F1 pre/post perturbation",
            "performance_value": "Clean: 89.18% intent, 79.35 F1 (slot) → Perturbed (oronym): 74.31% intent, 47.01 F1 (slot)",
            "baseline_performance": "Clean: 89.18% intent, 79.35 F1 (slot)",
            "performance_change": "IC-PDR = 16.68% relative drop; SF-PDR = 40.75% relative drop (LLaMA-7b+SFT on MASSIVE)",
            "experimental_setting": "Supervised fine-tuned LLaMA-7b; perturb up to 3 tokens; semantic filtering (BERTScore &gt;= 0.85).",
            "statistical_significance": null,
            "uuid": "e7446.4",
            "source_info": {
                "paper_title": "Prompt Perturbation Consistency Learning for Robust Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Synonym perturbation impact",
            "name_full": "Synonym replacement perturbation effect on prompting",
            "brief_description": "Replacing content words with synonyms (WordNet-based) causes measurable declines in IC-SF performance, with varying magnitude across intent and slot tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-7b (instruction fine-tuned)",
            "model_description": "Decoder-only transformer fine-tuned for generation of structured IC-SF outputs.",
            "model_size": "7B",
            "task_name": "IC-SF on MASSIVE",
            "task_description": "Assess robustness to synonym replacements in user utterances.",
            "problem_format": "Input perturbation via WordNet synonym substitution (up to 3 words), with BERTScore filtering (&gt;= 0.85). Prompts unchanged.",
            "format_category": "input perturbation / prompt presentation",
            "format_details": "Synonym replacement avoids substituting key stop words and label tokens; maintains utterance length and ordering when possible.",
            "performance_metric": "Performance Drop Rate (PDR); raw intent accuracy and slot F1",
            "performance_value": "Clean: 89.23% intent, 80.75 F1 (slot) → Perturbed (synonym): 76.79% intent, 72.90 F1 (slot)",
            "baseline_performance": "Clean: 89.23% intent, 80.75 F1 (slot)",
            "performance_change": "IC-PDR = 13.94% relative drop; SF-PDR = 9.72% relative drop (LLaMA-7b+SFT on MASSIVE)",
            "experimental_setting": "Supervised fine-tuned LLaMA-7b; WordNet synonyms, up to 3 token changes, semantic filtering.",
            "statistical_significance": null,
            "uuid": "e7446.5",
            "source_info": {
                "paper_title": "Prompt Perturbation Consistency Learning for Robust Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Paraphrase perturbation impact",
            "name_full": "Paraphrase perturbation (LLM-generated) effect on prompting",
            "brief_description": "Paraphrasing the input utterance (high-quality LLM paraphrases, filtered for semantic similarity) causes moderate-to-large drops in IC and SF performance depending on the model and dataset.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-7b (instruction fine-tuned)",
            "model_description": "Decoder-only transformer fine-tuned via instruction fine-tuning to map utterances to structured outputs.",
            "model_size": "7B",
            "task_name": "IC-SF on MASSIVE",
            "task_description": "Measure robustness when utterances are rephrased while preserving meaning (paraphrasing).",
            "problem_format": "Input paraphrases generated by a paraphrase-specialized LLM and filtered by BERTScore &gt;= 0.85; targets aligned via automatic slot label projection when length/order changes.",
            "format_category": "input perturbation / prompt presentation",
            "format_details": "Paraphrases can change length and word order; for paraphrase L_JS loss averaged distributions used in PPCL (paper-specific detail).",
            "performance_metric": "Performance Drop Rate (PDR); raw intent accuracy and slot F1",
            "performance_value": "Clean: 82.78% intent, 81.58 F1 (slot) → Perturbed (paraphrase): 80.21% intent, 68.41 F1 (slot) (LLaMA-7b+SFT)",
            "baseline_performance": "Clean: 82.78% intent, 81.58 F1 (slot)",
            "performance_change": "IC-PDR = 8.62% relative drop; SF-PDR = 16.14% relative drop (LLaMA-7b+SFT on MASSIVE)",
            "experimental_setting": "Supervised fine-tuned LLaMA-7b; paraphrases generated and filtered via BERTScore &gt;= 0.85; automatic slot-label projection for paraphrases.",
            "statistical_significance": null,
            "uuid": "e7446.6",
            "source_info": {
                "paper_title": "Prompt Perturbation Consistency Learning for Robust Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "PPCL (Prompt Perturbation Consistency Learning)",
            "name_full": "Prompt Perturbation Consistency Learning (PPCL): JS divergence regularization between clean and perturbed outputs",
            "brief_description": "A training objective augmentation that adds a Jensen-Shannon divergence term between output token probability distributions for clean and perturbed inputs, combined with supervised losses on both, to enforce consistent predictions and improve robustness to input perturbations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-7b (instruction fine-tuned + PPCL)",
            "model_description": "LLaMA-7b fine-tuned first with supervised instruction SFT, then further fine-tuned with PPCL objective combining cross-entropy on clean/perturbed samples and JS divergence regularizer.",
            "model_size": "7B",
            "task_name": "IC-SF on MASSIVE (mitigation of oronym/synonym/paraphrase perturbation effects)",
            "task_description": "Improve robustness so that model produces consistent structured outputs for clean and perturbed versions of the same utterance.",
            "problem_format": "Training-time augmentation + consistency regularization: supply paired (clean, perturbed) inputs and minimize CE(clean)+CE(perturbed)+λ * average JS(output distributions).",
            "format_category": "training objective / prompt presentation (paired clean/perturbed)",
            "format_details": "JS divergence computed per-token (or averaged output distribution for paraphrases); loss weights (λ1, λ2, λ3) tuned per dataset; uses one perturbed sample per clean by default (small augmentation size).",
            "performance_metric": "Performance Drop Rate (PDR) after mitigation and listed recovery percentage",
            "performance_value": "Oronyms (MASSIVE, LLaMA-7b): PPCL yields IC-PDR 8.74% and SF-PDR 15.41% after mitigation",
            "baseline_performance": "Baseline (Lc only) Oronyms: IC-PDR 16.67%, SF-PDR 40.75% (LLaMA-7b+SFT without PPCL)",
            "performance_change": "IC-PDR reduced by 7.93 percentage points (47% of drop recovered); SF-PDR reduced by 25.34 percentage points (62% of drop recovered). Reported average recovery across perturbations: 59% IC recovery, 69% SF recovery.",
            "experimental_setting": "Fine-tune SFT model for 2 additional epochs with paired clean and perturbed inputs; default augmentation: one perturbed sample per clean; compared with multi-sample augmentation (10x more samples).",
            "statistical_significance": null,
            "uuid": "e7446.7",
            "source_info": {
                "paper_title": "Prompt Perturbation Consistency Learning for Robust Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Do prompt-based models really understand the meaning of their prompts?",
            "rating": 2,
            "sanitized_title": "do_promptbased_models_really_understand_the_meaning_of_their_prompts"
        },
        {
            "paper_title": "Rethinking the role of demonstrations: What makes in-context learning work?",
            "rating": 2,
            "sanitized_title": "rethinking_the_role_of_demonstrations_what_makes_incontext_learning_work"
        },
        {
            "paper_title": "Promptbench: Towards evaluating the robustness of large language models on adversarial prompts.",
            "rating": 2,
            "sanitized_title": "promptbench_towards_evaluating_the_robustness_of_large_language_models_on_adversarial_prompts"
        },
        {
            "paper_title": "On robustness of prompt-based semantic parsing with large pre-trained language model: An empirical study on codex",
            "rating": 2,
            "sanitized_title": "on_robustness_of_promptbased_semantic_parsing_with_large_pretrained_language_model_an_empirical_study_on_codex"
        },
        {
            "paper_title": "The unreliability of explanations in few-shot prompting for textual reasoning",
            "rating": 1,
            "sanitized_title": "the_unreliability_of_explanations_in_fewshot_prompting_for_textual_reasoning"
        }
    ],
    "cost": 0.0177175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Prompt Perturbation Consistency Learning for Robust Language Models
24 Feb 2024</p>
<p>Yao Qiang 
Wayne State University
DetroitUSA</p>
<p>Subhrangshu Nandi 
Ninareh Mehrabi mninareh@amazon.com 
Greg Ver Steeg gssteeg@amazon.com 
Anoop Kumar 
Anna Rumshisky 
University of Massachusetts Lowell</p>
<p>Aram Galstyan argalsty@amazon.com 
Prompt Perturbation Consistency Learning for Robust Language Models
24 Feb 2024FF8D70B4E444941C8379FDC98B75318AarXiv:2402.15833v1[cs.CL]
Large language models (LLMs) have demonstrated impressive performance on a number of natural language processing tasks, such as question answering and text summarization.However, their performance on sequence labeling tasks, such as intent classification and slot filling (IC-SF), which is a central component in personal assistant systems, lags significantly behind discriminative models.Furthermore, there is a lack of substantive research on robustness of LLMs to various perturbations in the input prompts.The contributions of this paper are three-fold.First, we show that finetuning sufficiently large LLMs can produce IC-SF performance comparable to discriminative models.Next, we systematically analyze the performance deterioration of those fine-tuned models due to three distinct yet relevant types of input perturbations -oronyms, synonyms, and paraphrasing.Finally, we propose an efficient mitigation approach, prompt perturbation consistency learning (PPCL), which works by regularizing the divergence between losses from clean and perturbed samples.Our experiments show that PPCL can recover on an average 59% and 69% of the performance drop for IC and SF tasks, respectively.Furthermore, PPCL beats data augmentation approach while using ten times fewer augmented data samples.</p>
<p>Introduction</p>
<p>Voice controlled smart personal assistants like Amazon Echo and Google Home have flourished in recent years, enabling goal-oriented conversations and aiding tasks like setting reminders, checking weather, controlling smart devices, and online shopping.A core capability of those systems is to perform accurate and robust intent classification (IC) and slot filling (SF) (Tur and De Mori, 2011;Qin et al., 2021).The IC task involves identifying the speaker's desired intent from a given utterance, * This work was done while interning at Amazon.while the SF task involves recognizing the key arguments of the intent.For instance, given a user query "wake me up at five am this week.",the intent is 'set alarm', while the SF component should identify the specific details, such as 'five am' as time and 'this week' as date for the alarm setting.</p>
<p>Pre-trained LLMs hold promise of greatly improving personal assistant systems, owing to their impressive conversational and reasoning capabilities.In addition to generating fluent conversations, LLMs have shown SOTA performance on a variety of natural language processing (NLP) tasks such as text classification, question answering, text summarization (Chowdhery et al., 2022;Qin et al., 2023).Furthermore, some LLMs have shown promising ability to generate structured outputs such as code synthesis (Nijkamp et al., 2023) and API calls (Patil et al., 2023).However, the performance of LLMs on other structured prediction tasks such as slot filling lags significantly behind.</p>
<p>Another important issue is that LLMs can be highly sensitive to prompt variations (Webson and Pavlick, 2022;Min et al., 2022;Ye and Durrett, 2022).For instance, varying the order of few-shot examples, introducing minor typos or different expressions with the same semantic meaning can lead to qualitatively different results (Jin et al., 2020;Li et al., 2020;Huang et al., 2021;Zhuo et al., 2023).In conversational systems, such perturbations might be caused by automatic speech recognition (ASR) errors, linguistic differences, and userspecific expressions.Thus, adopting LLMs for voice-based personal assistants requires a good understanding of their robustness to above types of perturbations, and effective mitigation to have robust LLM-based IC-SF models.</p>
<p>In this paper we mainly consider the following questions: (1) How can we close the performance gap between LLMs and SOTA discriminative models on IC-SF tasks?(2) How does the performance of LLMs change due to minor changes in the origi- nal utterances?(3) Can we improve the robustness of LLMs in the cases of realistic perturbations?</p>
<p>To address the first question, we explore supervised fine-tuning (SFT) for the IC-SF task, where the base LLM is asked to generate a target output based on an input query.We conduct extensive experiments on three publicly available NLU benchmark datasets (ATIS, SNIPS, MASSIVE) and show that by combining prompt selection and SFT on moderately sized datasets, LLMs can learn to generate structured IC-SF hypotheses with accuracy that is on par with SOTA discriminative method.</p>
<p>Next, we analyze the robustness of the fine-tuned models to three different types of input perturbations that are relevant in the context of voice assistant systems -oronyms, synonyms, and paraphrasing.We find that all three types of perturbations negatively impact the model performance, resulting in a significant performance drop on IC-SF tasks.</p>
<p>Finally, we propose a novel framework that we call prompt perturbation consistency learning, or PPCL, to improve the robustness of LLMs against perturbations.Our framework (1) generates perturbed counterparts given the original utterance by either replacing a small subset of tokens or paraphrasing the utterance while constraining the semantic similarity, (2) fine-tunes LLMs with an additional consistency regularization term in the objective which explicitly encourages the model to generate consistent predictions for the original utterance and its perturbed counterpart.We conduct extensive experiments and demonstrate that PPCL can recover on an average 59% and 69% of the dropped performance for IC and SF tasks against perturbations, respectively.Furthermore, our results indicate that PPCL outperforms simple data augmentation approach while using only 10% of augmented dataset.</p>
<p>Related Work</p>
<p>Intent Classification and Slot Filling Various techniques have been explored for intent classification (Sarikaya et al., 2011;Chen et al., 2012;Ravuri and Stolcke, 2015), with recent work focusing on transformer-based models and transfer learning with pre-trained language models (Qin et al., 2021).Slot filling, on the other hand, is typically approached using sequence labeling models, such as conditional random fields (CRFs), bidirectional LSTMs, and transformer-based architectures (Weld et al., 2022a;Chen et al., 2019;Goo et al., 2018;He and Garner, 2023).For a recent survey of joint IC-SF methods, see (Weld et al., 2022b) Data Augmentation In NLP tasks, data augmentation methods have been explored to generate new instances by manipulating a few words in the original text (Feng et al., 2021;Chen et al., 2023).Some common techniques include word replacement, random deletion, and word position swap (Wei and Zou, 2019).Additionally, data augmentation in NLP can involve creating entirely artificial examples using back-translation (Sennrich et al., 2015) or generative models like variational auto-encoders (Malandrakis et al., 2019;Yoo et al., 2019).Data augmentation has also become popular for NER tasks and has been shown to be effective strategy for boosting model performance (Dai and Adel, 2020;Meng et al., 2021;Zhou et al., 2021).</p>
<p>Consistency Training</p>
<p>Consistency training methods aim to improve the robustness of models by enforcing the stability of their predictions under small perturbations, such as random noise, adversarial noise, or data augmentation techniques, applied to input examples or hidden states.Several attempts have been made to implement consistency training in NER tasks, utilizing both token-level and sequence-level approaches.</p>
<p>Token-level consistency involves regularizing the model to remain unaffected by Gaussian noise (Lowell et al., 2020) or word replacement, operating at the same granularity as NER (Dai and Adel, 2020;Liu et al., 2022).However, using such simplistic noise or augmentation methods may violate the assumption that the noised tokens should retain the same labels as the original tokens.Alternatively, a sequencelevel consistency method employs high-quality augmentation, like back-translation, to enhance consistency across the entire sentence (Xie et al., 2020).Nonetheless, this approach overlooks the precise location of entities due to word alignment issues, leading to a sub-optimal design.More recently, ConNER has been proposed to foster consistent predictions between a span of tokens in the original sentence and their corresponding projection in a translated sentence (Zhou et al., 2022).Unfortunately, ConNER's applicability is confined to cross-lingual NER tasks.Consistency training for fine-tuning LLMs on IC-SF tasks has not been thoroughly explored yet.</p>
<p>Method</p>
<p>Problem Formulation</p>
<p>Our main objective is to utilize LLMs for the purpose of generating structured hypotheses.As illustrated in Figure 1, LLMs are expected to generate correct, coherent, and structured responses, including domain, intent, and slot labels, based on user utterances.To fill the performance gap between LLMs and SOTA discriminative models, we apply instruction fine-tuning (Touvron et al., 2023).</p>
<p>We decompose our task into five steps: (1) Prompts Construction: we design several prompt structures, outlined in Appendix Table 1, to be employed during our instruction fine-tuning process.These prompts utilize the input utterances X and the target outputs Y , which encompass various labels such as Y domain , Y intent , and Y slots ; (2) Instruction Fine-tuning: during instruction fine-tuning, we utilize both the input (X) and output (Y ) within the prompt structure, denoted as Prompt(X, Y ).This approach assists LLMs in learning the task of predicting structured hypotheses, specifically focusing on tasks like IC-SF within our investigation; (3) Response Generation: subsequent to instruction finetuning, we employ prompts with only input data, referred to as Prompt(X), to elicit responses from the LLMs.These responses manifest as a generated text sequence, denoted as W = {w 1 , • • • , w n }; (4) Obtaining Structured Hypotheses: the gener-ated text sequence W is then transformed into structured hypotheses, culminating in the final outcomes denoted as { Ŷdomain , Ŷintent , Ŷslots }; (5) Performance Evaluation: we evaluate the performance by comparing the ground truth labels {Y domain , Y intent , Y slots } with the outputs from the LLMs { Ŷdomain , Ŷintent , Ŷslots }.Various metrics are employed for this evaluation, e.g., accuracy and F1-score for IC and SF, respectively.</p>
<p>LLMs exhibit vulnerability to perturbations (Zhuo et al., 2023;Zhu et al., 2023), leading to the generation of incorrect responses, as demonstrated in Figure 1.Introducing small perturbations to the inputs X or expressing them differently while preserving the same meaning would result in distinct inputs denoted as X ′ .Nevertheless, given that X ′ maintains identical structured hypotheses and target labels Y , our expectation is that LLMs should be able to generate correct responses.In other words, LLMs are expected to be robust against these perturbations and generate consistent responses.</p>
<p>Prompts Construction</p>
<p>The standard prompts employed during instruction fine-tuning process with LLMs typically involve presenting both the input context and its corresponding target output in a paired structure (Liu et al., 2023).The LLMs are then trained to generate the target output based on the input context.The primary objective here is to fine-tune the models' parameters aiming to minimize prediction errors and improve their ability to generate accurate and contextually appropriate responses.</p>
<p>We construct several prompt formats for IC-SF tasks as detailed in Appendix Table 1.The simple prompt format involves presenting the utterance and target outputs consecutively.Next, we design a structured prompt format that for predicting structured hypotheses.As shown in Appendix Table 1, this format associates the intent with its corresponding domain and aligns the slot labels with the arguments of the request.</p>
<p>Furthermore, in the context of the sequence labeling task, i.e., SF, it is expected that LLMs generate slot labels for each individual token within the given utterance.Effectively associating tokens with their respective slot labels is crucial to enhance the models' performance during instruction fine-tuning.Therefore, we construct three different SF prompt formats with the intention of improving model proficiency in the SF task.The tag-only for-mat represents the simplest approach, but it is more challenging since the model is required to implicitly track token indices as well (Raman et al., 2022).To simplify, we introduce sentinel-based formats.These sentinel markers enable us to avoid redundant inclusion of the original tokens in the target output.Instead, the sentinel tokens are employed to facilitate the learning of associations between tokens and their corresponding slot labels.</p>
<p>Our constructed prompt formats offer several advantages: (1) The structured format efficiently arranges the input and output labels within a coherent structure, facilitating the generation of structured hypotheses; (2) The sentinel-based formats eliminate the need for redundant input repetition, simplifying the decoding process and preventing hallucinations; (3) These formats enable a more straightforward method for token tracking (including indices) and establishing connections between tokens and their corresponding slot labels.</p>
<p>Perturbations</p>
<p>A robust model aims to convert all utterances with or without meaning-preserving perturbations into correct hypotheses.To evaluate model robustness in IC-SF tasks, we employ different types of perturbations: oronyms, synonyms, and paraphrases, covering both word-level and sentence-level perturbations aligned with real-world application scenarios.We show some examples of these perturbations in Appendix Table 8 and present more details of the generation process in Section 4.3.</p>
<p>Oronym perturbation involves making changes to a text by replacing words or phrases with those that are phonetically similar but carry a different meaning.Oronym perturbation is widely used for data augmentation in NLP tasks, especially for tasks that require robustness to speech recognition errors (ASR) or homophonic ambiguity (Cai et al., 2023).While the altered semantics of oronymperturbed expressions may differ from the initial utterances, our expectation is that LLMs should exhibit robustness to these changes and produce responses aligned with user intent.</p>
<p>Synonym perturbation replaces certain words or phrases with their synonyms while preserving the overall meaning of the text.It is commonly employed in NLP as data augmentation to enhance data diversity by generating new variations of a given sentence while retaining semantic coherence (Alfonso-Hermelo et al., 2021).Synonym perturbation tests robustness of LLMs in generating consis-tent hypotheses when presented with semantically similar utterances.</p>
<p>Paraphrasing perturbation entails rephrasing a given text to create variations while preserving its original meaning.This is highly consistent with our daily communications that present the same meaning in different ways.Hence, irrespective of the chosen words or structures, LLMs should consistently produce accurate hypotheses.</p>
<p>Data Augmentation</p>
<p>Data augmentation is widely used in fine-tuning LLMs to improve their generalization capabilities.There are two major benefits of data augmentation:</p>
<p>(1) It expands the dataset, which proves beneficial for overcoming limited training data in diverse realworld scenarios; (2) It diversifies the fine-tuning dataset, equipping the model to better handle linguistic variations and consequently enhancing its performance in downstream tasks.</p>
<p>We apply a range of data augmentation techniques, each designed to generate diverse data through specific perturbations.To elaborate, we utilize word replacement techniques involving oronyms and synonyms as forms of data augmentation.This approach improves LLM's ability to adapt to previously unseen data and comprehend language variations, addressing the challenges associated with speech recognition and linguistic ambiguity.We also paraphrase the training data, providing LLMs with more examples to learn different ways of expressing the same content.</p>
<p>However, even though data augmentation is advantageous, it is essential not to introduce noise or potentially misleading content.We establish specific constraints during the generation process and implement post-processing filters to reinforce the preservation of the original utterances' integrity.</p>
<p>Prompt Perturbation Consistency Learning (PPCL)</p>
<p>Despite the fact that data augmentation has been demonstrated to be efficient to improve model robustness and generalizability (Chen et al., 2021), it overlooks the similar semantic meaning shared between the original and augmented data.To address this, we propose perturbation consistency learning framework to further utilize these augmented data, particularly the perturbed counterparts of the original utterances in our study.dictions (and consequently, comparable responses) for both the original utterance and its perturbed counterpart.Through the incorporation of this additional constraint, our goal is to strengthen the model's ability to maintain consistency between the original and perturbed utterances, resulting in improved robustness and more reliable performance across real-world applications.</p>
<p>Our objective is to align the model's responses when presented with two semantically equivalent utterances.To achieve this, we add an extra component into the training objective: the Jensen-Shannon (JS) divergence of output probabilities between a clean utterance and its perturbed counterpart.This term is integrated with the standard cross-entropy loss utilized in the auto-regression phase of the fine-tuning process.</p>
<p>Figure 2 shows the architecture of PPCL.During the fine-tuning process, we simultaneously input the clean utterance denoted as x c and its perturbed counterpart labeled as x p to the LLMs.In response to these inputs, the LLMs generate corresponding outputs p j c and p j p , respectively, the probability distributions over vocabulary of the j-th output token for x c and x p , where p j c , p j p ∈ R |V| and V denotes the vocabulary size.Subsequently, we apply Softmax to p j c and p j p and get their respective probability distributions ŷj c and ŷj p , formally: ŷj c = Softmax(p j c ) and ŷj p = Softmax(p j p ).We then apply JS divergence to quantify the similarity between ŷj c and ŷj p .JS is a symmetric variation of Kullback-Leibler divergence (KL), defined as:
JS(ŷ j c ||ŷ j p ) = 1 2 (KL(ŷ j c ||ŷ j m )+KL(ŷ j p ||ŷ j m )),(1)
where ŷj m = 1 2 (ŷ j c + ŷj p ). JS smooths out the asymmetry of KL and offers a more balanced perspec-tive on similarity.We obtain the JS of the two probability distributions of j-th output, denoted as: JS(ŷ j c || ŷj p ).We use the average JS across all output probability distributions associated with x c and x p as our final perturbation consistency learning loss, formally:
L JS = 1 L L j=1 JS(ŷ j c || ŷj p ),(2)
where L denotes the response length.Utilizing Eq. 2 with oronym and synonym perturbations is straightforward, as these perturbations merely substitute tokens or phrases with their respective oronyms and synonyms while maintaining the utterance length.However, paraphrasing perturbations lead to varying lengths between the clean utterance and its modified counterpart.Instead of computing the JS for each token-pair in the output, we employ the averaged probability distribution to calculate the perturbation consistency learning loss for paraphrasing perturbations, formally:
L JS = JS(ŷ c || ŷp ),(3)</p>
<p>Training Objective</p>
<p>Our training objective integrates the supervised cross-entropy losses for both clean and perturbed utterances (i.e., L C and L P ) with the perturbation consistency learning loss L JS , formally:
L C = CE(ŷ c , y),(4)L P = CE(ŷ p , y),(5)L = λ 1 L C + λ 2 L P + λ 3 L JS ,(6)
where λ 1 , λ 2 , and λ 3 are weight coefficients.</p>
<p>In order to optimize the above objective, it is essential to have both the clean utterance and its corresponding perturbed counterpart.We generate these paired perturbed utterances using our proposed perturbation generation methods.Furthermore, to ensure the presence of semantically comparable pairs, we implement specific post-processing filtering procedures.These filters serve to verify that the generated perturbed utterances genuinely maintain semantic equivalence with their clean counterparts.</p>
<p>Experiments</p>
<p>Experimental Settings</p>
<p>Datasets We evaluate model performance on three NLU benchmark datasets, i.e., ATIS (Price, 1990), SNIPS (Coucke et al., 2018), MASSIVE (FitzGerald et al., 2022).More details of these datasets and their statistics are shown in the Appendix.</p>
<p>Prompt Formats We show our proposed prompt formats with an illustrated example for IC-SF tasks in Table 1.</p>
<p>Baselines We compare the performance of PPCL with the following baselines: supervised finetuning with discriminative models like JointBERT and JointBERT+CRF, zero-shot and few-shot learning with GPT variants, instruction fine-tuning with LLaMA.For additional information about these baselines and their specific experimental setups, please refer to the Appendix.</p>
<p>Evaluation Metrics</p>
<p>For the IC task, we use prediction accuracy on a held-out test set, and for the SF task, we use the F1-score as the evaluation metrics.Instead of using absolute differences in performance between models trained with clean and perturbed data, we use a relative measurement.We introduce Performance Drop Rate (PDR), which quantifies the relative performance decline following a perturbation, formally:
PDR(D, D ′ , f θ ) = 1 − (x,y)∈D ′ M[f θ (x), y] (x,y)∈D M[f θ (x), y]
.</p>
<p>(7) M here is the indicator function and f θ denotes the models.D and D ′ indicates the clean and perturbed test sets, respectively.We want to clarify that the clean and perturbed test sets are in a one-to-one correspondence, thus |D| == |D ′ |.In other words, each example in the clean test set has a corresponding example in the perturbed test set.This ensures a fair and direct comparison between the model's performance on clean and perturbed samples.</p>
<p>Perturbed Evaluation Sets</p>
<p>We generate perturbed evaluation sets for each benchmark dataset.The synonym perturbation involves randomly choosing and substituting words with their synonyms based on the WordNet synonym corpus.The oronym perturbation follows a similar procedure relying on the NLTK pronouncing corpus.Specifically, we compile a list of key stop words based on the domain, intent, and slot label sets, and do not substitute them.Additionally, we have imposed a limit of three words as the maximum number that can be perturbed in an utterance to prevent significant changes in semantic meaning.We generate the paraphrases using a specific LLM from Huggingface, which is specially pre-trained for generating high-quality paraphrases.To further ensure that clean and perturbed samples are semantically similar, we filter out perturbations with BERTScore (Zhang et al., 2019) with the original sample.We use a 0.85 threshold based on our empirical experimental studies.</p>
<p>With perturbations of samples, generating appropriate target labels is crucial for evaluation.For intent labels, we align them with those of the original utterances.For slot labels, the procedure is more complex.For perturbations that maintain the length and word order, such as oronyms and synonyms, we directly adopt the original slot labels as their corresponding counterparts.For paraphrased variations that may deviate in length and word order from the original utterance, we automatically generate new slot labels.The new slot labels are derived from the semantic annotations present in the original utterance.This strategy ensures that the perturbed versions retain their intended meaning while accommodating any structural changes arising from the paraphrasing process.</p>
<p>Results and Discussion</p>
<p>Performance Gap between LLMs and discriminative models</p>
<p>First, we show the model performance comparison of different baselines on three datasets in Table 2.These results demonstrate that LLMs, i.e., GPT2 and LLaMA, which have been instruction fine-tuned with our proposed sentinel-based structured format, achieve comparable intent classification performance to SOTA discriminative models like JointBERT across all three datasets.However, applying zero-shot and few-shot learning settings the performance of LLMs is notably worse, especially for the SF tasks.</p>
<p>The lower performance of LLMs on the SF task could be attributed to the mismatch between the nature of the semantic labeling task and the design of text generation models.The latter are not inherently optimized for SF tasks, which might lead to sub-optimal results in some cases.However they can still achieve comparable results for the sequence labeling task, such as SF, after supervised  It is important to highlight that the key advantage of using generative models over discriminative models for IC-SF tasks lies in their ability to create and understand a wider range of linguistic variations.Generative models can generate new examples, enhancing the training set with diverse phrases and structures.This leads to a more robust model that can better handle varied user inputs.In contrast, discriminative models typically rely on the existing training set, which might limit their ability to adapt to new or unexpected ways people express similar intents.</p>
<p>Prompt Formats</p>
<p>We compare the model performance using different prompt formats in Table 3.The sentinel-based structured prompt format achieves the best performance, particularly for the SF tasks.This outcome aligns with our initial hypothesis that the structured format is highly effective in organizing both the input and output labels, leading to improved learning ability for the models.In addition, sentinel-based slot formatting significantly improves performance.</p>
<p>Performance Drop due to Prompt Perturbations</p>
<p>Table 6 illustrates examples of clean and perturbed utterances and their difference in model predictions even though the BertScores between the clean and perturbed samples are higher than 0.85.We show the relative performance drops resulting from the following three perturbations: oronyms, synonyms, and paraphrases, on MASSIVE dataset in Table 4.</p>
<p>The results of ATIS and SNIPS are shown in Appendix.Results show that discriminative models, ICL approaches, and LLMs with instruction finetuning are vulnerable to these perturbations with large performance drops, most notably, in SF tasks with oronym perturbations.These findings highlight the vulnerabilities of both discriminative and generative models when exposed to perturbed data, emphasizing the need to improve model robustness for real-world applications.Identifying and mitigating the impact of perturbations, especially in tasks involving sequence labeling like SF, are critical to improving the performance and generalizability of these models.</p>
<p>PPCL Mitigation Results</p>
<p>We share results from two mitigation approaches for improving robustness of LLMs against prompt perturbations: data augmentation and PPCL.We show results with different augmentation sizes and different combinations of loss functions on MAS-SIVE dataset are in Table 5.All these are done on LLaMA-7b model.Both approaches decrease the significant performance drop.The ones where multiple perturbed samples are added for each clean sample the training data size increases by 50k or more.For example, data augmentation with one perturbed sample per clean sample, along with perturbation loss, shown as L C + L P recovers performance drops up to 45% on IC and 51% on SF tasks, respectively for Oronym perturbation.When augmented with 5 perturbed samples per clean sample, it performs better.However, PPCL, with only 1 perturbed sample per clean, which includes perturbation loss and JS loss, outperforms multiple sample augmentation in all cases, except for SF in paraphrase perturbation.For paraphrase perturbation, PPCL recovers 60% of SF-PDR compared to 74% by multi-sample augmentation, but at one-tenth the augmentation size.On average, PPCL is able to recover 59% in IC and 69% in SF performance drops.In comparison, multi-sample augmentation is able to recover 58% in IC and 59% in SF.PPCL achieves the recoveries with one-tenth the augmen-</p>
<p>Ablation Studies</p>
<p>In our training objective, there are three different terms in Eq. 6, and to better understand their contributions towards improving the robustness of LLMs against perturbations, we conducted an ablation study as shown in Table 5. Experimental results make it clear that the models achieve the best performance when all three loss terms (L c , L p , L js ) in the training objective are utilized, indicating each term plays a significant role in enhancing the robustness of the models.PPCL outperforms multisample augmentation with a fraction of augmentation volume in 5 out of 6 tasks in Massive data.</p>
<p>We have also carefully fine-tuned the three weights in the PPCL loss (Eq.6) for each dataset respectively to identify the best-performing model.To improve model performance, we believe that these weights should be carefully fine-tuned and selected under different settings and datasets.</p>
<p>Failure and Saved Examples</p>
<p>We provide two case studies in Table 6 to illustrate some failure due to the perturbations and the recoveries after applying PPCL.In these two examples, we observe that oronym substitution and paraphrasing lead the model to generate incorrect responses.These incorrect responses (red lines) are characterized as failure cases, as they do not accurately capture the user's intents or the relevant information in the utterances.However, after re-training the model with PPCL, we see improvement.The model is now able to generate the correct responses, which are demonstrated in blue lines.</p>
<p>Conclusion</p>
<p>We study, evaluate, and improve the robustness of LLMs in generating structured hypotheses, such as IC-SF tasks.We first propose a sentinel-based structured prompt format for instruction fine-tuning LLMs resulting in comparable performance to SOTA discriminative models.Next, we evaluate robustness of LLMs under various prompt perturbations, i.e., synonyms, oronyms, and paraphrases.Our results indicate that LLMs are vulnerable to these perturbations, with an average performance drop rate of 13.07% in IC accuracy and 22.20% in SF F1-score.We then propose two mitigation strategies, i.e., perturbation consistency learning and data augmentation, aiming to improve model robustness.These methods can recover up to 59% performance drop in IC task and 69% in SF task, making the resulting LLMs more robust to prompt perturbations.Finally, our findings show that PPCL surpasses the basic data augmentation method, achieving superior performance with just 10% of the augmented datasets, thereby exhibiting enhanced scalability.</p>
<p>Limitations</p>
<p>PPCL was developed based on observations on publicly available small datasets like Massive, ATIS, SNIPS.The improvement in performance might not be as pronounced in real world datasets whose distributions and noise structure might not mimic the public datasets.Improvement in robustness by implementing PPCL was evaluated on IC-SF tasks.We expect PPCL to work in other tasks as well, but we have not demonstrated it.We plan to do so in future work.</p>
<p>A Appendix</p>
<p>A.1 Datasets</p>
<p>We show the data statistics of the three datasets in Table 7 and present more details here.ATIS: ATIS dataset has been widely used to develop and evaluate natural language understanding systems, including intent detection, slot-filling, and dialogue act classification.The dataset consists of a collection of human-computer dialogues, where users interact with a simulated airline information system to obtain various travel-related information, such as flight schedules, ticket availability, and airport information.These dialogues were collected from real users interacting with the ATIS system.SNIPS: SNIPS dataset is designed to support the development and evaluation of voice-controlled systems for home automation tasks.It consists of a large collection of spoken language interactions, where users interact with a voice assistant to perform various tasks commonly found in a home setting, such as setting alarms, playing music, checking the weather, and controlling smart devices.</p>
<p>MASSIVE dataset is an open source multilingual NLU dataset from Amazon Alexa NLU systems consisting of 1 million labeled utterances spanning 51 language.For our experiments, we only use the en-US domain utterances.</p>
<p>A.2 Baselines</p>
<p>JointBERT and JointBERT+CRF: JointBERT was propose in (Chen et al., 2019) as a joint IC-SF model based on BERT.JointBERT+CRF investigates the efficacy of adding Conditional Random Field (CRF) for modeling slot label dependencies on top of the joint BERT model.We use English uncased BERT-Base model which has 12 layers, 768 hidden states, and 12 heads.For fine-tuning, all hyper-parameters are tuned on the development set.The maximum length is 50.The batch size is 32.Adam is used for optimization with an initial learning rate of 5e-5.The dropout probability is 0.1.The maximum number of epochs is set as 10.Zero/Few-shot Learning: In our experiments, we utilize the OpenAI API and GPT3.5 for conducting zero-shot and few-shot learning tasks.We use 10 examples in the few-shot learning.Different prompts are designed to evaluate the model's ability to generalize and perform tasks it hasn't been explicitly trained on, showcasing its capacity for zero-shot and few-shot learning scenarios.</p>
<p>LLMs: We evaluate several popular LLMs, including GPT-2 and LLaMA.GPT-2 is a large-scale unsupervised language model designed to generate human-like text based on the context given to it.We use the smallerst version of GPT-2 with 124M parameters.The LLaMA model is a collection of foundation language models ranging from 7B to 65B parameters proposed by Meta.We use the 7b, 13b, and 30b versions during our experiments.Supervised Fine-tuning: We first apply supervised fine-tuning with LLMs for IC-SF tasks.The maximum length is set as 256.The batch size is 32.Adam is also use for optimization with an initial learning rate of 3e-4 with 100 steps warm-up.We fine-tune the model 5 ecpochs.Perturbation Consistency Learning: We further fine-tune the models for another 2 epochs with out perturbation consistency learning objective.We use Adam as optimizer with an initial learning rate of 3e-4.</p>
<p>A.3 Perturbation Examples</p>
<p>We show several examples of different types of perturbations in Table 8.</p>
<p>A.4 More Results</p>
<p>We show some other results in the following tables.Table 9 and Table 10 show the comparison of model performance drops against different types of perturbations on ATIS and SNIPS datasets, respectively.Table 12 and Table 11 show the ablation studies on the different terms in training objective L (Eq. 6) on ATIS and SNIPS datasets, respectively.</p>
<p>Figure 1 :
1
Figure 1: Illustration examples.LLMs are expected to generate structured hypotheses, i.e., domain, intent, and slots, in their responses to given user requests.Model prediction (shown in red) changes for minor perturbance.</p>
<p>me the weather this weak : tell me the weather this week : O O O O date O : O O O O date date LLM Figure</p>
<p>2: Perturbation consistency learning architecture.xc and x p denote the clean and perturbed utterances, respectively.ŷc and ŷp here denote the slot labels generated by LLM.ŷj c and ŷj p represent the output probability distributions of current interest tokens, i.e., 'date' and 'O'.JS here denotes Jensen-Shannon divergence.</p>
<p>The key idea is to integrate a term into the training objective that explicitly encourages the generation of similar pre-: tell</p>
<p>Table 1 :
1
Illustration of prompt and SF formats for IC-SF tasks Utterance (u): wake me up at five am this week Domain (d): alarm Intent (i): alarm_set Slots (s): [Other Other Other Other time time date date] Arguments (a): [time : five am, date : this week]
Prompt FormatSamplesSimple PromptUtterance: u Domain: d Intent: i Slots: s Arguments: aStructured PromptUtterance: u Intent in Domain: i in d Slots with Arguments: s with aSF FormatSample Inputs &amp; SlotsTag OnlyInput: wake me up at five am this week Slots: Other Other Other Other time time date dateSentinel + TagInput: &lt;0&gt;wake &lt;1&gt;me &lt;2&gt;up &lt;3&gt;at &lt;4&gt;five &lt;5&gt;am &lt;6&gt;this &lt;7&gt;week Slots: &lt;0&gt;Other &lt;1&gt;Other &lt;2&gt;Other &lt;3&gt;Other &lt;4&gt;time &lt;5&gt;time &lt;6&gt;date &lt;7&gt;dateInput: &lt;0&gt;wake &lt;1&gt;me &lt;2&gt;up &lt;3&gt;at &lt;4&gt;five &lt;5&gt;am &lt;6&gt;this &lt;7&gt;week Extractive Sentinel + Tag Slots: &lt;4&gt;time &lt;5&gt;time &lt;6&gt;date &lt;7&gt;date</p>
<p>Table 2 :
2
Comparison of model performance on three datasets.The best performance of SOTA discriminative models and LLMs is highlighted in bold.
DatasetsModelIntent Acc Slot F1JointBERT89.4480.43JointBERT+CRF88.6780.58GPT3.5-ZS60.39-MASSIVE GPT3.5-FS67.1831.76GPT2+SFT84.1366.72LLaMA-7b+SFT88.0180.45LLaMA-13b+SFT88.8780.7LLaMA-30b+SFT89.0580.74JointBERT97.5395.83JointBERT+CRF96.7595.58ATISGPT3.5-ZS87.45-GPT3.5-FS93.1773.51GPT2+SFT97.3183.92LLaMA-7b+SFT98.2194.26JointBERT98.5796.67JointBERT+CRF98.2896.07SNIPSGPT3.5-ZS95.14-GPT3.5-FS94.4249.12GPT2+SFT97.1488.23LLaMA-7b+SFT98.1494.51fine-tuning with appropriate instructions or struc-tured formats. This is demonstrated by LLaMA-30b achieving and average SF accuracy (89.84%)within 1.3% of JointBERT performance (91.03%),and even superseding it for MASSIVE dataset.</p>
<p>Table 3 :
3
Comparison of model performance with different prompt formats: Simple and Structured prompt formats with tag-only, extractive sentinel-based with tag, and sentinel-based with tag slots formats, respectively.
DatasetsPrompt FormatsIntentSlotAccF1Simple + Tag98.43 86.04ATISSimple + Extractive Sentinel97.76 93.12Simple + Sentinel Tag98.21 94.26Simple + Tag97.85 89.11SNIPSSimple + Extractive Sentinel98.71 92.88Simple + Sentinel Tag98.14 94.51Simple + Tag88.68 72.91Simple + Extractive Sentinel88.33 73.42Simple + Sentinel Tag87.51 75.36MASSIVE Structured + Tag88.73 75.72Structured + Extractive Sentinel 87.82 75.13Structured + Sentinel88.01 80.45</p>
<p>Table 4 :
4
Comparison of model performance drops as a result of prompt perturbations, on MASSIVE dataset.The smaller PDR values imply higher model robustness.
PerturbModelClean IC Perutbed IC IC-PDR Clean SF Perturbed SF SF-PDRJointBERT90.1970.7721.5380.5042.2847.47JointBERT+CRF89.5071.1920.4580.6542.4147.41GPT3.5-ZS61.3960.691.15---OronymsGPT3.5-FS70.4348.9130.5531.9520.7535.05GPT2+SFT85.5267.7120.8365.1427.5158.40LLaMA-7b+SFT89.1874.3116.6779.3547.0140.75JointBERT90.4378.2913.4280.8374.777.49JointBERT+CRF89.4377.6113.2181.8675.877.31GPT3.5-ZS63.0458.666.95---SynonymsGPT3.5-FS65.5454.5916.7134.4331.578.30GPT2+SFT84.9970.4217.1467.9260.6210.74LLaMA-7b+SFT89.2376.7913.9480.7572.909.72JointBERT89.3082.967.0982.8171.6713.45JointBERT+CRF88.7180.888.8282.6470.0815.19GPT3.5-ZS60.8055.279.09---Paraphrases GPT3.5-FS65.5559.089.8834.8729.2216.20GPT2+SFT82.6076.717.1363.5352.3317.63LLaMA-7b+SFT82.7880.218.6281.5868.4116.14</p>
<p>Table 5 :
5
Mitigation results of data augmentation and PPCL on MASSIVE dataset.We show results with different augmentation sizes and different loss functions.For multi-sample augmentation the training size increase by ∼ 50k, for single sample it is similar to the original size.
PerturbMitigationAugmentation LossIC-PDR Recovery SF-PDR RecoveryBaseline-Lc16.67-40.75-JS Loss+3kLc + Ljs15.745%32.8019%OronymsPerturb Loss+3kLc + Lp8.9546%18.4455%Perturb Loss+50kLc + Lp9.0245%19.7351%PPCL (JS + Perturb Loss)+3kLc + Lp + Ljs8.7447%15.4162%Baseline-Lc13.94-9.72-JS Loss+5kLc + Ljs12.1113%7.8319%SynonymsPerturb Loss+5kLc + Lp5.5960%5.1347%Perturb Loss+50kLc + Lp4.0171%4.4953%PPCL (JS + Perturb Loss)+5kLc + Lp + Ljs3.7473%1.4485%Baseline-Lc8.62-16.14-JS Loss+6kLc + Ljs7.799%15.106%Paraphrases Perturb Loss+6kLc + Lp5.9231%8.8945%Perturb Loss+50kLc + Lp3.6957%4.2474%PPCL (JS + Perturb Loss)+6kLc + Lp + Ljs3.6957%6.3660%</p>
<p>Table 6 :
6
Some examples of clean and perturbed utterances, with BertScore &gt; 0.85.Red lines are a result of perturbation.Blue lines are post PPCL mitigation.
Perturbations UtterancesPred_Domain Pred_IntentPred_SlotsCleancreate an alarm for today at ten am alarmalarm_set[today: date , ten am: time]Paraphraseset a reminder for today at ten amcalendarcalendar_set[today: date , ten am: time]Paraphraseset a reminder for today at ten amalarmalarm_set[today: date , ten am: time]Cleangive me more liteiotiot_hue_lightup[]Oronymgive mi moore liteemailemail_querycontact [mi moore: person]Oronymgive mi moore liteiotiot_hue_lightup[]tation size. PPCL comparisons with augmentationon ATIS and SNIPS datasets as shown in Appendix,indicating the generalizability and effectiveness ofour approach across different domains and datasets.</p>
<p>Table 7 :
7
Dataset statistics
DatasetsTrainDevTest Intent Labels Slot LabelsATIS447850089318127SNIPS13084700700772MASSIVE 11514 2033 29746056</p>
<p>Table 8 :
8
Examples of different types of perturbations
Original UtterancesOronyms Perturbationsreview all alarmsreview aul alarmswhen is the event going to startwynn is the event going to startOriginal UtterancesSynonyms Perturbationsemail to new contactemail to novel contactpink is all we needpink is all we askOriginal UtterancesParaphrasing Perturbationstell me the weather this weekwhats the weather forecast for this weekhow old is mariah careywhat is the age of mariah carey</p>
<p>Table 9 :
9
Comparison of model performance drops against perturbations on ATIS dataset.
PerturbModelClean IC Perutbed IC IC-PDR Clean SF Perturbed SF SF-PDRJointBERT97.8796.111.7996.4778.3718.76JointBERT+CRF97.1795.751.4696.0076.0920.74GPT3.5-ZS87.8086.211.81---OronymsGPT3.5-FS91.5490.281.3777.8951.4233.98GPT2+SFT98.5896.282.3359.7543.4927.21LLaMA-7b+SFT99.1197.171.9594.2476.6818.63JointBERT97.9191.966.0793.1892.643.68JointBERT+CRF97.3289.288.2696.2892.463.96GPT3.5-ZS82.4476.487.22---SynonymsGPT3.5-FS89.5888.091.6677.5073.085.70GPT2+SFT97.3292.564.8960.1753.0011.91LLaMA-7b+SFT98.2191.366.9794.7389.335.70JointBERT97.6091.006.7695.8682.6413.79JointBERT+CRF98.8190.208.7195.6182.4313.78GPT3.5-ZS88.1582.336.71---Paraphrases GPT3.5-FS90.2087.123.4177.5070.019.66GPT2+SFT92.1290.192.0992.9644.7651.85LLaMA-7b+SFT98.1790.427.8993.7280.6313.97</p>
<p>Table 10 :
10
Comparison of model performance drops against perturbations on SNIPS dataset.
PerturbModelClean IC Perutbed IC IC-PDR Clean SF Perturbed SF SF-PDRJointBERT98.6196.062.5897.0579.1418.45JointBERT+CRF98.1494.673.5395.8778.6317.98GPT3.5-ZS95.6094.441.21---OronymsGPT3.5-FS93.9890.743.4450.3041.4817.53GPT2+SFT97.8695.262.6590.6665.2428.04LLaMA-7b+SFT98.1496.751.4294.4275.8419.67JointBERT99.0595.583.5096.0087.049.33JointBERT+CRF99.0595.583.5094.8786.688.63GPT3.5-ZS95.8984.8511.51---SynonymsGPT3.5-FS94.3280.4414.7148.0543.289.92GPT2+SFT98.7190.068.7690.8575.4116.99LLaMA-7b+SFT99.0594.324.7794.4583.2511.85JointBERT98.5393.095.5296.6758.6939.39JointBERT+CRF98.2391.776.5796.0658.8838.70GPT3.5-ZS95.7483.8412.42---Paraphrases GPT3.5-FS93.9780.7614.0549.4933.0133.29GPT2+SFT97.6090.097.6990.9649.4445.64LLaMA-7b+SFT98.2390.018.3694.4155.6441.06</p>
<p>Table 11 :
11
Ablation studies on the different terms in training objective L of SNIPS dataset.
PerturbLossesIC-PDR Recovery SF-PDR RecoveryLC1.42-19.67-OronymsLC + LP0.2384%2.6286%LC + LP + LJS0.0100%1.5892%LC4.77-11.85-SynonymsLC + LP1.7064%3.8967%LC + LP + LJS+0.31118%1.3189%LC8.36-41.06-Paraphrases LC + LP5.5234%28.9729%LC + LP + LJS4.6344%28.4530%</p>
<p>Table 12 :
12
Ablation studies on the different terms in training objective L of ATIS dataset.
PerturbLossesIC-PDR Recovery SF-PDR RecoveryLC1.95-18.63-OronymsLC + LP0.1883%+0.33101%LC + LP + LJS+0.01100%+0.71104%LC6.97-5.70-SynonymsLC + LP3.5549%2.3259%LC + LP + LJS2.1169%0.3394%LC7.89-13.97-Paraphrases LC + LP6.5117%8.9536%LC + LP + LJS4.8339%3.1977%
AcknowledgementsThe authors would like to thank the reviewers and area chairs for their suggestions and comments.Ethics StatementThe authors foresee no ethical concerns with the research presented in this work.They also completed an internal legal review process which verified that we are using publicly available models and datasets consistent with their intended use.
Abbas Ghaddar, Philippe Langlais, and Mehdi Rezagholizadeh. 2021. Nature: Natural auxiliary text utterances for realistic spoken language evaluation. David Alfonso-Hermelo, Ahmad Rashid, arXiv:2111.05196arXiv preprint</p>
<p>Zefan Cai, Xin Zheng, Tianyu Liu, Xu Wang, Haoran Meng, Jiaqi Han, Gang Yuan, Binghuai Lin, Baobao Chang, Yunbo Cao, arXiv:2305.14751Dialogvcs: Robust natural language understanding in dialogue system upgrade. 2023arXiv preprint</p>
<p>Hiddencut: Simple data augmentation for natural language understanding with better generalizability. Jiaao Chen, Dinghan Shen, Weizhu Chen, Diyi Yang, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211</p>
<p>An empirical survey of data augmentation for limited data learning in nlp. Jiaao Chen, Derek Tam, Colin Raffel, Mohit Bansal, Diyi Yang, Transactions of the Association for Computational Linguistics. 112023</p>
<p>Understanding user intent in community question answering. Long Chen, Dell Zhang, Levene Mark, 10.1145/2187980.2188206Proceedings of the 21st International Conference on World Wide Web, WWW '12 Companion. the 21st International Conference on World Wide Web, WWW '12 CompanionNew York, NY, USAAssociation for Computing Machinery2012</p>
<p>Bert for joint intent classification and slot filling. Qian Chen, Zhu Zhuo, Wen Wang, arXiv:1902.109092019arXiv preprint</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces. Alice Coucke, Alaa Saade, Adrien Ball, Théodore Bluche, Alexandre Caulier, David Leroy, Clément Doumouro, Thibault Gisselbrecht, Francesco Caltagirone, Thibaut Lavril, arXiv:1805.101902018arXiv preprint</p>
<p>An analysis of simple data augmentation for named entity recognition. Xiang Dai, Heike Adel, arXiv:2010.116832020arXiv preprint</p>
<p>Varun Steven Y Feng, Jason Gangal, Sarath Wei, Soroush Chandar, Teruko Vosoughi, Eduard Mitamura, Hovy, arXiv:2105.03075A survey of data augmentation approaches for nlp. 2021arXiv preprint</p>
<p>Massive: A 1m-example multilingual natural language understanding dataset with 51 typologically-diverse languages. Jack Fitzgerald, Christopher Hench, Charith Peris, Scott Mackie, Kay Rottmann, Ana Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Singh, arXiv:2204.085822022arXiv preprint</p>
<p>Slot-gated modeling for joint slot filling and intent prediction. Chih-Wen Goo, Guang Gao, Yun-Kai Hsu, Chih-Li Huo, Tsung-Chieh Chen, Keng-Wei Hsu, Yun-Nung Chen, Proceedings of the 2018 Conference of the North American Chapter. Short Papers. the 2018 Conference of the North American ChapterHuman Language Technologies20182</p>
<p>Can chatgpt detect intent? evaluating large language models for spoken language understanding. Mutian He, Philip N Garner, arXiv:2305.135122023arXiv preprint</p>
<p>Shuo Huang, Zhuang Li, Lizhen Qu, Lei Pan, arXiv:2102.01563On robustness of neural semantic parsers. 2021arXiv preprint</p>
<p>Is bert really robust? a strong baseline for natural language attack on text classification and entailment. Di Jin, Zhijing Jin, Joey Tianyi Zhou, Peter Szolovits, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202034</p>
<p>Bert-attack: Adversarial attack against bert using bert. Linyang Li, Ruotian Ma, arXiv:2004.099842020arXiv preprintQipeng Guo, Xiangyang Xue, and Xipeng Qiu</p>
<p>Lowresource ner by data augmentation with prompting. Jian Liu, Yufeng Chen, Jinan Xu, Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22. the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-222022</p>
<p>Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, ACM Computing Surveys. 5592023</p>
<p>Unsupervised data augmentation with naive augmentation and without unlabeled data. David Lowell, Brian E Howard, Zachary C Lipton, Byron C Wallace, arXiv:2010.119662020arXiv preprint</p>
<p>Controlled text generation for data augmentation in intelligent artificial agents. Nikolaos Malandrakis, Minmin Shen, Anuj Goyal, Shuyang Gao, arXiv:1910.034872019arXiv preprintAbhishek Sethi, and Angeliki Metallinou</p>
<p>Distantlysupervised named entity recognition with noiserobust learning and language model augmented selftraining. Yu Meng, Yunyi Zhang, Jiaxin Huang, Xuan Wang, Yu Zhang, Heng Ji, Jiawei Han, arXiv:2109.050032021arXiv preprint</p>
<p>Rethinking the role of demonstrations: What makes in-context learning work?. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, 10.18653/v1/2022.emnlp-main.759Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, Yingbo Zhou, Codegen2: Lessons for training llms on programming and natural languages. 2023</p>
<p>G Shishir, Tianjun Patil, Xin Zhang, Joseph E Wang, Gonzalez, Gorilla: Large language model connected with massive apis. 2023</p>
<p>Evaluation of spoken language systems: The atis domain. Patti Price, Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley. Pennsylvania1990. June 24-27, 1990</p>
<p>Is chatgpt a general-purpose natural language processing task solver?. Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, Diyi Yang, 2023</p>
<p>Libo Qin, Tianbao Xie, Wanxiang Che, Ting Liu, arXiv:2103.03095A survey on spoken language understanding: Recent advances and new frontiers. 2021arXiv preprint</p>
<p>Transforming sequence tagging into a seq2seq task. Karthik Raman, Iftekhar Naim, Jiecao Chen, Kazuma Hashimoto, Kiran Yalasangi, Krishna Srinivasan, arXiv:2203.083782022arXiv preprint</p>
<p>Recurrent neural network and lstm models for lexical utterance classification. Suman Ravuri, Andreas Stolcke, ISCA -International Speech Communication Association. 2015Proc. Interspeech</p>
<p>Deep belief nets for natural language call-routing. Ruhi Sarikaya, Geoffrey E Hinton, Bhuvana Ramabhadran, 10.1109/ICASSP.2011.5947649ICASSP. 2011</p>
<p>Improving neural machine translation models with monolingual data. Rico Sennrich, Barry Haddow, Alexandra Birch, arXiv:1511.067092015arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Albert Webson and Ellie Pavlick. 2022. Do promptbased models really understand the meaning of their prompts?. Gokhan Tur, Renato De Mori, 10.18653/v1/2022.naacl-main.167Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2011Spoken language understanding: Systems for extracting semantic information from speech</p>
<p>Jason Wei, Kai Zou, arXiv:1901.11196Eda: Easy data augmentation techniques for boosting performance on text classification tasks. 2019arXiv preprint</p>
<p>A survey of joint intent detection and slot filling models in natural language understanding. Henry Weld, Xiaoqi Huang, Siqu Long, Josiah Poon, Caren Soyeon, Han, ACM Computing Surveys. 5582022a</p>
<p>A survey of joint intent detection and slot filling models in natural language understanding. Henry Weld, Xiaoqi Huang, Siqu Long, Josiah Poon, Caren Soyeon, Han, 10.1145/3547138ACM Comput. Surv. 8552022b</p>
<p>Unsupervised data augmentation for consistency training. Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, Quoc Le, Advances in neural information processing systems. 202033</p>
<p>The unreliability of explanations in few-shot prompting for textual reasoning. Xi Ye, Greg Durrett, Advances in Neural Information Processing Systems. 2022</p>
<p>Data augmentation for spoken language understanding via joint variational generation. Min Kang, Youhyun Yoo, Sang-Goo Shin, Lee, AAAI. 2019</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, arXiv:1904.09675Bertscore: Evaluating text generation with bert. 2019arXiv preprint</p>
<p>Ran Zhou, Xin Li, Lidong Bing, Erik Cambria, Luo Si, Chunyan Miao, arXiv:2211.09394Conner: Consistency training for cross-lingual named entity recognition. 2022arXiv preprint</p>
<p>Melm: Data augmentation with masked entity language modeling for low-resource ner. Ran Zhou, Xin Li, Ruidan He, Lidong Bing, Erik Cambria, Luo Si, Chunyan Miao, arXiv:2108.136552021arXiv preprint</p>
<p>Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, arXiv:2306.04528Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. 2023arXiv preprint</p>
<p>On robustness of prompt-based semantic parsing with large pre-trained language model: An empirical study on codex. Terry Yue Zhuo, Zhuang Li, Yujin Huang, Yuan-Fang Li, Weiqing Wang, Gholamreza Haffari, Fatemeh Shiri, arXiv:2301.128682023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
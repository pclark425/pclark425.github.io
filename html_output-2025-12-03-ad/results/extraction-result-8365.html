<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8365 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8365</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8365</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-c90a99eeb57019732a6cc996bb9eaf13faedf00f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c90a99eeb57019732a6cc996bb9eaf13faedf00f" target="_blank">In-context Learning and Induction Heads</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is found that induction heads develop at precisely the same point as a sudden sharp increase in in-context learning ability, visible as a bump in the training loss.</p>
                <p><strong>Paper Abstract:</strong> "Induction heads"are attention heads that implement a simple algorithm to complete token sequences like [A][B] ... [A] ->[B]. In this work, we present preliminary and indirect evidence for a hypothesis that induction heads might constitute the mechanism for the majority of all"in-context learning"in large transformer models (i.e. decreasing loss at increasing token indices). We find that induction heads develop at precisely the same point as a sudden sharp increase in in-context learning ability, visible as a bump in the training loss. We present six complementary lines of evidence, arguing that induction heads may be the mechanistic source of general in-context learning in transformer models of any size. For small attention-only models, we present strong, causal evidence; for larger models with MLPs, we present correlational evidence.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8365.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8365.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Induction heads</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Induction attention heads / induction head circuit</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An attention-head-level circuit (typically a pair of heads) that implements pattern completion over the context by attending to previous occurrences of the current token and copying the token that followed it, thereby increasing the probability of repeating sequences and enabling in‑context pattern completion and nearest‑neighbor style generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Varied Transformer models (attention-only and with MLPs; 1 to 40 layers, up to ≈13B parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multiple transformer snapshots studied across training: attention-only small models and larger models with MLPs (examples include 2-layer attention-only models and 40-layer ~13B models). Training runs examined over many saved checkpoints (phase change observed around ~2.5e9–5e9 training tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Not directly evaluated for arithmetic in this paper; induction heads are proposed as a general mechanism that could underlie emergent capabilities (authors mention multi-digit addition as an example of an abruptly-forming capability in prior literature).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Attention-based pattern completion: typically a two-head circuit where (1) a 'previous-token' head copies information from the previous token into the residual stream and (2) an 'induction' head attends to prior positions where the present token appeared and copies the subsequent token into the prediction. In larger models this mechanism is hypothesized to operate on more abstract representations (fuzzy / nearest-neighbor matches in embedding space) enabling analogical completion (e.g., translation).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Empirical prefix-matching / copying tests on repeated random sequences; 'copying' and 'prefix matching' scores; direct-path logit attribution; attention visualization; per-token loss analysis and PCA over per-token losses; head ablations (remove single heads at test time); architectural intervention ('smeared-key' variant interpolating current and previous-token keys); training-time observation of phase change (coincident timing of induction-head formation and in-context learning).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No numeric arithmetic performance metrics reported. For in-context learning (their macro metric): in-context learning score = loss(500th token) − loss(50th token). Observed jump in this score from ≲0.15 nats to ≈0.4 nats during the phase change; induction heads form in the same window. Head-specific metrics: copying and prefix-matching scores (examples: literal copying head copying score 0.89 / prefix 0.75; translation head copying 0.20 / prefix 0.85 in a 40-layer model example).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Not described for arithmetic. For the induction-head hypothesis the paper notes limitations: (a) in larger models evidence is correlational rather than causal, (b) ablations are strong for small attention-only models but missing for full-scale models, (c) possible confound that a shared latent change (e.g., new ability to compose layers via residual stream) causes both induction-head formation and in-context learning, (d) metric choice (50th vs 500th token) can obscure changes in underlying mechanisms post-phase-change.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Six complementary lines of evidence summarized by the authors: (1) Macroscopic co-occurrence — induction heads appear during a narrow phase-change window that coincides with a sudden increase in in-context learning and a visible bump in training loss; (2) Macroscopic co-perturbation — architectural smear-key intervention causes induction-head formation and corresponding in-context learning to shift earlier or enable it in 1-layer models; (3) Direct ablation — removing induction heads at test time in small attention-only models greatly reduces in-context learning; (4) Specific examples — induction heads in large models show literal copying, translation, and fuzzy nearest-neighbor completions consistent with broader in-context behaviors; (5) Mechanistic plausibility — for small models the two-head copying mechanism is mechanistically traced and can be extended conceptually to fuzzy matches; (6) Continuity — behaviors and signals related to induction heads and in-context learning are smoothly continuous from small to larger models, suggesting a shared mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Primary challenges: (1) For large models with MLPs evidence is mainly correlational and time-resolution of checkpoints is low, making causal claims weaker; (2) Absence of large-model ablation experiments (no full-scale ablations reported) limits claims that induction heads account for arithmetic or other emergent capabilities at scale; (3) Alternative mechanisms (e.g., MLP-mediated composition, residual-stream composition) could produce similar macroscopic effects and co-occur with induction-head formation; (4) The in-context learning metric (difference between fixed token indices) could mask continuing mechanistic evolution after the phase change.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'In-context Learning and Induction Heads', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8365.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8365.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Emergent arithmetic mention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Emergent multi-digit arithmetic / abrupt capability emergence (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A brief mention that certain neural-network capabilities (for example, multi-digit addition) are known from prior work to sometimes appear abruptly during training, analogous to the phase-change phenomenon the paper studies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-digit addition (mentioned as an example of an abruptly-forming capability), no experiments on arithmetic in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>No direct mechanism or representation for arithmetic is provided; the paper suggests such emergent abilities could plausibly be tied to mechanisms like induction heads or other phase-change phenomena but does not provide mechanistic analysis specific to arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Only an offhand citation-level claim: emergent capabilities such as multi-digit addition have been observed to form abruptly in prior work; this paper draws an analogy between those abrupt emergences and the phase change they observe where induction heads form and in-context learning increases.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>No experimental data in this paper assessing arithmetic; the paper does not evaluate arithmetic tasks or report successes/failures, so no counterexamples or error analyses for arithmetic are provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'In-context Learning and Induction Heads', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language Models are Few-Shot Learners <em>(Rating: 2)</em></li>
                <li>Scaling Laws for Neural Language Models <em>(Rating: 2)</em></li>
                <li>Attention Is All You Need <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8365",
    "paper_id": "paper-c90a99eeb57019732a6cc996bb9eaf13faedf00f",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "Induction heads",
            "name_full": "Induction attention heads / induction head circuit",
            "brief_description": "An attention-head-level circuit (typically a pair of heads) that implements pattern completion over the context by attending to previous occurrences of the current token and copying the token that followed it, thereby increasing the probability of repeating sequences and enabling in‑context pattern completion and nearest‑neighbor style generalization.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Varied Transformer models (attention-only and with MLPs; 1 to 40 layers, up to ≈13B parameters)",
            "model_description": "Multiple transformer snapshots studied across training: attention-only small models and larger models with MLPs (examples include 2-layer attention-only models and 40-layer ~13B models). Training runs examined over many saved checkpoints (phase change observed around ~2.5e9–5e9 training tokens).",
            "arithmetic_task_type": "Not directly evaluated for arithmetic in this paper; induction heads are proposed as a general mechanism that could underlie emergent capabilities (authors mention multi-digit addition as an example of an abruptly-forming capability in prior literature).",
            "mechanism_or_representation": "Attention-based pattern completion: typically a two-head circuit where (1) a 'previous-token' head copies information from the previous token into the residual stream and (2) an 'induction' head attends to prior positions where the present token appeared and copies the subsequent token into the prediction. In larger models this mechanism is hypothesized to operate on more abstract representations (fuzzy / nearest-neighbor matches in embedding space) enabling analogical completion (e.g., translation).",
            "probing_or_intervention_method": "Empirical prefix-matching / copying tests on repeated random sequences; 'copying' and 'prefix matching' scores; direct-path logit attribution; attention visualization; per-token loss analysis and PCA over per-token losses; head ablations (remove single heads at test time); architectural intervention ('smeared-key' variant interpolating current and previous-token keys); training-time observation of phase change (coincident timing of induction-head formation and in-context learning).",
            "performance_metrics": "No numeric arithmetic performance metrics reported. For in-context learning (their macro metric): in-context learning score = loss(500th token) − loss(50th token). Observed jump in this score from ≲0.15 nats to ≈0.4 nats during the phase change; induction heads form in the same window. Head-specific metrics: copying and prefix-matching scores (examples: literal copying head copying score 0.89 / prefix 0.75; translation head copying 0.20 / prefix 0.85 in a 40-layer model example).",
            "error_types_or_failure_modes": "Not described for arithmetic. For the induction-head hypothesis the paper notes limitations: (a) in larger models evidence is correlational rather than causal, (b) ablations are strong for small attention-only models but missing for full-scale models, (c) possible confound that a shared latent change (e.g., new ability to compose layers via residual stream) causes both induction-head formation and in-context learning, (d) metric choice (50th vs 500th token) can obscure changes in underlying mechanisms post-phase-change.",
            "evidence_for_mechanism": "Six complementary lines of evidence summarized by the authors: (1) Macroscopic co-occurrence — induction heads appear during a narrow phase-change window that coincides with a sudden increase in in-context learning and a visible bump in training loss; (2) Macroscopic co-perturbation — architectural smear-key intervention causes induction-head formation and corresponding in-context learning to shift earlier or enable it in 1-layer models; (3) Direct ablation — removing induction heads at test time in small attention-only models greatly reduces in-context learning; (4) Specific examples — induction heads in large models show literal copying, translation, and fuzzy nearest-neighbor completions consistent with broader in-context behaviors; (5) Mechanistic plausibility — for small models the two-head copying mechanism is mechanistically traced and can be extended conceptually to fuzzy matches; (6) Continuity — behaviors and signals related to induction heads and in-context learning are smoothly continuous from small to larger models, suggesting a shared mechanism.",
            "counterexamples_or_challenges": "Primary challenges: (1) For large models with MLPs evidence is mainly correlational and time-resolution of checkpoints is low, making causal claims weaker; (2) Absence of large-model ablation experiments (no full-scale ablations reported) limits claims that induction heads account for arithmetic or other emergent capabilities at scale; (3) Alternative mechanisms (e.g., MLP-mediated composition, residual-stream composition) could produce similar macroscopic effects and co-occur with induction-head formation; (4) The in-context learning metric (difference between fixed token indices) could mask continuing mechanistic evolution after the phase change.",
            "uuid": "e8365.0",
            "source_info": {
                "paper_title": "In-context Learning and Induction Heads",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Emergent arithmetic mention",
            "name_full": "Emergent multi-digit arithmetic / abrupt capability emergence (mentioned)",
            "brief_description": "A brief mention that certain neural-network capabilities (for example, multi-digit addition) are known from prior work to sometimes appear abruptly during training, analogous to the phase-change phenomenon the paper studies.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "arithmetic_task_type": "Multi-digit addition (mentioned as an example of an abruptly-forming capability), no experiments on arithmetic in this paper.",
            "mechanism_or_representation": "No direct mechanism or representation for arithmetic is provided; the paper suggests such emergent abilities could plausibly be tied to mechanisms like induction heads or other phase-change phenomena but does not provide mechanistic analysis specific to arithmetic.",
            "probing_or_intervention_method": null,
            "performance_metrics": null,
            "error_types_or_failure_modes": null,
            "evidence_for_mechanism": "Only an offhand citation-level claim: emergent capabilities such as multi-digit addition have been observed to form abruptly in prior work; this paper draws an analogy between those abrupt emergences and the phase change they observe where induction heads form and in-context learning increases.",
            "counterexamples_or_challenges": "No experimental data in this paper assessing arithmetic; the paper does not evaluate arithmetic tasks or report successes/failures, so no counterexamples or error analyses for arithmetic are provided here.",
            "uuid": "e8365.1",
            "source_info": {
                "paper_title": "In-context Learning and Induction Heads",
                "publication_date_yy_mm": "2022-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language Models are Few-Shot Learners",
            "rating": 2
        },
        {
            "paper_title": "Scaling Laws for Neural Language Models",
            "rating": 2
        },
        {
            "paper_title": "Attention Is All You Need",
            "rating": 1
        }
    ],
    "cost": 0.0098035,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>In-context Learning and Induction Heads</h1>
<p>AUTHORS
Catherine Olsson $\dagger$ Nelson Elhage $\dagger$ Neel Nanda $\dagger$ Nicholas Joseph<em>, Nova DasSarma</em>, Tom Henighan<em>, Ben Mann</em>, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, Chris Olah*</p>
<p>AFFILIATION
Anthropic</p>
<p>PUBLISHED
Mar 8, 2022</p>
<ul>
<li>Core Research Contributor; † Core Infrastructure Contributor; 6 Correspondence to colah@anthropic.com; Author contributions statement below.</li>
</ul>
<h4>Abstract</h4>
<p>"Induction heads" are attention heads that implement a simple algorithm to complete token sequences like $[A][B] \ldots[A] \rightarrow[B]$. In this work, we present preliminary and indirect evidence for a hypothesis that induction heads might constitute the mechanism for the majority of all "incontext learning" in large transformer models (i.e. decreasing loss at increasing token indices). We find that induction heads develop at precisely the same point as a sudden sharp increase in incontext learning ability, visible as a bump in the training loss. We present six complementary lines of evidence, arguing that induction heads may be the mechanistic source of general in-context learning in transformer models of any size. For small attention-only models, we present strong, causal evidence; for larger models with MLPs, we present correlational evidence.</p>
<h2>We recommend reading this paper as an HTML article.</h2>
<p>As Transformer generative models continue to scale and gain increasing real world use, [1, 2, 3, 4, 5] addressing their associated safety problems becomes increasingly important. Mechanistic interpretability - attempting to reverse engineer the detailed computations performed by the model - offers one possible avenue for addressing these safety issues. If we can understand the internal structures that cause Transformer models to produce the outputs they do, then we may be able to address current safety problems more systematically, as well as anticipating safety problems in future more powerful models. ${ }^{1}$</p>
<p>In the past, mechanistic interpretability has largely focused on CNN vision models [6], but recently, we presented some very preliminary progress on mechanistic interpretability for Transformer language models [7]. Specifically, in our prior work we developed a mathematical framework for decomposing the operations of transformers, which allowed us to make sense of small (1 and 2 layer attention-only) models and give a near-complete account of how they function. Perhaps the most interesting finding was the induction head, a circuit whose function is to look back over the sequence for previous instances of the current token (call it A ), find the token that came after it last time (call it B ), and then predict that the same completion will occur again (e.g. forming the sequence $[A][B] \ldots[A] \rightarrow[B]$ ). In other words, induction heads "complete the pattern" by copying and completing sequences that have occurred before. Mechanically, induction heads in our models are implemented by a circuit of two attention heads: the first head is a "previous token head" which copies information from the previous token into the next token, while the second head (the actual "induction head") uses that information to find tokens preceded by the present token. For 2-layer attention-only models, ${ }^{2}$ we were able to show precisely that induction heads implement this pattern copying behavior and appear to be the primary source of in-context learning.</p>
<p>Ultimately, however, our goal is to reverse-engineer frontier language models (which often contain hundreds of layers and billions or trillions of parameters), not merely 2-layer attention-only models. Unfortunately, both the presence of many layers, and the presence of MLPs, makes it much more difficult to mathematically pin down the precise circuitry of these models. However, a different approach is possible: by empirically observing, perturbing, and studying the learning process and the formation of various structures, we can try to assemble an indirect case for what might be happening mechanistically inside the network. This is somewhat similar to how a neuroscientist might gain understanding of how part of the brain functions by looking at neural development over time, studying patients with an injury to that part of the brain, perturbing brain function in animals, or looking at a select small number of relevant neurons.</p>
<p>In this paper, we take the first preliminary steps towards building such an indirect case. In particular, we present preliminary and indirect evidence for a tantalizing hypothesis: that induction heads might constitute the mechanism for the actual majority of all in-context learning in large transformer models. Specifically, the thesis is that there are circuits which have the same or similar mechanism to the 2-layer induction heads and which perform a "fuzzy" or "nearest neighbor" version of pattern completion, completing $[A <em>]\left[B^{</em>}\right] \ldots[A] \rightarrow[B]$, where $A^{<em>}=A$ and $B^{</em>}=B$ are similar in some space; and furthermore, that these circuits implement most in-context learning in large models.</p>
<p>The primary way in which we obtain this evidence is via discovery and study of a phase change that occurs early in training for language models of every size (provided they have more than one layer), and which is visible as a bump in the training loss. During this phase change, the majority of incontext learning ability (as measured by difference in loss between tokens early and late in the sequence) is acquired, and simultaneously induction heads form within the model that are capable of implementing fairly abstract and fuzzy versions of pattern completion. We study this connection in detail to try to establish that it is causal, including showing that if we perturb the transformer architecture in a way that causes the induction bump to occur in a different place in training, then the formation of induction heads as well as formation of in-context learning simultaneously move along with it.</p>
<p>Specifically, the paper presents six complementary lines of evidence arguing that induction heads may be the mechanistic source of general in-context learning in transformer models of any size:</p>
<ul>
<li>Argument 1 (Macroscopic co-occurence): Transformer language models undergo a “phase change” early in training, during which induction heads form and simultaneously in-context learning improves dramatically.</li>
<li>Argument 2 (Macroscopic co-perturbation): When we change the transformer architecture in a way that shifts whether induction heads can form (and when), the dramatic improvement in in-context learning shifts in a precisely matching way.</li>
<li>Argument 3 (Direct ablation): When we directly “knock out” induction heads at test-time in small models, the amount of in-context learning greatly decreases.</li>
<li>Argument 4 (Specific examples of induction head generality): Although we define induction heads very narrowly in terms of copying literal sequences, we empirically observe that these same heads also appear to implement more sophisticated types of in-context learning, including highly abstract behaviors, making it plausible they explain a large fraction of in-context learning.</li>
<li>Argument 5 (Mechanistic plausibility of induction head generality): For small models, we can explain mechanistically how induction heads work, and can show they contribute to in-context learning. Furthermore, the actual mechanism of operation suggests natural ways in which it could be re-purposed to perform more general in-context learning.</li>
<li>Argument 6 (Continuity from small to large models): In the previous 5 arguments, the case for induction heads explaining in-context learning is stronger for small models than for large ones. However, many behaviors and data related to both induction heads and in-context learning are smoothly continuous from small to large models, suggesting the simplest explanation is that mechanisms are the same.</li>
</ul>
<p>Together the claims establish a circumstantial case that induction heads might be responsible for the majority of in-context learning in state-of-the-art transformer models. We emphasize that our results here are only the beginnings of evidence for such a case, and that like any empirical or interventional study, a large number of subtle confounds or alternative hypotheses are possible – which we discuss in the relevant sections. But we considered these results worth reporting, both because future work could build on our results to establish the claim more firmly, and because this kind of indirect evidence is likely to be common in interpretability as it advances, so we'd like to establish a norm of reporting it even when it is not fully conclusive.</p>
<p>Finally, in addition to being instrumental for tying induction heads to in-context learning, the phase change may have relevance to safety in its own right. Neural network capabilities — such as multi-digit addition — are known to sometimes abruptly form or change as models train or increase in scale [8, 1] , and are of particular concern for safety as they mean that undesired or dangerous behavior could emerge abruptly. For example reward hacking, a type of safety problem, can emerge in such a phase change [9] . Thus, studying a phase change “up close” and better understanding its internal mechanics could contain generalizable lessons for addressing safety problems in future systems. In particular, the phase change we observe forms an interesting potential bridge between the microscopic domain of interpretability and the macroscopic domain of scaling laws and learning dynamics.</p>
<p>The rest of the paper is organized as follows. We start by clarifying several key concepts and definitions, including in-context learning, induction heads, and a “per-token loss analysis” method we use throughout. We then present the 6 arguments one by one, drawing on evidence from analysis of 34 transformers over the course of training, including more than 50,000 attention head ablations (the data of which is shown in the Model Analysis Table). We then discuss some unexplained “curiosities” in our findings, as well as reviewing related work.</p>
<h2></h2>
<h1>Key Concepts</h1>
<h2>In-context Learning</h2>
<p>In modern language models, tokens later in the context are easier to predict than tokens earlier in the context. As the context gets longer, loss goes down. In some sense this is just what a sequence model is designed to do (use earlier elements in the sequence to predict later ones), but as the ability to predict later tokens from earlier ones gets better, it can increasingly be used in interesting ways (such as specifying tasks, giving instructions, or asking the model to match a pattern) that suggest it can usefully be thought of as a phenomenon of its own. When thought of in this way, it is usually referred to as in-context learning. ${ }^{3}$</p>
<p>Emergent in-context learning was noted in GPT-2 [10] and gained significant attention in GPT-3 [1]. Simply by adjusting a "prompt", transformers can be adapted to do many useful things without retraining, such as translation, question-answering, arithmetic, and many other tasks. Using "prompt engineering" to leverage in-context learning became a popular topic of study and discussion [11, 12].</p>
<p>At least two importantly different ways of conceptualizing and measuring in-context learning exist in the literature. The first conception, represented in Brown et al., focuses on few-shot learning [1] of specific tasks. The model is prompted with several instances of some "task" framed in a next-token-prediction format (such as few-digit addition, or English-to-French translation). The second conception of in-context learning, represented in Kaplan et al. [13], focuses on observing the loss at different token indices, in order to measure how much better the model gets at prediction as it receives more context. The first conception can be thought of as a micro perspective (focusing on specific tasks), where as the second conception can be seen as a macro perspective (focusing on general loss, which on average correlates with these tasks).</p>
<p>The "few-shot learning" conception of in-context learning has tended to receive greater community attention. The ability to do many different tasks with one large model, even without further finetuning, is a notable change to the basic economics of model training. Moreover, it gives evidence of wide-ranging general capabilities and the ability to adapt on the fly, which nudges us to re-examine what it means for a model to "understand" or to "reason".</p>
<p>However, for the purposes of this work, we focus instead on the Kaplan et al. conception: decreasing loss at increasing token indices. We do so because it's a more general framing of the phenomenon than "few-shot learning". A drawback of this definition is it fails to isolate specific behaviors of interest. At the same time, it allows us to measure models' overall ability to learn on-the-fly from the context, without depending on our specific choices of "task". We'll also see that, starting from this definition, we are also able to study a couple classic few-shot learning examples (see Argument 4).</p>
<p>Throughout this work we compute a simple heuristic measure of in-context learning:</p>
<ul>
<li>In-context learning score: the loss of the 500th token in the context minus the average loss of the 50th token in the context, averaged over dataset examples.</li>
</ul>
<p>We chose the 500th and 50th token indices somewhat arbitrarily. The 500th token is near the end of a length-512 context, and the 50th token is far enough into the context that some basic properties of the text have been established (such as language and document type) while still being near the start. We will also show that picking different numbers here does not change our conclusions.</p>
<p>Finally, it is worth noting that in-context learning is of potentially special relevance to safety. Incontext learning makes it harder to anticipate how a model might behave after a long context. In the longer run, concepts such as mesa-optimization or inner-alignment [14] postulate that meaningful learning or optimization could occur at test time (without changing the weights). In-context learning would be an obvious future mechanism for such hidden optimization to occur, whether or not it does so today. Thus, studying in-context learning seems valuable for the future.
(See Related Work for more on in-context learning, and Discussion for more on the connection to safety.)</p>
<h1>Induction Heads</h1>
<p>In our previous paper, we discovered a special kind of attention head - which we named induction heads - in two layer attention-only models. Induction heads are implemented by a circuit consisting of a pair of attention heads in different layers that work together to copy or complete patterns. The first attention head copies information from the previous token into each token. This makes it possible for the second attention head to attend to tokens based on what happened before them, rather than their own content. Specifically, the second head (which we call the "induction head") search for a previous place in the sequence where the present token A occurred and attends to the next token (call it B ), copying it and causing the model to be more likely to output B as the next token. That is, the two heads working together cause the sequence $\mathcal{L}[\mathrm{A}][\mathrm{B}] \ldots[\mathrm{A}]$ to be more likely to be completed with [B].</p>
<p>Induction heads are named by analogy to inductive reasoning. In inductive reasoning, we might infer that if $A$ is followed by $B$ earlier in the context, $A$ is more likely to be followed by $B$ again later in the same context. Induction heads crystallize that inference. They search the context for previous instances of the present token, attend to the token which would come next if the pattern repeated, and increase its probability. Induction heads attend to tokens that would be predicted by basic induction (over the context, rather than over the training data).</p>
<p>Notice that induction heads are implementing a simple algorithm, and are not memorizing a fixed table of n-gram statistics. The rule $[A][B] \ldots[A] \rightarrow[B]$ applies regardless of what $A$ and $B$ are. ${ }^{4}$ This means that induction heads can in some sense work out of distribution, as long as local statistics early in the context are representative of statistics later. This hints that they may be capable of more general and abstract behavior.</p>
<p>Our previous paper focused on a few explorations of induction heads, including showing that these heads occur in 2-layer attention-only models (but not 1-layer models); tracking down how they operate mechanistically as part of our mathematical decomposition of transformers; and making an eigenvalue-based test for detecting their presence. However, we were a bit vague on the exact definition of induction heads: it was more like we found a cluster of behaviors and mechanisms that tended to occur together, and called heads in that cluster "induction heads".</p>
<p>In this paper our goal is to provide evidence for something more expansive: that induction heads play a major role in general in-context learning (not just literal $[A][B] \ldots[A] \rightarrow[B]$ copying), for large models and not just for small 2-layer attention only models. To do this clearly and coherently, we need a more precise definition of induction heads. Mechanistic analysis of weights and eigenvalue analysis are much more complicated in large models with MLP's, so for this paper we choose to define induction heads by their narrow empirical sequence copying behavior (the $[A][B] \ldots[A] \rightarrow[B]$ ), and then attempt to show that they (1) also serve a more expansive function that can be tied to in-context learning, and (2) coincide with the mechanistic picture for small models.</p>
<p>Formally, we define an induction head as one which exhibits the following two properties ${ }^{5}$ on a repeated random sequence ${ }^{6}$ of tokens:</p>
<ul>
<li>Prefix matching: The head attends back to previous tokens that were followed by the current and/or recent tokens. ${ }^{7}$ That is, it attends to the token which induction would suggest comes next.</li>
<li>Copying: The head's output increases the logit corresponding to the attended-to token.</li>
</ul>
<p>In other words, induction heads are any heads that empirically increase the likelihood of [B] given [A][B]...[A] when shown a repeated sequence of completely random tokens. An illustration of induction heads' behavior is shown here:
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Note that, as a consequence, induction heads will tend to be good at repeating sequences wholesale. For example, given "The cat sat on the mat. The cat ...", induction heads will promote the continuation "sat on the mat". This gives a first hint of how they might be connected to general incontext learning and even few-shot learning: they learn to repeat arbitrary sequences, which is a (simple) form of few-shot learning.</p>
<p>One of things we'll be trying to establish is that when induction heads occur in sufficiently large models and operate on sufficiently abstract representations, the very same heads that do this sequence copying also take on a more expanded role of analogical sequence copying or in-context nearest neighbors. By this we mean that they promote sequence completions like $[A <em>]\left[B^{</em>}\right] \ldots[A] \rightarrow[B]$ where $A^{<em>}$ is not exactly the same token as $A$ but similar in some embedding space, and also $B$ is not exactly the same token as $B^{</em>}$. For example, $A$ and $A^{<em>}$ (as well as B and $\mathrm{B}^{</em>}$ ) might be the same word in different languages, and the induction head can then translate a sentence word by word by looking for "something like $A$ ", finding $A^{<em>}$ followed by $B^{</em>}$, and then completing with "something like B* " (which is B ). We are not yet able to prove mechanistically that induction heads do this in general, but in Argument 4 we show empirical examples of induction heads behaving in this way (including on translation), and in Argument 5 we point out that the known copying mechanism of induction heads in small models can be naturally adapted to function in this way.</p>
<h1>Per-Token Loss Analysis</h1>
<p>To better understand how models evolve during training, we analyze what we call the "per-token loss vectors." The core idea traces back to a method used by Erhan et al. [15], and more generally to the idea of "function spaces" in mathematics. ${ }^{8}$</p>
<p>We start with a collection of models. (In our use, we'll train several different model architectures, saving dozens of "snapshots" of each over the course of training. We'll use this set of snapshots as our collection of models.) Next, we collect the log-likelihoods each model assigns to a consistent set of 10,000 random tokens, each taken from a different example sequence. We combine these log-likelihoods into a "per-token loss vector" and apply Principal Component Analysis (PCA):</p>
<p>Step 1: Run each model / snapshot over the same set of multiple dataset examples, collecting one token's loss per example.</p>
<p>Step 2: For each sample, extract the loss of a consistent token. Combine these to make a vector of losses per model / snapshot.</p>
<p>Step 3: The vectors are jointly reduced with principal component analysis to project them into a shared 2D space.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>A more detailed discussion of technical details can be found in the Appendix.
By applying this method to snapshots over training for multiple models, we can visualize and compare how different models' training trajectories evolve in terms of their outputs. Since we're using PCA, each direction can be thought of as a vector of log-likelihoods that models are moving along. We particularly focus on the first two principal components, since we can easily visualize those. Of course, models also move in directions not captured by the first two principal components, but it's a useful visualization for capturing the highest-level story of training.</p>
<h1>Arguments that induction heads are the mechanism for the majority of in-context learning.</h1>
<p>Now we'll proceed to the main part of the paper, which makes the case that induction heads may provide the primary mechanism for the majority of in-context learning for transformer models in general. As stated in the introduction, this is a very broad hypothesis and much of our evidence is indirect, but nevertheless we believe that all the lines of evidence together make a relatively strong, though not conclusive, case.</p>
<p>Before we go through the arguments, it's useful to delineate where the evidence is more conclusive vs. less conclusive. This is shown in the table below. For small, attention-only models, we believe we have strong evidence that attention heads are the mechanism for the majority of in-context learning, as we have evidence supported by ablations and mechanistic reverse engineering. Conversely, for all models, we can make a strong case that induction heads play some role in incontext learning, as we can demonstrate examples and show suggestive correlations. However, the larger the models get, the harder it is to establish that induction heads account for the actual majority of in-context learning. For large models with MLP's, we must therefore rely on mainly correlational evidence, which could be confounded. We explore alternate hypotheses throughout, including at the end of Argument 1 and again briefly in Argument 6.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Small Attention-Only</th>
<th style="text-align: left;">Small with MLPs</th>
<th style="text-align: left;">Large Models</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Contributes Some</td>
<td style="text-align: left;">Strong, Causal</td>
<td style="text-align: left;">Strong, Causal</td>
<td style="text-align: left;">Medium, Correlational \&amp; <br> Mechanistic</td>
</tr>
<tr>
<td style="text-align: left;">Contributes Majority</td>
<td style="text-align: left;">Strong, Causal</td>
<td style="text-align: left;">Medium, Causal</td>
<td style="text-align: left;">Medium, Correlational</td>
</tr>
</tbody>
</table>
<p>Here is the list of arguments we'll be making, one per section, repeated from the introduction:</p>
<ul>
<li>Argument 1 (Macroscopic co-occurrence): Transformer language models undergo a "phase change" early in training, during which induction heads form and simultaneously in-context learning improves dramatically.</li>
<li>Argument 2 (Macroscopic co-perturbation): When we change the transformer architecture in a way that shifts whether induction heads can form (and when), the dramatic improvement in in-context learning shifts in a precisely matching way.</li>
<li>Argument 3 (Direct ablation): When we directly "knock out" induction heads at test-time in small models, the amount of in-context learning greatly decreases.</li>
<li>Argument 4 (Specific examples of induction head generality): Although we define induction heads very narrowly in terms of copying literal sequences, we empirically observe that these same heads also appear to implement more sophisticated types of in-context learning, including highly abstract behaviors, making it plausible they explain a large fraction of incontext learning.</li>
<li>Argument 5 (Mechanistic plausibility of induction head generality): For small models, we can explain mechanistically how induction heads work, and can show they contribute to incontext learning. Furthermore, the actual mechanism of operation suggests natural ways in which it could be repurposed to perform more general in-context learning.</li>
<li>Argument 6 (Continuity from small to large models): In the previous 5 arguments, the case for induction heads explaining in-context learning is stronger for small models than for large ones. However, many behaviors and data related to both induction heads and in-context learning are smoothly continuous from small to large models, suggesting the simplest explanation is that mechanisms are the same.</li>
</ul>
<p>For each argument, we'll have a similar table to the one in this section, showing the strength of the evidence provided by that claim as it applies to large/small models and some/most of context learning. The table above is the sum of the evidence from all six lines of reasoning.</p>
<h1>Argument 1: Transformer language models undergo a "phase change" during training, during which induction heads form and simultaneously in-context learning improves dramatically.</h1>
<p>Our first line of evidence comes from correlating measures of in-context learning and measures of induction head presence over the course of training. Specifically, we observe a tight co-variation between them across dozens of models, of different sizes, trained on different datasets (See Model Analysis Table for more on the models in which we observe this co-occurrence.).</p>
<p>The table below summarizes the quality of this evidence for the models we have studied: it applies to both large and small models, and is the expected outcome if induction heads were responsible for the majority of in-context learning, but it is only correlational and so could be confounded (discussed more below).</p>
<h1>STRENGTH OF ARGUMENT FOR SUB-CLAIMS</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Contributes Some</th>
<th style="text-align: left;">Small Attention-Only</th>
<th style="text-align: left;">Small with MLPs</th>
<th style="text-align: left;">Large Models</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Medium, Correlational</td>
<td style="text-align: left;">Medium, Correlational</td>
<td style="text-align: left;">Medium, Correlational</td>
</tr>
<tr>
<td style="text-align: left;">Contributes Majority</td>
<td style="text-align: left;">Medium, Correlational</td>
<td style="text-align: left;">Medium, Correlational</td>
<td style="text-align: left;">Medium, Correlational</td>
</tr>
</tbody>
</table>
<p>Our first observation is that if we measure in-context learning for transformer models over the course of training (defined as the 50th token loss minus the 500th token loss as described in Key Concepts), it develops abruptly in a narrow window early in training (roughly 2.5 to 5 billion tokens) and then is constant for the rest of training (see figure below). Before this window there is less than 0.15 nats of in-context learning, after it there is roughly 0.4 nats, an amount that remains constant for the rest of training and is also constant across many different model sizes (except for the one layer model where not much in-context learning ever forms). This seems surprising - naively, one might expect in-context learning to improve gradually over training, and improve with larger model sizes, ${ }^{9}$ as most things in machine learning do.</p>
<h2>MODELS WITH MORE THAN ONE LAYER HAVE AN ABRUPT IMPROVEMENT IN IN-CONTEXT LEARNING</h2>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>We highlight the "phase change" period of training in plots to make visual comparision between plots easier. The highlighted region is selected for each model based on the derivative of in-context learning.</p>
<p>Although we only show three models above, the pattern holds true very generally: many examples are shown in the Model Analysis Table later in the paper, including models of varying model architecture and size.</p>
<p>One might wonder if the sudden increase is somehow an artifact of the choice to define in-context learning in terms of the difference between the 500th and 50th tokens. We'll discuss this in more depth later. But for now, an easy way to see that this is a robust phenomena is to look at the derivative of loss with respect to the logarithm token index in context. You can think of this as measuring something like "in-context learning per $\epsilon \%$ increase in context length." We can visualize this on a 2D plot, where one axis is the amount of training that has elapsed, the other is the token index being predicted. Before the phase change, loss largely stops improving around token 50, but after the phase change, loss continues to improve past that point.</p>
<h1>DERIVATIVE OF LOSS WITH RESPECT TO LOG TOKEN INDEX</h1>
<p>The rate at which loss decreases with increasing token index can be thought of as something like "in-context learning per token". This appears to be most naturally measured with respect to the log number of tokens.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>It turns out that a sudden improvement in in-context learning isn't the only thing that changes in this window. If we go through the attention heads of a model and and score them for whether they are induction heads (using a prefix matching score which measures their ability to perform the task we used to define induction heads in Key Concepts), we find that induction heads form abruptly during exactly the same window where in-context learning develops (figure below). Again we show only a few models here, but a full set is shown in the Model Analysis Table. The exception is the one-layer model, where induction heads never form - just as in-context learning never substantially develops for the one-layer model.</p>
<h1>INDUCTION HEADS FORM IN PHASE CHANGE</h1>
<p>Each line is an attention head, scored by the "prefix matching" evaluation introduced below.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>This already strongly suggests some connection between induction heads and in-context learning, but beyond just that, it appears this window is a pivotal point for the training process in general: whatever's occurring is visible as a bump on the training curve (figure below). It is in fact the only place in training where the loss is not convex (monotonically decreasing in slope).</p>
<p>That might not sound significant, but the loss curve is averaging over many thousands of tokens. Many behaviors people find interesting in language models, such as the emergence of arithmetic, would be microscopic on the loss curve. For something to be visible at that scale suggests it's a widespread, major change in model behavior. This shift also appears to be the first point where, at least for small models, the loss curve diverges from a one-layer model - which does not display the bump, just as it does not display the other abrupt changes.</p>
<h1>LOSS CURVES DIVERGE DURING PHASE CHANGE</h1>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>The highlighted "phase change" portion of training is the same area highlighted in previous plots. It is selected based on the derivative of the in-context score.</p>
<p>We can also apply principal components analysis (PCA) to the per-token losses, as described in per-token-loss analysis, which allows us to summarize the main dimensions of variation in how several models' predictions vary over the course of training.</p>
<p>Below we show the first two principal components of these models' predictions, with the golden outline highlighting the same interval shown above, when in-context learning abruptly improved. We see that the training trajectories pivot during exactly the same window where the other changes happen. In some sense, whatever is occurring when in-context learning improves is the primary deviation from the basic trajectory our transformers follow during the course of their training. Once again the only exception is the one-layer model - where induction heads cannot form and incontext learning does not improve.</p>
<h2>PER-TOKEN LOSS PRINCIPAL COMPONENT ANALYSIS</h2>
<p>This method is described above, but roughly shows models' training trajectories on the two dimensions there is the most behavioral variance. Note how it abruptly pivots for models with more than one layer.</p>
<h2>ONE LAYER</h2>
<p>(ATTENTION-ONLY)
First Principal Component</p>
<p>TWO LAYER
(ATTENTION-ONLY)
First Principal Component</p>
<p>THREE LAYER
(ATTENTION-ONLY)
First Principal Component</p>
<p>THREE LAYER
(ATTENTION-ONLY)
First Principal Component</p>
<p>The "phase change" portion of training highlighted in orange is the same as in previous plots. It is selected based on the derivative of the in-context score.</p>
<p>One-layer model
trajectory stays same.</p>
<p>The trajectory of models with more than one layer pivots orthogonally at the same point.</p>
<p>In summary, the following things all co-occur during the same abrupt window:</p>
<ul>
<li>Capacity for in-context learning sharply improves (as measured via the in-context learning score).</li>
<li>Induction heads form.</li>
<li>Loss undergoes a small "bump" (that is, the loss curve undergoes a period of visibly steeper improvement than the parts of the curve before and after).</li>
<li>The model's trajectory abruptly changes (in the space of per-token losses, as visualized with PCA).</li>
</ul>
<p>Collectively these results suggest that some important transition is happening during the 2.5 e 9 to 5 e 9 token window early in training (for large models this is maybe $1-2 \%$ of the way through training). We call this transition "the phase change", in that it's an abrupt change that alters the model's behavior and has both macroscopic (loss and in-context learning curves) and microscopic (induction heads) manifestations, perhaps analogous to e.g. ice melting. ${ }^{10}$</p>
<h1>Looking at the Phase Change More Closely</h1>
<p>A natural explanation would be that for all these models, the induction heads implement in-context learning: their formation is what drives all the other changes observed. To strengthen this hypothesis a bit, we check a few things. First, the window where the phase change happens doesn't appear to correspond to a scheduled change in learning rate, warmup, or weight decay; there is not some known exogenous factor precipitating everything. Second, we tried out training some of the small models on a different dataset, and we observed the phase change develop in the same way (see Model Analysis Table for more details). ${ }^{11}$</p>
<p>Third, to strengthen the connection a little more, we look qualitatively and anecdotally at what's going on with the model's behavior during the phase change. One way to do this is to look at specific tokens the model gets better and worse at predicting. The model's loss is an average of billions of log-likelihood losses for individual tokens. By pulling them apart, we can get a sense for what's changed.</p>
<p>Concretely, let's pick a piece of text - for fun, we'll use the first paragraph of Harry Potter - and look at the differences in log-likelihoods comparing the start and the end of the phase change. ${ }^{12}$ We'll notice that the majority of the changes occur when tokens are repeated multiple times in the text. If a sequence of tokens occurs multiple times, the model is better at predicting the sequence the second time it shows up. On the other hand, if a token is followed by a different token than it previously was, the post-phase-change model is worse at predicting it:</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>The training trajectory of models abruptly pivots during a "phase change." We look at the difference and loss before and after. As seen on the right, the change seems to be related to whether text is repeated. The qualitiative change is similar across all models.</p>
<h2>5. 5 PER-TOKEN LOSSES ON HARRY POTTER</h2>
<p>Mr and Mrs Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense. Mr Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large moustache. Mrs Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbours. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere. The Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didn't think they could bear it if anyone found out about the Potters. Mrs Potter was Mrs Dursley's sister, but they hadn't met for several years; in fact, Mrs Dursley pretended she didn't have a sister, because her sister and her good- for-nothing husband were as unDursleysfit as it was possible to be. The Dursleys shuddered to think what the neighbours would say if the Potters arrived in the street. The Dursleys knew that the Potters had a small son, too, but they had never even seen him. This boy was another good reason for keeping the Potters away; they didn't want Dudley mixing with a child like that.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Mrs Dursley</th>
<th style="text-align: left;">Mrs Dursley</th>
<th style="text-align: left;">When text is repeated, the</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">the Potters</td>
<td style="text-align: left;">the Potters</td>
<td style="text-align: left;">post phase-change model</td>
</tr>
<tr>
<td style="text-align: left;">a small son</td>
<td style="text-align: left;">a small son</td>
<td style="text-align: left;">better predicts: tokens:</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">very large</th>
<th style="text-align: left;">very useful</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">didn't hold</td>
<td style="text-align: left;">didn't think</td>
</tr>
<tr>
<td style="text-align: left;">Mrs Dursley</td>
<td style="text-align: left;">Mrs Potter</td>
</tr>
</tbody>
</table>
<p>... while cases where the same token is followed by a different token are given lower probability.</p>
<p>We can also do this same analysis over the course of model training. The loss curve is the average of millions of per-token loss curves. We can break this apart and look at the loss curves for individual tokens.</p>
<p>In particular, let's take a look at per-token loss trajectories for two tokens in the first paragraph of Harry Potter. In red, we show a token whose prediction gets dramatically better during the phase change: it's the last of four tokens in " The Dursleys", a sequence that appears several times in the text. In blue, we show a token that gets meaningfully worse during the phase change: it's the firstever appearance of " Mrs Potter", after both previous instances of " Mrs" were followed by " Dursley" instead.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<h2>PER-TOKEN LOSSES OVER TRAINING</h2>
<p>We look at two specific tokens in the Harry Potter passage a to highlight their movement during the phase change.</p>
<p>Mrs Potter: This token's loss increases during the phase change, because the model incorrectly predicts that the next token should be "D" from "Dursley".</p>
<p>The Dursleys: This token's loss decreases during the phase change, because the model correctly predicts "leys" based on a previous example.</p>
<p>All of this shows that during the phase change, we see exactly the behaviors we'd expect to see if induction heads were indeed contributing the majority of in-context learning.</p>
<h1>Assessing the Evidence</h1>
<p>Despite all of the co-occurrence evidence above (and more in the Model Analysis Table), the fact remains that we haven't shown that induction heads are the primary mechanism for in-context learning. We have merely shown that induction heads form at the same time as in-context learning, and that in-context learning does not improve thereafter. There are a number of potential confounds in this story. Below we summarize reasons in favor and against thinking the connection is causal. The argument in favor might look like this:</p>
<ul>
<li>The formation of induction heads is correlated with a great increase in models' capacity for in-context learning, for a wide variety of models large and small.</li>
<li>It's highly improbable these two sharp transitions would co-occur across so many models by pure chance, without any causal connection.</li>
<li>There is almost surely some connection, and the simplest possibility is that induction heads are the primary mechanism driving the observed increase in in-context learning. (However, as discussed below, it could also be a confounding variable.)</li>
<li>Since over $75 \%$ of final in-context learning forms in this window, one might naively believe this to be the amount of in-context learning that induction heads are responsible for.</li>
</ul>
<p>However, the following issues and confounds suggest caution:</p>
<ul>
<li>In large models, we have low time resolution on our analysis over training. Co-occurrence when one only has 15 points in time is less surprising and weaker evidence.</li>
<li>Perhaps other mechanisms form in our models at this point, that contribute to not only induction heads, but also other sources of in-context learning. (For example, perhaps the phase change is really the point at which the model learns how to compose layers through the residual stream, enabling both induction heads and potentially many other mechanisms that also require composition of multiple heads) Put another way, perhaps the co-occurrence is primarily caused by a shared latent variable, rather than direct causality from induction heads to the full observed change in in-context learning.</li>
<li>The fact that in-context learning score is roughly constant (at 0.4 nats) after the phase change doesn't necessarily mean that the underlying mechanisms of in-context learning are constant after that point. In particular, the metric we use measures a relative loss between the token index 500 and 50, and we know that the model's performance at token 50 improves over training time. Reducing the loss a fixed amount from a lower baseline is likely harder, and so may be driven by additional mechanisms as training time goes on. ${ }^{13} 14$</li>
</ul>
<p>One point worth noting here is that the argument that induction heads account for most in-context learning at the transition point of the phase change is more solid than the argument that they account for most in-context learning at the end of training - a lot could be changing during training even as the in-context learning score remains constant.</p>
<h1>Argument 2: When we change the transformer architecture in a way that shifts when induction heads form or whether they can form, the dramatic improvement in incontext learning shifts in a precisely matching way.</h1>
<p>STRENGTH OF ARGUMENT FOR SUB-CLAIMS</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Small Attention-Only</th>
<th style="text-align: left;">Small with MLPs</th>
<th style="text-align: left;">Large Models</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Contributes Some</td>
<td style="text-align: left;">Medium, Interventional</td>
<td style="text-align: left;">Medium, Interventional</td>
<td style="text-align: left;">Weak, Interventional</td>
</tr>
<tr>
<td style="text-align: left;">Contributes Majority</td>
<td style="text-align: left;">Medium, Interventional</td>
<td style="text-align: left;">Medium, Interventional</td>
<td style="text-align: left;">Weak, Interventional</td>
</tr>
</tbody>
</table>
<p>One thing that falls short about Argument 1 is that we're just observing induction heads and incontext learning co-vary; like any observational study, it is not as convincing as if we actively changed one thing and measured what happened to the other. In this section we do a more "interventional" experiment, in which we change the model architecture in a way that makes induction heads easier to form, and observe the effect on in-context learning. The change makes a bigger difference for smaller models so is more convincing there, but also carries some amount of weight for larger models (see table above).</p>
<p>To design our experiment, we start with the observation (noted in the previous section) that the phase change and the corresponding improvement in in-context learning only occurs in transformers with more than one layer. This is what we'd predict if induction heads were the mechanism for the majority of in-context learning: induction heads require a composition of attention heads, which is only possible with two or more layers. ${ }^{15}$</p>
<p>Of course, the observation about one-layer models is pretty weak evidence by itself. (One could imagine one-layer models being different from models with more layers in all sorts of ways!) But it suggests a more general line of attack. If induction heads are the mechanism behind the large improvement in in-context learning, that makes predictions about the minimum architectural requirements in order to achieve the observed improvement. For a standard transformer, the important thing is to have two attention layers. But that's only because the key vectors need to be a function of the attended token and the token before it.</p>
<p>We define a "smeared key" architecture with a very simple modification that should make it easy for transformers of any depth to express induction heads. In our modified models, for each head $h$, we introduce a trainable real parameter $\alpha^{h}$ which we use as $\sigma\left(\alpha^{h}\right) \in[0,1]$ to interpolate between the key for the current token and previous token ${ }^{16}$ :</p>
<p>$$
k_{j}^{h}=\sigma\left(\alpha^{h}\right) k_{j}^{h}+\left(1-\sigma\left(\alpha^{h}\right)\right) k_{j-1}^{h}
$$</p>
<p>The hypothesis that induction heads are the primary mechanism of in-context learning predicts that the phase change will happen in one-layer models with this change, and perhaps might happen earlier in models with more layers. If they're one of several major contributing factors, we might expect some of the in-context learning improvement to happen early, and the rest at the same time as the original phase change.</p>
<p>The results (figure below) are in line with the predictions: when we use the smeared-key architecture, in-context learning does indeed form for one-layer models (when it didn't before), and it forms earlier for two-layer and larger models. More such results can be seen in the model analysis table.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>These plots are an excerpt of the Model Analysis Table; look there to see more analysis of how the smear models compare to their vanilla counterparts.</p>
<p>However, we probably shouldn't make too strong an inference about large models on this evidence. This experiment suggests that induction heads are the minimal mechanism for greatly increased incontext learning in transformers. But one could easily imagine that in larger models, this mechanism isn't the whole story, and also this experiment doesn't refute the idea of the mechanism of incontext learning changing over the course of training.</p>
<h1>Argument 3: When we directly "knock out" induction heads in small models at test-time, the amount of in-context learning greatly decreases.</h1>
<h2>STRENGTH OF ARGUMENT FOR SUB-CLAIMS</h2>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Small Attention-Only</th>
<th style="text-align: left;">Small with MLPs</th>
<th style="text-align: left;">Large Models</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Contributes Some</td>
<td style="text-align: left;">Strong, Causal</td>
<td style="text-align: left;">Strong, Causal</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Contributes Majority</td>
<td style="text-align: left;">Strong, Causal</td>
<td style="text-align: left;">Medium, Causal</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>For the cases they cover, ablations are by far our strongest evidence. The basic argument is that knocking out induction heads decreases the amount of in-context learning we observe in our models. By "knocking out" we mean that we remove a given attention head from the model at test time, doing a forward pass on the transformer without it. (See our methods section for exact details of how the ablation is done.)</p>
<p>The ablations presented below show how attention heads contribute to in-context learning, but we can also do ablations to study how attention heads contribute to the overall behavior change that occurs during the phase change (see the Model Analysis Table).
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>In fact, almost all the in-context learning in small attention-only models appears to come from these induction heads! This begins at the start of the phase change, and remains true through the end of training. ${ }^{17}$</p>
<p>Unfortunately, we do not have ablations for our full-scale models. ${ }^{18}$ For the models where we do have ablations, it seems like this evidence is clearly dispositive that induction heads increase incontext learning (at least as we've chosen to evaluate it). But can we further infer that they're the primary mechanism? A couple considerations:</p>
<ul>
<li>In attention-only models, in-context learning must essentially be a sum of contributions from different attention heads. ${ }^{19}$ But in models with MLPs, in-context learning could also come from interactions between MLP and attention layers. While ablating attention heads would affect such mechanisms, the relationship between the effect of the ablation on in-context learning and its true importance becomes more complicated. ${ }^{20}$ As a result, we can't be fully confident that head ablations in MLP models give us the full picture.</li>
<li>Our ablations measure the marginal effects of removing attention heads from the model. To the extent two heads do something similar and the layer norm before the logits rescales things, the importance of individual heads may be masked.</li>
</ul>
<p>All things considered, we feel comfortable concluding from this that induction heads are the primary mechanism for in-context learning in small attention-only models, but see this evidence as only suggestive for the MLP case.</p>
<h1>Argument 4: Despite being defined narrowly as copying random sequences, induction heads can implement surprisingly abstract types of in-context learning.</h1>
<p>STRENGTH OF ARGUMENT FOR SUB-CLAIMS</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Small Attention-Only</th>
<th style="text-align: left;">Small with MLPs</th>
<th style="text-align: left;">Large Models</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Contributes Some</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">Plausibility</td>
</tr>
<tr>
<td style="text-align: left;">Contributes Majority</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">Plausibility</td>
</tr>
</tbody>
</table>
<p>All of our previous evidence (in Arguments 1-3) focused on observing or perturbing the connection between induction head formation and macroscopic in-context learning. A totally different angle is to just find examples of induction heads implementing seemingly-difficult in-context learning behaviors; this would make it plausible that induction heads account for the majority of in-context learning. This evidence applies even to the very largest models (and we study up to 12B parameter models), but since it shows only a small number of tasks, it's only suggestive regarding in-context learning in general.</p>
<p>Recall that we define induction heads as heads that empirically copy arbitrary token sequences using a "prefix matching" attention pattern. Our goal is to find heads that meet this definition but also perform more interesting and sophisticated behaviors, essentially showing that induction heads in large models can be "generalizable".</p>
<p>In this argument, we show some anecdotal examples of induction heads from larger transformers (our 40-layer model with 13 billion parameters) that exhibit exactly such behaviors - namely literal copying, translation, and a specific type of abstract pattern matching. The behaviors are all of the form $[A <em>][B </em>] \ldots[A][B]$, aka the "fuzzy nearest neighbor match" or "find something similar early in the sequence and complete the sequence in analogy". We verify that these heads score highly on our "copying" and "prefix matching" evaluations (that is, they increase the probability of the token they attend to, and attend to tokens where the prefix matches the present token on random text), and are thus "induction heads" by our strict empirical definition, at the same time as they also perform these more sophisticated tasks.</p>
<p>The results of some example heads are shown in the table below, and described in the subsections below.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Head</th>
<th style="text-align: left;">Layer Depth</th>
<th style="text-align: left;">Copying score {?}</th>
<th style="text-align: left;">Prefix matching score {?}</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Literal copying head</td>
<td style="text-align: left;">$21 / 40$</td>
<td style="text-align: left;">0.89</td>
<td style="text-align: left;">0.75</td>
</tr>
<tr>
<td style="text-align: left;">Translation head</td>
<td style="text-align: left;">$7 / 40$</td>
<td style="text-align: left;">0.20</td>
<td style="text-align: left;">0.85</td>
</tr>
<tr>
<td style="text-align: left;">Pattern-matching head</td>
<td style="text-align: left;">$8 / 40$</td>
<td style="text-align: left;">0.69</td>
<td style="text-align: left;">0.94</td>
</tr>
</tbody>
</table>
<h1>Behavior 1: Literal sequence copying</h1>
<p>We'll start with the simplest case of a head that literally copies repeated text, to get familiar with the visualization interface we're using and the basic dynamics of these heads. We've selected an induction head which seems to perform very basic copying behavior and will look at how it behaves on the first paragraph of Harry Potter. We've repeated the first few sentences afterwards to show the head's behavior on longer segments of repeated text. For the following interactive visualizations, we recommend visiting the HTML article.</p>
<p>The visualization will show two different things:</p>
<ul>
<li>In red, "Attention" lets you see where the head is attending to predict the next token.</li>
<li>In blue, "Logit attr" shows the earlier tokens that contributed to the prediction of the current token, using "direct-path" logit attribution. ${ }^{21}$</li>
</ul>
<p>To start exploring the visualization, we suggest you visit the HTML article and try hovering your cursor over the second paragraph.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Tokens (hover to focus, click to lock) $\square$ Selected is source
<EOT>Mr and Mrs Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense. Mr Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large moustache. Mrs Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time cran ing over garden fences, spying on the neighbours. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere. The Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didn't think they could bear it if anyone found out about the Potters. Mrs Potter was Mrs Dursley's sister, but they hadn't met for several years; in fact, Mrs Dursley pretended she didn't have a sister, because her sister and her good-for-nothing husband were as unDursleyish as it was possible to be. The Dursleys shuddered to think what the neighbours would say if the Potters arrived in the street. The Dursleys knew that the Potters had a small son, too, but they had never even seen him. This boy was another good reason for keeping the Potters away; they didn't want Dudley mixing with a child like that.</p>
<p>Mr and Mrs Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense. Mr Dursley was the director of a firm called Grunnings, which made drills.</p>
<p>If you explore the visualization, you'll see that the head predicts repeating the names "Dursley" and "Potters"; the phrase "a small son"; and then the entire repeated sentences at the end. In all cases, these successful predictions are made by attending back to a previous instance where this phrase was present in the text.</p>
<h1>Behavior 2: Translation</h1>
<p>It's a well-known result that language models can translate between languages. Intriguingly, we've encountered many examples of induction heads that can do translation. Here, we explore a head we found in layer 7 of our 40-layer model, showcasing translation between English, French, and German. (As with the others in this section, this head is also an "induction head" by the same definition we've been using all along, because when shown repeated random tokens, it uses a "prefix matching" attention pattern to copy the sequence verbatim.)</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>These plots are a small excerpt of the Model Analysis Table; look there to see the full evidence. See our methods section for exact details of how the ablation is done.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
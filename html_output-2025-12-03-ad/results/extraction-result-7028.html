<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7028 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7028</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7028</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-265842153</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.00444v1.pdf" target="_blank">Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements</a></p>
                <p><strong>Paper Abstract:</strong> This work focuses on the novel problem setting of generating graphs conditioned on a description of the graph's functional requirements in a downstream task. We pose the problem as a text-to-text generation problem and focus on the approach of fine-tuning a pretrained large language model (LLM) to generate graphs. We propose an inductive bias which incorporates information about the structure of the graph into the LLM's generation process by incorporating message passing layers into an LLM's architecture. To evaluate our proposed method, we design a novel set of experiments using publicly available and widely studied molecule and knowledge graph data sets. Results suggest our proposed approach generates graphs which more closely meet the requested functional requirements, outperforming baselines developed on similar tasks by a statistically significant margin.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7028.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7028.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SGG-LLM serialization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Serialized Graph Generation serialization (g(·)) with node disambiguation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An injective, reversible bag-of-edges style serialization used to convert text‑labeled graphs into a token sequence for autoregressive LLM training; adds special delimiting tokens and a node disambiguation token so any graph can be losslessly serialized and deserialized.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Bag-of-edges with special tokens and node disambiguation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each edge is rendered as a delimited string: <PN> predecessor-node-feature <E> edge-feature <SN> successor-node-feature. Nodes with identical feature strings receive a disambiguation token <D> followed by a unique integer (e.g., C<D>0). Special tokens <PN>, <E>, <SN>, <D> are added to tokenizer and embedding layer so serialization is reversible and injective (g^{-1} exists).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>lossless, sequential, token-based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>bag-of-edges listing of edges in a predefined order (edge sequence); predecessor/edge/successor triple per edge</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>PCQM4M-derived Valency and QED datasets (from PCQM4M); WebNLG+ 2020 (knowledge graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Text-conditioned graph generation (functional-description → serialized graph); knowledge-graph generation (imperative descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BLOOM (560M) for molecule experiments; T5 (770M) for WebNLG experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained autoregressive / encoder-decoder LLMs (BLOOM 560M used as the base LM for molecule experiments; T5 770M used for knowledge-graph experiments) fine-tuned to predict serialized graph sequences conditioned on functional descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean Absolute Error (MAE) wrt requested functional property; Parsability (fraction parsable); Diversity (multimodality score); for WebNLG: precision/recall/F1 of triples</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>On PCQM4M-derived datasets (100k training): SGG-LLM (no MP) MAE QED 0.044 ± 0.011, MAE Valency 0.060 ± 0.018; SGG-LLM (edge MP) MAE QED 0.036 ± 0.005, MAE Valency 0.035 ± 0.014; SGG-LLM (correspondence MP) MAE QED 0.039 ± 0.007, MAE Valency 0.045 ± 0.017. Parsability (100k): SGG-LLM (no MP) QED 1.000 ± 0.000, Valency 0.999 ± 0.001; diversity QED 0.845 ± 0.017, Valency 0.506 ± 0.031. (Baselines: grapher MAE QED 0.157 ± 0.004, MAE Valency 1.268 ± 0.229; regen MAE QED 0.149 ± 0.018, MAE Valency 2.282 ± 1.156).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Enables direct fine-tuning of pretrained LLMs to generate graphs as text; combined with a proposed training loss (equal example weighting) it yielded substantially better MAE and diversity than baselines. Requires fine-tuning (pretrained LM without fine-tuning produced no parsable graphs).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Potentially long token sequences for large graphs (token cost not reported numerically); reliance on expensive fine-tuning of large autoregressive LLMs (compute and latency at generation); requires adding new special tokens and embeddings; training stability with message-passing integration required a learned gating term (tanh a) — without gating MP integration failed to produce parsable graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Outperforms previous bag-of-edges approaches (regen, grapher) on MAE and diversity when combined with the paper's special training objective and optional interleaved message-passing layers; unlike SMILES (DFS-based molecular string), this method is generic to any text‑labeled graph and reversible via explicit deserialization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7028.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7028.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bag-of-edges (prior)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bag-of-edges serialization (edge-list triples with delimiters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A common serialized-graph representation that lists edges as textual triples with special delimiters for predecessor node, edge, and successor node; used in prior text→graph generation works and reimplemented by authors as baselines (regen, grapher).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Bag-of-edges (predecessor-edge-successor triple)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each edge encoded as a delimited string <PN> node-feature <E> edge-feature <SN> node-feature; edges listed in sequence (order chosen by implementation); prior implementations sometimes use domain-specific ordering (e.g., SMILES for molecules).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>lossless if augmented to disambiguate repeated node labels; sequential, token-based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>edge-list ordering (bag-of-edges), sometimes using domain ordering heuristics in prior work; authors added node disambiguation to make it reversible for molecules</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Used / evaluated on PCQM4M-derived Valency and QED datasets (implemented variants used as baselines: regen, grapher); historically used on knowledge-graph datasets like WebNLG+</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Text-to-serialized-graph generation (knowledge-graph generation; molecule generation conditioned on text)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (as used in regen originally) and BLOOM (560M) in reimplementations</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>In baseline implementations regen and grapher, an LLM (T5 or updated BLOOM) is fine-tuned to predict sequences of edge triples representing graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>MAE (functional property), Parsability, Diversity; WebNLG: precision/recall/F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Baselines on 100k training: grapher MAE QED 0.157 ± 0.004, MAE Valency 1.268 ± 0.229; regen MAE QED 0.149 ± 0.018, MAE Valency 2.282 ± 1.156. Parsability (100k): grapher QED 1.000 ± 0.000, Valency 1.000 ± 0.000; regen QED 0.984 ± 0.008, Valency 0.991 ± 0.007. Diversity (100k) grapher QED 0.745 ± 0.038, Valency 0.410 ± 0.045; regen QED 0.854 ± 0.031, Valency 0.446 ± 0.124.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Standard bag-of-edges with conventional LLM objectives can be effective for parsability but (in these experiments) yielded worse alignment to functional constraints (higher MAE) and lower diversity than the paper's proposed setup and losses.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Prior bag-of-edges methods did not handle nodes with identical feature strings (authors had to add node disambiguation), and standard training objectives (predict-next-token weighting) produced worse performance in this conditional functional-generation setting; can rely on ordering assumptions (domain-specific) to succeed (e.g., SMILES for molecules).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to the paper's injective serialization+special loss, bag-of-edges baselines lag in MAE and diversity; graph-structure-aware message-passing interleaved with LLM layers (SGG-LLM edges/correspondences) further improves performance over plain bag-of-edges LLM fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7028.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7028.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMILES linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SMILES (Simplified Molecular Input Line Entry System) DFS linearization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specific text serialization for chemical molecules that serializes molecular graphs into strings via a depth-first traversal with tokens for atoms and bonds; cited as prior molecule serialization work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>SMILES (DFS linearization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Depth-first traversal ordering of molecular bonds/atoms producing a linear character sequence representing a molecular graph; uses domain-specific syntax for branching, rings, and atom/bond types.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>lossless for molecules (domain-specific), sequential, token/character-based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Depth-first-search (DFS) traversal of molecular graph with domain-specific syntax to encode branching and cycles</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>General molecular datasets historically (not specifically used by this paper's experiments), mentioned as prior art</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Molecule serialization for string-based generation or molecular language models</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Not used in experiments here; cited as an example of domain-specific canonical serialization used in prior molecule generation research.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Domain-specific to chemistry (cannot generalize to arbitrary text-labeled graphs); relies on domain encoding rules; not directly applicable to general text graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>SMILES is a specialized DFS-based canonical string format for molecules, whereas the paper's serialization is generic (text graphs) and uses explicit disambiguation tokens to handle repeated node labels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7028.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7028.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Edge-graph MP representation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Edge-graph representation for message passing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformation used by the authors to perform message passing within an LLM where original graph edges are treated as nodes in a secondary 'edge graph' and message passing passes information between edges that share original nodes, constrained to not pass information backwards in the token sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Edge-graph (edges-as-nodes) representation for MP</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Construct an edge graph G_edge where nodes correspond to original edges E, and edges in G_edge connect e_j and e_k if they share a node and j ≠ k; only include directed connections with k > j (sequence-order constraint) to avoid backward information flow with respect to serialized token order; the MP output for each edge is incorporated into the token embedding immediately after the edge's description with a learned gating tanh(a).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>lossless (transformational), graph-of-edges representation used for internal computation, not a textual serialization</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Transform original bag-of-edges serialization into an edge-centric graph indexed by edge sequence positions; use last-token feature of each edge token span as the node feature for that edge; MP aggregates along adjacency defined by shared original nodes and sequence-order constraint (j<k).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>PCQM4M-derived Valency and QED datasets (used in molecule experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Incorporating graph structure during autoregressive text generation to improve conditional graph generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SGG-LLM with interleaved GraphSAGE message-passing layers</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained LLM (BLOOM 560M) interleaved with single-layer GraphSAGE MP modules; MP outputs gated and added back into token embeddings before next LM layer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>MAE, Parsability, Diversity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SGG-LLM with edge-based MP (100k training) achieved MAE QED 0.036 ± 0.005, MAE Valency 0.035 ± 0.014; Parsability QED 0.998 ± 0.001, Valency 0.999 ± 0.000; Diversity QED 0.836 ± 0.029, Valency 0.540 ± 0.016.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Interleaving MP layers improved alignment to functional requirements (lower MAE) relative to SGG-LLM without MP; required a learned gating term for stable fine-tuning — without gating the model produced no parsable outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Increases model and integration complexity; requires per-token bookkeeping (mapping tokens to edge/node positions) and extra compute; required gating term and careful construction (directional edges) to avoid violating autoregressive causality; scaling to larger graphs or long feature strings may be expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Outperformed the SGG-LLM variant without MP in MAE; compared to correspondence-based MP variant, edge-based MP yielded similar or slightly better MAE on Valency in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7028.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7028.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Correspondence-graph MP representation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Correspondence-graph representation for message passing between repeated node instances</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An alternative MP construction where each occurrence of a node in the serialized sequence is treated as its own node in a correspondence graph and edges connect adjacent occurrences to propagate information forward in sequence order.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Correspondence-graph (instance-based) MP representation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Treat each textual occurrence of a graph node in the serialization as a separate node in the correspondence graph; add edges between occurrences that correspond to the same original node when adjacent in the serialization and always directed from earlier to later occurrence (to avoid backward information flow); run MP on this correspondence graph and fold outputs back into token embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>transformational, graph-of-instances representation used for internal MP</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Build nodes representing each serialized-node-instance and connect instance-nodes per same-original-node correspondences subject to adjacency and sequence-order constraints (edges from earlier to later occurrences).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>PCQM4M-derived Valency and QED datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Text-conditioned graph generation with internal graph-aware representations to aid autoregressive generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SGG-LLM with correspondence-message-passing</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained LLM (BLOOM 560M) with interleaved GraphSAGE MP layers operating on correspondence graph constructed from token occurrences; outputs gated and added to token embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>MAE, Parsability, Diversity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SGG-LLM (correspondences, 100k) MAE QED 0.039 ± 0.007, MAE Valency 0.045 ± 0.017. Parsability QED 0.995 ± 0.001, Valency 0.998 ± 0.000. Diversity QED 0.839 ± 0.008, Valency 0.608 ± 0.054.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Provided improved MAE relative to baseline LLM without MP and comparable performance to edge-based MP; maintained autoregressive constraints by constructing directed correspondence edges.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires tracking multiple instances of the same node across the token sequence and building an auxiliary graph per example; additional compute and memory overhead; no clear canonicalization of serialization order reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Similar benefits as edge-based MP (lower MAE than no-MP variant); on Valency diversity the correspondence variant reported higher diversity than edge-based in the experiments, suggesting differences in multimodality behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7028.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7028.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Node disambiguation token</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Node disambiguation token (<D>)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A special token appended to node feature strings with a unique integer index to distinguish multiple nodes that share the same textual label, enabling injective reversible serialization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Node disambiguation token appended to repeated node labels</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>When multiple nodes have identical feature strings (e.g., multiple carbons 'C'), the serialization appends <D>i (unique integer i) to each occurrence so nodes become distinct tokens C<D>0, C<D>1, ... enabling reversible mapping g^{-1}.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token-based augmentation to a sequential serialization</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Append the special token <D> and an integer index to node feature strings that are repeated in the graph before serialization</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>PCQM4M-derived Valency and QED datasets (molecular graphs with repeated atom labels)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Lossless serialization for text-graphs where multiple nodes share identical feature strings</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BLOOM (560M) used in experiments; implemented by adding special token embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Special tokens added to tokenizer and embedding matrix of pre-trained LLM prior to fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Crucial to making the serialization reversible for molecules and enabling correct deserialization and message-passing index mapping; authors had to add this to prior bag-of-edges baselines to generalize to molecule datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Increases token vocabulary usage (additional tokens and potentially longer token strings); requires consistent indexing scheme and bookkeeping during serialization/deserialization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Necessary augmentation relative to naive bag-of-edges when node labels repeat (a common case for molecules); without it the serialization would not be injective and deserialization ambiguous.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Regen: Reinforcement learning for text and knowledge base generation using pretrained language models <em>(Rating: 2)</em></li>
                <li>Grapher <em>(Rating: 2)</em></li>
                <li>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. <em>(Rating: 2)</em></li>
                <li>Inductive representation learning on large graphs <em>(Rating: 2)</em></li>
                <li>WebNLG+ 2020 <em>(Rating: 2)</em></li>
                <li>Explanation graph generation via pretrained language models: An empirical study with contrastive learning. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7028",
    "paper_id": "paper-265842153",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "SGG-LLM serialization",
            "name_full": "Serialized Graph Generation serialization (g(·)) with node disambiguation",
            "brief_description": "An injective, reversible bag-of-edges style serialization used to convert text‑labeled graphs into a token sequence for autoregressive LLM training; adds special delimiting tokens and a node disambiguation token so any graph can be losslessly serialized and deserialized.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Bag-of-edges with special tokens and node disambiguation",
            "representation_description": "Each edge is rendered as a delimited string: &lt;PN&gt; predecessor-node-feature &lt;E&gt; edge-feature &lt;SN&gt; successor-node-feature. Nodes with identical feature strings receive a disambiguation token &lt;D&gt; followed by a unique integer (e.g., C&lt;D&gt;0). Special tokens &lt;PN&gt;, &lt;E&gt;, &lt;SN&gt;, &lt;D&gt; are added to tokenizer and embedding layer so serialization is reversible and injective (g^{-1} exists).",
            "representation_type": "lossless, sequential, token-based",
            "encoding_method": "bag-of-edges listing of edges in a predefined order (edge sequence); predecessor/edge/successor triple per edge",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "PCQM4M-derived Valency and QED datasets (from PCQM4M); WebNLG+ 2020 (knowledge graphs)",
            "task_name": "Text-conditioned graph generation (functional-description → serialized graph); knowledge-graph generation (imperative descriptions)",
            "model_name": "BLOOM (560M) for molecule experiments; T5 (770M) for WebNLG experiments",
            "model_description": "Pretrained autoregressive / encoder-decoder LLMs (BLOOM 560M used as the base LM for molecule experiments; T5 770M used for knowledge-graph experiments) fine-tuned to predict serialized graph sequences conditioned on functional descriptions.",
            "performance_metric": "Mean Absolute Error (MAE) wrt requested functional property; Parsability (fraction parsable); Diversity (multimodality score); for WebNLG: precision/recall/F1 of triples",
            "performance_value": "On PCQM4M-derived datasets (100k training): SGG-LLM (no MP) MAE QED 0.044 ± 0.011, MAE Valency 0.060 ± 0.018; SGG-LLM (edge MP) MAE QED 0.036 ± 0.005, MAE Valency 0.035 ± 0.014; SGG-LLM (correspondence MP) MAE QED 0.039 ± 0.007, MAE Valency 0.045 ± 0.017. Parsability (100k): SGG-LLM (no MP) QED 1.000 ± 0.000, Valency 0.999 ± 0.001; diversity QED 0.845 ± 0.017, Valency 0.506 ± 0.031. (Baselines: grapher MAE QED 0.157 ± 0.004, MAE Valency 1.268 ± 0.229; regen MAE QED 0.149 ± 0.018, MAE Valency 2.282 ± 1.156).",
            "impact_on_training": "Enables direct fine-tuning of pretrained LLMs to generate graphs as text; combined with a proposed training loss (equal example weighting) it yielded substantially better MAE and diversity than baselines. Requires fine-tuning (pretrained LM without fine-tuning produced no parsable graphs).",
            "limitations": "Potentially long token sequences for large graphs (token cost not reported numerically); reliance on expensive fine-tuning of large autoregressive LLMs (compute and latency at generation); requires adding new special tokens and embeddings; training stability with message-passing integration required a learned gating term (tanh a) — without gating MP integration failed to produce parsable graphs.",
            "comparison_with_other": "Outperforms previous bag-of-edges approaches (regen, grapher) on MAE and diversity when combined with the paper's special training objective and optional interleaved message-passing layers; unlike SMILES (DFS-based molecular string), this method is generic to any text‑labeled graph and reversible via explicit deserialization.",
            "uuid": "e7028.0",
            "source_info": {
                "paper_title": "Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Bag-of-edges (prior)",
            "name_full": "Bag-of-edges serialization (edge-list triples with delimiters)",
            "brief_description": "A common serialized-graph representation that lists edges as textual triples with special delimiters for predecessor node, edge, and successor node; used in prior text→graph generation works and reimplemented by authors as baselines (regen, grapher).",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Bag-of-edges (predecessor-edge-successor triple)",
            "representation_description": "Each edge encoded as a delimited string &lt;PN&gt; node-feature &lt;E&gt; edge-feature &lt;SN&gt; node-feature; edges listed in sequence (order chosen by implementation); prior implementations sometimes use domain-specific ordering (e.g., SMILES for molecules).",
            "representation_type": "lossless if augmented to disambiguate repeated node labels; sequential, token-based",
            "encoding_method": "edge-list ordering (bag-of-edges), sometimes using domain ordering heuristics in prior work; authors added node disambiguation to make it reversible for molecules",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "Used / evaluated on PCQM4M-derived Valency and QED datasets (implemented variants used as baselines: regen, grapher); historically used on knowledge-graph datasets like WebNLG+",
            "task_name": "Text-to-serialized-graph generation (knowledge-graph generation; molecule generation conditioned on text)",
            "model_name": "T5 (as used in regen originally) and BLOOM (560M) in reimplementations",
            "model_description": "In baseline implementations regen and grapher, an LLM (T5 or updated BLOOM) is fine-tuned to predict sequences of edge triples representing graphs.",
            "performance_metric": "MAE (functional property), Parsability, Diversity; WebNLG: precision/recall/F1",
            "performance_value": "Baselines on 100k training: grapher MAE QED 0.157 ± 0.004, MAE Valency 1.268 ± 0.229; regen MAE QED 0.149 ± 0.018, MAE Valency 2.282 ± 1.156. Parsability (100k): grapher QED 1.000 ± 0.000, Valency 1.000 ± 0.000; regen QED 0.984 ± 0.008, Valency 0.991 ± 0.007. Diversity (100k) grapher QED 0.745 ± 0.038, Valency 0.410 ± 0.045; regen QED 0.854 ± 0.031, Valency 0.446 ± 0.124.",
            "impact_on_training": "Standard bag-of-edges with conventional LLM objectives can be effective for parsability but (in these experiments) yielded worse alignment to functional constraints (higher MAE) and lower diversity than the paper's proposed setup and losses.",
            "limitations": "Prior bag-of-edges methods did not handle nodes with identical feature strings (authors had to add node disambiguation), and standard training objectives (predict-next-token weighting) produced worse performance in this conditional functional-generation setting; can rely on ordering assumptions (domain-specific) to succeed (e.g., SMILES for molecules).",
            "comparison_with_other": "Compared to the paper's injective serialization+special loss, bag-of-edges baselines lag in MAE and diversity; graph-structure-aware message-passing interleaved with LLM layers (SGG-LLM edges/correspondences) further improves performance over plain bag-of-edges LLM fine-tuning.",
            "uuid": "e7028.1",
            "source_info": {
                "paper_title": "Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "SMILES linearization",
            "name_full": "SMILES (Simplified Molecular Input Line Entry System) DFS linearization",
            "brief_description": "A domain-specific text serialization for chemical molecules that serializes molecular graphs into strings via a depth-first traversal with tokens for atoms and bonds; cited as prior molecule serialization work.",
            "citation_title": "Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules.",
            "mention_or_use": "mention",
            "representation_name": "SMILES (DFS linearization)",
            "representation_description": "Depth-first traversal ordering of molecular bonds/atoms producing a linear character sequence representing a molecular graph; uses domain-specific syntax for branching, rings, and atom/bond types.",
            "representation_type": "lossless for molecules (domain-specific), sequential, token/character-based",
            "encoding_method": "Depth-first-search (DFS) traversal of molecular graph with domain-specific syntax to encode branching and cycles",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "General molecular datasets historically (not specifically used by this paper's experiments), mentioned as prior art",
            "task_name": "Molecule serialization for string-based generation or molecular language models",
            "model_name": null,
            "model_description": null,
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Not used in experiments here; cited as an example of domain-specific canonical serialization used in prior molecule generation research.",
            "limitations": "Domain-specific to chemistry (cannot generalize to arbitrary text-labeled graphs); relies on domain encoding rules; not directly applicable to general text graphs.",
            "comparison_with_other": "SMILES is a specialized DFS-based canonical string format for molecules, whereas the paper's serialization is generic (text graphs) and uses explicit disambiguation tokens to handle repeated node labels.",
            "uuid": "e7028.2",
            "source_info": {
                "paper_title": "Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Edge-graph MP representation",
            "name_full": "Edge-graph representation for message passing",
            "brief_description": "A transformation used by the authors to perform message passing within an LLM where original graph edges are treated as nodes in a secondary 'edge graph' and message passing passes information between edges that share original nodes, constrained to not pass information backwards in the token sequence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Edge-graph (edges-as-nodes) representation for MP",
            "representation_description": "Construct an edge graph G_edge where nodes correspond to original edges E, and edges in G_edge connect e_j and e_k if they share a node and j ≠ k; only include directed connections with k &gt; j (sequence-order constraint) to avoid backward information flow with respect to serialized token order; the MP output for each edge is incorporated into the token embedding immediately after the edge's description with a learned gating tanh(a).",
            "representation_type": "lossless (transformational), graph-of-edges representation used for internal computation, not a textual serialization",
            "encoding_method": "Transform original bag-of-edges serialization into an edge-centric graph indexed by edge sequence positions; use last-token feature of each edge token span as the node feature for that edge; MP aggregates along adjacency defined by shared original nodes and sequence-order constraint (j&lt;k).",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "PCQM4M-derived Valency and QED datasets (used in molecule experiments)",
            "task_name": "Incorporating graph structure during autoregressive text generation to improve conditional graph generation",
            "model_name": "SGG-LLM with interleaved GraphSAGE message-passing layers",
            "model_description": "Pretrained LLM (BLOOM 560M) interleaved with single-layer GraphSAGE MP modules; MP outputs gated and added back into token embeddings before next LM layer.",
            "performance_metric": "MAE, Parsability, Diversity",
            "performance_value": "SGG-LLM with edge-based MP (100k training) achieved MAE QED 0.036 ± 0.005, MAE Valency 0.035 ± 0.014; Parsability QED 0.998 ± 0.001, Valency 0.999 ± 0.000; Diversity QED 0.836 ± 0.029, Valency 0.540 ± 0.016.",
            "impact_on_training": "Interleaving MP layers improved alignment to functional requirements (lower MAE) relative to SGG-LLM without MP; required a learned gating term for stable fine-tuning — without gating the model produced no parsable outputs.",
            "limitations": "Increases model and integration complexity; requires per-token bookkeeping (mapping tokens to edge/node positions) and extra compute; required gating term and careful construction (directional edges) to avoid violating autoregressive causality; scaling to larger graphs or long feature strings may be expensive.",
            "comparison_with_other": "Outperformed the SGG-LLM variant without MP in MAE; compared to correspondence-based MP variant, edge-based MP yielded similar or slightly better MAE on Valency in reported experiments.",
            "uuid": "e7028.3",
            "source_info": {
                "paper_title": "Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Correspondence-graph MP representation",
            "name_full": "Correspondence-graph representation for message passing between repeated node instances",
            "brief_description": "An alternative MP construction where each occurrence of a node in the serialized sequence is treated as its own node in a correspondence graph and edges connect adjacent occurrences to propagate information forward in sequence order.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Correspondence-graph (instance-based) MP representation",
            "representation_description": "Treat each textual occurrence of a graph node in the serialization as a separate node in the correspondence graph; add edges between occurrences that correspond to the same original node when adjacent in the serialization and always directed from earlier to later occurrence (to avoid backward information flow); run MP on this correspondence graph and fold outputs back into token embeddings.",
            "representation_type": "transformational, graph-of-instances representation used for internal MP",
            "encoding_method": "Build nodes representing each serialized-node-instance and connect instance-nodes per same-original-node correspondences subject to adjacency and sequence-order constraints (edges from earlier to later occurrences).",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "PCQM4M-derived Valency and QED datasets",
            "task_name": "Text-conditioned graph generation with internal graph-aware representations to aid autoregressive generation",
            "model_name": "SGG-LLM with correspondence-message-passing",
            "model_description": "Pretrained LLM (BLOOM 560M) with interleaved GraphSAGE MP layers operating on correspondence graph constructed from token occurrences; outputs gated and added to token embeddings.",
            "performance_metric": "MAE, Parsability, Diversity",
            "performance_value": "SGG-LLM (correspondences, 100k) MAE QED 0.039 ± 0.007, MAE Valency 0.045 ± 0.017. Parsability QED 0.995 ± 0.001, Valency 0.998 ± 0.000. Diversity QED 0.839 ± 0.008, Valency 0.608 ± 0.054.",
            "impact_on_training": "Provided improved MAE relative to baseline LLM without MP and comparable performance to edge-based MP; maintained autoregressive constraints by constructing directed correspondence edges.",
            "limitations": "Requires tracking multiple instances of the same node across the token sequence and building an auxiliary graph per example; additional compute and memory overhead; no clear canonicalization of serialization order reported.",
            "comparison_with_other": "Similar benefits as edge-based MP (lower MAE than no-MP variant); on Valency diversity the correspondence variant reported higher diversity than edge-based in the experiments, suggesting differences in multimodality behavior.",
            "uuid": "e7028.4",
            "source_info": {
                "paper_title": "Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Node disambiguation token",
            "name_full": "Node disambiguation token (&lt;D&gt;)",
            "brief_description": "A special token appended to node feature strings with a unique integer index to distinguish multiple nodes that share the same textual label, enabling injective reversible serialization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Node disambiguation token appended to repeated node labels",
            "representation_description": "When multiple nodes have identical feature strings (e.g., multiple carbons 'C'), the serialization appends &lt;D&gt;i (unique integer i) to each occurrence so nodes become distinct tokens C&lt;D&gt;0, C&lt;D&gt;1, ... enabling reversible mapping g^{-1}.",
            "representation_type": "token-based augmentation to a sequential serialization",
            "encoding_method": "Append the special token &lt;D&gt; and an integer index to node feature strings that are repeated in the graph before serialization",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "PCQM4M-derived Valency and QED datasets (molecular graphs with repeated atom labels)",
            "task_name": "Lossless serialization for text-graphs where multiple nodes share identical feature strings",
            "model_name": "BLOOM (560M) used in experiments; implemented by adding special token embeddings",
            "model_description": "Special tokens added to tokenizer and embedding matrix of pre-trained LLM prior to fine-tuning.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Crucial to making the serialization reversible for molecules and enabling correct deserialization and message-passing index mapping; authors had to add this to prior bag-of-edges baselines to generalize to molecule datasets.",
            "limitations": "Increases token vocabulary usage (additional tokens and potentially longer token strings); requires consistent indexing scheme and bookkeeping during serialization/deserialization.",
            "comparison_with_other": "Necessary augmentation relative to naive bag-of-edges when node labels repeat (a common case for molecules); without it the serialization would not be injective and deserialization ambiguous.",
            "uuid": "e7028.5",
            "source_info": {
                "paper_title": "Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Regen: Reinforcement learning for text and knowledge base generation using pretrained language models",
            "rating": 2,
            "sanitized_title": "regen_reinforcement_learning_for_text_and_knowledge_base_generation_using_pretrained_language_models"
        },
        {
            "paper_title": "Grapher",
            "rating": 2
        },
        {
            "paper_title": "Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules.",
            "rating": 2,
            "sanitized_title": "smiles_a_chemical_language_and_information_system_1_introduction_to_methodology_and_encoding_rules"
        },
        {
            "paper_title": "Inductive representation learning on large graphs",
            "rating": 2,
            "sanitized_title": "inductive_representation_learning_on_large_graphs"
        },
        {
            "paper_title": "WebNLG+ 2020",
            "rating": 2,
            "sanitized_title": "webnlg_2020"
        },
        {
            "paper_title": "Explanation graph generation via pretrained language models: An empirical study with contrastive learning.",
            "rating": 1,
            "sanitized_title": "explanation_graph_generation_via_pretrained_language_models_an_empirical_study_with_contrastive_learning"
        }
    ],
    "cost": 0.01717975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>FORM FOLLOWS FUNCTION: TEXT-TO-TEXT CONDI-TIONAL GRAPH GENERATION BASED ON FUNCTIONAL REQUIREMENTS
1 Nov 2023</p>
<p>Peter A Zachares 
OATML Oxford University</p>
<p>Vahan Hovhannisyan 
OATML Oxford University</p>
<p>Alan Mosca 
OATML Oxford University</p>
<p>Yarin Gal yaringal@gmail.com 
OATML Oxford University</p>
<p>FORM FOLLOWS FUNCTION: TEXT-TO-TEXT CONDI-TIONAL GRAPH GENERATION BASED ON FUNCTIONAL REQUIREMENTS
1 Nov 202381DD060E46B763E5E6386F5C9AE7B6E4arXiv:2311.00444v1[cs.LG]
This work focuses on the novel problem setting of generating graphs conditioned on a description of the graph's functional requirements in a downstream task.We pose the problem as a text-to-text generation problem and focus on the approach of finetuning a pretrained large language model (LLM) to generate graphs.We propose an inductive bias which incorporates information about the structure of the graph into the LLM's generation process by incorporating message passing layers into an LLM's architecture.To evaluate our proposed method, we design a novel set of experiments using publicly available and widely studied molecule and knowledge graph data sets.Results suggest our proposed approach generates graphs which more closely meet the requested functional requirements, outperforming baselines developed on similar tasks by a statistically significant margin.</p>
<p>INTRODUCTION</p>
<p>Many concepts can be described by graphs; including molecules (19), abstract syntax trees (18), knowledge graphs (26), and project schedules (17).Each of these concepts is used in downstream tasks where graph structure has a direct impact on task performance.For example, some molecules can be used as medicine for a disease while others cannot and this is partially determined by the molecules' graphs.It would be useful if we could describe the functional requirements of a graph using natural language and query a model to conditionally generate a graph which meets these requirements.For example, a model using the prompt "generate a molecule with 47 valency electrons" would generate a valid molecule graph with 47 valency electrons.With such models we could speed up and improve the processes of drug discovery, software development, project management as well as many other applications of graphs.</p>
<p>In this work, we focus on the problem of generating graphs where node and edge features are strings of text and term this type of graph a text graph.Text graphs are a fairly flexible data format and can be used in most applications of graph-structured data including those listed above.To the best of our knowledge, text graph generation conditioned on text has only been studied in the fields of knowledge graph generation (8; 12; 13; 25; 26; 38) and explanation graph generation (32).In both setups the conditional text explicitly describes the graph.These explicit descriptions give an imperative "recipe" with step-by-step instructions of how to generate the graph.Contrary to giving explicit imperative descriptions, this work focuses on the case where the conditional text is a functional description of the graph, specifically for a downstream task.Additionally, in the case of imperative conditional text, there is an injective mapping between conditional text and the graph that should be generated, which means that methods can be evaluated by comparing the generated graph to the ground truth graph in terms of its structure.By contrast, many graphs might correspond to the same functional description and hence a new experimental design is required to study the problem.</p>
<p>Preprint</p>
<p>We propose solving this problem by fine tuning a large language model (LLM) to generate a serialized text description of a graph.This is motivated by the strong performance pre-trained LLMs have shown when fine-tuned to perform a specific task (6; 16; 33), including the task of text-to-serialized graph generation (8).However, prior work on serialized graph generation does not generalize to all domains containing text graph structured data.To further this approach, we propose a serialization method which is expressive enough to reversibly serialize any graph and a method to incorporate graph structure into the LLM's generation process motivated by the observation that the structures of generated graphs have a direct impact on their functional properties and consequently on model performance.While LLMs are probably expressive enough to learn to incorporate graph structure into their generation process, it is more efficient to provide the model with the ability to do so.However, it is also a non-trivial challenge because modern LLMs perform autoregressive sampling to generate text (4; 5; 6; 16; 27; 30; 33).This involves a multi-step sequential process, where at each step a forward pass is performed and then a new token is sampled.A token is an atomic unit of text like 'dog', ' ', or 'ch'.The generation process exhibits high computational complexity and cost due to the requirement of performing a forward model pass for each generated token.Consequently, LLMs are typically trained to generate text without actually performing generation at training time.Only a single forward pass is performed at each training step without sampling and the entire sequence (conditional plus desired text) is fed into the LLM for that forward pass.As such, an LLM has access to the full sequence at training time, while at generation time it only has access to the conditional text and the part of the sequence that has already been generated.This means that during training an LLM could learn to rely on information that it will not have access to at generation time.This issue is overcome by using causal masking, which guarantees the LLM cannot pass information backwards in the token sequence ensuring that generation conditions are simulated at training time.</p>
<p>In Section 3, we propose a method of incorporating graph structure into an LLM's generation process which passes information between tokens in a sequence.Our key contribution is demonstrating how to do so without passing information backwards in the sequence.Specifically, we propose to extend LLMs to process and generate sequences of text and graphs.For this we provide the LLM with the ability to deserialize graphs incorporated into input token sequences and introduce message passing layers into the LLM to calculate representations of graph structure in a manner that is conducive to autoregressive generation.</p>
<p>In this work, we specifically focus on the problem of generating text graphs from functional requirements within the range of those seen at training time (interpolation), instead of extrapolating outside the training set's domain, which we leave as future work.We propose a novel experiment design to evaluate methods in this new problem setting using the publicly available data sets WebNLG+ 2020 (10) and PCQM4M (19).Results suggest that our proposed model outperforms previous work on text graph generation conditioned on text by a statistically significant margin.And that our proposed approach for incorporating graph structure into the language model's generation process leads to models which generate examples which on average more closely meet their conditional functional requirements.All code used to perform experiments is publicly available at ...</p>
<p>PRELIMINARIES</p>
<p>A text graph G = {V, E} is composed of a set of N nodes v i ∈ V each with an associated text string describing the node and a set of M directed edges (v i , v j ) ∈ E each with an associated text string describing the edge.Each text graph G is associated with text D f containing a functional description of G. Our goal is to train a model to accurately predict p θ (G|D f ) with parameters θ, describing the distribution over text graphs G conditioned on a specific functional description D f .Importantly, this distribution may be multimodal as multiple graphs may have the same functional properties.This work focuses on the case where the parameterized model p θ (•) is a language model that generates a serialized text description of the graph.Hence, if D G is the serialized graph, then the language model predicts p θ (D G |D f ).This requires an injective serialization function g : G → D G with a known inverse mapping g −1 : D G → G from the serialized description D G back to the graph G such that G = g −1 (g(G)).g(•) is used at training time to transform graphs G into serialized graphs D G and the deserialization function is used at generation time to recover the graph G from the serialized graph D G (see Figure 1 top row).With this approach, we pose the problem of generating graphs Prior state-of-the-art work on serialized graph generation uses a bag-of-edges approach (8; 32), which describes each edge using a string with special tokens for delimiting graph elements and then lists out the edges in a predefined order.For example, the popular smiles molecule serialisation algorithm uses depth-first search traversal (39) to order chemical bonds (edges) in a molecule.Then each edge is described with the syntax <PN>predecessor node feature string<E>edge feature string<SN>successor node feature string, where <PN>,<E>,<SN> are special tokens used to delimit the different components of the edge.</p>
<p>Using LLMs, a description D is represented in three ways: 1) as a string (text), 2) as a sequence of token indicators, and 3) as a sequence of feature vectors / embeddings.To distinguish between these three representations, we use D S to denote the string representation, D T to denote the sequence of token indicators and D F to denote the sequence of feature vectors.For the serialized graph D G , its string representation is denoted by D G,S , its sequence of token indicators representation by D G,T and its sequence of feature vector representation by D G,F .</p>
<p>METHOD</p>
<p>The datasets LLMs are pre-trained on do not contain much, if any, data on generating serialized graphs (4; 6; 30; 33).Consequently, it is unlikely they could generate text graphs without further training.We provide evidence supporting this hypothesis in experiments where we evaluate a pretrained LLM without further fine-tuning.As shown in the results in Appendix D, an LLM without fine-tuning did not generate a single valid serialized graph.Hence, we propose fine-tuning an LLM on a training and validation set of (functional description, graph) pairs to generate serialized graphs conditioned on functional descriptions.While LLMs are probably expressive enough to learn to incorporate graph structure into their generation process, it is more efficient to provide the model with the ability to do so.State-of-the-art models for performing inference on graph structured data use message passing (MP) layers (14; 35).As such, we propose interleaving MP layers between language modelling layers in an LLM (Figure 1 block A).Any message passing layer can be used with our proposed approach1 .</p>
<p>FINE-TUNING OBJECTIVE</p>
<p>For fine-tuning, we propose minimizing the negative log-likelihood of serialized graph sequences D G as described in equation ( 1):
Loss = 1 n batch n batch i=1 − log p θ (d 1,i G,T |D i f ) + n i seq j=2 − log p θ (d j,i G,T |d 1:(j−1),i G,T , D i f )(1)
where
d i,j G,T ∈ D i G,T
, the superscript i is the index in a batch, the superscript j indicates the position of an element d i,j G,T in the sequence D i G,T , n batch is the number of elements in the batch, and n i seq is the number of elements in the sequence of example i in the batch.This objective differs from the traditional LLM fine-tuning objective of maximizing the average likelihood of the next token for each token in a batch of sequences as described in equation 2;
Loss = 1 n batch i=1 n i seq n batch i=1 − log p θ (d 1,i G,T |D i f ) + n i seq j=2 − log p θ (d j,i G,T |d 1:(j−1),i G,T , D i f )(2)
The difference between the two objectives is the red term in both equations.We can view the reciprocal of this term as a weight placed on the importance of an example in the training set with respect to the training objective.For the standard training objective in equation 2, the term
n batch i=1
n i seq in red changes from batch to batch, because batches contain randomly sampled examples which differ in A comparison in the performance of models trained with these objectives can be seen in Tables 2 and  3.The baseline regen only differs from our proposed model SGG-LLM without message passing in that it was trained using equation 2 instead of 1.We also report additional experiments shown Appendix D which suggests what is causing the difference in performance.</p>
<p>NODE DISAMBIGUATION DURING SERIALIZATION</p>
<p>One special case of graph-structured data, which is not handled by prior work on generating serialized graphs, is when a subset of nodes in a graph are described by the same feature string2 .However, this special case occurs in many domains, such as molecule data.For example, the molecule with multiple carbon atoms depicted in Figure 2 top row.To address this issue, we add a disambiguation token <D> to the feature strings of nodes with identical feature strings followed by a unique integer.An example of node disambiguation is shown the bottom row of Figure 2, where the three carbon atoms' feature strings are modified from C, C, C to C<D>0, C<D>1, C<D>2.With the addition of a disambiguation token, the serialization method becomes expressive enough to reversibly serialize any graph.In conjunction with this serialization function and special token <D>, we also add special tokens <PN> (predecessor node) <E> (edge) and <SN> (successor node) to the tokenizer of the pre-trained LLM, as well as additional rows to the embedding layer of the LLM for all four special tokens before fine-tuning.See our publicly available code for implementations of the proposed serialization g(•) and deserialization functions g −1 (•).</p>
<p>MESSAGE PASSING WITHIN A LANGUAGE MODEL</p>
<p>Our proposed architecture requires passing information from an LLM layer to an MP layer, then to the following LLM layer, and so on as shown in block B in Figure 1.As such, we need to convert the serialized graph representation outputted by an LLM layer into a representation of the graph that can be fed into an MP layer and vice versa.The inputs to an MP layer are node feature vectors D V,F ∈ R N ×H for each node in a graph, where H is the dimensionality of a feature vector, and the graph's adjacency matrix A ∈ R N ×N .An obvious choice for constructing D V,F would be simply Preprint using the feature vectors of D G,F .However, in D G,T , each node in the graph is described by multiple tokens and consequently multiple feature vectors in D G,F .For example, in Figure 2, the predecessor node of edge 1 is described by four tokens [<PN>, C, <D>, 0].We can calculate a single node vector from its multiple token feature vectors in D G,F by selecting the feature vector of the last element describing the node in D G,F because the last vector contains information about all previous tokens in the sequence including all those describing the node because of the causal attention layers in an LLM.</p>
<p>In addition, if a node has a degree greater than one, it will be described more than once in D G3 .While a node may occur more than once in a serialized graph, using our proposed serialization method, an edge will only occur once.In a graph G = {V, E}, let there be a set e k for each edge (v i , v j ) ∈ E where the superscript k indicates the position of the edge in D G and the set contains the two nodes the edge connects {v i , v j } ∈ e k .For the graph G and graph serialization D G pair depicted in Figure 2, e 2 would be the set of nodes {C<D>1, N} contained in edge 2 because edge 2 is the second edge (k = 2) described in the serialization D G .As a slight abuse of notation, we denote elements of a graph's edge set E either by (v i , v j ) or by e k .Hence, we define a new graph G edge = {V edge , E edge } where the edges in the original graph are treated as nodes V edge = E and nodes in the original graph define edges in the new graph E edge = {(e j , e k )|e j , e k ∈ E ∧ e j ∩ e k ̸ = ∅ ∧ j ̸ = k}.We call this new graph an edge graph.Figure 2 shows an example of transforming a graph into its edge graph where the four edges in the original graph become the nodes in the edge graph.Let f index : Z + → Z + be a mapping from the index of an edge in the sequence D V edge ,F to the index of the last token describing the edge in D G,F .To construct the sequence of feature vectors for the nodes in an edge graph G edge , for each MP layer we use the last token's feature vector of each edge in the serialized graph, so that
d k V edge ,F = d f index (k) G,F
.</p>
<p>MP layers pass information between nodes in a graph by aggregating feature vectors from neighboring nodes.To ensure that an MP layer does not pass information backwards in D G,F , the MP layer should only aggregate information for a node from nodes that are earlier in the sequence as depicted in causal graph attention in block D in Figure 1.causal attention, shown in block C in Figure 1, only attends to elements previous to the query element in the sequence.graph attention only attends to elements in the sequence which are neighboring nodes to the query element in the sequence based on a reference graph.causal graph attention respects both of these constraints.We propose MP layers to pass information on a graph's edge graph.To ensure information is not passed backwards in D G,F we must add an additional constraint when constructing the edge set of an edge graph which is ∀(e j , e k ) ∈ E edge , k &gt; j.This constraint has been applied to the edge graph shown in Figure 2.</p>
<p>After passing an edge graph through an MP layer, we add each resultant edge feature vector to the token feature vector immediately after its description in the serialized graph as described in equation 3;
d t G,F = d t G,F + d k V edge ,F tanh a, if f index (k) = t − 1; d t G,F , otherwise.(3)
During the development of this method, we found it necessary to multiply element-wise the output of an MP layer by a gating term tanh a when incorporating the output back into D G,F as shown in equation 3. a is a learned gating parameter of the MP layer initialized to 0. Without the gating term, we could not fine-tune an LLM with message passing incorporated to generate valid graphs as demonstrated by the results shown in Appendix D. We hypothesize this is because the gating term allows an LLM to gradually learn to take into account graph structure.Without it, the model most likely starts fine-tuning at a position on the loss landscape from which it cannot converge to a useful local minima.The use of the gating term is inspired by the work of (1), which faced a similar issue.</p>
<p>Preprint 4 RELATED WORK 4.1 INCORPORATING OTHER MODALITIES INTO TEXT GENERATION</p>
<p>This work proposes incorporating graph structure, an additional modality, into an LLM's text generation process (where the generated text is a serialized graph).There have been many works on incorporating other modalities into the text generation process.Speech-to-text can be posed as a sequence to sequence problem (15) and so similar methods to those used for text-to-text generation have been used to incorporate the modality of sound into language generation (2; 29; 37).Another widely studied problem is image-to-text generation, where most works focus on calculating dense sequence representations of images (20; 21; 23; 28; 40; 1) to pose image-to-text generation as a sequence-to-sequence generation problem as well.From this field of work, our proposed method is inspired by (1) which incorporates image information into the text generation process by representing sequences of images as sequences of dense vectors and interleaving special layers for combining the modalities of image and text between language modelling layers in an LLM.Inspired by this approach, we propose interleaving additional layers in-between language modelling layers in a pretrained LLM and incorporating additional tokens for representing the other modality.Unlike (1), we incorporate the modality of graphs as opposed to images into the generation process, and use a modified LLM to generate graphs rather than text.</p>
<p>GENERATING GRAPHS CONDITIONED ON TEXT</p>
<p>In the field of natural language processing, there have been many works on parsing natural language into syntax trees, some of which pose the problem as a serialized graph generation conditioned on text (9; 36).However, these works use a domain specific serialization method and do not incorporate graph structure into their proposed models' generation processes.There is one recent work (32) focused on generating explanation graphs from text which proposes a contrastive learning objective for fine-tuning the LLM T5 (30) to generate graphs, however the contrastive objective requires additional labels created by an expert.There is also a recent work published on generating protein graphs conditioned on text (24), however it cannot generalize past the domain of protein graph generation because the proposed method generates sequences of characters describing amino acid sequences.</p>
<p>Most prior work on text to graph generation focuses on generating knowledge graphs from text (8; 12; 13; 25; 26; 38).The best performing method on benchmark tasks is regen (8), which uses the LLM T5 (30) to generate knowledge graphs and is fine-tuned using the standard objective of predicting the next token in a sequence (equation 2).The second best performing method on benchmark knowledge graph generation tasks is grapher (26) which uses a combination of T5 and a recurrent neural network to generate graphs.This method differs from ours in the use of an additional model besides the LLM to predict edges and generate edge features as well as a training objective similar to equation 2.</p>
<p>EXPERIMENTS</p>
<p>Prior work on text graph generation conditioned on text does not provide an experimental design for evaluating methods that generate graphs based on functional descriptions.To do so, a dataset containing (functional requirements, graph) pairs is required for a task where it is possible to automatically calculate the functional properties of newly generated graphs.To generate such a dataset, we took the open graph benchmark large scale molecule dataset PCQM4M (19) and used the open source Python package rdkit (34) to generate two functional descriptions for each molecule: number of valency electrons in it and quantitative estimated-likeness (QED) (3).These are commonly used functional properties of a molecule's graph and are described in more detail in Appendix C.</p>
<p>DATASETS</p>
<p>For evaluation we created two datasets, each dataset is composed of all the molecules in the original PCQM4M (19)  called Valency dataset.The other dataset contains (QED functional description, graph) pairs and is called QED dataset.To evaluate the impact of amount of data on model performance, we created three subsets for each of the datasets.Each subset had 1000 randomly selected molecules in its test and validation sets.Importantly, these were the same across the three subsets and then the training sets of the three subsets were another 25, 000, 100, 000 and 400, 000 randomly chosen examples.</p>
<p>The training set of the smaller subsets was contained in the training set of the larger subset sets.See Figure 2 for examples of input output pairs for the constructed data sets.</p>
<p>METRICS AND EVALUATION</p>
<p>To evaluate models, we calculated three metrics on the test sets of the datasets described above: parsability, diversity, and most importantly mean absolute error (MAE) with respect to the conditional functional property.A generated graph is parsable if no error is thrown when calculating its functional property.If a molecule is parsable (i.e. it follows correct serialization syntax and doesn't violate basic laws of physics), it is given a score of 1, otherwise 0. In all experiments we use the following metrics:</p>
<p>• Parsability is the mean parsability score of samples in the test set.</p>
<p>• MAE is the mean absolute error between the generated graph's functional property value and the requested property value averaged over samples in the test set.To interpret the magnitudes reported for MAE results see Table 1 describing some summary statistics about functional properties of graphs in the datasets used in experiments.</p>
<p>• Diversity is a measure of the multimodality of the distribution p θ (D G |D f ) that the LLM learns.A sampled graph is assigned a diversity score of 1, if it, a second sampled graph and the ground truth do not share the same node and edge sets; and is assigned a score of 0 otherwise.Diversity is the average of diversity scores of samples in the test set.</p>
<p>CURRENT STATE-OF-THE-ART</p>
<p>We implemented current state-of-the-art models from prior work as baselines: specifically we implemented the version of grapher (26), which generates edge features instead of classifying them and regen (8).We had to make two modifications to both approaches so they could generalize to molecule data: we added node disambiguation from Section 3.2 to their serialization methods and updated their language models to a more recent model BLOOM (33) 100,000 1.000 ± 0.000 0.999 ± 0.001 0.845 ± 0.017 0.506 ± 0.031 SGG-LLM edges 100,000 0.998 ± 0.001 0.999 ± 0.000 0.836 ± 0.029 0.540 ± 0.016 SGG-LLM correspondences 100,000 0.995 ± 0.001 0.998 ± 0.000 0.839 ± 0.008 0.608 ± 0.054 SGG-LLM none 25,000 0.997 ± 0.002 0.991 ± 0.003 0.799 ± 0.008 0.518 ± 0.078 SGG-LLM none 400,000 0.998 ± 0.001 1.000 ± 0.000 0.857 ± 0.006 0.542 ± 0.051 grapher N/A 100,000 1.000 ± 0.000 1.000 ± 0.000 0.745 ± 0.038 0.410 ± 0.045 regen N/A 100,000 0.984 ± 0.008 0.991 ± 0.007 0.854 ± 0.031 0.446 ± 0.124 Table 3: Model performance in terms of parsability and diversity on QED and Valency datasets.The first, second, and third best performing models are highlighted using the colors shown here.</p>
<p>Experiments were repeated three times to estimate standard error.Models below the boldface line are from prior work.All models achieved a parsability near 1.0.</p>
<p>experiments by our proposed approach.See Appendix A for a description of the training process used to train grapher, regen and variants of our proposed approach.</p>
<p>RESULTS</p>
<p>On the task of generating graphs to meet functional requirements, the ideal model can generate a diverse set of parsable graphs with functional properties equal to the requested functional property.Tables 2 and 3 describe the results of our proposed approach on the QED and Valency datasets.Our proposed approach is referred to as SGG-LLM standing for serialized graph generator large language model.All variants of our approach and the baselines grapher, and regen achieve a parsability score near or at 1.The SGG-LLM variant with correspondences message passing (described in Appendix B) is another method of incorporating the MP layers into an LLM by passing messages in D G based on node correspondences rather than edges.</p>
<p>Results in table 2 suggest that all variants of SGG-LLM outperform baselines by a statistically significant margin, using the unpaired t-student test and a threshold p-value of 0.05, in terms of generating examples with functional properties close those requested.In addition, results suggest that all variants of SGG-LLM outperform grapher by a statistically significant margin in terms of generating a diverse set of candidates on the Valency dataset.Finally, the two variants of SGG-LLM that use MP layers outperform the variant that does not utilise MP layers.</p>
<p>The high performance of all variants of our approach over regen suggest the importance of the proposed loss function and of weighting tokens equally across batches during training.The high performance of all variants of our approach over grapher could be for the same reasons as regen, as it is trained with a similar loss, or it could be that single model which jointly estimates the existence of nodes, edges, and their features is more effective at generating graphs than a dual module model.</p>
<p>To demonstrate the generality of our proposed approach beyond generating molecules, we also evaluate it on the benchmark knowledge graph generation dataset WebNLG+ 2020 (10) using a different language model T5 (30).Note this task is generating graphs conditioned on an imperative description of the graph, so is not directly linked to the focus of this paper.See Appendix E for a description of the experiments and discussion of the results.</p>
<p>DISCUSSION</p>
<p>The main limitation of our proposed approach is its reliance on fine-tuning a pre-trained autoregressive LLMs, which are computationally expensive, slow at generation time and require substantial computational resources even to fine-tune.This limitation would become even more difficult when applying this method to tasks containing larger graphs or graphs containing elements with long feature strings.Hence an interesting next step from this method would be using quantized pre-trained models and low rank adapters on LLM layers for more efficient fine tuning (7).n i seq ] which is the expected value of the differing term in equation 2 from equation 1.This is to determine whether it is the magnitude of the differing term causing the difference in performance or the fact that n batch i=1 n i seq changes from batch to batch.In experiments, some summary statistics for the term n batch i=1 n i seq in equation 2 were mean = 4053, max = 4676, median = 4060, and inter-quartile range = 3915 − 4199.There was a marginal difference in performance in terms of all three metrics when comparing the performance of SGG-LLM w/ special loss and SGG-LLM without message passing.The marginal difference in performance suggests that the changing weighting between batches and across epochs is what hurts the performance of models trained with the objective described in equation 1. regen is a model trained with equation 1.
Preprint</p>
<p>Model</p>
<p>E RESULTS OF KNOWLEDGE GRAPH GENERATION EXPERIMENTS</p>
<p>To demonstrate the generality of our proposed approach beyond generating molecules, we also evaluate it on the benchmark knowledge graph generation dataset WebNLG+ 2020 (10) using a different language model T5 (30).Like in prior work, model performance is evaluated using the</p>
<p>Figure 1 :
1
Figure 1: At generation time (top) the input to the model is the graph's functional description D f and the output is a serialised description of the graph D G .During training time (below top) the input is a functional description, serialized graph pair (D f , D g ); the output is the probability p(D g ).Graph serialization method g(•) is used to serialize input graphs before training, while the deserialization method g −1 (•) is used to deserialize the generated serialized graph D g as well as to help perform message passing within the LLM.Block A describes the proposed architecture interleaving message passing layers between LLM layers.Block B depicts how information is passed between LLM and MP layers.The bottom visualization depicts the masked matrices used in the types of attention performed within LLM and message passing (MP) layers.</p>
<p>Figure 2 :
2
Figure 2: Top: the correspondence between a graph and its edge graph.Middle: the graph's bag-of-edges serialisation with special tokens <PN>, <SN>, <E> and <D>.Bottom: Examples of functional requirements used for condition graph generation for the molecule above.</p>
<p>Table 1 :
1
Summary statistics about functional requirements in datasets used in experiments.The standard deviations reported for both data sets provide context for evaluating Table2below.
PreprintMinimum Maximum MeanStandard DeviationMedianInterquantile RangeNumber of Valency Electrons212277.313.38070 -88QED0.060.980.7640.1330.780.68 -0.88Model Message Passing Training Set SizeMean Absolute Error QED ValencySGG-LLM none100,0000.044 ± 0.011 0.060 ± 0.018SGG-LLM edges100,0000.036 ± 0.005 0.035 ± 0.014SGG-LLM correspondences100,0000.039 ± 0.007 0.045 ± 0.017SGG-LLM none25,0000.062 ± 0.008 1.703 ± 0.074SGG-LLM none400,0000.020 ± 0.001 0.076 ± 0.034grapher N/A100,0000.157 ± 0.004 1.268 ± 0.229regenN/A100,0000.149 ± 0.018 2.282 ± 1.156
dataset with less than 20 atoms, which is still more than 2 million examples.One of the two datasets contains (number of valency electrons functional description, graph) pairs and is</p>
<p>Table 2 :
2
Model performance in terms of MAE on QED and Valency datasets.The first, second, and third best performing models are highlighted using the colors shown here.Experiments were repeated three times to estimate standard error.All fine-tuned variants of SGG-LLM outperform baselines by a statistically significant margin.Models below the boldface line are from prior work.</p>
<p>, which is the same LLM used in
PreprintModelMessage PassingTraining Set SizeQEDParsability ValencyQEDDiversity ValencySGG-LLM none</p>
<p>(3)&gt;0 edge 3 in the correspondence graph.Edges are constructed by connecting nodes in the correspondence graph which correspond to the same node in the original graph i.e. in the graphic C<D>0 edge 1 is connected to C<D>0 edge 3 because they refer to the node C<D>0 in the original graph.Nodes in the correspondence graph are only connected if they are adjacent occurrences in the serialized graph.By constructing edges such that they always point from an earlier instance of a node to a later instance we ensure the message passing layer does not pass information backwards in the serialized graph sequence at training time.As additional method, we propose incorporating MP layers into an LLM where the MP layers pass information based on a graph's correspondence graph.C FUNCTIONAL DESCRIPTIONS OF MOLECULESWe used two functional descriptions of molecules to generate datasets for our experiments -number of valency electrons and QED.The number of valency electrons in a molecule is the sum of the number of valency electrons in its atoms.Importantly, this property is a result of only a graph's node composition, but not its full structure.To generate descriptions of a functional property of a graph's full structure, we calculate a metric called quantitative estimated drug-likeness (QED)(3).QED quantifies multiple properties of a molecule which are attractive for applications in drug design and then calculates a single metric from these properties using a weighted logarithmic sum.The properties used to calculate QED are molecular weight, octanol-water partition coefficient, topological polar surface area, number of hydrogen bond donors and acceptors, the number of aromatic rings and rotatable bonds, and the presence of unwanted chemical functionalities.For experiments we set the weight of the properties: molecular weight, number of hydrogen bond donors and acceptors, and presence of unwanted chemical functionalities to zero in the QED calculation, because they are mainly determined by node composition instead of the entire graph structure.The functional descriptions generated were of the format "a molecule with number of valence electrons equal to ..." and "a molecule with a weighted quantitative estimation of drug-likeness equal to ...".D ADDITIONAL RESULTS OF MOLECULE GENERATION EXPERIMENTSBelow, we provide three tables describing the full results of experiments on the molecule data sets including ablation studies to empirically justify some design choices in our proposed method.Our proposed model is referred to as SGG-LLM.There are three additional models in the tables below; 1) SGG-LLM w/out fine-tuning, 2) SGG-LLM w/ special loss, and 3) SGG-LLM with edge based message passing without a gating term.The table below describing performance of models in terms of the parsability of generated examples shows that SGG-LLM w/out fine-tuning and SGG-LLM with edge based message passing without a gating term both did not produce a single parsable example on molecule datasets' test sets.These results suggests that both fine-tuning and a gating term (when incorporating message passing into an LLM) are required to achieve good performance with our proposed method.Note if a model cannot generate parsable examples, then the metric mean absolute error cannot be calculated and the diversity of generated examples is not a useful measure of model performance.Consequently, we do not report mean absolute error or diversity for SGG-LLM w/out fine-tuning and SGG-LLM with edge based message passing without a gating term.The model SGG-LLM w/ special loss is a version of SGG-LLM without message passing that was trained using equation 1, but instead of letting n batch = batch size as proposed in section 3.1, we equal n batch = E[
PreprintModelMessage Passing Training Set SizeQEDParsability ValencySGG-LLMnone100,0001.000 ± 0.000 0.999 ± 0.001SGG-LLM w/out fine-tuningnone100,0000.000 ± 0.000 0.000 ± 0.000SGG-LLM w/ special lossnone100,0000.986 ± 0.003-SGG-LLMedges100,0000.998 ± 0.001 0.999 ± 0.000SGG-LLM w/out gating termedges100,0000.000 ± 0.000 0.000 ± 0.000SGG-LLMcorrespondences100,0000.995 ± 0.001 0.998 ± 0.000SGG-LLMnone25,0000.997 ± 0.002 0.991 ± 0.003SGG-LLMnone400,0000.998 ± 0.001 1.000 ± 0.000grapherN/A100,0001.000 ± 0.000 1.000 ± 0.000regenN/A100,0000.984 ± 0.008 0.991 ± 0.007n batchi=1
We used GraphSAGE(14) in all experiments
Prior work on generating graphs conditioned on text focused on tasks requiring graphs that did not contain sets of nodes with the same feature string(8; 26;<br />
)
This makes it difficult to introduce graph information into the early elements in the graph serialization sequence. For a node with a degree greater than one, if we passed the feature vector outputted from an MP layer into its earliest instance in the graph serialization, we would be passing information backwards in the sequence
PreprintIn terms of societal impact, our proposed approach might help speed up and improve processes such as drug discovery, software development and project planning.But at the same time requires oversight to ensure it is not used for nefarious applications like the design of chemical weapons.In addition, if this approach is applied to a task in the social sciences, analysis should be required to ensure that the biases learned by the model are understood and any unfair preferences learned for a certain demographic or group should be mitigated.A HYPERPARAMETERS AND TRAININGFor experiments on the molecule data sets, all variants of our proposed approach and the implemented baseline grapher(26)and regen (8) used the pretrained version of BLOOM(33)with 560 million parameters as their language model.For experiments on the WebNLG2020+ data set(10), all models used the pretrained version of T5(30)with 770 million parameters as their language model.All models were trained for up to ten epochs and checkpointed based on minimizing validation loss.Model's were trained using the ADAM optimizer(22)with a learning rate of 3e − 5, a β 1 of 0.9, a β 2 of 0.999 and a regularization weight of 1e − 7 as well as a linear learning rate schedule.During training, model parameters' gradients norms were clipped to a value of 1.0.The models were trained with a batch size of 18 using stage 2 data parallelism using the method described in(31).All models were trained and evaluated on machines with 3 NVIDIA A100 GPUs.Variants of our proposed approach which incorporated message passing into the LLM by interleaving message passing layers in the LLM used a single GraphSage (14) layer in each message passing layer with an embedding size equal to the token embedding size of the LLM they were incorporated into.B MESSAGE PASSING BETWEEN NODE CORRESPONDENCESMost nodes appear at least twice in a bag-of-edges serialization.We can define a correspondence graph from a serialized graph D G by treating each instance of a node in a serialized graph D G as its own node in the correspondence graph.Then the correspondences between instances define edges in the correspondence graphs.An example of a correspondence graph is shown in the graphic below;In the graphic, the node C<D>0 occurs more than once in D G and each instance is treated as its own node in the correspondence graph, so the node C<D>0 corresponds to C<D>0 edge 1 and to Preprint metrics of F1-score, precision and recall when comparing a generated graph to the ground truth knowledge graph.See(8)for a more detailed explanation of these metrics.We compare our proposed method to the three best performing models on this data set; regen (8), grapher (26), and bt5(11).On the WebNLG+ 2020 data set, results in the table below suggest that the incorporating message passing into an LLM is not useful to the task of knowledge graph generation from an imperative description, but also that using edge message passing does not degrade performance.Interestingly, SGG-LLM without message passing was able to achieve state-of-the-art performance on the benchmark task of generating triples in a knowledge graph.regen's implementation for experiments on WebNLG2020+ was identical to SGG-LLM without message passing, except that SGG-LLM was trained with a different training objective (see equation 1 in the main paper), a more aggressive learning rate and a linear learning rate schedule.So the state-of-the-art performance of SG-LLM on WebNLG2020+ may be attributed to better hyperparameter selection or the modified training objective.Model performance on knowledge graph data set WebNLG+ 2020.Note: baseline model results do not report standard deviations because they were not reported in prior work and we felt it was more appropriate to report baseline results based their published results as opposed to reimplementing the baselines ourselves.Experiments with variants of our proposed approach were repeated three times to estimate standard error.Exact
Flamingo: a visual language model for few-shot learning. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, Karen Simonyan, ArXiv, abs/2204.141982022</p>
<p>SpeechT5: Unified-modal encoder-decoder pre-training for spoken language processing. Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei, 10.18653/v1/2022.acl-long.393Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20221</p>
<p>Quantifying the chemical beauty of drugs. Richard Bickerton, Gaia Paolini, Jérémy Besnard, Andrew Sorel Muresan, Hopkins, 10.1038/nchem.1243Nature chemistry. 42012</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, 2020</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet ; Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. Felipe Petroski Such. Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang,; Andrew N. Carr; Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrewJan Leike. 2021</p>
<p>. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, David Sepassi, Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck2022Jeff Dean, Slav Petrovand Noah Fiedel. Palm: Scaling language modeling with pathways</p>
<p>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, Qlora, arXiv:2305.14314Efficient finetuning of quantized llms. 2023arXiv preprint</p>
<p>Regen: Reinforcement learning for text and knowledge base generation using pretrained language models. Pierre L Dognin, Inkit Padhi, Igor Melnyk, Payel Das, 2021</p>
<p>Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, Noah A Smith, Recurrent neural network grammars. 2016</p>
<p>Creating training corpora for NLG micro-planners. Claire Gardent, Anastasia Shimorina, Shashi Narayan, Laura Perez-Beltrachini, 10.18653/v1/P17-1017Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational LinguisticsJuly 20171Long Papers)</p>
<p>Machine translation aided bilingual data-to-text generation and semantic parsing. Heming Ge, Mihir Sanjay Kale, Oshin Agarwal, Rami Al-Rfou, Siamak Shakeri, Yunhsuan Sung, 20203rd Workshop on Natural Language Generation from the Semantic Web</p>
<p>Machine translation aided bilingual data-to-text generation and semantic parsing. Heming Ge, Mihir Sanjay Kale, Oshin Agarwal, Rami Al-Rfou, Siamak Shakeri, Yunhsuan Sung, 20203rd Workshop on Natural Language Generation from the Semantic Web</p>
<p>Cyclegt: Unsupervised graph-to-text and text-to-graph generation via cycle training. Qipeng Guo, Zhijing Jin, Xipeng Qiu, Weinan Zhang, David Wipf, Zheng Zhang, 2020</p>
<p>Inductive representation learning on large graphs. Will Hamilton, Zhitao Ying, Jure Leskovec, 201730Advances in neural information processing systems</p>
<p>Deep speech: Scaling up end-to-end speech recognition. Y Awni, Carl Hannun, Jared Case, Bryan Casper, Greg Catanzaro, Erich Diamos, Ryan Elsen, Sanjeev Prenger, Shubho Satheesh, Adam Sengupta, Andrew Y Coates, Ng, CoRR, abs/1412.55672014</p>
<p>Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Tom Clark, Eric Hennigan, Katie Noland, George Millican, Bogdan Van Den Driessche, Aurelia Damoc, Simon Guy, Karen Osindero, Erich Simonyan, Jack W Elsen, Rae, 2022</p>
<p>Data-driven schedule risk forecasting for construction mega-projects. Vahan Hovhannisyan, Peter Zachares, Yael Grushka-Cockayne, Alan Mosca, Carlos Ledezma, Available at SSRN. 44961192023</p>
<p>Open graph benchmark: Datasets for machine learning on graphs. Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, Jure Leskovec, arXiv:2005.006872020arXiv preprint</p>
<p>Ogblsc: A large-scale challenge for machine learning on graphs. Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, Jure Leskovec, arXiv:2103.094302021arXiv preprint</p>
<p>Scaling up visual and vision-language representation learning with noisy text supervision. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yun-Hsuan Sung, Zhen Li, Tom Duerig, CoRR, abs/2102.059182021</p>
<p>Vilt: Vision-and-language transformer without convolution or region supervision. Wonjae Kim, Bokyung Son, Ildoo Kim, 2021</p>
<p>Adam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, 3rd International Conference on Learning Representations, ICLR 2015. San Diego, CA, USAMay 7-9, 2015. 2015Conference Track Proceedings</p>
<p>Visualbert: A simple and performant baseline for vision and language. Liunian Harold, Li , Mark Yatskar, Cho-Jui Da Yin, Kai-Wei Hsieh, Chang, CoRR, abs/1908.035572019</p>
<p>A text-guided protein design framework. Shengchao Liu, Yutao Zhu, Jiarui Lu, Zhao Xu, Weili Nie, Anthony Gitter, Chaowei Xiao, Jian Tang, Hongyu Guo, Anima Anandkumar, 2023</p>
<p>Unified structure generation for universal information extraction. Yaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Hongyu Lin, Xianpei Han, Le Sun, Hua Wu, 2022</p>
<p>Knowledge graph generation from text. Igor Melnyk, Pierre Dognin, Payel Das, 2022</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, ; , Ryan Lowe, Jan Leike,. 2022</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, CoRR, abs/2103.000202021</p>
<p>Robust speech recognition via large-scale weak supervision. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, Ilya Sutskever, 2022</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, J. Mach. Learn. Res. 1532-4435211jan 2020</p>
<p>Zero: Memory optimization towards training A trillion parameter models. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He, CoRR, abs/1910.020542019</p>
<p>Explanation graph generation via pretrained language models: An empirical study with contrastive learning. Swarnadeep Saha, Prateek Yadav, Mohit Bansal, 2022</p>
<p>Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova Del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina Mcmillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, David Daniel Van Strien, Dragomir Ifeoluwa Adelani, Eduardo González Radev, Efrat Ponferrada, Ethan Levkovizh, Eyal Kim, Francesco De Bar Natan, Gérard Toni, Germán Dupont, Giada Kruszewski, Hady Pistilli, Hamza Elsahar, Hieu Benyamina, Ian Tran, Idris Yu, Isaac Abdulmumin, Itziar Johnson, Javier Gonzalez-Dios, Jenny De La Rosa, Jesse Chim, Jian Dodge, Jonathan Zhu, Jörg Chang, Joseph Frohberg, Joydeep Tobing, Khalid Bhattacharjee, Kimbo Almubarak, Kyle Chen, Leandro Lo, Leon Von Werra, Long Weber, Loubna Phan, Ludovic Ben Allal, Manan Tanguy, Manuel Dey, Maraim Romero Muñoz, María Masoud, Mario Grandury, Max Šaško, Maximin Huang, Mayank Coavoux, Mike Singh, Preprint Tian-Jian, Minh Chien Jiang, Mohammad A Vu, Mustafa Jauhar, Nishant Ghaleb, Nora Subramani, Nurulaqilla Kassner, Olivier Khamis, Omar Nguyen, Ona Espejel, Paulo De Gibert, Peter Villegas, Pierre Henderson, Priscilla Colombo, Quentin Amuok, Rheza Lhoest, Rishi Harliman, Roberto Bommasani, Rui Luis López, Salomey Ribeiro, Sampo Osei, Sebastian Pyysalo, Shamik Nagel, Shamsuddeen Bose, Shanya Hassan Muhammad, Shayne Sharma, Somaieh Longpre, Stanislav Nikpoor, Suhas Silberberg, Sydney Pai, Tiago Zink, Timo Timponi Torrent, Tristan Schick, Valentin Thrush, Vassilina Danchev, Veronika Nikoulina, Violette Laippala, Vrinda Lepercq, Zaid Prabhu, Zeerak Alyafeai, Arun Talat, Benjamin Raja, Chenglei Heinzerling, Davut Emre Si, Elizabeth Taşar, Sabrina J Salesky, Wilson Y Mielke, Abheesht Lee, Andrea Sharma, Antoine Santilli, Arnaud Chaffin, Debajyoti Stiegler, Eliza Datta, Gunjan Szczechla, Han Chhablani, Harshit Wang, Hendrik Pandey, Jason Strobelt, Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, Saiful Bari, Maged S Al-Shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Pierre Patrick Von Platen, Pierre Cornette, Rémi François Lavallée, Samyam Lacroix, Sanchit Rajbhandari, Shaden Gandhi, Stéphane Smith, Suraj Requena, Tim Patil, Ahmed Dettmers, Amanpreet Baruwa, Anastasia Singh, Anne-Laure Cheveleva, Arjun Ligozat, Aurélie Subramonian, Charles Névéol, Dan Lovering, Deepak Garrette, Ehud Tunuguntla, Ekaterina Reiter, Ekaterina Taktasheva, Eli Voloshina, Genta Bogdanov, Hailey Indra Winata, Jan-Christoph Schoelkopf, Jekaterina Kalo, Jessica Novikova, Jordan Zosa Forde, Jungo Clive, Ken Kasai, Liam Kawamura, Marine Hazan, Miruna Carpuat, Najoung Clinciu, Newton Kim, Oleg Cheng, Omer Serikov, Oskar Antverg, Rui Van Der Wal, Ruochen Zhang, Sebastian Zhang, Shachar Gehrmann, Shani Mirkin, Tatiana Pais, Thomas Shavrina, Tian Scialom, Tomasz Yun, Verena Limisiewicz, Vitaly Rieser, Vladislav Protasov, Yada Mikhailov, Yonatan Pruksachatkun, Zachary Belinkov, Zdeněk Bamberger, Alice Kasner, Amanda Rueda, Amir Pestana, Ammar Feizpour, Amy Khan, Ana Faranak, Anthony Santos, Silas Hevia, Sourav Wang, Sylvain Roy, Thanh Viguier, Tobi Le, Trieu Oyebade, Yoyo Le, Zach Yang, Nguyen ; Chenxi, Chirag Zhou, Chuxin Jain, Clémentine Xu, Fourrier, Daniel Daniel León Periñán, Dian Molano, Enrique Yu, Fabio Manjavacas, Florian Barth, Gabriel Fuhrimann, Altay ; Jihyun, John Kang, Jonas Giorgi, Jose Golde, Karthik David Posada, Lokesh Rangasai Sivaraman, Lu Bulchandani, Luisa Liu, Shinzato, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash. Aycha Abdollahi, Azadeh Tammour, Bahareh Hajihosseini, Benjamin Behroozi, Bharat Ajibade, Carlos Saxena, Danish Muñoz Ferrandis, David Contractor, Davis Lansky, Douwe David, Kiela, A Duong, Edward Nguyen, Emi Tan, Ezinwanne Baylor, Fatima Ozoani, Frankline Mirza, Habib Ononiwu, Hessie Rezanejad, Indrani Jones, Irene Bhattacharya, Irina Solaiman, Isar Sedenko, Jesse Nejadgholi, Josh Passmore, Julio Bonis Seltzer, Livia Sanz, Mairon Dutra, Maraim Samagaio, Margot Elbadri, Marissa Mieskes, Martha Gerchick, Michael Akinlolu, Mike Mckenna, Muhammed Qiu, Mykola Ghauri, Nafis Burynok, Nazneen Abrar, Nour Rajani, Nour Elkott, Olanrewaju Fahmy, Ran Samuel, Rasmus An, Ryan Kromann, Samira Hao, Sarmad Alizadeh, Shubber, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts; Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz; Bo Wang, Caio Brito,; Maria A Castillo; Marianna Nezhurina, Mario Sänger, Matthias Samwald, Michael Cullan; Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo CanalliMaiko Takeuchi, Marc Pàmies2022Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam ; Sushil Bharati, Tanmay Laud, Théo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis LabrakHyung Won Chung, Jaesung Tae, Jason Phang. Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter. Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. Bloom: A 176b-parameter open-access multilingual language model</p>
<p>Gianluca Sforna, deric4 (github handle). 2023Greg Landrum, and Hans De Winter. rdkit</p>
<p>Everything is connected: Graph neural networks. Petar Veličković, Current Opinion in Structural Biology. 791025382023</p>
<p>Grammar as a foreign language. Oriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, Geoffrey Hinton, Advances in neural information processing systems. 282015</p>
<p>Large-scale self-and semi-supervised learning for speech translation. Changhan Wang, Anne Wu, Juan Miguel Pino, Alexei Baevski, Michael Auli, Alexis Conneau, CoRR, abs/2104.066782021</p>
<p>Language models are open knowledge graphs. Chenguang Wang, Xiao Liu, Dawn Song, 2020</p>
<p>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. David Weininger, 10.1021/ci00057a005J. Chem. Inf. Comput. Sci. 0095- 2338281feb 1988</p>
<p>Lit: Zero-shot transfer with locked-image text tuning. Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, Lucas Beyer, CoRR, abs/2111.079912021</p>            </div>
        </div>

    </div>
</body>
</html>
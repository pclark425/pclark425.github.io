<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1569 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1569</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1569</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-30.html">extraction-schema-30</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-30ec43601d48236f8983d7cc55bed9cc68262b57</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/30ec43601d48236f8983d7cc55bed9cc68262b57" target="_blank">Automated Curriculum Learning for Neural Networks</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> Experimental results for LSTM networks on three curricula demonstrate that the approach can significantly accelerate learning, in some cases halving the time required to attain a satisfactory performance level.</p>
                <p><strong>Paper Abstract:</strong> We introduce a method for automatically selecting the path, or syllabus, that a neural network follows through a curriculum so as to maximise learning efficiency. A measure of the amount that the network learns from each data sample is provided as a reward signal to a nonstationary multi-armed bandit algorithm, which then determines a stochastic syllabus. We consider a range of signals derived from two distinct indicators of learning progress: rate of increase in prediction accuracy, and rate of increase in network complexity. Experimental results for LSTM networks on three curricula demonstrate that our approach can significantly accelerate learning, in some cases halving the time required to attain a satisfactory performance level.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1569.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1569.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoCurriculum-Exp3.S</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated Curriculum Learning using Exp3.S with learning-progress rewards</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptive, bandit-based syllabus that selects tasks for neural network training by maximising measured learning progress per unit processing time; uses Exp3.S to produce a stochastic policy over tasks and several loss- and complexity-based progress signals as rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Stacked LSTM (task-specific variants)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Unidirectional stacked LSTM networks (experiments use one- and two-layer LSTMs with 512 cells per layer) trained with RMSProp; two training regimes used: maximum-likelihood (standard cross-entropy) and variational-inference (VI) with a mean/variance posterior over weights.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic curricula: (1) character n-gram language data; (2) RepeatCopy (algorithmic sequence copy + repeats); (3) bAbI question-answering suite</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Three text-based task suites: (1) datasets generated by Kneser-Ney n-gram generators (n=0..10) where the model predicts next characters given context; (2) repeat-copy: input random bit-vector sequences and output them a specified number of times (parametrised by sequence length and repeat count); (3) bAbI: 20 synthetic QA tasks expressed in text probing reasoning/commonsense (e.g., Single/Three Supporting Facts, Path Finding, Time Reasoning). Interactions are supervised sequence-in / sequence-out training (not interactive RL environments but text-based procedural/QA tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>commonsense procedures and algorithmic / procedural tasks (text-based reasoning and memory procedures)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>bAbI reasoning tasks (Single Supporting Fact, Three Supporting Facts, Path Finding, Time Reasoning); RepeatCopy (copy sequence and repeat K times for varying lengths); predicting next characters for n-gram generated text (0- to 10-gram sources).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Yes — tasks vary along orthogonal/compositional axes: repeat-copy composes two axes (sequence length × repeat count) forming a 2D compositional grid (169 tasks in experiments); bAbI tasks exhibit increasing compositional/relational complexity (e.g. single → multi-supporting facts); n-gram datasets increase structural depth (n) monotonically.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>Adaptive bandit syllabus (Exp3.S) using learning-progress rewards</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>At each training step a task index k is sampled from a stochastic policy π_t produced by the Exp3.S adversarial bandit algorithm. A batch is drawn from the chosen task, the network trains on it, and a learning-progress signal ν (one of several loss- or complexity-based gains) is computed and divided by processing time to obtain a raw reward. Rewards are adaptively rescaled to [-1,1] and used to update Exp3.S weights, producing an evolving non-uniform syllabus that focuses on tasks yielding the largest measured instantaneous progress per unit time.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>Adaptive optimisation of measured learning progress (prediction gain or complexity gain) — i.e., tasks are ordered responsively by which task currently yields greatest estimated learning progress per processing time.</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>n-gram: n = 0..10; RepeatCopy: sequence length 1..13 and repeats 1..13 (169 tasks); bAbI: 20 tasks from simple single-fact QA to multi-fact and path/time reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td>Reported qualitatively and relatively: (a) RepeatCopy (VI training): the gradient variational complexity gain (GVCG) syllabus solved the target (length=13,repeats=13) about twice as fast as uniform sampling (i.e., ≈2x acceleration in wall-clock training steps to target); GVCG also induced higher network complexity (per VI complexity cost) than uniform. (b) bAbI (ML training): Prediction Gain (PG) produced faster learning and a larger number of tasks completed (under the standard <5% error completion criterion) than uniform sampling; SPG also improved but less strongly. (c) n-gram: complexity-based gains (L2G, GL2G, GVCG) focused rapidly on the highest-n (10-gram) task; loss-based gains tended to move more gradually toward higher-n. Exact numeric curves are reported in the paper's figures; key quantified claim: GVCG ~2x speedup on RepeatCopy VI vs uniform.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Baselines: uniform sampling over tasks and direct training on target task. Uniform sampling is a surprisingly strong baseline. Direct training on the hardest target (e.g., RepeatCopy target task) failed to learn for the LSTM (no convergence). For many experiments the adaptive curriculum outperformed uniform (not always — some signals were worse), and target-only training was often much worse or failed.</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td>Multiple progress signals compared: (1) Loss-driven: Prediction Gain (PG), Gradient Prediction Gain (GPG), Self Prediction Gain (SPG), Target Prediction Gain (TPG), Mean Prediction Gain (MPG). (2) Complexity-driven: Variational Complexity Gain (VCG), Gradient Variational Complexity Gain (GVCG), L2 Gain (L2G) and its gradient approximation (GL2G). Findings: For maximum-likelihood training PG was the most consistent performer; for variational-inference training GVCG performed best (notably ~2x speedup on RepeatCopy). VCG showed higher noise and was less effective. GPG/GL2G often performed worse than uniform (bias from gradient approximation harmful). L2G was unreliable. Some signals (PG, SPG) discovered sensible task orderings (e.g., bAbI progression from Single Supporting Fact → Three Supporting Facts).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>Yes — the adaptive syllabuses exploit transfer: e.g., GVCG syllabuses for RepeatCopy first focus on short sequences with high repeats then long sequences with low repeats, reducing loss across many tasks that the policy does not focus on, so the network generalises across combinations without visiting every task; bAbI syllabuses discovered implicit orderings enabling transfer from simpler to harder QA tasks (e.g., solving Time Reasoning and Path Finding earlier than uniform). No numeric generalisation percentages beyond qualitative/relative speedups are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) An adaptive bandit syllabus (Exp3.S) driven by instantaneous learning-progress signals can substantially accelerate training compared to uniform sampling and is able to discover implicit task orderings that enable transfer. 2) Choice of progress signal is critical: prediction-gain style signals (PG, SPG) are robust for maximum-likelihood training, while gradient variational complexity gain (GVCG) is most effective under variational-inference training (notably ~2x speedup on RepeatCopy). 3) Some approximations are harmful: gradient-based approximations (GPG, GL2G) can introduce bias that degrades curriculum performance below uniform. 4) Uniform sampling remains a strong baseline; direct training on the hardest target often fails, highlighting the practical necessity of curriculum learning for some problems.</td>
                        </tr>
                        <tr>
                            <td><strong>follow_up_notes</strong></td>
                            <td>Experiments include detailed per-task policy and loss curves (figures) demonstrating syllabuses and comparative performance; the method is presented as general and applicable to other curricula where per-sample learning-progress estimates are available.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Curriculum Learning for Neural Networks', 'publication_date_yy_mm': '2017-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Unifying count-based exploration and intrinsic motivation <em>(Rating: 2)</em></li>
                <li>Vime: Variational information maximizing exploration <em>(Rating: 2)</em></li>
                <li>Intrinsic motivation systems for autonomous mental development <em>(Rating: 1)</em></li>
                <li>The strategic student approach for life-long exploration and learning <em>(Rating: 2)</em></li>
                <li>Curriculum learning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1569",
    "paper_id": "paper-30ec43601d48236f8983d7cc55bed9cc68262b57",
    "extraction_schema_id": "extraction-schema-30",
    "extracted_data": [
        {
            "name_short": "AutoCurriculum-Exp3.S",
            "name_full": "Automated Curriculum Learning using Exp3.S with learning-progress rewards",
            "brief_description": "An adaptive, bandit-based syllabus that selects tasks for neural network training by maximising measured learning progress per unit processing time; uses Exp3.S to produce a stochastic policy over tasks and several loss- and complexity-based progress signals as rewards.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Stacked LSTM (task-specific variants)",
            "agent_description": "Unidirectional stacked LSTM networks (experiments use one- and two-layer LSTMs with 512 cells per layer) trained with RMSProp; two training regimes used: maximum-likelihood (standard cross-entropy) and variational-inference (VI) with a mean/variance posterior over weights.",
            "agent_size": null,
            "environment_name": "Synthetic curricula: (1) character n-gram language data; (2) RepeatCopy (algorithmic sequence copy + repeats); (3) bAbI question-answering suite",
            "environment_description": "Three text-based task suites: (1) datasets generated by Kneser-Ney n-gram generators (n=0..10) where the model predicts next characters given context; (2) repeat-copy: input random bit-vector sequences and output them a specified number of times (parametrised by sequence length and repeat count); (3) bAbI: 20 synthetic QA tasks expressed in text probing reasoning/commonsense (e.g., Single/Three Supporting Facts, Path Finding, Time Reasoning). Interactions are supervised sequence-in / sequence-out training (not interactive RL environments but text-based procedural/QA tasks).",
            "procedure_type": "commonsense procedures and algorithmic / procedural tasks (text-based reasoning and memory procedures)",
            "procedure_examples": "bAbI reasoning tasks (Single Supporting Fact, Three Supporting Facts, Path Finding, Time Reasoning); RepeatCopy (copy sequence and repeat K times for varying lengths); predicting next characters for n-gram generated text (0- to 10-gram sources).",
            "compositional_structure": "Yes — tasks vary along orthogonal/compositional axes: repeat-copy composes two axes (sequence length × repeat count) forming a 2D compositional grid (169 tasks in experiments); bAbI tasks exhibit increasing compositional/relational complexity (e.g. single → multi-supporting facts); n-gram datasets increase structural depth (n) monotonically.",
            "uses_curriculum": true,
            "curriculum_name": "Adaptive bandit syllabus (Exp3.S) using learning-progress rewards",
            "curriculum_description": "At each training step a task index k is sampled from a stochastic policy π_t produced by the Exp3.S adversarial bandit algorithm. A batch is drawn from the chosen task, the network trains on it, and a learning-progress signal ν (one of several loss- or complexity-based gains) is computed and divided by processing time to obtain a raw reward. Rewards are adaptively rescaled to [-1,1] and used to update Exp3.S weights, producing an evolving non-uniform syllabus that focuses on tasks yielding the largest measured instantaneous progress per unit time.",
            "curriculum_ordering_principle": "Adaptive optimisation of measured learning progress (prediction gain or complexity gain) — i.e., tasks are ordered responsively by which task currently yields greatest estimated learning progress per processing time.",
            "task_complexity_range": "n-gram: n = 0..10; RepeatCopy: sequence length 1..13 and repeats 1..13 (169 tasks); bAbI: 20 tasks from simple single-fact QA to multi-fact and path/time reasoning.",
            "performance_with_curriculum": "Reported qualitatively and relatively: (a) RepeatCopy (VI training): the gradient variational complexity gain (GVCG) syllabus solved the target (length=13,repeats=13) about twice as fast as uniform sampling (i.e., ≈2x acceleration in wall-clock training steps to target); GVCG also induced higher network complexity (per VI complexity cost) than uniform. (b) bAbI (ML training): Prediction Gain (PG) produced faster learning and a larger number of tasks completed (under the standard &lt;5% error completion criterion) than uniform sampling; SPG also improved but less strongly. (c) n-gram: complexity-based gains (L2G, GL2G, GVCG) focused rapidly on the highest-n (10-gram) task; loss-based gains tended to move more gradually toward higher-n. Exact numeric curves are reported in the paper's figures; key quantified claim: GVCG ~2x speedup on RepeatCopy VI vs uniform.",
            "performance_without_curriculum": "Baselines: uniform sampling over tasks and direct training on target task. Uniform sampling is a surprisingly strong baseline. Direct training on the hardest target (e.g., RepeatCopy target task) failed to learn for the LSTM (no convergence). For many experiments the adaptive curriculum outperformed uniform (not always — some signals were worse), and target-only training was often much worse or failed.",
            "has_curriculum_comparison": true,
            "alternative_curriculum_performance": "Multiple progress signals compared: (1) Loss-driven: Prediction Gain (PG), Gradient Prediction Gain (GPG), Self Prediction Gain (SPG), Target Prediction Gain (TPG), Mean Prediction Gain (MPG). (2) Complexity-driven: Variational Complexity Gain (VCG), Gradient Variational Complexity Gain (GVCG), L2 Gain (L2G) and its gradient approximation (GL2G). Findings: For maximum-likelihood training PG was the most consistent performer; for variational-inference training GVCG performed best (notably ~2x speedup on RepeatCopy). VCG showed higher noise and was less effective. GPG/GL2G often performed worse than uniform (bias from gradient approximation harmful). L2G was unreliable. Some signals (PG, SPG) discovered sensible task orderings (e.g., bAbI progression from Single Supporting Fact → Three Supporting Facts).",
            "transfer_generalization": "Yes — the adaptive syllabuses exploit transfer: e.g., GVCG syllabuses for RepeatCopy first focus on short sequences with high repeats then long sequences with low repeats, reducing loss across many tasks that the policy does not focus on, so the network generalises across combinations without visiting every task; bAbI syllabuses discovered implicit orderings enabling transfer from simpler to harder QA tasks (e.g., solving Time Reasoning and Path Finding earlier than uniform). No numeric generalisation percentages beyond qualitative/relative speedups are reported.",
            "key_findings": "1) An adaptive bandit syllabus (Exp3.S) driven by instantaneous learning-progress signals can substantially accelerate training compared to uniform sampling and is able to discover implicit task orderings that enable transfer. 2) Choice of progress signal is critical: prediction-gain style signals (PG, SPG) are robust for maximum-likelihood training, while gradient variational complexity gain (GVCG) is most effective under variational-inference training (notably ~2x speedup on RepeatCopy). 3) Some approximations are harmful: gradient-based approximations (GPG, GL2G) can introduce bias that degrades curriculum performance below uniform. 4) Uniform sampling remains a strong baseline; direct training on the hardest target often fails, highlighting the practical necessity of curriculum learning for some problems.",
            "follow_up_notes": "Experiments include detailed per-task policy and loss curves (figures) demonstrating syllabuses and comparative performance; the method is presented as general and applicable to other curricula where per-sample learning-progress estimates are available.",
            "uuid": "e1569.0",
            "source_info": {
                "paper_title": "Automated Curriculum Learning for Neural Networks",
                "publication_date_yy_mm": "2017-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Unifying count-based exploration and intrinsic motivation",
            "rating": 2
        },
        {
            "paper_title": "Vime: Variational information maximizing exploration",
            "rating": 2
        },
        {
            "paper_title": "Intrinsic motivation systems for autonomous mental development",
            "rating": 1
        },
        {
            "paper_title": "The strategic student approach for life-long exploration and learning",
            "rating": 2
        },
        {
            "paper_title": "Curriculum learning",
            "rating": 2
        }
    ],
    "cost": 0.010631749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Automated Curriculum Learning for Neural Networks</h1>
<p>Alex Graves, Marc G. Bellemare, Jacob Menick, Rémi Munos, Koray Kavukcuoglu<br>{gravesa, bellemare, jmenick, munos, korayk}@google.com<br>Google DeepMind, London UK</p>
<h4>Abstract</h4>
<p>We introduce a method for automatically selecting the path, or syllabus, that a neural network follows through a curriculum so as to maximise learning efficiency. A measure of the amount that the network learns from each data sample is provided as a reward signal to a nonstationary multiarmed bandit algorithm, which then determines a stochastic syllabus. We consider a range of signals derived from two distinct indicators of learning progress: rate of increase in prediction accuracy, and rate of increase in network complexity. Experimental results for LSTM networks on three curricula demonstrate that our approach can significantly accelerate learning, in some cases halving the time required to attain a satisfactory performance level.</p>
<h2>1. Introduction</h2>
<p>Over two decades ago, in The importance of starting small, Elman put forward the idea that a curriculum of progressively harder tasks could significantly accelerate a neural network's training (Elman, 1993). However curriculum learning has only recently become prevalent in the field (e.g., Bengio et al., 2009), due in part to the greater complexity of problems now being considered. In particular, recent work on learning programs with neural networks has relied on curricula to scale up to longer or more complicated programs (Sutskever and Zaremba, 2014; Reed and de Freitas, 2015; Graves et al., 2016). We expect this trend to continue as the scope of neural networks widens.</p>
<p>One reason for the slow adoption of curriculum learning is that its effectiveness is highly sensitive to the mode of progression through the tasks. One popular approach is to define a hand-chosen performance threshold for advancement to the next task, along with a fixed probability of re-
turning to earlier tasks, to prevent forgetting (Sutskever and Zaremba, 2014). However, as well as introducing hard-totune parameters, this poses problems for curricula where appropriate thresholds may be unknown or variable across tasks. More fundamentally, it presupposes that the tasks can be ordered by difficulty, when in reality they may vary along multiple axes of difficulty, or have no predefined order at all.</p>
<p>We propose to instead treat the decision about which task to study next as a stochastic policy, continuously adapted to optimise some notion of what Oudeyer et al. (2007) termed learning progress. Doing so brings us into contact with the intrinsic motivation literature (Barto, 2013), where various indicators of learning progress have been used as reward signals to encourage exploration, including compression progress (Schmidhuber, 1991), information acquisition (Storck et al., 1995), Bayesian surprise (Itti and Baldi, 2009), prediction gain (Bellemare et al., 2016) and variational information maximisation (Houthooft et al., 2016). We focus on variants of prediction gain, and also introduce a novel class of progress signals which we refer to as complexity gain. Derived from minimum description length principles, complexity gain equates acquisition of knowledge with an increase in effective information encoded in the network weights.</p>
<p>Given a progress signal that can be evaluated for each training example, we use a multi-armed bandit algorithm to find a stochastic policy over the tasks that maximises overall progress. The bandit is nonstationary because the behaviour of the network, and hence the optimal policy, evolves during training. We take inspiration from a previous work that modelled an adaptive student with a multiarmed bandit in the context of developmental learning (Lopes and Oudeyer, 2012). Another related area is the field of active learning, where similar gain signals have been used to guide decisions about which data point to label next (Settles, 2010). Lastly, there are parallels with recent work on using Bayesian optimisation to find the best order in which to train a word embedding network on a language corpus (Tsvetkov, 2016); however this differs from our work in that the ordering was entirely determined be-</p>
<p>fore each training run, rather than adaptively altered in response to the model's progress.</p>
<h2>2. Background</h2>
<p>We consider supervised or unsupervised learning problems where target sequences $\mathbf{b}^{1}, \mathbf{b}^{2}, \ldots$ are conditionally modelled given their respective input sequences $\mathbf{a}^{1}, \mathbf{a}^{2}, \ldots$ For convenience we suppose that the targets are drawn from a finite set $\mathcal{B}$, noting our framework extends to continuous targets, with densities taking the place of probabilities. As is typical for neural networks, sequences may be grouped together in batches $\left(\mathbf{b}^{1: B}, \mathbf{a}^{1: B}\right)$ to accelerate training. The conditional probability output by the model is</p>
<p>$$
p\left(\mathbf{b}^{1: B} \mid \mathbf{a}^{1: B}\right)=\prod_{i, j} p\left(\mathbf{b}<em 1:="1:" j-1="j-1">{j}^{i} \mid \mathbf{b}</em>\right)
$$}^{i}, \mathbf{a}_{1: j-1}^{i</p>
<p>From here onwards, we consider each batch as a single example $\mathbf{x}$ from $\mathcal{X}:=(\mathcal{A} \times \mathcal{B})^{N}$, and write $p(\mathbf{x}):=$ $p\left(\mathbf{b}^{1: B} \mid \mathbf{a}^{1: B}\right)$ for its probability. Under this notation, a task is a distribution $D$ over sequences from $\mathcal{X}$. A curriculum is an ensemble of tasks $D_{1}, \ldots, D_{N}$, and a sample is an example drawn from one of the tasks of the curriculum. Finally, a syllabus is a time-varying sequence of distributions over tasks.</p>
<p>We consider a neural network to be a parametric probabilistic model $p_{\theta}$ over $\mathcal{X}$, whose parameters are denoted $\theta$. The expected loss of the network on the $k^{\text {th }}$ task is</p>
<p>$$
\mathcal{L}<em k="k">{k}(\theta):=\underset{\mathbf{x} \sim D</em>, \theta)
$$}}{\mathbb{E}} L(\mathbf{x</p>
<p>where $L(\mathbf{x}, \theta):=-\log p_{\theta}(\mathbf{x})$ is the sample loss on $\mathbf{x}$. Whenever unambiguous, we will simply denote the expected and sample losses by $\mathcal{L}_{k}$ and $L(\mathbf{x})$ respectively.</p>
<h3>2.1. Curriculum Learning</h3>
<p>We consider two related settings. In the multiple tasks setting, The goal is to perform as well as possible on all tasks in the ensemble $\left{D_{k}\right}$; this is captured by the objective function</p>
<p>$$
\mathcal{L}<em k="1">{\mathrm{MT}}:=\frac{1}{N} \sum</em>
$$}^{N} \mathcal{L}_{k</p>
<p>In the target task setting, we are only interested in minimizing the loss on the final task $D_{N}$. The other tasks then act as a series of stepping stones to the real problem. The objective function in this setting is simply $\mathcal{L}<em N="N">{\mathrm{TT}}:=\mathcal{L}</em>$.</p>
<h3>2.2. Adversarial Multi-Armed Bandits</h3>
<p>We view a curriculum containing $N$ tasks as an $N$-armed bandit (Bubeck and Cesa-Bianchi, 2012), and a syllabus as an adaptive policy which seeks to maximize payoffs from
this bandit. In the bandit setting, an agent selects a sequence of arms (actions) $a_{1} \ldots a_{T}$ over $T$ rounds of play $\left(a_{t} \in{1, \ldots, N}\right)$. After each round, the selected arm yields a payoff $r_{t}$; the payoffs for the other arms are not observed.</p>
<p>The classic algorithm for adversarial bandits is Exp3 (Auer et al., 2002), which uses multiplicative weight updates to guarantee low regret with respect to the best arm. On round $t$, the agent selects an arm stochastically according to a policy $\pi_{t}$. This policy is defined by a set of weights $w_{t, i}$ :</p>
<p>$$
\pi_{t}^{\operatorname{Exp3}}(i):=\frac{e^{w_{t, i}}}{\sum_{j=1}^{N} e^{w_{t, j}}}
$$</p>
<p>The weights are the sum of importance-sampled rewards:</p>
<p>$$
w_{t, i}:=\eta \sum_{s&lt;t} \tilde{r}<em i="i" s_="s,">{s, i} \quad \tilde{r}</em>}:=\frac{r_{s} \mathbb{I<em s="s">{[a</em>
$$}=i]}}{\pi_{s}(i)</p>
<p>Exp3 acts so as to minimize regret with respect to the single best arm evaluated over the whole history. However, a common occurrence is for an arm to be optimal for a portion of the history, then another arm, and so on; the best strategy is then piecewise stationary. This is generally the case in our setting, as the expected reward for each task changes as the model learns. The Fixed Share method (Herbster and Warmuth, 1998) addresses this issue by using an $\epsilon$-greedy strategy and mixing in the weights additively. In the bandit setting, this is known as the Exp3.S algorithm (also by Auer et al. (2002)):</p>
<p>$$
\begin{aligned}
\pi_{t}^{\operatorname{Exp} 3 . p}(i) &amp; :=(1-\epsilon) \pi_{t}^{\operatorname{Exp} 3}(i)+\frac{\epsilon}{N} \
w_{t, i}^{\mathrm{s}} &amp; :=\log \left[\left(1-\alpha_{t}\right) \exp \left{w_{t-1, i}^{\mathrm{s}}+\eta \tilde{r}<em t="t">{t-1, i}^{\beta}\right}\right. \
&amp; \left.+\frac{\alpha</em>}}{N-1} \sum_{j \neq i} \exp \left{w_{t-1, j}^{\mathrm{s}}+\eta \tilde{r<em 1_="1," i="i">{t-1, j}^{\beta}\right}\right] \
w</em>}^{\mathrm{s}} &amp; :=0 \quad \alpha_{t}:=t^{-1} \quad \tilde{r<em s="s">{s, i}^{\beta}:=\frac{r</em>} \mathbb{I<em s="s">{[a</em>
\end{aligned}
$$}=i]}+\beta}{\pi_{s}(i)</p>
<h3>2.3. Reward Scaling</h3>
<p>The appropriate step size $\eta$ depends on the magnitudes of the rewards, which may not be known a priori. The problem is particularly acute in our setting, where the magnitude depends on how learning progress is measured, and varies over time as the model learns. To address this issue, we adaptively rescale all rewards to lie in the interval $[-1,1]$ using the following procedure: Let $\mathcal{R}<em t="t">{t}$ be the history of unscaled rewards up to time $t$, i.e. $\mathcal{R}</em>}=\left{\tilde{r<em i="1">{i}\right}</em>}^{t-1}$. Let $q_{t}^{\mathrm{lo}}$ and $q_{t}^{\mathrm{hi}}$ be quantiles of $\mathcal{R<em t="t">{t}$, which we choose here to be the $20^{\text {th }}$ and $80^{\text {th }}$ percentiles respectively. The scaled reward $r</em>}$ is obtained by clipping $\tilde{r<em t="t">{t}$ to the interval $\left[q</em>\right]$}^{\mathrm{lo}}, q^{\mathrm{hi}_{t}</p>
<p>and then linearly mapping the result to lie in $[-1,1]$ :</p>
<p>$$
r_{t}= \begin{cases}-1 &amp; \text { if } \hat{r}<em t="t">{t}<q_{t}^{\mathrm{lo}} \\ 1 & \text { if } \hat{r}_{t}>q</em>}^{\mathrm{hi}} \ \frac{2\left(\hat{r<em t="t">{t}-q</em>
$$}^{\mathrm{lo}}\right)}{q_{t}^{\mathrm{lo}}-q_{t}^{\mathrm{hi}}}-1 &amp; \text { otherwise }\end{cases</p>
<p>Rather than keeping the entire history of rewards, we use reservoir sampling to maintain a representative sample, and compute approximate quantiles from this sample. These quantiles can be obtained in $\Theta\left(\log \left|\mathcal{R}_{t}\right|\right)$ time.</p>
<h2>3. Learning Progress Signals</h2>
<p>Our goal is to use the policy output by Exp3.S as a syllabus for training our models. Ideally we would like the policy to maximize the rate at which we minimize the loss, and the reward should reflect this rate - what Oudeyer et al. (2007) calls learning progress. However, it usually is computationally undesirable or even impossible to measure the effect of a training sample on the target objective, and we therefore turn to surrogate measures of progress. Broadly, these measures are either 1) loss-driven, in the sense that they equate reward with a decrease in some loss; or 2) complexity-driven, when they equate reward with an increase in model complexity.</p>
<p>Training proceeds as follows: at each time $t$, we first sample a task index $k \sim \pi_{t}$. We then generate a sample from this task, i.e. $\mathbf{x} \sim D_{k}$. Note that each $\mathbf{x}$ is in general a batch of training sequences, and that in order to reduce noise in the gain signal we draw the whole batch from a single task. We compute the chosen measure of learning progress $\nu$ then divide by the time $\tau(\mathbf{x})$ required to process the sample (since it is the rate of progress we are concerned with, and processing time may vary from task to task) to get the raw reward $\hat{r}=\nu / \tau(\mathbf{x})$ For the purposes of this work, $\tau(\mathbf{x})$ was simply the length of the longest input sequence in $\mathbf{x}$; for other tasks or architectures a more complex calculation may be required. We then rescale $\hat{r}$ into a reward $r_{t} \in[-1,1]$, and provide it to Exp3.S. The procedure is summarized as Algorithm 1.</p>
<h3>3.1. Loss-driven Progress</h3>
<p>We consider five loss-driven progress signals, all which compare the predictions made by the model before and after training on some sample $\mathbf{x}$. The first two signals we present are instantaneous in the sense that they only depend on $\mathbf{x}$. Such signals are appealing because they are typically cheaper to evaluate, and are agnostic about the overall goal of the curriculum. The remaining three signals more directly measure the effect of training on the desired objective, but require an additional sample $\mathbf{x}^{\prime}$. In what follows we denote the model parameters before and after training on $\mathbf{x}$ by $\theta$ and $\theta^{\prime}$ respectively.</p>
<p>Algorithm 1 Intrinsically Motivated Curriculum Learning
Initially: $w_{i}=0$ for $i \in[N]$
for $t=1 \ldots T$ do
$\pi(k):=(1-\epsilon) \frac{e^{w_{k}}}{\sum_{i} e^{w_{i}}}+\frac{\epsilon}{N}$
Draw task index $k$ from $\pi$
Draw training sample $\mathbf{x}$ from $D_{k}$
Train network $p_{\theta}$ on $\mathbf{x}$
Compute learning progress $\nu$ (Sections 3.1 \&amp; 3.2)
Map $\hat{r}=\nu / \tau(\mathbf{x})$ to $r \in[-1,1]$ (Section 2.3)
Update $w_{i}$ with reward $r$ using Exp3.S (1)
end for</p>
<p>Prediction gain (PG). Prediction gain is defined as the instantaneous change in loss for a sample $\mathbf{x}$, before and after training on $\mathbf{x}$ :</p>
<p>$$
\nu_{P G}:=L(\mathbf{x}, \theta)-L\left(\mathbf{x}, \theta^{\prime}\right)
$$</p>
<p>When $p_{\theta}$ is a Bayesian mixture model, prediction gain upper bounds the model's information gain (Bellemare et al., 2016), and is therefore closely related to the Bayesian precept that learning is a change in posterior.</p>
<p>Gradient prediction gain (GPG). Computing prediction gain requires an additional forward pass. When $p_{\theta}$ is differentiable, an alternative is to consider the first-order Taylor series approximation to prediction gain:</p>
<p>$$
L\left(\mathbf{x}, \theta^{\prime}\right) \approx L(\mathbf{x}, \theta)+[\nabla L(\mathbf{x}, \theta)]^{\top} \Delta_{\theta}
$$</p>
<p>where $\Delta_{\theta}$ is the descent step. Taking this step to be the negative gradient $-\nabla_{\theta} L(\mathbf{x}, \theta)$ we obtain the gradient prediction gain</p>
<p>$$
\nu_{G P G}:=|\nabla L(\mathbf{x}, \theta)|_{2}^{2}
$$</p>
<p>This measures the magnitude of the gradient vector, which has been used an indicator of data salience in the active learning literature (Settles et al., 2008). We will show below that gradient prediction gain is a biased estimate true expected learning progress, and in particular favours tasks whose loss has higher variance.</p>
<p>Self prediction gain (SPG). Prediction gain is a biased estimate of the change in $\mathcal{L}_{k}(\theta)$, the expected loss on task $k$. Having trained on $\mathbf{x}$, we naturally expect the sample loss $L(\mathbf{x}, \theta)$ to decrease, even though the loss at other points may increase. Self prediction gain addresses this issue by sampling a second time from the same task and estimating progress on the new sample:</p>
<p>$$
\nu_{S P G}:=L\left(\mathbf{x}^{\prime}, \theta\right)-L\left(\mathbf{x}^{\prime}, \theta^{\prime}\right) \quad \mathbf{x}^{\prime} \sim D_{k}
$$</p>
<p>Target prediction gain (TPG). We can take the selfprediction gain idea further and evaluate directly on the loss of interest, which has has also been considered in active learning (Roy and Mccallum, 2001). In the target task setting, this becomes</p>
<p>$$
\nu_{T P G}:=L\left(\mathbf{x}^{\prime}, \theta\right)-L\left(\mathbf{x}^{\prime}, \theta^{\prime}\right) \quad \mathbf{x}^{\prime} \sim D_{N}
$$</p>
<p>Although this might seem like the most accurate measure so far, it tends to suffer from high variance, and also runs counter to the premise that, early in training, the model cannot improve on the difficult target task and should instead train on a task that it can master.</p>
<p>Mean prediction gain (MPG). Mean prediction gain is the analogue of target prediction gain in the multiple tasks setting, where it is natural to evaluate our progress across all tasks. We write</p>
<p>$$
\nu_{M P G}:=L\left(\mathbf{x}^{\prime}, \theta\right)-L\left(\mathbf{x}^{\prime}, \theta^{\prime}\right) \quad \mathbf{x}^{\prime} \sim D_{k}, k \sim U_{N}
$$</p>
<p>where $U_{N}$ denotes the uniform distribution over ${1, \ldots, N}$. Mean prediction gain has additional variance from sampling an evaluation task $k \sim U_{N}$.</p>
<h3>3.2. Complexity-driven Progress</h3>
<p>So far we have considered gains that gauge the network's learning progress directly, by observing the rate of change in its predictive ability. We now turn to a novel set of gains that instead measure the rate at which the network's complexity increases. These gains are underpinned by the Minimum Description Length (MDL) principle (Rissanen, 1986; Grünwald, 2007): in order to best generalise from a particular dataset, one should minimise the number of bits required to describe the model parameters plus the number of bits required for the model to describe the data.</p>
<p>According to the MDL principle, increasing the model complexity by a certain amount is only worthwhile if it compresses the data by a greater amount. We would therefore expect the complexity to increase most in response to the training examples from which the network is best able to generalise. These examples are exactly what we seek when attempting to maximise learning progress.</p>
<p>MDL training for neural networks (Hinton and Van Camp, 1993) can be practically realised with stochastic variational inference (Graves, 2011; Kingma et al., 2015; Blundell et al., 2015). In this framework a variational posterior $P_{\phi}(\theta)$ over the network weights is maintained during training, with a single weight sample drawn for each training example. The parameters $\phi$ of the posterior are optimised, rather than $\theta$ itself. The total loss is the expected log-loss of the training dataset ${ }^{1}$ (which in our case is the complete</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>curriculum), plus the KL-divergence between the posterior and some fixed (Blundell et al., 2015) or adaptive (Graves, 2011) prior $Q_{\psi}(\theta)$ :</p>
<p>$$
L_{V I}(\phi, \psi)=\underbrace{K L\left(P_{\phi} | Q_{\psi}\right)}<em k="k">{\text {model complexity }}+\underbrace{\sum</em>
$$} \sum_{\mathbf{x} \in D_{k}} \underset{\theta \sim P_{\phi}}{\mathbb{E}} L(\mathbf{x}, \theta)}_{\text {data cost }</p>
<p>Since we are using stochastic gradient descent we need to determine the per-sample loss for both the model complexity and the data. Defining $S:=\sum_{k}\left|D_{k}\right|$ as the total number of samples in the curriculum we obtain</p>
<p>$$
L_{V I}(\mathbf{x}, \phi, \psi):=\frac{1}{S} K L\left(P_{\phi} | Q_{\psi}\right)+\underset{\theta \sim P_{\phi}}{\mathbb{E}} L(\mathbf{x}, \theta)
$$</p>
<p>with $L_{V I}(\phi, \psi)=\sum_{k} \sum_{\mathbf{x} \sim D_{k}} L_{V I}(\mathbf{x}, \phi, \psi)$. Some of the curricula we consider are algorithmically generated, meaning that the number of samples in each task is undefined. The treatment suggested by the MDL principle is to divide the complexity cost by the total number of samples generated so far. However we simplified matters by setting $S$ to a large constant that roughly matches the number of samples we expect to see during training.</p>
<p>We used a diagonal Gaussian for both P and Q , allowing us to determine the complexity cost analytically:</p>
<p>$$
K L\left(P_{\phi} | Q_{\psi}\right)=\frac{\left(\mu_{\phi}-\mu_{\psi}\right)^{2}+\sigma_{\phi}^{2}-\sigma_{\psi}^{2}}{2 \sigma_{\psi}^{2}}+\ln \left(\frac{\sigma_{\psi}}{\sigma_{\phi}}\right)
$$</p>
<p>where $\mu_{\phi}, \sigma_{\phi}^{2}$ and $\mu_{\psi}, \sigma_{\psi}^{2}$ are the mean and variance vectors for $P_{\phi}$ and $Q_{\psi}$ respectively. We adapted $\psi$ with gradient descent along with $\phi$, and the gradient of $\mathbb{E}<em _phi="\phi">{\theta \sim P</em>\right)$ was used to ensure that the variances were positive (Blundell et al., 2015).}} L(\mathbf{x}, \theta)$ with respect to $\phi$ was estimated using the reparameterisation trick ${ }^{2}$ (Kingma and Welling, 2013) with a single Monte-Carlo sample. The SoftPlus function $y=\ln \left(1+e^{x</p>
<p>Variational complexity gain (VCG). The increase of model complexity induced by a training example can be estimated from the change in complexity following a single parameter update from $\phi$ to $\phi^{\prime}$ and $\psi$ to $\psi^{\prime}$, yielding</p>
<p>$$
\nu_{V C G}:=K L\left(P_{\phi^{\prime}} | Q_{\psi^{\prime}}\right)-K L\left(P_{\phi} | Q_{\psi}\right)
$$</p>
<p>Gradient variational complexity gain (GVCG). As with prediction gain, we can derive a first order Taylor ap-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>proximation using the direction of gradient descent:</p>
<p>$$
\begin{aligned}
&amp; K L\left(P_{\phi^{\prime}} | Q_{\psi^{\prime}}\right) \approx K L\left(P_{\phi} | Q_{\psi}\right) \
&amp; \quad-\left[\nabla_{\phi, \psi} K L\left(P_{\phi} | Q_{\psi}\right)\right]^{\top} \nabla_{\psi, \phi} \mathcal{L}<em C="C" G="G" V="V">{M D L}(\mathbf{x}, \phi, \psi) \
&amp; \Longrightarrow \nu</em>, \theta)
\end{aligned}
$$} \approx C-\left[\nabla_{\phi, \psi} K L\left(P_{\phi} | Q_{\psi}\right)\right]^{\top} \nabla_{\phi} \underset{\theta \sim P_{\phi}}{\mathbb{E}} L(\mathbf{x</p>
<p>where $C$ is a term that does not depend on $\mathbf{x}$ and is therefore irrelevant to the gain signal. We define the gradient variational complexity gain as</p>
<p>$$
\nu_{G V C G}:=\left[\nabla_{\phi, \psi} K L\left(P_{\phi} | Q_{\psi}\right)\right]^{\top} \nabla_{\phi} \underset{\theta \sim P_{\phi}}{\mathbb{E}} L(\mathbf{x}, \theta)
$$</p>
<p>which is the directional derivative of the $K L$ along the gradient descent direction. We believe that the linear approximation is more reliable here than for prediction gain, as the model complexity has less curvature than the loss surface.</p>
<p>Relationship to VIME. Variational Information Maximizing Exploration (VIME) (Houthooft et al., 2016), uses a reward signal that is closely related to variational complexity gain. The difference is that while VIME measures the $K L$ between the posterior before and after a step in parameter space, we consider the change in KL between the posterior and prior induced by the step. Therefore, while VIME looks for any change to the posterior, we focus only on changes that alter the divergence from the prior. Further research will be needed to assess the relative merits of the two signals.</p>
<p>L2 gain (L2G). Variational inference tends to slow down learning, making it appealing to define a complexity-based progress signal applicable to more conventionally trained networks. Many of the standard neural network regularisation terms, such as Lp-norms, can be viewed as defining an upper bound on model description length (Graves, 2011). We therefore hypothesize that the increase in regularisation cost will be indicative of the increase in model complexity. To test this hypothesis we consider training with a standard L2 regularisation term added to the loss:</p>
<p>$$
L_{L 2}(\mathbf{x}, \theta)=L(\mathbf{x}, \theta)+\frac{\alpha}{2}|\theta|_{2}^{2}
$$</p>
<p>where $\alpha$ is an empirically chosen constant. In this case the complexity gain can be defined as</p>
<p>$$
\nu_{L 2 G}:=\left|\theta^{\prime}\right|<em 2="2">{2}^{2}-|\theta|</em>
$$}^{2</p>
<p>where we have dropped the $\alpha / 2$ term as the gain will anyway be rescaled to $[-1,1]$ before use. The corresponding first-order approximation is</p>
<p>$$
\nu_{G L 2 G}:=[\theta]^{\top} \nabla_{\theta} L(\mathbf{x}, \theta)
$$</p>
<p>It is possible to calculate L2 gain for unregularized networks; however we found this an unreliable signal, presumably because the network has no incentive to decrease complexity when faced with uninformative data.</p>
<h3>3.3. Prediction Gain Bias</h3>
<p>Prediction gain, self prediction gain and gradient prediction gain are all closely related, but incur varying degrees of bias and variance. We now present a formal analysis of the biases present in each, noting that a similar treatment can be applied to our complexity gains.
We assume that the loss $L$ is locally well-approximated by its first-order Taylor expansion:</p>
<p>$$
L\left(\mathbf{x}, \theta^{\prime}\right) \approx L(\mathbf{x}, \theta)+\nabla L(\mathbf{x}, \theta)^{\top} \Delta \theta
$$</p>
<p>where $\Delta \theta:=\theta^{\prime}-\theta$. For ease of exposition, we also suppose the network is trained with stochastic gradient descent (the same argument leads to similar conclusions when consider higher-order optimization methods):</p>
<p>$$
\Delta \theta:=-\alpha \nabla L(\mathbf{x}, \theta)
$$</p>
<p>We define the true expected learning progress as</p>
<p>$$
\nu:=\underset{\mathbf{x}^{\prime} \sim D}{\mathbb{E}}\left[\mathcal{L}(\theta)-\mathcal{L}\left(\theta^{\prime}\right)\right]=\alpha\left|\underset{\mathbf{x}^{\prime} \sim D}{\mathbb{E}} \nabla L(\mathbf{x}, \theta)\right|^{2}
$$</p>
<p>with the identity following from (8) (recall that $\mathcal{L}(\theta)=$ $\mathbb{E}_{\mathbf{x}} L(\theta)$ ). The expected prediction gain is then</p>
<p>$$
\nu_{\mathrm{PG}}=\underset{\mathbf{x}^{\prime} \sim D}{\mathbb{E}}\left[L(\mathbf{x}, \theta)-L\left(\mathbf{x}, \theta^{\prime}\right)\right]=\alpha \underset{\mathbf{x}^{\prime} \sim D}{\mathbb{E}}|\nabla L(\mathbf{x}, \theta)|^{2}
$$</p>
<p>Defining</p>
<p>$$
\mathbb{V}(\nabla L(\mathbf{x}, \theta)):=\mathbb{E}\left|\nabla L(\mathbf{x}, \theta)-\mathbb{E} \nabla L\left(\mathbf{x}^{\prime}, \theta\right)\right|^{2}
$$</p>
<p>we find that prediction gain is the sum of two terms: true expected learning progress, plus the gradient variance:</p>
<p>$$
\nu_{\mathrm{PG}}=\nu+\mathbb{V}(\nabla L(\mathbf{x}, \theta))
$$</p>
<p>We conclude that for equal learning progress, a prediction gain-based curriculum maximizes variance. The problem is made worse when using gradient prediction gain, which actually relies on the Taylor approximation (7). On the other hand, self prediction gain is an unbiased estimate of expected learning progress:</p>
<p>$$
\underset{\mathbf{x}}{\mathbb{E}} \nu_{\mathrm{SPG}}=\underset{\mathbf{x}, \mathbf{x}^{\prime} \sim D}{\mathbb{E}}\left[L\left(\mathbf{x}^{\prime}, \theta\right)-L\left(\mathbf{x}^{\prime}, \theta^{\prime}\right)\right]=\nu
$$</p>
<p>Naturally, its use of two samples results in higher variance than prediction gain, suggesting a bias-variance trade off between the two estimates.</p>
<h2>4. Experiments</h2>
<p>To test the efficacy of our approach, we applied all the gains defined in the previous section to three task suites: synthetic language modelling on text generated by n-gram</p>
<p>models, repeat copy (Graves et al., 2014) and the bAbI tasks (Weston et al., 2015)</p>
<p>The network architecture was stacked unidirectional LSTM (Graves, 2013) for all experiments, and the training loss was cross-entropy with either categorical targets and softmax output, or Bernoulli targets and sigmoid outputs, optimised by RMSProp with momentum (Tieleman, 2012; Graves, 2013), using a momentum of 0.9 and a learning rate of $10^{-5}$ unless specified otherwise. The parameters for the Exp3.S algorithm were $\eta=10^{-3}, \beta=0, \epsilon=0.05$. For all experiments, one set of networks was trained with variational inference (VI) to test the variational complexity gain signals, and another set was trained with normal maximum likelihood (ML) for the other signals; both sets were repeated 10 times with different random seeds to initialise the network weights. The $\alpha$ regularisation parameter from Eq. (4) for the networks trained with L2 gain signals was $10^{-4}$ for all experiments. For all plots with a time axis, time is defined as the total number of input steps processed so far. In the absence of hand-designed curricula for these tasks, our performance benchmarks are 1) a fixed uniform policy over all the tasks and 2) directly training on the target task (where applicable). All losses and error rates are measured on independent samples not used for training or reward calculation.</p>
<h3>4.1. N-Gram Language Modelling</h3>
<p>Our first experiment aims to illustrate and compare the behaviour induced by different gains. We trained characterlevel Kneser-Ney n-gram models (Kneser and Ney, 1995) on the King James Bible data from the Canterbury corpus (Arnold and Bell, 1997), with the maximum depth parameter $n$ ranging between 0 to 10 . We then used each model to generate a separate dataset of 1 M characters, which we divided into disjoint sequences of 150 characters. The first 50 characters of each sequence were used as burn-in context for the next 100, which the network was trained to predict. The LSTM network had two layers of 512 cells, and the batch size was 32.</p>
<p>An important characteristic of this dataset is that the amount of linguistic structure increases monotonically with $n$. Simultaneously, the entropy - and hence, minimum achievable loss - decreases almost monotonically in $n$. If we believe that learning progress should be higher for interesting data than for data that is difficult to predict, we would expect the gain signals to be drawn to higher $n$ : they should favour structure over noise. We note that in this experiment the curriculum is superfluous: the most efficient strategy for learning the 10-gram source is to directly train on it.</p>
<p>Fig. 1 shows that most of the complexity-based gain signals from Section 3.2 (L2G, GL2G, GVCG) progress rapidly</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. N-gram policies for different gain signals, truncated at $2 \times 10^{8}$ steps. All curves are averages over 10 runs
through the curriculum before focusing strongly on the 10gram task (though interestingly, GVCG appears to revisit 0 gram later on in training). The clarity of the result is striking, given that sequences generated from models beyond about 6 -gram are difficult to distinguish by eye. VCG follows a similar path, but with much less confidence, presumably due to the increased noise. The loss-based measures (PG, GPG, SPG, TG) also tend to move towards higher n, although more slowly and with less certainty. Unlike the complexity gains, they tend to initially favour the lower-n tasks, which may be desirable as we would expect early learning to be more efficient with simpler data.</p>
<h3>4.2. Repeat Copy</h3>
<p>In the repeat copy task (Graves et al., 2014) the network receives an input sequence of random bit vectors, and is then asked to output that sequence a given number of times. The task has two main dimensions of difficulty: the length of the input sequence and the required number of repeats, both of which increase the demand on the models memory. Neural Turing machines are able to learn a 'for-loop' like algorithm on simple examples that can directly generalise to much harder examples (Graves et al., 2014). For LSTM networks without access to external memory, however, sig-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Target task loss (per output), policy entropy and network complexity for the repeat copy task, truncated at $1.1 \times 10^{9}$ steps. Curves are averages over 10 runs, shaded areas show the standard deviation. Network complexity was computed by multiplying the per-sample complexity cost by the total size of the training set.
nificant retraining is required to adapt to harder tasks.
We devised a curriculum with both the sequence length and the number of repeats varying from 1 to 13, giving 169 tasks in all, with length 13, repeats 13 defined as the target task. The LSTM network had a single layer of 512 cells, and the batch size was 32. As the data was generated online, the number of samples $S$ in Eq. (3) (the per-sample VI loss) was undefined; we arbitrarily set it to 169 M ( 1 M per task in the curriculum).</p>
<p>Fig. 2 shows that GVCG solves the target task about twice as fast as uniform sampling for VI training, and that the PG, SPG and TPG gains are somewhat faster than uniform for ML training, especially in the early stages. From the entropy plots it is clear that these signals all lead to strongly non-uniform policies. The VI complexity curves also demonstrate that GVCG yields significantly higher network complexity than uniform sampling, supporting our hypothesis that increased complexity correlates with learning progress. Unlike GVCG, the VCG signal did not deviate far from a uniform policy,. L2G and particularly GPG
and GL2G were much worse than uniform, suggesting that (1) the bias induced by the gradient approximation has a pernicious effect on learning and (2) that the increase in L2 norm is not a reliable measure of increased network complexity. Training directly on the target task failed to learn at all, underlining the necessity of curriculum learning for this problem.</p>
<p>Fig. 3 reveals a consistent strategy for the GVCG syllabuses, first focusing on short sequences with high repeats, then long sequences with low repeats, thereby decoupling the two dimensions of difficulty. At each stage the loss is substantially reduced across many tasks that the policy does not focus on. Crucially, this means that the network does not have to visit each of the 169 tasks to solve them all, and the syllabus is able to exploit this fact to more efficiently complete the curriculum.</p>
<h3>4.3. Babi</h3>
<p>The bAbI dataset (Weston et al., 2015) consists of 20 synthetic question-answering problems designed to probe the basic reasoning capabilities of machine learning models. Although bAbI was not specifically designed for curriculum learning, some of the tasks follow a natural ordering of complexity (e.g. 'Two Arg Relations', 'Three Arg Relations') and all are based on a consistent probabilistic grammar, leading us to hope that an efficient syllabus could be found for learning the whole set. The usual performance measure for bAbI is the number of tasks 'completed' by the model, where completion is defined as getting less than $5 \%$ of the test set questions wrong.</p>
<p>The data representation followed (Graves et al., 2016), with each word in the observation and target sequences represented as a 1-hot vector, along with an extra binary channel to mark answer prompts. The original datasets were small, with either 1 K or 10 K questions per task, so as to test generalisation from limited samples. However LSTM is known to perform poorly in this setting (Sukhbaatar et al., 2015; Graves et al., 2016), and we wished to avoid the confounding effect of overfitting on curriculum learning. We therefore used the bAbI code (Weston et al., 2015) to generate 1 M stories (each containing one or more questions) for each of the 20 tasks. With so many examples, we found that training and evaluation set performance were indistinguishable, and therefore report training performance only. The LSTM network had two layer of 512 cells, the batch size was 16, and the RMSProp learning rate was $3 \times 10^{-5}$.</p>
<p>Fig. 4 shows that prediction gain (PG) clearly improved on uniform sampling in terms of both learning speed and number of tasks completed; for self-prediction gain (SPG) the same benefits were visible, though less pronounced. The other gains were either roughly equal to or worse than uniform. For variational inference training, GVCG was faster</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Average policy and loss per output over time for GVCG networks on the repeat copy task. Plots were made by dividing the first $4 \times 10^{8}$ steps into five equal bins, then averaging over the policies of all 10 networks over all times within each bin.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Completion and entropy curves for the bAbI curriculum, truncated at $3.5 \times 10^{8}$ steps. Curves are means over ten runs, shaded areas show standard deviation.
than uniform at first, then slightly worse later on, while VCG performed poorly for reasons that are unclear to us. In general, training with variational inference appeared to hamper progress on the bAbI tasks.</p>
<p>Fig. 5 shows how the PG and GVCG syllabuses accelerate the network's progress by selectively focusing on specific tasks until completion. For example, they both solve 'Time Reasoning' much faster than uniform sampling by concentrating on it early in training; similarly, PG focuses strongly on 'Path Finding' (one of the harder bAbI tasks) until it solves it. Also noteworthy is the way the syllabuses progress from 'Single Supporting Fact' to 'Three Supporting Facts' in order; this shows that our gain signals can discover implicit orderings, and hence opportunities for efficient transfer, in an unsorted curriculum.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Per-task policy and error curves for bAbI, truncated at $2 \times 10^{8}$ steps. All plots are averaged over 10 runs. Black dashed lines show the 5\% error threshold for task completion.</p>
<h2>5. Conclusion</h2>
<p>Our experiments suggest that using a stochastic syllabus to maximise learning progress can lead to significant gains in curriculum learning efficiency, so long as a suitable progress signal is used. We note however that uniformly sampling from all tasks is a surprisingly strong benchmark. We speculate that this is because learning is dominated by gradients from the tasks on which the network is making</p>
<p>fastest progress, inducing a kind of implicit curriculum, albeit with the inefficiency of unnecessary samples. For maximum likelihood training, we found prediction gain to be the most consistent signal, while for variational inference training, gradient variational complexity gain performed best. Importantly, both are instantaneous, in the sense that they can be evaluated using only the samples used for training. As well as being more efficient, this has broader applicability to tasks where external evaluation is difficult, and suggests that learning progress is best assessed on a local, rather than global basis.</p>
<h2>References</h2>
<p>Arnold, R. and Bell, T. (1997). A corpus for the evaluation of lossless compression algorithms. In Data Compression Conference, 1997. DCC'97. Proceedings, pages 201-210. IEEE.</p>
<p>Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R. E. (2002). The nonstochastic multiarmed bandit problem. SIAM Journal on Computing, 32(1):48-77.</p>
<p>Barto, A. G. (2013). Intrinsic motivation and reinforcement learning. In Intrinsically Motivated Learning in Natural and Artificial Systems, pages 17-47. Springer.</p>
<p>Bellemare, M. G., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and Munos, R. (2016). Unifying countbased exploration and intrinsic motivation. In Advances in Neural Information Processing Systems.</p>
<p>Bengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009). Curriculum learning. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML '09, pages 41-48, New York, NY, USA. ACM.</p>
<p>Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D. (2015). Weight uncertainty in neural networks. In Proceedings of The 32nd International Conference on Machine Learning, pages 1613-1622.</p>
<p>Bubeck, S. and Cesa-Bianchi, N. (2012). Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Machine Learning, 5(1):1-122.</p>
<p>Elman, J. L. (1993). Learning and development in neural networks: The importance of starting small. Cognition, 48(1):71-99.</p>
<p>Graves, A. (2011). Practical variational inference for neural networks. In Shawe-Taylor, J., Zemel, R. S., Bartlett, P. L., Pereira, F., and Weinberger, K. Q., editors, Advances in Neural Information Processing Systems 24, pages 2348-2356. Curran Associates, Inc.</p>
<p>Graves, A. (2013). Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850.</p>
<p>Graves, A., Wayne, G., and Danihelka, I. (2014). Neural turing machines. arXiv preprint arXiv:1410.5401.</p>
<p>Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-Barwińska, A., Colmenarejo, S. G., Grefenstette, E., Ramalho, T., Agapiou, J., et al. (2016). Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626):471-476.</p>
<p>Grünwald, P. D. (2007). The minimum description length principle. The MIT Press.</p>
<p>Herbster, M. and Warmuth, M. K. (1998). Tracking the best expert. Machine Learning, 32(2):151-178.</p>
<p>Hinton, G. E. and Van Camp, D. (1993). Keeping the neural networks simple by minimizing the description length of the weights. In Proceedings of the sixth annual conference on Computational learning theory, pages 5-13. ACM.</p>
<p>Houthooft, R., Chen, X., Duan, Y., Schulman, J., De Turck, F., and Abbeel, P. (2016). Vime: Variational information maximizing exploration. In Advances In Neural Information Processing Systems, pages 1109-1117.</p>
<p>Itti, L. and Baldi, P. (2009). Bayesian surprise attracts human attention. Vision research, 49(10):1295-1306.</p>
<p>Kingma, D. P., Salimans, T., and Welling, M. (2015). Variational dropout and the local reparameterization trick. In Advances in Neural Information Processing Systems, pages 2575-2583.</p>
<p>Kingma, D. P. and Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114.</p>
<p>Kneser, R. and Ney, H. (1995). Improved backing-off for m-gram language modeling. In IEEE International Conference on Acoustics, Speech, and Signal Processing, pages 181-184, Detroit, Michigan, USA.</p>
<p>Lopes, M. and Oudeyer, P.-Y. (2012). The strategic student approach for life-long exploration and learning. In IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL).</p>
<p>Oudeyer, P., Kaplan, F., and Hafner, V. (2007). Intrinsic motivation systems for autonomous mental development. IEEE Transactions on Evolutionary Computation, 11(2):265-286.</p>
<p>Reed, S. and de Freitas, N. (2015). Neural programmerinterpreters. arXiv preprint arXiv:1511.06279.</p>
<p>Rissanen, J. (1986). Stochastic complexity and modeling. Ann. Statist., 14(3):1080-1100.</p>
<p>Roy, N. and Mccallum, A. (2001). Toward optimal active learning through sampling estimation of error reduction. In In Proc. 18th International Conf. on Machine Learning.</p>
<p>Schmidhuber, J. (1991). A possibility for implementing curiosity and boredom in model-building neural controllers. In From animals to animats: proceedings of the first international conference on simulation of adaptive behavior.</p>
<p>Settles, B. (2010). Active learning literature survey. University of Wisconsin, Madison, 52(55-66):11.</p>
<p>Settles, B., Craven, M., and Ray, S. (2008). Multipleinstance active learning. In Advances in neural information processing systems, pages 1289-1296.</p>
<p>Storck, J., Hochreiter, J., and Schmidhuber, J. (1995). Reinforcement driven information acquisition in nondeterministic environments. In Proceedings of the International Conference on Artificial Neural Networks, vol. 2 .</p>
<p>Sukhbaatar, S., Weston, J., Fergus, R., et al. (2015). Endto-end memory networks. In Advances in neural information processing systems, pages 2440-2448.</p>
<p>Sutskever, I. and Zaremba, W. (2014). Learning to execute. arXiv preprint arXiv:1410.4615.</p>
<p>Tieleman, T., H. G. (2012). Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning.</p>
<p>Tsvetkov, Yulia, F. M. L. W. M. B. D. C. (2016). Learning the curriculum with bayesian optimization for taskspecific word representation learning. arXiv preprint arXiv:1605.03852.</p>
<p>Weston, J., Bordes, A., Chopra, S., and Mikolov, T. (2015). Towards ai-complete question answering: A set of prerequisite toy tasks. $C o R R$, abs/1502.05698.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ MDL deals with sets rather than distributions; in this context&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>we consider each $\mathcal{D}_{k}$ in the curriculum to be a dataset sampled from the task distribution, rather than the distribution itself
${ }^{2}$ The reparameterisation trick yields a better gradient estimator for the posterior variance than that used in (Graves, 2011), which requires either calculation of the diagonal of the Hessian, or a biased approximation using the empirical Fisher. The gradient estimator for the posterior mean is the same in both cases.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
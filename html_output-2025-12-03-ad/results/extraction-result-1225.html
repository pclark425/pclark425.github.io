<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1225 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1225</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1225</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-257767212</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2303.14889v2.pdf" target="_blank">Model-Based Reinforcement Learning with Isolated Imaginations</a></p>
                <p><strong>Paper Abstract:</strong> World models learn the consequences of actions in vision-based interactive systems. However, in practical scenarios like autonomous driving, noncontrollable dynamics that are independent or sparsely dependent on action signals often exist, making it challenging to learn effective world models. To address this issue, we propose Iso-Dream++, a model-based reinforcement learning approach that has two main contributions. First, we optimize the inverse dynamics to encourage the world model to isolate controllable state transitions from the mixed spatiotemporal variations of the environment. Second, we perform policy optimization based on the decoupled latent imaginations, where we roll out noncontrollable states into the future and adaptively associate them with the current controllable state. This enables long-horizon visuomotor control tasks to benefit from isolating mixed dynamics sources in the wild, such as self-driving cars that can anticipate the movement of other vehicles, thereby avoiding potential risks. On top of our previous work, we further consider the sparse dependencies between controllable and noncontrollable states, address the training collapse problem of state decoupling, and validate our approach in transfer learning setups. Our empirical study demonstrates that Iso-Dream++ outperforms existing reinforcement learning models significantly on CARLA and DeepMind Control.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1225.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1225.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Iso-Dream++</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iso-Dream++ (Model-Based Reinforcement Learning with Isolated Imaginations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A three-branch latent world model that explicitly disentangles controllable, noncontrollable, and static components of visual dynamics; uses inverse dynamics, min-max variance constraints, a dependency gate for sparse cross-branch effects, and attention-based latent imaginations of noncontrollable futures to train an actor-critic policy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Iso-Dream++</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A latent recurrent state-space world model (RSSM-style) with three branches: an action-conditioned (controllable) stochastic+deterministic GRU branch, an action-free (noncontrollable) stochastic+deterministic GRU branch, and a static branch (encoder+decoder). Posterior inference uses a shared encoder plus branch-specific encoders; training includes reconstruction/image log loss, reward prediction, KL terms, an inverse-dynamics regression MLP, and min-max variance constraints computed from hypothetical action batches. At decision time the action-free branch is rolled out several steps ahead to produce z_{t:t+τ}, which are integrated with s_t via an attention network to form a 'visionary' latent e_t for the policy and value networks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (RSSM-style modular latent model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>vision-based continuous control; evaluated on autonomous driving (CARLA) and vision control tasks (DeepMind Control Suite with dynamic video backgrounds)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>image log-likelihood / reconstruction loss, KL divergences between posterior and prior (for s and z branches), reward log loss; inverse-dynamics L2 regression loss; empirical variance of imagined latent states (used as an information proxy / mutual information approximation).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No single scalar fidelity number reported; world-model training objective uses image reconstruction loss + KLs + reward/dynamics losses. Empirically, video-prediction qualitatively matches ground truth and decoupled visual components produce interpretable masks; task performance (proxy for usable fidelity) — CARLA: average return ≈ 60 after 500k steps (paper), DMC tasks (video_easy): Finger Spin 938±51, Hopper Stand 877±34, Walker Walk 932±37, Cheetah Run 639±19.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Moderately interpretable: model produces per-branch reconstructed RGB components and spatial masks (M^s, M^z) that localize controllable vs noncontrollable pixels (e.g., hot spots over other vehicles); attention weights over future noncontrollable latents are visualizable and used to inspect which future steps influence decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of branch reconstructions and spatial masks; qualitative video-prediction comparisons; attention-map inspection for future-noncontrollable influence.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported as wall-clock or parameter counts. Architecturally more efficient than MCTS approaches (e.g., MuZero) because it uses latent actor-critic imagination rather than Monte-Carlo tree search; performs rollouts of latent z branch and latent s branch within RSSM, enabling many parallel imagined trajectories similar to Dreamer-style methods.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Claimed more efficient than MCTS-based planners (Stochastic MuZero) because no MCTS; comparable to DreamerV2 in computational paradigm (latent parallel imaginations) but with extra cost from maintaining two transition branches and computing min-max variance constraints and inverse dynamics during training. No quantitative FLOPs/throughput comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Significantly better than baselines on tested tasks: CARLA driving (avg return ~60 vs DreamerV2 ~10 and Denoised-MDP ~25 after 500k steps); DMC (video_easy) outperforming DreamerV2/Pro, DBC and Denoised-MDP on reported tasks (see fidelity_performance). Robustness experiments show improved generalization to video_hard and noise (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High task utility: disentangled dynamics enable proactive (forward-looking) decision-making by previewing noncontrollable futures; rolling out z_{t+1:t+τ} and integrating via attention improves long-horizon visuomotor control and safety (e.g., anticipating other vehicles in CARLA). Selective transfer of branches (action-free or action-conditioned) accelerates fine-tuning across domain shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-offs noted: longer rollouts of noncontrollable states (larger τ) can increase model error although τ≈5 is near-optimal; inverse-dynamics objective alone can cause training collapse (action-conditioned branch capturing all dynamics) requiring min-max variance constraints; modeling sparse dependency is helpful in domains where agent affects others (CARLA) but unnecessary or removed for some DMC setups.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Three-branch modular design (controllable s, noncontrollable z, static b); inverse-dynamics loss to enforce controllability in s; min-max variance constraints (maximize variance in s for different hypothetical actions; minimize variance in z across actions); dependency gate to capture sparse s→z dependencies; future-state attention to weigh multi-step z rollouts when forming policy inputs; rollouts of z branch during both training and deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to DreamerV2/V3 and DreamerPro and Denoised-MDP in experiments: Iso-Dream++ yields higher task returns and more robust generalization to dynamic visual distractions. Versus MCTS/MuZero-style planners: Iso-Dream++ trades some planning thoroughness for far lower computational cost by using latent imaginations and actor-critic instead of MCTS.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends using inverse dynamics + min-max variance constraints + future-state attention + rollouts of z with τ≈5 for CARLA; sparse dependency modeling should be enabled in domains with potential agent→environment interactions (e.g., driving) and can be disabled where irrelevant (e.g., some DMC video-background tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Model-Based Reinforcement Learning with Isolated Imaginations', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1225.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1225.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DreamerV2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DreamerV2 (latent-imagination model-based RL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A latent world-model-based RL algorithm (RSSM latent dynamics) that learns a differentiable simulator from pixels and optimizes actor-critic policies by imagining trajectories in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DreamerV2 (RSSM-based latent world model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RSSM-style recurrent stochastic latent dynamics learned with reconstruction objectives; actor and value networks trained on imagined latent rollouts (latent imagination) to optimize λ-returns. Uses a single (non-disentangled) latent state combining all dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>general vision-based control (used as baseline here on CARLA and DMC)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>image reconstruction loss and latent predictive accuracy (KL regularization); task return as downstream performance proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported as baseline task returns in this paper: CARLA after 500k steps ~10 average return; DMC tasks reported in the paper tables (examples appear in Table entries). Exact image-prediction fidelity numbers not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Relatively low interpretability in this paper: latent encodes mixed controllable and noncontrollable dynamics so disentanglement not explicit; no per-branch visual masks.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned for disentanglement in this paper context; treated as a black-box latent simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Enables thousands of parallel imagined trajectories (efficient for policy learning); computational cost comparable to other latent-model methods, cheaper than MCTS planners. No concrete hardware/time numbers given.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>More efficient than MCTS (Stochastic MuZero) because no tree search; baseline used to compare sample efficiency and returns — Iso-Dream++ outperforms it in sample efficiency and task returns on evaluated tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Baseline task performance reported lower than Iso-Dream++ in CARLA and some DMC settings (e.g., CARLA ~10 vs Iso-Dream++ ~60 after 500k steps).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Latent imagination provides computationally efficient planning; however combining all dynamics in one latent may reduce long-horizon decision quality in environments with noncontrollable dynamics (paper argument).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Single-branch latent trades simplicity and efficiency for reduced capacity to separate action-dependent vs action-independent dynamics, harming forward-looking decisions when noncontrollable dynamics matter.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Single RSSM branch; reconstruction-based training; uses latent imagination for actor-critic updates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared unfavorably on tasks with strong noncontrollable dynamics to Iso-Dream++; DreamerPro and DreamerV3 are improvements but Iso-Dream++ adds explicit disentanglement and future-noncontrollable rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper does not prescribe modifications but uses DreamerV2 as baseline; suggests that disentangling dynamics (as in Iso-Dream++) can improve performance in tasks with external noncontrollable dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Model-Based Reinforcement Learning with Isolated Imaginations', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1225.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1225.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DreamerPro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DreamerPro (reconstruction-free Dreamer with prototypical representations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of Dreamer that avoids pixel reconstruction by learning prototypical latent representations online, improving robustness to visual distractions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DreamerPro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Model-based RL that replaces pixel reconstruction objectives with prototype-based representation learning while retaining latent dynamics and imagination-based actor-critic learning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (prototype-augmented)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>vision-based RL with visual distractions (benchmarked on DMC and CARLA in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Not explicitly quantified here; uses representation clustering/prototype objectives rather than reconstruction loss; downstream task return used for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Benchmark task returns reported (lower than Iso-Dream++ on CARLA and DMC in this paper); specific numbers appear in comparative tables (e.g., DMC baseline entries).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Prototype representations increase robustness but do not provide explicit disentanglement into controllable/noncontrollable components; interpretability limited to cluster/prototype inspection (not discussed here).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Prototype clustering/online k-means style representations (paper referenced), but no visualization specifics in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Similar to Dreamer-style latent methods; avoids heavy reconstruction decoders which can reduce some compute, but extra clustering/prototype updates impose overhead; no concrete costs provided.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Reported less robust than Iso-Dream++ on dynamic-background tests despite being reconstruction-free; Iso-Dream++ outperforms DreamerPro on robustness benchmarks in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Outperformed by Iso-Dream++ on CARLA and noisy DMC tasks according to tables in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Prototype-based representation improves robustness to some distractions but lacks the explicit dynamic disentangling and future-noncontrollable imagination that provide Iso-Dream++ task advantages.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Removes reconstruction burden (improves robustness) but sacrifices the explicit modeling of controllable vs noncontrollable dynamics used by Iso-Dream++ for downstream planning improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Reconstruction-free prototypical representations integrated into Dreamer framework.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Per paper, Iso-Dream++ yields better performance in dynamic/distractioned environments where separating dynamics (action-free branch) and previewing them aids decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Model-Based Reinforcement Learning with Isolated Imaginations', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1225.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1225.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Denoised-MDP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Denoised-MDP (Learning world models better than the world itself)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that categorizes information into types by controllability and reward-relevance, then focuses the policy on information both controllable and reward-relevant to improve robustness to distractions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Denoised-MDP</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learns to filter observation information by controllability and reward relevance; integrates this filtering into world-model-based RL to focus policy learning on task-relevant controllable dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent/world-model filtering approach</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>vision-based RL with noisy distractions (benchmarked against Iso-Dream++ in CARLA and DMC)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Task return and robustness under visual distractions used as evaluation; fidelity in terms of filtering accuracy not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>In CARLA baseline reported as average return ≈25 after 500k steps (paper) — lower than Iso-Dream++ (~60). In DMC tasks reported lower than Iso-Dream++ on noisy backgrounds.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Provides a semantic categorization (controllable vs not, reward-relevant vs not) but does not output explicit per-pixel branch masks as in Iso-Dream++.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Conceptual categorization and model components that prioritize controllable & reward-relevant signals; no mask visualizations in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Comparable to other robust representation methods; no concrete compute numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Less effective than Iso-Dream++ at leveraging noncontrollable predictions for forward-looking behavior in CARLA according to reported returns.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Underperforms Iso-Dream++ on CARLA and the tested DMC noisy benchmarks per the paper's reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Useful when the best strategy is to ignore noncontrollable distractions; however, when anticipating external dynamics yields safety benefits (CARLA), Iso-Dream++'s explicit rollout of noncontrollable futures provides superior utility.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Filtering out noncontrollable dynamics can improve robustness but prevents future-conditioned decisions that could exploit predictable external dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Filter-based learning of task-relevant controllable information; omits explicit modeling/rollout of action-free futures.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Iso-Dream++ extends beyond filtering by modeling the noncontrollable branch and using rollout+attention to inform policy, yielding better performance where prediction of external agents matters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Model-Based Reinforcement Learning with Isolated Imaginations', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1225.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1225.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PlaNet / RSSM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PlaNet (Recurrent State Space Model / RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RSSM latent dynamics model that uses recurrent deterministic hidden states plus stochastic latent variables to model environment dynamics from pixel observations for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RSSM (PlaNet-style recurrent state-space model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Recurrent deterministic hidden state (GRU) combined with a stochastic latent state per timestep; used to parameterize priors and posteriors for latent transitions and emission models (decoders) for images and rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent recurrent world model (RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>vision-based control; foundational architecture for Dreamer-family and used in Iso-Dream++ action-conditioned branch</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction/image log-likelihood, KL between posterior and prior of stochastic latents; predictive accuracy of latent transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not separately quantified in this paper — used as architectural building block. PlaNet/Dreamer-style models provide effective latent imagination enabling competitive task returns historically.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent states are not inherently disentangled; interpretability depends on downstream analyses but in baseline form they are black-box latents.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None specific in this paper aside from PlaNet being the basis for the action-conditioned branch; Iso-Dream++ augments it with disentanglement and mask visualizations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Reasonable for latent-model methods; supports many parallel imagined trajectories. Specific resource numbers not given.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Efficient relative to pixel-based planning/MCTS; forms the backbone for Dreamer and Iso-Dream++ which exploit latent parallel imaginations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>As an architectural primitive, contributes to Dreamer-style performance; Iso-Dream++ builds on it to improve performance in mixed-dynamics scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>RSSM provides compact latent dynamics sufficient for policy learning; without disentanglement it can mix controllable and noncontrollable dynamics, reducing utility in some domains.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Compact latent dynamics reduce compute but risk conflating different dynamic sources; Iso-Dream++ addresses this via branching.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use of GRU-based deterministic hidden state plus stochastic latent variables; posterior conditioned on current observation and prior conditioned on previous latent and action.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>PlaNet/RSSM is the canonical latent dynamics choice used by Dreamer family; Iso-Dream++ retains RSSM concept but splits it into modular branches for different dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Model-Based Reinforcement Learning with Isolated Imaginations', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1225.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1225.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Stochastic MuZero</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stochastic MuZero (MuZero family with stochastic latent dynamics and MCTS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model that learns a dynamics model and uses Monte-Carlo Tree Search (MCTS) over learned latent states to plan, improving long-term decision-making at the expense of high computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Stochastic MuZero (MCTS over learned model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learns a latent dynamics and value/policy predictors and uses MCTS at decision time to search over action trajectories and estimated outcomes; typically computationally heavy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>hybrid (learned model + search/MCTS)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>long-horizon planning and control (general RL), referenced as comparison in related work</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Improved planning fidelity via explicit tree search; metrics typically include planning value accuracy and final task return (not provided numerically in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not evaluated experimentally in this paper; mentioned qualitatively as effective for long-term planning but computationally expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>MCTS provides some interpretability in the form of searchable action trajectories and visit statistics, but underlying learned dynamics remain neural and less transparent.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Tree search traces, visit counts; not elaborated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>High computational cost due to MCTS; paper explicitly contrasts Iso-Dream++ as lower-cost because it avoids MCTS and uses actor-critic imaginations.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Stochastic MuZero is more computationally expensive (MCTS) compared to Iso-Dream++ which performs actor-critic latent imaginations without tree search.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not directly compared in experiments within this paper; noted as powerful for long-horizon tasks but impractical for fast-decision settings like short-horizon driving due to cost.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High utility for tasks where heavy planning pays off; less suitable for time-constrained control where lower-latency latent imaginations suffice.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Planner improves long-horizon accuracy at significant computational expense; Iso-Dream++ chooses a cheaper actor-critic imagination tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Combines learned model with MCTS; stochastic variants add probabilistic latent transitions to support uncertainty-aware search.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Contrasted in paper: Iso-Dream++ avoids MCTS to remain practical and efficient for short-term control tasks such as autonomous driving.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Model-Based Reinforcement Learning with Isolated Imaginations', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1225.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1225.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>World Models (Ha & Schmidhuber)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>World Models (Ha & Schmidhuber, 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Early approach to learn a compressed latent visual model and then evolve policies/controllers in that latent space; demonstrates benefits of model-based learning from pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>World Models (latent compressed simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learn a variational/compressed latent representation of video observations and a dynamics model in latent space; then use that world model for policy evolution or planning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>visual RL and policy evolution in simulated environments (historical reference in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction/prediction fidelity in latent-video prediction; used qualitatively as foundational inspiration.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not quantified in this paper; cited as influential prior work demonstrating latent world-model benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Provides some interpretability via latent-space visualizations but not focused on explicit controllable vs noncontrollable disentanglement.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of decoded latent rollouts; not elaborated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Relative to modern RSSM-based methods, earlier world models were simpler; computational details not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Provided historical context; Iso-Dream++ builds on the broader world-model idea but emphasizes disentanglement and future-conditioned behavior learning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Historical demonstrations show viability, but no direct comparisons in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Motivates latent-model approaches; Iso-Dream++ extends the idea to isolate noncontrollable dynamics for improved task utility in specific domains.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Early world models trade fidelity of pixel reconstructions for compact latents; Iso-Dream++ focuses on modularity and downstream decision utility.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Compress observations into latent codes and learn dynamics; later works (including this paper) refine architectures and objectives for disentanglement.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Serves as conceptual predecessor to PlaNet/Dreamer family; Iso-Dream++ differs by explicit modular disentanglement and attention-based use of predicted noncontrollable futures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Model-Based Reinforcement Learning with Isolated Imaginations', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Dream to control: Learning behaviors by latent imagination <em>(Rating: 2)</em></li>
                <li>Mastering diverse domains through world models <em>(Rating: 2)</em></li>
                <li>Iso-dream: Isolating and leveraging noncontrollable visual dynamics in world models <em>(Rating: 2)</em></li>
                <li>Learning latent dynamics for planning from pixels <em>(Rating: 2)</em></li>
                <li>Denoised-MDP: Learning world models better than the world itself <em>(Rating: 2)</em></li>
                <li>Recurrent world models facilitate policy evolution <em>(Rating: 1)</em></li>
                <li>Stochastic MuZero <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1225",
    "paper_id": "paper-257767212",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "Iso-Dream++",
            "name_full": "Iso-Dream++ (Model-Based Reinforcement Learning with Isolated Imaginations)",
            "brief_description": "A three-branch latent world model that explicitly disentangles controllable, noncontrollable, and static components of visual dynamics; uses inverse dynamics, min-max variance constraints, a dependency gate for sparse cross-branch effects, and attention-based latent imaginations of noncontrollable futures to train an actor-critic policy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Iso-Dream++",
            "model_description": "A latent recurrent state-space world model (RSSM-style) with three branches: an action-conditioned (controllable) stochastic+deterministic GRU branch, an action-free (noncontrollable) stochastic+deterministic GRU branch, and a static branch (encoder+decoder). Posterior inference uses a shared encoder plus branch-specific encoders; training includes reconstruction/image log loss, reward prediction, KL terms, an inverse-dynamics regression MLP, and min-max variance constraints computed from hypothetical action batches. At decision time the action-free branch is rolled out several steps ahead to produce z_{t:t+τ}, which are integrated with s_t via an attention network to form a 'visionary' latent e_t for the policy and value networks.",
            "model_type": "latent world model (RSSM-style modular latent model)",
            "task_domain": "vision-based continuous control; evaluated on autonomous driving (CARLA) and vision control tasks (DeepMind Control Suite with dynamic video backgrounds)",
            "fidelity_metric": "image log-likelihood / reconstruction loss, KL divergences between posterior and prior (for s and z branches), reward log loss; inverse-dynamics L2 regression loss; empirical variance of imagined latent states (used as an information proxy / mutual information approximation).",
            "fidelity_performance": "No single scalar fidelity number reported; world-model training objective uses image reconstruction loss + KLs + reward/dynamics losses. Empirically, video-prediction qualitatively matches ground truth and decoupled visual components produce interpretable masks; task performance (proxy for usable fidelity) — CARLA: average return ≈ 60 after 500k steps (paper), DMC tasks (video_easy): Finger Spin 938±51, Hopper Stand 877±34, Walker Walk 932±37, Cheetah Run 639±19.",
            "interpretability_assessment": "Moderately interpretable: model produces per-branch reconstructed RGB components and spatial masks (M^s, M^z) that localize controllable vs noncontrollable pixels (e.g., hot spots over other vehicles); attention weights over future noncontrollable latents are visualizable and used to inspect which future steps influence decisions.",
            "interpretability_method": "Visualization of branch reconstructions and spatial masks; qualitative video-prediction comparisons; attention-map inspection for future-noncontrollable influence.",
            "computational_cost": "Not reported as wall-clock or parameter counts. Architecturally more efficient than MCTS approaches (e.g., MuZero) because it uses latent actor-critic imagination rather than Monte-Carlo tree search; performs rollouts of latent z branch and latent s branch within RSSM, enabling many parallel imagined trajectories similar to Dreamer-style methods.",
            "efficiency_comparison": "Claimed more efficient than MCTS-based planners (Stochastic MuZero) because no MCTS; comparable to DreamerV2 in computational paradigm (latent parallel imaginations) but with extra cost from maintaining two transition branches and computing min-max variance constraints and inverse dynamics during training. No quantitative FLOPs/throughput comparison provided.",
            "task_performance": "Significantly better than baselines on tested tasks: CARLA driving (avg return ~60 vs DreamerV2 ~10 and Denoised-MDP ~25 after 500k steps); DMC (video_easy) outperforming DreamerV2/Pro, DBC and Denoised-MDP on reported tasks (see fidelity_performance). Robustness experiments show improved generalization to video_hard and noise (Table 2).",
            "task_utility_analysis": "High task utility: disentangled dynamics enable proactive (forward-looking) decision-making by previewing noncontrollable futures; rolling out z_{t+1:t+τ} and integrating via attention improves long-horizon visuomotor control and safety (e.g., anticipating other vehicles in CARLA). Selective transfer of branches (action-free or action-conditioned) accelerates fine-tuning across domain shifts.",
            "tradeoffs_observed": "Trade-offs noted: longer rollouts of noncontrollable states (larger τ) can increase model error although τ≈5 is near-optimal; inverse-dynamics objective alone can cause training collapse (action-conditioned branch capturing all dynamics) requiring min-max variance constraints; modeling sparse dependency is helpful in domains where agent affects others (CARLA) but unnecessary or removed for some DMC setups.",
            "design_choices": "Three-branch modular design (controllable s, noncontrollable z, static b); inverse-dynamics loss to enforce controllability in s; min-max variance constraints (maximize variance in s for different hypothetical actions; minimize variance in z across actions); dependency gate to capture sparse s→z dependencies; future-state attention to weigh multi-step z rollouts when forming policy inputs; rollouts of z branch during both training and deployment.",
            "comparison_to_alternatives": "Compared to DreamerV2/V3 and DreamerPro and Denoised-MDP in experiments: Iso-Dream++ yields higher task returns and more robust generalization to dynamic visual distractions. Versus MCTS/MuZero-style planners: Iso-Dream++ trades some planning thoroughness for far lower computational cost by using latent imaginations and actor-critic instead of MCTS.",
            "optimal_configuration": "Paper recommends using inverse dynamics + min-max variance constraints + future-state attention + rollouts of z with τ≈5 for CARLA; sparse dependency modeling should be enabled in domains with potential agent→environment interactions (e.g., driving) and can be disabled where irrelevant (e.g., some DMC video-background tasks).",
            "uuid": "e1225.0",
            "source_info": {
                "paper_title": "Model-Based Reinforcement Learning with Isolated Imaginations",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "DreamerV2",
            "name_full": "DreamerV2 (latent-imagination model-based RL)",
            "brief_description": "A latent world-model-based RL algorithm (RSSM latent dynamics) that learns a differentiable simulator from pixels and optimizes actor-critic policies by imagining trajectories in latent space.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DreamerV2 (RSSM-based latent world model)",
            "model_description": "RSSM-style recurrent stochastic latent dynamics learned with reconstruction objectives; actor and value networks trained on imagined latent rollouts (latent imagination) to optimize λ-returns. Uses a single (non-disentangled) latent state combining all dynamics.",
            "model_type": "latent world model (RSSM)",
            "task_domain": "general vision-based control (used as baseline here on CARLA and DMC)",
            "fidelity_metric": "image reconstruction loss and latent predictive accuracy (KL regularization); task return as downstream performance proxy.",
            "fidelity_performance": "Reported as baseline task returns in this paper: CARLA after 500k steps ~10 average return; DMC tasks reported in the paper tables (examples appear in Table entries). Exact image-prediction fidelity numbers not reported here.",
            "interpretability_assessment": "Relatively low interpretability in this paper: latent encodes mixed controllable and noncontrollable dynamics so disentanglement not explicit; no per-branch visual masks.",
            "interpretability_method": "None mentioned for disentanglement in this paper context; treated as a black-box latent simulator.",
            "computational_cost": "Enables thousands of parallel imagined trajectories (efficient for policy learning); computational cost comparable to other latent-model methods, cheaper than MCTS planners. No concrete hardware/time numbers given.",
            "efficiency_comparison": "More efficient than MCTS (Stochastic MuZero) because no tree search; baseline used to compare sample efficiency and returns — Iso-Dream++ outperforms it in sample efficiency and task returns on evaluated tasks.",
            "task_performance": "Baseline task performance reported lower than Iso-Dream++ in CARLA and some DMC settings (e.g., CARLA ~10 vs Iso-Dream++ ~60 after 500k steps).",
            "task_utility_analysis": "Latent imagination provides computationally efficient planning; however combining all dynamics in one latent may reduce long-horizon decision quality in environments with noncontrollable dynamics (paper argument).",
            "tradeoffs_observed": "Single-branch latent trades simplicity and efficiency for reduced capacity to separate action-dependent vs action-independent dynamics, harming forward-looking decisions when noncontrollable dynamics matter.",
            "design_choices": "Single RSSM branch; reconstruction-based training; uses latent imagination for actor-critic updates.",
            "comparison_to_alternatives": "Compared unfavorably on tasks with strong noncontrollable dynamics to Iso-Dream++; DreamerPro and DreamerV3 are improvements but Iso-Dream++ adds explicit disentanglement and future-noncontrollable rollouts.",
            "optimal_configuration": "Paper does not prescribe modifications but uses DreamerV2 as baseline; suggests that disentangling dynamics (as in Iso-Dream++) can improve performance in tasks with external noncontrollable dynamics.",
            "uuid": "e1225.1",
            "source_info": {
                "paper_title": "Model-Based Reinforcement Learning with Isolated Imaginations",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "DreamerPro",
            "name_full": "DreamerPro (reconstruction-free Dreamer with prototypical representations)",
            "brief_description": "A variant of Dreamer that avoids pixel reconstruction by learning prototypical latent representations online, improving robustness to visual distractions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DreamerPro",
            "model_description": "Model-based RL that replaces pixel reconstruction objectives with prototype-based representation learning while retaining latent dynamics and imagination-based actor-critic learning.",
            "model_type": "latent world model (prototype-augmented)",
            "task_domain": "vision-based RL with visual distractions (benchmarked on DMC and CARLA in this paper)",
            "fidelity_metric": "Not explicitly quantified here; uses representation clustering/prototype objectives rather than reconstruction loss; downstream task return used for evaluation.",
            "fidelity_performance": "Benchmark task returns reported (lower than Iso-Dream++ on CARLA and DMC in this paper); specific numbers appear in comparative tables (e.g., DMC baseline entries).",
            "interpretability_assessment": "Prototype representations increase robustness but do not provide explicit disentanglement into controllable/noncontrollable components; interpretability limited to cluster/prototype inspection (not discussed here).",
            "interpretability_method": "Prototype clustering/online k-means style representations (paper referenced), but no visualization specifics in this paper.",
            "computational_cost": "Similar to Dreamer-style latent methods; avoids heavy reconstruction decoders which can reduce some compute, but extra clustering/prototype updates impose overhead; no concrete costs provided.",
            "efficiency_comparison": "Reported less robust than Iso-Dream++ on dynamic-background tests despite being reconstruction-free; Iso-Dream++ outperforms DreamerPro on robustness benchmarks in this paper.",
            "task_performance": "Outperformed by Iso-Dream++ on CARLA and noisy DMC tasks according to tables in the paper.",
            "task_utility_analysis": "Prototype-based representation improves robustness to some distractions but lacks the explicit dynamic disentangling and future-noncontrollable imagination that provide Iso-Dream++ task advantages.",
            "tradeoffs_observed": "Removes reconstruction burden (improves robustness) but sacrifices the explicit modeling of controllable vs noncontrollable dynamics used by Iso-Dream++ for downstream planning improvements.",
            "design_choices": "Reconstruction-free prototypical representations integrated into Dreamer framework.",
            "comparison_to_alternatives": "Per paper, Iso-Dream++ yields better performance in dynamic/distractioned environments where separating dynamics (action-free branch) and previewing them aids decisions.",
            "uuid": "e1225.2",
            "source_info": {
                "paper_title": "Model-Based Reinforcement Learning with Isolated Imaginations",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Denoised-MDP",
            "name_full": "Denoised-MDP (Learning world models better than the world itself)",
            "brief_description": "A framework that categorizes information into types by controllability and reward-relevance, then focuses the policy on information both controllable and reward-relevant to improve robustness to distractions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Denoised-MDP",
            "model_description": "Learns to filter observation information by controllability and reward relevance; integrates this filtering into world-model-based RL to focus policy learning on task-relevant controllable dynamics.",
            "model_type": "latent/world-model filtering approach",
            "task_domain": "vision-based RL with noisy distractions (benchmarked against Iso-Dream++ in CARLA and DMC)",
            "fidelity_metric": "Task return and robustness under visual distractions used as evaluation; fidelity in terms of filtering accuracy not reported here.",
            "fidelity_performance": "In CARLA baseline reported as average return ≈25 after 500k steps (paper) — lower than Iso-Dream++ (~60). In DMC tasks reported lower than Iso-Dream++ on noisy backgrounds.",
            "interpretability_assessment": "Provides a semantic categorization (controllable vs not, reward-relevant vs not) but does not output explicit per-pixel branch masks as in Iso-Dream++.",
            "interpretability_method": "Conceptual categorization and model components that prioritize controllable & reward-relevant signals; no mask visualizations in this paper.",
            "computational_cost": "Comparable to other robust representation methods; no concrete compute numbers provided.",
            "efficiency_comparison": "Less effective than Iso-Dream++ at leveraging noncontrollable predictions for forward-looking behavior in CARLA according to reported returns.",
            "task_performance": "Underperforms Iso-Dream++ on CARLA and the tested DMC noisy benchmarks per the paper's reported results.",
            "task_utility_analysis": "Useful when the best strategy is to ignore noncontrollable distractions; however, when anticipating external dynamics yields safety benefits (CARLA), Iso-Dream++'s explicit rollout of noncontrollable futures provides superior utility.",
            "tradeoffs_observed": "Filtering out noncontrollable dynamics can improve robustness but prevents future-conditioned decisions that could exploit predictable external dynamics.",
            "design_choices": "Filter-based learning of task-relevant controllable information; omits explicit modeling/rollout of action-free futures.",
            "comparison_to_alternatives": "Iso-Dream++ extends beyond filtering by modeling the noncontrollable branch and using rollout+attention to inform policy, yielding better performance where prediction of external agents matters.",
            "uuid": "e1225.3",
            "source_info": {
                "paper_title": "Model-Based Reinforcement Learning with Isolated Imaginations",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "PlaNet / RSSM",
            "name_full": "PlaNet (Recurrent State Space Model / RSSM)",
            "brief_description": "An RSSM latent dynamics model that uses recurrent deterministic hidden states plus stochastic latent variables to model environment dynamics from pixel observations for planning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RSSM (PlaNet-style recurrent state-space model)",
            "model_description": "Recurrent deterministic hidden state (GRU) combined with a stochastic latent state per timestep; used to parameterize priors and posteriors for latent transitions and emission models (decoders) for images and rewards.",
            "model_type": "latent recurrent world model (RSSM)",
            "task_domain": "vision-based control; foundational architecture for Dreamer-family and used in Iso-Dream++ action-conditioned branch",
            "fidelity_metric": "Reconstruction/image log-likelihood, KL between posterior and prior of stochastic latents; predictive accuracy of latent transitions.",
            "fidelity_performance": "Not separately quantified in this paper — used as architectural building block. PlaNet/Dreamer-style models provide effective latent imagination enabling competitive task returns historically.",
            "interpretability_assessment": "Latent states are not inherently disentangled; interpretability depends on downstream analyses but in baseline form they are black-box latents.",
            "interpretability_method": "None specific in this paper aside from PlaNet being the basis for the action-conditioned branch; Iso-Dream++ augments it with disentanglement and mask visualizations.",
            "computational_cost": "Reasonable for latent-model methods; supports many parallel imagined trajectories. Specific resource numbers not given.",
            "efficiency_comparison": "Efficient relative to pixel-based planning/MCTS; forms the backbone for Dreamer and Iso-Dream++ which exploit latent parallel imaginations.",
            "task_performance": "As an architectural primitive, contributes to Dreamer-style performance; Iso-Dream++ builds on it to improve performance in mixed-dynamics scenes.",
            "task_utility_analysis": "RSSM provides compact latent dynamics sufficient for policy learning; without disentanglement it can mix controllable and noncontrollable dynamics, reducing utility in some domains.",
            "tradeoffs_observed": "Compact latent dynamics reduce compute but risk conflating different dynamic sources; Iso-Dream++ addresses this via branching.",
            "design_choices": "Use of GRU-based deterministic hidden state plus stochastic latent variables; posterior conditioned on current observation and prior conditioned on previous latent and action.",
            "comparison_to_alternatives": "PlaNet/RSSM is the canonical latent dynamics choice used by Dreamer family; Iso-Dream++ retains RSSM concept but splits it into modular branches for different dynamics.",
            "uuid": "e1225.4",
            "source_info": {
                "paper_title": "Model-Based Reinforcement Learning with Isolated Imaginations",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "Stochastic MuZero",
            "name_full": "Stochastic MuZero (MuZero family with stochastic latent dynamics and MCTS)",
            "brief_description": "A model that learns a dynamics model and uses Monte-Carlo Tree Search (MCTS) over learned latent states to plan, improving long-term decision-making at the expense of high computational cost.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Stochastic MuZero (MCTS over learned model)",
            "model_description": "Learns a latent dynamics and value/policy predictors and uses MCTS at decision time to search over action trajectories and estimated outcomes; typically computationally heavy.",
            "model_type": "hybrid (learned model + search/MCTS)",
            "task_domain": "long-horizon planning and control (general RL), referenced as comparison in related work",
            "fidelity_metric": "Improved planning fidelity via explicit tree search; metrics typically include planning value accuracy and final task return (not provided numerically in this paper).",
            "fidelity_performance": "Not evaluated experimentally in this paper; mentioned qualitatively as effective for long-term planning but computationally expensive.",
            "interpretability_assessment": "MCTS provides some interpretability in the form of searchable action trajectories and visit statistics, but underlying learned dynamics remain neural and less transparent.",
            "interpretability_method": "Tree search traces, visit counts; not elaborated in this paper.",
            "computational_cost": "High computational cost due to MCTS; paper explicitly contrasts Iso-Dream++ as lower-cost because it avoids MCTS and uses actor-critic imaginations.",
            "efficiency_comparison": "Stochastic MuZero is more computationally expensive (MCTS) compared to Iso-Dream++ which performs actor-critic latent imaginations without tree search.",
            "task_performance": "Not directly compared in experiments within this paper; noted as powerful for long-horizon tasks but impractical for fast-decision settings like short-horizon driving due to cost.",
            "task_utility_analysis": "High utility for tasks where heavy planning pays off; less suitable for time-constrained control where lower-latency latent imaginations suffice.",
            "tradeoffs_observed": "Planner improves long-horizon accuracy at significant computational expense; Iso-Dream++ chooses a cheaper actor-critic imagination tradeoff.",
            "design_choices": "Combines learned model with MCTS; stochastic variants add probabilistic latent transitions to support uncertainty-aware search.",
            "comparison_to_alternatives": "Contrasted in paper: Iso-Dream++ avoids MCTS to remain practical and efficient for short-term control tasks such as autonomous driving.",
            "uuid": "e1225.5",
            "source_info": {
                "paper_title": "Model-Based Reinforcement Learning with Isolated Imaginations",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "World Models (Ha & Schmidhuber)",
            "name_full": "World Models (Ha & Schmidhuber, 2018)",
            "brief_description": "Early approach to learn a compressed latent visual model and then evolve policies/controllers in that latent space; demonstrates benefits of model-based learning from pixels.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "World Models (latent compressed simulator)",
            "model_description": "Learn a variational/compressed latent representation of video observations and a dynamics model in latent space; then use that world model for policy evolution or planning.",
            "model_type": "latent world model",
            "task_domain": "visual RL and policy evolution in simulated environments (historical reference in related work)",
            "fidelity_metric": "Reconstruction/prediction fidelity in latent-video prediction; used qualitatively as foundational inspiration.",
            "fidelity_performance": "Not quantified in this paper; cited as influential prior work demonstrating latent world-model benefits.",
            "interpretability_assessment": "Provides some interpretability via latent-space visualizations but not focused on explicit controllable vs noncontrollable disentanglement.",
            "interpretability_method": "Visualization of decoded latent rollouts; not elaborated in this paper.",
            "computational_cost": "Relative to modern RSSM-based methods, earlier world models were simpler; computational details not discussed here.",
            "efficiency_comparison": "Provided historical context; Iso-Dream++ builds on the broader world-model idea but emphasizes disentanglement and future-conditioned behavior learning.",
            "task_performance": "Historical demonstrations show viability, but no direct comparisons in this paper.",
            "task_utility_analysis": "Motivates latent-model approaches; Iso-Dream++ extends the idea to isolate noncontrollable dynamics for improved task utility in specific domains.",
            "tradeoffs_observed": "Early world models trade fidelity of pixel reconstructions for compact latents; Iso-Dream++ focuses on modularity and downstream decision utility.",
            "design_choices": "Compress observations into latent codes and learn dynamics; later works (including this paper) refine architectures and objectives for disentanglement.",
            "comparison_to_alternatives": "Serves as conceptual predecessor to PlaNet/Dreamer family; Iso-Dream++ differs by explicit modular disentanglement and attention-based use of predicted noncontrollable futures.",
            "uuid": "e1225.6",
            "source_info": {
                "paper_title": "Model-Based Reinforcement Learning with Isolated Imaginations",
                "publication_date_yy_mm": "2023-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Dream to control: Learning behaviors by latent imagination",
            "rating": 2,
            "sanitized_title": "dream_to_control_learning_behaviors_by_latent_imagination"
        },
        {
            "paper_title": "Mastering diverse domains through world models",
            "rating": 2,
            "sanitized_title": "mastering_diverse_domains_through_world_models"
        },
        {
            "paper_title": "Iso-dream: Isolating and leveraging noncontrollable visual dynamics in world models",
            "rating": 2,
            "sanitized_title": "isodream_isolating_and_leveraging_noncontrollable_visual_dynamics_in_world_models"
        },
        {
            "paper_title": "Learning latent dynamics for planning from pixels",
            "rating": 2,
            "sanitized_title": "learning_latent_dynamics_for_planning_from_pixels"
        },
        {
            "paper_title": "Denoised-MDP: Learning world models better than the world itself",
            "rating": 2,
            "sanitized_title": "denoisedmdp_learning_world_models_better_than_the_world_itself"
        },
        {
            "paper_title": "Recurrent world models facilitate policy evolution",
            "rating": 1,
            "sanitized_title": "recurrent_world_models_facilitate_policy_evolution"
        },
        {
            "paper_title": "Stochastic MuZero",
            "rating": 1,
            "sanitized_title": "stochastic_muzero"
        }
    ],
    "cost": 0.01937125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Model-Based Reinforcement Learning with Isolated Imaginations
17 Nov 2023</p>
<p>Minting Pan 
Xiangming Zhu 
Yitao Zheng 
Yunbo Wang 
Fellow, IEEEXiaokang Yang 
Model-Based Reinforcement Learning with Isolated Imaginations
17 Nov 20235E4F4A02D07EE6EADA9F4E43C9BD2D07arXiv:2303.14889v2[cs.LG]
World models learn the consequences of actions in vision-based interactive systems.However, in practical scenarios like autonomous driving, noncontrollable dynamics that are independent or sparsely dependent on action signals often exist, making it challenging to learn effective world models.To address this issue, we propose Iso-Dream++, a model-based reinforcement learning approach that has two main contributions.First, we optimize the inverse dynamics to encourage the world model to isolate controllable state transitions from the mixed spatiotemporal variations of the environment.Second, we perform policy optimization based on the decoupled latent imaginations, where we roll out noncontrollable states into the future and adaptively associate them with the current controllable state.This enables long-horizon visuomotor control tasks to benefit from isolating mixed dynamics sources in the wild, such as self-driving cars that can anticipate the movement of other vehicles, thereby avoiding potential risks.On top of our previous work [1], we further consider the sparse dependencies between controllable and noncontrollable states, address the training collapse problem of state decoupling, and validate our approach in transfer learning setups.Our empirical study demonstrates that Iso-Dream++ outperforms existing reinforcement learning models significantly on CARLA and DeepMind Control.</p>
<p>INTRODUCTION</p>
<p>Humans can infer and predict real-world dynamics by simply observing and interacting with the environment.Inspired by this, many cutting-edge AI agents use self-supervised learning [2,3,4] or reinforcement learning [5,6,7] techniques to acquire knowledge from their surroundings.Among them, world models [3] have received widespread attention in the field of robotic visuomotor control, and led the recent progress in model-based reinforcement learning (MBRL) with visual inputs [6,7,8,9].One representative approach called Dreamer [6] learns a differentiable simulator of the environment (i.e., the world model) using observations and actions of an actor-critic agent, then updates the agent by optimizing its behaviors based on future latent states and rewards (i.e., latent imagination) generated by the world model.However, since the observation trajectories are high-dimensional, highly non-stationary, and often driven by multiple sources of physical dynamics, how to learn effective world models in complex visual scenes remains an open problem.</p>
<p>In this paper, we propose to understand the world by decomposing it into controllable and noncontrollable state transitions, i.e., s t+1 ∼ p(• | s t , a t ) and z t+1 ∼ p(• | z t ), according to the responses to action signals.This idea is largely inspired by practical scenarios such as autonomous driving, in which we can naturally divide spatiotemporal dynamics in the system into controllable parts that perfectly respond to the actions (e.g., accelerating and steering) and parts beyond the control of the agent (e.g., movement of other vehicles).Decoupling latent state transitions in this way can improve MBRL in three aspects:</p>
<p>• It allows decisions to be made based on predictions of future noncontrollable dynamics that are independent (or indirectly dependent) of the action, thereby improving the performance on</p>
<p>• The authors are with MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, China.• Corresponding author: Y. Wang, yunbow@sjtu.edu.cn.</p>
<p>• Code: https:// github.com/panmt/ MBRL with Isolated Imaginations.</p>
<p>long-horizon control tasks.For example, in the CARLA selfdriving environment, potential risks can be better avoided by anticipating the movement of other vehicles.</p>
<p>• Modular world models improve the robustness of the RL agent in noisy environments, as demonstrated in our modified DeepMind Control Suite with the time-varying background.• The isolation of controllable state transitions further facilitates transfer learning across different but related domains.We can adapt parts of the world model to novel domains based on our prior knowledge of the domain gap.Specifically, we present Iso-Dream++, a novel MBRL framework that learns to decouple and leverage the controllable and noncontrollable state transitions.Accordingly, it improves the original Dreamer [6] from two perspectives: (i) a new form of world model representation and (ii) a new actor-critic algorithm to derive the behavior from the world model.</p>
<p>How to learn a decoupled world model?</p>
<p>From the perspective of representation learning, we improve the world model to separate mixed visual dynamics into an actionconditioned branch and an action-free branch of latent state transitions (see Fig. 1).These components are jointly trained to maximize the variational lower bounds.Besides, the actionconditioned branch is particularly optimized with inverse dynamics as an additional objective function, that is, to reason about the actions that have driven the "controllable" state transitions between adjacent time steps.</p>
<p>Nonetheless, as we have observed in our preliminary work at NeurIPS'2022 [1], which we call Iso-Dream, the learning process of inverse dynamics is prone to the problem of "training collapse", where the action-conditioned branch captures all dynamic information, while the action-free branch learns almost nothing.To further isolate different dynamics in an unsupervised manner, we use new forms of min-max variance constraints to regularize the information flow of dynamics in the decoupled world model.More concretely, we provide a batch of hypothetical actions to the world
o 1 o 1 o 2 o 2 z 1 z 2 s 1 s 2 s 2 s 2 r 1 r 1 s 1 s 1 z 1 z 1 z 2 z 2 z 4 z 4 a 1 a 1 o 3 o 3 z 2 s 2 s 3 s 3 r 2 r 2 z 3 z 3 a 2 a 2 z 5 z 5 • • • • • • r 3 r 3</p>
<p>Inverse dynamics</p>
<p>State inference</p>
<p>Action dependency</p>
<p>Controllable state / Noncontrollable state z t s t / Sparse state dependency Fig. 1: Graphic model of our approach.The world model learns to decouple mixed visual dynamics into controllable states (s t ) and noncontrollable states (z t ) by optimizing the inverse dynamics (as indicated by the red dashed arrows).With state decoupling, the RL agent can make decisions based on the forecasts of future noncontrollable dynamics of the environment (blue arrows).In forward modeling, we consider the sparse dependency of next-step noncontrollable states on current controllable states (green arrows).In representation learning, we further cope with the imbalance of dynamic information learned in different state transition branches.model, and encourage the action-conditioned branch to produce different state transitions based on the same state, while penalizing the diversity of those in the action-free branch.</p>
<p>How to improve behavior learning based on decoupled world models?</p>
<p>Humans can decide how to interact with the environment at each moment based on their anticipation of future changes in their surroundings.Accordingly, by decoupling the state transitions, our approach can explicitly forecast the evolution of action-independent dynamics in the system, thereby greatly benefiting downstream decision-making tasks.Unlike Dreamer, it performs latent state imagination in both the training phase and testing phase of the agent behaviors to make more forward-looking decisions.As shown by the blue arrows in Fig. 1, the policy network integrates the current controllable state and multiple steps of predicted noncontrollable states through an attention mechanism.Intuitively, since future noncontrollable states at different steps may have different weights of impact on the current decision of the agent, the attention mechanism enables the agent to adaptively consider possible future interactions with the environment.It ensures that only appropriate future states are fed back into the policy.</p>
<p>Despite the effectiveness of the new behavior learning scheme, it only considers the indirect influence of action-free dynamics on future action-conditioned dynamics through agent behaviors (i.e., z t:t+τ → a t → s t+1 ).Another improvement of our approach over Iso-Dream is that it further models the sparse dependency of future noncontrollable states on current controllable states (i.e., s t → z t+1 ), which is indicated by the green arrows in Fig. 1.In practical scenarios, for example, when we program a robot to compete with another one in a dynamic game, the opponent can adjust its policy according to the behavior of our agent.In autonomous driving, when an agent vehicle veers into the lane of other vehicles, typically those vehicles will slow down to avoid a collision.Because of the proposed solution to training collapse, modeling the sparse dependency does not affect the disentanglement learning ability of the world model.In behavior learning, actions are sampled from π(s t , z t:t+τ ), where z t+1 ∼ p(•|z t , s t ), while due to the sparsity of the cross-branch dependencies, the long-horizon noncontrollable future can be approximated as z t+2:t+τ ∼ p(•|z t+1:t+τ −1 ).</p>
<p>We evaluate Iso-Dream++ in the following domains: (i) the CARLA autonomous driving environment in which other vehicles can be naturally viewed as noncontrollable components; (ii) the modified DeepMind Control Suite with noisy video background.Our approach outperforms existing approaches by large margins and further achieves significant advantages in transfer learning by isolating dynamics.It can selectively transfer controllable or noncontrollable parts of the learned state transition functions from the source domain to the target domain according to the prior information.</p>
<p>The main contributions of this paper are summarized as follows:</p>
<p>In summary, we extend our previous studies with (i) the minmax variance constraints, (ii) the sparse dependence between the decoupled latent states, and (iii) the transfer learning experiments.</p>
<p>PROBLEM OVERVIEW</p>
<p>Problem Definition</p>
<p>In visual control tasks, the agent learns the action policy directly from high-dimensional observations.We formulate visual control as a partially observable Markov decision process (POMDP) with a tuple (S, A, T , R, O), where S is the state space, A is the action space, O is the observation space, R(s t , a t ) is the reward function, and T (s t+1 | s t , a t ) is the state-transition distribution.</p>
<p>At each timestep t ∈ [1; T ], the agent takes an action a t ∈ A to interact with the environment and receives a reward r t = R(s t , a t ).</p>
<p>The objective is to learn a policy that maximizes the expected cumulative reward
E p [ T τ =1 r τ ].
In this setting, the agent cannot access the true states in S. Fig. 2: Examples of sparse dependency of the noncontrollable state on the controllable state.(a) A game resembling ice hockey that is played on a desk, in which the ego-agent is controllable, while the opponent robot can be seen as the noncontrollable part, and the hockey puck can be mostly considered to have predictable dynamics independent of the agent's actions.Sparse dependency occurs at the moment the agent hits the puck because it may change direction depending on the agent's current state.(b) In autonomous driving, other vehicles (yellow) will change their driving directions to avoid collision when the ego-agent (white) takes up their driveway.</p>
<p>Key Challenges</p>
<p>Challenge 1: How to learn future-conditioned policies without the expensive Monte-Carlo planning?Forecasting future environmental changes is useful for decision-making in a non-stationary system.A typical solution, such as the cross-entropy method (CEM), is to perform Monte-Carlo sampling over future actions and value the consequences of multiple action trajectories [10,4,11].These algorithms are expensive in computational cost, especially when we have large action and state spaces.The question is: Can we design an RL algorithm that allows for future-conditioned decision-making without playing dice in the action space?Challenge 2: How to avoid "training collapse" in unsupervised dynamics disentanglement?Despite the great success in unsupervised representation learning [12,13,14], it remains a challenge to disentangle the controllable and noncontrollable dynamic patterns in non-stationary visual scenes.One potential solution is to employ modular structures that learn different dynamics in separate branches.However, without proper constraints, the model may suffer from "training collapse", where one branch captures all useful information and the other learns almost nothing.This phenomenon may occur when the noncontrollable dynamics components are easy to predict.In this case, we consider adding further constraints to the learning objects of the action-conditioned and action-free state transition branches, encouraging them to isolate the noncontrollable part from the mixed dynamics.</p>
<p>Challenge 3: How to model situations where the agent behavior has only a sparse/indirect impact on noncontrollable dynamics?As we know, in realistic scenarios, the noncontrollable component of the dynamics may not evolve independently but may depend on the motions of the controllable component.For instance, in Fig. 2 (a), the hockey puck on the desk (noncontrollable part) changes its direction when the agent (controllable part) hits it.For autonomous driving, in Fig. 2 (b), other vehicles (noncontrollable part) will slow down to avoid a collision when the agent (controllable part) takes their lane.If we assume that our actions do not indirectly affect other vehicles on the road, then for safety reasons, a suboptimal policy for handling heavy traffic could be to follow the vehicle in front of us instead of changing lanes.Accordingly, we propose a sparse dependency mechanism that enhances our model's decision-making ability.Empirical results are illustrated in Fig. 9.</p>
<p>Basic Assumptions</p>
<p>In our proposed framework as shown in Fig. 1, when the agent receives a sequence of visual observations o 1:T , the underlying spatiotemporal dynamics can be defined as u 1:T .The evolution of different dynamics can be caused by different forces, but here we aim to decouple u 1:T into controllable latent states s 1:T and time-varying noncontrollable latent states z 1:T , such that:
u 1:T ∼ (s, z) 1:T , s t+1 ∼ p(s t+1 | s t , a t ), z t+1 ∼ p(z t+1 | z t ),(1)
where a t is the action signal.By isolating s t and z t to each other, we model their state transitions of p(s t+1 | s t , a t ) and p(z t+1 | z t ) respectively.We assume that a more clear decoupling of s t and z t can benefit both long-term predictions and decision-making.As an extension of our preliminary work [1], we additionally model the sparse dependency of noncontrollable dynamics on controllable dynamics (as described below).Thus, when a sparse event is detected, the transition of noncontrollable state in Eq. ( 1) can be rewritten as z t+1 ∼ p(z t+1 | z t , s t ).</p>
<p>It assumes that the agent can greatly benefit from predicting the consequences of external noncontrollable forces.During behavior learning, we roll out the noncontrollable states and then associate them with the current controllable states for more proactive decision-making.We derive the action policy by
a t ∼ π(a t | s t , 1 ⊙ z t:t+τ ),(2)
where 1 is an indicator according to our prior knowledge about the environment.For example, in autonomous driving, since it is reasonable for the ego-agent to make decisions based on the predictions about the future states of other vehicles, we have 1 = 1 and calculate the relations between s t and the imagined noncontrollable states in a time horizon τ .Otherwise, for some specific tasks where the noncontrollable components are irrelevant to decision-making, we can simply set the indicator function to 1 = 0 and treat them as noisy distractions.
z t z t z t−1 z t−1 Enc ϕ 3 Enc ϕ 3 Dec φ 3 Dec φ 3 Static Enc θ Enc θ o t o t ô z t , M z t ô z t , M z t ô s t , M s t ô s t , M s t ô b ô b min KL a t−1 a t−1 a t−1 a t−1 Dependency Gate Action-conditioned GRU z GRU z GRU z GRU z GRU z GRU z GRU z GRU z GRU s GRU s GRU s GRU s s1 t s1s t−1 , a 1 t−1 s t−1 , a 1 t−1 s t−1 , a 2 t−1 s t−1 , a 2 t−1 s t−1 , a n t−1 s t−1 , a n t−1 z t−1 ,0 z t−1 ,0 z t−1 , a 1 t−1 z t−1 , a 1 t−1 z t−1 , a n t−1 z t−1 , a n t−1</p>
<p>METHOD</p>
<p>In this section, we present the technical details of Iso-Dream++ for decoupling and leveraging controllable and noncontrollable dynamics for visual MBRL.The overall pipeline is based on Dreamer [6], where we learn the world model from a dataset of past experience, learn behaviors from imagined sequences of compact model states, and execute the behavior policy in the environment to grow the experience dataset.In Section 3.1.1,we first introduce the three-branch world model and its training objectives of inverse dynamics.In Section 3.1.2,we propose the min-max variance constraints to regularize the dynamics representations in each state transition branch to enhance disentanglement learning and avoid training collapse.In Section 3.1.3,we present a network structure to model the phenomenon that future noncontrollable dynamics can be sparsely influenced by current controllable dynamics.In Section 3.2, we present an actor-critic method that is trained on the imaginations of the decoupled world model latent states, so that the agent may consider possible future states of noncontrollable dynamics in behavior learning.Finally, in Section 3.3, we discuss how our model is deployed to interact with the environment.</p>
<p>World Models with Dynamics Isolation</p>
<p>Inspired by prior research [15,16] that demonstrates the efficacy of modular structures for disentanglement learning, we use an architecture with multiple branches to model different dynamics independently, according to their respective physical laws.Each individual branch tends to present robust features, even when the dynamic patterns in other branches undergo changes.Specifically, our three-branch model, illustrated in the left panel of Fig. 3, disentangles visual observations into controllable dynamics state s t , noncontrollable dynamics state z t , and a time-invariant component of the environment.The action-conditioned branch models the controllable state transition p(s t+1 | s t , a t ).It follows the RSSM architecture from PlaNet [11] to use a recurrent neural network GRU s (•), the deterministic hidden state h t , and the stochastic state s t to form the transition model, where the GRU keeps the historical information of the controllable dynamics.The action-free branch models p(z t+1 | z t ) with similar network structures.The transition models with separate parameters can be written as follows:
p(s t | s &lt;t , a &lt;t ) = p(s t | h t ), p(z t | z &lt;t ) = p(z t | h ′ t ),(3)
where
h t = GRU s (h t−1 , s t−1 , a t−1 ), h ′ t = GRU z (h ′ t−1 , z t−1 )
. We here use st and zt to denote the prior representations.We optimize the transition models with posterior representations that are derived from s t ∼ q(s t | h t , o t , a t−1 ) and z t ∼ q(z t | h ′ t , o t , a t−1 ).We learn the posteriors from the observation at current time step o t ∈ R 3×H×W by a shared encoder Enc θ and subsequent branch-specific encoders Enc ϕ1 and Enc ϕ2 .Notably, we feed actions into both Enc ϕ1 and Enc ϕ2 , which differs from our previous work [1].In the static branch, where there is no state transition, we only use an encoder Enc ϕ3 and a decoder Dec φ3 to model simple time-invariant information in the environment.</p>
<p>Inverse Dynamics</p>
<p>To enable disentanglement representation learning that corresponds to the control signals, we introduce the training objective of inverse dynamics.This objective encourages the action-conditioned branch to learn a more deterministic state transition based on specific actions, while the action-free branch learns the remaining noncontrollable dynamics independent of the control signals.Accordingly, we design an Inverse Cell of a 2-layer MLP to infer the actions that lead to certain transitions of the controllable states:
Inverse dynamics: ãt−1 = MLP(s t−1 , s t ),(4)
where the inputs are the posterior representations in the actionconditioned branch.By learning to regress the true behavior a t−1 , the Inverse Cell facilitates the action-conditioned branch to isolate the representation of the controllable dynamics.We respectively use the prior state st and the posterior state z t to generate the controllable visual component ôs with M z t ∈ R 1×H×W .By further integrating the static information extracted from the first K frames, we have
t ∈ R 3×H×W with mask M s t ∈ R 1×H×W and the noncontrollable component ôz t ∈ R 3×H×Wôt = M s t ⊙ ôs t + M z t ⊙ ôz t + (1 − M s t − M z t ) ⊙ ôb , (5)
where ôb = Dec φ3 (Enc θ,ϕ3 (o 1:K ))).</p>
<p>For reward modeling, we have two options concerning the action-free branch.First, we may regard noncontrollable dynamics as irrelevant noises that do not contribute to the task and therefore do not involve z t in imagination.In other words, the policy and predicted rewards would solely rely on controllable states, e.g., p(r t | s t ).Alternatively in other cases, we need to consider the influence of future noncontrollable states on the agent's decisionmaking process and incorporate the action-free components during behavior learning.To achieve this, we train the reward predictor to model p(r t | s t , z t ) in the form of MLPs.</p>
<p>For a training sequence of (o t , a t , r t ) T t=1 sampled from the replay buffer, the world model can be optimized using the following loss functions, where α, β 1 , and β 2 are hyper-parameters:
L base =E { T t=1 − ln p(o t | h t , s t , h ′ t , z t ) image log loss + αℓ 2 (a t , ãt ) action loss − ln p(r t | h t , s t , h ′ t , z t ) reward log loss − ln p(γ t | h t , s t , h ′ t , z t ) discount log loss + β 1 KL[q(s t | h t , o t ) | p(s t | h t )]
KL divergence in the action-conditioned branch
+ β 2 KL[q(z t | h ′ t , o t ) | p(z t | h ′ t )]
KL divergence in the action-free branch }.</p>
<p>(6)</p>
<p>Training Collapse and Min-Max Variance Constraints</p>
<p>Despite modeling inverse dynamics, for the original Iso-Dream, we find that it is still challenging for the world model to isolate controllable and noncontrollable dynamics.We observed in the preliminary experiments that the disentanglement results were unstable over multiple runs of world model training, and that the action-conditioned branch occasionally learned mismatched representations of noncontrollable state transitions.An example is shown in Fig. 4. It implies that most useful information may collapse into the action-conditioned branch while the action-free branch learns almost nothing, which we call "training collapse".This phenomenon arises due to the inherent limitations of the training objective in inverse dynamics, which may not always ensure the complete exclusion of action-independent state transitions, particularly when the action-conditioned network branch possesses a strong capacity for modeling dynamics.</p>
<p>To keep the state transition branches from training collapse, we propose the min-max variance constraints, whose key idea is to (i) maximize the diversity of outcomes in the action-conditioned branch given distinct action inputs and (ii) minimize the diversity of outcomes in the action-free branch under similar conditions.To this end, unlike in the original Iso-Dream, we make the action-free branch also aware of the action signal during the world model learning process.But for behavior learning and policy deployment, we simply set the input action to 0-values.</p>
<p>There is an information-theoretic interpretation behind calculating variance.In order to investigate the connection between dynamics and action signals, the world model is enforced to identify the dynamics that provide information about our beliefs regarding the action signals.The expected information gain can be expressed as the conditional entropy of state and action:
I(s t ; a t−1 |s t−1 ) = H(s t |s t−1 ) − H(s t |s t−1 , a t−1 ). (7)
As shown in Fig. 3 (right), for the action-conditioned branch, we maximize the mutual information between the state and action signal to focus on the state transition of a specific action.Given a batch of hypothetical actions {a i t−1 |i ∈ [1, n]}, for the same controllable state s t−1 , we have different state transitions based on these actions:
si t ∼ p(s i t |s t−1 , a i t−1 ), i ∈ [1, n].
The empirical variance is used to approximate the information gain, and the objective can be written as
L s = max T t Var(s i t ) = max T t 1 n − 1 i (s i t − st ) 2 , st = 1 n i si t , i ∈ [1, n]. (8)
On the contrary, in the action-free branch, we minimize the variance of output states resulting from different actions, penalizing the diversity of state transitions:
L z = min T t Var(z i t ) = min T t 1 n − 1 i (z i t − zt ) 2 , zt = 1 n i zi t , i ∈ [1, n].(9)
The overall training objective of the world model is
L all = L base + L var ,(10)
where L var = λ 1 L s + λ 2 L z .λ 1 and λ 2 are hyper-parameters.</p>
<p>For convenience, we only use two opposite actions {a t , −a t } in the action-conditioned branch, and use the action set {a t , 0, −a t } in the action-free branch to figure out L s and L z .As for subsequent learning, we use a t and 0 in the action-conditioned and action-free branches, respectively.</p>
<p>Sparse Dependency between Decoupled States</p>
<p>In certain situations, the controllable and noncontrollable dynamics are not entirely independent, as shown in Fig. 2.This is particularly true in autonomous driving, where the actions of the ego-agent can influence the behavior of other vehicles, causing them to steer or slow down.To accurately predict future noncontrollable states based on current controllable states, it is essential to account for these sparse dependencies.</p>
<p>To achieve effective modeling of sparse dependency, it is essential to identify the moment when controllable states exert a significant influence on noncontrollable states.In order to facilitate this, we present a compact module called the dependency gate, which connects the previously isolated action-free and actionconditioned branches, as shown in Fig. 3 (left).We unfold the detailed structure dependency gate in time (see Fig. 5), where the controllable state st and noncontrollable state zt are concatenated and passed through a fully connected layer represented by f (s t , zt ).A sigmoid function is then applied as an activation signal to control the gate, which is formulated as
δ t (w t = 1|s t , zt ) = 1, sigmoid(f (s t , zt )) ≥ 0.5, 0, otherwise.(11)
When the gate detects a dependency between controllable and noncontrollable states (w t = 1), the subsequent noncontrollable state zt+1 is determined by both st and zt using the action-free transition, which is defined as follows:
zt+1 ∼ p(z t+1 | zt , w t ⊙ st ).(12)</p>
<p>Behavior Learning in Isolated Imaginations</p>
<p>Thanks to the decoupled world model, we can optimize agent behavior to adaptively consider the relationship between available actions and potential future states of the noncontrollable dynamics.</p>
<p>A practical example is autonomous driving, where the movement of other vehicles can be naturally viewed as noncontrollable but predictable components.As shown in Fig. 6, we here propose an improved actor-critic learning algorithm that (i) allows the actionfree branch to foresee the future ahead of the action-conditioned branch, and (ii) exploits the predicted future information of noncontrollable dynamics to make more forward-looking decisions.Suppose we are making decisions at time step t in the imagination period.A straightforward solution from the original Dreamer method is to learn an action model and a value model based on the isolated controllable state st ∈ R 1×d .With the aid of an attention mechanism, we can establish a connection between it and future noncontrollable states.It is important to note that we only employ sparse dependency in the initial imagination step to obtain zt+1 , as the subsequent controllable states are not yet available at time step t.Once we have predicted a sequence of future noncontrollable states zt:t+τ ∈ R τ ×d , where τ is the sliding
h 0 h 0 s 1 s 1 o 1 o 1 a 1 a 1 Enc θ,ϕ 2 Enc θ,ϕ 2 GRU s GRU s s 1 s 1 s2 s2 GRU s GRU s st−1 st−1 st st a t−1 a t−1 Action-conditioned Action-free z 1 z 1 z2 z2 zt zt zτ+1 zτ+1 zτ zτ a 1 , v 1 a 1 , v 1 r
This equation allows us to dynamically adjust the horizon of future noncontrollable states using the attention mechanism.In this way, st evolves to a more "visionary" representation e t ∈ R 1×d .We modify the action and value models in Dreamer [6] as follows:</p>
<p>Action model:
a t ∼ π(a t | e t ),
Value model:
v ξ (e t ) ≈ E π(•|et) t+L k=t γ k−t r k , (14)
where L is the imagination time horizon.As shown in Alg. 1, during imagination, we first use the action-free transition model to obtain sequences of noncontrollable states of length L + τ , denoted by {z i } i+L+τ i=t .At each time step in the imagination period, the agent draws an action a j from the visionary state e j , which is derived from Eq. ( 13).The action-conditioned branch uses the action a j in latent imagination and predicts the next controllable state s j+1 .We follow DreamerV2 [8] to train our action model with the objective of maximizing the λ-return [17], while our value model was trained to perform regression on the λ-return.For further information on the loss functions, please refer to Eq. (5-6) as detailed in the paper of DreamerV2 [8].</p>
<p>Policy Deployment by Rolling out Noncontrollable Dynamics</p>
<p>During policy deployment, as shown in Lines 22-24 in Alg. 1, the action-free branch predicts the next-step noncontrollable states zt+1 using Eq. ( 12) and then consecutively rolls out the future noncontrollable states zt+2:t+τ starting from zt+1 .Similar to Eq. ( 13) used in the process of behavior learning, the learned future state attention network is used to adaptively integrate s t , z t and zt+1:t+τ .Based on the integrated feature e t , the Iso-Dream++ agent then draws a t from the action model to interact with the environment.As discussed in Section 2.2, if the noncontrollable dynamics are irrelevant to the control task, the policy at each time step t is generated using only the state of controllable dynamics when interacting with the environment.Draw data sequences {(ot, at, rt)} T t=1 ∼ B.</p>
<p>6:</p>
<p>Compute the controllable state st ∼ q(st|ht, ot, at−1) and the noncontrollable state zt ∼ q(zt|h ′ t , ot, at−1).</p>
<p>7:</p>
<p>Compute world model loss using Eq. ( 10) and update model parameters.</p>
<p>8:</p>
<p>// Behavior learning 9:</p>
<p>for time step i = t . . .t + L do 10:</p>
<p>Compute the next noncontrollable state zi+1 using Eq. ( 12).</p>
<p>11:</p>
<p>Roll-out the noncontrollable states {zj} i+τ j=i+2 from zi+1 through the action-free branch alone.</p>
<p>12:</p>
<p>Compute latent state ei ∼ Attention(si, zi:i+τ ) using Eq. ( 13).</p>
<p>13:</p>
<p>Imagine an action ai ∼ π(ai|ei).</p>
<p>14:</p>
<p>Predict the next controllable state si+1 ∼ p(si, ai) using the action-conditioned branch alone.</p>
<p>15:</p>
<p>end for 16:</p>
<p>Update the policy and value models in Eq. ( 14) using estimated rewards and values.Calculate the posterior representation st ∼ q (st | ht, ot, at−1) , zt ∼ q (zt | h ′ t , ot, at−1).</p>
<p>22:</p>
<p>Compute the next noncontrollable state zt+1 ∼ p(zt+1|zt, wt ⊙ st) using Eq. ( 12).</p>
<p>23:</p>
<p>Roll-out the noncontrollable states zt+2:t+τ from zt+1 through the action-free branch alone.</p>
<p>24:</p>
<p>Generate at ∼ π(at | st, zt, zt+1:t+τ ) using future state attention in Eq. ( 13).</p>
<p>25:</p>
<p>rt, ot+1 ← env.step(at)</p>
<p>26:</p>
<p>end for</p>
<p>27:</p>
<p>Add experience to the replay buffer B ← B ∪ {(ot, at, rt) T t=1 }. 28: end while</p>
<p>EXPERIMENTS</p>
<p>Experimental Setup</p>
<p>Benchmarks.We evaluate Iso-Dream++ on two RL environments:</p>
<p>• CARLA [18]: CARLA is a simulator with complex and realistic visual observations for autonomous driving research.We train our model to perform the task of first-person highway driving in "Town04", where the agent's goal is to drive as far as possible in 1,000 time steps without colliding with any of the 30 other moving vehicles or barriers.In addition to our conference paper, we incorporate more diverse settings into our study, including both day and night modes as shown in Fig. 7. • DeepMind Control Suite [19]: DMC contains a set of continuous control tasks and serves as a standard benchmark for visionbased RL.To evaluate the generalization of our method by disentangling different components under complex visual dynamics, we use two modified benchmarks [20], namely video_easy, which contains 10 simple videos, and video_hard, which contains 100 complex videos.</p>
<p>Compared methods.We compare Iso-Dream++ with the following visual RL approaches:</p>
<p>• DreamerV2 [8]: A model-based RL method that learns directly from latent variables in world models.The latent representation allows agents to imagine thousands of trajectories in parallel.• DreamerV3 [21]: A further improved version of Dreamer that learns to master diverse domains with fixed hyperparameters.• DreamerPro [22]: A non-contrastive, reconstruction-free modelbased RL method that combines Dreamer [6] with prototypes to enhance robustness to distractions.• CURL [23]: A model-free RL method that uses contrastive learning to extract high-level features from raw pixels, maximizing agreement between augmented data of the same observation.</p>
<p>Day Night</p>
<p>Fig. 7: Examples of day and night modes in CARLA.</p>
<p>• SVEA [24]: A framework for data augmentation in deep Qlearning algorithms that improves stability and generalization on off-policy RL.</p>
<p>• SAC [25]: A model-free actor-critic method that optimizes a stochastic policy in an off-policy way.</p>
<p>• DBC [26]: A method that learns a bisimulation metric representation without reconstruction loss.This representation is invariant to different task-irrelevant details in the observation.• Denoised-MDP [27]: A framework that categorizes information out in the wild into four types based on controllability and relation with reward, and formulates useful information as that which is both controllable and reward-relevant.</p>
<p>CARLA Autonomous Driving Environment</p>
<p>Implementation.In the autonomous driving task, We use a camera with a 60 degree view on the roof of the ego-vehicle, which obtains images of 64 × 64 pixels.Following the setting in the DBC [26], in order to encourage highway progression and penalize collisions, the reward is formulated as
r t = v T ego ûh • ∆t − ξ 1 • 1−ξ 2 • |steer|,
where v ego is the velocity vector of the ego-vehicle, projected onto the highway's unit vector ûh , and multiplied by time discretization ∆t = 0.05 to measure highway progression in meters.We use   and ξ 2 = 1, respectively.We use β 1 = β 2 = 1 and α = 1 in Eq. ( 6), λ 1 = λ 2 = 1 in Eq. ( 10), and τ = 5 in Eq. ( 13).</p>
<p>Quantitative comparisons.We present the quantitative results in CARLA in Fig. 8(a).Iso-Dream++ outperforms the compared models, including DreamerV2, DreamerV3, DreamerPro, and Denoised-MDP, significantly.After 500k environment steps, Iso-Dream++ achieves an average return of around 60, while DreamerV2 and Denoised-MDP achieve 10 and 25 respectively.In DreamerV2, the latent representations contain both controllable and noncontrollable dynamics, which increases the complexity of modeling the state transitions in imagination.Compared with Denoised-MDP, which also decouples information according to controllability, Iso-Dream++ has the advantage of making forwardlooking decisions by rolling out future noncontrollable states.</p>
<p>Ablation studies.Fig. 8(b) provides the ablation study results that validate the effectiveness of inverse dynamics, the rollingout strategy of noncontrollable states, the attention mechanism, and the modeling of static information.As shown by the green curve, removing the Inverse Cell reduces the performance of Iso-Dream++, which indicates the importance of isolating controllable and noncontrollable components.For the model indicated by the red curve, we do not use the rollouts of future noncontrollable states as the inputs of the action model.The results demonstrate that rolling out noncontrollable states in the action-free branch significantly improves the agent's decision-making results by perceiving potential risks in advance.Moreover, we evaluate Iso-Dream++ without attention mechanism where the action model directly concatenates the current controllable state with a sequence of future noncontrollable states and takes them as inputs.As shown by the purple curve, the attention mechanism extracts valuable information from future noncontrollable dynamics better than concatenation.Furthermore, as shown by the brown curve, our approach's performance decreases by about 15% without a separate network branch for capturing the static information.Moreover, a comparison between the blue curve and the orange curve reveals a decline in our model's performance when we remove the min-max variance constraints and sparse dependency modeling.Unlike the DMC suite, where the original Iso-Dream is more vulnerable to training collapse, in the CARLA environment, the sparse dependency modeling method plays a crucial role in the improved performance of Iso-Dream++.In Fig. 9, we present visual examples produced by our models with and without the sparse dependency.Without sparse dependency (top row), the agent fails to predict that other vehicles will slow down or brake when changing lanes, making it safer to follow the vehicle ahead rather than overtake it during traffic congestion.However, as shown in the bottom row of Fig. 9, the agent can decide whether to overtake or not based on its surroundings.These results indicate that sparse dependency greatly models the situation that the noncontrollable dynamics are affected by the controllable dynamics, which is conducive to the downstream decision-making task by accurately predicting the noncontrollable dynamics at future moments.</p>
<p>Qualitative results.We show the video prediction results of Iso-Dream++ in the CARLA environment in Fig. 10.Because of the first-person view in this environment, the agent actions potentially affect all pixel values in the observation, as the camera on the main car (i.e., the agent) moves.Therefore, we can view the dynamics of other vehicles as a combination of controllable and non-controllable states.Accordingly, our model determines which component is dominant by learning attention mask values between 0 and 1 across the action-conditioned and action-free branches.The "action-free masks" present hot spots around other vehicles, while the attention values in corresponding areas on the "action-cond masks" are  still greater than zero.As shown in the third and fifth lines, Iso-Dream++ mainly learns the dynamics of mountains and trees in the action-conditioned branch and the dynamics of other driving vehicles in the action-free branch, respectively, which helps the agent avoid collisions by rolling out noncontrollable components to preview possible future states of other vehicles.
Hopper Stand Finger Spin Cheetah Run Walker Walk
Hyperparameter analyses of τ .We evaluate the effect of using different numbers of rollout steps of the noncontrollable states as the inputs of the action model.From the results in Fig. 11, we observe that our model achieves the best performance at τ = 5.However, there are no remarkable differences among τ ∈ [5,10,15], as long-term predictions for noncontrollable states may increase model errors.Besides, we implement a model without rolling out noncontrollable states into the future, i.e., τ = 0.It performs significantly worse than other baselines with τ ∈ [5, 10, 15], which demonstrates the benefit of rolling out the disentangled action-free branch in policy optimization.</p>
<p>DeepMind Control Suite</p>
<p>Implementation.We evaluate our model on video_easy and video_hard benchmarks from the DMC Generalization Benchmark [20], where the background is continually changing throughout an episode.All experiments use visual observations only, of shape 64 × 64 × 3. The episodes last for 1,000 steps and have randomized initial states.We apply a fixed action repeat of R = 2 across tasks.In this environment, since the background is randomly replaced by a real-world video, the noncontrollable motion of the background will affect the procedure of dynamics learning and behavior learning.Therefore, to obtain a better decision policy and avoid the disruption from noisy backgrounds, the agent may decouple noncontrollable representation (i.e., dynamic background) and controllable representation in spacetime, and only use controllable representation for control, thereby removing the modeling sparse dependency.Instead of training the actionfree branch with only reconstruction loss in our preliminary work [1], we follow the structure described in Section 3.1 since the noncontrollable dynamics in some video backgrounds are complicated for learning, particularly video_hard benchmark.We evaluate the models using 4 tasks, i.e., Finger Spin, Cheetah Run, Walker Walk, and Hopper Stand.The maximum number of environmental steps is 500k.We use β 1 = β 2 = 1 and α = 1 in Eq. ( 6) and λ 1 = λ 2 = 1 in Eq. (10).</p>
<p>Quantitative comparisons.We present the quantitative results of Iso-Dream++ for video_easy benchmark in Table 1.Our final model outperforms DreamerV2 and other baselines significantly in all tasks.Compared with DBC and Denoised-MDP, which both aim to extract task-relevant representation from complex visual distractions, our method is more powerful with large performance gains on all four tasks, indicating that disentangling different dynamics by modular structure and variance constraints provides more cleaner and useful information for the downstream task.Moreover, we have a better performance than DreamerPro, which is also based on Dreamer but learns the world model without reconstructing the observations.This demonstrates that our model effectively helps the agent to learn controllable visual representations and alleviate complex background interference.</p>
<p>Analyses of the min-max variance constraints.We investigate the effectiveness of the proposed variance constraints described As shown in Fig. 12, the compared models are trained for 10 seeds, and the proposed method improves the performance of our model in most tasks, especially in finger spin, where we have witnessed significant training collapse (see Fig. 4).In Fig. 13, we provide a qualitative comparison of the disentanglement results between models trained with and without variance constraints.</p>
<p>Comparing the fifth and sixth row of action-free branch outputs, we observe that the action-free dynamics (such as the light over the lake) are correctly assigned to the action-free branch by variance constraints, preventing the action-conditioned branch from capturing all dynamic information, i.e., training collapse.Because of the pure dynamics captured in the action-conditioned branch, our model with variance constraints gains definite improvements.</p>
<p>Qualitative results.We use Iso-Dream++ to perform video prediction in the DMC environment with video_easy backgrounds.The frame sequence and actions are randomly collected from test episodes.The first 5 frames are given to the model and the next 45 frames are predicted only based on action inputs.In this environment, the video background can be viewed as a combination of noncontrollable dynamics and static representations.Fig. 14 visualizes the entire generated RGB images, the decoupled RGB components, and the corresponding masks of the three network branches.From these results, we observe that our approach has the ability to predict long-term sequence and disentangle controllable  (agent) and noncontrollable dynamics (background motion) from complex visual images.As shown in the third and fourth rows in Fig. 14, the controllable representation has been successfully isolated and matches its mask.As shown in the fifth and sixth rows, the motion of fires and sea waves are captured as noncontrollable dynamics by the action-free branch.</p>
<p>Robustness to immediate distractions.To assess the ability of Iso-Dream++ to resist immediate visual distractions, we train the RL models on the video easy benchmark and evaluate them on (i) video hard; (ii) video easy with Gaussian noises.In Table 2, we compare the results from Iso-Dream++ with those from DreamerPro and Denoised-MDP, which both focus on learning robust representations against visual noises.We observe a remarkable advantage of Iso-Dream++ against unexpected distractions, which consistently outperforms the DreamerPro and Denoised-MDP across all tasks.</p>
<p>Transfer Learning Analyses</p>
<p>Transfer of noncontrollable dynamics in CARLA.Our model learns different dynamics in different branches, which makes it naturally suitable for transfer learning.Unlike common methods that transfer all knowledge from a pretrained source task, we can selectively transfer specific knowledge for a target task.Specifically, we only transfer relevant knowledge, such as shared dynamics between source and target tasks, to achieve precise disentanglement and robust decision-making on the target task.In Fig. 7, We can see that the noncontrollable dynamics are similar between day and night modes, i.e., the movement of other driving vehicles.We keep the action-free branch pretrained on the day mode of the CARLA environment and then train it on the night mode.The results are shown in Fig. 15.Comparing the orange curve and the blue curve, our model that transfers noncontrollable dynamics in the action-free branch has a significant improvement.However, the performance gain of DreamerV2 is small.Therefore, due to the modular structure in our Iso-Dream++, when there are two environments with similar dynamics, we can train on the easy environment first, and then load the specific pretrained branch to help the model learn on difficult tasks.Therefore, the modular structure of our Iso-Dream++ allows us to selectively transfer controllable or noncontrollable parts to novel domains based on our prior knowledge of the domain gap.</p>
<p>Transfer of controllable dynamics in DMC.For DMC, we use the video_easy (source) and video_hard (target) benchmarks to evaluate the transfer ability of our model.We transfer the controllable information in the action-conditioned branch because the controllable dynamics are the same in both environments, i.e., the motion of the agent.From Fig. 16, we have two key observations.First, upon loading the pretrained action-conditioned branch, Iso-Dream++ exhibits a significant advantage over the non-pretrained counterpart.Second, it is noteworthy that the performance improvement achieved by our model through pretraining surpasses that of DreamerV2 by a considerable margin.</p>
<p>RELATED WORK</p>
<p>Visual RL.In visual control tasks, RL agents learn the action policy directly from high-dimensional observations.They can be roughly grouped into two categories, that is, model-free methods [25,23,28,29,30,24] and model-based methods [10,5,3,11,6,9,7,26,31,27,22,32,33].Among them, the MBRL approaches explicitly model the state transitions and</p>
<p>Cheetah Run</p>
<p>Walker Walk Fig. 16: Transfer learning results in DMC across environments with video_easy and video_hard backgrounds.Unlike DreamerV2, which can only transfer the entire world model (red curve) to the target domain, Iso-Dream++ enables us to separately transfer the pretrained action-conditioned branch and obtain significantly better finetuning results (blue curve).</p>
<p>generally yield higher sample efficiency than the model-free methods.A notable branch of work is the MuZero models, such as Stochastic MuZero [34].These models simulate and explore possible future action trajectories through Monte Carlo tree search (MCTS), which can effectively improve long-term decision-making but introduce a vast computational cost.Notably, our model is different from Stochastic MuZero in two ways.First, we improve dynamics learning by encouraging representation decoupling, which we assume can enable the model to better understand the controllable and noncontrollable parts of the environment and greatly benefit the learned policy.Second, unlike in Stochastic MuZero, our model performs an actor-critic algorithm without MCTS, which is practical in short-term control tasks such as autonomous driving and ensures higher efficiency for both policy optimization and deployment.Another line of work is the socalled World Models.Ha and Schmidhuber [3] proposed to learn compressed latent states of the environment in a self-supervised manner and optimize potential behaviors based on the latent states generated by the world model.Similarly, PlaNet [11] introduces the recurrent state-space model (RSSM) as the world model and performs the cross-entropy method over the imagined recurrent states.DreamerV1-V3 [6,8,21] employ actor-critic methods to optimize the expected values and agent's behaviors over the predicted latent states in RSSM.Specifically, our model based on DreamerV2 outperforms DreamerV2-V3 remarkably in CARLA and DMC.We also note that the state decoupling and futureconditioned behavior learning techniques proposed in Iso-Dream++ can be seamlessly integrated with DreamerV2-V3, consistently enhancing their overall performance and convergence rate.</p>
<p>Visual RL with visual distractions.However, for complex visual environments with background or even dynamic distractions, it is still challenging to learn effective behavior policies.To tackle this problem, some approaches [26,31,27,22] learn a more robust representation by discarding pixel-reconstruction to avoid struggling with the presence of visual noises.DreamerPro [22] uses online clustering to learn prototypes from the recurrent states of the world model, eliminating the need for reconstruction.Denoised-MDP [27] categorizes system dynamics into four types based on their controllability and relation to rewards, and optimizes the policy model only with information that is both controllable and relevant to rewards.It is worth noting that Iso-Dream++ differs significantly from the aforementioned methods in two key ways.First, we explicitly model the state transitions of controllable and noncontrollable dynamics in two distinct branches.This modular structure empirically facilitates transfer learning between related but distinct domains.Second, the decoupled world model offers a more versatile method of learning behavior.By previewing possible future states of noncontrollable patterns, we can make informed decisions at present.This also allows us to choose whether or not to incorporate noncontrollable states into our decision-making process, based on our prior knowledge of the specific domain.</p>
<p>Action-conditioned video prediction.Another branch of deep learning solutions to visual control problems is to learn actionconditioned video prediction models [2,35,36,37,38] and then perform Monte-Carlo importance sampling and optimization algorithms, such as the cross-entropy methods, over available behaviors [10,4,39] [60,61,62,63,64,65,66], and space-time disentanglement [67,61,68,69].The corresponding technical improvements mainly involve the use of more effective neural architectures, novel probabilistic modeling methods, and specific forms of video representations.The disentanglement methods are closely related to the world model in Iso-Dream++.They commonly separate visual dynamics into content and motion vectors, or longterm and short-term states.In contrast, Iso-Dream++ is designed to learn a decoupled world model based on controllability, which contributes more to the downstream behavior learning process.</p>
<p>CONCLUSION</p>
<p>In this paper, we proposed an MBRL framework named Iso-Dream++, which mainly tackles the difficulty of vision-based prediction and control in the presence of complex visual dynamics.</p>
<p>Our approach has four novel contributions to world model representation learning and corresponding MBRL algorithms.First, it learns to decouple controllable and noncontrollable latent state transitions via modular network structures and inverse dynamics.Second, it introduces the min-max variance constraints to prevent "training collapse", where a single state transition branch captures all information.Third, it makes long-horizon decisions by rolling out the noncontrollable dynamics into the future and learning their influences on current behavior.Fourth, it models the sparse dependency of future noncontrollable dynamics on current controllable dynamics to deal with some practical dynamic environments.Iso-Dream++ achieves competitive results on the CARLA autonomous driving task, where other vehicles can be naturally viewed as noncontrollable components, indicating that with the help of decoupled latent states, the agent can make more forward-looking decisions by previewing possible future states in the action-free network branch.Besides, Our approach was shown to effectively improve the visual control task in a modified DeepMind Control Suite, achieving significant advantages over existing methods in standard, noisy, and transfer learning setups.</p>
<p>Fig. 3 :
3
Fig. 3: The overall architecture of the world model in Iso-Dream++.Left: The world model has three branches to explicitly disentangle controllable and noncontrollable state transitions, as well as the static components from visual data.Right: Illustration of calculating variance in different branches.Given the different action signals, our objective is to minimize the diversity of state transitions in the action-free branch and maximize the diversity of those in the action-conditioned branch.</p>
<p>Fig. 4 :
4
Fig. 4: A showcase of training collapse that the action-conditioned branch in the original version of Iso-Dream dominates the learning process of both controllable and noncontrollable dynamics.The corresponding results of Iso-Dream++ are shown in Fig. 14.</p>
<p>1 Fig. 5 :
15
Fig. 5: The dependency gate involves a binary gate that can either be open (w t = 1) or closed (w t = 0).When the gate is open, the transition of the next noncontrollable state zt+1 takes into account the dependency between st and zt .</p>
<p>Algorithm 1 2 : while not converged do 3 :
123
Iso-Dream++ (Highlight: Our modifications to behavior learning &amp; policy deployment of the original Dreamer) Hyper-parameters: L: Imagination horizon; τ : Window size for future state attention 1: Initialize the replay buffer B with random episodes.for update step c = 1 . . .C do</p>
<p>Fig. 8 :
8
Fig. 8: (a) Quantitative comparison with existing approaches for CARLA driving.(b) Ablation studies of individual effectiveness of inverse dynamics optimization (green), noncontrollable rollouts (red), future-state attention (purple), and the separate branch for static information modeling (brown).We also compare Iso-Dream++ with its predecessor in our conference paper (orange).</p>
<p>Fig. 9 :
9
Fig. 9: Examples of Iso-Dream++ without (top) and with (bottom) sparse dependency.The agent without sparse dependency tends to follow the vehicle in front of it when there are many vehicles in the way.In the bottom row, the agent overtakes and accelerates flexibly.</p>
<p>1 ∈
1
R + for collisions and a steering penalty steer ∈ [−1, 1] to facilitate lane-keeping.The hyper-parameters are set to ξ 1 = 10 −4</p>
<p>Fig. 10 :
10
Fig. 10: Video prediction results in CARLA.For each sequence, we use the first 5 images as context frames.The visual decoupled components (Rows 3, 5, 7) and masks (Rows 4, 6, 8) of each branch are presented.Iso-Dream++ successfully isolates noncontrollable dynamics from the complicated environment, i.e., other driving vehicles.</p>
<p>Fig. 11 :
11
Fig.11: The hyperparameter analysis of τ in CARLA.</p>
<p>Fig. 12 :
12
Fig. 12: The ablation study of the proposed variance constraints in DMC.We report the results averaged over 10 seeds.TABLE 1: Qualitative results in DMC.The agents are trained and evaluated in environments with video_easy background.* indicates a different setup from that of DBC.Iso-Dream (conf.) is the model from our conference paper, which only uses the reconstruction loss (w/o KL divergence) in the action-free branch.</p>
<p>Fig. 13 :
13
Fig. 13: Video prediction results from our approaches w/ and w/o the proposed variance constraints.</p>
<dl>
<dt>Fig. 14 :</dt>
<dt>14</dt>
<dt>Fig. 14: Video prediction results with noisy backgrounds in DMC.For each sequence, we use the first 5 images as context frames.Iso-Dream++ successfully disentangles controllable and noncontrollable components.</dt>
<dd>
<p>no pretrain Iso-Dream++: load action-free branch DreamerV2: no pretain DreamerV2: load all</p>
</dd>
</dl>
<p>Fig. 15 :
15
Fig. 15: Transfer learning results across Day and Night modes in DMC.Leveraging a pretrained Day-mode action-free branch can greatly benefit the finetuning results of Iso-Dream++ in the Night mode.Results are averaged over 10 seeds.</p>
<p>Fig.6: The agent learns future-dependent policies in world model's imaginations through a future state attention mechanism.window length from the present time, we explicitly compute the relations between them using the following equation:
1 r 1r t rattention mechanismconcatenation
t a t , v t a t , v t zt+τ zt+τ e t = softmax(s t zT t:t+τ ) zt:t+τ + st .</p>
<p>DreamerV2 755 ± 92 260 ± 366 655 ± 47 475 ± 159 DreamerV3 124 ± 52 472 ± 328 701 ± 114 546 ± 117 DreamerPro 721 ± 147 295 ± 129 813 ± 88 297 ± 63 Denoised-MDP 635 ± 284 104 ± 117 214 ± 56 233
MethodFinger SpinHopper StandWalker WalkCheetah RunSVEA562 ± 226 ± 8826 ± 65178 ± 64CURL280 ± 50 451 ± 250 443 ± 206 269 ± 24DBC*1 ± 25 ± 932 ± 715 ± 5
± 119 Iso-Dream (conf.)800 ± 59 746 ± 312 911 ± 50 659 ± 62 Iso-Dream 816 ± 16 769 ± 173 852 ± 97 597 ± 156 Iso-Dream++ 938 ± 51 877 ± 34 932 ± 37 639 ± 19</p>
<p>TABLE 2 :
2
The study of the generalization ability and robustness of the compared models to immediate visual distractions in DMC.
MethodFinger SpinHopper StandWalker WalkCheetah RunTrain: video easy; Test: video hardDreamerPro628 ± 151180 ± 96533 ± 212244 ± 27Denoised-MDP27 ± 2144 ± 25169 ± 61103 ± 46Iso-Dream++692 ± 185 643 ± 155 642 ± 129 441 ± 183Train: video easy; Test: video easy with Gaussian noisesDreamerPro663 ± 129223 ± 76824 ± 72263 ± 55Denoised-MDP652 ± 306103 ± 110180 ± 79195 ± 131Iso-Dream++851 ± 109806 ± 74906 ± 31582 ± 69
ACKNOWLEDGMENTSThis work was supported by the National Natural Science Foundation of China (Grant No. 62250062, 62106144, U19B2035), the Shanghai Municipal Science and Technology Major Project (Grant No. 2021SHZDZX0102), the Fundamental Research Funds for the Central Universities, and the Shanghai Sailing Program (Grant No. 21Z510202133).
Iso-dream: Isolating and leveraging noncontrollable visual dynamics in world models. M Pan, X Zhu, Y Wang, X Yang, NeurIPS2022191</p>
<p>Actionconditional video prediction using deep networks in atari games. J Oh, X Guo, H Lee, R L Lewis, S Singh, NeurIPS. 2015</p>
<p>Recurrent world models facilitate policy evolution. D Ha, J Schmidhuber, NeurIPS. 2018</p>
<p>Visual foresight: Model-based deep reinforcement learning for vision-based robotic control. F Ebert, C Finn, S Dasari, A Xie, A Lee, S Levine, arXiv:1812.005682018arXiv preprint</p>
<p>Value prediction network. J Oh, S Singh, H Lee, NeurIPS. 2017</p>
<p>Dream to control: Learning behaviors by latent imagination. D Hafner, T Lillicrap, J Ba, M Norouzi, ICLR2020</p>
<p>Planning to explore via self-supervised world models. R Sekar, O Rybkin, K Daniilidis, P Abbeel, D Hafner, D Pathak, ICML. 2020</p>
<p>Mastering atari with discrete world models. D Hafner, T P Lillicrap, M Norouzi, J Ba, ICLR2020</p>
<p>Model-based reinforcement learning for Atari. L Kaiser, M Babaeizadeh, P Milos, B Osinski, R H Campbell, K Czechowski, D Erhan, C Finn, P Kozakowski, S Levine, ICLR. 2020</p>
<p>Deep visual foresight for planning robot motion. C Finn, S Levine, ICRA. 2017</p>
<p>Learning latent dynamics for planning from pixels. D Hafner, T Lillicrap, I Fischer, R Villegas, D Ha, H Lee, J Davidson, ICML. 2019</p>
<p>Challenging common assumptions in the unsupervised learning of disentangled representations. F Locatello, S Bauer, M Lucic, G Raetsch, S Gelly, B Schölkopf, O Bachem, ICML. 2019</p>
<p>Momentum contrast for unsupervised visual representation learning. K He, H Fan, Y Wu, S Xie, R Girshick, CVPR. 2020</p>
<p>Unsupervised visual representation learning by online constrained k-means. Q Qian, Y Xu, J Hu, H Li, R Jin, CVPR. 202216649</p>
<p>Objectcentric learning with slot attention. F Locatello, D Weissenborn, T Unterthiner, A Mahendran, G Heigold, J Uszkoreit, A Dosovitskiy, T Kipf, NeurIPS. 112020</p>
<p>Recurrent independent mechanisms. A Goyal, A Lamb, J Hoffmann, S Sodhani, S Levine, Y Bengio, B Schölkopf, ICLR2021</p>
<p>Reinforcement learning: An introduction. R S Sutton, A G Barto, TNN. 1998</p>
<p>CARLA: an open urban driving simulator. A Dosovitskiy, G Ros, F Codevilla, A M López, V Koltun, 2017</p>
<p>Deepmind control suite. Y Tassa, Y Doron, A Muldal, T Erez, Y Li, D D L Casas, D Budden, A Abdolmaleki, J Merel, A Lefrancq, arXiv:1801.006902018arXiv preprint</p>
<p>Generalization in reinforcement learning by soft data augmentation. N Hansen, X Wang, ICRA. 202113617</p>
<p>Mastering diverse domains through world models. D Hafner, J Pasukonis, J Ba, T Lillicrap, arXiv:2301.041042023arXiv preprint</p>
<p>DreamerPro: Reconstructionfree model-based reinforcement learning with prototypical representations. F Deng, I Jang, S Ahn, ICML. 2022</p>
<p>CURL: contrastive unsupervised representations for reinforcement learning. M Laskin, A Srinivas, P Abbeel, ICML. 2020</p>
<p>Stabilizing deep qlearning with convnets and vision transformers under data augmentation. N Hansen, H Su, X Wang, NeurIPS. 2021</p>
<p>Soft actor-critic algorithms and applications. T Haarnoja, A Zhou, K Hartikainen, G Tucker, S Ha, J Tan, V Kumar, H Zhu, A Gupta, P Abbeel, arXiv:1812.059052018arXiv preprint</p>
<p>Learning invariant representations for reinforcement learning without reconstruction. A Zhang, R Mcallister, R Calandra, Y Gal, S Levine, ICLR2021</p>
<p>Denoised mdps: Learning world models better than the world itself. T Wang, S S Du, A Torralba, P Isola, A Zhang, Y Tian, ICML. 202222612</p>
<p>Improving sample efficiency in model-free reinforcement learning from images. D Yarats, A Zhang, I Kostrikov, B Amos, J Pineau, R Fergus, AAAI. 202110681</p>
<p>Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. D Yarats, I Kostrikov, R Fergus, ICLR2021</p>
<p>Reinforcement learning with augmented data. M Laskin, K Lee, A Stooke, L Pinto, P Abbeel, A Srinivas, NeurIPS202019895</p>
<p>Information prioritization through empowerment in visual model-based RL. H Bharadhwaj, M Babaeizadeh, D Erhan, S Levine, ICLR2022</p>
<p>Learning general world models in a handful of reward-free deployments. Y Xu, J Parker-Holder, A Pacchiano, P J Ball, O Rybkin, S J Roberts, T Rocktäschel, E Grefenstette, NeurIPS. 268382022</p>
<p>When to update your model: Constrained model-based reinforcement learning. T Ji, Y Luo, F Sun, M Jing, F He, W Huang, NeurIPS. 231632022</p>
<p>Planning in stochastic environments with a learned model. I Antonoglou, J Schrittwieser, S Ozair, T K Hubert, D Silver, ICLR2022</p>
<p>Unsupervised learning for physical interaction through video prediction. C Finn, I Goodfellow, S Levine, NeurIPS2016</p>
<p>. S Chiappa, S Racaniere, D Wierstra, S Mohamed, 2017Recurrent environment simulators," in ICLR</p>
<p>Predrnn: A recurrent neural network for spatiotemporal predictive learning. Y Wang, H Wu, J Zhang, Z Gao, J Wang, S Y Philip, M Long, TPAMI. 2022</p>
<p>Fitvid: Overfitting in pixel-level video prediction. M Babaeizadeh, M T Saffar, S Nair, S Levine, C Finn, D Erhan, arXiv:2106.131952021arXiv preprint</p>
<p>Goal-directed behavior under variational predictive coding: Dynamic organization of visual attention and working memory. M Jung, T Matsumoto, J Tani, IROS. 2019</p>
<p>Unsupervised learning of video representations using lstms. N Srivastava, E Mansimov, R Salakhudinov, ICML. 2015</p>
<p>Convolutional LSTM network: A machine learning approach for precipitation nowcasting. X Shi, Z Chen, H Wang, D.-Y Yeung, W.-K Wong, W.-C Woo, NeurIPS. 2015</p>
<p>Generating videos with scene dynamics. C Vondrick, H Pirsiavash, A Torralba, NeurIPS. 2016</p>
<p>Temporal coherency based criteria for predicting video frames using deep multi-stage generative adversarial networks. P Bhattacharjee, S Das, NeurIPS. 2017</p>
<p>Predrnn: Recurrent neural networks for predictive learning using spatiotemporal lstms. Y Wang, M Long, J Wang, Z Gao, P S Yu, NeurIPS. 2017</p>
<p>Learning to generate long-term future via hierarchical prediction. R Villegas, J Yang, Y Zou, S Sohn, X Lin, H Lee, ICML. 2017</p>
<p>Hierarchical long-term video prediction without supervision. N Wichers, R Villegas, D Erhan, H Lee, ICML. 2018</p>
<p>Sdc-net: Video prediction using spatially-displaced convolution. F A Reda, G Liu, K J Shih, R Kirby, J Barker, D Tarjan, A Tao, B Catanzaro, ECCV. 2018</p>
<p>Folded recurrent neural networks for future video prediction. M Oliu, J Selva, S Escalera, ECCV. 2018</p>
<p>Dyan: A dynamical atoms-based network for video prediction. W Liu, A Sharma, O Camps, M Sznaier, ECCV. 2018</p>
<p>Structure preserving video prediction. J Xu, B Ni, Z Li, S Cheng, X Yang, CVPR. 2018</p>
<p>Exploring spatial-temporal multi-frequency analysis for highfidelity and temporal-consistency video prediction. B Jin, Y Hu, Q Tang, J Niu, Z Shi, Y Han, X Li, CVPR. 2020</p>
<p>Unsupervised video representation learning by bidirectional feature prediction. N Behrmann, J Gall, M Noroozi, 2021WACV</p>
<p>Stochastic variational video prediction. M Babaeizadeh, C Finn, D Erhan, R H Campbell, S Levine, ICLR2018</p>
<p>Stochastic video generation with a learned prior. E Denton, R Fergus, ICML. 2018</p>
<p>High fidelity video prediction with large stochastic recurrent neural networks. R Villegas, A Pathak, H Kannan, D Erhan, Q V Le, H Lee, NeurIPS. 2019</p>
<p>Variational temporal abstraction. T Kim, S Ahn, Y Bengio, NeurIPS. 115792019</p>
<p>Improved conditional VRNNs for video prediction. L Castrejon, N Ballas, A Courville, ICCV. 2019</p>
<p>Stochastic latent residual video prediction. J.-Y Franceschi, E Delasalles, M Chen, S Lamprier, P Gallinari, ICML. 2020</p>
<p>Greedy hierarchical variational autoencoders for large-scale video prediction. B Wu, S Nair, R Martin-Martin, L Fei-Fei, C Finn, CVPR. 2021</p>
<p>Relational neural expectation maximization: Unsupervised discovery of objects and their interactions. S Van Steenkiste, M Chang, K Greff, J Schmidhuber, ICLR2018</p>
<p>Learning to decompose and disentangle representations for video prediction. J.-T Hsieh, B Liu, D.-A Huang, L F Fei-Fei, J C Niebles, NeurIPS. 2018</p>
<p>Multi-object representation learning with iterative variational inference. K Greff, R L Kaufman, R Kabra, N Watters, C Burgess, D Zoran, L Matthey, M Botvinick, A Lerchner, ICML. 2019</p>
<p>Unsupervised video decomposition using spatio-temporal iterative inference. P Zablotskaia, E A Dominici, L Sigal, A M Lehrmann, arXiv:2006.147272020arXiv preprint</p>
<p>Learning semantic-aware dynamics for video prediction. X Bei, Y Yang, S Soatto, CVPR. 2021</p>
<p>Neural expectation maximization. K Greff, S Van Steenkiste, J Schmidhuber, NeurIPS. 2017</p>
<p>Sequential attend, infer, repeat: generative modelling of moving objects. A R Kosiorek, H Kim, I Posner, Y W Teh, NeurIPS. 2018</p>
<p>Decomposing motion and content for natural video sequence prediction. R Villegas, J Yang, S Hong, X Lin, H Lee, ICLR2017</p>
<p>Disentangling physical dynamics from unknown factors for unsupervised video prediction. V L Guen, N Thome, CVPR. 202011484</p>
<p>Hierarchical video prediction using relational layouts for human-object interactions. N Bodla, G Shrivastava, R Chellappa, A Shrivastava, CVPR. 2021155</p>            </div>
        </div>

    </div>
</body>
</html>
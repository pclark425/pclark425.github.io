<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9435 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9435</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9435</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-53d8b356551a2361020a948f64454a6d599af69f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/53d8b356551a2361020a948f64454a6d599af69f" target="_blank">Prefix-Tuning: Optimizing Continuous Prompts for Generation</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> Prefix-tuning is proposed, a lightweight alternative to fine- Tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which is called the prefix.</p>
                <p><strong>Paper Abstract:</strong> Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were “virtual tokens”. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9435.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9435.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prefix-tuning (start-prefix)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prefix-tuning with prefix prepended to input (start of sequence)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Optimize a sequence of continuous, task-specific activation vectors prepended to the model input (a 'virtual token' prefix), keeping the LM parameters frozen; used to steer generation for conditional tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>MEDIUM / LARGE</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Table-to-text (E2E, WebNLG, DART)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Conditional natural language generation: generate textual descriptions from a linearized structured table (entity/relation triples or field:value lists).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Continuous learned prefix vectors prepended to the entire input context ([Prefix; x; y]); prefix length tuned (default small lengths like 5-10 for table-to-text). Prefix parameters only (0.1% of model parameters) are trained; LM parameters frozen.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Full fine-tuning (update all LM parameters), adapter-tuning (inserting small task-specific modules), FT-TOP2 (fine-tune top 2 layers).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>E2E (GPT-2 MEDIUM): Prefix (0.1%) BLEU = 74.8 (vs FT-FULL BLEU = 74.2); WebNLG: Prefix BLEU = 64.52 (FT-FULL = 66.03); DART: Prefix BLEU = 51.11 (FT-FULL = 50.46).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Compared to ADAPTER(0.1%) and FT-TOP2, prefix-tuning achieves comparable or better scores while tuning far fewer parameters (0.1% vs 3.0% or 100%).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Small differences vs full fine-tuning: +0.6 BLEU on E2E, -1.5 BLEU on WebNLG, +0.65 BLEU on DART (values reported above).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The prefix acts as a learned context that steers pretrained LM behavior without modifying LM weights, preserving pretrained inductive bias and enabling parameter-efficient adaptation; keeping LM frozen may help extrapolation to unseen topics.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Prefix length tuned (E2E/WebNLG used small lengths like 5; DART used 10); training used GPT-2 MEDIUM and GPT-2_LARGE; prefixes required storing ~0.1% of parameters (e.g., 250K–500K parameters); decoding used beam search (beam=5).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9435.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9435.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prefix-tuning (encoder-decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prefix-tuning applied to encoder-decoder models (prefixes on encoder and decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Apply learned continuous prefixes to encoder and/or decoder inputs of encoder-decoder LMs (BART) while keeping model weights frozen; used for summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>LARGE</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Abstractive summarization (XSUM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate an abstractive one-sentence summary from a news article.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Continuous learned prefix vectors prepended to encoder and decoder contexts; experiments with different overall parameter budgets (e.g., 2% and 0.1% of parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Full fine-tuning of BART (update all parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>With ~2% of parameters in prefixes: ROUGE-L ≈ 36.05 (prefix-tuning) vs ROUGE-L ≈ 37.25 (full fine-tuning). With 0.1% parameters prefix-tuning performs lower (ROUGE-L ≈ 35.05 vs 37.25).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Prefix-tuning underperforms full fine-tuning on XSUM in full-data setting by ~1.2 ROUGE-L points at 2% parameter budget and larger gap at 0.1%.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>-1.2 ROUGE-L (2% prefix vs full fine-tuning) as reported; larger negative gap at 0.1% budget.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Summarization is a more complex task (longer inputs, more content selection) and XSUM articles are much longer (avg 17x longer than table linearizations), so the expressivity of small prefixes may be insufficient relative to fine-tuning for this task/data scale.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>XSUM: ~225K examples; inputs truncated to 512 BPE tokens; prefix lengths tuned up to 200 (best often large, e.g., 100); training used BART_LARGE; decoding used beam size 6 and length normalization 0.8.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9435.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9435.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Embedding-only</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embedding-only tuning (optimize only input token embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Make the word embedding vectors of a sequence of virtual tokens trainable while leaving deeper activations produced by the Transformer unchanged; a restricted continuous prompt variant.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>MEDIUM</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Intrinsic table-to-text evaluation (same generation tasks used elsewhere)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Intrinsic comparison of prompt parameterizations for generation quality (BLEU, ROUGE, CIDEr, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Tune only the embedding vectors (first layer) of virtual tokens inserted before the input, with varying 'prefix lengths' (e.g., EmB-1, EmB-10, EmB-20).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Full prefix-tuning (optimize activations at all layers, i.e., MLP-reparametrized P_theta), and infix-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported intrinsic metrics (table): Prefix: BLEU=70.3; EmB-1 BLEU=48.1; EmB-10 BLEU=62.2; EmB-20 BLEU=61.9 (other metrics similarly lower).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Embedding-only underperforms full prefix-tuning by substantial margins (e.g., ~8–22 BLEU points for listed configurations), indicating embedding-only is less expressive.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Large negative effect vs full prefix: e.g., EmB-1 -22.2 BLEU; EmB-10 -8.1 BLEU (relative to Prefix BLEU=70.3 in that intrinsic eval table).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Tuning only the embedding layer cannot directly modify deeper-layer activations; prefix-tuning's ability to affect representations at all layers makes it strictly more expressive than embedding-only and discrete prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Embedding-only variants tested with prefix lengths 1, 10, 20; MLP reparametrization used for full prefix; intrinsic evaluation metrics include BLEU, NIST, METEOR, ROUGE, CIDEr (see Table 5).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9435.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9435.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Infix-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Infix-tuning (trainable activations placed between input and output)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Place the trainable continuous activations between the input x and the target y ([x; Infix; y]) so they only appear between encoder and decoder contexts (or between input and output in autoregressive setting).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>MEDIUM</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Table-to-text intrinsic evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Compare placement of trainable activations (prefix vs infix) and measure generation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Train continuous activation blocks positioned after the input but before the output (infix) rather than at the beginning of the context.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Prefix-tuning (trainable prefix at start).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Intrinsic metrics (Table 5): Infix-1 BLEU=67.9, Infix-10 BLEU=67.2, Infix-20 BLEU=66.7 compared to Prefix BLEU=70.3.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Infix-tuning slightly underperforms prefix-tuning by ~2.4–3.6 BLEU points in the intrinsic evaluation table.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>-2.4 to -3.6 BLEU (infix vs prefix in the reported intrinsic eval).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Prefix-tuning can influence activations of both x and later tokens y (it lives in the left context of the input), whereas infix-tuning can only directly influence y-generation; thus prefix placement yields more influence over both encoding and decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Infix variants tested with lengths 1, 10, 20; metrics reported include BLEU, NIST, METEOR, ROUGE, CIDEr (Table 5).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9435.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9435.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prefix length effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of learned prefix length on performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Varying the number of learned prefix positions affects generalization: performance increases with prefix length up to a dataset-specific threshold, after which test performance slightly drops (overfitting) though training loss continues to decrease.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART / GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>LARGE (BART) / MEDIUM (GPT-2)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Summarization (XSUM) and Table-to-text (E2E/WebNLG)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Measure generation quality as prefix length varies over a wide range (small to large).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Learned prefix vectors of varying lengths (e.g., table-to-text: tested up to 10; summarization: tested up to 300), inserted at start of context.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Observed trend: increasing prefix length improves performance up to a threshold (approx. 10 tokens for table-to-text, 200 tokens for summarization), beyond which test performance slightly falls.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Not reported as a single scalar; qualitative thresholds: ~10 (table-to-text) and ~200 (summarization) found to be near-optimal in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Longer prefixes increase capacity and reduce training loss but can overfit, resulting in slightly worse test performance past a dataset-specific optimum.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Prefix lengths searched: table-to-text {1,5,10,20,40}; summarization {1,10,20,50,80,100,200,300}; default summarization prefix length used in some experiments was 100.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9435.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9435.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Initialization (real-word activations)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Initialization of prefix with activations of real words vs random</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Initializing the learnable prefix activations to the LM's internal activations of real words substantially stabilizes and improves low-data learning compared to random initialization; task-relevant word initializations slightly outperform unrelated words.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 / BART</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>MEDIUM / LARGE</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Low-data table-to-text (E2E) and summarization (XSUM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Small-sample training regimes (e.g., training sizes 50, 100, 200, 500; and low-percentage subsets of full dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Prefixes are initialized by copying activations computed by the LM for one or more real tokens (e.g., 'summarize', 'table-to-text:') rather than random values.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Random initialization (uniform) of prefix activations; also compared different initializing words (task-relevant vs irrelevant).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>In low-data settings (e.g., 100 examples), initialization with real-word activations yields significantly higher and less variable generation scores than random initialization; task-relevant words slightly better than irrelevant.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Random init: low performance with high variance; real-word init: significantly improved performance (visualized in Figure 5 and Figure 8). In full-data settings initialization has no significant effect.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Quantitative effect sizes not given as a single scalar across tasks; figures show substantial gaps (visual) in low-data regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Initializing with activations from real tokens places the prefix in a region of activation space more congenial to the pretrained LM, stabilizing optimization and leveraging pretrained representations; in full-data settings the model can overcome random init.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Low-data experiments: sampled training sizes {50,100,200,500}, averaged over 5 dataset samples and 2 seeds (10 models per point); for table-to-text used prefix length matching BPE tokenization of initializer (e.g., 6 for 'table-to-text:'); early stopping and dev splits used.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9435.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9435.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Low-data comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prefix-tuning vs. full fine-tuning in low-data regimes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical comparison showing prefix-tuning often outperforms full fine-tuning when training data is scarce, while the gap narrows with more data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (table-to-text) and BART (summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>MEDIUM / LARGE</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Low-data Table-to-text (E2E) and Summarization (XSUM) subsamples</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Subsampled training sets of sizes {50,100,200,500} and percentage-subsamples (10%-80%) for data-efficiency experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Prefix-tuning as the adaptation mechanism (learned continuous prefix); compared against full fine-tuning across low-data settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Full fine-tuning (update all model parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On table-to-text low-data experiments, prefix-tuning outperforms fine-tuning by 2.9 BLEU on average across subsampled settings reported; figure-based results also show prefix superior at small sample sizes. For percentage-scale E2E experiments, prefix-tuning is more data-efficient when using >20% of data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Average +2.9 BLEU (prefix vs fine-tuning) reported in low-data regimes; gap decreases as dataset size increases.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+2.9 BLEU on average across low-data table-to-text subsampled settings (reported).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Freezing LM parameters preserves pretrained bias and prevents overfitting when task data is scarce; learning a small prefix provides a strong inductive bias and fewer parameters to fit.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Low-data sampling: for each size sampled 5 datasets and averaged over 2 training seeds (10 models per point); used dev splits (30% of training size) for early stopping; low-data hyperparams adjusted (learning rate 5e-5, prefix length small unless initialization used).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9435.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9435.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>In-context learning (GPT-3 mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 style in-context learning (manual prompts + few examples)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting a large autoregressive LM by prepending natural language instructions and a few examples to the context to induce task behavior, without updating model parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General few-shot prompting / in-context learning (discussion)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Few-shot adaptation by providing natural language instructions and exemplars in the context window to steer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Manual natural language prompts and a few examples prepended to the input; limited by context/window length (e.g., 2048 tokens for GPT-3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Contrasted with prefix-tuning and full fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not experimentally measured in this paper; cited as prior work demonstrating few-shot task performance without parameter updates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>In-context learning avoids parameter updates but is limited by the finite context window, restricting effective training set size for examples placed in-context.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Context window length limit noted (e.g., 2048 tokens for GPT-3) constrains in-context learning to very small training sets.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Autoprompt: Eliciting knowledge from language models with automatically generated prompts <em>(Rating: 2)</em></li>
                <li>P-tuning <em>(Rating: 2)</em></li>
                <li>The power of scale for parameter-efficient prompt tuning <em>(Rating: 2)</em></li>
                <li>Learning how to ask: Querying LMs with mixtures of soft prompts <em>(Rating: 2)</em></li>
                <li>Parameter-efficient transfer learning for NLP <em>(Rating: 2)</em></li>
                <li>Intrinsic dimensionality explains the effectiveness of language model fine-tuning <em>(Rating: 1)</em></li>
                <li>Plug and play language models: A simple approach to controlled text generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9435",
    "paper_id": "paper-53d8b356551a2361020a948f64454a6d599af69f",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "Prefix-tuning (start-prefix)",
            "name_full": "Prefix-tuning with prefix prepended to input (start of sequence)",
            "brief_description": "Optimize a sequence of continuous, task-specific activation vectors prepended to the model input (a 'virtual token' prefix), keeping the LM parameters frozen; used to steer generation for conditional tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2",
            "model_size": "MEDIUM / LARGE",
            "task_name": "Table-to-text (E2E, WebNLG, DART)",
            "task_description": "Conditional natural language generation: generate textual descriptions from a linearized structured table (entity/relation triples or field:value lists).",
            "presentation_format": "Continuous learned prefix vectors prepended to the entire input context ([Prefix; x; y]); prefix length tuned (default small lengths like 5-10 for table-to-text). Prefix parameters only (0.1% of model parameters) are trained; LM parameters frozen.",
            "comparison_format": "Full fine-tuning (update all LM parameters), adapter-tuning (inserting small task-specific modules), FT-TOP2 (fine-tune top 2 layers).",
            "performance": "E2E (GPT-2 MEDIUM): Prefix (0.1%) BLEU = 74.8 (vs FT-FULL BLEU = 74.2); WebNLG: Prefix BLEU = 64.52 (FT-FULL = 66.03); DART: Prefix BLEU = 51.11 (FT-FULL = 50.46).",
            "performance_comparison": "Compared to ADAPTER(0.1%) and FT-TOP2, prefix-tuning achieves comparable or better scores while tuning far fewer parameters (0.1% vs 3.0% or 100%).",
            "format_effect_size": "Small differences vs full fine-tuning: +0.6 BLEU on E2E, -1.5 BLEU on WebNLG, +0.65 BLEU on DART (values reported above).",
            "explanation_or_hypothesis": "The prefix acts as a learned context that steers pretrained LM behavior without modifying LM weights, preserving pretrained inductive bias and enabling parameter-efficient adaptation; keeping LM frozen may help extrapolation to unseen topics.",
            "null_or_negative_result": false,
            "experimental_details": "Prefix length tuned (E2E/WebNLG used small lengths like 5; DART used 10); training used GPT-2 MEDIUM and GPT-2_LARGE; prefixes required storing ~0.1% of parameters (e.g., 250K–500K parameters); decoding used beam search (beam=5).",
            "uuid": "e9435.0"
        },
        {
            "name_short": "Prefix-tuning (encoder-decoder)",
            "name_full": "Prefix-tuning applied to encoder-decoder models (prefixes on encoder and decoder)",
            "brief_description": "Apply learned continuous prefixes to encoder and/or decoder inputs of encoder-decoder LMs (BART) while keeping model weights frozen; used for summarization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BART",
            "model_size": "LARGE",
            "task_name": "Abstractive summarization (XSUM)",
            "task_description": "Generate an abstractive one-sentence summary from a news article.",
            "presentation_format": "Continuous learned prefix vectors prepended to encoder and decoder contexts; experiments with different overall parameter budgets (e.g., 2% and 0.1% of parameters).",
            "comparison_format": "Full fine-tuning of BART (update all parameters).",
            "performance": "With ~2% of parameters in prefixes: ROUGE-L ≈ 36.05 (prefix-tuning) vs ROUGE-L ≈ 37.25 (full fine-tuning). With 0.1% parameters prefix-tuning performs lower (ROUGE-L ≈ 35.05 vs 37.25).",
            "performance_comparison": "Prefix-tuning underperforms full fine-tuning on XSUM in full-data setting by ~1.2 ROUGE-L points at 2% parameter budget and larger gap at 0.1%.",
            "format_effect_size": "-1.2 ROUGE-L (2% prefix vs full fine-tuning) as reported; larger negative gap at 0.1% budget.",
            "explanation_or_hypothesis": "Summarization is a more complex task (longer inputs, more content selection) and XSUM articles are much longer (avg 17x longer than table linearizations), so the expressivity of small prefixes may be insufficient relative to fine-tuning for this task/data scale.",
            "null_or_negative_result": true,
            "experimental_details": "XSUM: ~225K examples; inputs truncated to 512 BPE tokens; prefix lengths tuned up to 200 (best often large, e.g., 100); training used BART_LARGE; decoding used beam size 6 and length normalization 0.8.",
            "uuid": "e9435.1"
        },
        {
            "name_short": "Embedding-only",
            "name_full": "Embedding-only tuning (optimize only input token embeddings)",
            "brief_description": "Make the word embedding vectors of a sequence of virtual tokens trainable while leaving deeper activations produced by the Transformer unchanged; a restricted continuous prompt variant.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2",
            "model_size": "MEDIUM",
            "task_name": "Intrinsic table-to-text evaluation (same generation tasks used elsewhere)",
            "task_description": "Intrinsic comparison of prompt parameterizations for generation quality (BLEU, ROUGE, CIDEr, etc.).",
            "presentation_format": "Tune only the embedding vectors (first layer) of virtual tokens inserted before the input, with varying 'prefix lengths' (e.g., EmB-1, EmB-10, EmB-20).",
            "comparison_format": "Full prefix-tuning (optimize activations at all layers, i.e., MLP-reparametrized P_theta), and infix-tuning.",
            "performance": "Reported intrinsic metrics (table): Prefix: BLEU=70.3; EmB-1 BLEU=48.1; EmB-10 BLEU=62.2; EmB-20 BLEU=61.9 (other metrics similarly lower).",
            "performance_comparison": "Embedding-only underperforms full prefix-tuning by substantial margins (e.g., ~8–22 BLEU points for listed configurations), indicating embedding-only is less expressive.",
            "format_effect_size": "Large negative effect vs full prefix: e.g., EmB-1 -22.2 BLEU; EmB-10 -8.1 BLEU (relative to Prefix BLEU=70.3 in that intrinsic eval table).",
            "explanation_or_hypothesis": "Tuning only the embedding layer cannot directly modify deeper-layer activations; prefix-tuning's ability to affect representations at all layers makes it strictly more expressive than embedding-only and discrete prompts.",
            "null_or_negative_result": true,
            "experimental_details": "Embedding-only variants tested with prefix lengths 1, 10, 20; MLP reparametrization used for full prefix; intrinsic evaluation metrics include BLEU, NIST, METEOR, ROUGE, CIDEr (see Table 5).",
            "uuid": "e9435.2"
        },
        {
            "name_short": "Infix-tuning",
            "name_full": "Infix-tuning (trainable activations placed between input and output)",
            "brief_description": "Place the trainable continuous activations between the input x and the target y ([x; Infix; y]) so they only appear between encoder and decoder contexts (or between input and output in autoregressive setting).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2",
            "model_size": "MEDIUM",
            "task_name": "Table-to-text intrinsic evaluation",
            "task_description": "Compare placement of trainable activations (prefix vs infix) and measure generation metrics.",
            "presentation_format": "Train continuous activation blocks positioned after the input but before the output (infix) rather than at the beginning of the context.",
            "comparison_format": "Prefix-tuning (trainable prefix at start).",
            "performance": "Intrinsic metrics (Table 5): Infix-1 BLEU=67.9, Infix-10 BLEU=67.2, Infix-20 BLEU=66.7 compared to Prefix BLEU=70.3.",
            "performance_comparison": "Infix-tuning slightly underperforms prefix-tuning by ~2.4–3.6 BLEU points in the intrinsic evaluation table.",
            "format_effect_size": "-2.4 to -3.6 BLEU (infix vs prefix in the reported intrinsic eval).",
            "explanation_or_hypothesis": "Prefix-tuning can influence activations of both x and later tokens y (it lives in the left context of the input), whereas infix-tuning can only directly influence y-generation; thus prefix placement yields more influence over both encoding and decoding.",
            "null_or_negative_result": true,
            "experimental_details": "Infix variants tested with lengths 1, 10, 20; metrics reported include BLEU, NIST, METEOR, ROUGE, CIDEr (Table 5).",
            "uuid": "e9435.3"
        },
        {
            "name_short": "Prefix length effect",
            "name_full": "Effect of learned prefix length on performance",
            "brief_description": "Varying the number of learned prefix positions affects generalization: performance increases with prefix length up to a dataset-specific threshold, after which test performance slightly drops (overfitting) though training loss continues to decrease.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BART / GPT-2",
            "model_size": "LARGE (BART) / MEDIUM (GPT-2)",
            "task_name": "Summarization (XSUM) and Table-to-text (E2E/WebNLG)",
            "task_description": "Measure generation quality as prefix length varies over a wide range (small to large).",
            "presentation_format": "Learned prefix vectors of varying lengths (e.g., table-to-text: tested up to 10; summarization: tested up to 300), inserted at start of context.",
            "comparison_format": null,
            "performance": "Observed trend: increasing prefix length improves performance up to a threshold (approx. 10 tokens for table-to-text, 200 tokens for summarization), beyond which test performance slightly falls.",
            "performance_comparison": null,
            "format_effect_size": "Not reported as a single scalar; qualitative thresholds: ~10 (table-to-text) and ~200 (summarization) found to be near-optimal in experiments.",
            "explanation_or_hypothesis": "Longer prefixes increase capacity and reduce training loss but can overfit, resulting in slightly worse test performance past a dataset-specific optimum.",
            "null_or_negative_result": null,
            "experimental_details": "Prefix lengths searched: table-to-text {1,5,10,20,40}; summarization {1,10,20,50,80,100,200,300}; default summarization prefix length used in some experiments was 100.",
            "uuid": "e9435.4"
        },
        {
            "name_short": "Initialization (real-word activations)",
            "name_full": "Initialization of prefix with activations of real words vs random",
            "brief_description": "Initializing the learnable prefix activations to the LM's internal activations of real words substantially stabilizes and improves low-data learning compared to random initialization; task-relevant word initializations slightly outperform unrelated words.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 / BART",
            "model_size": "MEDIUM / LARGE",
            "task_name": "Low-data table-to-text (E2E) and summarization (XSUM)",
            "task_description": "Small-sample training regimes (e.g., training sizes 50, 100, 200, 500; and low-percentage subsets of full dataset).",
            "presentation_format": "Prefixes are initialized by copying activations computed by the LM for one or more real tokens (e.g., 'summarize', 'table-to-text:') rather than random values.",
            "comparison_format": "Random initialization (uniform) of prefix activations; also compared different initializing words (task-relevant vs irrelevant).",
            "performance": "In low-data settings (e.g., 100 examples), initialization with real-word activations yields significantly higher and less variable generation scores than random initialization; task-relevant words slightly better than irrelevant.",
            "performance_comparison": "Random init: low performance with high variance; real-word init: significantly improved performance (visualized in Figure 5 and Figure 8). In full-data settings initialization has no significant effect.",
            "format_effect_size": "Quantitative effect sizes not given as a single scalar across tasks; figures show substantial gaps (visual) in low-data regimes.",
            "explanation_or_hypothesis": "Initializing with activations from real tokens places the prefix in a region of activation space more congenial to the pretrained LM, stabilizing optimization and leveraging pretrained representations; in full-data settings the model can overcome random init.",
            "null_or_negative_result": false,
            "experimental_details": "Low-data experiments: sampled training sizes {50,100,200,500}, averaged over 5 dataset samples and 2 seeds (10 models per point); for table-to-text used prefix length matching BPE tokenization of initializer (e.g., 6 for 'table-to-text:'); early stopping and dev splits used.",
            "uuid": "e9435.5"
        },
        {
            "name_short": "Low-data comparison",
            "name_full": "Prefix-tuning vs. full fine-tuning in low-data regimes",
            "brief_description": "Empirical comparison showing prefix-tuning often outperforms full fine-tuning when training data is scarce, while the gap narrows with more data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (table-to-text) and BART (summarization)",
            "model_size": "MEDIUM / LARGE",
            "task_name": "Low-data Table-to-text (E2E) and Summarization (XSUM) subsamples",
            "task_description": "Subsampled training sets of sizes {50,100,200,500} and percentage-subsamples (10%-80%) for data-efficiency experiments.",
            "presentation_format": "Prefix-tuning as the adaptation mechanism (learned continuous prefix); compared against full fine-tuning across low-data settings.",
            "comparison_format": "Full fine-tuning (update all model parameters).",
            "performance": "On table-to-text low-data experiments, prefix-tuning outperforms fine-tuning by 2.9 BLEU on average across subsampled settings reported; figure-based results also show prefix superior at small sample sizes. For percentage-scale E2E experiments, prefix-tuning is more data-efficient when using &gt;20% of data.",
            "performance_comparison": "Average +2.9 BLEU (prefix vs fine-tuning) reported in low-data regimes; gap decreases as dataset size increases.",
            "format_effect_size": "+2.9 BLEU on average across low-data table-to-text subsampled settings (reported).",
            "explanation_or_hypothesis": "Freezing LM parameters preserves pretrained bias and prevents overfitting when task data is scarce; learning a small prefix provides a strong inductive bias and fewer parameters to fit.",
            "null_or_negative_result": false,
            "experimental_details": "Low-data sampling: for each size sampled 5 datasets and averaged over 2 training seeds (10 models per point); used dev splits (30% of training size) for early stopping; low-data hyperparams adjusted (learning rate 5e-5, prefix length small unless initialization used).",
            "uuid": "e9435.6"
        },
        {
            "name_short": "In-context learning (GPT-3 mention)",
            "name_full": "GPT-3 style in-context learning (manual prompts + few examples)",
            "brief_description": "Prompting a large autoregressive LM by prepending natural language instructions and a few examples to the context to induce task behavior, without updating model parameters.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_size": "175B",
            "task_name": "General few-shot prompting / in-context learning (discussion)",
            "task_description": "Few-shot adaptation by providing natural language instructions and exemplars in the context window to steer generation.",
            "presentation_format": "Manual natural language prompts and a few examples prepended to the input; limited by context/window length (e.g., 2048 tokens for GPT-3).",
            "comparison_format": "Contrasted with prefix-tuning and full fine-tuning.",
            "performance": "Not experimentally measured in this paper; cited as prior work demonstrating few-shot task performance without parameter updates.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "In-context learning avoids parameter updates but is limited by the finite context window, restricting effective training set size for examples placed in-context.",
            "null_or_negative_result": null,
            "experimental_details": "Context window length limit noted (e.g., 2048 tokens for GPT-3) constrains in-context learning to very small training sets.",
            "uuid": "e9435.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "Autoprompt: Eliciting knowledge from language models with automatically generated prompts",
            "rating": 2
        },
        {
            "paper_title": "P-tuning",
            "rating": 2
        },
        {
            "paper_title": "The power of scale for parameter-efficient prompt tuning",
            "rating": 2
        },
        {
            "paper_title": "Learning how to ask: Querying LMs with mixtures of soft prompts",
            "rating": 2
        },
        {
            "paper_title": "Parameter-efficient transfer learning for NLP",
            "rating": 2
        },
        {
            "paper_title": "Intrinsic dimensionality explains the effectiveness of language model fine-tuning",
            "rating": 1
        },
        {
            "paper_title": "Plug and play language models: A simple approach to controlled text generation",
            "rating": 1
        }
    ],
    "cost": 0.017879,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Prefix-Tuning: Optimizing Continuous Prompts for Generation</h1>
<p>Xiang Lisa Li<br>Stanford University<br>xlisali@stanford.edu</p>
<h2>Percy Liang</h2>
<p>Stanford University
pliang@cs.stanford.edu</p>
<h2>Abstract</h2>
<p>Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were "virtual tokens". We apply prefix-tuning to GPT-2 for table-totext generation and to BART for summarization. We show that by modifying only $0.1 \%$ of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.</p>
<h2>1 Introduction</h2>
<p>Fine-tuning is the prevalent paradigm for using large pretrained language models (LMs) (Radford et al., 2019; Devlin et al., 2019) to perform downstream tasks (e.g., summarization), but it requires updating and storing all the parameters of the LM. Consequently, to build and deploy NLP systems that rely on large pretrained LMs, one currently needs to store a modified copy of all the LM parameters for each task. This can be prohibitively expensive given the size of current LMs; for example, GPT-2 has 774M parameters (Radford et al., 2019) and GPT-3 has 175B parameters (Brown et al., 2020).</p>
<p>A natural approach to this problem is lightweight fine-tuning, which freezes most of the pretrained parameters and only tunes a smaller set of parameters. For example, adapter-tuning (Rebuffi et al.,</p>
<p>Figure 1: Fine-tuning (top) updates all LM parameters (the red Transformer box) and requires storing a full model copy for each task. We propose prefixtuning (bottom), which freezes the LM parameters and only optimizes the prefix (the red prefix blocks). Consequently, we only need to store the prefix for each task, making prefix-tuning modular and space-efficient. Note that each vertical block denote transformer activations at one time step.</p>
<p>2017; Houlsby et al., 2019) inserts additional taskspecific layers between the layers of pretrained language models. Adapter-tuning has promising performance on natural language understanding and generation benchmarks, attaining comparable performance with fine-tuning while adding only around $2-4 \%$ task-specific parameters (Houlsby et al., 2019; Lin et al., 2020).</p>
<p>At the limit, GPT-3 (Brown et al., 2020) can be deployed using in-context learning, which is a form of prompting, without modifying any LM parameters. In in-context learning, Brown et al. (2020) prepend a natural language task instruction (e.g., $T L ; D R$ for summarization) and a few examples to the task input, and then generate the task output from the LM. However, since Transformers can only condition on a bounded-length context (e.g., 2048 tokens for GPT-3), in-context learning is restricted to very small training sets.</p>
<p>In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation (NLG) tasks, inspired by prompting. Consider the task of generating a textual description of a data table, as shown in Figure 1, where the task input is a linearized table (e.g., "name: Starbucks | type: coffee shop") and the output is a textual description (e.g., "Starbucks serves coffee."). Prefix-tuning prepends a sequence of continuous task-specific vectors to the input, which we call a prefix, depicted by red blocks in Figure 1 (bottom). To generate each token, the LM can attend to the prefix as if it were a sequence of "virtual tokens", but unlike prompting, the prefix consists entirely of free parameters which do not correspond to real tokens. In contrast to fine-tuning in Figure 1 (top), which updates all LM parameters and thus requires storing a tuned copy of the model for each task, prefix-tuning only optimizes the prefix. Consequently, we only need to store one copy of the large LM and a learned task-specific prefix, yielding a very small overhead for each additional task (e.g., 250 K parameters for table-to-text).</p>
<p>In contrast to full fine-tuning, prefix-tuning is also modular: we train an upstream prefix which steers an unmodified LM, and therefore, a single LM can support many tasks at once. In the context of personalization where the tasks correspond to users (Shokri and Shmatikov, 2015; McMahan et al., 2016), we would have a separate prefix for each user trained only on that user's data, thereby avoiding data cross-contamination. Moreover, the prefix-based architecture enables us to even process examples from multiple users/tasks in a single batch, something that is not possible with other lightweight fine-tuning approaches like adaptertuning.</p>
<p>We evaluate prefix-tuning on table-to-text generation using GPT-2 and abstractive summarization using BART. In terms of storage, prefix-tuning stores 1000x fewer parameters than full fine-tuning. In terms of performance when trained on full datasets, prefix-tuning and fine-tuning are comparable for table-to-text ( $\S 6.1$ ), while prefix-tuning suffers a small degradation for summarization (§6.2). In low-data settings, prefix-tuning outperforms finetuning on both tasks (§6.3). Prefix-tuning also extrapolates better to tables (for table-to-text) and articles (for summarization) with unseen topics (§6.4).</p>
<h2>2 Related Work</h2>
<p>Fine-tuning for natural language generation. Current state-of-the-art systems for natural language generation (NLG) are based on fine-tuning pretrained LMs. For table-to-text generation, Kale (2020) fine-tunes a sequence-to-sequence model (T5; Raffel et al., 2020). For extractive and abstractive summarization, researchers fine-tune masked language models (e.g., BERT; Devlin et al., 2019) and encode-decoder models (e.g., BART; Lewis et al., 2020), respectively (Zhong et al., 2020; Liu and Lapata, 2019; Raffel et al., 2020). For other conditional NLG tasks such as machine translation and dialogue generation, fine-tuning is also the prevalent paradigm (Zhang et al., 2020c; Stickland et al., 2020; Zhu et al., 2020; Liu et al., 2020). In this paper, we focus on table-to-text using GPT-2 and summarization using BART, but prefix-tuning in principle can be applied to other generation tasks and pretrained models, such as masked LMs.</p>
<p>Lightweight fine-tuning. Prefix-tuning falls under the broad class of lightweight fine-tuning methods, which freeze most of the pretrained parameters and only tune a smaller set of parameters. The key question is how to augment the LM architecture and decide which subset of pretrained parameters to tune. One line of research learns a task-specific parameter mask (Zhao et al., 2020; Radiya-Dixit and Wang, 2020). Another line of research inserts new modules with trainable parameters. For example, Zhang et al. (2020a) trains a "side" network that is fused with the pretrained model via summation; adapter-tuning inserts task-specific layers (adapters) between each layer of the pretrained LM (Houlsby et al., 2019; Lin et al., 2020; Rebuffi et al., 2017; Pfeiffer et al., 2020). Compared to this line of work, which tunes around $3.6 \%$ of the LM parameters, our method obtains a further 30x reduction in task-specific parameters, tuning only $0.1 \%$ while maintaining comparable performance on table-to-text tasks.</p>
<p>Prompting. Prompting is a way of leveraging a pretrained LM by prepending instructions and a few examples to the task input and generating the task output from the LM. For autoregressive LMs, the most successful form of prompting is GPT-3's in-context learning (Brown et al., 2020), which uses manually designed prompts to adapt its generation for different tasks in few-shot settings. For masked LMs like BERT and RoBERTa (Liu et al.,</p>
<p>2019), prompt engineering has been explored for natural language understanding tasks (Jiang et al., 2020; Schick and Schütze, 2020). For example, AutoPrompt (Shin et al., 2020) searches for a sequence of discrete trigger words and concatenates it with each input to elicit sentiment or factual knowledge from BERT and RoBERTa. In contrast with AutoPrompt, our method optimizes continuous prefixes, which are more expressive (§7.2); moreover, we focus on language generation tasks.</p>
<p>Continuous vectors have been used to steer LMs; for example, Subramani et al. (2020) showed that a pretrained LSTM language model can reconstruct arbitrary sentences by optimizing a continuous vector for each sentence, making the vector inputspecific. In contrast, prefix-tuning optimizes a taskspecific prefix that applies to all instances of that task. As a result, unlike the previous work whose application is limited to sentence reconstruction, prefix-tuning can be applied to NLG tasks.</p>
<p>Controllable generation. Controllable generation aims to steer a pretrained language model to match a sentence-level attribute (e.g., positive sentiment or sports). Such control can happen at training time: Keskar et al. (2019) pretrains the language model (CTRL) to condition on metadata such as keywords or URLs. The control can also happen at decoding time, by weighted decoding (GeDi, Krause et al., 2020) or iteratively updating the past activations (PPLM, Dathathri et al., 2020). However, there is no straightforward way to apply these controllable generation techniques to enforce fine-grained control over generated contents, as demanded by tasks like table-to-text and summarization.</p>
<p>P<em>-tuning. Prefix tuning is an instance of a new class of methods that has emerged, which we call $\mathrm{p}^{</em>}$-tuning (since the other prominent instances, ptuning and prompt-tuning, also start with p ), all based on the idea of optimizing a continuous prefix or prompt. Concurrent with our work, Qin and Eisner (2021) learn mixtures of soft fill-in-the-blank prompts to elicit knowledge from LMs such as BERT and BART. Hambardzumyan et al. (2021) learns task-specific embeddings that adapts BERT for sentiment classification. Both works show that tuning soft prompts outperforms previous work, which optimizes over discrete prompts. P-tuning (Liu et al., 2021) shows that jointly updating the prompt embeddings and LM parameters improves</p>
<p>GPT-2's performance on natural language understanding tasks, in both few-shot and full data settings. In a followup work, Prompt-tuning (Lester et al., 2021) simplifies our approach and applies it to T5 (Raffel et al., 2020), demonstrating that the performance gap between fine-tuning and $\mathrm{p}^{*}$ tuning vanishes as the model size grows.</p>
<h2>3 Problem Statement</h2>
<p>Consider a conditional generation task where the input $x$ is a context and the output $y$ is a sequence of tokens. We focus on two tasks, shown in Figure 2 (right): In table-to-text, $x$ corresponds to a linearized data table and $y$ is a textual description; in summarization, $x$ is an article and $y$ is a summary.</p>
<h3>3.1 Autoregressive LM</h3>
<p>Assume we have an autoregressive neural language model $p_{\phi}(y \mid x)$ parametrized by $\phi$ (e.g., GPT-2; Radford et al., 2019). As shown in Figure 2 (top), let $z=[x ; y]$ be the concatenation of $x$ and $y$; let $\mathrm{X}<em _mathrm_idx="\mathrm{idx">{\mathrm{idx}}$ denote the sequence of indices that corresponds to $x$, and $\mathrm{Y}</em>$ denote the same for $y$.}</p>
<p>The activation vector at time step $i$ is $h_{i} \in \mathbb{R}^{d}$, where $h_{i}=\left[h_{i}^{(1)} ; \cdots ; h_{i}^{(n)}\right]$ is a concatenation of all activation layers at this time step, and $h_{i}^{(j)}$ is the activation vector of the $j$-th layer at time step $i .{ }^{1}$</p>
<p>An autoregressive neural LM computes $h_{i}$ as a function of $z_{i}$ and the past activations in its left context, as follows:</p>
<p>$$
h_{i}=\mathrm{LM}<em i="i">{\phi}\left(z</em>\right)
$$}, h_{&lt;i</p>
<p>where the last layer of $h_{i}$ is used to compute the distribution for the next token: $p_{\phi}\left(z_{i+1} \mid h_{\leq i}\right)=$ $\operatorname{softmax}\left(W_{\phi} h_{i}^{(n)}\right)$ and $W_{\phi}$ is a matrix that maps $h_{i}^{(n)}$ to logits over the vocabulary.</p>
<h3>3.2 Encoder-Decoder Architecture</h3>
<p>We can also use an encoder-decoder architecture (e.g., BART; Lewis et al., 2020) to model $p_{\phi}(y \mid x)$, where $x$ is encoded by the bidirectional encoder, and the decoder predicts $y$ autoregressively (conditioned on the encoded $x$ and its left context). We use the same indexing and activation notation, as shown in Figure 2 (bottom): each $h_{i}$ for $i \in \mathrm{X}<em i="i">{\mathrm{idx}}$ is computed by the a bidirectional encoder; each $h</em>$ is computed by an autoregressive decoder using the same equation (1).}$ for $i \in \mathrm{Y}_{\mathrm{idx}</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: An annotated example of prefix-tuning using an autoregressive LM (top) and an encoder-decoder model (bottom). The prefix activations $\forall i \in \mathrm{P}<em i="i">{\mathrm{idx}}, h</em>$. The remaining activations are computed by the Transformer.}$ are drawn from a trainable matrix $P_{\theta</p>
<h3>3.3 Fine-tuning</h3>
<p>In the full fine-tuning framework, we initialize with the pretrained parameters $\phi$. Here $p_{\phi}$ is a trainable language model distribution and we perform gradient updates on the following log-likelihood objective:
$\max <em _phi="\phi">{\phi} \log p</em>(y \mid x)=\max <em _in="\in" _mathrm_Y="\mathrm{Y" i="i">{\phi} \sum</em><em _phi="\phi">{\mathrm{idx}}} \log p</em>\right)$.}\left(z_{i} \mid h_{&lt;i</p>
<h2>4 Prefix-Tuning</h2>
<p>We propose prefix-tuning as an alternative to full fine-tuning for conditional generation tasks. We first provide intuition in $\S 4.1$ before defining our method formally in $\S 4.2$.</p>
<h3>4.1 Intuition</h3>
<p>Prompting has demonstrated that conditioning on a proper context can steer the LM without changing its parameters. For example, if we want the LM to generate a word (e.g., Obama), we can prepend its common collocations as context (e.g., Barack), and the LM will assign much higher probability to the desired word. Extending this intuition beyond generating a single word or sentence, we want to find a context that steers the LM to solve an NLG task. Intuitively, the context could influence the encoding of the task input $x$ by guiding what to extract from $x$, and it could influence the generation of the task output $y$ by steering the next token distribution. However, it's non-obvious whether such a context exists. Using natural language task instructions (e.g., "summarize the following table in one sentence") for the context might guide a human to
solve the task, but this fails for moderately-sized pretrained LMs. ${ }^{2}$ Optimizing over the discrete instructions might help, but discrete optimization is computationally challenging.</p>
<p>Instead of optimizing over discrete tokens, we can optimize the instruction as continuous word embeddings, whose effects will be propagated upward to all Transformer activation layers and rightward to subsequent tokens. This is strictly more expressive than a discrete prompt which is constrained to the embeddings of real words. Prefix-tuning goes one step further in increasing expressivity by optimizing the activations of all the layers, not just the embedding layer. As another benefit, prefixtuning can directly modify representations deeper in the network, therefore, avoiding long computation paths across the depth of the network.</p>
<h3>4.2 Method</h3>
<p>Prefix-tuning prepends a prefix for an autoregressive LM to obtain $z=[\operatorname{REFIX} ; x ; y]$, or prepends prefixes for both encoder and decoder to obtain $z=[\operatorname{REFIX} ; x ; \operatorname{Prefix}^{\prime} ; y]$, as shown in Figure 2. Here, $\mathrm{P}<em _mathrm_idx="\mathrm{idx">{\mathrm{idx}}$ denotes the sequence of prefix indices, and we use $\left|\mathrm{P}</em>\right|$ to denote the length of the prefix.}</p>
<p>We follow the recurrence relation in equation (1), except that the activations of the prefix indices are free parameters, given by a matrix $P_{\theta}$ (parametrized by $\theta$ ) of dimension $\left|\mathrm{P}<em i="i">{\mathrm{idx}}\right| \times \operatorname{dim}\left(h</em>\right)$.</p>
<p>$$
h_{i}= \begin{cases}P_{\theta}[i,:], &amp; \text { if } i \in \mathrm{P}<em _phi="\phi">{\mathrm{idx}} \ \mathrm{LM}</em>
$$}\left(z_{i}, h_{&lt;i}\right), &amp; \text { otherwise }\end{cases</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>The training objective is the same as equation (2), but the set of trainable parameters changes: the language model parameters $\phi$ are fixed and the prefix parameters $\theta$ are the only trainable parameters.</p>
<p>Here, each $h_{i}$ is a function of the trainable $P_{\theta}$. When $i \in \mathrm{P}<em i="i">{\mathrm{idx}}$, this is clear because $h</em>}$ copies directly from $P_{\theta}$. When $i \notin \mathrm{P<em i="i">{\mathrm{idx}}, h</em>$, because the prefix activations are always in the left context and will therefore affect any activations to the right.}$ still depends on $P_{\theta</p>
<h3>4.3 Parametrization of $P_{\theta}$</h3>
<p>Empirically, directly updating the $P_{\theta}$ parameters leads to unstable optimization and a slight drop in performance. ${ }^{3}$ So we reparametrize the matrix $P_{\theta}[i,:]=\operatorname{MLP}<em _theta="\theta">{\theta}\left(P</em>}^{\prime}[i,:]\right)$ by a smaller matrix $\left(P_{\theta}^{\prime}\right)$ composed with a large feedforward neural network $\left(\mathrm{MLP<em _theta="\theta">{\theta}\right)$. Now, the trainable parameters include $P</em>}^{\prime}$ and the parameters of $\mathrm{MLP<em _theta="\theta">{\theta}$. Note that $P</em>$}$ and $P_{\theta}^{\prime}$ has the same number of rows (i.e., the prefix length), but different number of columns. ${ }^{4</p>
<p>Once training is complete, these reparametrization parameters can be dropped, and only the prefix $\left(P_{\theta}\right)$ needs to be saved.</p>
<h2>5 Experimental Setup</h2>
<h3>5.1 Datasets and Metrics</h3>
<p>We evaluate on three standard neural generation datasets for the table-to-text task: E2E (Novikova et al., 2017), WebNLG (Gardent et al., 2017), and DART (Radev et al., 2020), as shown in Table 1. The datasets are ordered by increasing complexity and size. E2E only has 1 domain (i.e. restaurant reviews); WebNLG has 14 domains, and DART is open-domain, using open-domain tables from Wikipedia. For evaluation, we report the metrics using the official evaluation scripts (see details in Appendix A.1).</p>
<p>For the summarization task, we use the XSUM (Narayan et al., 2018) dataset, which is an abstractive summarization dataset on news articles. We report ROUGE-1, ROUGE-2 and ROUGE-L.</p>
<h3>5.2 Methods</h3>
<p>For table-to-text generation, we compare prefixtuning with three other methods: full fine-tuning</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>(FT-FULL), fine-tuning only the top 2 layers (FTTOP2), and adapter-tuning (ADAPTER). ${ }^{5}$ We also report the current state-of-the-art results on these datasets: On E2E, Shen et al. (2019) uses a pragmatically informed model without pretraining. On WebNLG, Kale (2020) fine-tunes T5-large. On DART, no official models trained on this dataset version are released. ${ }^{6}$ For summarization, we compare against fine-tuning BART (Lewis et al., 2020).</p>
<h3>5.3 Architectures and Hyperparameters</h3>
<p>For table-to-text, we use GPT-2 MEDIUM and GPT$2_{\text {LARGE }}$. For summarization, we use BART $_{\text {LARGE }}$. Our implementation is based on the Hugging Face Transformers (Wolf et al., 2020).</p>
<p>At training time, we use the AdamW optimizer (Loshchilov and Hutter, 2019) and a linear learning rate scheduler, as suggested by the Hugging Face default setup. The hyperparameters we tune include the number of epochs, batch size, learning rate, and prefix length. Hyperparameter details are in the appendix. The default setting is 10 epochs, batch size 5 , learning rate $5 \cdot 10^{-5}$ and prefix length 10. The table-to-text models are trained on TITAN Xp or GeForce GTX TITAN X machines. Prefixtuning takes 0.2 hours per epoch to train on 22 K examples, whereas fine-tuning takes around 0.3 hours per epoch. The summarization models are trained on Tesla V100 machines, taking 1.25 hours per epoch on the XSUM dataset. For time efficiency, prefix-tuning is around $30 \%$ faster than fine-tuning. For GPU memory efficiency, prefixtuning with batchsize 1 takes $18 \%$ of the total GPU memory, whereas fine-tuning takes $50 \%$.</p>
<p>At decoding time, for table-to-text, we use beam search with beam size 5 . For summarization, we use beam size 6 and length normalization 0.8 . Decoding takes 1.2 seconds per sentence (without</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>batching) for table-to-text, and 2.6 seconds per batch (using a batch size of 10) for summarization.</p>
<h2>6 Main Results</h2>
<h3>6.1 Table-to-text Generation</h3>
<p>We find that by updating only $0.1 \%$ task-specific parameters, ${ }^{7}$ prefix-tuning is effective in table-to-text generation, outperforming other lightweight baselines (ADAPTER and FT-TOP2) even by updating 30x fewer parameters and achieving a comparable performance with (full) fine-tuning. This trend holds for all datasets: E2E, WebNLG, ${ }^{8}$ and DART.</p>
<p>If we match the number of parameters for prefixtuning and adapter-tuning to be $0.1 \%$, Table 2 shows that prefix-tuning is significantly better than ADAPTER ( $0.1 \%$ ), attaining 4.1 BLEU improvement per dataset on average. Even when we compare with fine-tuning ( $100 \%$ ) and adapter-tuning (3.0\%), which update significantly more parameters than prefix-tuning, prefix-tuning still achieves results comparable or better than those two systems. This demonstrates that prefix-tuning is more Pareto efficient than adapter-tuning, significantly reducing parameters while improving generation quality.</p>
<p>Additionally, attaining good performance on DART suggests that prefix-tuning can generalize to tables with diverse domains and a large number of relations. We will delve deeper into extrapolation performance (i.e., generalization to unseen categories or topics) in $\S 6.4$.</p>
<p>In summary, prefix-tuning is an effective and space-efficient method to adapt GPT-2 to table-totext generation. It also maintains the performance gains when scaling up to GPT-2 ${ }_{\text {LARGE }}$, suggesting it has the potential to scale to even larger models with a similar architecture, like GPT-3.</p>
<h3>6.2 Summarization</h3>
<p>As shown in Table 3, with 2\% parameters, prefixtuning obtains slightly lower performance than finetuning ( 36.05 vs. 37.25 in ROUGE-L). With only $0.1 \%$ parameters, prefix-tuning underperforms full fine-tuning ( 35.05 vs. 37.25 ). There are several differences between XSUM and the three table-totext datasets which could account for why prefixtuning has comparative advantage in table-to-text:</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>(1) XSUM contains 4 x more examples than the three table-to-text datasets on average; (2) the input articles are 17 x longer than the linearized table input of table-to-text datasets on average; (3) summarization is more complex than table-to-text because it requires selecting key contents from an article.</p>
<h3>6.3 Low-data Setting</h3>
<p>Based on the results from table-to-text (§6.1) and summarization (§6.2), we observe that prefixtuning has a comparative advantage when the number of training examples is smaller. To explore the low-data setting more systematically, we subsample the full dataset (E2E for table-to-text and XSUM for summarization) to obtain small datasets of size ${50,100,200,500}$. For each size, we sample 5 different datasets and average over 2 training random seeds. Thus, we average over 10 models for each low-data setting. ${ }^{9}$</p>
<p>Figure 3 (right) shows that prefix-tuning outperforms fine-tuning in low-data regimes by 2.9 BLEU on average, in addition to requiring much fewer parameters, but the gap narrows as the dataset size increases.</p>
<p>Qualitatively, Figure 3 (left) shows 8 examples generated by both prefix-tuning and fine-tuning models trained on different data levels. While both methods tend to undergenerate (missing table contents) in low data regimes, prefix-tuning tends to be more faithful than fine-tuning. For example, finetuning $(100,200)^{10}$ falsely claims a low customer rating while the true rating is average, whereas prefix-tuning $(100,200)$ generates a description that is faithful to the table.</p>
<h3>6.4 Extrapolation</h3>
<p>We now investigate extrapolation performance to unseen topics for both table-to-text and summarization. In order to construct an extrapolation setting, we split the existing datasets so that training and test cover different topics. For table-to-text, the WebNLG dataset is labeled with table topics. There are 9 categories that appear in training and dev, denoted as SEEN and 5 categories that only appear at test time, denoted as UNSEEN. So we evaluate extrapolation by training on the SEEN categories and testing on the UNSEEN categories. For summarization, we construct two extrapolation data splits:</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>|  | E2E |  |  |  |  |  | WebNLG |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 4: Prefix length vs. performance on summerization (left) and table-to-text (right). Performance increases as the prefix length increases up to a threshold (200 for summarization and 10 for table-to-text) and then a slight performance drop occurs. Each plot reports two metrics (on two vertical axes).</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>BLEU</td>
<td>NIST</td>
<td>MET</td>
<td>ROUGE</td>
<td>CIDEr</td>
</tr>
<tr>
<td>Prefix</td>
<td>70.3</td>
<td>8.82</td>
<td>46.3</td>
<td>72.1</td>
<td>2.46</td>
</tr>
<tr>
<td></td>
<td>Embedding-only: EmB-{PrefixLength}</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>EmB-1</td>
<td>48.1</td>
<td>3.33</td>
<td>32.1</td>
<td>60.2</td>
<td>1.10</td>
</tr>
<tr>
<td>EmB-10</td>
<td>62.2</td>
<td>6.70</td>
<td>38.6</td>
<td>66.4</td>
<td>1.75</td>
</tr>
<tr>
<td>EmB-20</td>
<td>61.9</td>
<td>7.11</td>
<td>39.3</td>
<td>65.6</td>
<td>1.85</td>
</tr>
<tr>
<td></td>
<td>Infix-tuning: Infix-{PrefixLength}</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Infix-1</td>
<td>67.9</td>
<td>8.63</td>
<td>45.8</td>
<td>69.4</td>
<td>2.42</td>
</tr>
<tr>
<td>Infix-10</td>
<td>67.2</td>
<td>8.48</td>
<td>45.8</td>
<td>69.9</td>
<td>2.40</td>
</tr>
<tr>
<td>Infix-20</td>
<td>66.7</td>
<td>8.47</td>
<td>45.8</td>
<td>70.0</td>
<td>2.42</td>
</tr>
</tbody>
</table>
<p>Table 5: Intrinsic evaluation of Embedding-only (§7.2) and Infixing (§7.3). Both Embedding-only ablation and Infix-tuning underperforms full prefix-tuning.</p>
<p>Length increases up to a threshold (200 for summarization, 10 for table-to-text) and then a slight performance drop occurs. Prefixes longer than the threshold lead to lower training loss, but slightly worse test performance, suggesting that they tend to overfit the training data.</p>
<h3>7.2 Full vs Embedding-only</h3>
<p>Recall in §4.1, we discussed optimizing the continuous embeddings of the "virtual tokens." We instantiate that idea and call it <em>embedding-only</em>. The word embeddings are free parameters, and the remaining activation layers are computed by the Transformer. Table 5 (top) shows that the performance drops significantly, suggesting that tuning only the embedding layer is not sufficiently expressive.</p>
<p>Embedding-only upper bounds the performance of discrete prompt optimization (Shin et al., 2020), because discrete prompt restricts the embedding layer to exactly match the embedding of a real word. Consequently, we have this chain of increasing expressive power: discrete prompting &lt; embedding-only &lt; prefix-tuning.</p>
<h3>7.3 Prefix-tuning vs Infix-tuning</h3>
<p>We also investigate how the trainable activations' position in the sequence affects performance. In</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 5: Initializing the prefix with activations of real words significantly outperforms random initialization, in low-data settings.</p>
<p>Prefix-tuning, we place them at the beginning [Prefix; <em>x; y</em>]. We can also place the trainable activations between <em>x</em> and <em>y</em> (i.e. [<em>x</em>; Infix; <em>y</em>]) and call this infix-tuning. Table 5 (bottom) shows that infix-tuning slightly underperforms prefix-tuning. We believe this is because prefix-tuning can affect the activations of <em>x</em> and <em>y</em> whereas infix-tuning can only influence the activations of <em>y</em>.</p>
<h3>7.4 Initialization</h3>
<p>We find that how the prefix is initialized has a large impact in low-data settings. Random initialization leads to low performance with high variance. Initializing the prefix with activations of real words significantly improves generation, as shown in Figure 5. In particular, initializing with task relevant words such as "summarization" and "table-to-text" obtains slightly better performance than task irrelevant words such as "elephant" and "divide", but using real words is still better than random. Moreover, in full data settings, the initialization trick has no impact, and random initialization leads to equally good performance.</p>
<p>Since we initialize the prefix with activations of real words computed by the LM, this initialization strategy is concordant with prefix-tuning's philosophy, which preserves the pretrained LM as much as possible.</p>
<h3>7.5 Data Efficiency</h3>
<p>We also investigate the data efficiency of prefix-tuning (without initialization trick, a.k.a random initialization) and full fine-tuning by comparing their performance on 5 different data scales of the E2E task (10%, 20%, 40%, 60%, and 80%). Figure 6 shows that prefix-tuning has better performance than fine-tuning when using more than 20% of the data. For data scale of 10%, prefix-tuning with random initialization yields comparable or slightly lower performance than full fine-tuning,</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 6: Data efficiency curves: percentage of training set vs. performance on table-to-text (E2E). Prefixtuning (without the initialization trick) is more dataefficient than fine-tuning when using more than $20 \%$ of the data.
necessitating the initialization trick ( $\S 6.3$ ) to improve the performance in this low-data regime.</p>
<h2>8 Discussion</h2>
<p>We will discuss several favorable properties of prefix-tuning and some open problems.</p>
<p>Personalization. As we note in $\S 1$, prefix-tuning is advantageous when there are a large number of tasks that needs to be trained independently. One practical setting is user privacy (Shokri and Shmatikov, 2015; McMahan et al., 2016). In order to preserve user privacy, each user's data needs to be separated and a personalized model needs to be trained independently for each user. Consequently, each user can be regarded as an independent task. If there are millions of users, prefix-tuning can scale to this setting and maintain modularity, enabling flexible addition or deletion of users by adding or deleting their prefixes without cross-contamination.</p>
<p>Batching across users. Under the same personalization setting, prefix-tuning allows batching different users' queries even though they are backed by different prefixes. When multiple users query a cloud GPU device with their inputs, it is computationally efficient to put these users in the same batch. Prefix-tuning keeps the shared LM intact; consequently, batching requires a simple step of prepending the personalized prefix to user input, and all the remaining computation is unchanged. In contrast, we can't batch across different users in adapter-tuning, which has personalized adapters between shared Transformer layers.</p>
<p>This batching benefit could also help create efficient ensembles of multiple prefixes trained on the same task (Lester et al., 2021).</p>
<p>Inductive bias of prefix-tuning. Recall that finetuning updates all pretrained parameters, whereas prefix-tuning and adapter-tuning preserve them.</p>
<p>Since the language models are pretrained on general purpose corpora, preserving the LM parameters might help generalization to domains unseen during training. In concordance with this intuition, we observe that both prefix-tuning and adaptertuning have significant performance gain in extrapolation settings ( $\S 6.4$ ); however, how these methods improve extrapolation is an open question.</p>
<p>While prefix-tuning and adapter-tuning both freeze the pretrained parameters, they tune different sets of parameters to affect the activation layers of the Transformer. Recall that prefix-tuning keeps the LM intact and uses the prefix and the pretrained attention blocks to affect the subsequent activations; adapter-tuning inserts trainable modules between LM layers, which directly add residual vectors to the activations. Moreover, we observe that prefixtuning requires vastly fewer parameters compared to adapter-tuning while maintaining comparable performance. We think this gain in parameter efficiency is because prefix-tuning keeps the pretrained LM intact as much as possible, and therefore exploits the LM more than adapter-tuning.</p>
<p>Recent work by Aghajanyan et al. (2020) uses intrinsic dimension to show that there exists a lowdimensional reparameterization that is as effective for fine-tuning as the full parametrization. This explains why good accuracy on downstream tasks can be obtained by updating only a small number of parameters. Our work echoes this finding by showing that good generation performance can also be attained by updating a very small prefix. However, prefix-tuning is not just about the size of trainable parameters, but more importantly, which subset of parameters to modify. Therefore, it would be interesting future work to explore other lightweight fine-tuning methods that achieve an even better accuracy-size tradeoff.</p>
<h2>Acknowledgments</h2>
<p>We thank the members of p-lambda group as well as anonymous reviewers for valuable feedback. We gratefully acknowledge the support of a PECASE award. XLL is supported by a Stanford Graduate Fellowship.</p>
<h2>Reproducibility</h2>
<p>Our code is available at https://github.com/ XiangLi1999/PrefixTuning.
Experiments and data are available at https: //worksheets.codalab.org/worksheets/ 0x16e0c8e7ab1f4b22aaccddc8b586541f.</p>
<h2>References</h2>
<p>Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. 2020. Intrinsic dimensionality explains the effectiveness of language model fine-tuning.</p>
<p>Anja Belz and Ehud Reiter. 2006. Comparing automatic and human evaluation of NLG systems. In 11th Conference of the European Chapter of the Association for Computational Linguistics, Trento, Italy. Association for Computational Linguistics.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.</p>
<p>Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. 2020. Plug and play language models: A simple approach to controlled text generation. In International Conference on Learning Representations.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. The WebNLG challenge: Generating text from RDF data. In Proceedings of the 10th International Conference on Natural Language Generation, pages 124-133, Santiago de Compostela, Spain. Association for Computational Linguistics.</p>
<p>Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. 2021. WARP: word-level adversarial reprogramming. CoRR, abs/2101.00121.</p>
<p>Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 2790-2799, Long Beach, California, USA. PMLR.</p>
<p>Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language
models know? Transactions of the Association for Computational Linguistics, 8:423-438.</p>
<p>Mihir Kale. 2020. Text-to-text pre-training for data-totext tasks.
N. Keskar, B. McCann, L. R. Varshney, Caiming Xiong, and R. Socher. 2019. Ctrl: A conditional transformer language model for controllable generation. ArXiv, abs/1909.05858.</p>
<p>Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq Joty, Richard Socher, and Nazneen Fatema Rajani. 2020. GeDi: Generative Discriminator Guided Sequence Generation. arXiv preprint arXiv:2009.06367.</p>
<p>Alon Lavie and Abhaya Agarwal. 2007. Meteor: An automatic metric for mt evaluation with high levels of correlation with human judgments. In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT '07, pages 228-231, Stroudsburg, PA, USA. Association for Computational Linguistics.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.</p>
<p>Zhaojiang Lin, Andrea Madotto, and Pascale Fung. 2020. Exploring versatile generative language model via parameter-efficient transfer learning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 441-459, Online. Association for Computational Linguistics.</p>
<p>Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021. Gpt understands, too. arXiv preprint arXiv:2103.10385.</p>
<p>Yang Liu and Mirella Lapata. 2019. Text summarization with pretrained encoders. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3730-3740, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020. Multilingual denoising pre-training for neural machine translation.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692.</p>
<p>Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In International Conference on Learning Representations.
H. Brendan McMahan, Eider Moore, Daniel Ramage, and Blaise Agüera y Arcas. 2016. Federated learning of deep networks using model averaging. Proceedings of the 20 th International Conference on Artificial Intelligence and Statistics (AISTATS) 2017, abs/1602.05629.</p>
<p>Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don't give me the details, just the summary! Topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium.</p>
<p>Jekaterina Novikova, Ondrej Dusek, and Verena Rieser. 2017. The E2E dataset: New challenges for end-toend generation. CoRR, abs/1706.09254.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL '02, pages 311-318, Stroudsburg, PA, USA. Association for Computational Linguistics.</p>
<p>Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. 2020. Adapterfusion: Non-destructive task composition for transfer learning.</p>
<p>Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying LMs with mixtures of soft prompts. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), Mexico City.</p>
<p>Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Nazneen Fatema Rajani, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Murori Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, and Richard Socher. 2020. Dart: Open-domain structured data record to text generation.
A. Radford, Jeffrey Wu, R. Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.</p>
<p>Evani Radiya-Dixit and Xin Wang. 2020. How fine can fine-tuning be? learning efficient language models. In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics,
volume 108 of Proceedings of Machine Learning Research, pages 2435-2443, Online. PMLR.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-totext transformer. Journal of Machine Learning Research, 21(140):1-67.</p>
<p>Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. 2017. Learning multiple visual domains with residual adapters. In Advances in Neural Information Processing Systems, volume 30, pages 506516. Curran Associates, Inc.</p>
<p>Timo Schick and Hinrich Schütze. 2020. Exploiting cloze questions for few shot text classification and natural language inference.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881-7892, Online. Association for Computational Linguistics.</p>
<p>Sheng Shen, Daniel Fried, Jacob Andreas, and Dan Klein. 2019. Pragmatically informative text generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4060-4067, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Taylor Shin, Yasaman Razeghi, Robert L. Logan IV au2, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts.</p>
<p>Reza Shokri and Vitaly Shmatikov. 2015. Privacypreserving deep learning. In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security, CCS '15, page 1310-1321, New York, NY, USA. Association for Computing Machinery.</p>
<p>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and Ralph Weischedel. 2006. A study of translation error rate with targeted human annotation. In In Proceedings of the Association for Machine Transaltion in the Americas (AMTA 2006.</p>
<p>Asa Cooper Stickland, Xian Li, and Marjan Ghazvininejad. 2020. Recipes for adapting pre-trained monolingual and multilingual models to machine translation.</p>
<p>Nishant Subramani, Samuel R. Bowman, and Kyunghyun Cho. 2020. Can unconditional language models recover arbitrary sentences?</p>
<p>Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. 2015. Cider: Consensus-based image description evaluation. In CVPR, pages 4566-4575. IEEE Computer Society.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, and Jitendra Malik. 2020a. Sidetuning: A baseline for network adaptation via additive side networks.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020b. BERTScore: Evaluating text generation with bert. In International Conference on Learning Representations.</p>
<p>Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. 2020c. DIALOGPT : Largescale generative pre-training for conversational response generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 270278, Online. Association for Computational Linguistics.</p>
<p>Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hinrich Schütze. 2020. Masking as an efficient alternative to finetuning for pretrained language models.</p>
<p>Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. 2019. MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 563-578, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Ming Zhong, Pengfei Liu, Yiran Chen, Danqing Wang, Xipeng Qiu, and Xuanjing Huang. 2020. Extractive summarization as text matching. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6197-6208, Online. Association for Computational Linguistics.</p>
<p>Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin, Wengang Zhou, Houqiang Li, and Tieyan Liu. 2020. Incorporating bert into neural machine translation. In International Conference on Learning Representations.</p>
<h2>A Supplementary Material</h2>
<h2>A. 1 Datasets and Metrics</h2>
<p>We evaluate on three standard neural generation datasets for the table-to-text task: E2E (Novikova et al., 2017), WebNLG (Gardent et al., 2017), and DART (Radev et al., 2020).</p>
<p>The E2E dataset contains approximately 50K examples with 8 distinct fields; it contains multiple test references for one source table, and the average output length is 22.9 . We use the official evaluation script, ${ }^{12}$ which reports BLEU (Papineni et al., 2002), NIST (Belz and Reiter, 2006), METEOR (Lavie and Agarwal, 2007), ROUGE-L (Lin, 2004), and CIDEr (Vedantam et al., 2015).</p>
<p>The WebNLG (Gardent et al., 2017) dataset consists of 22 K examples, and the input $x$ is a sequence of (subject, property, object) triples. The average output length is 22.5 . In the training and validation splits, the input describes entities from 9 distinct DBpedia categories (e.g., Monument). The test split consists of two parts: the first half contains DB categories seen in training data, and the second half contains 5 unseen categories. These unseen categories are used to evaluate extrapolation. We use the official evaluation script, which reports BLEU, METEOR and TER (Snover et al., 2006).</p>
<p>DART (Radev et al., 2020) is an open domain table-to-text dataset, with similar input format (entity-relation-entity triples) as WebNLG. The average output length is 21.6 . It consists of 82 K examples from WikiSQL, WikiTableQuestions, E2E, and WebNLG and applies some manual or automated conversion. We use the official evaluation script ${ }^{13}$ and report BLEU, METEOR, TER, MoverScore (Zhao et al., 2019), BERTScore (Zhang et al., 2020b) and BLEURT (Sellam et al., 2020).</p>
<p>For the summarization task, we use the XSUM (Narayan et al., 2018) dataset, which is an abstractive summarization dataset on news articles. There are 225 K examples. The average length of the articles is 431 words and the average length of the summaries is 23.3. We report ROUGE-1, ROUGE2 and ROUGE-L, computed by the python package rouge-score.</p>
<p>Data pre-processing. For table-to-text, we linearize a table $x$ in order to fit into a language model context. In the E2E dataset, for example, "(field A,</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>value A), (field B, value B)" is linearized to "field A : value A | field B : value B". Also, in WebNLG and DART, a sequence of triple "(entity1.1, relation1, entity1.2), (entity2.1, relation2, entity2.2)" is linearlized as "entity1.1 : relation1 : entity1.2 | entity2.1 : relation2 : entity2.2".</p>
<p>For summarization, we truncate the articles $x$ to 512 BPE tokens.</p>
<p>Extrapolation data splits. We construct two extrapolation data splits news-to-sports and within-news from the original XSUM dataset. XSUM dataset is drawn from BBC news, and we identify the topic of each article based on its URL. Since "news" and "sports" are the two domains with the most articles, we create our first train/test split. Additionally, "news" has subdomains such as "UK", "world", and "technology". Consequently, we create a second data split, using the top 3 news subdomains (i.e. {world, UK, business }) as training data and the rest as test data.</p>
<h2>A. 2 Hyperparameters</h2>
<p>In Table 6, we report the hyperparameters used to train the best-performing models documented in the experiment section.</p>
<p>As for the search range of each hyperparameters: the learning rates are selected from ${1 \mathrm{e}-5,5 \mathrm{e}-05$, $8 \mathrm{e}-05}$; the number of epochs are selected from ${5$, 10} for table-to-text and ${5,25,30}$ for summarization; We select the largest batch size that can fit into GPU memory and didn't explicitly tune for an optimal batch size. Prefix length are selected from ${1,5,10,20,40}$ for table-to-text and ${1,10,20$, $50,80,100,200,300}$ for summarization. We use perplexity and automatic generation metrics on the validation set to select the best-performing models.</p>
<p>For table-to-text in the low data settings, we use a learning rate of $5 \mathrm{e}-5$, and a batch size of 10 . We use a prefix length of 6 , since we apply the initialization trick and initialize the prefix with "table-to-text:", which contains 6 BPE tokens. Instead of tuning the number of epochs, we tune the max steps of updates in ${100,200,400,600}$, as shown in Table 8. We apply early stopping based on the performance of validation set, where the validation size $=30 \%$ training size.</p>
<p>For summarization in the low data settings, we use a learning rate of $5 \mathrm{e}-5$ and a warmup step of 100. We use a batch size of 5 for prefix-tuning and 6 for fine-tuning. We apply the initialization trick and use the word "summarize" to initialize</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">learning rate</th>
<th style="text-align: center;"># epoch</th>
<th style="text-align: center;">batch size</th>
<th style="text-align: center;">prefix length</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Prefix:</td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">E2E</td>
<td style="text-align: left;">$8 \mathrm{e}-05$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">WebNLG</td>
<td style="text-align: left;">$5 \mathrm{e}-05$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">DART</td>
<td style="text-align: left;">$5 \mathrm{e}-05$</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: left;">XSUM</td>
<td style="text-align: left;">$5 \mathrm{e}-05$</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;">Adapter:</td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">E2E (3\%)</td>
<td style="text-align: left;">$5 \mathrm{e}-05$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">E2E (0.1\%)</td>
<td style="text-align: left;">$8 \mathrm{e}-05$</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">WebNLG (3\%)</td>
<td style="text-align: left;">$5 \mathrm{e}-05$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">WebNLG (0.1\%)</td>
<td style="text-align: left;">$5 \mathrm{e}-05$</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">DART (3\%)</td>
<td style="text-align: left;">$5 \mathrm{e}-05$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">DART (0.1\%)</td>
<td style="text-align: left;">$8 \mathrm{e}-05$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Fine-tune:</td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">E2E</td>
<td style="text-align: left;">$5 \mathrm{e}-05$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">WebNLG</td>
<td style="text-align: left;">$1 \mathrm{e}-05$</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">DART</td>
<td style="text-align: left;">$1 \mathrm{e}-05$</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">FT-top2:</td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">E2E</td>
<td style="text-align: left;">$5 \mathrm{e}-05$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">WebNLG</td>
<td style="text-align: left;">$5 \mathrm{e}-05$</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">DART</td>
<td style="text-align: left;">$5 \mathrm{e}-05$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">within-news</td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Fine-tune</td>
<td style="text-align: left;">$3 \mathrm{e}-5$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Prefix</td>
<td style="text-align: left;">$5 \mathrm{e}-5$</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">80</td>
</tr>
<tr>
<td style="text-align: left;">news-to-sports</td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Fine-tune</td>
<td style="text-align: left;">$3 \mathrm{e}-5$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Prefix</td>
<td style="text-align: left;">$5 \mathrm{e}-5$</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">40</td>
</tr>
</tbody>
</table>
<p>Table 6: Hyperparameter settings for our method and baseline methods.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">R-1 $\uparrow$</th>
<th style="text-align: center;">R-2 $\uparrow$</th>
<th style="text-align: center;">R-L $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\operatorname{PrEfiX}(2 \%)$</td>
<td style="text-align: center;">43.30</td>
<td style="text-align: center;">20.35</td>
<td style="text-align: center;">35.21</td>
</tr>
<tr>
<td style="text-align: left;">$\operatorname{PrEfiX}(0.1 \%)$</td>
<td style="text-align: center;">41.54</td>
<td style="text-align: center;">18.56</td>
<td style="text-align: center;">33.13</td>
</tr>
</tbody>
</table>
<p>Table 7: Metrics for summarization on XSUM validation set.
the prefix, resulting in a prefix length of 1 . We tune the number of epochs in ${3,5,10,20,30}$, shown in Table 8 . We also apply early stopping based on validation performance.</p>
<p>For the extrapolation setting, the hyperparameters for our table-to-text model is the same as the hyperparameters of WebNLG. The hyperparameters for summarization is shown in the last block of Table 6.</p>
<h2>A. 3 Validation Performance</h2>
<p>Table 9 shows the validation performance on the three table-to-text datasets. Table 7 shows the validation performance on XSUM.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">size $=50$</th>
<th style="text-align: center;">size $=100$</th>
<th style="text-align: center;">size $=200$</th>
<th style="text-align: center;">size $=500$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Prefix (max steps)</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">400</td>
</tr>
<tr>
<td style="text-align: left;">Finetune (max steps)</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">400</td>
</tr>
<tr>
<td style="text-align: left;">Prefix (epoch)</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: left;">Finetune (epoch)</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3</td>
</tr>
</tbody>
</table>
<p>Table 8: Max # update steps for low data settings.</p>
<h2>A. 4 Additional Results for Low-data Settings</h2>
<p>Figure 7 supplements the low-data performance curves in Figure 3 by plotting the relationship between training size and generation metrics for both prefix-tuning and fine-tuning.</p>
<h2>A. 5 Additional Results for the Initialization Experiment</h2>
<p>Figure 8 supplements Figure 3 by plotting additional metrics for our initialization technique $\S 7.4$. It validates that random initialization (from a uniform $(0,1)$ distirbution) significantly underperforms initializing with real words; Additionally, initializing with task-relevant words (e.g., "summarization" and "table-to-text") attains slightly better generation scores than initializing with task-irrelevant words (e.g., "elephant" and "banana").</p>
<h2>A. 6 Qualitative Examples for Extrapolation</h2>
<p>Table 10 contains qualitative examples from both seen and unseen categories in WebNLG. We find that for unseen categories, both prefix-tuning and fine-tuning tend to undergenerate (generated output do not cover full table contents) or generate untruthfully (generated output is inconsistent with table contents). In particular, prefix-tuning tends to undergenerate whereas fine-tuning tends to generate untruthfully. For seen categories, both perform fairly well in terms of coverage and truthfulness.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">E2E</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">WebNLG</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">DART</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">NIST</td>
<td style="text-align: center;">MET</td>
<td style="text-align: center;">R-L</td>
<td style="text-align: center;">CIDEr</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">MET</td>
<td style="text-align: center;">TER $\downarrow$</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">MET</td>
<td style="text-align: center;">TER $\downarrow$</td>
<td style="text-align: center;">Mover</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">BLEURT</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2 MEDIUM</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">FT-FULL</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">8.76</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">2.66</td>
<td style="text-align: center;">66.03</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">50.46</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.41</td>
</tr>
<tr>
<td style="text-align: center;">FT-TOP2</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">8.51</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">2.60</td>
<td style="text-align: center;">54.61</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">48.41</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: center;">ADAPTER(3\%)</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">8.53</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">2.60</td>
<td style="text-align: center;">60.63</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">48.56</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.40</td>
</tr>
<tr>
<td style="text-align: center;">ADAPTER( $0.1 \%)$</td>
<td style="text-align: center;">68.1</td>
<td style="text-align: center;">8.30</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">71.4</td>
<td style="text-align: center;">2.41</td>
<td style="text-align: center;">53.24</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">44.72</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">0.35</td>
</tr>
<tr>
<td style="text-align: center;">Prefix( $0.1 \%)$</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">8.80</td>
<td style="text-align: center;">49.4</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">2.69</td>
<td style="text-align: center;">64.52</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">51.11</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.42</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2_LARGE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">FT-FULL</td>
<td style="text-align: center;">72.1</td>
<td style="text-align: center;">8.62</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">75.1</td>
<td style="text-align: center;">2.56</td>
<td style="text-align: center;">64.69</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">51.00</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.43</td>
</tr>
<tr>
<td style="text-align: center;">Prefix</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">8.81</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">2.72</td>
<td style="text-align: center;">64.11</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">50.84</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.42</td>
</tr>
</tbody>
</table>
<p>Table 9: Metrics on the development set (higher is better, except for TER) for table-to-text generation on E2E (left), WebNLG (middle) and DART (right).
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 7: Prefix-tuning (orange) outperforms fine-tuning (blue) in low-data regimes in addition to requiring many fewer parameters. The top three plots correspond to summarization, measured by ROUGE-1, ROUGE-2, and ROUGE-L. The bottom three plots correspond to table-to-text, measured by NIST, METEOR, and CIDEr. The x -axis is the training size and the y -axis is the evaluation metric (higher is better).
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 8: Initializing the prefix with activations of real words significantly outperforms random initialization, in a low-data setting with 100 training data.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Source [Unseen, Athelete]</th>
<th style="text-align: center;">(Al Kharaitiyat SC, club, Alaa Abdul-Zahra), (Al Khor, ground, Al Kharaitiyat SC), (Shabab Al-Ordon Club, club, Alaa Abdul-Zahra) (Amar Osim, manager, Al Kharaitiyat SC)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Prefix-tuning</td>
<td style="text-align: center;">Al Kharaitiyat SC are managed by Amar Osim and play at their ground at Al Khor. Al Kharaitiyat SC are also the club for which Alaa Abdul-Zahra is a player.</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tuning</td>
<td style="text-align: center;">Alaa Abdul-Zahra plays for Al-Kharaitiyat SC and Shabab Al-Ordon Club. He also plays for Al-Khor and manages Al-Kharaitiyat SC.</td>
</tr>
<tr>
<td style="text-align: center;">Reference</td>
<td style="text-align: center;">Alaa Abdul Zahra plays for Al Kharaitiyat SC which is located at Al Khor and managed by Amar Osim. The Shabab Al-Ordon club is associated with Alaa Abdul-Zahra.</td>
</tr>
<tr>
<td style="text-align: center;">Source [Unseen, Transportation]</td>
<td style="text-align: center;">(Genoa, location, Costa Crociere), (AIDA Cruises, operator, AIDAstella), (Costa Crociere, owner, AIDAstella)</td>
</tr>
<tr>
<td style="text-align: center;">Prefix-tuning <br> Fine-tuning <br> Reference</td>
<td style="text-align: center;">AID Astella is operated by Aida Cruises and is owned by the Costa Rican tourist resort of Genoa. <br> AID Astella, operated by AIDA-Cruises, is located in Genoa and is owned by the Costa Rican government. Costa Crociere is the owner of the AIDAstella and are based in Genoa. The operator of AIDAstella is AIDA Cruises.</td>
</tr>
<tr>
<td style="text-align: center;">Source [Unseen, Politician]</td>
<td style="text-align: center;">(Euro, currency, Netherlands), (Stellendam, birthPlace, Ab Klink ), (Netherlands, nationality, Ab Klink)</td>
</tr>
<tr>
<td style="text-align: center;">Prefix-tuning <br> Fine-tuning <br> Reference</td>
<td style="text-align: center;">Ab Klink was born in Stellendam and is a national of the Netherlands where the currency is the Euro. Ab Klink is a national of the Netherlands where the currency is the Euro. He was born in Stellendam. Ab Klink was born in Stellendam in the Netherlands, where the national currency is the euro.</td>
</tr>
<tr>
<td style="text-align: center;">Source [Unseen, Politician]</td>
<td style="text-align: center;">(Robert E, Lee, commander, Battle of Salem Church), (American Civil War, isPartOfMilitaryConflict, Battle of Salem Church), (Battle of Salem Church, battles, Aaron S. Daggett)</td>
</tr>
<tr>
<td style="text-align: center;">Prefix-tuning</td>
<td style="text-align: center;">Robert E. Lee was the commander of the Battle of Salem Church which was part of the military conflict in the American Civil war.</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tuning <br> Reference</td>
<td style="text-align: center;">The Battle of Salem Church is part of the American Civil War and was commanded by Robert E. Lee. Robert E Lee was a commander in the Battle of Salem Church, which was one of the military conflicts in the American Civil War. Aaron S Daggett fought in the same battle.</td>
</tr>
<tr>
<td style="text-align: center;">Source [Unseen, Artist]</td>
<td style="text-align: center;">(Christian alternative rock, musicSubgenre, Alternative rock), (Alternative rock, genre, Andrew White (musician))</td>
</tr>
<tr>
<td style="text-align: center;">Prefix-tuning <br> Fine-tuning <br> Reference</td>
<td style="text-align: center;">Andrew White is a Christian alternative rock musician. <br> Andrew White, a Christian alternative rocker, performs. <br> The musician Andrew White's genre is alternative rock, the genre which has the sub genre Christian alternative rock.</td>
</tr>
<tr>
<td style="text-align: center;">Source [Unseen, Artist]</td>
<td style="text-align: center;">(Hip hop music, genre, Allen Forrest), (solo singer, background, Allen Forrest)</td>
</tr>
<tr>
<td style="text-align: center;">Prefix-tuning <br> Fine-tuning <br> Reference</td>
<td style="text-align: center;">Allen Forrest is a solo singer. <br> Born in <br> Allen Forrest is a solo singer whose genre is Hip Hop music.</td>
</tr>
<tr>
<td style="text-align: center;">Source [Seen, ComicsCharacter]</td>
<td style="text-align: center;">(Americans, nationality, Ducan Rouleau), (Ducan Rouleau, creator, Baymax),(Alan Tudyk, starring, Big Hero 6 (film)), (Steven T Segle, creator, Baymax), (Big Hero 6 (film), serires, Baymax)</td>
</tr>
<tr>
<td style="text-align: center;">Prefix-tuning</td>
<td style="text-align: center;">Baymax is a character in Big Hero 6 which stars Alan Tudyk. He was created by Steven T. Seagle and the American, Duncan Rouleau.</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tuning</td>
<td style="text-align: center;">Alan Tudyk stars in the film Big Hero 6 in which Baymax is a character created by Steven T. Seagle and the American, Duncan Rouleau.</td>
</tr>
<tr>
<td style="text-align: center;">Reference</td>
<td style="text-align: center;">Baymax is a character who appeared in Big Hero 6 starring Alan Tudyk. It was created by Steven T Seagle and the American, Duncan Rouleau.</td>
</tr>
<tr>
<td style="text-align: center;">Source [Seen, City]</td>
<td style="text-align: center;">(Washington, D.C., capital, United States), (White Americans, ethnicGroup, United States), (United States, country, New Jersey), (New York City, largest City, United States), (New Jersy, isPartOf, Atlantic City)</td>
</tr>
<tr>
<td style="text-align: center;">Prefix-tuning</td>
<td style="text-align: center;">Washington D.C. is the capital of the United States where the largest city is New York City and the White Americans are an ethnic group. Atlantic City, New Jersey is also part of the United States.</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tuning</td>
<td style="text-align: center;">Atlantic City, New Jersey is part of New Jersey in the United States. The capital city is Washington D.C. and one of the ethnic groups is White Americans.</td>
</tr>
<tr>
<td style="text-align: center;">Reference</td>
<td style="text-align: center;">New York City (NYC) is the largest U.S. city. Atlantic City, New Jersey are also part of the United States with its capital as Washington, DC and home to White Americans.</td>
</tr>
</tbody>
</table>
<p>Table 10: Qualitative examples from WebNLG. The first 6 examples are from the unseen categories, labeled next to source; the last two examples are from the seen categories. For unseen categories, both prefix-tuning and finetuning tend to undergenerate (generated output do not cover full table contents) or generate untruthfully (generated output is inconsistent with table contents). In particular, prefix-tuning tends to undergenerate more often than generate untruthfully whereas fine-tuning tends to generate untruthfully. For seen categories, both perform fairly well in terms of coverage and truthfulness.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{12}$ https://github.com/tuetschek/ e2e-metrics
${ }^{13}$ https://github.com/Yale-LILY/dart&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{9}$ We also sample a dev split (with dev size $=30 \% \times$ training size) for each training set. We use the dev split to choose hyperparameters and perform early stopping.
${ }^{10}$ The number in the parenthesis refers to the training size.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
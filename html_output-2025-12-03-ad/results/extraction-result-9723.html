<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9723 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9723</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9723</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-273162350</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.03131v1.pdf" target="_blank">AIME: AI S YSTEM O PTIMIZATION VIA M ULTIPLE LLM E VALUATORS</a></p>
                <p><strong>Paper Abstract:</strong> Text-based AI system optimization typically involves a feedback loop scheme where a single LLM generates an evaluation in natural language of the current output to improve the next iteration’s output. However, in this work, we empirically demonstrate that for a practical and complex task (code generation) with multiple criteria to evaluate, utilizing only one LLM evaluator tends to let errors in generated code go undetected, thus leading to incorrect evaluations and ultimately suboptimal test case performance. Motivated by this failure case, we assume there exists an optimal evaluation policy that samples an evaluation between response and ground truth. We then theoretically prove that a linear combination of multiple evaluators can approximate this optimal policy. From this insight, we propose AI system optimization via Multiple LLM Evaluators (AIME) . AIME is an evaluation protocol that utilizes multiple LLMs that each independently generate an evaluation on separate criteria and then combine them via concatenation. We provide an extensive empirical study showing AIME outperforming baseline methods in code generation tasks, with up to 62% higher error detection rate and up to 16% higher success rate than a single LLM evaluation protocol on LeetCodeHard and HumanEval datasets. We also show that the selection of the number of evaluators and which criteria to utilize is non-trivial as it can impact pact success rate by up to 12% .</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9723.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9723.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AIME</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI System Optimization via Multiple LLM Evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A protocol that uses multiple independent LLM-based evaluators, each conditioned on a specific role (e.g., correctness, logic, syntax), concatenates their natural-language evaluations, and uses that aggregated evaluation to drive iterative prompt/code optimization; demonstrated on code generation with GPT-4o.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>code generation (LeetCodeHard, HumanEval)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4o (used for all evaluator calls in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Multiple independent evaluator calls to the same LLM (GPT-4o), each given a role-specific system prompt (e.g., correctness, logic, syntax, readability, runtime, redundancy); each evaluator produces a natural-language evaluation and evaluations are aggregated via string concatenation (uniform token budget across evaluators).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>No direct LLM-vs-human agreement metric reported; comparisons are between single-LLM evaluator and multi-LLM evaluator protocols using Error Detection Rate (EDR) and Robustness to Adversarial Evaluator (RAE) as internal metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>When using a single LLM-as-a-judge (Single-Eval), complex multi-criteria outputs (like code judged by correctness, logic, syntax, readability, etc.) suffer from undetected errors and a fixed suboptimality gap relative to an optimal evaluator; single evaluators are less robust to adversarial or incorrect evaluations and tend to produce less thorough/explanatory judgments than independent role-specific evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Figure 1: Single-Eval failed to detect an error in generated code that failed all unit tests, while AIME's separate logic evaluator detected a logical error; quantitative results: AIME achieved up to ~62% higher Error Detection Rate (EDR) than Single-Eval on LeetCodeHard and up to ~16% higher task success rate (SR) after optimization. AIME also shows ~16% higher RAE on LeetCodeHard, indicating Single-Eval is more vulnerable to an adversarial evaluator that always claims correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>The paper notes that LLM-based evaluation has been reported to align with human preferences in past work and that panels of smaller LLM judges can correlate well with human judgment (cited related work). Also, downstream feedback LLMs in the optimization pipeline can sometimes mitigate incorrect single-evaluator judgments, reducing the practical gap in final task performance even if EDR is low.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Sections: Introduction, Section 4.1 (AIME is robust to incorrect evaluations), Figures 1 and 2, Table(s) and Appendix A.3 for EDR / RAE statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AIME: AI S YSTEM O PTIMIZATION VIA M ULTIPLE LLM E VALUATORS', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9723.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9723.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-Judge (related work: panel vs single)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Panel of LLM Judges vs Single LLM Judge (as discussed in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work (cited) finds that a panel of smaller LLM judges can produce numeric scores that better correlate with human judgments than a single larger LLM judge, suggesting diversity of model judges can better approximate human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Replacing judges with juries: Evaluating llm generations with a panel of diverse models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>general LLM output evaluation / text evaluation (cited prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Panel of smaller LLM judges (not concretely specified in this paper; referenced from Verga et al. 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Panel-based numeric scoring from multiple smaller LLMs (referenced work); the paper cites this as evidence that multiple evaluators can better correlate with human judgment than one large judge.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Reported in cited prior work as correlation to human judgment (exact numeric values not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>By contrast, using a single LLM judge may yield weaker correlation with human judgments; single-model evaluation can miss aspects captured by aggregated/diverse judges.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Not provided in detail in this paper; cited as related evidence motivating multiple evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>The present paper builds on this prior observation: it empirically shows multi-evaluator AIME improves internal error detection and optimization outcomes vs a single evaluator for code generation, but the authors do not perform direct human-vs-LLM comparisons themselves.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Related work paragraph: 'Verga et al. (2024) showed a panel of smaller LLM judges can provide numeric scores correlating to human judgment than a single larger LLM model can.'</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AIME: AI S YSTEM O PTIMIZATION VIA M ULTIPLE LLM E VALUATORS', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9723.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9723.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Single-Eval Suboptimality (theoretical)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fixed Suboptimality Gap of a Single LLM Evaluator vs Optimal Evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper proves that sampling evaluations from a single evaluator π_e induces a fixed suboptimality gap (upper-bounded by a constant times the total variation distance between the single evaluator distribution and the unknown optimal evaluator distribution), which cannot be reduced without changing the evaluator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>text-based AI system optimization; applied to code generation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Abstract single evaluator π_e (realized in experiments as a single GPT-4o evaluator conditioned on all roles together).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Single LLM evaluator receives a system prompt that lists all evaluation roles together and outputs a single natural-language evaluation; used as the sole evaluation signal in iterative optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>No LLM-vs-human agreement metric reported for this result; the theoretical metric is a suboptimality gap ∆_π_Eva-sub-opt and an upper bound involving total variation distance (Eq. 2).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Using a single evaluator leads to a constant evaluation gap relative to an ideal evaluator (π*_e) that cannot be reduced without changing the evaluator distribution; practically, this manifests as undetected errors in multi-criteria tasks (e.g., failing to detect correctness or logic errors in generated code).</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Equation (2) formalizes the fixed gap; Figure 1 and empirical EDR results illustrate the practical effect where a single evaluator fails to flag errors that cause all unit tests to fail.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>The paper proves that forming a mixture (linear combination) of multiple independent evaluators can reduce the suboptimality gap (Theorem 1). Also, downstream components in the optimization pipeline (the feedback LLM) can sometimes compensate for missed evaluations from the single evaluator, reducing final task-performance differences.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Section 2 (Fixed Gap in Evaluation with Single Evaluation Policy), Equation (2), Theorem 1 and accompanying discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AIME: AI S YSTEM O PTIMIZATION VIA M ULTIPLE LLM E VALUATORS', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Replacing judges with juries: Evaluating llm generations with a panel of diverse models. <em>(Rating: 2)</em></li>
                <li>An empirical study of llm-as-a-judge for llm evaluation: Fine-tuned judge models are task-specific classifiers <em>(Rating: 2)</em></li>
                <li>On scalable oversight with weak llms judging strong llms <em>(Rating: 2)</em></li>
                <li>Large language models are state-of-the-art evaluators of translation quality <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9723",
    "paper_id": "paper-273162350",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "AIME",
            "name_full": "AI System Optimization via Multiple LLM Evaluators",
            "brief_description": "A protocol that uses multiple independent LLM-based evaluators, each conditioned on a specific role (e.g., correctness, logic, syntax), concatenates their natural-language evaluations, and uses that aggregated evaluation to drive iterative prompt/code optimization; demonstrated on code generation with GPT-4o.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "code generation (LeetCodeHard, HumanEval)",
            "llm_judge_model": "GPT-4o (used for all evaluator calls in experiments)",
            "llm_judge_setup": "Multiple independent evaluator calls to the same LLM (GPT-4o), each given a role-specific system prompt (e.g., correctness, logic, syntax, readability, runtime, redundancy); each evaluator produces a natural-language evaluation and evaluations are aggregated via string concatenation (uniform token budget across evaluators).",
            "human_evaluation_setup": null,
            "agreement_metric": "No direct LLM-vs-human agreement metric reported; comparisons are between single-LLM evaluator and multi-LLM evaluator protocols using Error Detection Rate (EDR) and Robustness to Adversarial Evaluator (RAE) as internal metrics.",
            "losses_identified": "When using a single LLM-as-a-judge (Single-Eval), complex multi-criteria outputs (like code judged by correctness, logic, syntax, readability, etc.) suffer from undetected errors and a fixed suboptimality gap relative to an optimal evaluator; single evaluators are less robust to adversarial or incorrect evaluations and tend to produce less thorough/explanatory judgments than independent role-specific evaluators.",
            "examples_of_loss": "Figure 1: Single-Eval failed to detect an error in generated code that failed all unit tests, while AIME's separate logic evaluator detected a logical error; quantitative results: AIME achieved up to ~62% higher Error Detection Rate (EDR) than Single-Eval on LeetCodeHard and up to ~16% higher task success rate (SR) after optimization. AIME also shows ~16% higher RAE on LeetCodeHard, indicating Single-Eval is more vulnerable to an adversarial evaluator that always claims correctness.",
            "counterexamples_or_caveats": "The paper notes that LLM-based evaluation has been reported to align with human preferences in past work and that panels of smaller LLM judges can correlate well with human judgment (cited related work). Also, downstream feedback LLMs in the optimization pipeline can sometimes mitigate incorrect single-evaluator judgments, reducing the practical gap in final task performance even if EDR is low.",
            "paper_reference": "Sections: Introduction, Section 4.1 (AIME is robust to incorrect evaluations), Figures 1 and 2, Table(s) and Appendix A.3 for EDR / RAE statistics.",
            "uuid": "e9723.0",
            "source_info": {
                "paper_title": "AIME: AI S YSTEM O PTIMIZATION VIA M ULTIPLE LLM E VALUATORS",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LLM-as-a-Judge (related work: panel vs single)",
            "name_full": "Panel of LLM Judges vs Single LLM Judge (as discussed in related work)",
            "brief_description": "Prior work (cited) finds that a panel of smaller LLM judges can produce numeric scores that better correlate with human judgments than a single larger LLM judge, suggesting diversity of model judges can better approximate human evaluation.",
            "citation_title": "Replacing judges with juries: Evaluating llm generations with a panel of diverse models.",
            "mention_or_use": "mention",
            "task_domain": "general LLM output evaluation / text evaluation (cited prior work)",
            "llm_judge_model": "Panel of smaller LLM judges (not concretely specified in this paper; referenced from Verga et al. 2024)",
            "llm_judge_setup": "Panel-based numeric scoring from multiple smaller LLMs (referenced work); the paper cites this as evidence that multiple evaluators can better correlate with human judgment than one large judge.",
            "human_evaluation_setup": null,
            "agreement_metric": "Reported in cited prior work as correlation to human judgment (exact numeric values not provided in this paper).",
            "losses_identified": "By contrast, using a single LLM judge may yield weaker correlation with human judgments; single-model evaluation can miss aspects captured by aggregated/diverse judges.",
            "examples_of_loss": "Not provided in detail in this paper; cited as related evidence motivating multiple evaluators.",
            "counterexamples_or_caveats": "The present paper builds on this prior observation: it empirically shows multi-evaluator AIME improves internal error detection and optimization outcomes vs a single evaluator for code generation, but the authors do not perform direct human-vs-LLM comparisons themselves.",
            "paper_reference": "Related work paragraph: 'Verga et al. (2024) showed a panel of smaller LLM judges can provide numeric scores correlating to human judgment than a single larger LLM model can.'",
            "uuid": "e9723.1",
            "source_info": {
                "paper_title": "AIME: AI S YSTEM O PTIMIZATION VIA M ULTIPLE LLM E VALUATORS",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Single-Eval Suboptimality (theoretical)",
            "name_full": "Fixed Suboptimality Gap of a Single LLM Evaluator vs Optimal Evaluator",
            "brief_description": "The paper proves that sampling evaluations from a single evaluator π_e induces a fixed suboptimality gap (upper-bounded by a constant times the total variation distance between the single evaluator distribution and the unknown optimal evaluator distribution), which cannot be reduced without changing the evaluator.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "text-based AI system optimization; applied to code generation",
            "llm_judge_model": "Abstract single evaluator π_e (realized in experiments as a single GPT-4o evaluator conditioned on all roles together).",
            "llm_judge_setup": "Single LLM evaluator receives a system prompt that lists all evaluation roles together and outputs a single natural-language evaluation; used as the sole evaluation signal in iterative optimization.",
            "human_evaluation_setup": null,
            "agreement_metric": "No LLM-vs-human agreement metric reported for this result; the theoretical metric is a suboptimality gap ∆_π_Eva-sub-opt and an upper bound involving total variation distance (Eq. 2).",
            "losses_identified": "Using a single evaluator leads to a constant evaluation gap relative to an ideal evaluator (π*_e) that cannot be reduced without changing the evaluator distribution; practically, this manifests as undetected errors in multi-criteria tasks (e.g., failing to detect correctness or logic errors in generated code).",
            "examples_of_loss": "Equation (2) formalizes the fixed gap; Figure 1 and empirical EDR results illustrate the practical effect where a single evaluator fails to flag errors that cause all unit tests to fail.",
            "counterexamples_or_caveats": "The paper proves that forming a mixture (linear combination) of multiple independent evaluators can reduce the suboptimality gap (Theorem 1). Also, downstream components in the optimization pipeline (the feedback LLM) can sometimes compensate for missed evaluations from the single evaluator, reducing final task-performance differences.",
            "paper_reference": "Section 2 (Fixed Gap in Evaluation with Single Evaluation Policy), Equation (2), Theorem 1 and accompanying discussion.",
            "uuid": "e9723.2",
            "source_info": {
                "paper_title": "AIME: AI S YSTEM O PTIMIZATION VIA M ULTIPLE LLM E VALUATORS",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Replacing judges with juries: Evaluating llm generations with a panel of diverse models.",
            "rating": 2,
            "sanitized_title": "replacing_judges_with_juries_evaluating_llm_generations_with_a_panel_of_diverse_models"
        },
        {
            "paper_title": "An empirical study of llm-as-a-judge for llm evaluation: Fine-tuned judge models are task-specific classifiers",
            "rating": 2,
            "sanitized_title": "an_empirical_study_of_llmasajudge_for_llm_evaluation_finetuned_judge_models_are_taskspecific_classifiers"
        },
        {
            "paper_title": "On scalable oversight with weak llms judging strong llms",
            "rating": 2,
            "sanitized_title": "on_scalable_oversight_with_weak_llms_judging_strong_llms"
        },
        {
            "paper_title": "Large language models are state-of-the-art evaluators of translation quality",
            "rating": 1,
            "sanitized_title": "large_language_models_are_stateoftheart_evaluators_of_translation_quality"
        }
    ],
    "cost": 0.01246525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AIME: AI SYSTEM OPTIMIZATION VIA MULTIPLE LLM EVALUATORS
29 Oct 2024</p>
<p>Bhrij Patel 
University of Maryland
College Park</p>
<p>Souradip Chakraborty 
University of Maryland
College Park</p>
<p>Wesley A Suttle 
Army Research Laboratory
U.S
MDUSA</p>
<p>Mengdi Wang 
Princeton University</p>
<p>AmritSingh Bedi 
University of Central Florida</p>
<p>Denotes Equal Advising</p>
<p>Dinesh Manocha 
University of Maryland
College Park</p>
<p>Denotes Equal Advising</p>
<p>AIME: AI SYSTEM OPTIMIZATION VIA MULTIPLE LLM EVALUATORS
29 Oct 2024E666833303CFE6A16F5EBB60E73C6C84arXiv:2410.03131v3[cs.AI]
Text-based AI system optimization typically involves a feedback loop scheme where a single LLM generates an evaluation in natural language of the current output to improve the next iteration's output.However, in this work, we empirically demonstrate that for a practical and complex task (code generation) with multiple criteria to evaluate, utilizing only one LLM evaluator tends to let errors in generated code go undetected, thus leading to incorrect evaluations and ultimately suboptimal test case performance.Motivated by this failure case, we assume there exists an optimal evaluation policy that samples an evaluation between response and ground truth.We then theoretically prove that a linear combination of multiple evaluators can approximate this optimal policy.From this insight, we propose AI system optimization via Multiple LLM Evaluators (AIME).AIME is an evaluation protocol that utilizes multiple LLMs that each independently generate an evaluation on separate criteria and then combine them via concatenation.We provide an extensive empirical study showing AIME outperforming baseline methods in code generation tasks, with up to 62% higher error detection rate and up to 16% higher success rate than a single LLM evaluation protocol on LeetCodeHard and HumanEval datasets.We also show that the selection of the number of evaluators and which criteria to utilize is non-trivial as it can impact pact success rate by up to 12%.</p>
<p>INTRODUCTION</p>
<p>Pre-trained foundation models, such as Large Language Models (LLMs), have developed rapidly over the recent years (Achiam et al., 2023;Touvron et al., 2023).With these advancements, AI systems have grown in popularity for various tasks such as code generation (Chen et al., 2024;Gulwani, 2010), question-answering (Patel et al., 2024;Wang et al., 2024), mathematical reasoning (Trinh et al., 2024;Song et al., 2024), exploration (Dorbala et al., 2024;2023;Ren et al., 2024), and information retrieval (Gao et al., 2023) etc.As the application complexity increases, the shift to AI systems containing multiple components such as LLM-based agents and web search (Xiong et al., 2024), will continue (Zaharia et al., 2024;Yuksekgonul et al., 2024).Thus, automatically optimizing these systems, AI system optimization (Yuksekgonul et al., 2024), becomes increasingly necessary.</p>
<p>An emerging paradigm is text-based optimization, also known as prompt optimization (Cheng et al., 2023;Wang et al., 2023;Zhou et al., 2022), whereby the natural language input prompt is tuned to generate an optimal output.This method requires no numerical gradient descent updates typical in optimization for machine learning models (Van Der Malsburg, 1986;Hassoun, 1995;Barto, 1992) and is thus appropriate for optimizing AI systems with fixed LLM components.Recently, there has been a growing class of iterative online methods for text-based optimization (Cheng et al., 2024;Yuksekgonul et al., 2024;Shinn et al., 2024), where a single LLM generates an evaluation based on the current output to help generate the next iteration's prompt.</p>
<p>While prior art has compared the abilities of a single LLM for evaluations against those of multiple LLMs (Kocmi &amp; Federmann, 2023;Ankner et al., 2024), in AI system optimization literature, there  (Yuksekgonul et al., 2024) using our multiple LLM evaluator approach AIME (orange) and with single-evaluator approach (blue).[TOP RIGHT] The single-evaluator approach cannot detect an error in the generated code that fails all test cases.However, one of the evaluators of AIME could because the logical evaluator was independent from the correctness evaluator.[BOTTOM RIGHT] AIME-based optimization achieves ∼ 16% higher success rate than a single-evaluator approach in code generation tasks.has been a lack of studies questioning the capabilities of using a single LLM evaluator to drive the optimization process.Recently, Yuksekgonul et al. (2024) has viewed the evaluation as a text-based analogy to the objective function for backpropagation (Hinton et al., 2006;Rumelhart et al., 1986) in deep learning optimization.The objective function is a crucial element in optimizing machine learning models (Christiano et al., 2017;Mescheder et al., 2018;Chakraborty et al., 2023;Kingma &amp; Welling, 2014).This importance motivates us to analyze and strengthen the evaluation protocol of state-of-the-art (SoTA) AI system optimization frameworks by addressing a critical research question: What are the failure cases or tasks of utilizing only one LLM-based evaluator for text-based AI system optimization?</p>
<p>For this question, we empirically demonstrate the shortcomings of a single evaluator protocol in judging complex outputs like code based on multiple diverse criteria, such as correctness, readability, and runtime.We emphasize its practical limitations to give optimal evaluation while being instructed to judge based on all criteria simultaneously.Figure 1 illustrates the suboptimality in the practice of an AI system optimization framework with a single-evaluator approach to code generation.Furthermore, by assuming there exists an optimal evaluation policy that in expectation samples the true evaluation between the generated response and ground truth, we also theoretically highlight that the suboptimality gap between a single evaluator and an optimal evaluator is fixed and cannot be reduced given the same output and problem task.With this insight, we then naturally ask the following subsequent query: Can we develop a principled evaluation method for text-based optimization to handle multiple criteria?We address this question by assuming there exists an optimal evaluation policy that in expectation samples the true evaluation between the generated response and ground truth.We then theoretically prove that, under a linear additivity assumption, increasing the number of evaluators can reduce the suboptimality gap.We capitalize on this theoretical insight by proposing AIME: AI system optimization via Multiple Evaluators.AIME generates and combines via concatenation independent natural language evaluations from multiple evaluators based on different evaluation instructions.We demonstrate on code generation tasks with LeetCodeHard and HumanEval benchmarks the superior performance of AIME over a single evaluator in code error detection and the success rate of test cases.</p>
<p>Our main contributions are as follows:</p>
<p>• Novel Evaluation Approach for AI system Optimization: We propose using multiple LLM-based evaluators and introduce our AIME approach for iterative AI system optimization.We concatenate independent diverse samples from multiple LLM-based evaluation policies to better critique system outputs.</p>
<p>• Theoretical Motivation for Multiple Evaluators: We prove that through a linear additivity assumption increasing the number of evaluations can reduce the suboptimatity gap from an optimal evaluation policy while a single evaluator has a fixed gap.This theoretical result helps justify our formulation for a multiple evaluation-based protocol.</p>
<p>• Empirical Performance Over Single-Evaluation Approach: Using popular code generation dataset, LeetCodeHard (Shinn et al., 2024) and HumanEval (Chen et al., 2021), we perform an extensive study showing the superior prowess of AIME with 6 evaluators over single evaluation to detect errors, with AIME achieve up to 62% higher error detection rate than single evaluation.We then show that AIME-based optimization achieves up to a 16% higher success rate on test cases than optimization with only a single evaluator.We also reveal that the choice of the number of evaluators and the combination of criteria to utilize can affect the success rate by up to 12%, emphasizing the design of AIME-based optimization is non-trivial.We provide a code repository.1</p>
<p>TEXT-BASED AI SYSTEM OPTIMIZATION</p>
<p>Objective Function.In this section, we now characterize mathematically text-based prompt optimization as a system of LLM-based policies.Let π(•|x) be the LLM-based AI system parameterized by fixed LLM-based policy that samples an output response y ∼ π(•|x) given an input prompt x ∈ X from the set of prompts X .We aim to sample a y ∼ π(•|x * ) by finding an input prompt x * corresponding to x prompt such that y is closer to the optimal response y * .For code generation, π θ would be the LLM generator; x would be the input prompt; y is the generated code; and the y * here would be a code snippet that is a readable, efficient solution to the problem.Mathematically, we can write
x * = arg min x∈X E y∼π θ (•|x) [l(y * , y)],(1)
where l is a loss function to capture the closeness of sampled response y to the ground truth y * .</p>
<p>Iterative text-based optimization.Given an initial prompt x 1 , we perform an iterative text-based optimization method to find x * as follows.For each iteration t = 1 to T , we start by (i) sampling For simplicity, we use the same variable π for all LLM-based policies because the outputs are dependent on the input variables the policy is conditioned on, so the same LLM model can be utilized.In this paper, we use the same model, GPT-4o, for all steps.However, distinct LLM models can be employed at different steps.
y t ∼ π θ (•|x t ),(
Challenges.In an ideal setting, if we had the access to y * as in supervised learning (Tiwari, 2022), then we can achieve the optimal performance with larger data.However, in practice, they are hard to obtain or simply unknown for many tasks such as code generation (Chen et al., 2024).Therefore, a direct comparison to an optimal output y * and the resulting calculation of e in step (ii) are both infeasible.Current SoTA work instead sample an evaluation e from an evaluation policy conditioned by the response output y and prompt x as e ∼ π(•|x, y).Let us denote π e = π(•|x, y) for notation simplicitiy.Ideally, we would like the evaluation e of y to be l(y * , y).More specifically, let's assume the existence of an optimal evaluator LLM denoted by π * e , sampling from which will give us samples of the true loss function l(y * , y).</p>
<p>Fixed Gap in Evaluation with Single Evaluation Policy from Prior SOTA.As π * e is unavailable as discussed before, current SoTA methods sample the evaluation loss from a single evaluator as e ∼ π e .Now, we know that in the majority of the scenarios π e will not be the true evaluator policy π * e .Thus e = l(ŷ, y), where ŷ is an implicit approximation of y * from π e .Under this scenario, we define the suboptimality gap in evaluation of prior SOTA as
∆ π Eva-sub-opt = E e * ∼π * (•|x,y) [e * ] − E e∼π(•|x,y) [e] ≤ |e| max d TV (π * e (•|x, y), π(•|x, y))
(2) where we first expand upon the sub-optimality in evaluation and then upper-bound using the total variation distance (Sriperumbudur et al., 2009).We see that the term d TV (π * e (•|x, y), π(•|x, y)) is fixed and it cannot be improved once we have the evaluator π.This result shows the hardness of a single evaluator reaching π * e due to this constant gap and it will only reduce if our current LLM evaluator is near-optimal which is not true in majority of the scenarios.Empirically, Figure 1 demonstrates a practical observation where a single evaluator lets code errors go undetected, causing a large suboptimality gap from oracle performance in code generation tasks.</p>
<p>AIME: AI SYSTEM OPTIMIZATION VIA MULTIPLE LLM EVALUATORS</p>
<p>Our key idea is to utilize multiple evaluations than single evaluators used in state-of-the-art.The thought that multiple evaluators would work better than one sounds intuitive but a naive introduction of multiple evaluators does not work in practice.We theoretically prove the merit of multiple evaluators and then discuss how to introduce them into the pipeline described in Section 2.</p>
<p>INCREASING EVALUATIONS REDUCES THE EVALUATION SUBOPTIMALITY GAP
Let Π = {π k (•|x, y)} K
k=1 be the set of diverse evaluators for x, y.We start our theoretical justification by defining the sub-optimality metric to measure the evaluation performance between π * e and Π as
∆ Π Eva-sub-opt = E e∼π * e (•|x,y) [e] − E {e k ∼π k (•|x,y)} K k=1 [g(e 1 , • • • , e K )] ,(3)
which is nothing but the difference between the expected value of the evaluation under the optimal unknown evaluation distribution, and the expected function g(• • • ) which maps the K different evaluations to one.In practice, g can be seen as an aggregation function such as concatenation.</p>
<p>Note that if we had access to the optimal evaluator π * e , we would have been able to get the groundtruth evaluation e * = l(y * , y) to perform the AI text optimization.However, in place of that, we have a diverse set of evaluators Π = (π 1 , π 2 • • • π K ) and g(e 1 , e 2 • • • e K ) is the aggregation function to combine the losses from the diverse evaluators.We provide the following theorem to relate the number of evaluations to the ∆ Π Eva-sub-opt .Theorem 1.Let dTV denote the total variation distance between two distributions and let K k=1 α k = 1.Assuming all pairs π1, π2 ∈ Π are independent of one another,
∆ Π Eva-sub-opt ≤ |e * |dTV(π * e , K k=1 α k π k ).(4)
Proof.First, we characterize the sub-optimality of our proposed evaluation method as
∆ = E e * ∼π * e [e * ] − E e1∼π1(•|x,y),e2∼π2(•|x,y)•••π K [g(e 1 , e 2 , e 3 • • • e K )].
Note that if ∆ is zero, we are doing the optimal evaluation.Thus, we want ∆ to be as low as possible.For simplicity of the expression, we will keep to two evaluators and it can easily extend to K without loss of generality.
∆ = E e * ∼π * e [e * ] − E e1∼π1(•|x,y),e2∼π2(•|x,y) [g(e 1 , e 2 )] = E e * ∼π * e [e * ] − E e∼π d (•|x,y) [e] ∆1 + E e∼π d (•|x,y) [e] − E e1∼π1(•|x,y),e2∼π2(•|x,y) [g(e 1 , e 2 )]
∆2</p>
<p>.</p>
<p>where we add and subtract the terms
E e∼π d (•|x,y) , with π d = απ 1 + (1 − α)π 2 (0 &lt; α &lt; 1)
and then separate the two terms as ∆ 1 , ∆ 2 .We next individually analyze the terms ∆ 1 , ∆ 2 .</p>
<p>We can now bound ∆ 1 as,
∆ 1 = E e * ∼π * e [e * ] − E e∼π d (•|x,y) [l] ≤ |e * |d TV (π * , π d ) = |e * |d TV (π * , απ 1 + (1 − α)π 2 )
where we use the property of integral probability metric to bound ∆ 1 as the total variation distance between the optimal evaluation policy and the mixture evaluation policy.Next, we proceed to ∆ 2 ,
∆ 2 = E e∼π d (•|x,y) [e] − E e1∼π1(•|x,y),e2∼π2(•|x,y) [g(e 1 , e 2 )] = E e∼π d (•|x,y) [e] − E e1∼π1(•|x,y),e2∼π2(•|x,y) [αe 1 + (1 − α)e 2 ] = E e * ∼π d (•|x,y) [e * ] − αE e1∼π1(•|x,y) [e 1 ] − (1 − α)E e2∼π2(•|x,y) [e 2 ] = 0
Algorithm 1 AIME: AI System Optimization via Multiple LLM Evaluators 1: Input: Initial input prompt x 1 , number of iterations T , pre-trained LLM-based AI system π θ , list of K role descriptions R 2: for t in 1, . . ., T : do 3:</p>
<p>Initialize empty list of evaluations E t 4:
y t ∼ π θ (•|x t ) 5:
for k from 1, . . ., K: do
6: Sample e k,t ∼ π θ (•|x t , y t , R k ) 7: Append e k,t to E t 8:
Aggregate all e k,t ∈ E t into e t via concatenation 9:
Sample f t ∼ π θ (•|y t , e t , x t ) 10: Sample x t+1 ∼ π θ (•|y t , f t , x t )
where we expand upon the definition of ∆ 2 and use linear additivity assumption on the aggregation function, where we assume g(e 1 , e 2 ) = αe 1 + (1 − α)e 2 .Under this assumption, the two terms cancel out with the final result ∆ 2 = 0. Combining both terms concluded the proof.This bound indicates that the sub-optimality in evaluation can be expressed as the total variation distance between the optimal evaluator and the available mixture of evaluators.We know from Blei et al. (2003); Nguyen et al. ( 2016) that as we increase the number of mixture components and diversity amongst the components increase, it can approximate any distribution under certain assumptions.</p>
<p>OVERVIEW OF AIME: MULTIPLE ROLE-SPECIFIC EVALUATORS</p>
<p>Now that we have motivated utilizing multiple LLM-based evaluators, we now address the question on how to utilize multiple evaluators.To do so, we look at the ideas of roles.The LLM-based evaluation policy has an evaluation system prompt to specify what the evaluation should be based on.For tasks such as code generation, there may be multiple criteria or objectives to evaluate for such as correctness, clarity, and efficiency.Furthermore, aspects such correctness of code can rely on various aspects such as logic and syntax.Normally, with a single evaluator, all the criteria are specified together in the system prompt.However, we see from Figure 1 and later in Section 4 that this approach can fail significantly to reach the optimal performance.We thus propose splitting the evaluation instruction across multiple evaluators, each one getting a specific role.We then aggregate via string concatenation them into a final evaluation.We chose concatenation as the aggregation method as it is analogous to creating a linear combination of the outputs (Yuksekgonul et al., 2024).We call this approach AIME: AI System Optimization with Multiple Evaluators.</p>
<p>Our AIME approach is a simple-to-implement approach that requires minimal changes to the already established methods (Yuksekgonul et al., 2024;Cheng et al., 2024) for system optimization.Our approach requires only modifying the evaluation step of the optimization pipeline from one evaluator to multiple.In Algorithm 1, given an output y, set of k roles R, and pre-trained LLM π θ we sample k evaluations, {e k } K k=1 .We obtain e i by conditioning π θ by x, y and R k ∈ R. Conditioning on r k is to specify the role in the evaluation system prompt.</p>
<p>EXPERIMENTS AND RESULTS</p>
<p>We test the merits of our AIME approach via the code generation task because of its practicalness and its multiple plausible criteria (e.g., correctness, efficiency).Here, the AI system is an LLM generator that is given a code prompt and must produce a code snippet that passes the unit tests for that prompt.This code generation task is a form of instance optimization (Yuksekgonul et al., 2024), whereby the optimization variable, the input prompt, is defined as x t+1 := (y t , f t ).y 0 , f 0 are empty strings.We provide empirical results showing that AIME is superior to the single-evaluation (Single-Eval) approach in detecting code errors and that AIME-based optimization achieves higher success in test cases than Single-Eval-based optimization.Experiments were run on an Apple M1 Pro and macOS 14.5.</p>
<p>AIME and Single-Eval Implementation Details:</p>
<p>We use TextGrad from Yuksekgonul et al. (2024) to implement AIME and Single-Eval.We chose TextGrad because it separates the evaluation and feedback into two separate LLM calls, making it better to analyze the evaluation module in isolation.In TextGrad, the system prompt that generates the initial code, p init , is different from the system prompt that updates the code in the following refinement iterations p update .At t = 0, p init specifies to the LLM that it is a code generator while the p update from 1 ≤ t ≤ T specifies that it generates a new version y t+1 given the current code y t and the feedback f t .The transition from p init to p update is explicitly programmed and not caused by the optimization process.Because the scope of this paper lies within the evaluation protocol, our AI system is a single LLM generator.2LLM Setup Details: We use GPT-4o for all LLM calls and run 10 iterations of optimization for each coding problem.Across all trials for both methods, we use the same initial generated code for a given problem so both evaluation protocols can judge the same code in the initial iteration.For Single-Eval, the solitary LLM evaluator call is allowed 3600 max output tokens.For our AIME approach, each of the K evaluators is allowed 3600 K max output tokens.This decision is to model a uniform distribution of weights α.Note that when k = 1, Single-Eval and AIME are equivalent.We share the evaluation system prompt for both methods in Appendix A.1.We ablate on the temperature of the evaluation LLM.All other LLM calls in the Textgrad pipeline are given 2000 max output tokens with call temperature set to 0 similar to Yuksekgonul et al. (2024).For all experiments, the top p = 0.99.</p>
<p>Roles for Evaluating Code:</p>
<p>The set of evaluation roles R we used for this task are as follows: syntax errors, logic errors, correctness, readability, runtime, and code redundancy.The following results are based on utilizing all these roles.We chose three roles that correlate to maximizing the number of passed test cases: correctness, logic, and syntax.We specifically chose these three to incorporate an overall correctness role with two more specific roles.We will see in Section 4.1 that having overlapping roles can help with the robustness of evaluation in terms of error detection.The three other roles (readability, runtime, redundancy), correlate to criteria such as clarity and efficiency.We will later see in Section 4.3.2 that utilizing only these roles for evaluation decreases the overall performance of the code generation task.</p>
<p>Datasets: We use the following two datasets, LeetCodeHard (Shinn et al., 2024) and HumanEval (Chen et al., 2021), where each dataset contains a set of coding problem prompts and multiple unit tests for each problem to evaluate the generated code.We use the entire LeetCodeHard dataset of 39 problems with an average of 2.2 unit tests per problem and the first 20 problems of HumanEval with an average of 4.4 unit tests per problem.We withhold giving any of the evaluators of either method any information on unit tests to simulate the scenario where unit tests may be unavailable to help judge (Chen et al., 2024).</p>
<p>AIME IS ROBUST TO INCORRECT EVALUATIONS</p>
<p>AIME has a higher chance to catch errors: Figure 1 displays portions of an evaluation generated by Single-Eval and AIME.In this scenario, the evaluations were generated for the same coding problem at the second iteration of optimization.For both Single-Eval and AIME, the code failed all test cases, thus meaning there exists some error in the code.The evaluation from Single-Eval for both correctness and logic states there is nothing wrong.For AIME, the correctness evaluator incorrectly states nothing is wrong with the generated code but the logic evaluator detects a logical error.In the next iteration of optimization, the code generated based on the Single-Eval evaluation still fails all cases but the code generated from AIME passes them all.</p>
<p>Error Detection Measurement: To quantitatively analyze the error detection of AIME, we develop a heuristic measurement, Error Detection Rate (EDR).For each optimization iteration that has at least one failed test case, if the given evaluation contains at least one phrase indicating failure, we consider that as an error was detected.For example if the phrase "has a logical error" appears in the evaluation, we count that as an error detected.We provide a complete list of phrases used for detection in Appendix A.2. Let Z fail be the set of iterations with at least one failed test case and let q(z) = 1 error detected be the indicator value of whether an error was detected at iteration z ∈ Z fail .We calculate the EDR as 1 |Zfail| z∈Zfail q z .Left of Figure 2 shows AIME has up to ∼ 62% higher EDR than Single-Eval.AIME has a higher EDR score on both datasets indicating it is less prone to letting errors go undetected.AIME has a higher resistance to an adversarial evaluator on LeetCodeHard and a comparable resistance on HumanEval, suggesting its robustness over Single-Eval multiple independent evaluators can ensure a more accurate assessment than conditioning a single evaluator with all roles at once.</p>
<p>Robustness to Adversarial Evaluator (RAE):</p>
<p>To further highlight the robustness of AIME to incorrect evaluations, we introduce an adversarial evaluator.For AIME, we specify in the system prompt of the correctness evaluator to always generate an evaluation stating that the code solution works.Similarly, for Single-Eval, we specify in the system prompt of the single evaluator to output an evaluation claiming that code works when discussing correctness.We provide these adversarial system prompts in Figure 6.We run experiments with an evaluation temperature of 1.To measure the robustness to the adversarial evaluator (RAE), we calculate the percent decrease of the EDR from the non-adversarial setting to the adversarial one.We then report the absolute value of the percent decrease subtracted from 1. Formally, let p c be the percent change of the EDR, our RAE metric is 1 − |p c |. Right of Figure 2 reports the mean and standard deviation RAE over 3 trials.AIME achieves 16% higher RAE over Single-Eval on LeetCodeHard and comparable RAE over HumanEval, emphasizing AIME increased safety for AI systems.</p>
<p>AIME evaluations are more thorough: In Figure 3, we highlight the readability portions of the same evaluation in Figure 1.Even though both Single-Eval and AIME did not see errors in readability, AIME is more thorough and explains its evaluation while Single-Eval only gives a one-sentence judgment.We believe this also to be because of the independence of the readability evaluator in AIME as the evaluator does not feel the need to move on to the next role like in Single-Eval even though there is nothing to critique.AIME is thus more helpful in terms of explainability.Please see Appendix A.5 for more comparisons between evaluations AIME and Single-Eval.</p>
<p>AIME-BASED OPTIMIZATION ACHIEVES HIGHER TASK PERFORMANCE</p>
<p>Now that we have established the error detection capabilities of AIME over Single-Eval, we now focus on the overall performance of system optimization with AIME on the code generation task.For these experiments, we provide results with two additional baselines: 1) Zero-Shot: Initial generated code with no iterative optimization process; 2) Refinement with No Separate Text-based Evaluation Step (Implicit Eval): The evaluation and feedback steps are within the same LLM "reflection" call.The LLM reflection call is allowed 3600 max output tokens and is sampled once per iteration.We implement this baseline with Reflexion by Shinn et al. (2024).</p>
<p>Metrics for Code Correctness:</p>
<p>We report the following metrics to inspect the correctness of the code generated; for AIME, Single-Eval, and Implicit Eval, we report these metrics using the bestperforming code generated in the optimization process after the initial zero-shot generation: 1) Success Rate (SR), the percentage of test cases passed across the entire dataset; 2) Completion Rate (CR), the percentage of coding problems with all passed test cases.</p>
<p>Test Case Results: We plot the performance over 3 trials on both datasets in Figure 4. Please see Table 3 in Appendix A.3 where we report the standard deviation and ablate the temperature of the Figure 3: Independent evaluator of AIME provides more thorough explanations: Example evaluations for readability generated by Single-Eval and AIME.Both evaluations are for the same coding task at the same iteration which failed all test cases.Even though both Single-Eval and AIME believe that the code is readable with no criticisms, AIME's readability comment is more thorough.This result may be because it was generated independently from evaluations of other criteria.Without having other to worry about other roles, the readability evaluator was allowed to focus its entire output on readability.evaluation LLM call.Over both datasets, AIME consistently has the highest SR and CR rates with up to ∼ 13% higher SR and ∼ 18% higher CR.</p>
<p>Remark: The analysis on EDR in Section 4.1 is specifically for comparing the error detection capabilities of the evaluation protocols, it does not take into account the downstream feedback LLM call in Textgrad system pipeline.This point may explain why Single-Eval can have a significantly lower error detection rate than AIME but then have a much smaller gap in SR and CR, as the feedback LLM is possibly also detecting errors and disregarding the incorrect evaluations.Another possibility for the low error detection rate could be more detection phrases are needed to give a better estimate for Single-Eval.</p>
<p>ABLATION STUDIES</p>
<p>INCREASING NUMBER OF EVALUATORS AND DIVERSITY OF ROLES HELPS</p>
<p>We perform two experiments: 1) for AIME-based optimization, we ablate on the number of evaluators from 1 → 3 → 6.However, each evaluator has the same role.Max output tokens in each experiment across all evaluators is 3600.When all the evaluators have the correctness role (left of Figure 5), the EDR for AIME increases.This result emphasizes that AIME-based evaluations, even without role-specific evaluators, can detect more errors than Single-Eval.This finding then begs the question of whether there is a need for different roles to optimize for passed test cases if increasing the number of same-role evaluators already helps.When comparing the SR, CR, and EDR of AIME with 6 correctness evaluators against AIME with 6 distinct roles (correctness, logic, syntax, readability, runtime, redundancy), the increased diversity of roles raises these metrics (right of Figure 5).In the following study, we analyze which roles impact performance.When setting all the evaluators of AIME to the same role, correctness, and increasing the number of evaluators from 1 → 3 → 6 increases EDR.This result shows that even if there is only one role, multiple independent evaluations can help catch errors.</p>
<p>[RIGHT] With six evaluators, having 6 distinct roles has better SR, CR, and EDR, than all of the evaluators having the same role, correctness.Single-Eval and AIME given different combinations of roles.We report the mean and standard deviation of 3 trials.For the experiments with a single role, as in K = 1, Single-Eval and AIME are the same.We see that SR and CR drops when not utilizing syntax, logic, or correctness evaluators.We also see that the SR and CR drop is not as significant for Single-Eval as it is for AIME, suggesting that Single-Eval protocol is less dependent on the roles correlated with maximizing passed test cases.</p>
<p>Syntax</p>
<p>COMBINATION OF EVALUATION ROLES AFFECTS OPTIMIZATION PERFORMANCE</p>
<p>We now analyze the effect the different roles have on SR and CR on LeetCodeHard.We perform this study for two reasons: 1) to see the change in performance due to utilizing various evaluation roles and 2) to see how the relative performance between Singl-Eval and our AIME changes based on the roles given.The total max output tokens for evaluation is still 3600, and for AIME, it is distributed equally across the evaluators.Therefore, for experiments with 3 evaluators, each one has max output tokens of 1200.</p>
<p>Table 1 summarizes our results and reports the mean and standard deviation over 3 trials for each experiment.All experiments were run with an evaluation temperature of 1.When only utilizing the readability, runtime, and code redundancy evaluators, SR and CR degrade by ∼ 12% and ∼ 18%, respectively, for AIME.Interestingly, this combination of roles is also the only time in this ablation that Single-Eval performs higher in SR and CR than AIME.This outperformance is because the degradation in SR and CR for Single-Eval is significantly less than for AIME, suggesting that AIME was more dependent on the correctness, logic, and syntax roles for optimizing unit tests than Single-Eval.However, for all other experiments, AIME still has higher SR and CR, supporting the idea that separating the evaluation into role-specific policies allows for generally higher performance than a single evaluator across different combinations of roles.</p>
<p>Furthermore, for both Single-Eval and AIME, the SR drops by 3−5% when going from using syntax, correctness, and logic, to using only one of them.This suggests that using all three in combination increases the evaluation in terms of maximizing passed unit tests.In Appendix A.4, we perform two similar ablation studies.In one study, we give the evaluators information on what test cases passed and failed.In the second study, we provide information on what passed and failed and include an explanation of each failure.</p>
<p>RELATED WORKS</p>
<p>AI System Optimization: Many prior works have studied the optimization of complex AI systems.Madaan et al. (2024) was one of the first works to propose a text-based iterative feedback loop for refining LLMs, and Pryzant et al. ( 2023) established text-based gradients, or Textual Gradients, as feedback to an AI system.DSPy (Khattab et al., 2024;2022;Singhvi et al., 2023), Trace Cheng et al. (2024), and TextGrad (Yuksekgonul et al., 2024) have formulated LLM and AI-based systems as a network of multiple layers and provided methods to optimize these system analogous to backpropagation and autodifferentiation.Chakraborty et al. (2024a); Ding et al. (2024) used a bi-level optimization formulation to align AI agents and systems.Text-based reinforcement learning has also been used to improve LLM-based systems (Shinn et al., 2024).Decoding and RLHF is an alternative method to optimize or align an LLM with gradient descent (Chakraborty et al., 2024b;Mudgal et al., 2023;Chakraborty et al., 2024c).While these works have shown tremendous results, there has been a gap in the literature we aim to address analyzing the effect of using multiple independent evaluations to optimize the AI system for a complex task, code generation (Chen et al., 2024;Zeng et al., 2024;Zhang et al., 2023;Jha et al., 2010;Shinn et al., 2024;Yuksekgonul et al., 2024;Zan et al., 2022;Jiang et al., 2024;Chen et al., 2021;Gulwani, 2010).</p>
<p>LLM-based Evaluation: LLM-based evaluation, or LLM-as-a-Judge (Zheng et al., 2023), has been growing in interest due to the ability of LLMs to evaluate large outputs like text (Sellam et al., 2020;Kocmi &amp; Federmann, 2023) quickly and to align with human preferences.Verga et al. (2024) showed a panel of smaller LLM judges can provide numeric scores correlating to human judgment than a single larger LLM model can.Prior work has also studied finetuning LLMs to be judges (Zhou et al., 2022).Ankner et al. (2024) used LLM-generated critiques to augment the scalar reward from a reward model.Li et al. (2023) used discussion between multiple LLMs to select a strong LLMbased evaluator for question-answering.Strong LLM judges have been shown to generalize across tasks (Huang et al., 2024).Weak LLM evaluators have been used to judge the debate between two stronger LLMs (Kenton et al., 2024).We are the first to use multiple LLM-based evaluators for iterative AI system optimization.</p>
<p>CONCLUSION, LIMITATIONS, AND FURTHER WORKS</p>
<p>In this work, we tackle AI system optimization by introducing AIME.AIME utilizes multiple LLMbased evaluators to provide natural language evaluation for the current system output, improving on prior methods that only use a single evaluator.Our key insight is to condition each evaluator with a specific role rather than giving all the roles to a single evaluator.We prove that increasing the number of evaluations reduces the suboptimality evaluation gap, and empirically demonstrate that AIME outperforms Single-Eval in code generation tasks, analyzing success, completion, and error detection rates.Furthermore, we study AIME's robustness to the adversarial evaluator that generate incorrect evaluations.We also provide ablations such as on the diversity of roles, role combinations, and evaluation temperature, consistently demonstrating AIME's superior performance and the need for multiple evaluators.</p>
<p>Limitations and Further Work.We only empirically study our approach in code generation.Further work could extend this evaluation approach to other tasks that require multiple criteria like molecule optimization or text generation.In terms of system complexity, we only study multiple evaluators for AI systems comprising a single LLM-based agent, and using a compound system with multiple elements such as a web search agent (Agentic AI system) could be interesting.Another aspect of the work that can be explored further is weighting the different LLM-based evaluations.We gave uniform weighting to all evaluations by giving them the same max output tokens and concatenating them.Future research could investigate methods of weighting and aggregation, possibly using another LLM to summarize or perform best-of-N on the evaluations.</p>
<p>A APPENDIX A.1 EVALUATION SYSTEM PROMPT</p>
<p>We provide the evaluation system prompt in Figure 6.For Single-Eval the system prompt is given to only one LLM call and all the roles utilized are listed together in Remark: It may seem conflicting that we specify conciseness in the evaluation system prompt and highlight that the evaluations from AIME are more descriptive in Figure 3.However, we would like to clarify that we do not believe that the evaluations are verbose, using more words without giving more information.The longer, thorough evaluations from AIME like in Figure 3 provide more information on their judgment, helping with the explainability of the evaluation model.• not correct</p>
<p>• some issue</p>
<p>• there seems to be some issues</p>
<p>Figure 1 :
1
Figure 1: AI System Optimization Pipeline and Increased Error Detection and Success Rate with AIME-based Evaluation: [LEFT] Text-based AI system optimization with SoTA framework(Yuksekgonul et al., 2024) using our multiple LLM evaluator approach AIME (orange) and with single-evaluator approach (blue).[TOP RIGHT] The single-evaluator approach cannot detect an error in the generated code that fails all test cases.However, one of the evaluators of AIME could because the logical evaluator was independent from the correctness evaluator.[BOTTOM RIGHT] AIME-based optimization achieves ∼ 16% higher success rate than a single-evaluator approach in code generation tasks.</p>
<p>Figure 2 :
2
Figure 2: Using LeetCodeHard and HumanEval benchmarks we compare evaluations generated from Single-Eval against those of AIME in terms of [LEFT] EDR and [RIGHT] RAE scores.AIME has a higher EDR score on both datasets indicating it is less prone to letting errors go undetected.AIME has a higher resistance to an adversarial evaluator on LeetCodeHard and a comparable resistance on HumanEval, suggesting its robustness over Single-Eval</p>
<p>Figure 4 :
4
Figure 4: [BAR PLOT] Success Rate and Completion Rate and [LINE PLOT] Best Completion Rate over max number of iterations for [LEFT] LeetCodeHard and [RIGHT] HumanEval.Over 10 iterations for each coding problem, AIME has the highest SR and CR over both datasets.</p>
<p>Figure 5 :
5
Figure 5: Increasing Number of Evaluator and Diversity Helps: [LEFT]When setting all the evaluators of AIME to the same role, correctness, and increasing the number of evaluators from 1 → 3 → 6 increases EDR.This result shows that even if there is only one role, multiple independent evaluations can help catch errors.[RIGHT]With six evaluators, having 6 distinct roles has better SR, CR, and EDR, than all of the evaluators having the same role, correctness.</p>
<p>[INSERT UTILIZED ROLE].For AIME, each evaluator gets one role specified in [INSERT UTILIZED ROLE].</p>
<p>Figure 6 :
6
Figure 6: Evaluation System Prompt.</p>
<p>Figure 7 :
7
Figure 7: Comparison of evaluations from Single-Eval and AIME for LeetCodeHard problem: minimum-time-to-visit-a-cell-in-a-grid.These evaluations are the full versions of the ones analyzed in the main body in Figures 1 and 3.</p>
<p>Figure 8 :
8
Figure 8: Comparison of evaluations from Single-Eval and AIME for LeetCodeHard problem: paths-in-matrix-whose-sum-is-divisible-by-k.</p>
<p>Figure 9 :
9
Figure 9: Comparison of evaluations from Single-Eval and AIME for LeetCodeHard problem: count-number-of-possible-root-nodes.</p>
<p>Figure 10 :
10
Figure 10: Comparison of evaluations from Single-Eval and AIME for LeetCodeHard problem: minimum-number-of-visited-cells-in-a-grid.</p>
<p>Yuksekgonul et al. (2024) y t to obtain evaluation e t = l(y * , y t ), and then finally (iii) generate the next prompt x t+1 ∼ π(•|y t , e t , x t ).Recent work byYuksekgonul et al. (2024)decompose step (iii) into two separate steps and (iii.a)first generate the feedback f t ∼ π(•|y t , e t , x t ), and then (iii.b)generate the next prompt x t+1 ∼ π(•|y t , f t , x t ).</p>
<p>Table 2 in Appendix A.3 summarizes the EDR for Single-Eval and AIME across various evaluation call temperatures.AIME achieves ∼ 53 − 62% higher error detection rate than Single-Eval on LeetCodeHard and ∼ 38 − 57% higher rate on HumanEval.This demonstrates that</p>
<p>Table 1 :
1
Utilzing Different Roles Affects SR and CR: This table summarizes the SR and CR for
Correct-nessLogicRead-abilityRun-TimeCode Redun-dancyMetric (%)Single-EvalAIME (Ours)✓✓✓✓✓✓SR CR83.70 ± 2.28 89.26 ± 2.10 76.07 ± 1.21 82.91 ± 1.21✗✗✗✓✓✓SR CR80.74 ± 2.10 77.41 ± 1.39 66.67 ± 4.19 64.96 ± 3.20✓✓✓✗✗✗SR CR87.78 ± 1.81 81.20 ± 1.21 80.34 ± 1.21 88.89 ± 0.91✓✗✗✗✗✗SR CR83.70 ± 1.05 5.21 ± 1.21✗✓✗✗✗✗SR CR85.55 ± 3.27 75.21 ± 3.20✗✗✓✗✗✗SR CR85.93 ± 2.28 77.78 ± 3.20✗✗✓✓✗✗SR CR87.04 ± 3.78 79.49 ± 5.54 80.34 ± 3.20 88.51 ± 1.89✗✗✗✓✗✗SR CR79.26 ± 1.39 70.01 ± 3.20</p>
<p>Table 2 :
2
AIME detects more code errors than Single-Eval: Error Detection Rates of evaluations generated by Single-Eval and AIME.Over all temperatures, AIME detects has up to 61% and 72% higher rate than Single-Eval on LeetCodeHard and HumanEval, respectively.Thus, multiple independent role-specific evaluators are more likely to detect errors than a single evaluator with all roles.A.4 GIVING EVALUATORS TEST RESULT INFORMATIONA.5 EXAMPLE EVALUATIONSTo emphasize the more thorough evaluations from our AIME method, we provide a few more comparisons of evaluations generated by AIME and Single-Eval.
• has issue• have issueA.3 EVALUATION TEMPERATURE ABLATION ON EDR AND OVERALL PERFORMANCEEval LLM Call TempDatasetSingle-EvalAIME (Ours)0LeetCodeHard 38.06 ± 6.80 91.20 ± 0.90 HumanEval 10.99 ± 2.33 49.0 ± 6.020.25LeetCodeHard 34.19 ± 2.88 90.67 ± 1.05 HumanEval 19.65 ± 1.27 76.37 ± 12.880.50LeetCodeHard 29.49 ± 4.06 HumanEval 17.90 ± 9.15 55.80 ± 2.54 91.93 ± 2.420.75LeetCodeHard 35.43 ± 2.53 HumanEval 3.80 ± 1.1590.09 ± 0.39 53.61 ± 5.981LeetCodeHard 31.36 ± 4.25 HumanEval 8.13 ± 5.80 60.45 ± 12.99 91.07 ± 2.45</p>
<p>Table 3 :
3
The success and completion rates for AIME (ours) and Single-Eval on LeetCodeHard code generation datasets with varying values for evaluating LLM call temperature.Consistent with Figure4, AIME generally outperforms Single-Eval.
Syn-taxCorrect-nessLogicRead-abilityRun-TimeCode Redun-dancyMetric (%)Single-EvalAIME (Ours)Tests given with failure explanations✓✓✓✓✓✓SR CR88.15 ± 1.39 90.00 ± 1.57 81.20 ± 2.42 82.91 ± 3.20✗✗✗✓✓✓SR CR86.30 ± 0.52 89.26 ± 1.89 79.49 ± 2.09 83.76 ± 3.20✓✓✓✗✗✗SR CR87.78 ± 3.27 88.14 ± 1.39 80.34 ± 4.36 80.34 ± 1.21Tests given with no failure explanation✓✓✓✓✓✓SR CR85.19 ± 0.52 90.37 ± 1.89 79.49 ± 2.09 82.91 ± 2.42✗✗✗✓✓✓SR CR84.44 ± 1.81 86.67 ± 0.91 74.36 ± 3.63 78.63 ± 2.42✓✓✓✗✗✗SR CR86.67 ± 1.81 86.30 ± 3.78 79.49 ± 0.00 77.78 ± 4.36</p>
<p>Table 4 :
4
Impact of different role combination like in Table1.Here, we give the evaluators which test passed or failed[TOP]with failure explanations [BOTTOM] without failure explanation.Failure explanations could be runtime errors or incorrect return values.</p>
<p>Repository to code: https://github.com/Bridge00/aime
We repeat the link to the repository: https://github.com/Bridge00/aime</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Zachary Ankner, Mansheej Paul, Brandon Cui, Jonathan D Chang, Prithviraj Ammanabrolu, arXiv:2408.11791Critique-out-loud reward models. 2024arXiv preprint</p>
<p>Reinforcement learning and adaptive critic methods. Handbook of intelligent control: Neural, fuzzy, and adaptive approaches. G Andrew, Barto, 1992</p>
<p>Latent dirichlet allocation. Andrew Y David M Blei, Michael I Ng, Jordan, Journal of machine Learning research. 3Jan. 2003</p>
<p>Rebel: A regularization-based solution for reward overoptimization in reinforcement learning from human feedback. Souradip Chakraborty, Amisha Bhaskar, Anukriti Singh, Pratap Tokekar, Dinesh Manocha, Amrit Singh Bedi, arXiv:2312.144362023arXiv preprint</p>
<p>Parl: A unified framework for policy alignment in reinforcement learning from human feedback. Souradip Chakraborty, Amrit Singh Bedi, Alec Koppel, Dinesh Manocha, Huazheng Wang, Mengdi Wang, Furong Huang, 2024a</p>
<p>Souradip Chakraborty, Soumya Suvra Ghosal, Ming Yin, Dinesh Manocha, Mengdi Wang, Amrit Singh Bedi, Furong Huang, arXiv:2405.20495Transfer q star: Principled decoding for llm alignment. 2024barXiv preprint</p>
<p>Souradip Chakraborty, Jiahao Qiu, Hui Yuan, Alec Koppel, Furong Huang, Dinesh Manocha, Amrit Singh Bedi, Mengdi Wang, arXiv:2402.08925Maxmin-rlhf: Towards equitable alignment of large language models with diverse human preferences. 2024carXiv preprint</p>
<p>A survey on evaluating large language models in code generation tasks. Liguo Chen, Qi Guo, Hongrui Jia, Zhengran Zeng, Xin Wang, Yijiang Xu, Jian Wu, Yidong Wang, Qing Gao, Jindong Wang, arXiv:2408.164982024arXiv preprint</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De, Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Trace is the new autodiff-unlocking efficient optimization of computational workflows. Ching-An Cheng, Allen Nie, Adith Swaminathan, arXiv:2406.162182024arXiv preprint</p>
<p>Black-Box Prompt Optimization: Aligning Large Language Models without Model Training. Jiale Cheng, Xiao Liu, Kehan Zheng, Pei Ke, Hongning Wang, Yuxiao Dong, Jie Tang, Minlie Huang, 10.48550/arXiv.2311.04155arXiv:2311.04155November 2023arXiv e-prints, art</p>
<p>Deep reinforcement learning from human preferences. Advances in neural information processing systems. Jan Paul F Christiano, Tom Leike, Miljan Brown, Shane Martic, Dario Legg, Amodei, 201730</p>
<p>Sail: Self-improving efficient online alignment of large language models. Mucong Ding, Souradip Chakraborty, Vibhu Agrawal, Zora Che, Alec Koppel, Mengdi Wang, Amrit Bedi, Furong Huang, arXiv:2406.155672024arXiv preprint</p>
<p>Can an embodied agent find your" cat-shaped mug"? llm-guided exploration for zero-shot object navigation. Sashank Vishnu, James F Dorbala, Dinesh MullenJr, Manocha, arXiv:2303.034802023arXiv preprint</p>
<p>Right place, right time! towards objectnav for non-stationary goals. Bhrij Vishnu Sashank Dorbala, Amrit Patel, Dinesh Singh Bedi, Manocha, 2024</p>
<p>Retrieval-augmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Haofen Wang, arXiv:2312.109972023arXiv preprint</p>
<p>Dimensions in program synthesis. Sumit Gulwani, Proceedings of the 12th international ACM SIGPLAN symposium on Principles and practice of declarative programming. the 12th international ACM SIGPLAN symposium on Principles and practice of declarative programming2010</p>
<p>Fundamentals of Artificial Neural Networks. Mh Hassoun, 1995The MIT Press</p>
<p>A fast learning algorithm for deep belief nets. Geoffrey E Hinton, Simon Osindero, Yee-Whye Teh, Neural computation. 1872006</p>
<p>An empirical study of llm-asa-judge for llm evaluation: Fine-tuned judge models are task-specific classifiers. Hui Huang, Yingqi Qu, Jing Liu, Muyun Yang, Tiejun Zhao, arXiv:2403.028392024arXiv preprint</p>
<p>Oracle-guided component-based program synthesis. Susmit Jha, Sumit Gulwani, Sanjit A Seshia, Ashish Tiwari, Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering. the 32nd ACM/IEEE International Conference on Software Engineering20101</p>
<p>A survey on large language models for code generation. Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, Sunghun Kim, arXiv:2406.005152024arXiv preprint</p>
<p>Zachary Kenton, Noah Y Siegel, János Kramár, Jonah Brown-Cohen, Samuel Albanie, Jannis Bulian, Rishabh Agarwal, David Lindner, Yunhao Tang, Noah D Goodman, arXiv:2407.04622On scalable oversight with weak llms judging strong llms. 2024arXiv preprint</p>
<p>Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp. Omar Khattab, Keshav Santhanam, Lisa Xiang, David Li, Percy Hall, Christopher Liang, Matei Potts, Zaharia, arXiv:2212.140242022arXiv preprint</p>
<p>Dspy: Compiling declarative language model calls into state-of-the-art pipelines. Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Saiful Haq, Ashutosh Sharma, Thomas T Joshi, Hanna Moazam, Heather Miller, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Auto-Encoding Variational Bayes. P Diederik, Max Kingma, Welling, ICLR 20142nd International Conference on Learning Representations. Conference Track Proceedings. Banff, AB, CanadaApril 14-16, 2014. 2014</p>
<p>Large language models are state-of-the-art evaluators of translation quality. Tom Kocmi, Christian Federmann, Proceedings of the 24th Annual Conference of the European Association for Machine Translation. Mary Nurminen, Judith Brenner, Maarit Koponen, Sirkku Latomaa, Mikhail Mikhailov, Frederike Schierl, Tharindu Ranasinghe, Eva Vanmassenhove, Alvarez Sergi, Nora Vidal, Mara Aranberri, Carla Nunziatini, Mikel Parra Escartín, Maja Forcada, Carolina Popovic, Helena Scarton, Moniz, the 24th Annual Conference of the European Association for Machine TranslationTampere, FinlandJune 2023European Association for Machine Translation</p>
<p>Ruosen Li, arXiv:2307.02762Teerth Patel, and Xinya Du. Prd: Peer rank and discussion improve large language model based evaluations. 2023arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Advances in Neural Information Processing Systems. 202436</p>
<p>Which training methods for gans do actually converge. Lars Mescheder, Andreas Geiger, Sebastian Nowozin, International conference on machine learning. PMLR2018</p>
<p>Sidharth Mudgal, Jong Lee, Harish Ganapathy, Yaguang Li, Tao Wang, Yanping Huang, Zhifeng Chen, Heng-Tze Cheng, Michael Collins, Trevor Strohman, arXiv:2310.17022Controlled decoding from language models. 2023arXiv preprint</p>
<p>A universal approximation theorem for mixture-of-experts models. Luke R Lloyd-Jones Hien D Nguyen, Geoffrey J Mclachlan, Neural computation. 28122016</p>
<p>Embodied question answering via multi-llm systems. Bhrij Patel, Sashank Vishnu, Dinesh Dorbala, Amrit Singh Manocha, Bedi, 2024</p>
<p>Automatic prompt optimization with" gradient descent" and beam search. Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, Michael Zeng, arXiv:2305.034952023arXiv preprint</p>
<p>Explore until confident: Efficient exploration for embodied question answering. Jaden Allen Z Ren, Anushri Clark, Masha Dixit, Anirudha Itkina, Dorsa Majumdar, Sadigh, arXiv:2403.159412024arXiv preprint</p>
<p>Learning representations by backpropagating errors. Geoffrey E David E Rumelhart, Ronald J Hinton, Williams, nature. 32360881986</p>
<p>BLEURT: Learning robust metrics for text generation. Thibault Sellam, Dipanjan Das, Ankur Parikh, doi: 10.18653Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Dan Jurafsky, Joyce Chai, Natalie Schluter, Joel Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 2020</p>
<p>URL. </p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202436</p>
<p>Arnav Singhvi, Manish Shetty, Shangyin Tan, Christopher Potts, Koushik Sen, Matei Zaharia, Omar Khattab, arXiv:2312.13382Dspy assertions: Computational constraints for self-refining language model pipelines. 2023arXiv preprint</p>
<p>Towards large language models as copilots for theorem proving in lean. Peiyang Song, Kaiyu Yang, Anima Anandkumar, arXiv:2404.125342024arXiv preprint</p>
<p>Kenji Bharath K Sriperumbudur, Arthur Fukumizu, Bernhard Gretton, Gert Rg Schölkopf, Lanckriet, arXiv:0901.2698On integral probability metrics,\phi-divergences and binary classification. 2009arXiv preprint</p>
<p>Chapter 2 -supervised learning: From theory to applications. Ashish Tiwari, org/10.1016/B978-0-12-824054-0.00026-5Artificial Intelligence and Machine Learning for EDGE Computing. Rajiv Pandey, Sunil Kumar Khatri, Neeraj Kumar Singh, Parul Verma, Academic Press2022</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Solving olympiad geometry without human demonstrations. Yuhuai Trieu H Trinh, Wu, He Quoc V Le, Thang He, Luong, Nature. 62579952024</p>
<p>Frank rosenblatt: Principles of neurodynamics: Perceptrons and the theory of brain mechanisms. C Van Der, Malsburg, Brain Theory. Günther Palm, Ad Aertsen, Berlin, Heidelberg; Berlin HeidelbergSpringer1986</p>
<p>Replacing judges with juries: Evaluating llm generations with a panel of diverse models. Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White, Patrick Lewis, 2024</p>
<p>Leave no document behind: Benchmarking long-context llms with extended multi-doc qa. Minzheng Wang, Longze Chen, Cheng Fu, Shengyi Liao, Xinghua Zhang, Bingli Wu, Haiyang Yu, Nan Xu, Lei Zhang, Run Luo, arXiv:2406.174192024arXiv preprint</p>
<p>PromptAgent: Strategic Planning with Language Models Enables Expertlevel Prompt Optimization. Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P Xing, Zhiting Hu, 10.48550/arXiv.2310.16427arXiv:2310.16427October 2023arXiv e-prints, art</p>
<p>When search engine services meet large language models: Visions and challenges. Haoyi Xiong, Jiang Bian, Yuchen Li, Xuhong Li, Mengnan Du, Shuaiqiang Wang, Dawei Yin, Sumi Helal, IEEE Transactions on Services Computing. 2024</p>
<p>Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, James Zou, arXiv:2406.07496Textgrad: Automatic "differentiation" via text. 2024arXiv preprint</p>
<p>The shift from models to compound ai systems. Matei Zaharia, Omar Khattab, Lingjiao Chen, Jared Quincy Davis, Heather Miller, Chris Potts, James Zou, Michael Carbin, Jonathan Frankle, Naveen Rao, Ali Ghodsi, 2024</p>
<p>Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Yongji Wang, Jian-Guang Lou, arXiv:2212.09420Large language models meet nl2code: A survey. 2022arXiv preprint</p>
<p>Coderujb: An executable and unified java benchmark for practical programming scenarios. Zhengran Zeng, Yidong Wang, Rui Xie, Wei Ye, Shikun Zhang, 10.1145/3650212.3652115ISBN. 20242024Association for Computing Machinery</p>
<p>Planning with large language models for code generation. Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B Tenenbaum, Chuang Gan ; Lianmin, Wei-Lin Zheng, Ying Chiang, Siyuan Sheng, Zhanghao Zhuang, Yonghao Wu, Zi Zhuang, Zhuohan Lin, Dacheng Li, Eric P Li, Hao Xing, Joseph E Zhang, Ion Gonzalez, Stoica, arXiv:2303.055102023. 2023arXiv preprintJudging llm-as-a-judge with mt-bench and chatbot arena</p>
<p>Large language models are human-level prompt engineers. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba, arXiv:2211.019102022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
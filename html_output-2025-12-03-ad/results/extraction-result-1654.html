<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1654 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1654</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1654</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-211506949</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2002.11635v1.pdf" target="_blank">Sim2Real Transfer for Reinforcement Learning without Dynamics Randomization</a></p>
                <p><strong>Paper Abstract:</strong> In this work we show how to use the Operational Space Control framework (OSC) under joint and cartesian constraints for reinforcement learning in cartesian space. Our method is therefore able to learn fast and with adjustable degrees of freedom, while we are able to transfer policies without additional dynamics randomizations on a KUKA LBR iiwa peg in-hole task. Before learning in simulation starts, we perform a system identification for aligning the simulation environment as far as possible with the dynamics of a real robot. Adding constraints to the OSC controller allows us to learn in a safe way on the real robot or to learn a flexible, goal conditioned policy that can be easily transferred from simulation to the real robot.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1654.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1654.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OSC-KUKA-sim2real</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim-to-Real Transfer using Operational Space Control on a KUKA LBR iiwa</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL system that learns cartesian-space peg-in-hole insertion policies in simulation using Operational Space Control (OSC) with joint/cartesian constraints and transfers them to a real KUKA LBR iiwa without using dynamics randomization, enabled by system identification and torque-level constraint-aware control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>KUKA LBR iiwa (14kg) with Weiss WSG50 gripper</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A 7-DOF collaborative industrial manipulator (LBR iiwa) instrumented with a two-finger Weiss WSG50 gripper; controlled at torque level via KUKA FRI and an OSC-based C++ controller implementing joint/cartesian constraints and torque commands at <=5 ms loop time.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation (peg-in-hole insertion, contact-rich assembly)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>A physics simulator (PyBullet) used to simulate rigid-body dynamics, joint torques, and contact interactions for the KUKA robot and gripper; simulation stepped at 5 ms intervals and commanded with torques to joints.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>approximate physics with robot-dynamics model (high-quality model after system identification), not fully high-fidelity for complex contacts/soft bodies</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>rigid-body dynamics of robot links, joint torques, gravity, link masses, joint damping, contact interactions (basic contact/penetration handled by PyBullet), operational-space dynamics (simulated command torques via OSC mapping)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>complex contact phenomena (detailed frictional micro-dynamics, soft-body deformation, snap-in events) and some dynamic terms (Coriolis) were neglected or approximated; PyBullet's contact and friction modeling limitations noted; visual/perceptual noise modeling not emphasized (only Aruco marker pose used for goal), sensor noise and some dynamic subtleties simplified unless captured by system identification.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Laboratory setup with a real KUKA LBR iiwa robot and Weiss WSG50 gripper; Aruco marker + camera used to detect hole pose; robot commanded via KUKA FRI and OSC C++ controller with safety constraints (joint/cartesian limits, velocity/accel limits) in place.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Peg-in-hole insertion (goal-conditioned policy for translation and θ rotation around vertical axis), including search strategy and contact-aware insertion behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement Learning (Soft Actor-Critic) in simulation using OSC to map cartesian commands to torques; occasional experiments stacking past observations/actions; policy trained goal-conditioned on relative end-effector positions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Insertion success rate (successful insertions / trials) and qualitative measures of robustness (recovery from perturbations, ability to handle moved targets up to ~2 cm).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Successful transfer: baseline (non-dynamics-randomized) policies achieved 20/20 successful insertions across tested target positions; some variants (stacked observations, different peg shape) showed minor drops (e.g., 18/20, 19/20 in some cases).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Only start and goal locations were randomized; no dynamics/environment randomization applied for the successful transfer experiments (dynamics randomization was tested separately and found to harm training stability).</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>mismatch in robot dynamics (gravity, link masses, joint damping), imperfect contact/friction modeling, simplified dynamics (neglected Coriolis), sensor/perception calibration errors (pose estimation), timing/loop differences between sim and real.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Accurate system identification (optimizing gravity, link mass scaling, joint damping), using OSC to operate in cartesian space with torque-level control and explicit joint/cartesian constraints (SJS and SCS) for safety and compliance, goal-conditioned policy, compact observation space (unstacked) to reduce overfitting, and a high-quality robot model in PyBullet; only randomizing start/goal positions.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>High fidelity in core robot dynamics (gravity, masses, damping) is important; detailed contact/ friction/soft-body fidelity acknowledged as difficult to simulate and not fully captured by PyBullet—paper recommends system identification and constraint-aware control rather than full dynamics randomization for their setup. No numeric fidelity thresholds provided.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Comparisons showed policies trained without dynamics randomization (but with system identification and OSC) trained faster, were more stable, and transferred as well or better than dynamics-randomized policies; dynamics randomization increased instability during training and in some cases reduced transfer performance; stacking observations sometimes degraded transfer (more overfitting) compared to compact observations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Sim-to-real transfer for a contact-rich peg-in-hole task is achievable without dynamics randomization when using Operational Space Control with joint/cartesian constraints and careful system identification to align simulator dynamics with the real robot; this approach speeds training, improves final performance, and yields robust policies that recover from perturbations up to ~2 cm in target displacement. Dynamics randomization was found to make learning more unstable and sometimes worsen transfer in this setup.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim2Real Transfer for Reinforcement Learning without Dynamics Randomization', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1654.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1654.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DynamicsRandomization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamics Randomization (prior sim-to-real technique)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commonly-used sim-to-real technique that trains policies in simulation with randomized physical/environmental parameters (e.g., mass, friction, damping, gravity) to obtain robust policies that transfer to varied real-world dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-real transfer of robotic control with dynamics randomization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation / sim-to-real methodology</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>technique applied to varying simulation fidelities (often used with approximate simulators to make policies robust to mismatch)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>intended to cover uncertainty in dynamics such as mass, inertia, damping, friction, and gravity by randomizing them during training.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Does not itself increase simulator intrinsic fidelity for complex phenomena (soft-body, detailed contact microslip); addresses mismatch by producing robust policies rather than simulating fine-grained dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>General technique reported for diverse tasks (e.g., manipulation, locomotion); in paper referenced as prior method for sim-to-real.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning with environment/parameter randomization in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Typical parameters randomized in referenced literature and in some experiments: gravity, link masses, joint damping, surface friction; the present paper tested dynamics randomization in experiments and found it made training more unstable.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Addresses mismatch by enlarging variability seen during training; can still fail if important phenomena (e.g., complex contact, soft-body effects) are not captured by the randomized parameter space.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Works best when the randomized parameter distributions cover the true real-world variability; often combined with large simulation budgets and robust learning algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Paper notes that dynamics randomization increases training instability and hyperparameter complexity; does not provide quantitative thresholds but implies that with OSC and system identification, full dynamics randomization may be unnecessary for their task.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Referenced as a prevalent method for sim-to-real; in this work, dynamics randomization was empirically tested and found to slow learning and reduce stability/transfer performance compared to the authors' approach combining OSC and system identification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim2Real Transfer for Reinforcement Learning without Dynamics Randomization', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1654.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1654.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SystemIdentification-CMAES</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>System Identification via CMA-ES parameter optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system-identification procedure used by the authors: run scripted action trajectories on the real robot, then use CMA-ES to optimize simulator parameters (gravity vector, link mass scaling, joint damping) to minimize end-effector trajectory error between sim and real.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>KUKA LBR iiwa (same as main experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Real robot used to collect data for identification (see main entry).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics simulation and sim-to-real transfer</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>PyBullet simulator whose parameters were tuned to match real robot trajectories via optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>improved robot-dynamics fidelity through parameter calibration (gravity, masses, damping) but still limited in detailed contact fidelity</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>gravity vector, link mass scaling, joint damping, resulting in closer match of end-effector trajectories between sim and real.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Did not claim to capture complex frictional/contact micro-dynamics or soft-body effects; optimization focused on kinematic/dynamic trajectory matching.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Real robot rollouts using scripted trajectories to collect end-effector positions over time for optimization objective.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Used as preparatory step to improve sim fidelity for subsequent RL policy training and transfer (peg-in-hole task).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Not a learning policy per se; used CMA-ES (evolutionary optimization) to minimize 2-norm error between simulated and real end-effector trajectories over collected rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Reduction in trajectory error (2-norm of end-effector position differences) between sim and real over test trajectories; empirical improvement shown in trajectory plots (Fig. 3 and 4).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Parameter mismatches in gravity, link masses, and joint damping were primary contributors to sim-to-real discrepancy addressed by this step.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Collecting representative scripted trajectories on the real robot and optimizing key physical parameters (gravity vector, mass scaling, damping) via CMA-ES to align simulator behavior with real trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Demonstrated that tuning gravity, link mass scaling, and joint damping substantially reduces trajectory mismatch; suggests accurate modeling of these parameters is critical for transfer in their setup.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>System identification (optimizing gravity, link mass scaling, joint damping using CMA-ES against real trajectories) significantly improved simulator-to-robot trajectory correspondence and was a key enabler for successful sim-to-real transfer without dynamics randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim2Real Transfer for Reinforcement Learning without Dynamics Randomization', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>Learning dexterous in-hand manipulation <em>(Rating: 2)</em></li>
                <li>Sim-to-real: Learning agile locomotion for quadruped robots <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer with neural-augmented robot simulation <em>(Rating: 2)</em></li>
                <li>Closing the sim-to-real loop: Adapting simulation randomization with real world experience <em>(Rating: 2)</em></li>
                <li>Flexible robotic grasping with sim-to-real transfer based reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1654",
    "paper_id": "paper-211506949",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "OSC-KUKA-sim2real",
            "name_full": "Sim-to-Real Transfer using Operational Space Control on a KUKA LBR iiwa",
            "brief_description": "An RL system that learns cartesian-space peg-in-hole insertion policies in simulation using Operational Space Control (OSC) with joint/cartesian constraints and transfers them to a real KUKA LBR iiwa without using dynamics randomization, enabled by system identification and torque-level constraint-aware control.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "KUKA LBR iiwa (14kg) with Weiss WSG50 gripper",
            "agent_system_description": "A 7-DOF collaborative industrial manipulator (LBR iiwa) instrumented with a two-finger Weiss WSG50 gripper; controlled at torque level via KUKA FRI and an OSC-based C++ controller implementing joint/cartesian constraints and torque commands at &lt;=5 ms loop time.",
            "domain": "general robotics manipulation (peg-in-hole insertion, contact-rich assembly)",
            "virtual_environment_name": "PyBullet",
            "virtual_environment_description": "A physics simulator (PyBullet) used to simulate rigid-body dynamics, joint torques, and contact interactions for the KUKA robot and gripper; simulation stepped at 5 ms intervals and commanded with torques to joints.",
            "simulation_fidelity_level": "approximate physics with robot-dynamics model (high-quality model after system identification), not fully high-fidelity for complex contacts/soft bodies",
            "fidelity_aspects_modeled": "rigid-body dynamics of robot links, joint torques, gravity, link masses, joint damping, contact interactions (basic contact/penetration handled by PyBullet), operational-space dynamics (simulated command torques via OSC mapping)",
            "fidelity_aspects_simplified": "complex contact phenomena (detailed frictional micro-dynamics, soft-body deformation, snap-in events) and some dynamic terms (Coriolis) were neglected or approximated; PyBullet's contact and friction modeling limitations noted; visual/perceptual noise modeling not emphasized (only Aruco marker pose used for goal), sensor noise and some dynamic subtleties simplified unless captured by system identification.",
            "real_environment_description": "Laboratory setup with a real KUKA LBR iiwa robot and Weiss WSG50 gripper; Aruco marker + camera used to detect hole pose; robot commanded via KUKA FRI and OSC C++ controller with safety constraints (joint/cartesian limits, velocity/accel limits) in place.",
            "task_or_skill_transferred": "Peg-in-hole insertion (goal-conditioned policy for translation and θ rotation around vertical axis), including search strategy and contact-aware insertion behavior.",
            "training_method": "Reinforcement Learning (Soft Actor-Critic) in simulation using OSC to map cartesian commands to torques; occasional experiments stacking past observations/actions; policy trained goal-conditioned on relative end-effector positions.",
            "transfer_success_metric": "Insertion success rate (successful insertions / trials) and qualitative measures of robustness (recovery from perturbations, ability to handle moved targets up to ~2 cm).",
            "transfer_performance_sim": null,
            "transfer_performance_real": "Successful transfer: baseline (non-dynamics-randomized) policies achieved 20/20 successful insertions across tested target positions; some variants (stacked observations, different peg shape) showed minor drops (e.g., 18/20, 19/20 in some cases).",
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": "Only start and goal locations were randomized; no dynamics/environment randomization applied for the successful transfer experiments (dynamics randomization was tested separately and found to harm training stability).",
            "sim_to_real_gap_factors": "mismatch in robot dynamics (gravity, link masses, joint damping), imperfect contact/friction modeling, simplified dynamics (neglected Coriolis), sensor/perception calibration errors (pose estimation), timing/loop differences between sim and real.",
            "transfer_enabling_conditions": "Accurate system identification (optimizing gravity, link mass scaling, joint damping), using OSC to operate in cartesian space with torque-level control and explicit joint/cartesian constraints (SJS and SCS) for safety and compliance, goal-conditioned policy, compact observation space (unstacked) to reduce overfitting, and a high-quality robot model in PyBullet; only randomizing start/goal positions.",
            "fidelity_requirements_identified": "High fidelity in core robot dynamics (gravity, masses, damping) is important; detailed contact/ friction/soft-body fidelity acknowledged as difficult to simulate and not fully captured by PyBullet—paper recommends system identification and constraint-aware control rather than full dynamics randomization for their setup. No numeric fidelity thresholds provided.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Comparisons showed policies trained without dynamics randomization (but with system identification and OSC) trained faster, were more stable, and transferred as well or better than dynamics-randomized policies; dynamics randomization increased instability during training and in some cases reduced transfer performance; stacking observations sometimes degraded transfer (more overfitting) compared to compact observations.",
            "key_findings": "Sim-to-real transfer for a contact-rich peg-in-hole task is achievable without dynamics randomization when using Operational Space Control with joint/cartesian constraints and careful system identification to align simulator dynamics with the real robot; this approach speeds training, improves final performance, and yields robust policies that recover from perturbations up to ~2 cm in target displacement. Dynamics randomization was found to make learning more unstable and sometimes worsen transfer in this setup.",
            "uuid": "e1654.0",
            "source_info": {
                "paper_title": "Sim2Real Transfer for Reinforcement Learning without Dynamics Randomization",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "DynamicsRandomization",
            "name_full": "Dynamics Randomization (prior sim-to-real technique)",
            "brief_description": "A commonly-used sim-to-real technique that trains policies in simulation with randomized physical/environmental parameters (e.g., mass, friction, damping, gravity) to obtain robust policies that transfer to varied real-world dynamics.",
            "citation_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "mention_or_use": "mention",
            "agent_system_name": null,
            "agent_system_description": null,
            "domain": "general robotics manipulation / sim-to-real methodology",
            "virtual_environment_name": null,
            "virtual_environment_description": null,
            "simulation_fidelity_level": "technique applied to varying simulation fidelities (often used with approximate simulators to make policies robust to mismatch)",
            "fidelity_aspects_modeled": "intended to cover uncertainty in dynamics such as mass, inertia, damping, friction, and gravity by randomizing them during training.",
            "fidelity_aspects_simplified": "Does not itself increase simulator intrinsic fidelity for complex phenomena (soft-body, detailed contact microslip); addresses mismatch by producing robust policies rather than simulating fine-grained dynamics.",
            "real_environment_description": null,
            "task_or_skill_transferred": "General technique reported for diverse tasks (e.g., manipulation, locomotion); in paper referenced as prior method for sim-to-real.",
            "training_method": "Reinforcement learning with environment/parameter randomization in simulation.",
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": true,
            "domain_randomization_details": "Typical parameters randomized in referenced literature and in some experiments: gravity, link masses, joint damping, surface friction; the present paper tested dynamics randomization in experiments and found it made training more unstable.",
            "sim_to_real_gap_factors": "Addresses mismatch by enlarging variability seen during training; can still fail if important phenomena (e.g., complex contact, soft-body effects) are not captured by the randomized parameter space.",
            "transfer_enabling_conditions": "Works best when the randomized parameter distributions cover the true real-world variability; often combined with large simulation budgets and robust learning algorithms.",
            "fidelity_requirements_identified": "Paper notes that dynamics randomization increases training instability and hyperparameter complexity; does not provide quantitative thresholds but implies that with OSC and system identification, full dynamics randomization may be unnecessary for their task.",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Referenced as a prevalent method for sim-to-real; in this work, dynamics randomization was empirically tested and found to slow learning and reduce stability/transfer performance compared to the authors' approach combining OSC and system identification.",
            "uuid": "e1654.1",
            "source_info": {
                "paper_title": "Sim2Real Transfer for Reinforcement Learning without Dynamics Randomization",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "SystemIdentification-CMAES",
            "name_full": "System Identification via CMA-ES parameter optimization",
            "brief_description": "A system-identification procedure used by the authors: run scripted action trajectories on the real robot, then use CMA-ES to optimize simulator parameters (gravity vector, link mass scaling, joint damping) to minimize end-effector trajectory error between sim and real.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "KUKA LBR iiwa (same as main experiment)",
            "agent_system_description": "Real robot used to collect data for identification (see main entry).",
            "domain": "robotics simulation and sim-to-real transfer",
            "virtual_environment_name": "PyBullet",
            "virtual_environment_description": "PyBullet simulator whose parameters were tuned to match real robot trajectories via optimization.",
            "simulation_fidelity_level": "improved robot-dynamics fidelity through parameter calibration (gravity, masses, damping) but still limited in detailed contact fidelity",
            "fidelity_aspects_modeled": "gravity vector, link mass scaling, joint damping, resulting in closer match of end-effector trajectories between sim and real.",
            "fidelity_aspects_simplified": "Did not claim to capture complex frictional/contact micro-dynamics or soft-body effects; optimization focused on kinematic/dynamic trajectory matching.",
            "real_environment_description": "Real robot rollouts using scripted trajectories to collect end-effector positions over time for optimization objective.",
            "task_or_skill_transferred": "Used as preparatory step to improve sim fidelity for subsequent RL policy training and transfer (peg-in-hole task).",
            "training_method": "Not a learning policy per se; used CMA-ES (evolutionary optimization) to minimize 2-norm error between simulated and real end-effector trajectories over collected rollouts.",
            "transfer_success_metric": "Reduction in trajectory error (2-norm of end-effector position differences) between sim and real over test trajectories; empirical improvement shown in trajectory plots (Fig. 3 and 4).",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Parameter mismatches in gravity, link masses, and joint damping were primary contributors to sim-to-real discrepancy addressed by this step.",
            "transfer_enabling_conditions": "Collecting representative scripted trajectories on the real robot and optimizing key physical parameters (gravity vector, mass scaling, damping) via CMA-ES to align simulator behavior with real trajectories.",
            "fidelity_requirements_identified": "Demonstrated that tuning gravity, link mass scaling, and joint damping substantially reduces trajectory mismatch; suggests accurate modeling of these parameters is critical for transfer in their setup.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "System identification (optimizing gravity, link mass scaling, joint damping using CMA-ES against real trajectories) significantly improved simulator-to-robot trajectory correspondence and was a key enabler for successful sim-to-real transfer without dynamics randomization.",
            "uuid": "e1654.2",
            "source_info": {
                "paper_title": "Sim2Real Transfer for Reinforcement Learning without Dynamics Randomization",
                "publication_date_yy_mm": "2020-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_of_robotic_control_with_dynamics_randomization"
        },
        {
            "paper_title": "Learning dexterous in-hand manipulation",
            "rating": 2,
            "sanitized_title": "learning_dexterous_inhand_manipulation"
        },
        {
            "paper_title": "Sim-to-real: Learning agile locomotion for quadruped robots",
            "rating": 2,
            "sanitized_title": "simtoreal_learning_agile_locomotion_for_quadruped_robots"
        },
        {
            "paper_title": "Sim-to-real transfer with neural-augmented robot simulation",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_with_neuralaugmented_robot_simulation"
        },
        {
            "paper_title": "Closing the sim-to-real loop: Adapting simulation randomization with real world experience",
            "rating": 2,
            "sanitized_title": "closing_the_simtoreal_loop_adapting_simulation_randomization_with_real_world_experience"
        },
        {
            "paper_title": "Flexible robotic grasping with sim-to-real transfer based reinforcement learning",
            "rating": 1,
            "sanitized_title": "flexible_robotic_grasping_with_simtoreal_transfer_based_reinforcement_learning"
        }
    ],
    "cost": 0.012588,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Sim2Real Transfer for Reinforcement Learning without Dynamics Randomization</p>
<p>Manuel Kaspar 
Juan David 
Munoz Osorio 
Juergen Bock 
Sim2Real Transfer for Reinforcement Learning without Dynamics Randomization
1
In this work we show how to use the Operational Space Control framework (OSC) under joint and cartesian constraints for reinforcement learning in cartesian space. Our method is therefore able to learn fast and with adjustable degrees of freedom, while we are able to transfer policies without additional dynamics randomizations on a KUKA LBR iiwa pegin-hole task. Before learning in simulation starts, we perform a system identification for aligning the simulation environment as far as possible with the dynamics of a real robot. Adding constraints to the OSC controller allows us to learn in a safe way on the real robot or to learn a flexible, goal conditioned policy that can be easily transferred from simulation to the real robot. 1</p>
<p>Sim2Real Transfer for Reinforcement Learning without Dynamics Randomization</p>
<p>Manuel Kaspar, Juan David Munoz Osorio, Juergen Bock * Abstract-In this work we show how to use the Operational Space Control framework (OSC) under joint and cartesian constraints for reinforcement learning in cartesian space. Our method is therefore able to learn fast and with adjustable degrees of freedom, while we are able to transfer policies without additional dynamics randomizations on a KUKA LBR iiwa pegin-hole task. Before learning in simulation starts, we perform a system identification for aligning the simulation environment as far as possible with the dynamics of a real robot. Adding constraints to the OSC controller allows us to learn in a safe way on the real robot or to learn a flexible, goal conditioned policy that can be easily transferred from simulation to the real robot. 1</p>
<p>I. INTRODUCTION</p>
<p>Most of today's Reinforcement Learning (RL) research with robots is still dealing with artificially simplified tasks, that do not reach the requirements of industrial problems. This is partly due to the fact that training on real robots is very time-consuming. Moreover, it is not trivial to setup a system where the robot can learn a task, but does not damage itself or any task relevant items. Therefore, the idea of sim to real transfer [1] was introduced. While this idea seems convincing in the first place, bridging the reality gap is a major difficulty, especially when contact dynamics, soft bodies etc. are involved, where dynamics are difficult to simulate. This paper investigates possibilities for sim to real transfer while trying to make the task to learn as easy as possible by using the Operational Space Control framework (OSC) [2]. The controller takes care of the redundancy resolution and allows to reduce the task dimension. For instance, our current setup tries to perform a peg-in-hole task as shown in Fig. 1, where we currently fix two rotational dimensions as we know the required final rotation and just learn the necessary translation and θ-rotation (around the vertical axis) for a successful insertion.</p>
<p>However, pure OSC does not contain information about joint or cartesian limits. We solved that problem by using a novel approach to avoid joint and cartesian limits [3] [4]. In this way, the output of the controller are joint torques to command the robot that respect joint and cartesian constraints. By limiting not only position but also acceleration and velocity in joint and cartesian space, we avoid damages of the robot or the environment. Moreover, the compliance achieved by using torque control enables the robot to learn tasks, that require robot-environment contacts.</p>
<p>In our opinion those are tasks where RL can bring benefits compared to traditional techniques. This paper presents a novel approach of integrating RL with OSC, which outperforms traditional approaches that are based on dynamics randomization. Moreover, the combination of RL and OSC bears benefits by avoiding damages of the robot and/or its environment through joint and cartesian constraints. A video of the results can be found under https://sites.google.com/view/ rl-wo-dynamics-randomization.</p>
<p>II. RELATED WORK</p>
<p>Over the past years an increasing number of works tried to use sim to real transfer for learning robotic control: Progressive Nets [5] were proposed for giving the neural network a flexible way of using or not using past experience which was collected in simulation, when fine tuning on a real system. Successful sim to real transfer for robots was demonstrated by [6] and [7] where in hand manipulation of a cube is learned while also the degree of randomization is adjusted dynamically. In [1] a policy to move an object to a specific position on a table is learned. The work introduced and analyzed the idea of dynamics randomization in simulation. Golemo et al. [8] try to learn the differences between the real robot and the simulator and then augment the simulation to be closer to the real robot. This is basically a form of system identification, where instead of finding a right set of parameters for a simulator a more sophisticated identification model is learned. Van Baar et al. [9] perform dynamics randomization for solving a maze game and report easier fine tuning after training a randomized policy in simulation. In [10] an independent perception and control module is used, while the perception module creates arXiv:2002.11635v1 [cs.AI] 19 Feb 2020 a semantic map of the scene. The control module then uses this map as part of its observations. This approach is good for transferring the perception part of a problem from simulation to reality, while the problem of transferring dynamics uncertainties is not discussed in this paper. Yan et al. [11] use Dagger [12] to learn grasping in simulation and by expert demonstration. As they perform position control and have a rather easy gripping setup, they do not have to deal with erroneous robot or contact dynamics. Like previous work they use a semantic map in their perception module. Tan et al. [13] perform sim to real transfer on learning gates for quadruped robots. They use the Bullet [14] simulation engine (with some improvements) and perform a system identification and dynamics randomization. Furthermore, they find that a compact observation space is helpful for sim to real transfer, because the policy can not overfit to unimportant details of the observation. Breyer et al. [15] try to learn grasping objects, leveraging an RL formulation of the problem. They train on some objects in simulation and then transfer the policy to an ABB YuMI. They also use some kind of curriculum learning by starting with a small workspace and then increasing its size.</p>
<p>Inoue et al. [16] show how to use a recurrent network to learn search and insertion actions on a high precision assembly task. While they achieve success on insertion with high precision requirements, it is only directly applicable to search and insertion tasks. They train two separate networks and need a initial calibration of the system. Furthermore, they apply some form of curriculum learning by increasing the initial offset of the peg. They do not use a simulation environment but directly train on the robot. In [17] strategies of insertion are learned in task space by using a large number of demonstrations. We think that our work can figure out strategies more efficiently then leveraging hundreds of demonstrations from humans. Chebotar et. al [18] tried estimating parameters of the robot and process from rollouts on the real robot. In the work of Lee et. al [19] a representation of sensory inputs is learned for performing a peg in hole task, while several sensor modalities are used. They use the Operational Space Control framework with an impedance controller and do also command a 4 DOF action vector. While using multimodal sensor inputs is an interesting direction, we believe that the insertion performance of our system regarding generalization is comparable to their multimodal system, without additional sensors, while our system runs faster and is more flexible regarding start and target locations.</p>
<p>III. REINFORCEMENT LEARNING</p>
<p>Reinforcement learning is the task to find a policy π(a t |s t ) which selects actions a t while observing the state of the environment s t . The selected actions should maximize a reward r(s t , a t ). The state s t+1 and s t are connected over (stochastic) dynamics p(s t+1 |s t , a t ) which finally creates the trajectory τ : (s 0 , a 0 , s 1 , a 1 , ..., s t , a t ).</p>
<p>In our case the observation vector s t contains following variables:
• Joint angles [q 1 ...q 7 ] • End effector x, y z positions [ee x , ee y , ee z ] • End effector theta rotation [ee θ ] • End effector velocities [ėe x ,ėe y ,ėe z ]
The target position of the hole is implicitly encoded into the observation vector. E.g. for the X-dimension ee x = ee xcur − ee xtarget . ee xcur describes the currently measured X-position of the flange, ee xtarget the target x-position in the hole. This gives us a goal-conditioned policy.</p>
<p>As an option to give the policy a better hint about the recent history, we also tested stacking n past observations and actions into the observation vector thereby trying to recover the Markov-condition [20] and giving the network the possibility to figure out the dynamics of the system.</p>
<p>When the observations are stacked we use those values and the last actions and stack it to
s = (s t , a t , s t−1 , a t−1 , ..., s t−n , a t−n ) T(1)
The details of the action vector a t is described in IV-A.</p>
<p>In this work we used the Soft-Actor-Critic (SAC) algorithm explained in [21]. We also tried the PPO and DDPG implementation from SurrealAI [22] but found, that in our experiments SAC was much more sample efficient and stable.</p>
<p>We also investigated the Guided Policy Search algorithm [23] which we found to learn easy tasks really fast. Also Levine et al. showed the general applicability to real world robotics tasks and even integrated vision [24], we found that the technique strongly depends on the right set of hyperparameters and often fails, when moving to higher dimensional action spaces.</p>
<p>What makes the Soft-Actor-Critic algorithm so powerful is the fact, that not only a reward r is maximized, but also the entropy of the actor. The usage of this maximum entropy framework leads to robust policies, that do not collapse into a single successful trajectory but explore the complete range of successful trajectories. This makes the algorithm especially suitable for performing fine tuning on the real robot, after training in simulation. The objective in the maximum entropy framework is
π = arg max π t E (st,at)∼pπ [r(s t , a t ) + αH(π(·|s t ))] (2)
where α is an automatically adjusted temperature parameter that determines the importance of the entropy term. For more details of the SAC algorithm please take a look at [25]. The algorithm itself works as shown in 1.</p>
<p>SAC is furthermore an off-policy algorithm, what makes it more sample efficient than algorithms like PPO, that also showed to be capable of learning complex policies [6] and also worked for our task (but slower).</p>
<p>IV. OPERATIONAL SPACE CONTROL Typically, in OSC, the operational point (in our case, the end effector) is modeled to behave as a unit mass spring damper system:
f * = Ke − DẊ,(3)
Algorithm 1 Sampling strategy in the Soft-Actor-Critic algorithm [25] 1: Initialize policy π, critic Q and replay buffer R 2: for i &lt; max iterations do 3: for n &lt; environment steps do 4: a t ∼ π θ (a t |s t ) 5:
s t+1 ∼ p(s t+1 |s t , a t ) 6: R ← R ∪ (s t , a t , r(s t , a t ), s t+1 ) 7:
end for 8: for each gradient step do 9:</p>
<p>Get batch from R 10:</p>
<p>Update π and Q like in Haarnoja et. al [25] 11:</p>
<p>end for 12: end for where f * is the command vector,Ẋ is the vector velocity of the end effector and e is the vector error, that is the difference between the current and the desired offset position of the end effector. K and D are diagonal matrices that represent the stiffness and damping of the system.</p>
<p>RL actions are directly applied on the command vector f * and are then mapped to the joint space to command the robot using the OSC equation:
τ = J T (Λf * ) + N τ any ,(4)
where Λ is the inertia matrix in the operational space, J is the Jacobian that maps the joint space into the cartesian space and τ is the vector of command torques that can be send to command the robot. The gravity compensation is done by the lowest torque controller level. Note that the Coriolis terms are despised. In practice, due to inaccuracy of the dynamic model, the performance does not increase by the inclusion of these terms. N = I − J TJ T is the null space projector of J and it exists only for redundant cases (the dimension of f * is smaller than the number of joints of the robot n), with the dynamically consistent Jacobian pseudo inverseJ = M −1 J T Λ. τ any is any torque vector that does not produce any accelerations in the space of the main task, typically choosen to reduce the kinetic energy as τ any = M (−k jointDampq ) where k jointDamp is a joint damper term.</p>
<p>To run a policy on the real robot without breaking or stopping the robot while learning, constraints as joint position and velocity limits should be included in the control algorithm. Cartesian limits are also useful to reduce the work space of learning or to protect the robot to damage itself or objects in the environment.</p>
<p>A. Inclusion of unilateral constraints</p>
<p>The classic approach to avoid joint limits or cartesian obstacles is to implement potential fields in the proximity to the limit. However, this approach requires a proper setting of the parameters to avoid oscillations or to have a smooth behavior in the control law as shown in [26], [3]. In [3], a simple approach that overcomes these problems is presented.</p>
<p>The Saturation in Joint Space (SJS), algorithm 2, works by estimating the joint accelerations produced by the torque computed from e.g. the impedance law in eq. 4 (or other task or stack of tasks), and then saturating the joint to its limit (in case of possible violation of the limit). The desired force Λf * is then achieved at best by the remaining joints. The output of the algorithm is the command torque vector τ c that respect the joint limits. Note that a Jacobian that maps from the space of the saturated joints to the whole joint space is denoted by J lim and it is defined by:
J lim = 0 1 0 0 0 0 0 0(5)
if for instance, the second joint is saturated. To have a better understanding of the SJS approach see [3]. 
τ sjs = τ lim + N lim τ 4:q = M −1 (τ sjs − g − c) 5:Q max = min(2 (Qmax−q−qdt) dt 2 , (Vmax−q) dt , A max ) 6:Q min = max(2 (Qmin−q−qdt) dt 2 , (Vmin−q) dt , A min ) 7:q sat,i = Q max,i ifq i &gt;Q max,ï Q min,i ifq i <Q min,i 8: f * lim =q sat 9: τ lim = J T lim (Λ lim f * lim )
10:
N lim = I − J T limJ T lim 11: whileq i >Q max,i orq i <Q min,i
To avoid cartesian limits a similar algorithm to 2 is used [4]. The only difference is that everything must be defined in the cartesian space. Algorithm 3 shows how the process works. J ev does the mapping between the sub space of the cartesian space that is being limited and the joint space. For instance, if only the cartesian position is being limited J ev is the first three rows of the whole Jacobian. Note that J lim must do the mapping only from the saturated cartesian space to the Joint space, e.g., it is the third row of J lim if the z dimension gets saturated.  The final control law works by giving the torque vector τ from Eq. 4 to Algorithm 3 as input. The output vector τ scs is then given as input to Algorithm 2. The output vector τ sjs is then the torque vector that commands the robot. The highest priority is given to the joint limits avoidance that must be respected always. The cartesian limits will be respected as good as they do not interfere with joint limits avoidance. This control law allows now to learn a policy without breaking the robot or objects in the environment.
τ scs = τ lim + N lim τ 4:q = M −1 (τ scs − g − c) 5:ẍ = J evq +J evq 6:Ẍ max = min(2 (Xmax−x−ẋdt) dt 2 , (Vmax−x) dt , A max ) 7:Q min = max(2 (Xmin−x−ẋdt) dt 2 , (Vmin−x) dt , A min ) 8:ẍ sat,i = Ẍ max,i ifẍ i >Ẍ max,ï X min,i ifẍ i &lt;Ẍ
The action vector a t of the learning algorithm consists of
[f * x , f * y , f * z , θ des ]
. Translational commands f * x , f * y and f * z are given directly to eq. 4, while the rotational command f * θ is computed by θ des using eq.3. The error e is calculated in this case by quaternion algebra. Taking θ des instead of f * θ in a t showed slightly better performance.</p>
<p>V. LEARNING FLEXIBLE CARTESIAN COMMANDS BY</p>
<p>USING OPERATIONAL SPACE CONTROL In our approach we use the OSC to control the robot at torque level (&lt;= 5ms loop) and do learning on top of this layer (e.g. with 50ms). In detail our control scheme (OSC + SJS + SCS) allows us to have:</p>
<p>• Joint limit avoidance • Cartesian walls, where the robot experiences an adversarial force and cannot penetrate them • Velocity saturation (prohibits too fast motions)</p>
<p>A. System architecture</p>
<p>The system architecture is shown in Fig. 2. We use Python for running reinforcement learning algorithms and PyBullet [14] for simulation. Additionally we have a C++ program that runs the OSC algorithm and uses FRI (KUKA Fast Robotics Interface) [27] to command the robot or the simulation. This enables us to smoothly switch between simulation and the real robot. The fast C++ implementation ensures that FRI cycle times are met preventing the robot to stop due to timeout errors. For the simulation we developed a Python interface for FRI. The Python RL algorithm and the C++ controller algorithm communicate over gRPC.</p>
<p>B. Learn task specific cartesian dimensions</p>
<p>When learning torques it is almost always necessary to learn n joints together to perform an action. The problem increases with complex robots with high number of joints. Nevertheless, tasks like peg-in-hole are almost always easier solvable in cartesian space than in joint space. Therefore, we rely on the OSC-framework to map from cartesian commands to torques per joint. This gives us a large amount of flexibility to simplify the learning tasks, if necessary.</p>
<p>For instance, if we want to learn a 6 DOF cartesian task, we would still need to learn 7 torque dimensions for the LBR iiwa. In cartesian space it is enough to learn the 3 translational dimensions and the 3 rotational dimensions. If the necessary rotation of a task is clear, this can be given as a fixed setting to the OSC-framework as a task for holding this rotation, and then only the 3 translational dimensions need to be learned. Therefore every task specific combination is possible: 2
• XY ZABC • XY Z (with fixed rotation) • XY ZA • ZA • ...
XY ZA would, e.g., make sense for a peg-in-hole task where a quadratic object needs to be fitted and a rotation around this axis could be necessary to have the right rotation for aligning peg and hole. A combination XA could, e.g., be used for clipping an electrical component into a rail by performing an approach and rotate/clip motion.</p>
<p>VI. SIM TO REAL TRANSFER</p>
<p>A. Simulation environment</p>
<p>We use the PyBullet [14] simulation environment, where we load an KUKA LBR iiwa 14kg with appropriate dynamics values and an attached Weiss WSG50 gripper. We directly command torques to the joints of the robot and use a simulation interval of 5ms.</p>
<p>B. Dynamics and Environment Randomization</p>
<p>[6] and [1] performed dynamics and environment randomization for being able to transfer their policy from simulation to the real world. We found that when using the OSCframework, system identification and a high-quality model of the robot, we can transfer policies without additional dynamics randomization, which speeds up learning massively and also gives us a higher final performance. The only parameters we randomize is the start and goal location.</p>
<p>C. System Identification</p>
<p>In our first trials for using a policy, which was learned in simulation and transferred to the real robot, we found, that it worked pretty poorly. The dynamics of the real robot were too different from the dynamics of the simulation. Therefore, we performed a special type of system identification, where we run scripted trajectories of actions a t for n timesteps on the real robot.</p>
<p>Then we used the CMA-ES [28] algorithm to change the simulation parameters and let them optimize to minimize the 2-norm (
n i=1 (v i ) 2 ) 1 2
where v is the end effector position. The optimized simulation parameters are:</p>
<p>• Gravity X, Y , Z • Robot link masses scaling • Joint Damping Fig. 3 and 4 show the real and simulated trajectory before the system identification and afterwards. We see, that we got much closer to the real trajectory of the robot.  </p>
<p>VII. EVALUATION</p>
<p>In this section we show the results that we found in a simulated environment as well as the results when a policy is transferred to the real robot. The plots were generated by using five training trials per experiment with a moving average window of 10 and the light-colored background shows the standard deviation of the trials. In SAC we kept the standard parameters and the maximum number of steps is set to 200, while the episode ends early when the insertion was successful. We installed and calibrated a camera and an Aruco Marker detector for determining the position and rotation of the hole.</p>
<p>By retrieving this position in the control loop and updating the goal conditioned policy, we achieve to learn a policy that can interactively react on changes in the goal position during rollouts and can recover from perturbations (see the video for more details).</p>
<p>As a cost function we used:
C pos = α · x dist 2 + β · x dist 1 + γ · θ dist 1 (6) C bonus = 50 if insertion was successful (7) C total = −C pos + C bonus(8)
We used α = 0.6, β = 0.4 and γ = 0.1.</p>
<p>Training results can be seen in Fig. 5. We see that the normal and stacked observation vector perform similarly well in the simulation environment (other training scenarios showed, that this is not always the case and training with stacked observations can slow down and worsen training). The red plot shows training, when we perform dynamics randomization. Inspired by [6] we randomize gravity, link masses, joint damping and surface friction. We see that the algorithm still mostly succeeds in learning the task but gets much more unstable and sometimes also fails in learning the task at all.</p>
<p>For testing the transfer of the learned policy to the real robot we set the target to three different locations with different x, y, z, θ the detailed results can be found in Table I. The unstacked policy transfers slightly better to the real robot and insertion is faster. We assume this is the case, because overfitting to the simulation could be less serious, when a compact observation space is used like stated in [13]. We additionally tried using a different peg-shape (triangle) than the shape for training in simulation. Insertion with the triangle shape is slightly more difficult. While insertion with the normal policy works still fine, the performance of the stacked policy degrades. Transferring the policy which was trained with dynamics randomization does also transfer slightly worse.</p>
<p>Also training the policy (for one fixed position) directly on the real robot works well (for more details see the video). These results indicate that a policy trained without dynamics randomization gets trained faster and more reliable and still seems to transfer as well or better than the randomized policy. Additional findings are that policies, which were purely trained in simulation without dynamics randomization are still very robust against perturbations on the real robot. For instance, a human can apply forces on the robot arm, while the policy is executed, and it can still recover from those perturbations. Also moving the target object during execution is possible, as the goal conditioned policy can adapt to the changed situation. The learned search strategy can find the hole even with perturbations in the target location up to 2 cm (if the camera is covered and the hole is moved after the covering). The system also learns, that when being below the hole surface it first needs to go over the hole -taking into account preliminary lower reward -to successfully finish insertion. This is indeed making the problem much more difficult than on plain surfaces and increases training times massively.</p>
<p>VIII. CONCLUSION AND FUTURE WORK</p>
<p>We showed in this work, that it is possible to perform sim to real transfer without doing dynamics randomization. This helps speeding up training, can increase performance and reduces the number of hyperparameters.</p>
<p>In our future roadmap, we plan to investigate the possibilities of using sim to real transfer on more industrial robotic tasks and we believe that our current setup is a good starting point. In our view, tasks that involve contact are the most interesting class of problems for applying reinforcement learning in robotics. They are more difficult to solve, but classic position control tasks can often be solved easier with traditional techniques. With today's industrial robots, force sensitive task require a large amount of expert knowledge to program and a big amount of time for fine tuning it to specific applications. Nevertheless, very often those tasks are also inherently difficult to simulate with today's simulators. Friction, soft objects, snap-in events etc. are difficult or even impossible to simulate with tools like PyBullet or MuJoCo. Specialized simulation environments that can deal with those challenges in a better way partly exist, but often have other downsides like price or simulation speed. We therefore want to investigate how far we can extend sim to real transfer with simulators like PyBullet or MuJoCo on realistic industrial tasks and if industrial requirements for precision, speed and robustness can be met.</p>
<p>Fig. 1 :
1Simulated and real setting</p>
<p>Algorithm 2
2Saturation in Joint Space (SJS) 1: τ lim = 0 [n×1], N lim = I [n×n]],q sat = 0</p>
<p>Algorithm 3
3Saturation in Cartesian space (SCS) 1: τ lim = 0 [n×1], N lim = I [n×n]],ẍ sat = 0</p>
<p>lim =ẍ sat 10: τ lim = J T lim (Λ lim f * lim ) 11: N lim = I − J T limJ T lim 12: whileẍ i &gt;Ẍ max,i orẍ i &lt;Ẍ min,i</p>
<p>Fig. 2 :
2Architecture for learning and controlling robot and simulation Fig. 3: Real and simulated trajectories at the beginning of the optimization process. Every sub-trajectory consists of 50 time steps (x-axis). The figure shows 10 trajectories behind each other, where 10 different action sequences where chosen. The y-axis corresponds to the position of the flange.</p>
<p>Fig. 4 :
4Real and simulated trajectories after the system identification.</p>
<p>Fig. 5 :
5Training with and without dynamics randomization on different start and goal positions. The green plot shows training, when 4 past actions and observations are stacked into the observation vector. The plots show the average and standard deviation over five training runs.</p>
<p>TABLE I :
ITransfer Results (successful vs. tried insertions) Dynamics Randomization 20/20 20/20 20/20 Dynamics Rnd. -Triangle 20/20 20/20 10/20Train on real robot 20/20Policy 
Pos 1 Pos 2 Pos 3 
Stack 0 
20/20 20/20 20/20 
Stack 4 
20/20 20/20 20/20 
Stack 0 -Triangle 
20/20 20/20 20/20 
Stack 4 -Triangle 
18/20 20/20 19/20 </p>
<p>ABC is the euler angle notation for rotations, where A rotates around Z, B around Y and C around X. The frame is expressed in the world coordinate system.</p>
<p>Sim-to-real transfer of robotic control with dynamics randomization. X B Peng, M Andrychowicz, W Zaremba, abs/1710.06537CoRR. X. B. Peng, M. Andrychowicz, W. Zaremba, et al., "Sim-to-real transfer of robotic control with dynamics randomization," CoRR, vol. abs/1710.06537, 2017.</p>
<p>A unified approach for motion and force control of robotic manipulators: The operational space formulation. O Khatib, IEEE Journal of Robotics and Automation. O. Khatib, "A unified approach for motion and force control of robotic manipulators: The operational space formulation," IEEE Journal of Robotics and Automa- tion, 1987.</p>
<p>Operational space framework under joint constraints. J D M Osorio, M D Fiore, F Allmendinger, IDETC/CIE International Design Engineering Technical Conferences and Computers and Information in Engineering Conference. ProceedingsJ. D. M. Osorio, M. D. Fiore, and F. Allmendinger, "Operational space framework under joint constraints," in Proceedings. 2018 IDETC/CIE International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, 2018.</p>
<p>Physical human-robot interaction under joint and cartesian constraints. J D M Osorio, F Allmendinger, M D Fiore, 2019 19th International Conference on Advanced Robotics (ICAR). J. D. M. Osorio, F. Allmendinger, M. D. Fiore, et al., "Physical human-robot interaction under joint and carte- sian constraints," in 2019 19th International Conference on Advanced Robotics (ICAR), 2019, pp. 185-191.</p>
<p>Progressive neural networks. A A Rusu, N C Rabinowitz, G Desjardins, abs/1606.04671CoRR. A. A. Rusu, N. C. Rabinowitz, G. Desjardins, et al., "Progressive neural networks," CoRR, vol. abs/1606.04671, 2016.</p>
<p>Learning dexterous in-hand manipulation. M Openai, B Andrychowicz, Baker, abs/1808.00177CoRR. OpenAI, M. Andrychowicz, B. Baker, et al., "Learn- ing dexterous in-hand manipulation," CoRR, vol. abs/1808.00177, 2018.</p>
<p>Solving rubik's cube with a robot hand. I Openai, M Akkaya, Andrychowicz, abs/1910.07113ArXiv. OpenAI, I. Akkaya, M. Andrychowicz, et al., "Solv- ing rubik's cube with a robot hand," ArXiv, vol. abs/1910.07113, 2019.</p>
<p>Sim-toreal transfer with neural-augmented robot simulation. F Golemo, A A Taiga, A Courville, Proceedings of The 2nd Conference on Robot Learning. A. Billard, A. Dragan, J. Peters, et al., Eds., serThe 2nd Conference on Robot LearningPMLR87Proceedings of Machine Learning ResearchF. Golemo, A. A. Taiga, A. Courville, et al., "Sim-to- real transfer with neural-augmented robot simulation," in Proceedings of The 2nd Conference on Robot Learn- ing, A. Billard, A. Dragan, J. Peters, et al., Eds., ser. Proceedings of Machine Learning Research, vol. 87, PMLR, 2018, pp. 817-828.</p>
<p>Simto-real transfer learning using robustified controllers in robotic tasks involving complex dynamics. J Van Baar, A Sullivan, R Cordorel, abs/1809.04720CoRR. J. van Baar, A. Sullivan, R. Cordorel, et al., "Sim- to-real transfer learning using robustified controllers in robotic tasks involving complex dynamics," CoRR, vol. abs/1809.04720, 2018.</p>
<p>Virtual-to-real: Learning to control in visual semantic segmentation. Z Hong, Y Chen, S Su, abs/1802.00285CoRR. Z. Hong, Y. Chen, S. Su, et al., "Virtual-to-real: Learn- ing to control in visual semantic segmentation," CoRR, vol. abs/1802.00285, 2018.</p>
<p>Sim-to-real transfer of accurate grasping with eye-in-hand observations and continuous control. M Yan, I Frosio, S Tyree, abs/1712.03303CoRR. M. Yan, I. Frosio, S. Tyree, et al., "Sim-to-real transfer of accurate grasping with eye-in-hand observations and continuous control," CoRR, vol. abs/1712.03303, 2017.</p>
<p>No-regret reductions for imitation learning and structured prediction. S Ross, G J Gordon, J A Bagnell, abs/1011.0686CoRR. S. Ross, G. J. Gordon, and J. A. Bagnell, "No-regret reductions for imitation learning and structured predic- tion," CoRR, vol. abs/1011.0686, 2010.</p>
<p>Sim-toreal: Learning agile locomotion for quadruped robots. J Tan, T Zhang, E Coumans, abs/1804.10332CoRR. J. Tan, T. Zhang, E. Coumans, et al., "Sim-to- real: Learning agile locomotion for quadruped robots," CoRR, vol. abs/1804.10332, 2018.</p>
<p>. PyBullet. PyBullet, https://pypi.org/project/pybullet/, Accessed: 2019-05-02.</p>
<p>Flexible robotic grasping with sim-to-real transfer based reinforcement learning. M Breyer, F Furrer, T Novkovic, abs/1803.04996CoRR. M. Breyer, F. Furrer, T. Novkovic, et al., "Flexible robotic grasping with sim-to-real transfer based rein- forcement learning," CoRR, vol. abs/1803.04996, 2018.</p>
<p>Deep reinforcement learning for high precision assembly tasks. T Inoue, G D Magistris, A Munawar, abs/1708.04033CoRR. T. Inoue, G. D. Magistris, A. Munawar, et al., "Deep re- inforcement learning for high precision assembly tasks," CoRR, vol. abs/1708.04033, 2017.</p>
<p>Contact skill imitation learning for robot-independent assembly programming. S Scherzinger, A Roennau, R Dillmann, 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). S. Scherzinger, A. Roennau, and R. Dillmann, "Contact skill imitation learning for robot-independent assembly programming," in 2019 IEEE/RSJ International Confer- ence on Intelligent Robots and Systems (IROS), 2019.</p>
<p>Closing the sim-to-real loop: Adapting simulation randomization with real world experience. Y Chebotar, A Handa, V Makoviychuk, abs/1810.05687CoRR. Y. Chebotar, A. Handa, V. Makoviychuk, et al., "Clos- ing the sim-to-real loop: Adapting simulation ran- domization with real world experience," CoRR, vol. abs/1810.05687, 2018.</p>
<p>Making sense of vision and touch: Self-supervised learning of multimodal representations for contact-rich tasks. M A Lee, Y Zhu, K Srinivasan, 2019 IEEE International Conference on Robotics and Automation (ICRA). M. A. Lee, Y. Zhu, K. Srinivasan, et al., "Making sense of vision and touch: Self-supervised learning of multimodal representations for contact-rich tasks," in 2019 IEEE International Conference on Robotics and Automation (ICRA), 2019.</p>
<p>Deep reinforcement learning: An overview. Y Li, abs/1701.07274CoRR. Y. Li, "Deep reinforcement learning: An overview," CoRR, vol. abs/1701.07274, 2017.</p>
<p>Soft actorcritic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. T Haarnoja, A Zhou, P Abbeel, abs/1801.01290CoRR. T. Haarnoja, A. Zhou, P. Abbeel, et al., "Soft actor- critic: Off-policy maximum entropy deep reinforce- ment learning with a stochastic actor," CoRR, vol. abs/1801.01290, 2018.</p>
<p>Surreal: Open-source reinforcement learning framework and robot manipulation benchmark. L Fan, Y Zhu, J Zhu, Conference on Robot Learning. L. Fan, Y. Zhu, J. Zhu, et al., "Surreal: Open-source re- inforcement learning framework and robot manipulation benchmark," in Conference on Robot Learning, 2018.</p>
<p>Learning neural network policies with guided policy search under unknown dynamics. S Levine, P , Advances in Neural Information Processing Systems (NIPS). S. Levine and P. Abbeel, "Learning neural network policies with guided policy search under unknown dy- namics," in Advances in Neural Information Processing Systems (NIPS), 2014.</p>
<p>End-to-end training of deep visuomotor policies. S Levine, C Finn, T Darrell, abs/1504.00702CoRR. S. Levine, C. Finn, T. Darrell, et al., "End-to-end training of deep visuomotor policies," CoRR, vol. abs/1504.00702, 2015.</p>
<p>Soft actor-critic algorithms and applications. T Haarnoja, A Zhou, K Hartikainen, abs/1812.05905CoRR. T. Haarnoja, A. Zhou, K. Hartikainen, et al., "Soft actor-critic algorithms and applications," CoRR, vol. abs/1812.05905, 2018.</p>
<p>Robot control near singularity and joint limit using a continuous task transition algorithm. H Han, J Park, International Journal of Advanced Robotic Systems. 1010346H. Han and J. Park, "Robot control near singularity and joint limit using a continuous task transition algorithm," International Journal of Advanced Robotic Systems, vol. 10, no. 10, p. 346, 2013.</p>
<p>. Kuka Deutschland Gmbh, Sunrise, FRI 1.16KUKA Deutschland GmbH, KUKA Sunrise.FRI 1.16.</p>
<p>Completely derandomized self-adaptation in evolution strategies. N Hansen, A Ostermeier, Evolutionary Computation. 92N. Hansen and A. Ostermeier, "Completely derandom- ized self-adaptation in evolution strategies," Evolution- ary Computation, vol. 9, no. 2, pp. 159-195, 2001.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8279 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8279</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8279</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-278714886</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.10772v1.pdf" target="_blank">Ranked Voting based Self-Consistency of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Majority voting is considered an effective method to enhance chain-of-thought reasoning, as it selects the answer with the highest"self-consistency"among different reasoning paths (Wang et al., 2023). However, previous chain-of-thought reasoning methods typically generate only a single answer in each trial, thereby ignoring the possibility of other potential answers. As a result, these alternative answers are often overlooked in subsequent voting processes. In this work, we propose to generate ranked answers in each reasoning process and conduct ranked voting among multiple ranked answers from different responses, thereby making the overall self-consistency more reliable. Specifically, we use three ranked voting methods: Instant-runoff voting, Borda count voting, and mean reciprocal rank voting. We validate our methods on six datasets, including three multiple-choice and three open-ended question-answering tasks, using both advanced open-source and closed-source large language models. Extensive experimental results indicate that our proposed method outperforms the baselines, showcasing the potential of leveraging the information of ranked answers and using ranked voting to improve reasoning performance. The code is available at https://github.com/szu-tera/RankedVotingSC.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8279.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8279.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-Shot-CoT-SC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-Shot Chain-of-Thought with Majority-Voting Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline self-consistency method that samples multiple chain-of-thought reasoning traces and selects the final answer by majority vote over single-answer outputs from each sampled trace.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs (LLaMA-3 series, Qwen-2.5, Gemma-2, Phi-3, GPT-3.5-turbo, GPT-4-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The experiments evaluate multiple open-source LLMs (LLaMA-3.2-3B, LLaMA-3-8B, Qwen-2.5 variants, Gemma-2, Phi-3, etc., ranging from ~2B to ~9B) and closed-source GPT-3.5-turbo and GPT-4-turbo; sizes and checkpoints are reported in Appendix B of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought prompting', 'self-consistency via majority voting on single answers']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Chain-of-thought (CoT) prompting elicits step-by-step rationales. Self-consistency (Wang et al., 2023) samples multiple reasoning traces at high temperature and then aggregates the single answer produced by each trace using majority voting to pick the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Baseline: sample k responses with chain-of-thought, record the single final answer from each response, and aggregate by majority vote. Compared directly against ranked-answer + ranked-voting approaches in the same experimental setup (same k, same few-shot templates, same temperature τ=0.7).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>CommonsenseQA, ARC-Challenge, AQUA-RAT (multiple-choice); WikiData, Date Understanding, Word Unscrambling (open-ended, from Big-bench).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Baseline performance is reported per-model and per-dataset in Tables 3–6; for example, Few-Shot-CoT-SC averages reported in the paper (across tasks) are used as the reference: LLaMA-3.2-3B Few-Shot-CoT-SC baseline (used for delta comparisons), GPT-3.5-turbo Few-Shot-CoT-SC ~71.36 (avg in table), GPT-4-turbo Few-Shot-CoT-SC ~86.93 (avg in table). Exact per-task baselines are in the paper tables; ranked-voting deltas are reported relative to these baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Majority-vote self-consistency sometimes improves reasoning accuracy but can miss useful information because each sampled response provides only a single answer (omits ranked alternatives), leading to ties and poorer robustness on some tasks (e.g., word unscrambling where SC sometimes hurts). Majority voting has higher tie rates (4.33%–5.08% reported) compared to ranked-voting methods.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Majority-voting self-consistency is a strong baseline but has limitations because single-answer outputs per sample discard secondary candidate information; leveraging ranked candidate information and rank-aware aggregation can yield improved accuracy and reduced tie rates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ranked Voting based Self-Consistency of Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8279.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8279.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RankedVoting-SC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ranked Voting based Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's proposed approach: have the model output a ranked list (preference ordering) of candidate answers in each sampled reasoning response, then aggregate those ranked lists across samples using ranked-voting rules to produce a final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs (LLaMA-3 series, Qwen-2.5, Gemma-2, Phi-3, GPT-3.5-turbo, GPT-4-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same set of open- and closed-source LLMs as above; experiments use few-shot CoT prompts with n=8 few-shot exemplars and sampling temperature τ=0.7; main experiments use k=8 sampled responses.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought prompting', 'ranked answers per sampled reasoning trace', 'rank-aware aggregation (ranked voting)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Instead of extracting a single final answer per sample, each sampled run returns an ordered list A_r = [a_r1 ≻ a_r2 ≻ ... ≻ a_rm] of candidate answers. These ranked answers across k runs are aggregated by one of several ranked-voting mechanisms (IRV, Borda count, MRR-based) to select the final answer. The few-shot demonstrations are constructed to elicit such ranked outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Direct comparison of standard self-consistency (single-answer majority voting) vs. ranked-answer + ranked-voting aggregation, holding models, prompts, k, and temperature constant. Ablations: (1) ranked answers but no voting (take top-ranked candidate from a single response) vs. full ranked-voting; (2) vary number of samples k; (3) vary number of candidates c in ranked lists; (4) measure tie rates; (5) compare three different rank-aggregation rules.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Same six datasets: CommonsenseQA, ARC-Challenge, AQUA-RAT, WikiData, Date Understanding, Word Unscrambling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Across models/datasets, ranked-voting improved over majority-vote SC. Representative summaries: average improvement for LLaMA-3.2-3B ≈ +3.32% versus Few-Shot-CoT-SC; Qwen-2.5-3B ≈ +4.95% average; closed-source: GPT-3.5 average accuracy 76.69% (ranked-voting) vs baseline, an improvement of ≈ +3.4 absolute; GPT-4 average 87.41% with ranked-voting, ≈ +0.48 absolute over best baseline. Per-task large gains observed (e.g., Data Understanding with Qwen-2.5-3B shows improvements up to ≈ +10.84%–+12.46%). Ranked-voting methods typically improved more on smaller/less-capable models and some open-ended tasks; gains on GPT-4 are smaller.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Ranked-voting leverages secondary candidate signals from each sample, yielding more robust aggregation and fewer ties. Ranked answers alone (without multi-response voting) do not consistently help and can worsen performance on some tasks (ablation shows ranked-answers-only sometimes harmful, especially WikiData). Increasing k (number of sampled responses) and c (number of ranked candidates per response) generally further improves ranked-voting performance; ranked-voting reduces tie rates versus majority voting. Weighting schemes (Borda, MRR) reduce ties more than elimination-based IRV.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Generating ranked answers per sampled reasoning trace and aggregating them with rank-aware voting rules improves self-consistency and reasoning accuracy relative to majority voting on single answers; the improvement is larger for smaller models and some open-ended tasks, and is driven primarily by the ranked-voting aggregation rather than merely producing ranked answers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ranked Voting based Self-Consistency of Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8279.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8279.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IRV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instant-Runoff Voting (Elimination-based Ranked Aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An elimination-style ranked voting rule used to aggregate ranked answer lists from multiple sampled responses by iteratively eliminating the candidate with fewest first-choice votes until some candidate has a majority.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs (experiments reported for LLaMA-3 variants, Qwen-2.5, Gemma-2, Phi-3, GPT-3.5, GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied to ranked answer outputs generated by the LLMs under CoT few-shot prompting; run with k=8 samples in main experiments (k varied in ablations).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['ranked answers per-sample', 'rank-aggregation via IRV']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Each sampled response outputs a full ranking of m candidate answers; IRV counts first choices, eliminates lowest first-choice candidate when no majority, and redistributes votes to the next ranked candidate on ballots, repeating until a candidate reaches majority.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>IRV is compared head-to-head with majority-vote SC and two weighting-based rank aggregators (Borda and MRRV). Ablations include varying k (number of sampled responses) and c (number of candidates); tie-rate analysis compares IRV to BCV and MRRV.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>CommonsenseQA, ARC-Challenge, AQUA-RAT, WikiData, Date Understanding, Word Unscrambling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>IRV yields consistent improvements over majority voting on many settings: example deltas reported include +1.40% average on LLaMA-3.2-3B (Table 3) and comparable improvements for other open-source models; on GPT-3.5 IRV showed +4.94 absolute average improvement vs Few-Shot-CoT-SC (Table 5). On some tasks IRV matches baseline (WikiData) while Borda/MRRV provide better gains. IRV reduces errors and ties relative to baseline but less so than weighting methods.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>IRV (elimination) is effective in scenarios where first-choice plurality is split; however IRV can produce ties when multiple answers have similar rankings due to its discrete elimination rounds. IRV sometimes yields different winners than weighting methods, showing the aggregation rule matters.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>IRV is a viable ranked-aggregation technique for LLM self-consistency and often improves accuracy relative to majority voting, but elimination-based mechanics make it more tie-prone than weighting-based methods in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ranked Voting based Self-Consistency of Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8279.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8279.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BCV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Borda Count Voting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A positional (weighting) ranked voting rule that assigns points according to rank position (higher points for higher ranks) and selects the candidate with largest total Borda score across ballots.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs (LLaMA-3, Qwen-2.5, Gemma-2, Phi-3, GPT-3.5, GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used to aggregate ranked answers returned by LLMs; point scheme: for m candidates, candidate in rank r receives (m - r + 1) points per ballot; aggregated over k ballots.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['ranked answers per-sample', 'rank-aggregation via Borda count weighting']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Each sampled response returns a ranking over m candidates; Borda count assigns positional scores per ranking and sums across responses; the candidate with highest sum is chosen.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared to majority-vote SC and other ranked-voting rules; experiments vary k and c, and include tie-rate comparisons. Examples and Borda score calculations are shown in the paper's case studies.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>CommonsenseQA, ARC-Challenge, AQUA-RAT, WikiData, Date Understanding, Word Unscrambling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>BCV is among the top-performing ranked aggregation methods in many settings. Representative numbers: BCV yields average gains comparable to IRV and MRRV (see Tables 3–6); BCV and MRRV often reduce tie rates more than IRV; on some models/datasets BCV attains best or second-best performance (deltas in tables show +0.98% to +2.87% improvements for specific models/datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Weighting-based aggregation like Borda tends to be more stable (fewer ties) and captures secondary preferences effectively; in example cases BCV corrected majority-vote mistakes by leveraging consistent high-rank placements of the correct answer across samples.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Borda count is an effective rank-aware aggregation for LLM-generated ranked answers, often providing robust improvements and lower tie rates compared to majority voting and elimination-based IRV.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ranked Voting based Self-Consistency of Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8279.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8279.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MRRV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mean Reciprocal Rank Voting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A rank-aware aggregation that computes the mean reciprocal rank (MRR) of each candidate across sampled ranked lists and selects the candidate with highest mean reciprocal rank.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs (LLaMA-3, Qwen-2.5, Gemma-2, Phi-3, GPT-3.5, GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>MRRV computes for each candidate A: MRR(A) = (1/k) * sum_{i=1..k} 1 / rank_A(A_r_i), and selects candidate with largest MRR.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['ranked answers per-sample', 'rank-aggregation via mean reciprocal rank weighting']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Each sampled response returns an ordered ranking; MRRV converts rankings into reciprocal-rank scores (1/position) per ballot and averages over ballots to score candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared against majority-vote SC, IRV, Borda; ablations include varying k and c, tie-rate analysis. Results reported across the same six datasets and LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>CommonsenseQA, ARC-Challenge, AQUA-RAT, WikiData, Date Understanding, Word Unscrambling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>MRRV often provided among the largest average improvements across models; example reported averages: LLaMA-3.2-3B average improvement ≈ +3.32% (MRRV included in that average), Qwen-2.5-3B improvements up to +4.95% average; per-task MRRV achieved up to +12.46% on Data Understanding with Qwen-2.5-3B. On GPT-3.5 MRRV contributed to overall ~+3.4 absolute improvement vs best baseline; on GPT-4 gains are smaller (~+0.48 absolute across ranked methods).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>MRRV is effective at promoting candidates that are consistently ranked near the top across samples. It yields low tie rates and often matches or outperforms Borda in practice. MRRV is numerically stable and interpretable via reciprocal-rank scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>MRR-based aggregation is a practical and effective ranked-voting strategy that leverages rank positions to significantly improve self-consistency over majority voting, particularly for less-capable models and data-understanding tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ranked Voting based Self-Consistency of Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8279.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8279.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RankedAnswers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ranked candidate answers per sampled reasoning trace</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting pattern and output format where each sampled chain-of-thought response includes an ordered list of candidate answers (preference ranking), rather than a single final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs (LLaMA-3 series, Qwen-2.5, Gemma-2, Phi-3, GPT-3.5, GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Ranked answers are elicited by few-shot demonstrations that show both rationales and a ranked ordering of candidates in each demo; experiments typically request m ≈ 4–5 ranked candidates per response.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought prompting producing ranked candidate lists']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Few-shot exemplars are designed to show reasoning plus an explicit 'ranking of options by likelihood' line; at test time, sampled responses produce a ranked list A_r = [a_r1 ≻ a_r2 ≻ ...] which preserves more information about the model's internal preference distribution over answers.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Ablation compares: (A) producing ranked answers but taking only top-ranked answer from a single response (no ranked-voting) vs (B) producing ranked answers across k responses and aggregating via ranked-voting. Also experiments vary number of candidates c to assess impact of richer ranked lists.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Same six datasets as main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Ranked answers alone (no cross-sample ranked-voting) do not reliably improve performance; in some cases performance decreased (e.g., WikiData). The large performance gap observed is between: (i) majority-vote on single answers vs (ii) ranked-voting across k ranked-answer samples — showing that aggregation across multiple ranked lists is crucial. Empirically, ranked-answers-only seldom match ranked-voting performance.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Producing ranked answers per sample exposes secondary plausible candidates which, when aggregated across samples, yield stronger signals; however the ranking information within a single sample is insufficient on its own to outperform multi-sample ranked aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>The benefit comes from combining ranked answers across multiple diverse reasoning samples and aggregating them via rank-aware voting; simply eliciting ranked answers without cross-sample aggregation is not enough.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ranked Voting based Self-Consistency of Large Language Models', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Adaptive consistency for efficient reasoning and coding with LLMs <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 1)</em></li>
                <li>Found in the middle: Permutation self-consistency improves listwise ranking in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8279",
    "paper_id": "paper-278714886",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "Few-Shot-CoT-SC",
            "name_full": "Few-Shot Chain-of-Thought with Majority-Voting Self-Consistency",
            "brief_description": "Baseline self-consistency method that samples multiple chain-of-thought reasoning traces and selects the final answer by majority vote over single-answer outputs from each sampled trace.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "use",
            "model_name": "various LLMs (LLaMA-3 series, Qwen-2.5, Gemma-2, Phi-3, GPT-3.5-turbo, GPT-4-turbo)",
            "model_description": "The experiments evaluate multiple open-source LLMs (LLaMA-3.2-3B, LLaMA-3-8B, Qwen-2.5 variants, Gemma-2, Phi-3, etc., ranging from ~2B to ~9B) and closed-source GPT-3.5-turbo and GPT-4-turbo; sizes and checkpoints are reported in Appendix B of the paper.",
            "reasoning_methods": [
                "chain-of-thought prompting",
                "self-consistency via majority voting on single answers"
            ],
            "reasoning_methods_description": "Chain-of-thought (CoT) prompting elicits step-by-step rationales. Self-consistency (Wang et al., 2023) samples multiple reasoning traces at high temperature and then aggregates the single answer produced by each trace using majority voting to pick the final answer.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Baseline: sample k responses with chain-of-thought, record the single final answer from each response, and aggregate by majority vote. Compared directly against ranked-answer + ranked-voting approaches in the same experimental setup (same k, same few-shot templates, same temperature τ=0.7).",
            "task_or_benchmark": "CommonsenseQA, ARC-Challenge, AQUA-RAT (multiple-choice); WikiData, Date Understanding, Word Unscrambling (open-ended, from Big-bench).",
            "performance_results": "Baseline performance is reported per-model and per-dataset in Tables 3–6; for example, Few-Shot-CoT-SC averages reported in the paper (across tasks) are used as the reference: LLaMA-3.2-3B Few-Shot-CoT-SC baseline (used for delta comparisons), GPT-3.5-turbo Few-Shot-CoT-SC ~71.36 (avg in table), GPT-4-turbo Few-Shot-CoT-SC ~86.93 (avg in table). Exact per-task baselines are in the paper tables; ranked-voting deltas are reported relative to these baselines.",
            "qualitative_findings": "Majority-vote self-consistency sometimes improves reasoning accuracy but can miss useful information because each sampled response provides only a single answer (omits ranked alternatives), leading to ties and poorer robustness on some tasks (e.g., word unscrambling where SC sometimes hurts). Majority voting has higher tie rates (4.33%–5.08% reported) compared to ranked-voting methods.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Majority-voting self-consistency is a strong baseline but has limitations because single-answer outputs per sample discard secondary candidate information; leveraging ranked candidate information and rank-aware aggregation can yield improved accuracy and reduced tie rates.",
            "uuid": "e8279.0",
            "source_info": {
                "paper_title": "Ranked Voting based Self-Consistency of Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "RankedVoting-SC",
            "name_full": "Ranked Voting based Self-Consistency",
            "brief_description": "The paper's proposed approach: have the model output a ranked list (preference ordering) of candidate answers in each sampled reasoning response, then aggregate those ranked lists across samples using ranked-voting rules to produce a final answer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "various LLMs (LLaMA-3 series, Qwen-2.5, Gemma-2, Phi-3, GPT-3.5-turbo, GPT-4-turbo)",
            "model_description": "Same set of open- and closed-source LLMs as above; experiments use few-shot CoT prompts with n=8 few-shot exemplars and sampling temperature τ=0.7; main experiments use k=8 sampled responses.",
            "reasoning_methods": [
                "chain-of-thought prompting",
                "ranked answers per sampled reasoning trace",
                "rank-aware aggregation (ranked voting)"
            ],
            "reasoning_methods_description": "Instead of extracting a single final answer per sample, each sampled run returns an ordered list A_r = [a_r1 ≻ a_r2 ≻ ... ≻ a_rm] of candidate answers. These ranked answers across k runs are aggregated by one of several ranked-voting mechanisms (IRV, Borda count, MRR-based) to select the final answer. The few-shot demonstrations are constructed to elicit such ranked outputs.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Direct comparison of standard self-consistency (single-answer majority voting) vs. ranked-answer + ranked-voting aggregation, holding models, prompts, k, and temperature constant. Ablations: (1) ranked answers but no voting (take top-ranked candidate from a single response) vs. full ranked-voting; (2) vary number of samples k; (3) vary number of candidates c in ranked lists; (4) measure tie rates; (5) compare three different rank-aggregation rules.",
            "task_or_benchmark": "Same six datasets: CommonsenseQA, ARC-Challenge, AQUA-RAT, WikiData, Date Understanding, Word Unscrambling.",
            "performance_results": "Across models/datasets, ranked-voting improved over majority-vote SC. Representative summaries: average improvement for LLaMA-3.2-3B ≈ +3.32% versus Few-Shot-CoT-SC; Qwen-2.5-3B ≈ +4.95% average; closed-source: GPT-3.5 average accuracy 76.69% (ranked-voting) vs baseline, an improvement of ≈ +3.4 absolute; GPT-4 average 87.41% with ranked-voting, ≈ +0.48 absolute over best baseline. Per-task large gains observed (e.g., Data Understanding with Qwen-2.5-3B shows improvements up to ≈ +10.84%–+12.46%). Ranked-voting methods typically improved more on smaller/less-capable models and some open-ended tasks; gains on GPT-4 are smaller.",
            "qualitative_findings": "Ranked-voting leverages secondary candidate signals from each sample, yielding more robust aggregation and fewer ties. Ranked answers alone (without multi-response voting) do not consistently help and can worsen performance on some tasks (ablation shows ranked-answers-only sometimes harmful, especially WikiData). Increasing k (number of sampled responses) and c (number of ranked candidates per response) generally further improves ranked-voting performance; ranked-voting reduces tie rates versus majority voting. Weighting schemes (Borda, MRR) reduce ties more than elimination-based IRV.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Generating ranked answers per sampled reasoning trace and aggregating them with rank-aware voting rules improves self-consistency and reasoning accuracy relative to majority voting on single answers; the improvement is larger for smaller models and some open-ended tasks, and is driven primarily by the ranked-voting aggregation rather than merely producing ranked answers.",
            "uuid": "e8279.1",
            "source_info": {
                "paper_title": "Ranked Voting based Self-Consistency of Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "IRV",
            "name_full": "Instant-Runoff Voting (Elimination-based Ranked Aggregation)",
            "brief_description": "An elimination-style ranked voting rule used to aggregate ranked answer lists from multiple sampled responses by iteratively eliminating the candidate with fewest first-choice votes until some candidate has a majority.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "various LLMs (experiments reported for LLaMA-3 variants, Qwen-2.5, Gemma-2, Phi-3, GPT-3.5, GPT-4)",
            "model_description": "Applied to ranked answer outputs generated by the LLMs under CoT few-shot prompting; run with k=8 samples in main experiments (k varied in ablations).",
            "reasoning_methods": [
                "ranked answers per-sample",
                "rank-aggregation via IRV"
            ],
            "reasoning_methods_description": "Each sampled response outputs a full ranking of m candidate answers; IRV counts first choices, eliminates lowest first-choice candidate when no majority, and redistributes votes to the next ranked candidate on ballots, repeating until a candidate reaches majority.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "IRV is compared head-to-head with majority-vote SC and two weighting-based rank aggregators (Borda and MRRV). Ablations include varying k (number of sampled responses) and c (number of candidates); tie-rate analysis compares IRV to BCV and MRRV.",
            "task_or_benchmark": "CommonsenseQA, ARC-Challenge, AQUA-RAT, WikiData, Date Understanding, Word Unscrambling.",
            "performance_results": "IRV yields consistent improvements over majority voting on many settings: example deltas reported include +1.40% average on LLaMA-3.2-3B (Table 3) and comparable improvements for other open-source models; on GPT-3.5 IRV showed +4.94 absolute average improvement vs Few-Shot-CoT-SC (Table 5). On some tasks IRV matches baseline (WikiData) while Borda/MRRV provide better gains. IRV reduces errors and ties relative to baseline but less so than weighting methods.",
            "qualitative_findings": "IRV (elimination) is effective in scenarios where first-choice plurality is split; however IRV can produce ties when multiple answers have similar rankings due to its discrete elimination rounds. IRV sometimes yields different winners than weighting methods, showing the aggregation rule matters.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "IRV is a viable ranked-aggregation technique for LLM self-consistency and often improves accuracy relative to majority voting, but elimination-based mechanics make it more tie-prone than weighting-based methods in some cases.",
            "uuid": "e8279.2",
            "source_info": {
                "paper_title": "Ranked Voting based Self-Consistency of Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "BCV",
            "name_full": "Borda Count Voting",
            "brief_description": "A positional (weighting) ranked voting rule that assigns points according to rank position (higher points for higher ranks) and selects the candidate with largest total Borda score across ballots.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "various LLMs (LLaMA-3, Qwen-2.5, Gemma-2, Phi-3, GPT-3.5, GPT-4)",
            "model_description": "Used to aggregate ranked answers returned by LLMs; point scheme: for m candidates, candidate in rank r receives (m - r + 1) points per ballot; aggregated over k ballots.",
            "reasoning_methods": [
                "ranked answers per-sample",
                "rank-aggregation via Borda count weighting"
            ],
            "reasoning_methods_description": "Each sampled response returns a ranking over m candidates; Borda count assigns positional scores per ranking and sums across responses; the candidate with highest sum is chosen.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Compared to majority-vote SC and other ranked-voting rules; experiments vary k and c, and include tie-rate comparisons. Examples and Borda score calculations are shown in the paper's case studies.",
            "task_or_benchmark": "CommonsenseQA, ARC-Challenge, AQUA-RAT, WikiData, Date Understanding, Word Unscrambling.",
            "performance_results": "BCV is among the top-performing ranked aggregation methods in many settings. Representative numbers: BCV yields average gains comparable to IRV and MRRV (see Tables 3–6); BCV and MRRV often reduce tie rates more than IRV; on some models/datasets BCV attains best or second-best performance (deltas in tables show +0.98% to +2.87% improvements for specific models/datasets).",
            "qualitative_findings": "Weighting-based aggregation like Borda tends to be more stable (fewer ties) and captures secondary preferences effectively; in example cases BCV corrected majority-vote mistakes by leveraging consistent high-rank placements of the correct answer across samples.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Borda count is an effective rank-aware aggregation for LLM-generated ranked answers, often providing robust improvements and lower tie rates compared to majority voting and elimination-based IRV.",
            "uuid": "e8279.3",
            "source_info": {
                "paper_title": "Ranked Voting based Self-Consistency of Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "MRRV",
            "name_full": "Mean Reciprocal Rank Voting",
            "brief_description": "A rank-aware aggregation that computes the mean reciprocal rank (MRR) of each candidate across sampled ranked lists and selects the candidate with highest mean reciprocal rank.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "various LLMs (LLaMA-3, Qwen-2.5, Gemma-2, Phi-3, GPT-3.5, GPT-4)",
            "model_description": "MRRV computes for each candidate A: MRR(A) = (1/k) * sum_{i=1..k} 1 / rank_A(A_r_i), and selects candidate with largest MRR.",
            "reasoning_methods": [
                "ranked answers per-sample",
                "rank-aggregation via mean reciprocal rank weighting"
            ],
            "reasoning_methods_description": "Each sampled response returns an ordered ranking; MRRV converts rankings into reciprocal-rank scores (1/position) per ballot and averages over ballots to score candidates.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Compared against majority-vote SC, IRV, Borda; ablations include varying k and c, tie-rate analysis. Results reported across the same six datasets and LLMs.",
            "task_or_benchmark": "CommonsenseQA, ARC-Challenge, AQUA-RAT, WikiData, Date Understanding, Word Unscrambling.",
            "performance_results": "MRRV often provided among the largest average improvements across models; example reported averages: LLaMA-3.2-3B average improvement ≈ +3.32% (MRRV included in that average), Qwen-2.5-3B improvements up to +4.95% average; per-task MRRV achieved up to +12.46% on Data Understanding with Qwen-2.5-3B. On GPT-3.5 MRRV contributed to overall ~+3.4 absolute improvement vs best baseline; on GPT-4 gains are smaller (~+0.48 absolute across ranked methods).",
            "qualitative_findings": "MRRV is effective at promoting candidates that are consistently ranked near the top across samples. It yields low tie rates and often matches or outperforms Borda in practice. MRRV is numerically stable and interpretable via reciprocal-rank scoring.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "MRR-based aggregation is a practical and effective ranked-voting strategy that leverages rank positions to significantly improve self-consistency over majority voting, particularly for less-capable models and data-understanding tasks.",
            "uuid": "e8279.4",
            "source_info": {
                "paper_title": "Ranked Voting based Self-Consistency of Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "RankedAnswers",
            "name_full": "Ranked candidate answers per sampled reasoning trace",
            "brief_description": "Prompting pattern and output format where each sampled chain-of-thought response includes an ordered list of candidate answers (preference ranking), rather than a single final answer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "various LLMs (LLaMA-3 series, Qwen-2.5, Gemma-2, Phi-3, GPT-3.5, GPT-4)",
            "model_description": "Ranked answers are elicited by few-shot demonstrations that show both rationales and a ranked ordering of candidates in each demo; experiments typically request m ≈ 4–5 ranked candidates per response.",
            "reasoning_methods": [
                "chain-of-thought prompting producing ranked candidate lists"
            ],
            "reasoning_methods_description": "Few-shot exemplars are designed to show reasoning plus an explicit 'ranking of options by likelihood' line; at test time, sampled responses produce a ranked list A_r = [a_r1 ≻ a_r2 ≻ ...] which preserves more information about the model's internal preference distribution over answers.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Ablation compares: (A) producing ranked answers but taking only top-ranked answer from a single response (no ranked-voting) vs (B) producing ranked answers across k responses and aggregating via ranked-voting. Also experiments vary number of candidates c to assess impact of richer ranked lists.",
            "task_or_benchmark": "Same six datasets as main experiments.",
            "performance_results": "Ranked answers alone (no cross-sample ranked-voting) do not reliably improve performance; in some cases performance decreased (e.g., WikiData). The large performance gap observed is between: (i) majority-vote on single answers vs (ii) ranked-voting across k ranked-answer samples — showing that aggregation across multiple ranked lists is crucial. Empirically, ranked-answers-only seldom match ranked-voting performance.",
            "qualitative_findings": "Producing ranked answers per sample exposes secondary plausible candidates which, when aggregated across samples, yield stronger signals; however the ranking information within a single sample is insufficient on its own to outperform multi-sample ranked aggregation.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "The benefit comes from combining ranked answers across multiple diverse reasoning samples and aggregating them via rank-aware voting; simply eliciting ranked answers without cross-sample aggregation is not enough.",
            "uuid": "e8279.5",
            "source_info": {
                "paper_title": "Ranked Voting based Self-Consistency of Large Language Models",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Adaptive consistency for efficient reasoning and coding with LLMs",
            "rating": 2,
            "sanitized_title": "adaptive_consistency_for_efficient_reasoning_and_coding_with_llms"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 1,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Found in the middle: Permutation self-consistency improves listwise ranking in large language models",
            "rating": 1,
            "sanitized_title": "found_in_the_middle_permutation_selfconsistency_improves_listwise_ranking_in_large_language_models"
        }
    ],
    "cost": 0.01648075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Ranked Voting based Self-Consistency of Large Language Models</p>
<p>Weiqin Wang 
College of Computer Science and Software Engineering
Shenzhen University</p>
<p>Yile Wang wangyile@szu.edu.cn 
College of Computer Science and Software Engineering
Shenzhen University</p>
<p>Hui Huang 
College of Computer Science and Software Engineering
Shenzhen University</p>
<p>Ranked Voting based Self-Consistency of Large Language Models
C752B6BFC1BE57613953850643AE5797
Majority voting is considered an effective method to enhance chain-of-thought reasoning, as it selects the answer with the highest "self-consistency" among different reasoning paths(Wang et al., 2023).However, previous chain-of-thought reasoning methods typically generate only a single answer in each trial, thereby ignoring the possibility of other potential answers.As a result, these alternative answers are often overlooked in subsequent voting processes.In this work, we propose to generate ranked answers in each reasoning process and conduct ranked voting among multiple ranked answers from different responses, thereby making the overall self-consistency more reliable.Specifically, we use three ranked voting methods: Instant-runoff voting, Borda count voting, and mean reciprocal rank voting.We validate our methods on six datasets, including three multiple-choice and three openended question-answering tasks, using both advanced open-source and closed-source large language models.Extensive experimental results indicate that our proposed method outperforms the baselines, showcasing the potential of leveraging the information of ranked answers and using ranked voting to improve reasoning performance.The code is available at https: //github.com/szu-tera/RankedVotingSC.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have shown strong performance in recent years (Ouyang et al., 2022;OpenAI, 2023;Dubey et al., 2024;Yang et al., 2024;DeepSeek-AI, 2024).Chain-of-thought prompting (Wei et al., 2022) further improves the performance of LLMs in commonsense (Talmor et al., 2019) and mathematical (Cobbe et al., 2021) reasoning tasks.Building on these advancements, Wang et al. (2023)</p>
<p>propose a majority voting based</p>
<p>The answer is (B).</p>
<p>The answer is (A).</p>
<p>The answer is (A).</p>
<p>The answer is (C).self-consistency approach, which leverages multiple reasoning paths through sampling to identify the most self-consistent answer, thereby improving the reasoning performance of LLMs.An example of majority voting based selfconsistency is shown in Figure 1(a).In four responses by the model, option (A) was answered twice, (B) and (C) were each answered once, therefore (A) is considered the answer with the highest "self-consistency".In each response, the model replies with only one option, thereby omitting the possibility and priority of other options, which may introduce biases in the following majority voting process.In this work, we consider obtaining ranked answers instead of only a single answer in each response and employ ranked voting based selfconsistency from multiple ranked answers, which we hope can lead to more reliable self-consistency and better reasoning performance.We show the example of ranked answers in Figure 1(b).The model generates the possibility ranking of all options each time.In this case, we find that option (B) ranks in the top two positions in all four responses, while option (A) ranks last in two responses.Considering the overall results, option (A) may not be the best suitable final answer.</p>
<p>To further decide the best answer according to the ranking information, we attempt to use ranked voting to determine the most appropriate answer based on the obtained ranked answers in responses.Ranked voting has been widely used in election systems.Specifically, we use three ranking voting methods to facilitate the final answer, including instant-runoff voting (Cary, 2011), Borda count voting (Emerson, 2013), and mean reciprocal rank voting.The first two methods are widely used in elections, while the latter one is related with the ranking-based MRR metric.We consider these ranked voting based approaches, compared with majority voting on single answers, can provide more reliable final answer by leveraging the ranking information of ordered answers.</p>
<p>We validate our ranked voting based selfconsistency by using advanced LLMs, including four open-source models LLaMa-3 (Dubey et al., 2024), Qwen-2.5 (Yang et al., 2024), Gemma-2 (Team Gemma et al., 2024), Phi-3 (Abdin et al., 2024), and two closed-source models GPT-3.5 (OpenAI, 2022) and GPT-4 (OpenAI, 2023).Empirical results show that our method consistently outperforms baselines on three multiple-choice and three open-ended question-answering datasets.</p>
<p>Method</p>
<p>We first introduce the majority voting based selfconsistency baseline ( §2.1) and propose obtaining the ranked answers in each LLMs' response ( §2.2).Then we provide detailed descriptions to three ranked voting methods as shown in Figure 2, including instant-runoff voting ( §2.3), Borda count voting ( §2.4), and mean reciprocal rank voting ( §2.5).In the final we briefly present overall comparison and how to handle tie votes ( §2.7).</p>
<p>Majority Voting based Self-Consistency</p>
<p>Given a model M and question Q, following chainof-thought reasoning (Wei et al., 2022), we have
R, A = M(Q 1 , r 1 , a 1 ..., Q n , r n , a n , Q), (1)
where {Q j , r j , a j } n j=1 are questions, reasoning paths, and answers in few-shot examples, respectively.R and A are generated reasoning path and answer of the question Q. Wang et al. (2023) further propose finding the most "consistent" answer among multiple responses.In particular, by setting a high decoding temperature (e.g., τ = 0.7), the final answer is the
A &gt; B &gt; D &gt; C. (A r 1 ≻A r 2 ≻A r 3 ≻A r 4 ) Open-Ended Question-Answering /<em> Few-Shot Examples </em>/ Question: [...] (Q j ) Answer: [...] (a r 1
j ≻a r 2 j ≻a r 3 j ≻a r 4 j ≻a r 5 j ) /<em> Test Question </em>/ Question: The language used in Cameroon is (Q) Answer: Cameroon is a multilingual country located in Central Africa.However, the official languages of the country are French and English.While many indigenous languages are also spoken, they do not have the same status as the official languages.The ranking of answers by likelihood is: French &gt; English &gt; Pidgin English &gt; Fulfulde &gt; Hausa.(A r 1 ≻A r 2 ≻A r 3 ≻A r 4 ≻A r 5 )
A majority final = argmax A k i=1 1(A i = A). (2)</p>
<p>From Single Answer to Ranked Answers</p>
<p>In Eq. 1, A usually indicates a single answer such as a specific option in multiple-choice questionanswering.However, it is difficult to reflect the possibility of other options from the single answer A. Thus, we consider obtaining the ranked answers A r , which contains multiple ranked candidates according to the preference of LLMs:
R, A r = M(Q 1 , r 1 , a r 1 ..., Q n , r n , a r n , Q),(3)a r j = a r 1 j , a r 2 j , ..., a rm j , A r = A r 1 , A r 2 , ..., A rm ,(4)
where a r 1 j ≻a r 2 j ≻...≻a rm j and A r 1 ≻A r 2 ≻...≻A rm indicate that a r 1 j or A r 1 is the most possible answer, a rm j or A rm is the least possible answer in corresponding responses, respectively.Examples of Ranked Answers.For obtaining the ranked answers A r , we design the demonstrations in few-shot settings and show two examples in Table 1 on both multiple-choice and open-ended question-answering scenarios.Ranked Voting.Instead of majority voting, we leverage the information of ranked answers and get the final answer according to k possible ranked solutions
{A r i } k i=1 = {A r 1 i , A r 2 i , ..., A rm i } k i=1 : A ranked final = RANKVOTE(A r 1 , A r 2 , ..., A r k ),(5)
where RANKVOTE indicates three ranked voting methods we used, as shown in Figure 2. We describe them in detail in the following subsections.</p>
<p>Instant-Runoff Voting</p>
<p>Instant-runoff voting (IRV) is a voting system that allows voters to rank candidates in order of preference (Cary, 2011).The main idea of IRV is to eliminate the candidate with the fewest votes in each round until a candidate receives a majority (more than 50%) of the votes.In a situation where the number of votes is relatively balanced, this voting method can determine the most suitable candidate through multiple rounds of selection.We provide the IRV procedure in Algorithm 1 and a specific example below.For example, consider the following ranked answers: "a≻b≻c" appearing on 3 responses, "b≻c≻a" appearing on 2 responses, and "c≻a≻b" appearing on 3 responses, respectively.In the initial round, candidate "a" and "c" each secure 3 first-choice votes (37.5%), while "b" garners only 2 first-choice votes (25%).Since no candidate achieves a majority of votes, the candidate "b" is eliminated due to the fewest first-choice votes.The second-choice votes from the responses that originally ranked "b" first are redistributed to "c", resulting in "c" accumulating a total of 5 first-choice votes (62.5%).Meanwhile, "a" retains its 3 firstchoice votes (37.5%).Consequently, "c" emerges as the winner with a clear majority.</p>
<p>Borda Count Voting</p>
<p>Borda count voting (BCV) is a positional voting rule that gives each candidate a number of points (i.e., Borda count) based on their ranking (Emerson, 2013).Suppose we have m ranked answers A r 1 , A r 2 , ..., A rm , the Borda count for candidate A is calculated as follows:
BordaCount(A) = k i=1 (m−rank A (A r i )+1),(6)
where rank A (A r i ) is the rank of candidate A in the ranked answers A r i = A r 1 i , A r 2 i , ..., A rm i from the i-th response.For example, for ranked answers "a≻b≻c" and "b≻c≻a", the Borda count for candidates "a", "b", and "c" are 3+1=4, 2+3=5, and 1+2=3, respectively.The final answer A BCV final is the select candidate with the largest Borda count:
A BCV final = argmax A BordaCount(A). (7)</p>
<p>Mean Reciprocal Rank Voting</p>
<p>The mean reciprocal rank (MRR) is a metric used in information retrieval to evaluate the effectiveness of search algorithms.For a sample of queries, it is calculated as the average of the multiplicative inverse of the rank of the first correct candidate: 1 for the first place, 1/2 for the second place, etc. Inspired by such rank-aware metrics, we consider evaluating the answer according to its place in multiple ranked answers, and obtain the final answer according to the MRR scores in k responses, which we called mean reciprocal rank voting (MRRV).In particular, for a candidate answer a, we calculate the corresponding MRR score as follows:
MRR(A) = 1 k k i=1 1 rank A (A r i ) ,(8)
where rank A (A r i ) is the rank of candidate A in the ranked answers A r i = A r 1 i , A r 2 i , ..., A rm i from the i-th response.For example, for ranked answers "a≻c≻b" and "b≻a≻c" in two responses, the MRR scores for candidates "a", "b", and "c"
are 1 2 ( 1 1 + 1 2 )=0.75, 1 2 ( 1 3 + 1 1 )=0.66, 1 2 ( 1 2 + 1 3 )=0
.42, respectively.MRR score of 1.00 indicates that the corresponding candidate is always ranked first.The final answer A MRRV final is the selected candidate with the highest MRR score:
A MRRV final = argmax A MRR(A).
(9)</p>
<p>Construction of Few-Shot Examples</p>
<p>Constructing few-shot examples for our method is straightforward.The primary criterion we follow is to ensure strong semantic relevance between the question and its relevant candidates.To demonstrate how to construction an examples, we show one example in Table 2.A high-quality example should not only justify the correct answer, but also assess the plausibility of the other candidates in addressing the question, as well as their semantic relevance to it.We refer to such an example as a template.Once created, the template can be reused to construct additional examples for other questions.To scale up the few-shot example set, we begin by using existing examples as few-shot, and The most likely answer is a television, as it commonly requires cable for broadcasting or streaming services.A substation and radio shack are unrelated to home entertainment equipment, and a cabinet is purely for storage, not requiring a cable for its function.</p>
<p>The most likely answer is (C).The ranking of options by likelihood is: C &gt; B &gt; A &gt; D.</p>
<p>Table 2: A few-shot example for question answering, showing the rationale and a candidate ranking based on semantic relevance (explanation highlighted in blue).</p>
<p>then leverage large language models to automatically generate additional demonstrations, requiring only minimal human verification.</p>
<p>Overall Comparison and Tiebreaker Rule</p>
<p>Comparison of Methods.The ranked voting methods IRV, BCV, and MRRV are different from each other.Specifically, IRV can be regarded as an elimination-based ranked voting, while BCV and MRRV can be seen as ranked voting with different weighting schemes.In Figure 2, all three methods arrive at the same final answer (C).However, in practice, their results may vary due to the specific outcomes of ranked answers.Tiebreaker Rules.Although ties are rare in voting outcomes, they may occur particularly for complex questions.Compared with self-consistency baseline, our ranking-based approach mitigates this issue by incorporating preference ranking information (see details in §4).To ensure rigor, we handle tie scenarios through confidence scoring for all methods, including the baseline: when multiple candidates receive equal votes, we compute each answer's confidence score
S i = n t=1 log(p(C i,t ))
, where C i,t denotes the t-th token's predicted probability in the i-th candidate.The final selection chooses the candidate with the highest sum of logarithmic probabilities.For closed-source models, we defer to the model itself to select the best answer from the candidates.</p>
<p>Experiments</p>
<p>Settings</p>
<p>Datasets.We validate our method on six tasks across different domains.The three multiplechoice QA tasks include: CommonsenseQA (Talmor et al., 2019) is a multiple-choice question answering dataset that requires different types of commonsense knowledge.ARC-Challenge (Clark et al., 2018) includes genuine grade-school level, multiple-choice science questions.AQUA-RAT (Ling et al., 2017) is a multiple-choice QA dataset which involves five-options algebraic word problems with rationales.We also evaluate on three open-ended QA tasks form Big-bench (Srivastava et al., 2023): WikiData involves performing opendomain cloze-style question answering.Date Understanding aims to measure models' ability to understand date-related information.Word Unscrambling asks models to unscramble the given letters to form an English word.We list examples of each dataset in Appendix A. Models.We compare with widely-used LLMs, including open-source models LLaMA-3 (Dubey et al., 2024), Qwen-2.5 (Yang et al., 2024), Gemma-2 (Team Gemma et al., 2024), andPhi-3 (Abdin et al., 2024), as well as closed-source GPT-3.5-turbo (OpenAI, 2022) and GPT-4-turbo (Ope-nAI, 2023) models.For open-source models, we use both lightweight (2B∼4B) and medium-sized (7B∼9B) models for comprehensive evaluations, and the checkpoints are listed in Appendix B. Baseline Methods.We compare with the following baselines: Few-Shot-CoT (Wei et al., 2022) with chain-of-thought reasoning.Best-of-N (Stiennon et al., 2020) involves sampling multiple solutions and selecting the best one based on the scores of a fine-tuned Qwen2.5-7B(Yang et al., 2024) reward model.Few-Shot-CoT-SC (Wang et al., 2023) uses majority voting for aggregating multiple answers.Adaptive-SC (Aggarwal et al., 2023) uses dynamic number of responses instead of fixed ones.Implementation Details.For all tasks, we use LM-evaluation-harness (Gao et al., 2024) for fair comparison.Following Wei et al. (2022), we set the number of few-shot examples n = 8 and decoding temperature τ = 0.7 for baselines and our methods.Considering the budget constraints, we set the number of responses k = 8 in main experiments.</p>
<p>Main Results</p>
<p>2B∼4B Open-Source LLMs.The results are shown in Table 3. Overall, our method demonstrates improved average performance across various datasets and models, surpassing all baseline methods.Compared to the Few-Shot-CoT-SC, our three ranked voting methods achieve an average improvement of 3.32% with LLaMA-3.2-3B and 4.95% with Qwen-2.5-3B.Our method also outperforms Best-of-N, which relies on a reward model to enhance generation quality and incurs additional computational overhead.</p>
<p>Across different tasks, we achieve the largest improvement when performing data understanding tasks using Qwen-2.5-3B.Compared to Few-Shot-CoT-SC, the improvement can reach 10.84% to 12.46%.This suggests that, in contrast to providing a single answer for majority voting, offering multiple candidate answers along with their ranking and conducting ranked voting can offer more useful context.</p>
<p>We also find that the majority voting based self-  Closed-Source LLMs.The results are shown in Table 5.Our method achieves an average accuracy of 76.69% and 87.41% on gpt-3.5-turbo-0125and gpt-4-turbo-2024-04-09, respectively, outperforming the best baseline methods by 3.4% and 0.48%.These two results reflect the differences in GPT-3.5 and GPT-4.Similar to the open-source models, we see large improvements on GPT-3.5, while on the state-of-the-art GPT-4 model, the gains from voting across multiple responses decrease.Nevertheless, our ranking voting method still performs slightly better than the baselines.</p>
<p>Analyses</p>
<p>We further analyze our proposed method from different perspectives.Unless otherwise specified, we conduct experiments on CommonsenseQA and WikiData using LLaMa-3-8B.</p>
<p>The Impact of k. Figure 3 shows the performance of different number of samples k.For Common-senseQA, under different settings of k, ranked voting methods consistently outperform majority voting based self-consistency.Moreover, compared with baseline with k = 12 or more, our method achieves better results when k = 8, and its performance improves as k increases.In contrast, the baseline shows limited improvement as k increases.Table 5: Comparison between baselines and our method (in gray) across closed-source GPT-3.5 and GPT-4 models.</p>
<p>For WikiData, the results of instant-runoff voting and baseline are similar, while the other two methods consistently achieve an accuracy improvement of 0.5∼1.0 with different settings of k.</p>
<p>Robustness Analysis.The performance of LLMs is relatively sensitive to the order of examples in few-shot setting, such as exhibiting some recency bias (Zhao et al., 2021).We randomly shuffled the order of few-shot the performance of our method surpasses that of the baseline consistently, and it exhibits relatively smaller variance, indicating that our approach leverages additional ranking information to yield more robust results.</p>
<p>Compare the Impact of Ranked Answers and the Impact of Ranked Voting.To investigate whether the improvement stems from the ranking of answers or the ranked voting, we conducted experiments with the settings where only a single response with ranked answers is provided, taking  the top-ranked answer as the final result.In this setting, the voting mechanisms are not employed.</p>
<p>The results are shown in Figure 5.We find that the ranked answers along do not lead to consistent improvement, even making the results worse on WikiData.The gap between ranked voting and majority voting is much larger than that between Few-Shot-CoT (w/ ranked answers) and Few-Shot-CoT (w/ single answer), indicating that ranked voting can indeed enhance the model's performance.</p>
<p>The Impact of the Number of Candidates.We observed that the number of candidates can significantly affect the performance of ranked voting methods.To systematically examine this effect, we select a set of values C = 1, 2, 3, 4, 5 and conduct experiments using our approach under each candidate setting.As shown in Figure 6, we find that the performance of our methods is positively correlated with   the number of candidates c: as c increases, the performance also improves.On CommonsenseQA, our methods surpass the baseline with only c = 2 candidates, and by c = 4, all ranked voting strategies consistently outperform SC.On WikiData, even with c = 1, ranked methods already achieve better performance than SC, and their accuracy continues to rise as more candidates are considered.These results demonstrate that incorporating a richer set of candidate responses allows our ranked approaches to extract stronger reasoning signals, leading to improved robustness and generalization across tasks.Moreover, increasing the number of candidates effectively encourages the model to explore a broader solution space, promoting deeper reasoning and more diverse perspectives during the generation process.</p>
<p>The Impact of Ranking Information on Tie Situations.Both majority voting and ranked voting may result in ties among multiple candidates, which can lead to confusing and incorrect answers.</p>
<p>The ranked voting methods leverage ranking information, which effectively reduces the occurrence of ties, as confirmed by our experiments.</p>
<p>The results are shown in Table 6, we find that there are 4.33%∼5.08%ties in majority voting based self-consistency, while our methods reduce this to 2.29%∼4.33%.Among the three ranked voting methods, the weighting-based methods (BCV and MRRV) are less likely to result in ties compared to the elimination-based method (IRV).The difference arises from the vote aggregation mechanisms.Weighting-based methods assign varying confidence scores to candidate answers, reducing the likelihood of ties.In contrast, IRV iteratively eliminates lower-ranked options, which can lead to ties when multiple answers have similar rankings.</p>
<p>Related Work</p>
<p>Chain-of-Thought Reasoning.Chain-of-thought (CoT) have shown effectiveness for eliciting the reasoning ability of LLMs through generating rationales in contexts (Wei et al., 2022;Kojima et al., 2022;Chen et al., 2023;Zhang et al., 2023).Although the generated rationales improve the performance, previous studies focus on designing prompts for certain tasks.For example, Yao et al. (2023) and Besta et al. (2024) further propose treeof-thought (ToT) and graph-of-thought (GoT) for solving search-style or elaborate problems, respectively.These works usually generate a single final answer in each trial and there is a lack of study on prompting the model to generate multiple possible answers and corresponding priority relationships.Self-Consistency of LLMs.Wang et al. (2023) propose self-consistency to improve chain-ofthought reasoning, a decoding method that improves the accuracy of LLMs by taking the majority vote among multiple answers generated through diverse reasoning paths.Aggarwal et al. (2023) further propose adaptive self-consistency to reduce the computational cost for obtain diverse reasoning paths, which dynamically adjusts the number of responses based on the model's confidence using a stopping criteria.Xiong et al. (2023) introduce inter-consistency among LLMs for multi agents collaboration.Wang et al. (2024) introduce soft self-consistency in long-duration interactive tasks for language model agents.Huang et al. (2024) enhance the coding ability of LLMs through multipleperspective self-consistency.As a comparison, these methods rely on majority voting for specific tasks and ignore the possibility of multiple potential answers, which could offer useful information for obtaining the final correct answer in general reasoning and question-answering tasks.Ranked Voting.Ranked voting expresses the preferences of voters by ranking multiple candidates, organizing selections on an ordinal scale, which has been widely applied in elections, competitions, and recommendation systems (Myatt, 2007;Colomer, 2013).Similar ideas have also been applied to enhance the performance of neural models.Schwartz (2021) propose using an ensemble method to aggre-gate predictions from multiple MRR and NDCG based models.Liu et al. (2024) proposes pairwisepreference search to address bias in text evaluation by constructing a global ranking.Zhao et al. (2024) introduces general electoral decision-making interface to enhance collaboration among multiple LLM agents through ordinal preferential voting.In recent years, ranking systems have also been valued and proposed to be introduced into more NLP benchmarks (Colombo et al., 2022;Rofin et al., 2023) and Chatbot arenas (Min et al., 2025).Tang et al. (2024) similarly leverage multiple generations via permutation self-consistency to enhance ranking robustness, though their focus is on mitigating positional bias rather than vote-based aggregation.</p>
<p>Conclusion</p>
<p>We introduce ranked voting based self-consistency to enhance the reasoning performance of large language models.Our method outperforms traditional majority voting techniques by generating and incorporating ranking information among multiple candidate answers during the reasoning process.The proposed method improves the reliability of self-consistency and boosts the performance.Extensive experiments demonstrate the effectiveness of our method, as it consistently outperforms baselines across a variety of multiple-choice and open-ended question-answering datasets, using both open-source and closed-source LLMs.</p>
<p>Limitations</p>
<p>First, ranked voting has a rich history and comes in many forms.In our main experiments, we demonstrate three effective ranked voting strategies for large language models, however, there are some other ranked voting methods show limited improvement in datasets we used.For example, in the Appendix C, we introduce approval voting and find that this method performs similarly to majority voting when applied to ranked answers from models.Second, as shown in Figure 6, the number of candidates in the ranked answers may have an impact on the performance.Increasing the number of candidates allows a broader range of potential options to be considered, which could improve the accuracy of the final decision.In this work, we enable the model to autonomously rank a small set of options or open-ended generated candidates (usually 4∼5), without imposing additional constraints by designing prompts to expand the candidate pool.</p>
<p>A Examples of Datasets</p>
<p>We list examples of different datasets in Table 7.For CommonsenseQA, we evaluated all instances from the validation set since the test set does not provide answers.For ARC-Challenge, AQUA-RAT, and Date Understanding, we tested on all instances from the test set.For WikiData and Word Unscrambling, we randomly sampled 300 instances from the test set for evaluation.</p>
<p>B Checkpoints of Models</p>
<p>The checkpoints of open-source models in our experiments are shown in Table 8.</p>
<p>C More Results of Other Ranked Voting based Methods</p>
<p>Approval Voting.In addition to the three voting methods discussed in the main pages, we introduce an additional approach called approval voting (Laslier and Vander Straeten, 2003).This voting system enables participants to express their stance on an issue-whether to approve or disapprove.To streamline this process, approval voting assumes that higher-ranked answers are more likely to be accepted as correct.Therefore, we establish a threshold t: answers ranked before this threshold are considered approved, while those ranked after are deemed disapproved:
A r 1:t = A r 1 , A r 2 , . . . , A rt , A r approved = A r 1:t .(10)
Next, we conduct majority voting to select the answer with the most supporters, where each approval vote carries equal weight in the majority voting process.In the experiments, we set the threshold t = 2. Results.Table 9 shows the performance of approval voting.It is evident that in certain cases, approval voting outperforms self-consistency and can even achieve the best performance.For example, in the WikiData task, LLaMA-3-8B attains the highest performance using approval voting.However, this method has its limitations as well, since it is not particularly stable.Although it performs well in some situations, it may sometimes underperform compared to Few-Shot-CoT.</p>
<p>D Results of Multiple Experimental Runs</p>
<p>The standard deviation results for multiple experimental runs are shown in Table 10.The comparable and relatively small standard deviations between baseline and ours indicate the stability of the implementation and our ranked voting methods.</p>
<p>E Case Study</p>
<p>We show cases of the ranked voting on Common-senseQA in Table 11, Table 12, and Table 13.The answer should focus on an action that represents a casual, carefree approach, contrasting with the elegance and precision of dance."Trip" fits well because it conveys a playful or carefree moment that can happen when you're not focusing on being precise.This aligns with the idea of just having fun.The ranking of options by likelihood is: B &gt; D &gt; E &gt; C &gt; A.</p>
<p>Response 2: The answer should be the action that is the opposite of being elegant and specific, which implies a more casual or carefree approach.Of the above choices, the closest match is "just have fun and fall down", which implies a more playful and carefree attitude.The ranking of options by likelihood is: A</p>
<blockquote>
<p>D &gt; E &gt; C &gt; B.
Response 3: The answer should be the phrase that describes the opposite of elegance, which is a more casual and carefree way of dancing.Of the above choices, "falling down" implies a loss of control and a more playful approach to dancing.The ranking of options by likelihood is:
A &gt; E &gt; D &gt; C &gt; B.
Response 4: The answer should be a phrase that describes an action that is carefree and fun, which is a common aspect of dancing.The ranking of options by likelihood is: E &gt; D &gt; A &gt; C &gt; B.</p>
</blockquote>
<p>Response 5: The answer should be the action that is often associated with having fun while dancing.Of the above choices, "move around" is a general action that is often done while having fun and dancing, without any specific technique or elegance.The ranking of options by likelihood is: D</p>
<blockquote>
<p>E &gt; A &gt; C &gt; B.
Response 6: The answer should be the opposite of "elegant and specific", which implies a more carefree and casual attitude towards dancing.Of the above choices, "just have fun" is closely related to "move around" in a playful way.The ranking of options by likelihood is: D &gt; E &gt; A &gt; C &gt; B.</p>
</blockquote>
<p>Response 7: The answer should be the action that is often associated with having fun while dancing.Of the above choices, "have fun" is often associated with "celebrate".The ranking of options by likelihood is: E &gt; D &gt; A &gt; C &gt; B.</p>
<p>Response 8: The answer should be the opposite of "elegant" and "specific", which implies a more casual or playful approach to dance.The ranking of options by likelihood is: A &gt; E &gt; D &gt; C &gt; B.    Response 2: The answer should be the place where an old film would likely be stored.Of the above choices, a sealed cabinet makes the most sense for storing old films.The ranking of options by likelihood is: D &gt; C &gt; A &gt; E &gt; B.</p>
<blockquote>
<p>E &gt; A × 1 merge − −−− → D &gt; E &gt; A × 3 A &gt; D &gt; E × 1 A &gt; D &gt; E × 1 A &gt; E &gt; D × 2 A &gt; E &gt; D × 2 E &gt; D &gt; A × 2 E &gt; D &gt; A × 2 D &gt; E &gt; A ×
Response 3: The answer should be a place where an old film could be found, likely in a sealed container.Of the above choices, a cabinet is a common place to store things, and it could be sealed.The ranking of options by likelihood is: D &gt; C &gt; A &gt; E &gt; B.</p>
</blockquote>
<p>Response 4: The answer should be the item that is used to seal something.Of the above choices, clingfilm is used to seal things.The ranking of options by likelihood is: A &gt; D &gt; C &gt; E &gt; B.</p>
<p>Response 5: The answer should be a container or packaging that can be sealed.Of the above choices, clingfilm is a type of plastic wrap that can be sealed.The ranking of options by likelihood is: A &gt; D &gt; C &gt; E &gt; B.</p>
<p>Response 6: The answer should be the container that is typically sealed to keep things inside.Of the above choices, clingfilm is a type of plastic wrap used to seal things.The ranking of options by likelihood is: A &gt; D &gt; C &gt; E &gt; B.</p>
<p>Response 7: The answer should be a container or object that can be sealed.Of the above choices, clingfilm (also known as plastic wrap) is commonly used to seal items.The ranking of options by likelihood is: A &gt; D &gt; C &gt; E &gt; B.</p>
<p>Response 8: The answer should be the container where an old film could be stored and sealed.Of the above choices, the most likely container is a cabinet.</p>
<p>Is option (A) the most suitable choice as the final answer?</p>
<p>Figure 1 :
1
Figure 1: Example of (a) majority voting based selfconsistency among single answers (Wang et al., 2023) and (b) ranked answers in four responses by models.</p>
<p>Figure 2 :
2
Figure 2: Examples of the procedures for three ranked voting methods.(a) The obtained ranked answers (k = 8).(b) Instant-runoff voting.(c) Borda count voting.(d) Mean reciprocal rank voting.</p>
<p>Multiple-Choice Question-Answering /<em> Few-Shot Examples </em>/ Question: [...] (Q j ) Answer: [...] (a r1 j ≻a r2 j ≻a r3 j ≻a r4 j ) /<em> Test Question </em>/ Question: George wants to warm his hands quickly by rubbing them.Which skin surface will produce the most heat?(A) dry palms.(B) wet palms.(C) palms covered with oil.(D) palms covered with lotion.(Q) Answer: Heat is generated by friction when two surfaces rub together.The amount of friction depends on the dryness and texture of the surfaces.Dry palms have [...].Wet palms reduce [...].Palms covered with oil lower [...], and palms covered with lotion reduce [...].The ranking of options by likelihood is:</p>
<p>(n = 8) examples and conducted four experiments, and the results are shown in Figure 4.For both CommonsenseQA and WikiData,</p>
<p>Figure 4 :
4
Figure 3: Comparison of methods with different number of responses k.</p>
<p>Figure 5 :
5
Figure 5: The impact of ranked answers (in blue dashed box) and ranked voting (in red dashed box).Ranked voting is more effective and capable of delivering substantial and reliable improvements.</p>
<p>must someone do before they shop?Options: (A) get money (B) have money (C) bring cash (D) go to market (E) bring cash Answer: (A).ARC-Challenge (1172) Question: There is a very slight change in gravity at the top of a mountain.Which property would most likely be less at the top of a mountain?Options: (A) mass (B) weight (C) body density (D) body temperature Answer: (B).AQUA-RAT (254) Question: I have a money pouch containing Rs. 700.There are equal number of 25 paise coins, 50 paise coins and one rupee coins.How many of each are there?Options: (A) 453 (B) 651 (C) 400 (D) 487 (E) 286 Answer: (C).</p>
<p>CommonsenseQAQuestion:</p>
<p>Dance can be elegant and specific, or you can just have fun and what?Options: (A) falling down (B) trip (C) fall down (D) move around (E) celebrate.Ground-Truth Label: D Self-Consistency Answer through Majority Voting: A. ✗ Our Ranked Answers: Response 1:</p>
<blockquote>
<p>E &gt; C &gt; A × 1 A &gt; D &gt; E &gt; C &gt; B × 1 A &gt; E &gt; D &gt; C &gt; B × 2 E &gt; D &gt; A &gt; C &gt; B × 2 D &gt; E &gt; A &gt; C &gt; B × 2Count: A (3 times), B (1 times), C (0 times), D (2 times), E (2 times).No option gets more than half the votes, and C has the fewest, so C is eliminated.Round 2: B &gt; D &gt; E &gt; A × 1 A &gt; D &gt; E &gt; B × 1 A &gt; E &gt; D &gt; B × 2 E &gt; D &gt; A &gt; B × 2 D &gt; E &gt; A &gt; B × 2 Count: A (3times), B (1 times), D (2 times), E (2 times).No option gets more than half the votes, and B has the fewest, so B is eliminated.Round 3: D</p>
</blockquote>
<p>2 Count: A (3 times), D (3 times), E (2 times).No option gets more than half the votes, and E has the fewest, so E is eliminated.), D (5 times).D gets more than half the votes, so D is the winner.</p>
<p>CommonsenseQAQuestion:</p>
<p>Danny found an old film in a sealed what?Options: (A) clingfilm (B) disneyland (C) cave (D) cabinet (E) movie.Ground-Truth Label: D Self-Consistency Answer through Majority Voting: A. ✗ Our Ranked Answers: Response 1: The answer should be the container where the film was found.Of the above choices, clingfilm and cabinet are both plausible containers, but sealed clingfilm is more likely to preserve an old film.The ranking of options by likelihood is: A &gt; D &gt; C &gt; E &gt; B.</p>
<p>The ranking of options by likelihood is: D &gt; C &gt; A &gt; E &gt; B. Borda Count Voting: D. ✓ Borda Count Voting Calculation: A: 5 + 3 + 3 + 5 + 5 + 5 + 5 + 3 = 34 B: 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 = 8 C: 3 + 4 + 4 + 3 + 3 + 3 + 3 + 4 = 27 D: 4 + 5 + 5 + 4 + 4 + 4 + 4 + 5 = 35 ✓ E: 2 + 2 + 2 + 2 + 2 + 2 + 2 + 2 = 16</p>
<p>Table 1 :
1
The examples to obtain ranked answers A r (colored in blue) on multiple-choice and open-ended question-answering tasks in few-shot settings.</p>
<p>majority voting results across k possible solutions {R i , A i } k i=1 in totally k responses:</p>
<p>Open-Ended QA (Exact Match) Average CommonsenseQA ARC-Challenge AQUA-RAT WikiData Date Understanding Word Unscrambling
(LLAMA-3.2-3B)Few-Shot-CoT71.9978.6758.2766.6757.7225.3359.78Best-of-N74.6180.6367.7270.8363.6926.5064.00Few-Shot-CoT-SC73.4680.5461.8173.6760.9824.3362.47Adaptive-SC74.2080.8964.1773.3361.2526.3363.36Instant-Runoff Voting74.86 +1.4081.31 +0.7769.29 +7.4874.00 +0.3364.23 +3.2527.67 +3.3465.23 +2.76Borda Count Voting74.69 +1.2380.55 +0.0167.32 +5.5177.00 +3.3360.16 −0.8228.33 +4.0064.67 +2.20Mean Reciprocal Rank Voting74.45 +0.9981.40 +0.8671.26 +9.4576.00 +2.3262.60 +1.6229.00 +4.6765.79 +3.32(QWEN-2.5-3B)Few-Shot-CoT77.1571.4270.8769.0042.556.6756.28Best-of-N77.9775.9075.3970.6750.148.0059.68Few-Shot-CoT-SC77.8976.9677.9572.6747.977.3360.13Adaptive-SC77.4878.0777.1772.6748.787.6760.31Instant-Runoff Voting78.95 +1.0683.45 +6.4979.13 +1.1874.67 +2.0060.70 +12.7311.67 +4.3464.76 +4.63Borda Count Voting78.38 +0.4983.19 +6.2380.31 +2.3676.33 +3.6658.81 +10.8411.33 +4.0064.72 +4.59Mean Reciprocal Rank Voting78.79 +0.9083.70 +6.7479.92 +1.9775.33 +2.6660.43 +12.4612.33 +5.0065.08 +4.95(GEMMA-2-2B)Few-Shot-CoT69.5371.3336.6172.6736.0424.3351.75Best-of-N70.7673.5942.3272.1737.4020.8352.84Few-Shot-CoT-SC69.2971.4236.2273.6737.6721.6751.66Adaptive-SC70.7673.4633.4674.3337.1323.0052.02Instant-Runoff Voting71.50 +2.2174.74 +3.3244.49 +8.2775.67 +2.0037.94 +0.2723.00 +1.3354.56 +2.90Borda Count Voting70.93 +1.6773.12 +1.7042.52 +6.3077.67 +4.0037.40 −0.2722.00 +0.3353.94 +2.28Mean Reciprocal Rank Voting71.42 +2.1372.18 +0.7642.91 +6.6977.33 +3.6639.02 +1.3522.67 +1.0054.26 +2.60(PHI-3-4B)Few-Shot-CoT74.5386.8666.5476.0062.8724.6765.25Best-of-N76.2986.3574.4171.5060.7022.3365.26Few-Shot-CoT-SC75.8490.1373.6277.3366.1223.3367.73Adaptive-SC75.7688.5771.2677.6765.3124.0067.09Instant-Runoff Voting78.54 +2.7090.70 +0.5774.41 +0.7979.00 +2.3466.40 +0.2827.00 +3.6769.34 +1.61Borda Count Voting78.71 +2.8788.23 −1.9075.20 +1.5878.67 +1.3463.96 −2.1627.00 +3.6768.63 +0.90Mean Reciprocal Rank Voting78.95 +3.1190.44 +0.3175.20 +1.5880.00 +2.6765.58 −0.5427.00 +3.6769.53 +1.80</p>
<p>Table 3 :
3
Comparison between baselines and our method (in gray) across open-source LLMs with 2B∼4B parameters.In each column, the best results are in bold, and the second-best results are underlined.The subscript values represent the differences from Few-Shot-CoT-SC baseline.
consistency does not always help. For example, theperformance drops (25.33→24.33, 24.33→21.67,and 24.67→23.33) on word unscrambling task.This indicates that for some tasks, due to the dif-ficulty of problems and the inherent limitationsof LLMs, majority voting based self-consistencydoes not always lead to improvements. Overall,our ranked voting based self-consistency generallyoutperforms majority voting based methods suchas Few-Shot-CoT-SC and Adaptive-SC.7B∼9B Open-Source LLMs. The results areshown in Table 4. Similarly, our methods achievedthe best and second-best results, indicating thatthey are still effective on the medium-size LLMsof ranked voting. Across different scenarios, com-pared to Few-Shot-CoT-SC, LLaMa-3-8B achievesan average performance improvement of 2.68%to 3.51%. Compared to the 2B∼4B lightweightLLMs, the overall improvement has slightly de-creased, possibly due to the increase in the LLMs'inherent capacity.</p>
<p>Table 4 :
4
Comparison between baselines and our method (in gray) across open-source LLMs with 7B∼9B parameters.
Multiple-Choice QA (Accuracy)Open-Ended QA (Exact Match)MethodCommonsenseQA ARC-Challenge AQUA-RAT WikiData Date Understanding Word UnscramblingAverage(LLAMA-3-8B)Few-Shot-CoT78.1384.1362.9978.3370.1925.6766.67Best-of-N79.0385.1572.4477.8369.9226.8368.53Few-Shot-CoT-SC78.7186.7766.9380.0071.8224.0068.04Adaptive-SC78.9586.9568.5079.6771.0025.3368.40Instant-Runoff Voting79.12 +0.4187.29 +0.5274.02 +7.0980.00 +0.0072.09 +0.2735.00 +11.0071.25 +3.21Borda Count Voting79.69 +0.9887.12 +0.3570.87 +3.9480.33 +0.3371.00 −0.8235.33 +11.3370.72 +2.68Mean Reciprocal Rank Voting79.36 +0.6587.46 +0.6975.20 +8.2780.67 +0.6771.27 −0.4535.33 +11.3371.55 +3.51(QWEN-2.5-7B)Few-Shot-CoT83.2987.8080.3168.0068.2917.0067.45Best-of-N84.3289.0482.8769.0070.0523.1769.74Few-Shot-CoT-SC84.2889.1684.6570.3370.7321.0070.03Adaptive-SC84.6889.2583.4671.3370.4622.3370.25Instant-Runoff Voting84.77 +0.4989.16 +0.0085.04 +0.3977.67 +7.3471.27 +0.5424.00 +3.0071.98 +1.95Borda Count Voting84.52 +0.2489.25 +0.0984.25 −0.4079.33 +9.0070.73 +0.0023.33 +2.3371.90 +1.87Mean Reciprocal Rank Voting84.93 +0.6587.80 −1.3685.83 +1.1878.67 +8.3471.27 +0.5424.00 +3.0072.08 +2.05(GEMMA-2-9B)Few-Shot-CoT80.7588.0563.3978.0077.2445.6772.18Best-of-N80.2288.5271.4677.5076.2943.1772.86Few-Shot-CoT-SC80.1089.5967.3281.3378.0544.3373.45Adaptive-SC81.4186.7766.9380.6777.7845.6773.20Instant-Runoff Voting83.21 +3.1189.68 +0.0972.83 +5.5182.67 +1.3478.59 +0.5446.33 +2.0075.55 +2.10Borda Count Voting82.96 +1.8689.33 −0.2670.87 +3.5583.33 +2.0079.40 +1.3546.00 +1.6775.31 +1.86Mean Reciprocal Rank Voting83.05 +2.9589.25 −0.3472.44 +5.1283.00 +1.6778.32 +0.2746.33 +2.0075.40 +1.95(PHI-3-7B)Few-Shot-CoT80.4391.4765.7579.3368.5627.6768.87Best-of-N80.7991.3077.1777.1773.0429.6771.52Few-Shot-CoT-SC81.2491.8973.6279.3373.7128.3371.35Adaptive-SC81.1691.9874.0279.0073.1729.6771.50Instant-Runoff Voting81.98 +0.7492.24 +0.3577.17 +3.5580.00 +0.6775.34 +1.6330.33 +2.0072.84 +1.49Borda Count Voting82.31 +1.0791.81 −0.0875.20 +1.5880.00 +0.6775.61 +1.9028.33 +0.0072.21 +0.86Mean Reciprocal Rank Voting82.15 +0.9191.98 +0.0977.17 +3.5581.00 +1.6775.34 +1.6328.67 +0.3472.72 +1.37Multiple-Choice QA (Accuracy)Open-Ended QA (Exact Match)MethodCommonsenseQA ARC-Challenge AQUA-RAT WikiData Date Understanding Word UnscramblingAverage(GPT-3.5-TURBO-0125)Few-Shot-CoT79.8586.6558.2778.0058.2749.5068.42Best-of-N80.3487.8672.4479.8361.1158.1773.29Few-Shot-CoT-SC79.6187.5469.4979.0057.9954.5071.36Adaptive-SC79.7788.2570.6779.3358.4054.5071.82Instant-Runoff Voting80.84 +1.2389.33 +1.7970.67 +1.1882.00 +3.0068.97 +10.9866.50 +12.0076.30 +4.94Borda Count Voting80.92 +1.3189.59 +2.0568.90 −0.5981.50 +2.5068.70 +10.7167.67 +13.1776.21 +4.85Mean Reciprocal Rank Voting81.00 +1.3989.25 +1.7171.06 +1.5782.33 +3.3368.83 +10.8467.67 +13.1776.69 +5.33(GPT-4-TURBO-2024-04-09)Few-Shot-CoT86.3293.6980.3182.0088.0878.6784.84Best-of-N87.3994.7085.4382.3386.9981.6786.42Few-Shot-CoT-SC87.3995.3086.2283.0088.3581.3386.93Adaptive-SC87.2295.2186.2282.6788.6281.0086.82Instant-Runoff Voting87.47 +0.0897.01 +1.7186.61 +0.3983.00 +0.0088.35 +0.0082.00 +0.6787.41 +0.48Borda Count Voting87.31 −0.0896.84 +1.5485.04 −1.1883.33 +0.3388.35 +0.0081.67 +0.3487.09 +0.16Mean Reciprocal Rank Voting87.31 −0.0897.01 +1.7186.22 +0.0083.00 +0.0088.62 +0.2781.67 +0.3487.30 +0.37</p>
<p>Table 6 :
6
The tie rates for different voting methods.</p>
<p>WikiData (300) Question: The language of The Rise and Fall of Ziggy Stardust and the Spiders from Mars is Answer: English.Jane got her job in 2016.Today is her 3-year work anniversary.She still remember that on Dec 2, her second day at work, she spilled coffee on her laptop.What is the date today in MM/DD/YYYY?Answer: 12/01/2019.
Date Understanding (369)Question: Word Unscrambling (300)Question: The word eptnerrreenu is a scrambled versionof the English wordAnswer: entrepreneur.</p>
<p>Table 7 :
7
Examples of each dataset, the numbers in brackets denote the number of evaluation questions.</p>
<p>Table 10 :
10
The standard deviation results in subscripts of multiple experimental runs.</p>
<p>Table 11 :
11
Example of instant-runoff voting on CommonsenseQA.</p>
<p>Table 12 :
12
Example of Borda count voting voting on CommonsenseQA.</p>
<p>CommonsenseQAQuestion: The man acted ridiculous at the funeral, what attitude should he have taken?Options: (A) straightforward (B) serious (C) solemn (D) somber (E) funny.Ground-Truth Label: C Self-Consistency Answer through Majority Voting: B. ✗ Our Ranked Answers: Response 1: The most likely attitude to take at a funeral is solemn.This word implies a sense of respect and seriousness that is appropriate for such a setting.The other options, such as straightforward, serious, and somber, are all related to seriousness and respect, but "funny" would be inappropriate and insensitive.The ranking of options by likelihood is:Response 2: The most appropriate attitude for the man to take at a funeral would be "serious" or "somber".Funeral are solemn occasions requiring a display of respect for the deceased and the grieving process.Funny or straight-forward would be inappropriate.The ranking of options by likelihood is:Response 3: The most appropriate demeanor at a funeral is one of solemnity, as it is a moment for grieving and honoring the departed.Words like straightforward, serious, and somber all reflect an attitude of respect and mourning, while funny would be seen as unsuitable and disrespectful.The ranking of options by likelihood is:The most likely answer is (B) serious, as a funeral is a somber occasion and a serious attitude is appropriate and respectful.The other options, like straightforward, solemn, and somber, are less likely to be the correct response to a funeral.The ranking of options by likelihood is: B &gt; C &gt; A &gt; D &gt; E.Response 5: The most likely attitude to take at a funeral is (B) serious, as this is a time of mourning and respect, and a funeral is a solemn occasion.The other options, such as straightforward, somber, and funny, are less appropriate for such an event.The ranking of options by likelihood is: B &gt; E &gt; A &gt; C &gt; D.Response 6: The most appropriate attitude for the man to have taken at a funeral would be "somber", as it reflects a sense of respect and sadness appropriate for such a solemn occasion.The other options, while potentially applicable in other contexts, are not the best match for a funeral, which is generally a time for mourning and reflection.The ranking of options by likelihood is:Response 7: The most likely attitude to take at a funeral is solemn, as it is a time for mourning and respect for the deceased.Straightforward, serious, and somber are all synonyms for showing respect and mourning, while funny would be inappropriate and disrespectful.The ranking of options by likelihood is: C &gt; B &gt; D &gt; A &gt; E.Response 8: The most likely attitude the man should have taken is (B) serious, as funerals are somber occasions requiring respect and solemnity.The other options, such as straightforward, solemn, and somber, are less appropriate for such an event.The ranking of options by likelihood is: B &gt; C &gt; D &gt; E &gt; A.Mean Reciprocal Rank Voting: C. ✓Mean Reciprocal Rank Voting Calculation:
Jyoti Marah Abdin, Hany Aneja, Ahmed Awadalla, Ammar Awadallah, Nguyen Ahmad Awan, Amit Bach, Arash Bahree, Jianmin Bakhtiari, Harkirat Bao, Behl, arXiv:2404.14219Phi-3 technical report: A highly capable language model locally on your phone. 2024arXiv preprint</p>
<p>Let's sample step by step: Adaptiveconsistency for efficient reasoning and coding with LLMs. Pranjal Aggarwal, Aman Madaan, Yiming Yang, Mausam , 10.18653/v1/2023.emnlp-main.761Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, Torsten Hoefler, 10.1609/aaai.v38i16.29720Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. David Cary, Proceedings of the 2011 Conference on Electronic Voting Technology/Workshop on Trustworthy Elections, EVT/WOTE'11. Usa Wenhu Usenix Association, Xueguang Chen, Xinyi Ma, William W Wang, Cohen, the 2011 Conference on Electronic Voting Technology/Workshop on Trustworthy Elections, EVT/WOTE'112011. 2023Transactions on Machine Learning Research</p>
<p>Think you have solved question answering? try arc, the ai2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, arXiv:1803.054572018arXiv preprint</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>What are the best systems? new perspectives on nlp benchmarking. Pierre Colombo, Nathan Noiry, Ekhine Irurozki, Stephan Clémençon, 10.1007/s00355-011-0598-2Advances in Neural Information Processing Systems. Curran Associates, Inc. Josep M Colomer2022. 201335Ramon llull: from 'ars electionis' to social choice theory</p>
<p>Deepseek-Ai , arXiv:2412.19437Deepseek-v3 technical report. 2024arXiv preprint</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.21783The llama 3 herd of models. 2024arXiv preprint</p>
<p>The original borda count and partial voting. Peter Emerson, 10.1007/s00355-011-0603-9Social Choice and Welfare. 402013</p>
<p>Leo Gao, Jonathan Tow, Stella Baber Abbasi, Sid Biderman, Anthony Black, Charles Dipofi, Laurence Foster, Jeffrey Golding, Alain Hsu, Haonan Le Noac'h, Kyle Li, Mcdonell, 10.5281/zenodo.12608602Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite. Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf; Ben Wang, Kevin Wangand Andy Zou. 2024. A framework for few-shot language model evaluation</p>
<p>Enhancing large language models in coding through multi-perspective self-consistency. Baizhou Huang, Shuai Lu, Xiaojun Wan, Nan Duan, 10.18653/v1/2024.acl-long.78Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, ( Shixiang, Machel Shane) Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. Curran Associates, Inc202235</p>
<p>Approval voting: an experiment during the french 2002 presidential election. Laboratoire d'Econométrie. Jean-François Laslier, Karine Vander Straeten, Ecole Polytechnique. 2003</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, 10.18653/v1/P17-1015Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20171</p>
<p>Aligning with human judgement: The role of pairwise preference in large language model evaluators. Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulic, Anna Korhonen, Nigel Collier, arXiv:2403.169502024arXiv preprint</p>
<p>Rui Min, Tianyu Pang, Chao Du, Qian Liu, Minhao Cheng, Min Lin, arXiv:2501.17858Improving your model ranking on chatbot arena by vote rigging. 2025arXiv preprint</p>
<p>On the theory of strategic voting. The Review of Economic Studies. P David, Myatt, 200774</p>
<p>arXiv:2303.08774GPT-4 technical report. 2023OpenAIarXiv preprint</p>
<p>. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jan Leike, and Ryan Lowe. 2022Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano</p>
<p>Elena Tutubalina, Daniel Karabekyan, and Ekaterina Artemova. 2023. Vote'n'rank: Revision of benchmarking with social choice theory. Mark Rofin, Vladislav Mikhailov, Mikhail Florinsky, Andrey Kravchenko, Tatiana Shavrina, 10.18653/v1/2023.eacl-main.48Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. the 17th Conference of the European Chapter of the Association for Computational LinguisticsDubrovnik, CroatiaAssociation for Computational Linguistics</p>
<p>Ensemble of MRR and NDCG models for visual dialog. Idan Schwartz, 10.18653/v1/2021.naacl-main.262Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, Transactions on Machine Learning Research. Featured Certification. 2023</p>
<p>CommonsenseQA: A question answering challenge targeting commonsense knowledge. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul F Christiano ; Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, 10.18653/v1/N19-1421Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American ChapterMinneapolis, MinnesotaAssociation for Computational Linguistics2020. 201933Advances in Neural Information Processing Systems</p>
<p>Found in the middle: Permutation self-consistency improves listwise ranking in large language models. Raphael Tang, Crystina Zhang, Xueguang Ma, Jimmy Lin, Ferhan Ture, 10.18653/v1/2024.naacl-long.129Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational Linguistics20241</p>
<p>. Team Gemma, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, arXiv:2408.00118arXiv preprintet al. 2024. Gemma 2: Improving open language models at a practical size</p>
<p>Soft self-consistency improves language models agents. Han Wang, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal, 10.18653/v1/2024.acl-short.28Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20242Short Papers)</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. Curran Associates, Inc202235</p>
<p>Examining inter-consistency of large language models collaboration: An in-depth analysis via debate. Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, Bing Qin, 10.18653/v1/2023.findings-emnlp.508Findings of the Association for Computational Linguistics: EMNLP 2023. Singapore2023Association for Computational Linguistics</p>
<p>An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, arXiv:2412.15115Qwen2. 5 technical report. 2024arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik R Narasimhan, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Yifan Zhang, Jingqin Yang, Yang Yuan, Andrew Chi-Chih Yao, arXiv:2308.04371Cumulative reasoning with large language models. 2023arXiv preprint</p>
<p>An electoral approach to diversify LLM-based multiagent collective decision-making. Xiutian Zhao, Ke Wang, Wei Peng, 10.18653/v1/2024.emnlp-main.158Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>8: Checkpoints of open-source models in our experiments. Note that LLaMA-3 series only include 8B and 70B models, while LLaMA-3.2 series include 1B, 3B, 11B, and 90B models. Therefore, in our experiments, we use LLaMA-3-8B and LLaMA-3.2-3B for validating on the LLaMA models. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh, Proceedings of the 38th International Conference on Machine Learning. the 38th International Conference on Machine Learning2021139Phi-3-mini-4k-instruct Phi-3-7B https://huggingface.co/microsoft/Phi-3-small-8k-instruct TableCalibrate before use: Improving few-shot performance of language models. RAT WikiData Date Understanding Word Unscrambling (LLAMA-3-8B</p>
<p>Results for approval voting using open-source LLMs, where the overall improvement is limited. Method Multiple-Choice QA Open-Ended QA. 9</p>
<p>CommonsenseQA ARC-Challenge AQUA-RAT WikiData Date Understanding Word Unscrambling (LLAMA-3.2-3B). </p>            </div>
        </div>

    </div>
</body>
</html>
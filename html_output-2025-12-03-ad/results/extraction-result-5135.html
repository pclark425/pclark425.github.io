<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5135 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5135</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5135</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-108.html">extraction-schema-108</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-270620677</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.13163v1.pdf" target="_blank">LLMatDesign: Autonomous Materials Discovery with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Discovering new materials can have significant scientific and technological implications but remains a challenging problem today due to the enormity of the chemical space. Recent advances in machine learning have enabled data-driven methods to rapidly screen or generate promising materials, but these methods still depend heavily on very large quantities of training data and often lack the flexibility and chemical understanding often desired in materials discovery. We introduce LLMatDesign, a novel language-based framework for interpretable materials design powered by large language models (LLMs). LLMatDesign utilizes LLM agents to translate human instructions, apply modifications to materials, and evaluate outcomes using provided tools. By incorporating self-reflection on its previous decisions, LLMatDesign adapts rapidly to new tasks and conditions in a zero-shot manner. A systematic evaluation of LLMatDesign on several materials design tasks, in silico, validates LLMatDesign's effectiveness in developing new materials with user-defined target properties in the small data regime. Our framework demonstrates the remarkable potential of autonomous LLM-guided materials discovery in the computational setting and towards self-driving laboratories in the future.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5135.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5135.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMatDesign</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMatDesign: Autonomous Materials Discovery with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A language-driven, closed-loop materials design framework that uses LLM agents to propose compositional/structural modifications, validate them with machine-learned force fields and property predictors, and iteratively self-reflect to reach user-defined target properties in a zero-shot/small-data regime.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>framework (uses GPT-4o, Gemini-1.0-pro)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>LLM-driven autonomous agent framework (uses transformer LLMs as reasoning engines coupled with MLFF and ML property predictors)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Framework-level method; no model fine-tuning reported. Uses Materials Project database and MatBench-derived datasets for training the ML components (MLFF and MLPP).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials science / inorganic materials discovery (design of materials with target band gaps and low formation energies; potential photovoltaic materials and stable compounds).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt engineering of LLM agents in an iterative closed-loop: at each iteration the LLM proposes one of four discrete compositional modifications (add, remove, substitute, exchange) plus a hypothesis; the modification is applied to an ASE Atoms object, relaxed with a machine-learned force field (MLFF), evaluated with a machine-learned property predictor (MLPP); LLM self-reflection on outcomes is optionally included and fed back as history.</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Modified chemical compositions and crystal structures represented as chemical formulas and ASE Atoms objects (textual Python-dict output for hypotheses and modification actions); final structures optionally validated by DFT (CIF/structure files).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Average number of modifications to reach a target band gap (convergence within 10% tolerance), average and final band gap (eV), average and minimum formation energy per atom (eV/atom), DFT validation formation energy and DFT job success/completion rate, percent compliance with natural language constraints, convergence plots over iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Materials Project (structure retrieval), MatBench datasets (mp_gap, mp_form) used to train ML property predictors, MLFF training set curated from Materials Project (187,687 structures).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LLMatDesign successfully generated new candidate inorganic materials meeting user-defined targets in silico. With GPT-4o (with history and self-reflection) the framework reached a 1.4 eV band-gap target in an average of 10.8 modifications (average final band gap ≈1.39 eV) and produced low-formation-energy candidates (ML surrogate avg ≈ -1.97 eV/atom; DFT-validated avg ≈ -2.32 eV/atom with 73.3% job success). Inclusion of modification history and self-reflection substantially improved efficiency; prompt refinements further reduced average modifications (GPT-4o Refined: avg 8.69 mods).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared against a random modification baseline, LLMatDesign (both LLMs) substantially outperformed random sampling on convergence speed and final property values; direct comparisons to other generative-model approaches were discussed qualitatively in the introduction but not benchmarked experimentally in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Current implementation restricts modifications to simple compositional operations (add/remove/substitute/exchange) and does not yet propose direct lattice/positional edits; relies on ML surrogate models (MLFF and MLPP) for iterative evaluations which may differ from DFT/experimental results (a final DFT validation step is optional); some LLM variants produce chemically infeasible suggestions (noted for Gemini-1.0-pro); performance depends strongly on prompt engineering and availability of modification history and self-reflection; DFT validation had non-unity job completion rates, and in some tasks LLMs could not beat formation energies of already low-energy starting materials.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMatDesign: Autonomous Materials Discovery with Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5135.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5135.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art transformer-based large language model used as the primary reasoning engine in LLMatDesign to propose and explain materials modifications; operated zero-shot with prompt templates and iterative self-reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLMatDesign: Autonomous Materials Discovery with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-based autoregressive LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in this paper; metadata reported: knowledge cutoff October 2023 and long context (128k) as provided by OpenAI; general pretraining on large text corpora is implied but not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Inorganic materials design: proposing compositional modifications to reach a target band gap (1.4 eV) and to minimize formation energy per atom.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt engineering (task-specific prompt templates: base, Refined, Persona) in an iterative closed-loop; outputs a Python-dictionary formatted hypothesis + discrete modification action; uses modification history and explicit self-reflection to guide subsequent steps. No fine-tuning reported (zero-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Textual Python-dictionary specifying Hypothesis and Modification (['type', 'element1', 'element2'...]); modifications applied to ASE Atoms structures; final structures validated by MLFF/MLPP and optionally DFT (structure files).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Average # of modifications to reach 1.4 eV band gap, average final band gap (eV), average and minimum formation energy per atom (eV/atom) from ML surrogate, DFT-validated formation energy and job success rate, convergence behavior across iterations, constraint compliance rates.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Materials Project (structure retrieval); MatBench mp_gap and mp_form used to train ML property predictors; MLFF training set from Materials Project (187,687 structures); DFT via VASP for final validation.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GPT-4o (with history + self-reflection) achieved the best performance in LLMatDesign experiments: average 10.8 modifications to reach band gap target (avg final band gap ≈1.39 eV); formation-energy task yielded ML surrogate averages near -1.97 eV/atom and minimum -2.72 eV/atom; DFT validation of GPT-4o-generated candidates averaged -2.32 eV/atom with a 73.3% job completion rate. Prompt refinement reduced required modifications further (GPT-4o Refined avg 8.69 modifications).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperformed the random baseline substantially (random baseline required ≈27.4 modifications on average and achieved lower final band gaps ≈1.06 eV). GPT-4o outperformed Gemini-1.0-pro in consistency and chemical plausibility (fewer infeasible suggestions) and benefitted more from history and self-reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No model size or training-data specifics provided in the paper; operated zero-shot without chemical fine-tuning (authors note fine-tuning could improve hypotheses); oscillatory behavior was observed when history/self-reflection were omitted (historyless GPT-4o sometimes alternated between moves and failed to converge efficiently); relies on surrogate ML models for most iterations; DFT validation had non-unity completion rates; compositional-only action space limits types of designs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMatDesign: Autonomous Materials Discovery with Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5135.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5135.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-1.0-pro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-1.0-pro (Google)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's multimodal large language model used as an alternative LLM engine in LLMatDesign; applied with modified prompts due to output formatting inconsistencies and evaluated in both history and historyless modes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLMatDesign: Autonomous Materials Discovery with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-1.0-pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-based multimodal LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in paper; general pretraining on large corpora is implied (context length reported as 32k), but no chemical fine-tuning was performed here.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Inorganic materials design (same tasks as GPT-4o): proposing compositional modifications to reach band gap targets and to minimize formation energy per atom.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt engineering with a tailored Gemini-specific template (framework included a distinct prompt because Gemini produced inconsistent standardized output); iterative closed-loop with optional inclusion of modification history (no self-reflection for Gemini in this work). No fine-tuning reported.</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Textual list or Python-like outputs indicating modification action and a textual reason; modifications map to ASE Atoms objects and are evaluated by MLFF/MLPP and optionally DFT.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same metrics as for GPT-4o: avg # of modifications to target, avg final band gap, avg/min formation energy per atom from ML surrogate, convergence behavior, compliance with constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>Materials Project, MatBench mp_gap and mp_form datasets used by ML components; MLFF dataset from Materials Project. (Gemini itself not fine-tuned on these datasets within this study.)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Gemini-1.0-pro with history reached the band-gap target in an average of 13.7 modifications (avg final band gap ≈1.35 eV), outperforming random but underperforming GPT-4o. For formation energy tasks, Gemini achieved low minimum formation energies comparable to GPT-4o in some runs but had worse average formation energies (history avg ≈ -0.56 eV/atom; historyless ≈ -0.17 eV/atom in surrogate predictions), indicating less consistency. Gemini occasionally suggested chemically infeasible elements (e.g., noble gases) in some runs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperformed the random baseline but was generally less consistent and chemically reliable than GPT-4o in these experiments; prompt tailoring was required to produce stable/parsable outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Produced some chemically infeasible suggestions (noble gases) in experiments; lack of self-reflection in experiments (self-reflection was included only for GPT-4o) limited its iterative improvement; required a custom prompt template due to inconsistent output format; generally less robust than GPT-4o in these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMatDesign: Autonomous Materials Discovery with Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models can generate molecules, materials, and protein binding sites directly in three dimensions as xyz, cif, and pdb files. <em>(Rating: 2)</em></li>
                <li>Fine-tuned language models generate stable inorganic materials as text. <em>(Rating: 2)</em></li>
                <li>Crystal structure generation with autoregressive large language modeling. <em>(Rating: 2)</em></li>
                <li>Mattergen: a generative model for inorganic materials design. <em>(Rating: 2)</em></li>
                <li>Leveraging large language models for predictive chemistry. <em>(Rating: 2)</em></li>
                <li>Are large language models superhuman chemists? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5135",
    "paper_id": "paper-270620677",
    "extraction_schema_id": "extraction-schema-108",
    "extracted_data": [
        {
            "name_short": "LLMatDesign",
            "name_full": "LLMatDesign: Autonomous Materials Discovery with Large Language Models",
            "brief_description": "A language-driven, closed-loop materials design framework that uses LLM agents to propose compositional/structural modifications, validate them with machine-learned force fields and property predictors, and iteratively self-reflect to reach user-defined target properties in a zero-shot/small-data regime.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "framework (uses GPT-4o, Gemini-1.0-pro)",
            "model_type": "LLM-driven autonomous agent framework (uses transformer LLMs as reasoning engines coupled with MLFF and ML property predictors)",
            "model_size": null,
            "training_data": "Framework-level method; no model fine-tuning reported. Uses Materials Project database and MatBench-derived datasets for training the ML components (MLFF and MLPP).",
            "application_domain": "Materials science / inorganic materials discovery (design of materials with target band gaps and low formation energies; potential photovoltaic materials and stable compounds).",
            "generation_method": "Prompt engineering of LLM agents in an iterative closed-loop: at each iteration the LLM proposes one of four discrete compositional modifications (add, remove, substitute, exchange) plus a hypothesis; the modification is applied to an ASE Atoms object, relaxed with a machine-learned force field (MLFF), evaluated with a machine-learned property predictor (MLPP); LLM self-reflection on outcomes is optionally included and fed back as history.",
            "output_representation": "Modified chemical compositions and crystal structures represented as chemical formulas and ASE Atoms objects (textual Python-dict output for hypotheses and modification actions); final structures optionally validated by DFT (CIF/structure files).",
            "evaluation_metrics": "Average number of modifications to reach a target band gap (convergence within 10% tolerance), average and final band gap (eV), average and minimum formation energy per atom (eV/atom), DFT validation formation energy and DFT job success/completion rate, percent compliance with natural language constraints, convergence plots over iterations.",
            "benchmarks_or_datasets": "Materials Project (structure retrieval), MatBench datasets (mp_gap, mp_form) used to train ML property predictors, MLFF training set curated from Materials Project (187,687 structures).",
            "results_summary": "LLMatDesign successfully generated new candidate inorganic materials meeting user-defined targets in silico. With GPT-4o (with history and self-reflection) the framework reached a 1.4 eV band-gap target in an average of 10.8 modifications (average final band gap ≈1.39 eV) and produced low-formation-energy candidates (ML surrogate avg ≈ -1.97 eV/atom; DFT-validated avg ≈ -2.32 eV/atom with 73.3% job success). Inclusion of modification history and self-reflection substantially improved efficiency; prompt refinements further reduced average modifications (GPT-4o Refined: avg 8.69 mods).",
            "comparison_to_other_methods": "Compared against a random modification baseline, LLMatDesign (both LLMs) substantially outperformed random sampling on convergence speed and final property values; direct comparisons to other generative-model approaches were discussed qualitatively in the introduction but not benchmarked experimentally in this work.",
            "limitations_or_challenges": "Current implementation restricts modifications to simple compositional operations (add/remove/substitute/exchange) and does not yet propose direct lattice/positional edits; relies on ML surrogate models (MLFF and MLPP) for iterative evaluations which may differ from DFT/experimental results (a final DFT validation step is optional); some LLM variants produce chemically infeasible suggestions (noted for Gemini-1.0-pro); performance depends strongly on prompt engineering and availability of modification history and self-reflection; DFT validation had non-unity job completion rates, and in some tasks LLMs could not beat formation energies of already low-energy starting materials.",
            "uuid": "e5135.0",
            "source_info": {
                "paper_title": "LLMatDesign: Autonomous Materials Discovery with Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o (OpenAI)",
            "brief_description": "A state-of-the-art transformer-based large language model used as the primary reasoning engine in LLMatDesign to propose and explain materials modifications; operated zero-shot with prompt templates and iterative self-reflection.",
            "citation_title": "LLMatDesign: Autonomous Materials Discovery with Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_type": "Transformer-based autoregressive LLM",
            "model_size": null,
            "training_data": "Not specified in this paper; metadata reported: knowledge cutoff October 2023 and long context (128k) as provided by OpenAI; general pretraining on large text corpora is implied but not detailed here.",
            "application_domain": "Inorganic materials design: proposing compositional modifications to reach a target band gap (1.4 eV) and to minimize formation energy per atom.",
            "generation_method": "Prompt engineering (task-specific prompt templates: base, Refined, Persona) in an iterative closed-loop; outputs a Python-dictionary formatted hypothesis + discrete modification action; uses modification history and explicit self-reflection to guide subsequent steps. No fine-tuning reported (zero-shot).",
            "output_representation": "Textual Python-dictionary specifying Hypothesis and Modification (['type', 'element1', 'element2'...]); modifications applied to ASE Atoms structures; final structures validated by MLFF/MLPP and optionally DFT (structure files).",
            "evaluation_metrics": "Average # of modifications to reach 1.4 eV band gap, average final band gap (eV), average and minimum formation energy per atom (eV/atom) from ML surrogate, DFT-validated formation energy and job success rate, convergence behavior across iterations, constraint compliance rates.",
            "benchmarks_or_datasets": "Materials Project (structure retrieval); MatBench mp_gap and mp_form used to train ML property predictors; MLFF training set from Materials Project (187,687 structures); DFT via VASP for final validation.",
            "results_summary": "GPT-4o (with history + self-reflection) achieved the best performance in LLMatDesign experiments: average 10.8 modifications to reach band gap target (avg final band gap ≈1.39 eV); formation-energy task yielded ML surrogate averages near -1.97 eV/atom and minimum -2.72 eV/atom; DFT validation of GPT-4o-generated candidates averaged -2.32 eV/atom with a 73.3% job completion rate. Prompt refinement reduced required modifications further (GPT-4o Refined avg 8.69 modifications).",
            "comparison_to_other_methods": "Outperformed the random baseline substantially (random baseline required ≈27.4 modifications on average and achieved lower final band gaps ≈1.06 eV). GPT-4o outperformed Gemini-1.0-pro in consistency and chemical plausibility (fewer infeasible suggestions) and benefitted more from history and self-reflection.",
            "limitations_or_challenges": "No model size or training-data specifics provided in the paper; operated zero-shot without chemical fine-tuning (authors note fine-tuning could improve hypotheses); oscillatory behavior was observed when history/self-reflection were omitted (historyless GPT-4o sometimes alternated between moves and failed to converge efficiently); relies on surrogate ML models for most iterations; DFT validation had non-unity completion rates; compositional-only action space limits types of designs.",
            "uuid": "e5135.1",
            "source_info": {
                "paper_title": "LLMatDesign: Autonomous Materials Discovery with Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Gemini-1.0-pro",
            "name_full": "Gemini-1.0-pro (Google)",
            "brief_description": "Google's multimodal large language model used as an alternative LLM engine in LLMatDesign; applied with modified prompts due to output formatting inconsistencies and evaluated in both history and historyless modes.",
            "citation_title": "LLMatDesign: Autonomous Materials Discovery with Large Language Models",
            "mention_or_use": "use",
            "model_name": "Gemini-1.0-pro",
            "model_type": "Transformer-based multimodal LLM",
            "model_size": null,
            "training_data": "Not specified in paper; general pretraining on large corpora is implied (context length reported as 32k), but no chemical fine-tuning was performed here.",
            "application_domain": "Inorganic materials design (same tasks as GPT-4o): proposing compositional modifications to reach band gap targets and to minimize formation energy per atom.",
            "generation_method": "Prompt engineering with a tailored Gemini-specific template (framework included a distinct prompt because Gemini produced inconsistent standardized output); iterative closed-loop with optional inclusion of modification history (no self-reflection for Gemini in this work). No fine-tuning reported.",
            "output_representation": "Textual list or Python-like outputs indicating modification action and a textual reason; modifications map to ASE Atoms objects and are evaluated by MLFF/MLPP and optionally DFT.",
            "evaluation_metrics": "Same metrics as for GPT-4o: avg # of modifications to target, avg final band gap, avg/min formation energy per atom from ML surrogate, convergence behavior, compliance with constraints.",
            "benchmarks_or_datasets": "Materials Project, MatBench mp_gap and mp_form datasets used by ML components; MLFF dataset from Materials Project. (Gemini itself not fine-tuned on these datasets within this study.)",
            "results_summary": "Gemini-1.0-pro with history reached the band-gap target in an average of 13.7 modifications (avg final band gap ≈1.35 eV), outperforming random but underperforming GPT-4o. For formation energy tasks, Gemini achieved low minimum formation energies comparable to GPT-4o in some runs but had worse average formation energies (history avg ≈ -0.56 eV/atom; historyless ≈ -0.17 eV/atom in surrogate predictions), indicating less consistency. Gemini occasionally suggested chemically infeasible elements (e.g., noble gases) in some runs.",
            "comparison_to_other_methods": "Outperformed the random baseline but was generally less consistent and chemically reliable than GPT-4o in these experiments; prompt tailoring was required to produce stable/parsable outputs.",
            "limitations_or_challenges": "Produced some chemically infeasible suggestions (noble gases) in experiments; lack of self-reflection in experiments (self-reflection was included only for GPT-4o) limited its iterative improvement; required a custom prompt template due to inconsistent output format; generally less robust than GPT-4o in these tasks.",
            "uuid": "e5135.2",
            "source_info": {
                "paper_title": "LLMatDesign: Autonomous Materials Discovery with Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models can generate molecules, materials, and protein binding sites directly in three dimensions as xyz, cif, and pdb files.",
            "rating": 2,
            "sanitized_title": "language_models_can_generate_molecules_materials_and_protein_binding_sites_directly_in_three_dimensions_as_xyz_cif_and_pdb_files"
        },
        {
            "paper_title": "Fine-tuned language models generate stable inorganic materials as text.",
            "rating": 2,
            "sanitized_title": "finetuned_language_models_generate_stable_inorganic_materials_as_text"
        },
        {
            "paper_title": "Crystal structure generation with autoregressive large language modeling.",
            "rating": 2,
            "sanitized_title": "crystal_structure_generation_with_autoregressive_large_language_modeling"
        },
        {
            "paper_title": "Mattergen: a generative model for inorganic materials design.",
            "rating": 2,
            "sanitized_title": "mattergen_a_generative_model_for_inorganic_materials_design"
        },
        {
            "paper_title": "Leveraging large language models for predictive chemistry.",
            "rating": 2,
            "sanitized_title": "leveraging_large_language_models_for_predictive_chemistry"
        },
        {
            "paper_title": "Are large language models superhuman chemists?",
            "rating": 1,
            "sanitized_title": "are_large_language_models_superhuman_chemists"
        }
    ],
    "cost": 0.01334725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LLMatDesign: Autonomous Materials Discovery with Large Language Models
19 Jun 2024</p>
<p>Shuyi Jia 
Computational Science and Engineering
Georgia Institute of Technology
AtlantaGAUSA</p>
<p>Chao Zhang 
Computational Science and Engineering
Georgia Institute of Technology
AtlantaGAUSA</p>
<p>Victor Fung victorfung@gatech.edu 
Computational Science and Engineering
Georgia Institute of Technology
AtlantaGAUSA</p>
<p>LLMatDesign: Autonomous Materials Discovery with Large Language Models
19 Jun 202464F7535B0E80C0E421DC3B6400ADB104arXiv:2406.13163v1[cond-mat.mtrl-sci]
Discovering new materials can have significant scientific and technological implications but remains a challenging problem today due to the enormity of the chemical space.Recent advances in machine learning have enabled data-driven methods to rapidly screen or generate promising materials, but these methods still depend heavily on very large quantities of training data and often lack the flexibility and chemical understanding often desired in materials discovery.We introduce LLMatDesign, a novel language-based framework for interpretable materials design powered by large language models (LLMs).LLMatDesign utilizes LLM agents to translate human instructions, apply modifications to materials, and evaluate outcomes using provided tools.By incorporating self-reflection on its previous decisions, LLMatDesign adapts rapidly to new tasks and conditions in a zero-shot manner.A systematic evaluation of LLMatDesign on several materials design tasks, in silico, validates LLMatDesign's effectiveness in developing new materials with user-defined target properties in the small data regime.Our framework demonstrates the remarkable potential of autonomous LLM-guided materials discovery in the computational setting and towards self-driving laboratories in the future.</p>
<p>Introduction</p>
<p>Discovering novel materials with useful functional properties is a longstanding challenge in materials science due to the vast and diverse composition and structure space these materials can inhabit [1,2].Traditional approaches to materials discovery often involve exhaustively screening materials via lab-based experiments or in silico simulations, which can be time-consuming and resource-intensive [3,4,5].Recent advancements have introduced machine learning surrogate models to predict material structures and properties [6,7], as well as generative modeling techniques to propose novel materials [8,9,10,11,12,13,14].However, these data-driven methods rely heavily on extensive training datasets, generally derived from density functional theory (DFT) calculations.These methods are less useful in most instances where such data is unavailable, or when only a limited budget exists to perform experiments or high fidelity simulations.In contrast, a human expert would be far more effective here by being able to draw from domain knowledge and prior experiences, and reason from limited examples.Therefore, a different materials design paradigm is needed in these situations where models should be developed to exhibit similar proficiencies as human experts.</p>
<p>Fueled by ever-expanding textual datasets and significant increases in computing power, large language models (LLMs) have witnessed a meteoric rise in capabilities and usage in recent years.More broadly, the remarkable performance of LLMs across diverse tasks they have not been explicitly trained on has sparked a burgeoning interest in developing and utilizing LLM-based agents capable of reasoning, self-reflection, and decision-making [15,16,17].These autonomous agents are typically augmented with tools or action modules, empowering them to go beyond conventional text processing and directly interact with the physical world, such as robotic manipulation [18,19] and scientific experimentation [20,21].As the capabilities of LLMs and LLM-based autonomous agents continue to expand, they are increasingly being recognized for their potential in scientific domains, particularly in chemistry [22].This surge in interest stems from the fact that the majority of information in chemistry exists as text, aligning closely with the text-centric nature of LLMs [23].For instance, recent studies have demonstrated the use of LLMs to extract chemical reaction information [24,25], predict chemical properties [26,27,28,29], and generate crystal structures [30,31,32], among many other applications.In particular, chemical research, such as materials discovery, traditionally hinges on human expertise and experience encapsulated in scientific publications.LLMs, capable of ingesting vast quantities of these publications beyond human capacity, have the potential to act as intelligent copilots that might be able to extract key insights, uncover hidden patterns, and propose novel methodologies, thereby accelerating scientific progress [23].In this work, we present LLMatDesign (Fig. 1), a language-based framework for materials design powered by state-of-the-art LLMs.LLMatDesign is capable of interpreting human-provided instructions and design constraints, using computational tools for materials evaluation, and leveraging existing chemical knowledge and feedback to act as a highly effective autonomous materials design agent.Unlike traditional methods that rely on explicit mathematical formulations and programmed solvers, LLMatDesign as an autonomous agent works with natural language directly, allowing it to quickly adapt to a diverse set of tasks, materials and target properties by simply modifying the prompt.In each step, LLMatDesign generates new designs of a material by choosing a modification of a starting material along with a corresponding hypothesis.It then applies the modification to the material and validates its property.Here, we use surrogate models as a stand-in for DFT to perform property validation, which can be readily replaced with any other computational or, potentially, experimental validation method.Following this, LLMatDesign reflects on the applied modification and its outcome.This reflection, along with the modified material and hypothesis, is then incorporated into the prompt in an iterative process.Moreover, LLMatDesign's flexibility allows incorporation of the entire modification history or userdefined requirements, offering even finer control over the discovery process.</p>
<p>LLMatDesign</p>
<p>By utilizing state-of-the-art LLMs as chemical reasoning engines, LLMatDesign represents a novel framework for materials discovery which, unlike many current data-driven generative methods, eliminates the need for large training datasets derived from ab initio calculations.LLMatDesign's ability to interpret human instructions and incorporate design constraints enables rapid adaptation to new conditions, tasks, materials, and target properties via prompt modification-a flexibility that is often very difficult for current materials discovery methods such as those using generative models.More importantly, LLMatDesign's ability to generate hypothesis, evaluate outcomes, and self-reflect on past decisions in a closed-loop manner showcases the potential for a fully automated artificial intelligence (AI) agent for materials design in both a computational setting or towards robotic laboratories in the future.</p>
<p>Results</p>
<p>LLMatDesign Framework</p>
<p>LLMatDesign is a flexible framework powered by an LLM and empowered with the necessary tools to perform materials discovery.The discovery process with LLMatDesign begins by taking the chemical composition and property of a starting material, along with a target property value, as user-provided inputs.If a chemical composition is specified without an initial structure, LLMatDesign will automatically query the Materials Project [33] database to retrieve the corresponding structure.If multiple candidates match the query, the structure with the lowest formation energy per atom is selected.LLMatDesign then intelligently recommends one of four possible modifications-addition, removal, substitution, or exchange-to the material's composition and structure to achieve the target value.Specifically, "exchange" refers to swapping two elements within the material, while "substitution" involves replacing one type of element with another."Removal" means eliminating a specific element from the material.In the case of "addition," an atom of the suggested element is added to the unit cell of the material, with its position randomly determined.These four choices act as a proxy to physical processes in materials modification, such as doping or creating defects, and additional modification choices can also be readily added or removed as desired within the framework.</p>
<p>LLMatDesign Prompt Template (GPT-4o)</p>
<p>I have a material and its <property>.<definition of property>.</p>
<p>(<chemical composition>, <property value>)</p>
<p>Please propose a modification to the material that results in <objective>.You can choose one of the four following modifications:</p>
<ol>
<li>
<p>exchange: exchange two elements in the material 2. substitute: substitute one element in the material with another 3. remove: remove an element from the material 4. add: add an element to the material <additional constraints> Your output should be a python dictionary of the following the format:
{Hypothesis: $HYPOTHESIS, Modification: [$TYPE, $ELEMENT 1, $ELEMENT 2]}.
Here are the requirements:</p>
</li>
<li>
<p>$HYPOTHESIS should be your analysis and reason for choosing a modification 2. $TYPE should be the modification type; one of "exchange", "substitute", "remove", "add" 3. $ELEMENT should be the selected element type to be modified.For "exchange" and "substitute", two $ELEMENT placeholders are needed.For "remove" and "add", one $ELEMENT placeholder is needed.</p>
</li>
</ol>
<p><modification history></p>
<p>Figure 2: Prompt template for LLMatDesign with GPT-4o.Text placeholders in red angular brackets are specific to the task given to LLMatDesign.Text placeholders in blue angular brackets are optional and can be omitted if not needed.For Gemini-1.0-pro'sprompt template, see Appendix A.</p>
<p>Self-reflection Prompt Template</p>
<p>After completing the following modification on <previous composition>, we obtained <current composition> and the <property> changed from <previous value> to <current value>.Please write a brief post-action reflection on the modification, explaining how successful it was in achieving the <objective> and the reasons for its success or failure: <hypothesis>, <modification> Alongside the proposed modification, LLMatDesign provides a hypothesis explaining why the suggested change could be beneficial.This hypothesis generated by the LLM provides a window into the reasoning behind its choices and provides a degree of interpretability which is not possible with traditional optimization algorithms.</p>
<p>Next, LLMatDesign modifies the material based on the given suggestion, relaxes the structure using a machine learning force field (MLFF), and predicts its properties using a machine learning property predictor (MLPP).</p>
<p>If the predicted property of the new material does not match the target value within a defined threshold, LLMatDesign then evaluates the effectiveness of the modification through a process called self-reflection where commentary is provided on the success of failure of the chosen modification.</p>
<p>After self-reflection, a modification history message is created.This message includes the modified chemical composition, the modification itself, the hypothesis behind the modification, and the self-reflection results.This history is then fed back into LLMatDesign, which enters the next design decision-making phase towards the goal of achieving the target property.The entire process repeats in a loop until termination conditions are met.Optionally, density functional theory (DFT) calculations can be performed on the final material.</p>
<p>At the core of the entire workflow, LLMatDesign utilizes an LLM engine or agent which translates user-defined objectives into appropriate Materials Project API calls, drives the design decision-making process, and conducts self-reflection on previous decisions to enhance performance.In this work, we demonstrate the capabilities of LLMatDesign using two state-of-the-art LLMs: GPT-4o [34] and Gemini-1.0-pro[35].However, the framework is model-agnostic and should function effectively with any capable LLMs.The overall architecture and algorithm of LLMatDesign is depicted in Fig. 1 and Algo. 1 respectively.The modification and self-reflection prompt templates are shown in Fig. 2 and 3 respectively.</p>
<p>Algorithm 1 LLMatDesign Algorithm</p>
<p>Input: (x 0 , y 0 ): chemical composition and property of the starting material.y target : target property value to achieve.M := ∅: set of history messages, if any.Output: (x i , y i ): chemical composition and property of the new material.
for i = 1 : N do ▷ N : maximum number of modifications s i , h i ← LLM(x i−1 , y i−1 , y target , M) ▷ s: modification; h: hypothesis xi ← perform modification(x i−1 , s i ) x i ← MLFF(x i ) y i ← MLPP(x i ) if |y i − y target |/|y target | ≤ ε then ▷ ε: error tolerance return (x i , y i ) end if r i ← LLM (x i−1 , x i , y i−1 , y i , s i , h i ) ▷ r: self-reflection m i ← create history message (s i , h i , r i ) ▷ m: history message M ← M ∪ {m i } end for</p>
<p>Evaluation</p>
<p>To evaluate the effectiveness of LLMatDesign, we performed a set of experiments with 10 starting materials randomly selected from the Materials Project [6].Specifically, we focus on designing materials targeting two material properties and their corresponding objectives:</p>
<p>• Band gap (eV): design a new material with a band gap of 1.4 eV.</p>
<p>• Formation energy per atom (eV/atom): design a new material with the most negative formation energy possible.</p>
<p>The objective of achieving a band gap value of 1.4 eV is chosen as an example of designing an ideal photovoltaic material with a band gap within the range of 1-1.8 eV [36], and the aim of obtaining the most negative formation energy requires LLMatDesign to suggest modifications that could result in more stable materials.</p>
<p>For the band gap experiments, we record the average number of modifications taken by LLMatDesign, with a maximum budget of up to 50 modifications.A 10% tolerance of error to the target is used as the convergence criterion.For the formation energy experiments, a fixed budget of 50 modifications is used, and both the average and minimum formation energies are recorded.The experiment is then repeated 30 times for each starting material.We present results for two different LLM engines: Gemini-1.0-proand GPT-4o.Within each LLM engine, two variants of experiments-history and historyless-are conducted to evaluate the impact of including the knowledge of prior modification history.All results are compared against a random baseline, where modifications to materials are randomly selected.The results for band gap and formation energy per atom are shown in Table 1 and Table 2 respectively.Note that self-reflection is included only for GPT-4o and not for Gemini-1.0-pro.We observe that GPT-4o with past modification history performs the best in achieving the target band gap value of 1.4 eV, requiring an average of 10.8 modifications (Table 1).In comparison, Gemini-1.0-prowith history takes an average of 13.7 modifications.Both methods signifcantly outperform the baseline, whic requires 27.4 modifications.Adding modification history to subsequent prompts allows the LLMs to converge to the target more quickly, as both Gemini-1.0-proand GPT-4o with modification history outperform their historyless counterparts.Notably, the performance gap between the history and historyless variants is smaller for Gemini-1.0-prothan for GPT-4o.From a closer inspection of the modification paths of GPT-4o without history, we find that GPT-4o often alternates between a few of the same modifications until reaching the maximum number of allowed iterations (see Fig. 4).For the two starting materials where GPT-4o without history performs the best (Co 2 TiO 4 and SrTiO 3 ), the final materials frequently converge to identical composition by following the same modification sequence.This indicates a lack of diversity in the newly generated materials when no history is included in LLMatDesign's iterative loop.In addition, GPT-4o with history achieves the best final band gap value, averaging 1.39 eV, followed by Gemini-1.0-proat 1.35 eV, and random at 1.06 eV.</p>
<p>LLMatDesign's superior performance is also apparent when finding new materials with the lowest formation energy per atom (Table 2), consistently outperforming the random baseline.Specifically, both the history and historyless variants of GPT-4o achieve the lowest average formation energies, with −1.97 eV/atom and −1.99 eV/atom, respectively.GPT-4o with history also achieves the lowest minimum formation energy per atom at −2.72 eV/atom.Interestingly, while the minimum formation energy per atom values achieved by Gemini-1.0-proare close to that of GPT-4o, its average formation energy per atom values are significantly higher, indicating that it struggles to consistently suggest chemically stable modifications for the materials.Nonetheless, Gemini-1.0-prostill noticeably outperforms the baseline.In</p>
<p>GPT-4o</p>
<p>History Historyless Random In Fig. 4, we plot the band gaps and formation energies per atom over 50 modifications, averaged across 10 starting materials.The target band gap of 1.4 eV is indicated by the grey horizontal line.Both history and historyless variants of Gemini-1.0-proand GPT-4o demonstrate quick convergence to the target band gap.However, the GPT-4o historyless variant exhibits zig-zag oscillations in band gap values as modifications increase.This occurs because, without historical information, GPT-4o tends to oscillate between a few of the same moves, causing the band gap to fluctuate without improving.In contrast, the random baseline fails to converge to 1.4 eV within the maximum allowed 50 modifications.For formation energy, our findings indicate that GPT-4o is consistently able to suggest modifications which keep formation energy low on average around −2 eV/atom, though Gemini-1.0-prostruggles to do so despite being able to obtain a low minimum formation energy.Notably, neither GPT-4o nor Gemini-1.0-proare able to beat the formation energy of the starting materials, likely due to the the fact that these materials are already at or near the lowest energy states.</p>
<p>Fig. 5 presents heatmaps over the periodic table displaying the element occurrences in the modifications for both the band gap and formation energy tasks, which reveal additional insights into the reason for the good performance for LLM-driven design.The number of occurrences of each element is collected across all runs and starting materials.In the heatmaps for the random baseline, all elements are chosen at nearly uniform frequencies.This result is to be expected, as the random algorithm samples elements with atomic numbers up to 99 uniformly.Meanwhile, in the heatmaps for the LLM cases, there is a clear distribution towards certain elements, mostly focusing on elements within the first four rows of the periodic  metals and Actinides.Both LLM models share similar distributions, such as a preference for elements like oxygen, however Gemini-1.0-pro'ssuggestions appear to exhibit a greater element diversity compared to GPT-4o, including some of the transition metals.With Gemini-1.0-pro,we also occasionally observe modifications suggested by the LLM that include noble gases, which is not chemically feasible due to their inert nature.With GPT-4o, this does not occur (see Fig. C.1). Regardless, both LLM models are able to consistently suggest chemically viable elements for modification, which is akin to how a human expert would make similar choices based on chemical intuition or from past examples in the literature.</p>
<p>In Fig. 6, we present an example of the full process whereby LLMatDesign successfully completes a design task to achieve a band gap of 1.40 eV.In the first step, LLMatDesign suggests modifying the starting material CdCu 2 GeS 4 by substituting S with Se, given the hypothesis that increasing atomic radius and changing the electronegativity can alter the band gap.Upon modification, the new material CdCu 2 GeSe 4 was found to have an even smaller band gap, which is contrary to the desired effect as noted by the reflection.This history is included in the second step of modification, whereby LLMatDesign suggests a subsequent modification of Ge with Si, which increases the gap.The reflection notes a partial success is achieved, but is still not enough to reach the target, whereupon a third step is taken.In the third step, Cu is substituted with Zn, which finally achieves the desired band gap within an acceptable threshold, ending the process.From this example, we can observe the LLM is successful at 1) recognizing differences in element properties (i.e.Se having a larger atomic radius than S), 2) highlighting these properties as being relevant to the design task (i.e.atomic radius, electronegativity, and electronic configuration affecting the band hap), 3) and recognizing whether a modification is successful and the degree of success in the reflection.We will show in the subsequent section that it is this reasoning and reflection process which has a significant impact on its success.</p>
<p>In the final step of the design process, a DFT calculation is performed to validate the material's properties which were obtained from a ML surrogate model.Here, we use DFT to compute the formation energy of the minimum energy structures in all 30 runs for each 10 starting materials obtained with GPT-4o and random sampling.On average, structures generated by LLMatDesign using GPT-4o with history achieved a formation energy of -2.32 eV/atom with a job completion rate of 73.3%.In comparison, the random baseline obtained an average formation energy of -1.51 eV/atom, with a significantly lower job completion rate of 40.0%(see Fig.</p>
<p>E.1).The lower completion rate is likely due to a much higher likelihood of unreasonable compositions and configurations obtained from random sampling which cause the DFT calculations to fail.These results also follow the same relative trends in Table 2.In a situation where DFT is directly used in each iteration rather than a surrogate model, this step can be omitted, or potentially the materials can be validated further with experiments.Post-modification Reflection: The modification of substituting Ge with Si in Cd2Cu4Ge2Se8 to form Cd2Cu4Si2Se8 resulted in an increase in the band gap from 0.01 eV to 0.39 eV, which indicates a partial success in increasing the band gap, but it fell short of the target value of 1.4 eV, suggesting that additional modifications or different approaches may be necessary to achieve the desired band gap.</p>
<p>Hypothesis: Substituting germanium (Ge) with silicon (Si) is likely to increase the band gap of the material.Silicon has a smaller atomic radius and different electronegativity compared to germanium, which can affect the electronic structure and potentially increase the band gap.</p>
<p>Self-reflection</p>
<p>To quantify the effect of self-reflection on the performance of LLMatDesign, we conduct band gap experiments using GPT-4o and the same set of 10 starting materials, where we aim to find a new material with a target band gap of 1.4 eV.Like with the history variant, past modifications are incorporated into the prompting loop.However, in this case, self-reflection is omitted completely.In other words, the history message only includes the modification and hypothesis pairs (see Algo. 1).The results from these experiments are shown in Table 3.As previously discussed, GPT-4o with history achieves an average of 10.8 modifications, while GPT-4o without history requires 26.6 modifications.In comparison, GPT-4o with history but without self-reflection now needs an average of 23.4 modifications, which is over twice as many compared to including self-reflection.These results suggest that self-reflection, which involves the LLM evaluating and reasoning through its previous design choices, plays a crucial role in enhancing the efficiency of LLMatDesign in achieving the given objective.</p>
<p>Prompting</p>
<p>Well-crafted prompts are essential for eliciting accurate and useful responses from LLMs.While the base prompt template, shown in Fig. 2, works as intended, we subsequently show that optimizing this prompt can improve the performance of LLMatDesign even further.To this end, we develop two additional prompt templates in a non-exhaustive demonstration.The first template, termed GPT-4o Refined, is an enhancement of the original prompt (Fig. 2) created by GPT-4o itself.This refinement includes rephrasing and reformatting parts of the original prompt and appending the following sentence: "Take a deep breath and work on this problem stepby-step.Your thoughtful and detailed analysis is highly appreciated."The second template, named Persona, mirrors the original prompt but incorporates the persona of a materials specialist.Specifically, it begins with a declaration that the LLM is a materials design expert working on developing new materials with specific properties.Detailed descriptions of these prompt templates are provided in Appendix A.</p>
<p>We conduct the same experiments on the band gap task using GPT-4o as the LLM engine for LLMatDesign across all 10 starting materials.The results, shown in Table 4, indicate that both the GPT-4o Refined and Persona prompt templates outperform the GPT-4o with history, with the GPT-4o Refined template achieving the best performance, requiring an average of only 8.69 modifications to complete the task.The improvement over the original prompt template indicates that careful prompt optimization can positively enhance the efficiency and accuracy of LLM-directed materials discovery frameworks, and that this process can even be performed by the LLM itself.This is a particularly intriguing discovery as it hints towards an unprecedented level of autonomy which can be enabled by LLMs, whereby the prompts and instructions in the framework can be continuously tuned in an automated manner with minimal human intervention.</p>
<p>Constrained Materials Design</p>
<p>Materials discovery with constraints ensures scientific, economic, and political viability.For instance, avoiding the use of rare earth metals can reduce dependency on limited and expensive resources, mitigate supply chain risks, and align with environmental and ethical standards.To this end, we evaluate LLMatDesign under three constraints limiting its action space.Experiments are conducted on the band gap task using the starting material SrTiO 3 with GPT-4o to test whether these constraints are obeyed.Like before, each experiment is repeated 30 times, and the percentage of modifications adhering strictly to the constraints is calculated across all runs.As shown in Table 5, LLMatDesign perfectly adheres to the constraints of "do not use Ba or Ca" and "do not modify Sr," achieving 100% compliant modifications.For the constraint "do not have more than 4 distinct elements," only 4 out of 509 modifications by LLMatDesign include 5 distinct elements, resulting in a high compliance rate of 99.02%.These results demonstrate LLMatDesign's robust capability in adhering to predefined constraints as described by natural language, an advantage unique to LLM-driven design.</p>
<p>Further Discussion</p>
<p>Through extensive experiments, we find LLMatDesign consistently outperforms baselines by a significant margin, demonstrating the viability of using LLM-based autonomous agents for materials discovery tasks under a limited budget.While the random baseline uniformly samples from a set of elements for modification (see Fig. 5), LLMatDesign, whether utilizing GPT-4o or Gemini-1.0-pro,exhibits inherent chemical knowledge, enabling it to provide chemically meaningful suggestions.Furthermore, GPT-4o accurately recognizes periodic trends such as atomic radius and electronegativity in its hypotheses and self-reflections in guiding its decisions.In contrast, Gemini-1.0-pro is more prone to errors in this regard, likely due to it being a less robust LLM.Further experiments also show the critical role of self-reflection in the performance of the LLM.This indicates that by reviewing and learning from its previous decisions, LLMatDesign can refine its future suggestions more effectively.This iterative learning process helps the model understand the implications of its modifications better, leading to quicker convergence.In general, it is evident that there are more complex underpinnings behind the remarkable effectiveness of LLM-driven design than simply predicting most likely outcomes.</p>
<p>This work also demonstrates the lower-bound capabilities of LLM-based design, which is performed without further fine-tuning in a zero-shot manner.A natural extension of this approach would be to further train LLMs on chemical and materials knowledge, such as those obtained from literature articles.In the future, it would be highly desirable for a chemically fine-tuned to provide more insightful hypotheses and explanations, and even refer to specific references of prior published experiments to support them.These capabilities can potentially be within reach given the growing prevalence of powerful open-source LLMs and parameter-efficient fine-tuning.</p>
<p>In the current examples, LLMatDesign comes up with new materials designs from a limited set of modifications on the composition of a material.Nonetheless, this framework is general and can include more complex modifications which act not only on the composition space but also the structure space.Future work in this direction will focus on incorporating structural information when describing the material being modified, and also suggest modifications which directly act on the positions and lattice of the crystal structure.To this end, recent advances in multimodal LLMs can be applied here, where the atomic structure is considered to be an additional modality to be encoded in addition to the text modality.</p>
<p>Conclusion</p>
<p>In this work, we present LLMatDesign, a novel materials design framework powered by state-of-the-art LLMs that works directly with user-defined design requirements and constraints in natural language.It integrates computational tools for structure relaxation and property evaluation, incorporates internal chemical knowledge, and learns from previous iterations to function as an automated material design framework with high efficiency.</p>
<p>Additionally, LLMatDesign quickly adapt to different tasks, target properties and design constraints by simply modifying the prompt.In our experiments, LLMatDesign consistently outperforms the baseline, demonstrating the effectiveness of the framework in developing new materials.Our work highlights the potential for fully automated AI-driven materials discovery that can be seamlessly integrated into autonomous laboratories in the future.</p>
<p>Methods</p>
<p>Large Language Models</p>
<p>Large language models (LLMs) are a class of machine learning models built on the transformer architecture [37].By training on vast amounts of text data, these models can understand and generate text in a human-like manner.In this work, GPT-4o [34] refers to OpenAI's gpt-4o model, which has a context length of 128K and a knowledge cutoff date of October 2023.Gemini-1.0-pro[35] refers to Google's LLM with the same name, featuring a context length of 32K.</p>
<p>Machine Learning Force Field</p>
<p>Machine learning force fields (MLFFs) represent a significant advancement in computational chemistry and materials science.By utilizing state-of-the-art machine learning models and training on extensive datasets of atomic structures with energies, forces, and stresses, MLFFs can achieve high accuracy in predicting these properties, often rivaling ab initio methods such as density functional theory (DFT) [38].More importantly, MLFFs provide these high-accuracy predictions with unprecedented computational efficiency, enabling the simulation of larger systems and longer timescales.In this study, we train a TorchMD-Net model [39] using the MatDeepLearn framework [40,41].The training dataset, curated from the Materials Project [6], comprises 187,687 crystal structures with associated energies, forces, and stresses.The model is trained for 400 epochs on a single Nvidia A100 80GB GPU.</p>
<p>Machine Learning Property Predictor</p>
<p>Similar to machine learning force fields (MLFFs), machine learning property predictors (MLPPs) leverage advanced machine learning models trained on large datasets to make fast and accurate predictions for specific target properties.In this study, we train TorchMD-Net models to predict two separate properties: band gap and formation energy per atom.The datasets used are the mp gap and mp form datasets from the MatBench benchmark [42], containing 106,113 and 132,752 structures from the Materials Project [6], respectively.Each model is trained for 200 epochs on a single Nvidia A100 80GB GPU.</p>
<p>Modification of Material</p>
<p>Once LLMatDesign suggests a modification to achieve the user's target objective, the material is modified accordingly.Specifically, as illustrated in Fig. 1, there are four types of modifications: exchange, substitute, remove, and add.Each modification is applied directly to an ase.Atoms object representing the material.For example, given the modification ['exchange', 'Sr', 'Ti'], all Sr atoms in the material are replaced with Ti atoms and vice versa.After applying the modification, the structure undergoes relaxation using a machine learning force field (MLFF).</p>
<p>Modification of Material</p>
<p>The DFT calculations were performed using the Vienna Ab Initio Simulation Package (VASP) [43,44].All calculations followed the same settings specified by the "MPRelaxSet" in the Pymatgen library [45] used in Materials Project.</p>
<p>Data Availability</p>
<p>The authors declare that the data, materials and code supporting the results reported in this study are available upon the publication of this manuscript.C Heatmaps</p>
<p>B Convergence Plots</p>
<p>E DFT Calculations</p>
<p>Figure 1 :
1
Figure 1: Overview of LLMatDesign.The discovery process with LLMatDesign begins with user-provided inputs of chemical composition and target property.It recommends modifications (addition, removal, substitution, or exchange), and uses machine learning tools for structure relaxation and property prediction.Driven by an LLM, this iterative process continues until the target property is achieved, with self-reflection on past modifications fed back into the decision-making process at each step.</p>
<p>Figure 3 :
3
Figure 3: Prompt template for self-reflection.Text placeholders in red angular brackets are specific to the task given to LLMatDesign.</p>
<p>Fig. D.1 and D.2, we visualize 20 materials discovered byLLMatDesign for the band gap and formation energy tasks, respectively.These materials are obtained from the first run of all 10 starting materials.For the band gap task, the final materials are selected.For the formation energy task, the materials with the lowest formation energy per atom are chosen.energy (eV/atom)</p>
<p>Figure 4 :
4
Figure 4: Average band gaps and formation energies over 50 modifications.The grey horizontal line indicates the target band gap of 1.4 eV.The colored dots on the x-axis indicate the average number of modifications taken for each method to reach the target.For formation energy, the goal is to achieve the lowest possible value.</p>
<p>Figure 5 :
5
Figure 5: Heatmaps of element frequencies in band gap (BG) and formation energy (FE) tasks.The periodic table is color-coded to indicate the frequency of each element's occurrence in all modified materials (both intermediate and final) across all runs and starting materials.Darker colors represent higher frequencies, while lighter colors denote lower frequencies or absence.The visualization employs log-scaling to effectively highlight the distribution and prevalence of elements.</p>
<p>Given</p>
<p>CdCu2GeS4, which has a band gap of 0.388 eV, find a new material with a target band gap of 1.copper (Cu) with zinc (Zn) is likely to increase the band gap of the material.Zinc has a different electronic configuration and electronegativity compared to copper, which can affect the electronic structure and potentially increase the band gap.Post-modification Reflection: The modification of substituting copper (Cu) with zinc (Zn) in Cd2Cu4Si2Se8 was highly successful in achieving the target band gap value of 1.4 eV, as the band gap increased from 0.39 eV to 1.42 eV, closely aligning with our hypothesis.I have found a new material, Cd2Zn4Si2Se8, which has a band gap of 1.42 eV, closely matching the target value of 1.4 eV.Hypothesis: substituting sulfur (S) with selenium (Se) is likely to increase the band gap of the material.Selenium has a larger atomic radius and different electronegativity compared to sulfur, which can affect the electronic structure and increase the band gap.Post-modification Reflection: the modification of substituting sulfur (S) with selenium (Se) in Cd2Cu4Ge2S8 to obtain Cd2Cu4Ge2Se8 was unsuccessful in achieving the target band gap value of 1.4 eV, as the band gap decreased significantly to 0.01 eV instead of increasing, contrary to our hypothesis.</p>
<p>Figure 6 :
6
Figure 6: Example of LLMatDesign with GPT-4o on the task of modifying the starting material CdCu 2 GeS 4 to achieve a band gap of 1.40 eV.The starting material is retrieved from the Materials Project with chemical formula Cd 2 Cu 4 Ge 2 S 8 .</p>
<p>Figure B. 1 :
1
Figure B.1: Average band gaps over 50 modifications for all 10 starting materials using GPT-4o.The grey horizontal line indicates the target band gap of 1.4 eV.The colored dots on the x-axis indicate the average number of modifications taken for each method to reach the target.</p>
<p>Figure B. 2 :
2
Figure B.2: Average band gaps over 50 modifications for all 10 starting materials using Gemini-1.0-pro.The grey horizontal line indicates the target band gap of 1.4 eV.The colored dots on the x-axis indicate the average number of modifications taken for each method to reach the target.</p>
<p>Figure B. 3 :
3
Figure B.3: Average formation energies over 50 modifications for all 10 starting materials using GPT-4o.The goal is to achieve the lowest possible formation energy per atom.</p>
<p>Figure B. 4 :
4
Figure B.4: Average formation energies over 50 modifications for all 10 starting materials using Gemini-1.0-pro.The goal is to achieve the lowest possible formation energy per atom.</p>
<p>Figure C.1: Heatmaps of element frequencies in band gap (BG) and formation energy (FE) tasks for Gemini-1.0proand GPT-4o without history.The periodic table is color-coded to indicate the frequency of each element's occurrence in all modified materials (both intermediate and final) across all runs and starting materials.Darker colors represent higher frequencies, while lighter colors denote lower frequencies or absence.The visualization employs log-scaling to effectively highlight the distribution and prevalence of elements.</p>
<p>Figure D. 1 :BaV 2 2 4 Li 2 2 4 Li 2 2 2 F − 2 Figure D. 2 :
122422422222
Figure D.1: Visualization of the final structures obtained by LLMatDesign for the band gap task.These structures are obtained from the first run of all 10 starting materials.The chemical formulae in red represent the starting materials, followed by the formulae of the final structures and their corresponding band gaps.GPT-4o with history is utilized as the LLM engine.</p>
<p>Table 1 :
1
LLMatDesign's performance in achieving a new material with a target band gap of 1.4 eV.Each experiment is repeated 30 times, and the average number of modifications taken to reach the target value is recorded.
Average # of ModificationsAverage Final Band Gap (eV)Starting MaterialGemini-1.0-proGPT-4oRandomGemini-1.0-proGPT-4oRandomHistoryHistorylessHistoryHistorylessHistory HistorylessHistory HistorylessBaV 2 Ni 2 O 817.714.417.730.422.41.231.421.391.891.12CdCu 2 GeS 411.113.43.39.528.71.411.391.441.381.01CeAlO 314.315.17.416.926.71.421.391.411.681.21Co 2 TiO 48.813.15.51.629.71.401.301.361.421.02ErNi 2 Ge 226.824.819.347.631.81.181.261.360.430.90Ga 2 O 310.312.312.737.732.81.341.381.361.760.87Li 2 CaSiO 415.720.514.329.327.41.361.371.411.811.09LiSiNO12.410.44.12.827.41.381.391.391.501.09Na 2 ZnGeO 413.015.011.549.422.91.401.391.392.351.15SrTiO 37.28.812.040.624.31.421.411.451.641.11Avg.13.714.810.826.627.41.351.371.391.591.06</p>
<p>Table 2 :
2Average Formation Energy (eV/atom)Minimum Formation Energy (eV/atom)Starting MaterialGemini-1.0-proGPT-4oRandomGemini-1.0-proGPT-4oRandomHistoryHistorylessHistoryHistorylessHistory HistorylessHistory HistorylessBaV 2 Ni 2 O 8-0.80-0.20-2.45-2.50-0.12-2.69-2.30-2.91-2.74-1.99CdCu 2 GeS 4-0.190.11-1.05-0.610.29-1.31-1.59-1.61-0.72-1.37CeAlO 3-0.77-0.28-2.79-2.24-0.04-3.44-3.22-3.73-3.73-2.50TiO 4-0.390.03-1.57-1.490.0-2.64-2.08-2.48-2.10-1.80ErNi 2 Ge 2-0.02-0.19-0.54-0.74-0.11-0.96-1.71-0.94-1.57-1.40Ga 2 O 3-0.19-0.12-1.61-2.07-0.16-2.05-1.82-3.31-3.29-1.67Li 2 CaSiO 4-0.77-0.41-2.30-2.69-0.20-2.94-2.60-3.13-2.98-2.27LiSiNO-0.38-0.19-1.75-1.54-0.15-2.01-2.01-2.60-1.72-1.75Na 2 ZnGeO 4-0.79-0.25-2.62-2.520.05-2.48-2.34-2.87-2.55-1.85SrTiO 3-1.26-0.23-3.01-3.54-0.02-3.40-3.09-3.65-3.57-2.38Avg.-0.56-0.17-1.97-1.99-0.05-2.39-2.28-2.72-2.50-1.90
LLMatDesign's performance in achieving a new material with a as low as possible formation energy per atom.Each experiment consists of 50 modifications, and is repeated 30 times.</p>
<p>table and avoiding noble Cr Mn Fe Co Ni Cu Zn Ga Ge As Se Br Kr Rb Sr Y Zr Nb Mo Tc Ru Rh Pd Ag Cd In Sn Sb Te I Xe Cs Ba Hf Ta W Re Os Ir Pt Au Hg Tl Pb Bi Po At Rn Fr Ra Rf Db Sg Bh Hs Mt Ds Rg Cn Nh Fl Mc Lv Ts Og La Ce Pr Nd Pm Sm Eu Gd Tb Dy Ho Er Tm Yb Lu Ac Th Pa U Np Pu Am Cm Bk Cf Es Fm Md No Lr Cr Mn Fe Co Ni Cu Zn Ga Ge As Se Br Kr Rb Sr Y Zr Nb Mo Tc Ru Rh Pd Ag Cd In Sn Sb Te I Xe Cs Ba Hf Ta W Re Os Ir Pt Au Hg Tl Pb Bi Po At Rn Fr Ra Rf Db Sg Bh Hs Mt Ds Rg Cn Nh Fl Mc Lv Ts Og La Ce Pr Nd Pm Sm Eu Gd Tb Dy Ho Er Tm Yb Lu Ac Th Pa U Np Pu Am Cm Bk Cf Es Fm Md No Lr Cr Mn Fe Co Ni Cu Zn Ga Ge As Se Br Kr Rb Sr Y Zr Nb Mo Tc Ru Rh Pd Ag Cd In Sn Sb Te I Xe Cs Ba Hf Ta W Re Os Ir Pt Au Hg Tl Pb Bi Po At Rn Fr Ra Rf Db Sg Bh Hs Mt Ds Rg Cn Nh Fl Mc Lv Ts Og La Ce Pr Nd Pm Sm Eu Gd Tb Dy Ho Er Tm Yb Lu Ac Th Pa U Np Pu Am Cm Bk Cf Es Fm Md No Lr Cr Mn Fe Co Ni Cu Zn Ga Ge As Se Br Kr Rb Sr Y Zr Nb Mo Tc Ru Rh Pd Ag Cd In Sn Sb Te I Xe Cs Ba Hf Ta W Re Os Ir Pt Au Hg Tl Pb Bi Po At Rn Fr Ra Rf Db Sg Bh Hs Mt Ds Rg Cn Nh Fl Mc Lv Ts Og La Ce Pr Nd Pm Sm Eu Gd Tb Dy Ho Er Tm Yb Lu Ac Th Pa U Np Pu Am Cm Bk Cf Es Fm Md No Lr Cr Mn Fe Co Ni Cu Zn Ga Ge As Se Br Kr Rb Sr Y Zr Nb Mo Tc Ru Rh Pd Ag Cd In Sn Sb Te I Xe Cs Ba Hf Ta W Re Os Ir Pt Au Hg Tl Pb Bi Po At Rn Fr Ra Rf Db Sg Bh Hs Mt Ds Rg Cn Nh Fl Mc Lv Ts Og La Ce Pr Nd Pm Sm Eu Gd Tb Dy Ho Er Tm Yb Lu Ac Th Pa U Np Pu Am Cm Bk Cf Es Fm Md No Lr Re Os Ir Pt Au Hg Tl Pb Bi Po At Rn Fr Ra Rf Db Sg Bh Hs Mt Ds Rg Cn Nh Fl Mc Lv Ts Og La Ce Pr Nd Pm Sm Eu Gd Tb Dy Ho Er Tm Yb Lu Ac Th Pa U Np Pu Am Cm Bk Cf Es Fm Md No Lr
BG-History-GPT-4oBG-RandomHHeHHeCountHHeCount15143194285512Li BeBCNOF NeLi BeBCNOF NeLi BeBCNOF NeNa MgAl SiPSCl Ar1364Na MgAl SiPSCl Ar1644Na MgAl SiPSCl Ar638K Ca Sc TiV 0 10 122K Ca Sc Ti10 V 0 138K Ca Sc TiV0 7 73BG: GPT-4o with history FE-History-GPT-4oFE-RandomBG: RandomHHeHHeCountHHeCount34152879146502Li BeBCNOF NeLi BeBCNOF NeLi BeBCNOF NeNa MgAl SiPSCl Ar2511Na MgAl SiPSCl Ar5104Na MgAl SiPSCl Ar723K Ca Sc TiV 0 12 183K Ca Sc Ti16 V 0 295K Ca Sc Ti Cs Ba Hf Ta W 0 V Cr Mn Fe Co Ni Cu Zn Ga Ge As Se Br Kr 7 Rb Sr Y Zr Nb Mo Tc Ru Rh Pd Ag Cd In Sn Sb Te I Xe 79FE: GPT-4o with historyFE: Random
Count BG-History-Gemini-1.0-pro BG: Gemini-1.0-prowith history Count FE-History-Gemini-1.0-pro FE: Gemini-1.0-prowith history</p>
<p>Table 3 :
3
LLMatDesign with and without self-reflection.GPT-4o is used as the LLM engine.
Average # of ModificationsStarting MaterialHistory Historyless History without reflectionBaV 2 Ni 2 O 817.730.445.1CdCu 2 GeS 43.39.55.0CeAlO 37.416.927.6Co 2 TiO 45.51.67.9ErNi 2 Ge 219.347.631.0Ga 2 O 312.737.713.1Li 2 CaSiO 414.329.331.4LiSiNO4.12.85.1Na 2 ZnGeO 411.549.431.5SrTiO 312.040.636.7Avg.10.826.623.4</p>
<p>Table 4 :
4
LLMatDesign with different prompts.GPT-4o is used as the LLM engine.
Average # of ModificationsStarting MaterialHistory GPT-4o Refined PersonaBaV 2 Ni 2 O 817.713.49.6CdCu 2 GeS 43.33.15.3CeAlO 37.47.28.7Co 2 TiO 45.58.911.9ErNi 2 Ge 219.311.911.7Ga 2 O 312.78.48.3Li 2 CaSiO 414.311.611.9LiSiNO4.15.61.0Na 2 ZnGeO 411.56.98.8SrTiO 312.09.913.9Avg.10.88.699.11</p>
<p>Table 5 :
5
LLMatDesign with different constraints on SrTiO 3 .
Constraint% compliant modificationsDo not use Ba or Ca100Do not modify Sr100Do not have more than 4 distinct elements99.02</p>
<p>Cr Mn Fe Co Ni Cu Zn Ga Ge As Se Br Kr Rb Sr Y Zr Nb Mo Tc Ru Rh Pd Ag Cd In Sn Sb Te I Xe Cs Ba Hf Ta W Re Os Ir Pt Au Hg Tl Pb Bi Po At Rn Fr Ra Rf Db Sg Bh Hs Mt Ds Rg Cn Nh Fl Mc Lv Ts Og La Ce Pr Nd Pm Sm Eu Gd Tb Dy Ho Er Tm Yb Lu Ac Th Pa U Np Pu Am Cm Bk Cf Es Fm Md No Lr Cr Mn Fe Co Ni Cu Zn Ga Ge As Se Br Kr Rb Sr Y Zr Nb Mo Tc Ru Rh Pd Ag Cd In Sn Sb Te I Xe Cs Ba Hf Ta W Re Os Ir Pt Au Hg Tl Pb Bi Po At Rn Fr Ra Rf Db Sg Bh Hs Mt Ds Rg Cn Nh Fl Mc Lv Ts Og La Ce Pr Nd Pm Sm Eu Gd Tb Dy Ho Er Tm Yb Lu Ac Th Pa U Np Pu Am Cm Bk Cf Es Fm Md No Lr Cr Mn Fe Co Ni Cu Zn Ga Ge As Se Br Kr Rb Sr Y Zr Nb Mo Tc Ru Rh Pd Ag Cd In Sn Sb Te I Xe Cs Ba Hf Ta W Re Os Ir Pt Au Hg Tl Pb Bi Po At Rn Fr Ra Rf Db Sg Bh Hs Mt Ds Rg Cn Nh Fl Mc Lv Ts Og La Ce Pr Nd Pm Sm Eu Gd Tb Dy Ho Er Tm Yb Lu Ac Th Pa U Np Pu Am Cm Bk Cf Es Fm Md No Lr Cr Mn Fe Co Ni Cu Zn Ga Ge As Se Br Kr Rb Sr Y Zr Nb Mo Tc Ru Rh Pd Ag Cd In Sn Sb Te I Xe Cs Ba Hf Ta W Re Os Ir Pt Au Hg Tl Pb Bi Po At Rn Fr Ra Rf Db Sg Bh Hs Mt Ds Rg Cn Nh Fl Mc Lv Ts Og La Ce Pr Nd Pm Sm Eu Gd Tb Dy Ho Er Tm Yb Lu Ac Th Pa U Np Pu Am Cm Bk Cf Es Fm Md No Lr
BG-Historyless-Gemini-1.0-proBG-Historyless-GPT-4oHHeCountHHeCount1566645237Li BeBCNOF NeLi BeBCNOF NeNa MgAl SiPSCl Ar1399Na MgAl SiPSCl Ar3100K Ca Sc TiV 0 10 124K Ca Sc Ti13 V 0 211FE-Historyless-Gemini-1.0-proBG: GPT-4o without historyHHeCountHHe15040Li BeBCNOF NeLi BeBCNOF NeNa MgAl SiPSCl Ar1357Na MgAl SiPSCl ArK Ca Sc Ti10 121 V 0K Ca Sc TiV
BG: Gemini-1.0-prowithout history FE: Gemini-1.0-prowithout history</p>
<p>D Visualization of Selected StructuresSrTiO 3 Ba 2 Tl 2 PNO 6 1.51 eV BaV 2 Ni 2 O 8 BaCd 2 (MoO 5 ) 2 1.30 eV Co 2 TiO 4 Co 2 SO 4 1.42 eV ErNi 2 Ge 2 ErGa 2 S 2 O 1.39 eV CdSiSe 4 1.42 eV Na 2 ZnGeO 4 Na 4 MnFe 2 (GeO 4 ) 2 1.41 eV
CeAlO 3Li 2 CaSiO 4FeGeO 3LiSnO 21.49 eV1.29 eVCdCu2GeS4 Zn 2 LiSiNO LiGePSGa 2 O 3 Ga 4 SnO 61.30 eV1.33 eV</p>
<p>Table E.1: DFT results for lowest-energy structures obtained from the formation energy task, averaged across all starting materials and runs.
GPT-4o with history RandomFormation energy per atom (eV/atom)−2.31−1.51Job success rate (%)73.340.0
AcknowledgementsWe thank Lingkai Kong and Rui Feng for helpful discussions.This research used resources of the National Energy Research Scientific Computing Center (NERSC), a U.S. Department of Energy Office of Science User Facility located at Lawrence Berkeley National Laboratory, operated under Contract No. DE-AC02-05CH11231 using NERSC award BES-ERCAP0022842.Supplementary InformationA Prompt Templates for LLMatDesignA slightly different prompt template to Fig.2is designed for Gemini-1.0-produe to its inconsistency in generating standardized output.LLMatDesign Prompt Template (Gemini-1.0-pro)I have a material and its <property>.<definition of property>.(<chemical composition>, <property value>)You will be given a starting material to be modified.Try to achieve <objective>.Make an informed choice of modification based on the given material and past modifications and property values obtained after those modifications.Output a list for the suggested modification, and a string of the reason why you think it is a good modification to take to achieve <objective>.Make sure the modification is physically meaningful.Material to be modified: <chemical composition> Current property value: <property value> <modification history> Available modifications:1. exchange: exchange two elements in the material 2. substitute: substitute one element in the material with another 3. remove: remove an element from the material 4. add: add an element to the material Example output format:1. ["exchange", "O", "N"], "some reason here" 2. ["substitute", "Ti", "Fe"], "some reason here" 3. ["add", "O"], "some reason here" 4. ["remove", "O"], "some reason here" GPT-4o Refined Prompt Template for LLMatDesign I have a material with a known <property>.<definition of property>.Material information:• Chemical formula: <chemical composition> • <property>: <property value> Objective: Propose a modification to this material to achieve <objective>.You can choose one of the following modification types:1. exchange: exchange two elements in the material 2. substitute: substitute one element in the material with another 3. remove: remove an element from the material 4. add: add an element to the material Your response should be a Python dictionary in the following format:Requirements:1. $HYPOTHESIS: Provide a detailed analysis and rationale for your proposed modification.2. $TYPE:Specify the type of modification ("exchange", "substitute", "remove", "add").3. $Identify the element(s) involved in the modification.For "exchange" and "substitute", include two elements ($ELE-MENT 1 and $ELEMENT 2).For "remove" and "add", include one element ($ELEMENT 1).<modification history>Take a deep breath and work on this problem step-by-step.Your thoughtful and detailed analysis is highly appreciated.Persona Prompt Template for LLMatDesignYou are a materials design expert working on the development of new materials with specific properties.You will be given a composition (chemical formula) and its corresponding <property>.You will be asked to propose a modification to the material to achieve a target <property>.Material information:• Chemical formula: <chemical composition> • <property>: <property value>Objective:Propose a modification to this material to achieve <objective>.You can choose one of the following modification types:1. exchange: exchange two elements in the material 2. substitute: substitute one element in the material with another 3. remove: remove an element from the material 4. add: add an element to the material Your response should be a Python dictionary in the following format:''' {Hypothesis: $HYPOTHESIS, Modification: [$TYPE, $ELEMENT 1, $ELEMENT 2]}.'''Requirements:1. $HYPOTHESIS: Provide a detailed analysis and rationale for your proposed modification.2. $TYPE:Specify the type of modification ("exchange", "substitute", "remove", "add").3. $Identify the element(s) involved in the modification.For "exchange" and "substitute", include two elements ($ELE-MENT 1 and $ELEMENT 2).For "remove" and "add", include one element ($ELEMENT 1).<modification history>Take a deep breath and work on this problem step-by-step.Your thoughtful and detailed analysis is highly appreciated.
Computational screening of all stoichiometric inorganic materials. D W Davies, 20161</p>
<p>Structure prediction drives materials discovery. A R Oganov, C J Pickard, Q Zhu, R J Needs, Nature Reviews Materials. 42019</p>
<p>Materials discovery and design using machine learning. Y Liu, T Zhao, W Ju, S Shi, Journal of Materiomics. 32017</p>
<p>From the computer to the laboratory: materials discovery and design using first-principles calculations. G Hautier, A Jain, S P Ong, Journal of Materials Science. 472012</p>
<p>What is high-throughput virtual screening? a perspective from organic materials discovery. E O Pyzer-Knapp, C Suh, R Gómez-Bombarelli, J Aguilera-Iparraguirre, A Aspuru-Guzik, Annual Review of Materials Research. 452015</p>
<p>A universal graph deep learning interatomic potential for the periodic table. C Chen, S P Ong, Nature Computational Science. 22022</p>
<p>Scaling deep learning for materials discovery. A Merchant, Nature. 6242023</p>
<p>Data-driven approach to encoding and decoding 3-d crystal structures. J Hoffmann, arXiv:1909.009492019arXiv preprint</p>
<p>3-d inorganic crystal structure generation and property prediction via representation learning. C J Court, B Yildirim, A Jain, J M Cole, Journal of Chemical Information and Modeling. 602020</p>
<p>Crystal diffusion variational autoencoder for periodic material generation. T Xie, X Fu, O.-E Ganea, R Barzilay, T Jaakkola, arXiv:2110.061972021arXiv preprint</p>
<p>Constrained crystals deep convolutional generative adversarial network for the inverse design of crystal structures. T Long, Computational Materials. 7662021</p>
<p>An invertible crystallographic representation for general inverse design of inorganic crystals with targeted properties. Z Ren, Matter. 52022</p>
<p>Atomic structure generation from reconstructing structural fingerprints. V Fung, Machine Learning: Science and Technology. 3450182022</p>
<p>Mattergen: a generative model for inorganic materials design. C Zeni, arXiv:2312.036872023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, Advances in neural information processing systems. 352022</p>
<p>J Huang, K C Chang, .-C, arXiv:2212.10403Towards reasoning in large language models: A survey. 2022arXiv preprint</p>
<p>Pre-trained language models for interactive decision-making. S Li, Advances in Neural Information Processing Systems. 202235</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. M Ahn, arXiv:2204.016912022arXiv preprint</p>
<p>Voxposer: Composable 3d value maps for robotic manipulation with language models. W Huang, arXiv:2307.059732023arXiv preprint</p>
<p>Autonomous chemical research with large language models. D A Boiko, R Macknight, B Kline, G Gomes, Nature. 6242023</p>
<p>A M Bran, arXiv:2304.05376Augmenting large-language models with chemistry tools. 2023arXiv preprint</p>
<p>The impact of large language models on scientific discovery: a preliminary study using gpt-4. M R Ai4science, M A Quantum, arXiv:2311.073612023arXiv preprint</p>
<p>Are large language models superhuman chemists?. A Mirza, arXiv:2404.014752024arXiv preprint</p>
<p>Openchemie: An information extraction toolkit for chemistry literature. V Fan, arXiv:2404.014622024arXiv preprint</p>
<p>Extracting structured data from organic synthesis procedures using a fine-tuned large language model. Q Ai, F Meng, J Shi, B Pelkie, C W Coley, 10.26434/chemrxiv-2024-979fz2024ChemRxiv preprint</p>
<p>Benchmarking large language models for molecule prediction tasks. Z Zhong, K Zhou, D Mottin, arXiv:2403.050752024arXiv preprint</p>
<p>Fine-tuning gpt-3 for machine learning electronic and functional properties of organic molecules. Z Xie, Chemical science. 152024</p>
<p>Leveraging large language models for predictive chemistry. K M Jablonka, P Schwaller, A Ortega-Guerrero, B Smit, Nature Machine Intelligence. 2024</p>
<p>Catalyst energy prediction with catberta: Unveiling feature exploration strategies through large language models. J Ock, C Guntuboina, A Barati Farimani, ACS Catalysis. 132023</p>
<p>Language models can generate molecules, materials, and protein binding sites directly in three dimensions as xyz, cif, and pdb files. D Flam-Shepherd, A Aspuru-Guzik, arXiv:2305.057082023arXiv preprint</p>
<p>Crystal structure generation with autoregressive large language modeling. L M Antunes, K T Butler, R Grau-Crespo, arXiv:2307.043402023arXiv preprint</p>
<p>Fine-tuned language models generate stable inorganic materials as text. N Gruver, arXiv:2402.043792024arXiv preprint</p>
<p>Commentary: The materials project: A materials genome approach to accelerating materials innovation. A Jain, APL materials. 12013</p>
<p>. J Achiam, arXiv:2303.087742023Gpt-4 technical report. arXiv preprint</p>
<p>Gemini: a family of highly capable multimodal models. G Team, arXiv:2312.118052023arXiv preprint</p>
<p>Solar materials find their band gap. B R Sutherland, Joule. 42020</p>
<p>Advances in neural information processing systems. A Vaswani, 201730Attention is all you need</p>
<p>Recent advances and outstanding challenges for machine learning interatomic potentials. T W Ko, S P Ong, Nature Computational Science. 32023</p>
<p>Torchmd-net: Equivariant transformers for neural network based molecular potentials. P Thölke, G De Fabritiis, arXiv:2202.025412022arXiv preprint</p>
<p>Benchmarking graph neural networks for materials chemistry. V Fung, J Zhang, E Juarez, B G Sumpter, Computational Materials. 7842021</p>
<p>Derivative-based pre-training of graph neural networks for materials property predictions. S Jia, Digital Discovery. 32024</p>
<p>Benchmarking materials property prediction methods: the matbench test set and automatminer reference algorithm. A Dunn, Q Wang, A Ganose, D Dopp, A Jain, Computational Materials. 61382020</p>
<p>Efficient iterative schemes for ab initio total-energy calculations using a plane-wave basis set. G Kresse, J Furthmüller, Physical review B. 54111691996</p>
<p>Efficiency of ab-initio total energy calculations for metals and semiconductors using a plane-wave basis set. G Kresse, J Furthmüller, Computational materials science. 61996</p>
<p>Python materials genomics (pymatgen): A robust, open-source python library for materials analysis. S P Ong, Computational Materials Science. 682013</p>            </div>
        </div>

    </div>
</body>
</html>
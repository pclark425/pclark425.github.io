<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5637 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5637</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5637</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-268998578</p>
                <p><strong>Paper Title:</strong> <a href="https://medinform.jmir.org/2024/1/e55318/PDF" target="_blank">An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study</a></p>
                <p><strong>Paper Abstract:</strong> Background: Large language models (LLMs) have shown remarkable capabilities in natural language processing (NLP), especially in domains where labeled data are scarce or expensive, such as the clinical domain. However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data. This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches. Objective: The objective of this study is to assess the effectiveness of various prompt engineering techniques, including 2 newly introduced typesâ€”heuristic and ensemble prompts, for zero-shot and few-shot clinical information extraction using pretrained language models. Methods: This comprehensive experimental study evaluated different prompt types (simple prefix, simple cloze, chain of thought, anticipatory, heuristic, and ensemble) across 5 clinical NLP tasks: clinical sense disambiguation, biomedical evidence extraction, coreference resolution, medication status extraction, and medication attribute extraction. The performance of these prompts was assessed using 3 state-of-the-art language models: GPT-3.5 (OpenAI), Gemini (Google), and LLaMA-2 (Meta). The study contrasted zero-shot with few-shot prompting and explored the effectiveness of ensemble approaches. Results: The study revealed that task-specific prompt</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5637.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5637.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Heuristic prompt (clinical sense disambiguation, GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Heuristic prompt for clinical sense disambiguation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A rule-based prompt design that applies predefined heuristic transformations/rules to guide the LLM to expand/disambiguate clinical abbreviations; used here in zero-shot evaluation for clinical abbreviation sense disambiguation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Clinical sense disambiguation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given clinical context, choose the correct sense/expansion of an abbreviation (text classification).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot heuristic prompt: prompts incorporate rule-based instructions and decomposition (explicit heuristic rules about capitalization, surrounding tokens, likely senses) without in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against other prompt types (simple prefix, simple cloze, anticipatory, chain-of-thought, ensemble) and baseline models (BERT, ELMO).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>accuracy: 0.96</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Best alternative prompt (simple prefix): accuracy 0.88; BERT baseline: accuracy 0.42; ELMO baseline: accuracy 0.55.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+0.08 accuracy vs best non-heuristic prompt (0.96 vs 0.88); +0.54 vs BERT baseline (0.96 vs 0.42).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize that heuristic prompts encode domain-specific rules/constraints that directly map to the task (clinical abbreviation patterns), giving the LLM precise guidance and reducing ambiguity compared with generic prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5637.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5637.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Heuristic prompt (biomedical evidence extraction, GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Heuristic prompt for biomedical evidence extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Rule-based prompt framing that decomposes the extraction task via rules to identify interventions/evidence in biomedical abstracts; evaluated zero-shot on EBM-NLP.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Biomedical evidence extraction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Named-entity-style extraction of interventions/evidence from biomedical abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot heuristic prompt: rule-driven instructions to extract entities (no in-context examples).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared with chain-of-thought, simple prefix, simple cloze, anticipatory, ensemble, and to PubMedBERT-CRF baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>accuracy: 0.94</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Chain-of-thought also achieved 0.94; simple prefix 0.92; PubMedBERT-CRF baseline: 0.35. Few-shot (2-shot) reported accuracy: 0.96 (slight improvement for this task).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+0.59 accuracy vs PubMedBERT-CRF (0.94 vs 0.35); +0.02 vs zero-shot heuristic when using few-shot (0.96 vs 0.94).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (heuristic and chain-of-thought strong; few-shot provided small additional gain)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Heuristic and chain-of-thought prompts provide constraints and structured guidance enabling the model to focus on extraction patterns; for this task few-shot examples further guide extraction decisions in complex contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5637.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5637.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-thought prompt (coreference resolution, GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-thought prompting for coreference resolution</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting style that asks the model to generate intermediate reasoning steps (natural-language chain-of-thought) before the final answer, used to guide resolution of antecedents (who 'the patient' refers to).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Coreference resolution</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Identify antecedents referring to the same entity (resolve references to 'the patient' or other entities).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot chain-of-thought: instruct model to produce stepwise reasoning leading to final coreference label (no in-context examples).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against heuristic, simple prefix/cloze, anticipatory, ensemble, and published methods (Toshniwal et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>accuracy: 0.94</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Heuristic also reported 0.94 for GPT-3.5; best non-LLM baseline (Toshniwal et al.) reported 0.69.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+0.25 accuracy vs Toshniwal et al. (0.94 vs 0.69).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Chain-of-thought supplies intermediate reasoning structure that helps the LLM apply logical constraints to resolve references, improving performance on resolution tasks requiring multi-step inference.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5637.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5637.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Simple prefix prompt (medication status, GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simple prefix prompt for medication status extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A short control phrase prepended to the input (e.g., 'Identify medication status:') that guides output format and relevance, evaluated zero-shot for medication status extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Medication status extraction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Identify whether a medication mention corresponds to current, past, or other status relative to the patient (NER + classification).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot simple prefix: a concise instruction/prefix indicating task and desired output format, no examples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against heuristic, chain-of-thought, simple cloze, anticipatory, ensemble, and ScispaCy baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>accuracy: 0.76</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Heuristic: 0.74; simple cloze: 0.72; ScispaCy baseline: 0.52.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+0.24 accuracy vs ScispaCy baseline (0.76 vs 0.52).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Simple prefix provides clear instruction and expected output format which suffices for this extraction+classification task; concise control signals reduce model output variance compared with unguided prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5637.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5637.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Heuristic prompt (medication attribute extraction, GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Heuristic prompt for medication attribute extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Rule-based prompt guiding the model to extract medication attributes and relations (NER + relation extraction) from clinical text using explicit heuristic rules in zero-shot mode.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Medication attribute extraction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Extract medication attributes (dose, frequency, route, etc.) and relations from clinical notes.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot heuristic prompt: structured, rule-driven instructions describing attribute types and patterns to extract.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared with chain-of-thought, simple prefix, simple cloze, anticipatory, ensemble, and ScispaCy baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>accuracy: 0.96</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Chain-of-thought also reported 0.96; ScispaCy baseline: 0.70; few-shot reported also 0.96 (zero-shot heuristic at top).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+0.26 vs ScispaCy baseline (0.96 vs 0.70); no gain from few-shot over zero-shot for this prompt/task.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (heuristic highly effective; few-shot provided no additional benefit)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Heuristic prompts encode domain constraints and expected attribute structures, which are particularly effective for structured extraction tasks; few-shot did not further help when heuristics already constrained outputs sufficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Few-shot did not improve (or was not better) than zero-shot heuristic for this task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5637.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5637.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ensemble prompts (majority voting)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ensemble prompt combining multiple prompt types via majority voting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that runs multiple prompt types on the same input and selects the final answer by majority vote; intended to combine strengths and reduce weaknesses of individual prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple clinical NLP tasks (example: clinical sense disambiguation, medication attribute extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Applies to classification and extraction tasks by aggregating outputs from different prompt types.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot ensemble: generate outputs from multiple prompt types (authors used 5 types) and select mode(output) as final answer; no in-context examples unless a few-shot ensemble variant used.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to single best prompt types per task (heuristic, chain-of-thought, prefix).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>clinical sense disambiguation accuracy: 0.90 (GPT-3.5); medication attribute extraction accuracy: 0.90 (GPT-3.5); other tasks varied.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>For clinical sense disambiguation, best single prompt (heuristic) was 0.96; ensemble 0.90 (worse). For medication attribute extraction, best single prompt was 0.96; ensemble 0.90 (slightly worse).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>-0.06 accuracy vs best single prompt in clinical sense disambiguation (0.90 vs 0.96).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced (ensemble improved over weak prompts but did not beat best single prompt; introduced inconsistency/noise for tasks requiring precise outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Majority voting leverages prompt diversity but can introduce ambiguity and inconsistency when prompts produce heterogeneous outputs; ensemble helps avoid very poor prompts but may dilute strengths of a well-tailored single prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Ensemble was never the top-performing strategy on any task in authors' experiments; it sometimes reduced precision for tasks needing high consistency (e.g., coreference resolution).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5637.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5637.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot prompting (general effect)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot prompting (2-shot in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Providing a small number (two) of in-context examples (2-shot) alongside the instruction; authors compared few-shot vs zero-shot across tasks and models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple clinical NLP tasks (biomedical evidence extraction example)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks include classification, extraction, and resolution where few-shot examples supply exemplar input-output pairs to guide output formatting and disambiguation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot (2 examples) added to prompt context plus the task instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Zero-shot variants of same prompt types (heuristic, chain-of-thought, prefix, cloze, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example - biomedical evidence extraction (GPT-3.5) few-shot accuracy: 0.96</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Zero-shot heuristic/chain-of-thought accuracy: 0.94 for same task; few-shot improved accuracy by 0.02 for this task.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+0.02 accuracy for biomedical evidence extraction (0.96 vs 0.94); authors reported few-shot improved most task+model combinations except some (see counterexample).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (generally)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Few-shot exemplars provide concrete mapping from input to desired output, reducing ambiguity in complex scenarios and helping the LLM calibrate output formatting and label mapping; more pronounced effect for tasks needing deeper contextual mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Clinical sense disambiguation and medication attribute extraction: some zero-shot prompt types (heuristic, chain-of-thought) outperformed few-shot; i.e., few-shot did not always help when zero-shot prompt already provided precise rules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5637.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5637.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Persona pattern prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Persona-based prompting (instructing the LLM to act as a domain persona)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Asking the model to adopt a task-relevant persona (e.g., 'act as a clinical NLP expert') to bias outputs toward domain-appropriate style and constraints; authors tested and observed accuracy improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Clinical sense disambiguation (example)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Applying a persona instruction to focus the model on domain-relevant interpretation and consistent outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot persona pattern: prepend instruction to adopt a domain persona plus the task instruction (no examples).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard prompts without persona instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>No numeric aggregate provided; authors reported persona patterns can improve accuracy and quality of outputs across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (reported qualitatively)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Persona constrains the model's decision framing and preferred lexicon, reducing generation of irrelevant or contradictory content and guiding it toward domain-relevant reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5637.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5637.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt specificity & randomness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt specificity to control randomness in LLM outputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observation that LLM outputs vary due to inherent randomness; specific, constrained prompts reduce harmful variability for clinical extraction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 / Gemini / LLaMA-2 (generalized observation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple clinical NLP tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where consistent factual extraction is required (e.g., biomedical evidence extraction, medication status extraction).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Design principle rather than a single format: increase prompt specificity (explicit instructions, output format constraints, rule-based elements) to mitigate stochastic output variations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Less specific/generic prompts (higher randomness) vs more specific prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: authors state randomness can introduce noise and errors; specific prompts produce more reliable outputs. No unified numeric metric provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (specific prompts reduce harmful randomness)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>When prompts are underspecified the LLM has freedom to produce varied formats and content; adding explicit formatting/constraint reduces output space and aligns responses to the resolver used for accuracy computation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Rethinking the role of demonstrations: what makes in-context learning work? <em>(Rating: 2)</em></li>
                <li>A prompt pattern catalog to enhance prompt engineering with chatGPT <em>(Rating: 2)</em></li>
                <li>Improving large language models for clinical named entity recognition via prompt engineering <em>(Rating: 2)</em></li>
                <li>Large language models are few-shot clinical information extractors <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5637",
    "paper_id": "paper-268998578",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "Heuristic prompt (clinical sense disambiguation, GPT-3.5)",
            "name_full": "Heuristic prompt for clinical sense disambiguation",
            "brief_description": "A rule-based prompt design that applies predefined heuristic transformations/rules to guide the LLM to expand/disambiguate clinical abbreviations; used here in zero-shot evaluation for clinical abbreviation sense disambiguation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_size": null,
            "task_name": "Clinical sense disambiguation",
            "task_description": "Given clinical context, choose the correct sense/expansion of an abbreviation (text classification).",
            "problem_format": "Zero-shot heuristic prompt: prompts incorporate rule-based instructions and decomposition (explicit heuristic rules about capitalization, surrounding tokens, likely senses) without in-context examples.",
            "comparison_format": "Compared against other prompt types (simple prefix, simple cloze, anticipatory, chain-of-thought, ensemble) and baseline models (BERT, ELMO).",
            "performance": "accuracy: 0.96",
            "performance_comparison": "Best alternative prompt (simple prefix): accuracy 0.88; BERT baseline: accuracy 0.42; ELMO baseline: accuracy 0.55.",
            "format_effect_size": "+0.08 accuracy vs best non-heuristic prompt (0.96 vs 0.88); +0.54 vs BERT baseline (0.96 vs 0.42).",
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Authors hypothesize that heuristic prompts encode domain-specific rules/constraints that directly map to the task (clinical abbreviation patterns), giving the LLM precise guidance and reducing ambiguity compared with generic prompts.",
            "counterexample_or_null_result": null,
            "uuid": "e5637.0",
            "source_info": {
                "paper_title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Heuristic prompt (biomedical evidence extraction, GPT-3.5)",
            "name_full": "Heuristic prompt for biomedical evidence extraction",
            "brief_description": "Rule-based prompt framing that decomposes the extraction task via rules to identify interventions/evidence in biomedical abstracts; evaluated zero-shot on EBM-NLP.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_size": null,
            "task_name": "Biomedical evidence extraction",
            "task_description": "Named-entity-style extraction of interventions/evidence from biomedical abstracts.",
            "problem_format": "Zero-shot heuristic prompt: rule-driven instructions to extract entities (no in-context examples).",
            "comparison_format": "Compared with chain-of-thought, simple prefix, simple cloze, anticipatory, ensemble, and to PubMedBERT-CRF baseline.",
            "performance": "accuracy: 0.94",
            "performance_comparison": "Chain-of-thought also achieved 0.94; simple prefix 0.92; PubMedBERT-CRF baseline: 0.35. Few-shot (2-shot) reported accuracy: 0.96 (slight improvement for this task).",
            "format_effect_size": "+0.59 accuracy vs PubMedBERT-CRF (0.94 vs 0.35); +0.02 vs zero-shot heuristic when using few-shot (0.96 vs 0.94).",
            "format_effect_direction": "improved (heuristic and chain-of-thought strong; few-shot provided small additional gain)",
            "explanation_or_hypothesis": "Heuristic and chain-of-thought prompts provide constraints and structured guidance enabling the model to focus on extraction patterns; for this task few-shot examples further guide extraction decisions in complex contexts.",
            "counterexample_or_null_result": null,
            "uuid": "e5637.1",
            "source_info": {
                "paper_title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Chain-of-thought prompt (coreference resolution, GPT-3.5)",
            "name_full": "Chain-of-thought prompting for coreference resolution",
            "brief_description": "A prompting style that asks the model to generate intermediate reasoning steps (natural-language chain-of-thought) before the final answer, used to guide resolution of antecedents (who 'the patient' refers to).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_size": null,
            "task_name": "Coreference resolution",
            "task_description": "Identify antecedents referring to the same entity (resolve references to 'the patient' or other entities).",
            "problem_format": "Zero-shot chain-of-thought: instruct model to produce stepwise reasoning leading to final coreference label (no in-context examples).",
            "comparison_format": "Compared against heuristic, simple prefix/cloze, anticipatory, ensemble, and published methods (Toshniwal et al.).",
            "performance": "accuracy: 0.94",
            "performance_comparison": "Heuristic also reported 0.94 for GPT-3.5; best non-LLM baseline (Toshniwal et al.) reported 0.69.",
            "format_effect_size": "+0.25 accuracy vs Toshniwal et al. (0.94 vs 0.69).",
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Chain-of-thought supplies intermediate reasoning structure that helps the LLM apply logical constraints to resolve references, improving performance on resolution tasks requiring multi-step inference.",
            "counterexample_or_null_result": null,
            "uuid": "e5637.2",
            "source_info": {
                "paper_title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Simple prefix prompt (medication status, GPT-3.5)",
            "name_full": "Simple prefix prompt for medication status extraction",
            "brief_description": "A short control phrase prepended to the input (e.g., 'Identify medication status:') that guides output format and relevance, evaluated zero-shot for medication status extraction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_size": null,
            "task_name": "Medication status extraction",
            "task_description": "Identify whether a medication mention corresponds to current, past, or other status relative to the patient (NER + classification).",
            "problem_format": "Zero-shot simple prefix: a concise instruction/prefix indicating task and desired output format, no examples.",
            "comparison_format": "Compared against heuristic, chain-of-thought, simple cloze, anticipatory, ensemble, and ScispaCy baseline.",
            "performance": "accuracy: 0.76",
            "performance_comparison": "Heuristic: 0.74; simple cloze: 0.72; ScispaCy baseline: 0.52.",
            "format_effect_size": "+0.24 accuracy vs ScispaCy baseline (0.76 vs 0.52).",
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Simple prefix provides clear instruction and expected output format which suffices for this extraction+classification task; concise control signals reduce model output variance compared with unguided prompts.",
            "counterexample_or_null_result": null,
            "uuid": "e5637.3",
            "source_info": {
                "paper_title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Heuristic prompt (medication attribute extraction, GPT-3.5)",
            "name_full": "Heuristic prompt for medication attribute extraction",
            "brief_description": "Rule-based prompt guiding the model to extract medication attributes and relations (NER + relation extraction) from clinical text using explicit heuristic rules in zero-shot mode.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_size": null,
            "task_name": "Medication attribute extraction",
            "task_description": "Extract medication attributes (dose, frequency, route, etc.) and relations from clinical notes.",
            "problem_format": "Zero-shot heuristic prompt: structured, rule-driven instructions describing attribute types and patterns to extract.",
            "comparison_format": "Compared with chain-of-thought, simple prefix, simple cloze, anticipatory, ensemble, and ScispaCy baseline.",
            "performance": "accuracy: 0.96",
            "performance_comparison": "Chain-of-thought also reported 0.96; ScispaCy baseline: 0.70; few-shot reported also 0.96 (zero-shot heuristic at top).",
            "format_effect_size": "+0.26 vs ScispaCy baseline (0.96 vs 0.70); no gain from few-shot over zero-shot for this prompt/task.",
            "format_effect_direction": "improved (heuristic highly effective; few-shot provided no additional benefit)",
            "explanation_or_hypothesis": "Heuristic prompts encode domain constraints and expected attribute structures, which are particularly effective for structured extraction tasks; few-shot did not further help when heuristics already constrained outputs sufficiently.",
            "counterexample_or_null_result": "Few-shot did not improve (or was not better) than zero-shot heuristic for this task.",
            "uuid": "e5637.4",
            "source_info": {
                "paper_title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Ensemble prompts (majority voting)",
            "name_full": "Ensemble prompt combining multiple prompt types via majority voting",
            "brief_description": "An approach that runs multiple prompt types on the same input and selects the final answer by majority vote; intended to combine strengths and reduce weaknesses of individual prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_size": null,
            "task_name": "Multiple clinical NLP tasks (example: clinical sense disambiguation, medication attribute extraction)",
            "task_description": "Applies to classification and extraction tasks by aggregating outputs from different prompt types.",
            "problem_format": "Zero-shot ensemble: generate outputs from multiple prompt types (authors used 5 types) and select mode(output) as final answer; no in-context examples unless a few-shot ensemble variant used.",
            "comparison_format": "Compared to single best prompt types per task (heuristic, chain-of-thought, prefix).",
            "performance": "clinical sense disambiguation accuracy: 0.90 (GPT-3.5); medication attribute extraction accuracy: 0.90 (GPT-3.5); other tasks varied.",
            "performance_comparison": "For clinical sense disambiguation, best single prompt (heuristic) was 0.96; ensemble 0.90 (worse). For medication attribute extraction, best single prompt was 0.96; ensemble 0.90 (slightly worse).",
            "format_effect_size": "-0.06 accuracy vs best single prompt in clinical sense disambiguation (0.90 vs 0.96).",
            "format_effect_direction": "reduced (ensemble improved over weak prompts but did not beat best single prompt; introduced inconsistency/noise for tasks requiring precise outputs)",
            "explanation_or_hypothesis": "Majority voting leverages prompt diversity but can introduce ambiguity and inconsistency when prompts produce heterogeneous outputs; ensemble helps avoid very poor prompts but may dilute strengths of a well-tailored single prompt.",
            "counterexample_or_null_result": "Ensemble was never the top-performing strategy on any task in authors' experiments; it sometimes reduced precision for tasks needing high consistency (e.g., coreference resolution).",
            "uuid": "e5637.5",
            "source_info": {
                "paper_title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Few-shot prompting (general effect)",
            "name_full": "Few-shot prompting (2-shot in experiments)",
            "brief_description": "Providing a small number (two) of in-context examples (2-shot) alongside the instruction; authors compared few-shot vs zero-shot across tasks and models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_size": null,
            "task_name": "Multiple clinical NLP tasks (biomedical evidence extraction example)",
            "task_description": "Tasks include classification, extraction, and resolution where few-shot examples supply exemplar input-output pairs to guide output formatting and disambiguation.",
            "problem_format": "Few-shot (2 examples) added to prompt context plus the task instruction.",
            "comparison_format": "Zero-shot variants of same prompt types (heuristic, chain-of-thought, prefix, cloze, etc.).",
            "performance": "Example - biomedical evidence extraction (GPT-3.5) few-shot accuracy: 0.96",
            "performance_comparison": "Zero-shot heuristic/chain-of-thought accuracy: 0.94 for same task; few-shot improved accuracy by 0.02 for this task.",
            "format_effect_size": "+0.02 accuracy for biomedical evidence extraction (0.96 vs 0.94); authors reported few-shot improved most task+model combinations except some (see counterexample).",
            "format_effect_direction": "improved (generally)",
            "explanation_or_hypothesis": "Few-shot exemplars provide concrete mapping from input to desired output, reducing ambiguity in complex scenarios and helping the LLM calibrate output formatting and label mapping; more pronounced effect for tasks needing deeper contextual mapping.",
            "counterexample_or_null_result": "Clinical sense disambiguation and medication attribute extraction: some zero-shot prompt types (heuristic, chain-of-thought) outperformed few-shot; i.e., few-shot did not always help when zero-shot prompt already provided precise rules.",
            "uuid": "e5637.6",
            "source_info": {
                "paper_title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Persona pattern prompting",
            "name_full": "Persona-based prompting (instructing the LLM to act as a domain persona)",
            "brief_description": "Asking the model to adopt a task-relevant persona (e.g., 'act as a clinical NLP expert') to bias outputs toward domain-appropriate style and constraints; authors tested and observed accuracy improvements.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_size": null,
            "task_name": "Clinical sense disambiguation (example)",
            "task_description": "Applying a persona instruction to focus the model on domain-relevant interpretation and consistent outputs.",
            "problem_format": "Zero-shot persona pattern: prepend instruction to adopt a domain persona plus the task instruction (no examples).",
            "comparison_format": "Standard prompts without persona instruction.",
            "performance": "No numeric aggregate provided; authors reported persona patterns can improve accuracy and quality of outputs across tasks.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved (reported qualitatively)",
            "explanation_or_hypothesis": "Persona constrains the model's decision framing and preferred lexicon, reducing generation of irrelevant or contradictory content and guiding it toward domain-relevant reasoning.",
            "counterexample_or_null_result": null,
            "uuid": "e5637.7",
            "source_info": {
                "paper_title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Prompt specificity & randomness",
            "name_full": "Prompt specificity to control randomness in LLM outputs",
            "brief_description": "Observation that LLM outputs vary due to inherent randomness; specific, constrained prompts reduce harmful variability for clinical extraction tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 / Gemini / LLaMA-2 (generalized observation)",
            "model_size": null,
            "task_name": "Multiple clinical NLP tasks",
            "task_description": "Tasks where consistent factual extraction is required (e.g., biomedical evidence extraction, medication status extraction).",
            "problem_format": "Design principle rather than a single format: increase prompt specificity (explicit instructions, output format constraints, rule-based elements) to mitigate stochastic output variations.",
            "comparison_format": "Less specific/generic prompts (higher randomness) vs more specific prompts.",
            "performance": "Qualitative: authors state randomness can introduce noise and errors; specific prompts produce more reliable outputs. No unified numeric metric provided.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved (specific prompts reduce harmful randomness)",
            "explanation_or_hypothesis": "When prompts are underspecified the LLM has freedom to produce varied formats and content; adding explicit formatting/constraint reduces output space and aligns responses to the resolver used for accuracy computation.",
            "counterexample_or_null_result": null,
            "uuid": "e5637.8",
            "source_info": {
                "paper_title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Rethinking the role of demonstrations: what makes in-context learning work?",
            "rating": 2,
            "sanitized_title": "rethinking_the_role_of_demonstrations_what_makes_incontext_learning_work"
        },
        {
            "paper_title": "A prompt pattern catalog to enhance prompt engineering with chatGPT",
            "rating": 2,
            "sanitized_title": "a_prompt_pattern_catalog_to_enhance_prompt_engineering_with_chatgpt"
        },
        {
            "paper_title": "Improving large language models for clinical named entity recognition via prompt engineering",
            "rating": 2,
            "sanitized_title": "improving_large_language_models_for_clinical_named_entity_recognition_via_prompt_engineering"
        },
        {
            "paper_title": "Large language models are few-shot clinical information extractors",
            "rating": 2,
            "sanitized_title": "large_language_models_are_fewshot_clinical_information_extractors"
        }
    ],
    "cost": 0.01449825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study</p>
<p>Sonish Sivarajkumar 
Intelligent Systems Program
University of Pittsburgh
PittsburghPAUnited States</p>
<p>BSMark Kelley 
Department of Health Information Management
University of Pittsburgh
PittsburghPAUnited States</p>
<p>MSAlyssa Samolyk-Mazzanti 
Department of Health Information Management
University of Pittsburgh
PittsburghPAUnited States</p>
<p>MSShyam Visweswaran 
Intelligent Systems Program
University of Pittsburgh
PittsburghPAUnited States</p>
<p>Department of Biomedical Informatics
University of Pittsburgh
PittsburghPAUnited States</p>
<p>PhDYanshan Wang yanshan.wang@pitt.edu 
Intelligent Systems Program
University of Pittsburgh
PittsburghPAUnited States</p>
<p>Department of Health Information Management
University of Pittsburgh
PittsburghPAUnited States</p>
<p>Department of Biomedical Informatics
University of Pittsburgh
PittsburghPAUnited States</p>
<p>Xsl â€¢ Fo 
Â©sonish Sivarajkumar </p>
<p>Department of Health Information Management
University of Pittsburgh
6026, 15260Forbes Tower Pittsburgh, United StatesPA</p>
<p>An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study
0D7E346C6A761D182AB104ADE8D5BA9D10.2196/55318large language modelLLMLLMsnatural language processingNLPin-context learningprompt engineeringevaluationzero-shotfew shotpromptingGPTlanguage modellanguagemodelsmachine learningclinical dataclinical informationextractionBARDGeminiLLaMA-2heuristicpromptpromptsensemble Coreference resolution Coreference resolution
Background: Large language models (LLMs) have shown remarkable capabilities in natural language processing (NLP), especially in domains where labeled data are scarce or expensive, such as the clinical domain.However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data.This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches.Objective:The objective of this study is to assess the effectiveness of various prompt engineering techniques, including 2 newly introduced types-heuristic and ensemble prompts, for zero-shot and few-shot clinical information extraction using pretrained language models.Methods: This comprehensive experimental study evaluated different prompt types (simple prefix, simple cloze, chain of thought, anticipatory, heuristic, and ensemble) across 5 clinical NLP tasks: clinical sense disambiguation, biomedical evidence extraction, coreference resolution, medication status extraction, and medication attribute extraction.The performance of these prompts was assessed using 3 state-of-the-art language models: GPT-3.5 (OpenAI), Gemini (Google), and LLaMA-2 (Meta).The study contrasted zero-shot with few-shot prompting and explored the effectiveness of ensemble approaches.Results:The study revealed that task-specific prompt tailoring is vital for the high performance of LLMs for zero-shot clinical NLP.In clinical sense disambiguation, GPT-3.5 achieved an accuracy of 0.96 with heuristic prompts and 0.94 in biomedical evidence extraction.Heuristic prompts, alongside chain of thought prompts, were highly effective across tasks.Few-shot prompting improved performance in complex scenarios, and ensemble approaches capitalized on multiple prompt strengths.GPT-3.5 consistently outperformed Gemini and LLaMA-2 across tasks and prompt types.Conclusions:This study provides a rigorous evaluation of prompt engineering methodologies and introduces innovative techniques for clinical information extraction, demonstrating the potential of in-context learning in the clinical domain.These findings offer clear guidelines for future prompt-based clinical NLP research, facilitating engagement by non-NLP experts in clinical NLP advancements.To the best of our knowledge, this is one of the first works on the empirical evaluation of different prompt engineering approaches for clinical NLP in this era of generative artificial intelligence, and we hope that it will inspire and inform future research in this area.</p>
<p>Introduction</p>
<p>Clinical information extraction (IE) is the task of identifying and extracting relevant information from clinical narratives, such as clinical notes, radiology reports, or pathology reports.Clinical IE has many applications in health care, such as improving diagnosis, treatment, and decision-making; facilitating clinical research; and enhancing patient care [1,2].However, clinical IE faces several challenges, such as the scarcity and heterogeneity of annotated data, the complexity and variability of clinical language, and the need for domain knowledge and expertise.</p>
<p>Zero-shot IE is a promising paradigm that aims to overcome these challenges by leveraging large pretrained language models (LMs) that can perform IE tasks without any task-specific training data [3].In-context learning is a framework for zero-shot and few-shot learning, where a large pretrained LM takes a context and directly decodes the output without any retraining or fine-tuning [4].In-context learning relies on prompt engineering, which is the process of crafting informative and contextually relevant instructions or queries as inputs to LMs to guide their output for specific tasks [5].The use of prompt engineering lies in its ability to leverage the powerful capabilities of large LMs (LLMs), such as GPT-3.5 (OpenAI) [6], Gemini (Google) [7], LLaMA-2 (Meta) [8], even in scenarios where limited or no task-specific training data are available.In clinical natural language processing (NLP), where labeled data sets tend to be scarce, expensive, and time-consuming to create, splintered across institutions, and constrained by data use agreements, prompt engineering becomes even more crucial to unlock the potential of state-of-the-art LLMs for clinical NLP tasks.</p>
<p>While prompt engineering has been widely explored for general NLP tasks, its application and impact in clinical NLP remain relatively unexplored.Most of the existing literature on prompt engineering in the health care domain focuses on biomedical NLP tasks rather than clinical NLP tasks that involve processing real-world clinical notes.For instance, Chen et al [9] used a fixed template as the prompt to measure the performance of LLMs on biomedical NLP tasks but did not investigate different kinds of prompting methods.Wang et al [10] gave a comprehensive survey of prompt engineering for health care NLP applications such as question-answering systems, text summarization, and machine translation.However, they did not compare and evaluate different types of prompts for specific clinical NLP tasks and how the performance varies across different LLMs.There is a lack of systematic and comprehensive studies on how to engineer prompts for clinical NLP tasks, and the existing literature predominantly focuses on general NLP problems.This creates a notable gap in the research, warranting a dedicated investigation into the design and development of effective prompts specifically for clinical NLP.Currently, researchers in the field lack a comprehensive understanding of the types of prompts that exist, their relative effectiveness, and the challenges associated with their implementation in clinical settings.</p>
<p>The main research question and objectives of this study are to investigate how to engineer prompts for clinical NLP tasks, identify best practices, and address the challenges in this emerging field.By doing so, we aim to propose a guideline for future prompt-based clinical NLP studies.In this work, we present a comprehensive empirical evaluation study on prompt engineering for 5 diverse clinical NLP tasks, namely, clinical sense disambiguation, biomedical evidence extraction, coreference resolution, medication status extraction, and medication attribute extraction [11,12].By systematically evaluating different types of prompts proposed in recent literature, including prefix [13], cloze [14], chain of thought [15], and anticipatory prompts [16], we gain insights into their performance and suitability for each task.Two new types of prompting approaches were also introduced: (1) heuristic prompts and (2) ensemble prompts.The rationale behind these novel prompts is to leverage the existing knowledge and expertise in rule-based NLP, which has been prominent and has shown significant results in the clinical domain [17].We hypothesize that heuristic prompts, which are based on rules derived from domain knowledge and linguistic patterns, can capture the salient features and constraints of the clinical IE tasks.We also conjecture that ensemble prompts, which are composed of multiple types of prompts, can benefit from the complementary strengths and mitigate the weaknesses of each individual prompt.</p>
<p>One of the key aspects of prompt engineering is the number of examples or shots that are provided to the model along with the prompt.Few-shot prompting is a technique that provides the model with a few examples of input-output pairs, while zero-shot prompting does not provide any examples [3,18].By contrasting these strategies, we aim to shed light on the most efficient and effective ways to leverage prompt engineering in clinical NLP.Finally, we propose a prompt engineering framework to build and deploy zero-shot NLP models for the clinical domain.This study covers 3 state-of-the-art LMs, including GPT-3.5, Gemini, and LLaMA-2, to assess the generalizability of the findings across various models.This work yields novel insights and guidelines for prompt engineering specifically for clinical NLP tasks.</p>
<p>Methods</p>
<p>Tasks</p>
<p>We selected 5 distinct clinical NLP tasks representing diverse categories of natural language understanding: clinical sense disambiguation (text classification) [19], biomedical evidence extraction (named entity recognition) [20], coreference resolution [21], medication status extraction (named entity recognition+classification) [22], and medication attribute extraction (named entity recognition+relation extraction) [23].Table 1 provides a succinct overview of each task, an example scenario, and the corresponding prompt type used for each task.</p>
<p>Text extraction Biomedical evidence extraction</p>
<p>Identify the antecedent for the patient in the clinical note.</p>
<p>The goal here is to identify all mentions in clinical text that refer to the same entity.</p>
<p>Data Sets and Evaluation</p>
<p>The prompts were evaluated on 3 LLMs, GPT-3.5, Gemini, and LLaMA-2, under both zero-shot and few-shot prompting conditions, using precise experimental settings and parameters.To simplify the evaluation process and facilitate clear comparisons, we adopted accuracy as the sole evaluation metric for all tasks.Accuracy is defined as the proportion of correct outputs generated by the LLM for each task, using a resolver that maps the output to the label space.Table 2 shows the data sets and sample size for each clinical NLP task.The data sets are as follows:</p>
<p>â€¢ Clinical abbreviation sense inventories: This is a data set of clinical abbreviations, senses, and instances [28].It contains 41 acronyms from 18,164 notes, along with their expanded forms and contexts.We used a randomly sampled subset from this data set for clinical sense disambiguation, coreference resolution, medication status extraction, and medication attribute extraction tasks (Table 2).</p>
<p>â€¢ Evidence-based medicine-NLP: This is a data set of evidence-based medicine annotations for NLP [29].It contains 187 abstracts and 20 annotated abstracts, with interventions extracted from the text.We used this data set for the biomedical evidence extraction task.</p>
<p>XSL â€¢ FO</p>
<p>RenderX</p>
<p>All experiments were carried out in different system settings.All GPT-3.5 experiments were conducted using the GPT-3.5 Turbo application programming interface as of the September 2023 update.The LLaMA-2 model was directly accessed for our experiments.Gemini was accessed using the Gemini application (previously BARD)-Google's generative artificial intelligence conversational system.These varied system settings and access methods were taken into account to ensure the reliability and validity of our experimental results, given the differing architectures and capabilities of each LLM.</p>
<p>In evaluating the prompt-based approaches on GPT-3.5, Gemini, and LLaMA-2, we have also incorporated traditional NLP baselines to provide a comprehensive understanding of the LLMs' performance in a broader context.These baselines include well-established models such as Bidirectional Encoder Representations From Transformers (BERT) [30], Embeddings From Language Models (ELMO) [31], and PubMedBERT-Conditional Random Field (PubMedBERT-CRF) [32], which have previously set the standard in clinical NLP tasks.By comparing the outputs of LLMs against these baselines, we aim to offer a clear perspective on the advancements LLMs represent in the field.This comparative analysis is crucial for appreciating the extent to which prompt engineering techniques can leverage the inherent capabilities of LLMs, marking a significant evolution from traditional approaches to more dynamic and contextually aware methodologies in clinical NLP.</p>
<p>Prompt Creation Process</p>
<p>A rigorous process was followed to create suitable prompts for each task.These prompts were carefully crafted to match the specific context and objectives of each task.There is no established method for prompt design and selection as of now.Therefore, we adopted an iterative approach where prompts, which are created by health care experts, go through a verification and improvement process in an iterative cycle, which involved design, experimentation, and evaluation, as depicted in Figure 1. Figure 1 illustrates the 3 main steps of our prompt creation process: sampling, prompt designing, and deployment.In the sampling step (step 1), we defined the clinical NLP task (eg, named entity recognition, relation extraction, and text classification) and collected a sample of data and annotations as an evaluation for the task.In the prompt designing step (step 2), a prompt was designed for the task using one of the prompt types (eg, simple prefix prompt, simple cloze prompt, heuristic prompt, chain of thought prompt, question prompt, and anticipatory prompt).We also optionally performed few-shot prompting by providing some examples along with the prompt.The LLMs and the evaluation metrics for the experiment setup were then configured.We ran experiments with various prompt types and LLMs and evaluated their performance on the task.Based on the results, we refined or modified the prompt design until we achieved satisfactory performance or reached a limit.In the deployment step (step 3), the best prompt-based models were selected based on their performance metrics, and the model was deployed for the corresponding task.</p>
<p>Prompt Engineering Techniques</p>
<p>Overview</p>
<p>Prompt engineering is the process of designing and creating prompts that elicit desired responses from LLMs.Prompts can be categorized into different types based on their structure, function, and complexity.</p>
<p>Each prompt consists of a natural language query that is designed to elicit a specific response from the pretrained LLM.The prompts are categorized into 7 types, as illustrated in Figure 2 (all prompts have been included in Multimedia Appendix 1).Prefix prompts are the simplest type of prompts, which prepend a word or phrase indicating the type or format or tone of response for control and relevance.Cloze prompts are based on the idea of fill in the blank exercises, which create a masked token in the input text and ask the LLM to predict the missing word or phrase [3].Anticipatory prompts are the prompts anticipating the next question or command based on experience or knowledge, guiding the conversation.Chain of thought prompting involves a series of intermediate natural language reasoning steps that lead to the final output [15].</p>
<p>In addition to the existing types of prompts, 2 new novel prompts were also designed: heuristic prompts and ensemble prompts, which will be discussed in the following sections.</p>
<p>Heuristic Prompts</p>
<p>Heuristic prompts are rule-based prompts that decompose complex queries into smaller, more manageable components for comprehensive answers.Adopting the principles of traditional rule-based NLP, which relies on manually crafted, rule-based algorithms for specific clinical NLP applications, we have integrated these concepts into our heuristic prompts approach.These prompts use a set of predefined rules to guide the LLM in expanding abbreviations within a given context.For instance, a heuristic prompt might use the rule that an abbreviation is typically capitalized, followed by a period, and preceded by an article or a noun.This approach contrasts with chain of thought prompts, which focus on elucidating the reasoning or logic behind an output.Instead, heuristic prompts leverage a series of predefined rules to direct the LLM in executing a specific task.</p>
<p>Mathematically, we can express a heuristic prompt as H(x), a function applied to an input sequence x.This function is defined as a series of rule-based transformations T i , where i indicates the specific rule applied.The output of this function, denoted as y H , is then:
y H =H(x)=T n (T {n-1} (... T 1 (x)))
Here, each transformation T i applies a specific heuristic rule to modify the input sequence, making it more suitable for processing by LLMs.</p>
<p>From an algorithmic standpoint, heuristic prompts are implemented by defining a set of rules R={R 1 , R 2 , ..., R m }.Each rule R j is a function that applies a specific heuristic criterion to an input token or sequence of tokens.Algorithmically, the heuristic prompting process can be summarized as follows:</p>
<p>By merging the precision and specificity of traditional rule-based NLP methods with the advanced capabilities of LLMs, the heuristic prompts offer a robust and accurate system for clinical information processing and analysis.</p>
<p>Ensemble Prompts</p>
<p>Ensemble prompts are prompts that combine multiple prompts using majority voting for aggregated outputs.They use various types of prompts to generate multiple responses to the same XSL â€¢ FO RenderX input, subsequently selecting the most commonly occurring output as the final answer.For instance, an ensemble prompt might use 3 different prefix prompts, or a combination of other prompt types, to produce 3 potential expansions for an abbreviation.The most frequently appearing expansion is then chosen.For the sake of simplicity, we amalgamated the outputs from all 5 different prompt types using a majority voting approach.</p>
<p>Mathematically, consider a set of m different prompting methods P 1 , P 2 , ..., P m applied to the same input x.Each method generates an output y i for i=1,2, ..., m.The ensemble prompt's output y E is then the mode of these outputs:
y E =mode (y 1 , y 2 , ..., y m )
Algorithmically, the ensemble prompting process is as follows:</p>
<p>The rationale behind an ensemble prompt is that by integrating multiple types of prompts, we can use the strengths and counterbalance the weaknesses of each individual prompt, offering a robust and potentially more accurate response.Some prompts may be more effective for specific tasks or models, while others might be more resilient to noise or ambiguity.Majority voting allows us to choose the most likely correct or coherent output from the variety generated by different prompt types.</p>
<p>Results</p>
<p>Overview</p>
<p>In this section, we present the results of our experiments on prompt engineering for zero-shot clinical IE.Various prompt types were evaluated across 5 clinical NLP tasks, aiming to understand how different prompts influence the accuracy of different LLMs.Zero-shot and few-shot prompting strategies were also compared, exploring how the addition of context affects the model performance.Furthermore, we tested an ensemble approach that combines the outputs of different prompt types using majority voting.Finally, the impact of different LLMs on task performance was analyzed, and some interesting patterns were observed.Table 3 illustrates that different prompt types have different levels of effectiveness for different tasks and LLMs.We can also observe some general trends across the tasks and models.</p>
<p>Prompt Optimization and Evaluation</p>
<p>For clinical sense disambiguation, the heuristic and prefix prompts consistently achieved the highest performance across all LLMs, significantly outperforming baselines such as BERT [30] and ELMO, with GPT-3.5 achieving an accuracy of 0.96, showcasing its advanced understanding of clinical context using appropriate prompting strategies.For biomedical evidence extraction, the heuristic and chain of thought prompts excelled across all LLMs in zero-shot setting.This indicates that these prompt types were able to provide enough information and constraints for the model to extract the evidence from the clinical note.GPT-3.5 achieved an accuracy of 0.94 with these prompt types, which was higher than any other model or prompt type combination.For coreference resolution, the chain of thought prompt type performed best among all prompt types with 2 LLMs-GPT-3.5 and LLaMA-2.This indicates that this prompt type was able to provide enough structure and logic for the model to resolve the coreference in the clinical note.GPT-3.5 displayed high accuracy with this prompt type, achieving an accuracy of 0.94.For medication status extraction, simple prefix and heuristic prompts yielded good results across all LLMs.These prompt types were able to provide enough introduction or rules for the model to extract the status of the medication in relation to the patient or condition.GPT-3.5 excelled with these prompt types, achieving an accuracy of 0.76 and 0.74, respectively.For medication attribute extraction, we found that the chain of thought and heuristic prompts were effective across all LLMs.These prompt types were able to provide enough reasoning or rules for the model to extract and label the attributes of medications from clinical notes.Anticipatory prompts, however, had the best accuracy for Gemini among all the prompts.GPT-3.5 achieved an accuracy of 0.96 with these prompt types, which was higher than any other model or prompt type combination.Thus, we can see that task-specific prompt tailoring is crucial for achieving high accuracy.Different tasks require different levels of information and constraints to guide the LLM to produce the desired output.The experiments show that heuristic, prefix, and chain of thought prompts are generally very effective for guiding the LLM to produce clear and unambiguous outputs.As shown in Figure 3, it is clear that GPT-3.5 is a superior and versatile LLM that can handle various clinical NLP tasks in zero-shot settings, outperforming other models in most cases.Overall, the prompt-based approach has demonstrated remarkable superiority over traditional baseline models across all the 5 tasks.For clinical sense disambiguation, GPT-3.5'sheuristic prompts achieved a remarkable accuracy of 0.96, showcasing a notable improvement over baselines such as BERT (0.42) and ELMO (0.55).In biomedical evidence extraction, GPT-3.5 again set a high standard with an accuracy of 0.94 using heuristic prompts, far surpassing the baseline performance of PubMedBERT-CRF at 0.35.Coreference resolution saw GPT-3.5 reaching an accuracy of 0.94 with chain of thought prompts, eclipsing the performance of existing methods such as Toshniwal et al [34] (0.69).In medication status extraction, GPT-3.5 outperformed the baseline ScispaCy (0.52) with an accuracy of 0.76 using simple prefix prompts.Finally, for medication attribute extraction, GPT-3.5'sheuristic prompts achieved an impressive accuracy of 0.96, significantly higher than the ScispaCy baseline (0.70).These figures not only showcase the potential of LLMs in clinical settings but also set a foundation for future research to build upon, exploring even more sophisticated prompt engineering strategies and their implications for health care informatics.</p>
<p>Zero-Shot Versus Few-Shot Prompting</p>
<p>The performance of zero-shot prompting and few-shot prompting strategies was compared for each clinical NLP task.The same prompt types and LLMs were used as in the previous experiments, but some context was added to the input in the form of examples or explanations.Two examples or explanations were used for each task (2-shot) depending on the complexity and variability of the task.Table 3 shows that few-shot prompting consistently improved the accuracy of all combinations for all tasks except for clinical sense disambiguation and medication attribute extraction, where some zero-shot prompt types performed better.We also observed some general trends across the tasks and models.</p>
<p>We found that few-shot prompting enhanced accuracy by providing limited context that aided complex scenario understanding.The improvement was more pronounced compared to simple cloze prompts, which had lower accuracy in most of the tasks.We also found that some zero-shot prompt types were very effective for certain tasks, even outperforming few-shot prompting.These prompt types used a rule-based or reasoning approach to generate sentences that contained definitions or examples of the target words or concepts, which helped the LLM to understand and match the context.For example, heuristic prompts achieved higher accuracy than few-shot prompting for clinical sense disambiguation and medication attribute extraction, while chain of thought prompts achieved higher accuracy than few-shot prompting for coreference resolution and medication attribute extraction.Alternatively, the clinical evidence extraction task likely benefits from additional context provided by few-shot examples, which can guide the model more effectively than the broader inferences made in zero-shot scenarios.This suggests that tasks requiring deeper contextual understanding might be better suited to few-shot learning approaches.</p>
<p>From these results, we can infer that LLMs can be effectively used for clinical NLP in a no-data scenario, where we do not have many publicly available data sets, by using appropriate zero-shot prompt types that guide the LLM to produce clear and unambiguous outputs.However, few-shot prompting can also improve the performance of LLMs by providing some context that helps the LLM to handle complex scenarios.</p>
<p>Other Observations</p>
<p>Ensemble Approaches</p>
<p>We experimented with an ensemble approach by combining outputs from multiple prompts using majority voting.The ensemble approach was not the best-performing strategy for any of the tasks, but it was better than the low-performing prompts.The ensemble approach was able to benefit from the diversity and complementarity of different prompt types and avoid some of the pitfalls of individual prompts.For example, for clinical sense disambiguation, the ensemble approach achieved an accuracy of 0.9 with GPT-3.5, which was the second best-performing prompt type.Similarly, for medication attribute extraction, the ensemble approach achieved an accuracy of 0.9 with GPT-3.5 and 0.76 with Gemini, which were close to the best single prompt type (anticipatory).However, the ensemble approach also had some drawbacks, such as inconsistency and noise.For tasks that required more specific or consistent outputs, such as coreference resolution, the ensemble approach did not improve the accuracy over the best single prompt type and sometimes even decreased it.This suggests that the ensemble approach may introduce ambiguity for tasks that require more precise or coherent outputs.</p>
<p>While the ensemble approach aims to reduce the variance introduced by individual prompt idiosyncrasies, our specific implementation observed instances where the combination of diverse prompt types introduced additional complexity.This complexity occasionally manifested as inconsistency and noise in the outputs contrary to our objective of achieving higher performance.Future iterations of this approach may include refinement of the prompt selection process to enhance consistency and further reduce noise in the aggregated outputs.</p>
<p>Impact of LLMs</p>
<p>Variations in performance were observed among different LLMs (Table 3).We found that GPT-3.5 generally outperformed Gemini and LLaMA-2 on most tasks.This suggests that GPT-3.5 has a better generalization ability and can handle a variety of clinical NLP tasks with different prompt types.However, Gemini and LLaMA-2 also showed some advantages over GPT-3.5 on certain tasks and prompt types.For example, Gemini achieved the highest accuracy of 0.81 with simple cloze prompts and LLaMA-2 achieved the highest accuracy of 0.8 with simple prefix prompts for coreference resolution.This indicates that Gemini and LLaMA-2 may have some domain-specific knowledge that can benefit certain clinical NLP tasks for specific prompt types.</p>
<p>Persona Patterns</p>
<p>Persona patterns are a way of asking the LLM to act like a persona or a system that is relevant to the task or domain.For example, one can ask the LLM to "act as a clinical NLP expert."This can help the LLM to generate outputs that are more appropriate and consistent with the persona or system.For example, one can use the following prompt for clinical sense disambiguation:</p>
<p>Act as a clinical NLP expert.Disambiguate the word "cold" in the following sentence: "She had a cold for three days."</p>
<p>We experimented with persona patterns for different tasks and LLMs and found that they can improve the accuracy and quality of the outputs.Persona patterns can help the LLM to focus on the relevant information and constraints for the task and avoid generating outputs that are irrelevant or contradictory to the persona or system.</p>
<p>Randomness in Output</p>
<p>Most LLMs do not produce the output in the same format every time.There is inherent randomness in the outputs the LLMs produce.Hence, the prompts need to be specific in the way they are done for the task.Prompts are powerful when they are specific and if we use them in the right way.</p>
<p>Randomness in output can be beneficial or detrimental for different tasks and scenarios.In the clinical domain, randomness can introduce noise and errors in the outputs, which can make them less accurate and reliable for the users.For example, for tasks that involve extracting factual information, such as biomedical evidence extraction and medication status extraction, randomness can cause the LM to produce outputs that are inconsistent or contradictory with the input or context.</p>
<p>Guidelines and Suggestions for Optimal Prompt Selection</p>
<p>In recognizing the evolving nature of clinical NLP, we expand our discussion to contemplate the adaptability of our recommended prompt types and LM combinations across a wider spectrum of clinical tasks and narratives.This speculative analysis aims to bridge the gap between our current findings and their applicability to unexplored clinical NLP challenges, setting a foundation for future research to validate and refine these recommendations.In this section, we synthesize the main findings from our experiments and offer some practical advice for prompt engineering for zero-shot and few-shot clinical IE.We propose the following steps for selecting optimal prompts for different tasks and scenarios:</p>
<p>The first step is to identify the type of clinical NLP task, which can be broadly categorized into three types: (1) classification, (2) extraction, and (3) resolution.Classification tasks involve assigning a label or category to a word, phrase, or sentence in a clinical note, such as clinical sense disambiguation or medication status extraction.Extraction tasks involve identifying and extracting relevant information from a clinical note, such as biomedical evidence extraction or medication attribute extraction.Resolution tasks involve linking or matching entities or concepts in a clinical note, such as coreference resolution.</p>
<p>The second step is to choose the prompt type that is most suitable for the task type.We found that different prompt types have different strengths and weaknesses for different task types, depending on the level of information and constraints they provide to the LLM.Table 4 summarizes our findings and recommendations for optimal prompt selection for each task type.</p>
<p>The third step is to choose the LLM that is most compatible with the chosen prompt type.We found that different LLMs have different capabilities and limitations for different prompt types, depending on their generalization ability and domain-specific knowledge.Table 5 summarizes our findings and recommendations for optimal LLM selection for each prompt type.</p>
<p>The fourth step is to evaluate the performance of the chosen prompt type and LLM combination on the clinical NLP task using appropriate metrics such as accuracy, precision, recall, or F 1 -score.If the performance is satisfactory, then the prompt engineering process is complete.If not, then the process can be repeated by choosing a different prompt type or LLM or by modifying the existing prompt to improve its effectiveness.</p>
<p>Discussion</p>
<p>Principal Findings</p>
<p>In this paper, we have presented a novel approach to zero-shot and few-shot clinical IE using prompt engineering.Various prompt types were evaluated across 5 clinical NLP tasks: clinical sense disambiguation, biomedical evidence extraction, coreference resolution, medication status extraction, and medication attribute extraction.The performance of different LLMs, GPT-3.5, Gemini, and LLaMA-2, was also compared.Our main findings are as follows:</p>
<ol>
<li>Task-specific prompt tailoring is crucial for achieving high accuracy.Different tasks require different levels of information and constraints to guide the LLM to produce the desired output.Therefore, it is important to design prompts that are relevant and specific to the task at hand and avoid using generic or vague prompts that may confuse the model or lead to erroneous outputs.It is noteworthy that context size has a significant impact on the performance of LLMs in zero-shot IE [36].In the scope of this study, we have avoided the context size dependence on performance, as it is a complex issue that requires careful consideration.</li>
</ol>
<p>This study serves as an initial exploration into the efficacy of prompt engineering in clinical NLP, providing foundational insights rather than exhaustive guidelines.Given the rapid advancements in generative artificial intelligence and the complexity of clinical narratives, we advocate for continuous empirical testing of these prompt strategies across diverse clinical tasks and data sets.This approach will not only validate the generalizability of our findings but also uncover new avenues for enhancing the accuracy and applicability of LLMs in clinical settings.</p>
<p>Limitations</p>
<p>In this study, we primarily focused on exploring the capabilities and versatility of generative LLMs in the context of zero-shot and few-shot learning for clinical NLP tasks.Our approach also has some limitations that we acknowledge in this work.First, it relies on the quality and availability of pretrained LLMs, which may vary depending on the domain and task.As LLMs are rapidly evolving, some parts of the prompt engineering discipline may be timeless, while some parts may evolve and adapt over time as different capabilities of models evolve.Second, it requires a lot of experimentation and iteration to optimize prompts for different applications, which may be iterative and time-consuming.However, once optimal prompts are identified, the approach offers time savings in subsequent applications by reusing these prompts or making minor adjustments for similar tasks.We may not have explored all the possible combinations and variations of prompts that could potentially improve the performance of the clinical NLP tasks.Third, the LLMs do not release the details of the data set that they were trained on.Hence, the high accuracy could be because the models would have already seen the data during training and not because of the effectiveness of the prompts.</p>
<p>Future Work</p>
<p>We plan to address these challenges and limitations in our future work.We aim to develop more systematic and automated methods for prompt design and evaluation, such as using prompt-tuning or meta-learning techniques.We also aim to incorporate more domain knowledge or external resources into the prompts or the LLMs, such as using ontologies, knowledge graphs, or databases.We also aim to incorporate more quality control or error correction mechanisms into the prompts or the LLMs, such as using adversarial examples, confidence scores, or human feedback.</p>
<p>Conclusions</p>
<p>In this paper, we have benchmarked different prompt engineering techniques for both zero-shot and few-shot clinical NLP tasks.Two new types of prompts, heuristic and ensemble prompts, were also conceptualized and proposed.We have demonstrated that prompt engineering can enable the use of pretrained LMs for various clinical NLP tasks without requiring any fine-tuning or additional data.We have shown that task-specific prompt tailoring, heuristic prompts, chain of thought prompts, few-shot prompting, and ensemble approaches can improve the accuracy and quality of the outputs.We have also shown that GPT-3.5 is very adaptable and precise across all tasks and prompt types, while Gemini and LLaMA-2 may have some domain-specific advantages for certain tasks and prompt types.</p>
<p>We believe that a prompt-based approach has several benefits over existing methods for clinical IE.It reduces the cost and time in the initial phases of clinical NLP application development, where prompt-based methods offer a streamlined alternative to the conventional data preparation and model training processes.It is flexible and adaptable, as it can be applied to various clinical NLP tasks with different prompt types and LLMs.It is interpretable and explainable, as it uses natural language prompts that can be easily understood and modified by humans.</p>
<p>Figure 1 .
1
Figure 1.Iterative prompt design process: a schematic diagram of the iterative prompt creation process for clinical NLP tasks.The process consists of 3 steps: sampling, prompt designing, and deployment.The sampling step involves defining the task and collecting data and annotations.The prompt designing step involves creating and refining prompts using different types and language models.The deployment step involves selecting the best model and deploying the model for clinical use.LLM: large language model; NER: named entity recognition; NLP: natural language processing; RE: relation extraction.</p>
<p>Figure 2 .
2
Figure 2. Types of prompts: examples of 7 types of prompts that we used to query the pretrained language model for different clinical information extraction tasks.[X]: context; [Y]: abbreviation; [Z]: expanded form.</p>
<p>a</p>
<p>Best performance on a task regardless of the model (ie, for each GPT-3.5 or Gemini or LLaMA-2 triple).b Best performance for each model on a task.c BERT: Bidirectional Encoder Representations From Transformers.d ELMO: Embeddings From Language Models.e PubMedBERT-CRF: PubMedBERT-Conditional Random Field.</p>
<p>Figure 3 .
3
Figure 3. Graphical comparison of prompt types in the 5 clinical natural language processing tasks used in this study.</p>
<p>Table 1 .
1
Task descriptions.
TaskNLP a task categoryDescriptionExample promptClinical sense disambigua-Text classificationThis task involves identifying the correctWhat is the meaning of the abbreviationtiongiven context. meaning of clinical abbreviations within aCR b in the context of cardiology?In this task, interventions are extracted fromIdentify the psychological interventions inbiomedical abstracts.the given text?</p>
<p>Table 2 .
2
Evaluation data sets and samples for different tasks.
TaskData setData set exampleSamplesClinical sense disambigua-tionCASI aThe abbreviation "CR b " can refer to "car-diac resuscitation" or "computed radiogra-11 acronyms from 55 notesphy."Biomedical evidence extrac-tionEBM c -NLP dIdentifying panic, avoidance, and agorapho-bia (psychological interventions)187 abstracts and 20 annotated abstractsCoreference resolutionCASIResolving references to "the patient" or "the105 annotated examplesstudy" within a clinical trial report.Identifying that a patient is currently taking105 annotated examples with 340 medica-insulin for diabetes.tion status pairs</p>
<p>Table 3 .
3
Performance comparison of different prompt types and language models.
Task and language modelSimple pre-Simple clozeAnticipatoryHeuristicChain ofEnsembleFew shotfixthoughtClinical sense disambiguationGPT-3.50.880.860.880.96 a0.90.90.82Gemini0.76 b0.680.710.750.720.710.67LLaMA-20.88 b0.760.820.820.780.820.78BERT c (from [33])0.420.420.420.420.420.420.42ELMO d (from [33])0.550.550.550.550.550.550.55Biomedical evidence extractionGPT-3.50.920.820.880.940.940.880.96 aGemini0.890.890.91 b0.90.91 b0.90.88LLaMA-20.850.88 b0.870.88 b0.870.880.86PubMedBERT-CRF e (from [29])0.350.350.350.350.350.350.35Coreference resolutionGPT-3.50.780.60.740.94 a0.94 a0.740.74Gemini0.690.81 b0.730.670.710.690.7LLaMA-20.8 b0.640.740.760.8 b0.780.68Toshniwal et al [34]0.690.690.690.690.690.690.69Medication status extractionGPT-3.50.76 a0.720.750.740.730.750.72Gemini0.67 b0.510.650.550.590.580.55LLaMA-20.580.480.520.64 b0.520.580.42ScispaCy [35]0.520.520.520.520.520.520.52Medication attribute extractionGPT-3.50.880.840.90.96 a0.96 a0.90.96 aGemini0.680.720.88 c0.70.740.760.88 bLLaMA-20.60.660.580.660.72 b0.640.6</p>
<p>Table 4 .
4
Optimal prompt types for different clinical natural language processing task types.
Prompt type</p>
<p>Table 5 .
5
Optimal language models for different prompt types.
Language model
JMIR Med Inform 2024 | vol. 12 | e55318 | p. 2 https://medinform.jmir.org/2024/1/e55318 (page number not for citation purposes)
JMIR Med Inform 2024 | vol. 12 | e55318 | p. 4 https://medinform.jmir.org/2024/1/e55318 (page number not for citation purposes)
JMIR Med Inform 2024 | vol. 12 | e55318 | p. 5 https://medinform.jmir.org/2024/1/e55318 (page number not for citation purposes)
JMIR Med Inform 2024 | vol. 12 | e55318 | p. 6 https://medinform.jmir.org/2024/1/e55318 (page number not for citation purposes)
JMIR Med Inform 2024 | vol. 12 | e55318 | p. 7 https://medinform.jmir.org/2024/1/e55318 (page number not for citation purposes)
JMIR Med Inform 2024 | vol. 12 | e55318 | p. 8 https://medinform.jmir.org/2024/1/e55318 (page number not for citation purposes)
JMIR Med Inform 2024 | vol. 12 | e55318 | p. 9 https://medinform.jmir.org/2024/1/e55318 (page number not for citation purposes)
JMIR Med Inform 2024 | vol. 12 | e55318 | p. 10 https://medinform.jmir.org/2024/1/e55318 (page number not for citation purposes)
JMIR Med Inform 2024 | vol. 12 | e55318 | p. 11 https://medinform.jmir.org/2024/1/e55318 (page number not for citation purposes)
JMIR Med Inform 2024 | vol. 12 | e55318 | p. 12 https://medinform.jmir.org/2024/1/e55318 (page number not for citation purposes)
(page number not for citation purposes)
AcknowledgmentsThis work was supported by the National Institutes of Health (awards U24 TR004111 and R01 LM014306).The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.Authors' ContributionsSS conceptualized, designed, and organized this study; analyzed the results; and wrote, reviewed, and revised the paper.MK and AS-M analyzed the results, and wrote, reviewed, and revised the paper.SV wrote, reviewed, and revised the paper.YW conceptualized, designed, and directed this study and wrote, reviewed, and revised the paper.Conflicts of InterestNone declared.Multimedia Appendix 1Prompts for clinical natural language processing tasks.[DOCX File , 31 KB-MultimediaAppendix
Clinical information extraction applications: a literature review. Y Wang, L Wang, M Rastegar-Mojarad, S Moon, F Shen, N Afzal, 10.1016/j.jbi.2017.11.011J Biomed Inform. 772018FREE Full text. Medline: 29162496</p>
<p>Information extraction from electronic medical documents: state of the art and future research directions. M Y Landolsi, L Hlaoua, L B Romdhane, 10.1007/s10115-022-01779-1Knowl Inf Syst. 6522023FREE Full text. Medline: 36405956</p>
<p>HealthPrompt: a zero-shot learning paradigm for clinical natural language processing. S Sivarajkumar, Y Wang, Medline: 37128372]AMIA Annu Symp Proc. 20222022FREE Full text</p>
<p>Rethinking the role of demonstrations: what makes in-context learning work?. S Min, X Lyu, A Holtzman, M Artetxe, M Lewis, H Hajishirzi, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022. December 7-11, 2022</p>
<p>United Arab Emirates. Abu Dhabi, 10.18653/v1/2022.emnlp-main.759</p>
<p>A prompt pattern catalog to enhance prompt engineering with chatGPT. J White, Q Fu, S Hays, M Sandborn, C Olea, H Gilbert, 10.48550/arXiv.2302.11382ArXiv. Preprint posted online on. February 21. 2023FREE Full text</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, Advances in Neural Information Processing Systems. Red Hook, NYCurran Associates, IncNeurIPS 2022. 202235</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Google , 10.48550/arXiv.2312.11805ArXiv. Preprint posted online on. December 19, 2023FREE Full text</p>
<p>Llama 2: open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, 10.48550/arXiv.2307.09288ArXiv. Preprint posted online on. July 28, 2023FREE Full text</p>
<p>Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. Q Chen, J Du, Y Hu, V K Keloth, X Peng, K Raja, 10.48550/arXiv.2305.16326ArXiv. Preprint posted online on May. 102023FREE Full text</p>
<p>Prompt engineering for healthcare: methodologies and applications. J Wang, E Shi, S Yu, Z Wu, C Ma, H Dai, 10.48550/arXiv.2304.14670April 28. 2023FREE Full text</p>
<p>Improving large language models for clinical named entity recognition via prompt engineering. Y Hu, Q Chen, J Du, X Peng, V K Keloth, X Zuo, 10.48550/arXiv.2303.16416ArXiv. Preprint posted online on March. 292023FREE Full text</p>
<p>Zero-shot temporal relation extraction with chatGPT. C Yuan, Q Xie, S Ananiadou, The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks. 2023. July 13. 2023</p>
<p>. Canada Toronto, 10.18653/v1/2023.bionlp-1.7</p>
<p>Prefix-tuning: optimizing continuous prompts for generation. X Li, L Liang, 10.18653/v1/2021.acl-long.353Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing2021. August 1-6, 20211Virtual Event</p>
<p>Pre-train, prompt, and predict: a systematic survey of prompting methods in natural language processing. P Liu, W Yuan, J Fu, Z Jiang, H Hayashi, G Neubig, 10.1145/3560815ACM Comput Surv. 5592023FREE Full text</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, Advances in Neural Information Processing Systems. Red Hook, NYCurran Associates IncNeurIPS 2022. 202235</p>
<p>Learning from dialogue after deployment: feed yourself, chatbot!. B Hancock, A Bordes, P E Mazare, J Weston, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019. July 28-August 2, 2019</p>
<p>. Italy Florence, 10.18653/v1/p19-1358</p>
<p>Rule-based information extraction from patients' clinical data. A Mykowiecka, M Marciniak, A KupÅ›Ä‡, 10.1016/j.jbi.2009.07.007J Biomed Inform. 4252009FREE Full text. Medline: 19646551</p>
<p>Large language models are few-shot clinical information extractors. M Agrawal, S Hegselmann, H Lang, Y Kim, D Sontag, The 2022 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics2022. December 7-11. 2022. 1998-2022</p>
<p>United Arab Emirates. Abu Dhabi, 10.18653/v1/2022.emnlp-main.130</p>
<p>Clinical abbreviation disambiguation using neural word embeddings. Y Wu, J Xu, Y Zhang, H Xu, Proceedings of the 2015 Workshop on Biomedical Natural Language Processing. the 2015 Workshop on Biomedical Natural Language Processing2015. 2015. July 30. 2015</p>
<p>. China Beijing, 10.18653/v1/w15-3822</p>
<p>Machine learning approaches to retrieve high-quality, clinically relevant evidence from the biomedical literature: systematic review. W Abdelkader, T Navarro, R Parrish, C Cotoi, F Germini, A Iorio, 10.2196/30401JMIR Med Inform. 99e304012021FREE Full text. Medline: 34499041</p>
<p>Evaluating the state of the art in coreference resolution for electronic medical records. O Uzuner, A Bodnari, S Shen, T Forbush, J Pestian, B R South, 10.1136/amiajnl-2011-000784J Am Med Inform Assoc. 1952012FREE Full text. Medline: 22366294</p>
<p>Extracting timing and status descriptors for colonoscopy testing from electronic medical records. J C Denny, J F Peterson, N N Choma, H Xu, R A Miller, L Bastarache, 10.1136/jamia.2010.004804J Am Med Inform Assoc. 1742010FREE Full text. Medline: 20595304</p>
<p>Overview of the first natural language processing challenge for extracting medication, indication, and adverse drug events from electronic health record notes (MADE 1.0). A Jagannatha, F Liu, W Liu, H Yu, 10.1007/s40264-018-0762-zDrug Saf. 4212019FREE Full text</p>
<p>Dynamic text categorization of search results for medical class recognition in real world evidence studies in the Chinese language. Y Chen, X Wu, M Chen, Q Song, J Wei, X Li, Proceedings of the International Conference on Bioinformatics and Computational Intelligence. the International Conference on Bioinformatics and Computational IntelligenceAssociation for Computing Machinery20172017</p>
<p>. China Beijing, 10.1145/3135954.3135962</p>
<p>Cognitive Informatics and Soft Computing Proceeding of CISC 2017. P K Mallick, V E Balas, A K Bhoi, A F Zobaa, Advances in Intelligent Systems and Computing. New YorkSpringer Verlag2019768</p>
<p>S Ananiadou, D Lee, H Xu, M Song, 10.1145/2396761.239875812-The Proceedings of the Sixth ACM International Workshop on Data and Text Mining in Biomedical Informatics. New York2012. 2012. 2012Conjunction with the 21st ACM International Conference on Information and Knowledge Management</p>
<p>I Elghandour, R State, M Brorsson, L Le, N Antonopoulos, Y Xie, 10.1109/bdcat.2018.00008IEEE/ACM 3rd International Conference on Big Data Computing Applications and Technologies (BDCAT). Shanghai, China2016. 2016. December 6-9, 2016IEEE/ACM International Symposium on Big Data Computing (BDC)</p>
<p>A sense inventory for clinical abbreviations and acronyms created using clinical notes and medical dictionary resources. S Moon, S Pakhomov, N Liu, J O Ryan, G B Melton, 10.1136/amiajnl-2012-001506J Am Med Inform Assoc. 2122014FREE Full text. Medline: 23813539</p>
<p>A corpus with multi-level annotations of patients, interventions and outcomes to support language processing for medical literature. B Nye, J J Li, R Patel, Y Yang, I Marshall, A Nenkova, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational Linguistics2018. July 15-20. 20181Presented at</p>
<p>. Australia Melbourne, 10.18653/v1/p18-1019</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. J Devlin, M W Chang, K Lee, K Toutanova, ArXiv. Preprint posted online on May. 242019FREE Full text</p>
<p>Detecting formal thought disorder by deep contextualized word representations. J Sarzynska-Wawer, A Wawer, A Pawlak, J Szymanowska, I Stefaniak, M Jarkiewicz, 10.1016/j.psychres.2021.114135Psychiatry Res. 3041141352021Medline: 34343877</p>
<p>Domain-specific language model pretraining for biomedical natural language processing. Y Gu, R Tinn, H Cheng, M Lucas, N Usuyama, X Liu, 10.1145/3458754ACM Trans Comput Healthcare. 312021</p>
<p>Zero-shot clinical acronym expansion via latent meaning cells. G Adams, M Ketenci, S Bhave, A Perotte, N Elhadad, Proc Mach Learn Res. 1362020FREE Full text. Medline: 34790898</p>
<p>On generalization in coreference resolution. S Toshniwal, P Xia, S Wiseman, K Livescu, K Gimpel, 10.18653/v1/2021.crac-1.12ArXiv. Preprint posted online on. September 20, 2021FREE Full text</p>
<p>ScispaCy: fast and robust models for biomedical natural language processing. M Neumann, D King, I Beltagy, W Ammar, 10.18653/v1/w19-5034ArXiv. Preprint posted online on October. 92019FREE Full text</p>
<p>Evaluation of healthprompt for zero-shot clinical text classification. S Sivarajkumar, Y Wang, 10.1109/ichi57859.2023.000812023 IEEE 11th International Conference on Healthcare Informatics (ICHI). Houston, TX, USA2023. June 26-29, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9840 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9840</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9840</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-166.html">extraction-schema-166</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-892dba6e3240433b30c8ac776c3b6dad61ebdfa9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/892dba6e3240433b30c8ac776c3b6dad61ebdfa9" target="_blank">Automated Statistical Model Discovery with Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This work introduces a method for language model driven automated statistical model discovery within the principled framework of Box's Loop, and identifies models on par with human expert designed models and extends classic models in interpretable ways.</p>
                <p><strong>Paper Abstract:</strong> Statistical model discovery is a challenging search over a vast space of models subject to domain-specific constraints. Efficiently searching over this space requires expertise in modeling and the problem domain. Motivated by the domain knowledge and programming capabilities of large language models (LMs), we introduce a method for language model driven automated statistical model discovery. We cast our automated procedure within the principled framework of Box's Loop: the LM iterates between proposing statistical models represented as probabilistic programs, acting as a modeler, and critiquing those models, acting as a domain expert. By leveraging LMs, we do not have to define a domain-specific language of models or design a handcrafted search procedure, which are key restrictions of previous systems. We evaluate our method in three settings in probabilistic modeling: searching within a restricted space of models, searching over an open-ended space, and improving expert models under natural language constraints (e.g., this model should be interpretable to an ecologist). Our method identifies models on par with human expert designed models and extends classic models in interpretable ways. Our results highlight the promise of LM-driven model discovery.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9840.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9840.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BoxLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language model driven automated model discovery (BoxLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative system that uses a proposal LM to generate probabilistic programs (statistical models) from datasets and a critic LM to produce natural-language model criticism; models are fit with probabilistic programming (pymc) and evaluated with posterior predictive checks and ELPD-LOO to drive revisions in a Box's Loop.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 V (gpt4-11-06-preview) as proposal/critic in BoxLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal GPT-4 variant (referred to as GPT-4 V / gpt4-11-06-preview) used as both a proposal LM (generates probabilistic programs and chain-of-thought reflections) and a critic LM (generates natural-language criticism). The paper does not specify architecture details or parameter counts for GPT-4 V and treats it as an externally hosted multimodal LM.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Statistical modeling / applied domains including time-series forecasting, ecology (growth/ODE models), epidemiology/surgery outcomes, and other small-sample applied datasets</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Statistical models and empirical mathematical relationships (probabilistic program representations: GPs with compositional kernels, ODE systems, growth functions, hierarchical models, polynomial/empirical fits)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Prompt-driven in-context learning: the proposal LM conditions on dataset visual/textual representations, metadata, exemplars (past high-scoring probabilistic programs), and chain-of-thought scaffolded hypotheses; the proposed probabilistic programs are fit with pymc (MCMC/HMC) or gradient-based fitting for ODE parameters; posterior predictive means/variances and scalar scores (ELPD-LOO, MAE) are provided to a critic LM which synthesizes natural-language feedback that is appended to the prompt for subsequent rounds (iterative Box's Loop).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Collections of small, real-world datasets (not large scholarly-paper corpora): six univariate time series used for GP kernel search (e.g., monthly air passengers, beer sales), four Stan PosteriorDB datasets (eight schools, dugongs age/length, surgical mortality across hospitals, peregrine population time series), simulated analogs for ablation, and simulated ODE time series (perturbed Lotka-Volterra). Data were provided as plots and/or textual dataframes; no large corpus of scholarly papers was used as input in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Model fit evaluated using expected log predictive density estimated via leave-one-out cross validation (ELPD-LOO), held-out mean absolute error (MAE) for time series/ODE experiments, posterior predictive checks (posterior predictive means and variances), and standard MCMC diagnostics (R̂ and effective sample size) for fitted probabilistic programs.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>BoxLM (using GPT-4 V) discovered probabilistic programs that matched or outperformed strong baselines: matched Automatic Statistician on GP kernel search benchmarks (test MAE comparable across datasets), matched or exceeded expert Stan programs on 9 of 12 dataset/ablation conditions (Table 1), and produced improved ODE models for perturbed Lotka-Volterra dynamics that outperformed standard Lotka-Volterra, neural ODEs, and hybrid baselines on test MAE. Success rates for proposed pymc programs that could be fit were reported (70-78% across LM input modalities).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Discovered compositional GP kernels (e.g., periodic × linear to capture increasing amplitude), von Bertalanffy and logistic growth functions for biological growth datasets (with data-informed priors), quartic polynomial fits improving over domain-informed models in some ablations, hybrid ODE corrections including Holling type-II style handling-time modulation (via MLP) and additive/multiplicative neural corrections to Lotka-Volterra dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Key limitations reported: experiments used small/one-dimensional datasets so criticism was limited to simple statistics (posterior predictive mean/variance and residuals); LMs can be strongly influenced by metadata/prior knowledge (sometimes leading to suboptimal bias); context length limits constrain how much history and data can be fed into prompts (visual-only interface explored as partial mitigation); not all LM-proposed programs were successfully fit (Table 2: GPT-4 textual dataframe 78%, GPT-4 vision-only 70%, GPT-3.5 76%); concerns about dataset leakage mitigated via simulated-data ablations; criticism step was manually specified (not fully automated); the paper does not fine-tune the LM and notes potential gains from finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against Automatic Statistician (greedy GP kernel search), spectral mixture and periodic GP kernels, N-BEATS (neural baseline), human expert Stan programs (translated to pymc), neural ODE and hybrid neural ODE baselines. BoxLM matched Automatic Statistician on GP tasks and matched or outperformed expert programs in most Stan/ablation conditions; BoxLM variations outperformed neural/standard baselines on ODE correction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Statistical Model Discovery with Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9840.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9840.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 V</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 V (multimodal GPT-4 variant, gpt4-11-06-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The multimodal GPT-4 variant used as both proposal and critic LM in BoxLM to generate probabilistic programs, reflect on datasets, and synthesize natural-language model criticism.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT-4 Technical Report.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 V (gpt4-11-06-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal GPT-4 variant referenced by Achiam et al. (2023) in the paper's references; treated as an externally hosted large LM with multimodal inputs (text + images). The paper does not provide architecture, parameter counts, or training data details.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General-purpose language model applied to statistical model discovery across applied scientific datasets (time series, growth/biology, hierarchical clinical data, ODE systems).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Empirical/statistical relationships and parametric differential-equation models (e.g., growth curves, ODE corrections, compositional kernel formulas).</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>In-context learning with exemplars and chain-of-thought scaffolding: GPT-4 V is prompted with dataset visualizations/text, metadata, exemplar probabilistic programs, and asked to (1) reflect on data, (2) propose hypotheses and modeling approaches, and (3) output commented probabilistic programs (pymc or Python ODE code).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Per-experiment small datasets (plots and/or dataframes) and previously proposed exemplar programs included in prompts; no large corpus of scholarly papers was used as input for GPT-4 V in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Proposals evaluated by fitting generated probabilistic programs with pymc or gradient-based ODE fitting, scored via ELPD-LOO, MAE on held-out data, posterior predictive summaries, and MCMC diagnostics.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GPT-4 V powered proposals and criticism that led to models comparable to or exceeding baselines across several tasks; success/failure rates for programs being successfully fit were reported (≈70–78% depending on modality and model).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>GPT-4 V proposed domain-appropriate functional forms (von Bertalanffy, logistic), kernel compositions for GPs, and interpretable hybrid ODE corrections informed by ecological constraints (Holling type II handling-time modulation via MLP).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Paper notes GPT-4 V can struggle with long-context reasoning (prompt length), proposals sometimes produce programs that fail to run or to be fit, and the LM's prior knowledge (metadata) can bias modeling choices; no claims about discovery from large scholarly-paper corpora were made.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Also ran GPT-3.5 for ablations (slightly worse in some cases); no fine-tuned LM baseline was used in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Statistical Model Discovery with Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9840.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9840.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-thought (CoT) prompting / reflective reasoning in LM proposals</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting technique where the LM is instructed to generate intermediate reasoning steps: reflect on dataset properties, sketch high-level modeling approach, state hypotheses to address before writing code, and add comments addressing hypotheses in the generated probabilistic programs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chain-of-thought prompting applied to GPT-4 V (and LMs generally)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A prompt-engineering approach that elicits intermediate reasoning steps from LMs (cited works include Wei et al. 2022, Kojima et al. 2022). The paper instructs the proposal LM to produce reflections and hypotheses before outputting code.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>LM-driven statistical model construction across applied datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Supports discovery of parametric/statistical relationships by improving the quality of model proposals (not itself a model of laws).</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>In-context chain-of-thought: LM is directed to reason about dataset plots, generate modeling hypotheses and a high-level plan, then produce commented probabilistic program code; this improves LM proposal quality according to paper motivation and prior literature.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Same dataset inputs as BoxLM experiments (plots/dataframes and exemplars); CoT used within proposals to structure LM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Indirect evaluation via downstream model quality (ELPD-LOO, MAE) and success rate of generated programs being fit.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The paper reports that instructing the proposal LM to reflect (CoT-style) was part of the prompt design and is motivated by prior results that CoT improves LM reasoning; CoT is used to improve proposal fidelity though no ablation isolating CoT effect is quantitatively reported.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>LM outputs included commented code with stated hypotheses (e.g., 'Hypothesis 1: Logistic growth for prey with carrying capacity kappa'), which guided probabilistic program structure and prior choices.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>No direct quantitative isolation of chain-of-thought benefit provided; context length and verbosity can exacerbate prompt-size limits.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Chain-of-thought is presented as augmentation on top of GPT-4 V in the proposal pipeline; compared implicitly to naive code generation without structured reflection (not separately benchmarked).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Statistical Model Discovery with Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9840.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9840.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hypothesis Search with LMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hypothesis search / inductive reasoning with language models (Wang et al., Qiu et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recent lines of work using LMs to generate and refine hypotheses or programs via inductive reasoning; cited by the paper as related work motivating LM-driven model discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs used for hypothesis search (e.g., GPT-family in cited works)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Cited prior work explores LMs' inductive reasoning and hypothesis refinement capabilities (Wang et al., 2024; Qiu et al., 2024); specifics of model variants and architectures are in those cited papers rather than this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General inductive reasoning / hypothesis generation applicable across domains (cognitive science, ML).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Hypothesis-level relations and symbolic rules; prior work focuses on deterministic input-output rules rather than noisy statistical laws.</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>LMs generate candidate hypotheses/programs and iteratively refine them (related to Box's Loop idea), often using prompt engineering and in-context exemplars; differs from BoxLM in that those works target deterministic rule extraction rather than statistical modeling of noisy data.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Cited works operate on sets of example input-output pairs or tasks constructed to probe inductive reasoning; not large scholarly-paper corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Evaluation in cited works is on correctness of induced deterministic rules/hypotheses and model refinement ability; this paper cites them as conceptual antecedents.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited works demonstrate LMs can perform inductive hypothesis refinement in toy settings; the present paper contrasts that with its focus on noisy statistical model discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Not detailed in this paper; refer to Wang et al. (2024) and Qiu et al. (2024) for specific hypothesis refinement tasks and results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Cited works focus on deterministic relations and may not generalize directly to noisy, probabilistic, real-world data; BoxLM addresses noisy statistical modeling explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Cited as related work rather than a direct baseline; differences with BoxLM are highlighted (deterministic rule learning vs. statistical model discovery).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Statistical Model Discovery with Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9840.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9840.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Schmidt & Lipson 2009</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distilling free-form natural laws from experimental data (Schmidt & Lipson, 2009)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior system for discovering closed-form mathematical relationships (physical laws) from experimental time-series using symbolic regression / genetic programming.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Distilling free-form natural laws from experimental data.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Symbolic regression / genetic programming (as used in Schmidt & Lipson)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Method uses symbolic regression (genetic programming) to search a space of symbolic expressions that explain dynamical data; not an LM-based approach and predates modern LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Physics and nonlinear dynamical system identification from experimental data.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Closed-form physical laws and symbolic mathematical relationships (explicit equations discovered from measured variables).</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Search over symbolic expression space guided by fitness measures (e.g., prediction error) and sparsity/regularization, producing candidate analytic equations that describe dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Empirical experimental time-series datasets (physical experiments); not large scholarly-paper corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Quality of recovered equations judged by predictive accuracy and match to known physical laws; demonstration-style examples in the original paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as foundational automated model discovery work; the present paper positions BoxLM as extending model-discovery automation but using LMs and probabilistic programming to avoid designing DSLs and handcrafted search procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Original Schmidt & Lipson work recovered known mechanical laws (e.g., Hamiltonians) from measured motion data (details in their paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Classic symbolic-regression approaches require hand-specified expression grammars and search heuristics; they are not integrated with probabilistic inference frameworks as BoxLM is.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Mentioned among prior automated discovery systems (along with Bongard & Lipson 2007, McKinney et al. 2006); BoxLM differs by using LMs to generate probabilistic-program representations and by leveraging probabilistic programming inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Statistical Model Discovery with Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9840.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9840.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automatic Statistician</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic Statistician (Duvenaud et al., Lloyd et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior automated model-discovery system that used a hand-designed DSL of Gaussian process kernels and a greedy search over kernel compositions to discover interpretable regression models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Structure discovery in nonparametric regression through compositional kernel search.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Greedy compositional kernel search over a hand-crafted GP kernel DSL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>System defines a DSL of base GP kernels and compositional operators (addition, multiplication) and performs greedy search to build composite kernels, with hyperparameters optimized via marginal likelihood.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Nonparametric regression and time-series modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Functional relationships represented as Gaussian process kernels (interpretable kernel compositions corresponding to trends, periodicity, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Hand-specified kernel DSL + greedy compositional search + marginal likelihood-based hyperparameter optimization; outputs interpretable kernel expressions and natural-language descriptions in later work.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Time-series / regression datasets (e.g., UCI-style or forecasting datasets) rather than large collections of scholarly papers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Predictive accuracy on held-out data (MAE) and interpretability of kernel structure; used as a baseline in GP kernel experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Automatic Statistician is a strong baseline for GP kernel search; BoxLM matched its performance on the evaluated time-series datasets while avoiding the need to hand-design a DSL.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Compositional kernels capturing trend + periodicity or periodic × linear amplitude growth, etc.; BoxLM produced similar kernel forms.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires a hand-specified DSL and bespoke search heuristics; BoxLM claims to relax these constraints by letting an LM generate programmatic model representations in a generic language.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>BoxLM matched the Automatic Statistician on several GP tasks and in some cases BoxLM+ (augmented kernel space) improved performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated Statistical Model Discovery with Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Distilling free-form natural laws from experimental data. <em>(Rating: 2)</em></li>
                <li>Automated reverse engineering of nonlinear dynamical systems. <em>(Rating: 2)</em></li>
                <li>Structure discovery in nonparametric regression through compositional kernel search. <em>(Rating: 2)</em></li>
                <li>Hypothesis search: Inductive reasoning with language models <em>(Rating: 2)</em></li>
                <li>From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought <em>(Rating: 1)</em></li>
                <li>Automated model discovery for human brain using constitutive artificial neural networks. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9840",
    "paper_id": "paper-892dba6e3240433b30c8ac776c3b6dad61ebdfa9",
    "extraction_schema_id": "extraction-schema-166",
    "extracted_data": [
        {
            "name_short": "BoxLM",
            "name_full": "Language model driven automated model discovery (BoxLM)",
            "brief_description": "An iterative system that uses a proposal LM to generate probabilistic programs (statistical models) from datasets and a critic LM to produce natural-language model criticism; models are fit with probabilistic programming (pymc) and evaluated with posterior predictive checks and ELPD-LOO to drive revisions in a Box's Loop.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 V (gpt4-11-06-preview) as proposal/critic in BoxLM",
            "model_description": "Multimodal GPT-4 variant (referred to as GPT-4 V / gpt4-11-06-preview) used as both a proposal LM (generates probabilistic programs and chain-of-thought reflections) and a critic LM (generates natural-language criticism). The paper does not specify architecture details or parameter counts for GPT-4 V and treats it as an externally hosted multimodal LM.",
            "scientific_domain": "Statistical modeling / applied domains including time-series forecasting, ecology (growth/ODE models), epidemiology/surgery outcomes, and other small-sample applied datasets",
            "law_type": "Statistical models and empirical mathematical relationships (probabilistic program representations: GPs with compositional kernels, ODE systems, growth functions, hierarchical models, polynomial/empirical fits)",
            "method_description": "Prompt-driven in-context learning: the proposal LM conditions on dataset visual/textual representations, metadata, exemplars (past high-scoring probabilistic programs), and chain-of-thought scaffolded hypotheses; the proposed probabilistic programs are fit with pymc (MCMC/HMC) or gradient-based fitting for ODE parameters; posterior predictive means/variances and scalar scores (ELPD-LOO, MAE) are provided to a critic LM which synthesizes natural-language feedback that is appended to the prompt for subsequent rounds (iterative Box's Loop).",
            "input_corpus_description": "Collections of small, real-world datasets (not large scholarly-paper corpora): six univariate time series used for GP kernel search (e.g., monthly air passengers, beer sales), four Stan PosteriorDB datasets (eight schools, dugongs age/length, surgical mortality across hospitals, peregrine population time series), simulated analogs for ablation, and simulated ODE time series (perturbed Lotka-Volterra). Data were provided as plots and/or textual dataframes; no large corpus of scholarly papers was used as input in the experiments.",
            "evaluation_method": "Model fit evaluated using expected log predictive density estimated via leave-one-out cross validation (ELPD-LOO), held-out mean absolute error (MAE) for time series/ODE experiments, posterior predictive checks (posterior predictive means and variances), and standard MCMC diagnostics (R̂ and effective sample size) for fitted probabilistic programs.",
            "results_summary": "BoxLM (using GPT-4 V) discovered probabilistic programs that matched or outperformed strong baselines: matched Automatic Statistician on GP kernel search benchmarks (test MAE comparable across datasets), matched or exceeded expert Stan programs on 9 of 12 dataset/ablation conditions (Table 1), and produced improved ODE models for perturbed Lotka-Volterra dynamics that outperformed standard Lotka-Volterra, neural ODEs, and hybrid baselines on test MAE. Success rates for proposed pymc programs that could be fit were reported (70-78% across LM input modalities).",
            "notable_examples": "Discovered compositional GP kernels (e.g., periodic × linear to capture increasing amplitude), von Bertalanffy and logistic growth functions for biological growth datasets (with data-informed priors), quartic polynomial fits improving over domain-informed models in some ablations, hybrid ODE corrections including Holling type-II style handling-time modulation (via MLP) and additive/multiplicative neural corrections to Lotka-Volterra dynamics.",
            "limitations_challenges": "Key limitations reported: experiments used small/one-dimensional datasets so criticism was limited to simple statistics (posterior predictive mean/variance and residuals); LMs can be strongly influenced by metadata/prior knowledge (sometimes leading to suboptimal bias); context length limits constrain how much history and data can be fed into prompts (visual-only interface explored as partial mitigation); not all LM-proposed programs were successfully fit (Table 2: GPT-4 textual dataframe 78%, GPT-4 vision-only 70%, GPT-3.5 76%); concerns about dataset leakage mitigated via simulated-data ablations; criticism step was manually specified (not fully automated); the paper does not fine-tune the LM and notes potential gains from finetuning.",
            "baseline_comparison": "Compared against Automatic Statistician (greedy GP kernel search), spectral mixture and periodic GP kernels, N-BEATS (neural baseline), human expert Stan programs (translated to pymc), neural ODE and hybrid neural ODE baselines. BoxLM matched Automatic Statistician on GP tasks and matched or outperformed expert programs in most Stan/ablation conditions; BoxLM variations outperformed neural/standard baselines on ODE correction tasks.",
            "uuid": "e9840.0",
            "source_info": {
                "paper_title": "Automated Statistical Model Discovery with Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-4 V",
            "name_full": "GPT-4 V (multimodal GPT-4 variant, gpt4-11-06-preview)",
            "brief_description": "The multimodal GPT-4 variant used as both proposal and critic LM in BoxLM to generate probabilistic programs, reflect on datasets, and synthesize natural-language model criticism.",
            "citation_title": "GPT-4 Technical Report.",
            "mention_or_use": "use",
            "model_name": "GPT-4 V (gpt4-11-06-preview)",
            "model_description": "Multimodal GPT-4 variant referenced by Achiam et al. (2023) in the paper's references; treated as an externally hosted large LM with multimodal inputs (text + images). The paper does not provide architecture, parameter counts, or training data details.",
            "scientific_domain": "General-purpose language model applied to statistical model discovery across applied scientific datasets (time series, growth/biology, hierarchical clinical data, ODE systems).",
            "law_type": "Empirical/statistical relationships and parametric differential-equation models (e.g., growth curves, ODE corrections, compositional kernel formulas).",
            "method_description": "In-context learning with exemplars and chain-of-thought scaffolding: GPT-4 V is prompted with dataset visualizations/text, metadata, exemplar probabilistic programs, and asked to (1) reflect on data, (2) propose hypotheses and modeling approaches, and (3) output commented probabilistic programs (pymc or Python ODE code).",
            "input_corpus_description": "Per-experiment small datasets (plots and/or dataframes) and previously proposed exemplar programs included in prompts; no large corpus of scholarly papers was used as input for GPT-4 V in these experiments.",
            "evaluation_method": "Proposals evaluated by fitting generated probabilistic programs with pymc or gradient-based ODE fitting, scored via ELPD-LOO, MAE on held-out data, posterior predictive summaries, and MCMC diagnostics.",
            "results_summary": "GPT-4 V powered proposals and criticism that led to models comparable to or exceeding baselines across several tasks; success/failure rates for programs being successfully fit were reported (≈70–78% depending on modality and model).",
            "notable_examples": "GPT-4 V proposed domain-appropriate functional forms (von Bertalanffy, logistic), kernel compositions for GPs, and interpretable hybrid ODE corrections informed by ecological constraints (Holling type II handling-time modulation via MLP).",
            "limitations_challenges": "Paper notes GPT-4 V can struggle with long-context reasoning (prompt length), proposals sometimes produce programs that fail to run or to be fit, and the LM's prior knowledge (metadata) can bias modeling choices; no claims about discovery from large scholarly-paper corpora were made.",
            "baseline_comparison": "Also ran GPT-3.5 for ablations (slightly worse in some cases); no fine-tuned LM baseline was used in the paper.",
            "uuid": "e9840.1",
            "source_info": {
                "paper_title": "Automated Statistical Model Discovery with Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Chain-of-thought prompting",
            "name_full": "Chain-of-thought (CoT) prompting / reflective reasoning in LM proposals",
            "brief_description": "Prompting technique where the LM is instructed to generate intermediate reasoning steps: reflect on dataset properties, sketch high-level modeling approach, state hypotheses to address before writing code, and add comments addressing hypotheses in the generated probabilistic programs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Chain-of-thought prompting applied to GPT-4 V (and LMs generally)",
            "model_description": "A prompt-engineering approach that elicits intermediate reasoning steps from LMs (cited works include Wei et al. 2022, Kojima et al. 2022). The paper instructs the proposal LM to produce reflections and hypotheses before outputting code.",
            "scientific_domain": "LM-driven statistical model construction across applied datasets.",
            "law_type": "Supports discovery of parametric/statistical relationships by improving the quality of model proposals (not itself a model of laws).",
            "method_description": "In-context chain-of-thought: LM is directed to reason about dataset plots, generate modeling hypotheses and a high-level plan, then produce commented probabilistic program code; this improves LM proposal quality according to paper motivation and prior literature.",
            "input_corpus_description": "Same dataset inputs as BoxLM experiments (plots/dataframes and exemplars); CoT used within proposals to structure LM outputs.",
            "evaluation_method": "Indirect evaluation via downstream model quality (ELPD-LOO, MAE) and success rate of generated programs being fit.",
            "results_summary": "The paper reports that instructing the proposal LM to reflect (CoT-style) was part of the prompt design and is motivated by prior results that CoT improves LM reasoning; CoT is used to improve proposal fidelity though no ablation isolating CoT effect is quantitatively reported.",
            "notable_examples": "LM outputs included commented code with stated hypotheses (e.g., 'Hypothesis 1: Logistic growth for prey with carrying capacity kappa'), which guided probabilistic program structure and prior choices.",
            "limitations_challenges": "No direct quantitative isolation of chain-of-thought benefit provided; context length and verbosity can exacerbate prompt-size limits.",
            "baseline_comparison": "Chain-of-thought is presented as augmentation on top of GPT-4 V in the proposal pipeline; compared implicitly to naive code generation without structured reflection (not separately benchmarked).",
            "uuid": "e9840.2",
            "source_info": {
                "paper_title": "Automated Statistical Model Discovery with Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Hypothesis Search with LMs",
            "name_full": "Hypothesis search / inductive reasoning with language models (Wang et al., Qiu et al.)",
            "brief_description": "Recent lines of work using LMs to generate and refine hypotheses or programs via inductive reasoning; cited by the paper as related work motivating LM-driven model discovery.",
            "citation_title": "Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement.",
            "mention_or_use": "mention",
            "model_name": "LLMs used for hypothesis search (e.g., GPT-family in cited works)",
            "model_description": "Cited prior work explores LMs' inductive reasoning and hypothesis refinement capabilities (Wang et al., 2024; Qiu et al., 2024); specifics of model variants and architectures are in those cited papers rather than this paper.",
            "scientific_domain": "General inductive reasoning / hypothesis generation applicable across domains (cognitive science, ML).",
            "law_type": "Hypothesis-level relations and symbolic rules; prior work focuses on deterministic input-output rules rather than noisy statistical laws.",
            "method_description": "LMs generate candidate hypotheses/programs and iteratively refine them (related to Box's Loop idea), often using prompt engineering and in-context exemplars; differs from BoxLM in that those works target deterministic rule extraction rather than statistical modeling of noisy data.",
            "input_corpus_description": "Cited works operate on sets of example input-output pairs or tasks constructed to probe inductive reasoning; not large scholarly-paper corpora.",
            "evaluation_method": "Evaluation in cited works is on correctness of induced deterministic rules/hypotheses and model refinement ability; this paper cites them as conceptual antecedents.",
            "results_summary": "Cited works demonstrate LMs can perform inductive hypothesis refinement in toy settings; the present paper contrasts that with its focus on noisy statistical model discovery.",
            "notable_examples": "Not detailed in this paper; refer to Wang et al. (2024) and Qiu et al. (2024) for specific hypothesis refinement tasks and results.",
            "limitations_challenges": "Cited works focus on deterministic relations and may not generalize directly to noisy, probabilistic, real-world data; BoxLM addresses noisy statistical modeling explicitly.",
            "baseline_comparison": "Cited as related work rather than a direct baseline; differences with BoxLM are highlighted (deterministic rule learning vs. statistical model discovery).",
            "uuid": "e9840.3",
            "source_info": {
                "paper_title": "Automated Statistical Model Discovery with Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Schmidt & Lipson 2009",
            "name_full": "Distilling free-form natural laws from experimental data (Schmidt & Lipson, 2009)",
            "brief_description": "A prior system for discovering closed-form mathematical relationships (physical laws) from experimental time-series using symbolic regression / genetic programming.",
            "citation_title": "Distilling free-form natural laws from experimental data.",
            "mention_or_use": "mention",
            "model_name": "Symbolic regression / genetic programming (as used in Schmidt & Lipson)",
            "model_description": "Method uses symbolic regression (genetic programming) to search a space of symbolic expressions that explain dynamical data; not an LM-based approach and predates modern LLMs.",
            "scientific_domain": "Physics and nonlinear dynamical system identification from experimental data.",
            "law_type": "Closed-form physical laws and symbolic mathematical relationships (explicit equations discovered from measured variables).",
            "method_description": "Search over symbolic expression space guided by fitness measures (e.g., prediction error) and sparsity/regularization, producing candidate analytic equations that describe dynamics.",
            "input_corpus_description": "Empirical experimental time-series datasets (physical experiments); not large scholarly-paper corpora.",
            "evaluation_method": "Quality of recovered equations judged by predictive accuracy and match to known physical laws; demonstration-style examples in the original paper.",
            "results_summary": "Cited as foundational automated model discovery work; the present paper positions BoxLM as extending model-discovery automation but using LMs and probabilistic programming to avoid designing DSLs and handcrafted search procedures.",
            "notable_examples": "Original Schmidt & Lipson work recovered known mechanical laws (e.g., Hamiltonians) from measured motion data (details in their paper).",
            "limitations_challenges": "Classic symbolic-regression approaches require hand-specified expression grammars and search heuristics; they are not integrated with probabilistic inference frameworks as BoxLM is.",
            "baseline_comparison": "Mentioned among prior automated discovery systems (along with Bongard & Lipson 2007, McKinney et al. 2006); BoxLM differs by using LMs to generate probabilistic-program representations and by leveraging probabilistic programming inference.",
            "uuid": "e9840.4",
            "source_info": {
                "paper_title": "Automated Statistical Model Discovery with Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Automatic Statistician",
            "name_full": "Automatic Statistician (Duvenaud et al., Lloyd et al.)",
            "brief_description": "A prior automated model-discovery system that used a hand-designed DSL of Gaussian process kernels and a greedy search over kernel compositions to discover interpretable regression models.",
            "citation_title": "Structure discovery in nonparametric regression through compositional kernel search.",
            "mention_or_use": "mention",
            "model_name": "Greedy compositional kernel search over a hand-crafted GP kernel DSL",
            "model_description": "System defines a DSL of base GP kernels and compositional operators (addition, multiplication) and performs greedy search to build composite kernels, with hyperparameters optimized via marginal likelihood.",
            "scientific_domain": "Nonparametric regression and time-series modeling.",
            "law_type": "Functional relationships represented as Gaussian process kernels (interpretable kernel compositions corresponding to trends, periodicity, etc.).",
            "method_description": "Hand-specified kernel DSL + greedy compositional search + marginal likelihood-based hyperparameter optimization; outputs interpretable kernel expressions and natural-language descriptions in later work.",
            "input_corpus_description": "Time-series / regression datasets (e.g., UCI-style or forecasting datasets) rather than large collections of scholarly papers.",
            "evaluation_method": "Predictive accuracy on held-out data (MAE) and interpretability of kernel structure; used as a baseline in GP kernel experiments in this paper.",
            "results_summary": "Automatic Statistician is a strong baseline for GP kernel search; BoxLM matched its performance on the evaluated time-series datasets while avoiding the need to hand-design a DSL.",
            "notable_examples": "Compositional kernels capturing trend + periodicity or periodic × linear amplitude growth, etc.; BoxLM produced similar kernel forms.",
            "limitations_challenges": "Requires a hand-specified DSL and bespoke search heuristics; BoxLM claims to relax these constraints by letting an LM generate programmatic model representations in a generic language.",
            "baseline_comparison": "BoxLM matched the Automatic Statistician on several GP tasks and in some cases BoxLM+ (augmented kernel space) improved performance.",
            "uuid": "e9840.5",
            "source_info": {
                "paper_title": "Automated Statistical Model Discovery with Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Distilling free-form natural laws from experimental data.",
            "rating": 2
        },
        {
            "paper_title": "Automated reverse engineering of nonlinear dynamical systems.",
            "rating": 2
        },
        {
            "paper_title": "Structure discovery in nonparametric regression through compositional kernel search.",
            "rating": 2
        },
        {
            "paper_title": "Hypothesis search: Inductive reasoning with language models",
            "rating": 2
        },
        {
            "paper_title": "From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought",
            "rating": 1
        },
        {
            "paper_title": "Automated model discovery for human brain using constitutive artificial neural networks.",
            "rating": 1
        }
    ],
    "cost": 0.0169615,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Automated Statistical Model Discovery with Language Models</h1>
<p>Michael Y. Li ${ }^{1}$ Emily B. Fox ${ }^{123}$ Noah D. Goodman ${ }^{14}$</p>
<h4>Abstract</h4>
<p>Statistical model discovery is a challenging search over a vast space of models subject to domainspecific constraints. Efficiently searching over this space requires expertise in modeling and the problem domain. Motivated by the domain knowledge and programming capabilities of large language models (LMs), we introduce a method for language model driven automated statistical model discovery. We cast our automated procedure within the principled framework of Box's Loop: the LM iterates between proposing statistical models represented as probabilistic programs, acting as a modeler, and critiquing those models, acting as a domain expert. By leveraging LMs, we do not have to define a domain-specific language of models or design a handcrafted search procedure, which are key restrictions of previous systems. We evaluate our method in three settings in probabilistic modeling: searching within a restricted space of models, searching over an open-ended space, and improving expert models under natural language constraints (e.g., this model should be interpretable to an ecologist). Our method identifies models on par with human expert designed models and extends classic models in interpretable ways. Our results highlight the promise of LM-driven model discovery.</p>
<h2>1. Introduction</h2>
<p>Modeling, or generating a parsimonious but explanatory representation of a complex system, is at the heart of scientific discovery. Model discovery is challenging because it involves searching over a vast space of candidate models subject to domain-specific constraints (e.g., find the best</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>model that remains interpretable to domain experts). Efficiently searching over the space requires extensive human expertise: modelers need broad knowledge of different modeling approaches and must work closely with domain experts to adapt these approaches to a given problem domain. As a concrete example, consider modeling blood-glucose dynamics in Type 1 diabetes (T1D) patients; accurately modeling these dynamics can enable better insulin regulation and reduce complications from the disease. To model these dynamics, modelers need to understand biomedical models that capture blood-glucose dynamics in idealized, lab settings but they also need to understand techniques for adapting these models to handle real data with noise and missingness (Miller et al., 2020). Domain experts play a crucial role in this process: modelers must work closely with clinicians to ensure that the model is consistent with human physiology. As this example illustrates, model discovery can require significant human expertise. Automating this process could accelerate and democratize scientific discovery.</p>
<p>Automated model discovery is not a new ambition. Previous systems have been successfully deployed for discovering physical laws (Bongard \&amp; Lipson, 2007; McKinney et al., 2006; Linka et al., 2023), reverse-engineering non-linear dynamical systems (Schmidt \&amp; Lipson, 2009), nonparametric regression (Duvenaud et al., 2013) and unsupervised learning (Grosse, 2014). However, in these systems, a human expert had to carefully design a domain specific language (DSL) of models and specify a hand-crafted search procedure for composing models in that DSL. For example, in the Automatic Statistician (Duvenaud et al., 2013; Lloyd et al., 2014), a system for nonparametric regression and time series modeling, human experts defined a DSL of Gaussian process kernels and a search procedure that composes kernels via addition and multiplication. Defining the DSL and the operators for composing models requires significant modeling expertise. These systems also compromised flexibility for automation: rather than choosing a model class bestsuited for a problem, experts chose models that compose conveniently. This breaks the core principle of separation of modeling from inference (van de Meent et al., 2021).</p>
<p>As we saw above, there are two key roles in the model discovery pipeline: the domain expert and the modeler. We hypothesize that LMs can supplement these roles and re-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Language model driven automated model discovery (BoxLM). 1) The prompt for the LM contains the dataset in visual and/or textual form, dataset metadata (e.g., dataset description), the code for previous probabilistic programs, and natural language feedback. 2) Given this, the proposal LM proposes new models expressed as probabilistic programs. 3) To fit these programs in a generic way, we leverage probabilistic programming languages and obtain scores and posterior predictive samples. 4) After we fit models, we compute the posterior predictive mean and variance. We provide these statistics to a critic LM which produces natural language feedback to guide the next round of model building. 5) We propagate the best programs, their posterior predictive means and variances, and natural language feedback forward by updating the prompt.
duce the human expertise required in model discovery. Our hypothesis is motivated by recent work exploring LM capabilities. First, LMs have been successfully applied to domains including law (Bommarito \&amp; Katz, 2022), medicine (Lee et al., 2023), and mathematics (Wu et al., 2023). This suggests that LMs have broad domain knowledge which may enable them to supplement the domain expert. Second, LMs can reliably write code (Rozière et al., 2023; Chen et al., 2021) which means we do not require a human expert to define a DSL. Instead, we can search over a more openended space of models, provided they can be expressed in a generic programming language like Python. Third, LMs have strong inductive reasoning capabilities (e.g., they can generate hypotheses from limited data) (Wang et al., 2024; Qiu et al., 2024). We hypothesize these capabilities may enable them to reason about data (Zhong et al., 2023). These capabilities, in addition to their knowledge of various modeling approaches, may enable the LM to supplement the modeler.</p>
<p>Leveraging LMs for automated model discovery is enticing at a conceptual level. However, in order to deliver on this promising idea, we need to make several important design choices. These design choices should exploit the strengths of LMs but remain grounded in principled statistical modeling. First, we need a generic and flexible representation of a statistical model that can be expressed programmatically and is amenable to automated inference (e.g., model fit-
ting). Second, we need a method for guiding LM proposals through natural language.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Automated Model Discovery with LMs
input dataset \(\mathcal{D}\), number of rounds \(T, k\) number of ex-
    emplars, \(m\) number of proposals per round, (optional)
    warm-start example \(z_{0}\), function for scoring a program
    score, (optional) function for producing natural lan-
    guage feedback criticize
    \(\mathcal{Z} \leftarrow \emptyset\)
    while \(t&lt;T\) do
        \(\left\{z_{i}^{t}\right\}_{i=1}^{m} \sim q_{\mathrm{LM}}\left(\cdot \mid \mathcal{Z}, z_{0}, h^{t}, \mathcal{D}\right)\)
        \(\left\{s_{i}\right\}_{i=1}^{m} \leftarrow\) score-all \(\left(\right.\) score, \(\left.\left\{z_{i}^{t}\right\}_{i=1}^{m}, \mathcal{D}\right)\)
        \(\mathcal{Z} \leftarrow\) select-exemplars \(\left(k,\left\{z_{i}^{t}\right\}_{i=1}^{m},\left\{s_{i}\right\}_{i=1}^{m}\right)\)
        \(h^{t+1} \leftarrow \operatorname{criticize}\left(\left\{z_{i}^{t}\right\}_{i=1}^{m},\left\{s_{i}\right\}_{i=1}^{m}, h^{t}\right)\)
    end while
</code></pre></div>

<p>To fulfill these requirements, we draw on research in probabilistic programming and inductive reasoning with LMs. In particular, we introduce the following method: LMs propose statistical models expressed as probabilistic programs, given a dataset and some metadata (e.g., dataset description). We then fit these models using generic probabilistic inference techniques, compute statistics assessing the model fit, summarize findings from these statistics in natural language, and select exemplar models to guide the next round of proposals (Figure 1). Our approach is connected to recent</p>
<p>work on hypothesis search and inductive reasoning with LMs, but targets a fundamentally different problem (Wang et al., 2024; Qiu et al., 2024). While we also leverage the inductive reasoning capabilities of LMs, our focus is on statistical modeling of noisy real-world data, while previous work focused on learning deterministic input-output rules that can be implemented with standard Python programs.</p>
<p>We evaluate our method in three settings that cover common use cases in probabilistic modeling: searching within a DSL, searching over an open-ended space of probabilistic models, and improving expert models subject to modeling constraints expressed in natural language. In the first use case, we illustrate that our LM system is effective even in a DSL and matches the performance of the Automatic Statistician (Duvenaud et al., 2013). We then consider the more general setting of automatically constructing probabilistic models for real world data; crucially, we do not require a user to define a DSL and this generality is enabled by our choice to use probabilistic programs. Our method identifies probabilistic programs that match the performance of human expert written programs. In the third setting, we use LMs to improve classic models and illustrate a compelling advantage of using LMs for model discovery: given that certain modeling constraints can be difficult to express formally but easy to express in natural language (e.g., this model should be more physical), we use natural language to guide LMs towards models that balance interpretability and flexibility.</p>
<h2>2. Automated Box's Loop with Language Models</h2>
<p>We begin with a brief background on the probabilistic modeling paradigm. We then formally introduce our problem setting and describe our approach. For an overview, see Figure 1 and Algorithm 1.</p>
<h3>2.1. Background</h3>
<p>In probabilistic modeling, our goal is to describe a dataset in terms of unobserved, latent structure. We describe a dataset through a probabilistic model which is a joint distribution $p(x, z \mid \eta)$; here $x=x_{1: N}$ denotes $N$ observed data points, $z=z_{1: M}$ denotes $M$ latent variables, and $\eta$ corresponds to non-random quantities in the model (Blei, 2014). After specifying a probabilistic model, we fit the model to observed data. Fitting a model involves inference, or conditioning on observed data and computing the posterior distribution $p(z \mid x)$. After fitting a model, we perform model criticism. In the model criticism step, we evaluate the model by interrogating the posterior. In this work, our model criticism is inspired by a common model criticism technique known as a posterior predictive check: to identify discrepancies, samples are drawn from the posterior predictive distribution
and statistics of these samples are compared against those of the observed data (Gelman et al., 2013). Model building and model criticism typically take place over multiple iterations in an iterative process known as Box's Loop (Box \&amp; Hunter, 1962).</p>
<p>There are many different representations of a probabilistic model. Since our goal is automated model discovery, we use probabilistic programs. Probabilistic programming languages provide ways to flexibly represent probabilistic models as programs and support generic inference methods for any arbitrary program (Wood et al., 2014; Goodman et al., 2008; van de Meent et al., 2021). In the context of automated model discovery, these are highly desirable properties, since LMs can write code reliably and other representations of probabilistic models can require custom inference methods. Our approach of using LMs to generate probabilistic programs is related to the approach taken by Wong et al. (2023) but is motivated by a different problem. Our emphasis is on using LMs as a tool for developing statistical models of real-world datasets, while their focus was on integrating symbolic methods with LMs.</p>
<h3>2.2. Problem formulation</h3>
<p>Our framework is motivated by recent work in inductive reasoning with LMs (Qiu et al., 2024; Wang et al., 2024), integrating tools with LMs (Gao et al., 2023), and driving LMs via linguistic feedback (Shinn et al., 2023).</p>
<p>At a high level, we consider a method for learning probabilistic models from data that involves two steps: a model building step and a criticism step ${ }^{1}$. Crucially, by learning a model, we mean searching over a space of model structures and not just learning the parameters of some fixed model class. In each step, we leverage LMs. In the proposal step, a proposal $L M$ proposes probabilistic programs for a dataset. We then fit these probabilistic programs and evaluate them. In the criticism step, we provide a critic $L M$ with programs and statistics assessing model fit (e.g., model criticism statistics) and ask the critic LM to provide feedback to guide the next round of proposals.</p>
<p>We start with a dataset $\mathcal{D}=\left{\mathbf{x}<em i="i">{\mathbf{i}}, y</em>\right}<em _mathbf_i="\mathbf{i">{i=1}^{n}$. Here $\mathbf{x}</em>$ (e.g., length of animal). Context informs how human modelers leverage prior knowledge; for example, if a modeler knows their dataset consists of monthly carbon dioxide measurements over a fifty-year}} \in \mathbb{R}^{d}$ are fixed $d$-dimensional input values (e.g., features) and $y_{i} \in \mathbb{R}$ are the observations. Let $\Sigma$ be the vocabulary of the LM. For each dataset, we have an associated metadata set $\mathcal{C} \in \Sigma^{d+1}$, which consists of natural language descriptions of $\mathcal{D}$ (e.g., animal ages vs length) and natural language descriptions of each feature in $\mathcal{D</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>time span, they will choose a model that can capture periodicity and a linear trend. Our goal is to find a probabilistic program $z \sim \Sigma^{*}$ that maximizes some notion of quality, which we take here to be either the log marginal likelihood or expected log predictive density (ELPD) estimated via cross validation (LOO) (Vehtari et al., 2017).</p>
<h3>2.3. Approach</h3>
<p>Model Building Step In the model building step, we automatically generate probabilistic programs for modeling a dataset given information about the dataset and previously proposed programs. In particular, to generate candidates for round $t$, we sample $m$ probabilistic programs $z_{i}^{t}$ from the proposal LM, $q_{\mathrm{LM}}(\cdot)$. In our experiments, we use GPT-4 V (Achiam et al., 2023) (gpt4-11-06-preview), which has multimodal capabilities. We leverage in-context learning, or LM's ability to learn from examples in a prompt, to guide the LM's proposals based on high-scoring programs in the past (Brown et al., 2020). Specifically, $q_{\mathrm{LM}}$ "conditions" on $h^{t} \in \Sigma^{*}$, a natural language instruction synthesizing previous modeling approaches and suggesting new approaches, $k$ exemplars $\left{z_{1}, \ldots z_{k}\right}$, and a visual or textual representation of $\mathcal{D}$. Optionally, $q_{\mathrm{LM}}$ also conditions on a warm-start expert program $z_{0}$ :</p>
<p>$$
z_{i}^{t} \sim q_{\mathrm{LM}}\left(\cdot \mid z_{0}, z_{1}, \ldots, z_{k}, h^{t-1}, \mathcal{D}\right)
$$</p>
<p>We run this at a temperature of 0.7 . Chain-of-thought reasoning, or generating intermediate reasoning steps, improves the performance of LMs (Wei et al., 2022; Kojima et al., 2022). Motivated by this, we instruct $q_{\mathrm{LM}}$ to reflect on the properties of the dataset or plot of the data, sketch a highlevel modeling approach, state the hypotheses that it will address before writing a program, and add comments to code that address specific hypotheses.</p>
<p>To create exemplars $z_{1}, \ldots, z_{k}$ for round $t$ for $q_{\mathrm{LM}}$, we choose the best $k$ programs among the $m$ proposed programs in round $t-1$.</p>
<p>Model Fitting Step In the model fitting step, we fit a probabilistic program to data. This requires us to perform (approximate) inference for generic probabilistic models. To accomplish this, we leverage pymc (Abril-Pla et al., 2023), a Python probabilistic programming library. pymc automatically assigns a Markov Chain Monte Carlo (MCMC) sampler to perform inference; by default pymc uses a Hamiltonian Monte Carlo sampler (Homan \&amp; Gelman, 2014). Crucially, by using a probabilistic program, we decouple modeling from inference: the proposal LM's role is to build a good model, while pymc takes care of inference. Our approach of offloading computations to an external tool is connected to recent work enhancing LMs by giving them tools (e.g., Python interpreter) (Gao et al., 2023; Schick et al., 2023).</p>
<p>Model Criticism Step In the criticism step, we ask the critic LM, $p_{L M}$, to produce natural language criticism of fitted models; we use this criticism to drive model revision. First, we can obtain a scalar score measuring the model fit (e.g., ELPD LOO) for each proposed model. Second, to enable the critic LM to do something akin to a posterior predictive check, we obtain samples from the posterior predictive distribution (e.g., $p\left(x^{\prime} \mid z, x\right)=\int_{z} p\left(x^{\prime} \mid z\right) p(z \mid x)$ ) and then compute summary statistics of these posterior predictive samples (Gelman et al., 2013). For simplicity, we compute the posterior predictive means and variances for each fitted probabilistic program. We then provide $p_{L M}$ with select probabilistic programs, their scores $\mathcal{S}$ (e.g., ELPD LOO), the posterior predictive means and variances $\mathcal{P}$, and the dataset itself $\mathcal{D}$; we explore both visual and textual dataframe based representations of the posterior predictive and the dataset. Finally, we ask $p_{L M}$ to distill this criticism in natural language (e.g., von Bertalanffy growth function with informative priors) which we use to drive the next round of model building (Shinn et al., 2023).</p>
<p>The key design choice is which programs to provide to $p_{L M}$ (e.g., critic exemplars). Naively, we could provide all proposed programs across all rounds to $p_{L M}$. However, this list grows each round and has redundancy. Furthermore, LMs struggle to reason over long contexts. We therefore explore a simple approach to selecting exemplars where we provide $p_{L M}$ with the top $d$ programs $\tilde{z}<em d="d">{1}, \ldots \tilde{z}</em>$, we sample:}$ from the current round $t$. To produce $h^{t+1</p>
<p>$$
h^{t+1} \sim p_{\mathrm{LM}}\left(\cdot \mid \tilde{z}<em d="d">{1}, \ldots, \tilde{z}</em>\right)
$$}, \mathcal{D}, \mathcal{S}, \mathcal{P</p>
<p>In the appendix, we report additional results for an approach inspired by a state-space update (Baum \&amp; Petrie, 1966; Kalman, 1960; Rabiner, 1989). To avoid storing the entire history of proposed programs, we interpret $h^{t+1}$ as a latent state and compute it using the previous state $h^{t}$ and the new fitted programs $\left{z_{i}\right}_{i=1}^{m}$ at round $t$ :</p>
<p>$$
h^{t+1} \sim p_{\mathrm{LM}}\left(\cdot \mid z_{1}, \ldots, z_{m}, h^{t}, \mathcal{D}, \mathcal{S}, \mathcal{P}\right)
$$</p>
<p>In practice, we implement this by asking $p_{\mathrm{LM}}$ to add and delete hypotheses. We find that these two approaches have similar performances. We run this criticism step at a temperature of 0.0 to limit stochasticity in the criticism produced.</p>
<p>We refer to our full LM-driven automated Box's loop as BoxLM.</p>
<h2>3. Experiments</h2>
<h3>3.1. Searching over a DSL: automated Gaussian process kernel discovery</h3>
<p>We first evaluate LM's ability to search over a constrained space of models; in some settings, a domain expert may require the modeler to search over a DSL for reasons such as</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>BoxLM+</th>
<th>BoxLM</th>
<th>Periodic</th>
<th>AS</th>
<th>SM</th>
<th>N-BEATS</th>
</tr>
</thead>
<tbody>
<tr>
<td>Air</td>
<td>0.08</td>
<td>0.07</td>
<td>0.15</td>
<td>0.19</td>
<td>$\mathbf{0 . 0 6}$</td>
<td>0.22</td>
</tr>
<tr>
<td>Beer</td>
<td>0.07</td>
<td>0.22</td>
<td>0.15</td>
<td>0.06</td>
<td>$\mathbf{0 . 0 5}$</td>
<td>0.02</td>
</tr>
<tr>
<td>Heart</td>
<td>0.21</td>
<td>0.21</td>
<td>$\mathbf{0 . 2 0}$</td>
<td>0.21</td>
<td>0.21</td>
<td>0.07</td>
</tr>
<tr>
<td>Milk</td>
<td>0.12</td>
<td>0.10</td>
<td>0.10</td>
<td>0.11</td>
<td>$\mathbf{0 . 0 9}$</td>
<td>0.04</td>
</tr>
<tr>
<td>Wine</td>
<td>0.14</td>
<td>0.16</td>
<td>0.21</td>
<td>$\mathbf{0 . 1 3}$</td>
<td>0.18</td>
<td>0.17</td>
</tr>
<tr>
<td>Wool</td>
<td>0.20</td>
<td>0.19</td>
<td>0.19</td>
<td>0.23</td>
<td>$\mathbf{0 . 1 3}$</td>
<td>0.18</td>
</tr>
</tbody>
</table>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Test set performance on time series datasets. Our BoxLM system identifies compositional kernels with performance on par with strong baselines. (left) Comparison of BoxLM test mean absolute error (MAE) against Automatic Statistician using greedy search (AS), spectral mixture kernel (SM), periodic kernel (Periodic), and N-BEATS. BoxLM+ searches over an augmented kernel space. We bold the best and underline the second best among the GP methods, treating N-BEATS as a powerful non-GP-constrained baseline. (right) Extrapolations from GP with a BoxLM-discovered kernel.
interpretability. In particular, we consider time-series modeling with Gaussian processes (GPs) as in the Automatic Statistician (Duvenaud et al., 2013). GPs are probabilistic models that specify a distribution over functions. GPs are defined by a mean function and a positive-definite kernel function $k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)$ that gives the covariance between $f(\mathbf{x})$ and $f\left(\mathbf{x}^{\prime}\right)$ as a function of $\mathbf{x}$ and $\mathbf{x}^{\prime}$. The properties of a GP depend significantly on the kernel and therefore choosing a kernel that reflects domain knowledge (e.g., linearity, periodicity) is a key design choice. One common way to produce a more flexible kernel is to compose kernels via addition and multiplication, leveraging the closure properties of kernels. Here, we evaluate LMs' ability to search over a space of kernels to identify an appropriate composition of kernels.</p>
<p>Setup Mirroring Duvenaud et al. (2013), we define a space of base kernels. In the prompt, we ask $q_{L M}$ to perform one of the three operations (addition, multiplication, and replacement) on one of the in-context exemplar programs. For the first round, we ask $q_{L M}$ to produce an initial guess based on the structure of the dataset, which we provide as a plot. Given a kernel expression, we learn the kernel hyperparameters via gradient-based optimization of the marginal likelihood.</p>
<p>Results We evaluate our method on six common univariate time series datasets. We compare against the Automatic Statistician, a greedy algorithm proposed by Duvenaud et al. (2013) (run until a depth of 10), and two GP baselines: a periodic kernel and a spectral mixture kernel, a strong non-compositional baseline. We also compare against N-BEATS, a strong neural baseline (Wilson \&amp; Adams, 2013; Oreshkin et al., 2019). In Figure 2, we compare the mean absolute error (MAE) on held-out test data for all datasets. To account for stochasticity in the Automatic Statistician implementation we used (Saad et al., 2023) and
stochasticity in different repetitions of our pipeline, we average the test MAE across three model runs. Our method, denoted LM in the table, matches the performance of the Automatic Statistician, showing that BoxLM can efficiently search over a constrained space of models. We also experiment with augmenting the base kernel space with additional kernels (denoted BoxLM+ in the table). In some cases, this additional flexibility is beneficial. For example, by using the additional kernels, BoxLM+ can much better capture the Australian beer sales dataset than LM; in other cases, the additional flexibility does not appreciably improve performance. On the right panel, we show the extrapolations and identified kernel for the monthly air passenger dataset; BoxLM+ identifies a kernel with a periodic times linear component to capture the increasing amplitude in the data.</p>
<h3>3.2. Open-ended probabilistic model discovery for real-world datasets</h3>
<p>By integrating LMs into the model discovery process, we can search over a much broader class of models. Here, we explore BoxLMs' ability to automatically construct pymc probabilistic programs (Abril-Pla et al., 2023) for datasets.
Dataset We consider four real world datasets from the Stan PosteriorDB dataset (Magnusson et al., 2023): (1) a dataset consisting of average improvements in SAT scores after an SAT improvement program across eight different high schools, (2) a dataset consisting of ages of twenty-seven dugongs and their lengths, (3) a surgical dataset consisting of mortality rates in twelve hospitals performing cardiac surgery on babies, and (4) a dataset of peregrine population counts in the French Jura from 1964 to 2003. Each dataset has an associated human expert written probabilistic program in Stan, which we translate into pymc. These expert programs are generally open-source contributions from the Stan developer community. The datasets cover</p>
<p>common modeling motifs, such as hierarchical modeling and regression.</p>
<p>Ablations To study how domain knowledge affects the LM's modeling capabilities and to mitigate concerns about dataset leakage, we consider two ablations. In the first ablation, we construct a simulated analog for each dataset: for each dataset, we generate synthetic observations by sampling from the prior of a generative model or by sampling from a model with fixed parameters. We denote these datasets as simulated datasets. For example, to generate a simulated dataset for the eight schools dataset, we sample from the prior of the human expert program. In some cases, such as the dugongs dataset, the prior distributions over parameters are highly unconstrained and we therefore fix values for the parameters instead. For example, to generate a simulated analog of the dugongs dataset, we fix the values of the parameters $\alpha, \beta, \gamma$ used in the expert model $y_{i}=\alpha-\beta \gamma^{\epsilon_{i}}+\epsilon_{i}$ where $\epsilon_{i} \sim \mathcal{N}\left(0, \sigma^{2}\right)$. Since it is unlikely that these simulated datasets appeared in the training data, this ablation helps mitigate concerns about dataset leakage.</p>
<p>In the second ablation, we remove the dataset metadata from the LM prompt. In particular, we remove the dataset description, replace the column names with domain-agnostic column names (e.g., $x_{0}, y$ ), and replace the axis labels with uninformative labels. We denote these datasets as no metadata datasets. By removing this metadata, we can characterize how LMs use domain knowledge.</p>
<p>Quantitative Results In Table 1, we compare the performance of our method across different datasets and ablations relative to the expert programs. We emphasize that the expert programs are strong baselines, especially for the simulated datasets where the expert programs generated the data. We highlight numbers corresponding to significant differences, where significance is defined as an ELPD LOO difference of greater than four times the standard error estimate.</p>
<p>BoxLM reliably identifies programs on par with expert programs; we validate convergence using standard Markov Chain Monte Carlo diagnostics. Interestingly, removing metadata or switching to simulated datasets does not generally reduce performance relative to the expert program. The main exception is the surgical dataset. By replacing the labels with $x_{0}$ and $y$, BoxLM formulates this problem as a regression problem instead of using a hierarchical model like the expert model. For the peregrine dataset, removing metadata improves performance. We discuss this further in the next section.</p>
<p>Qualitative Analysis: Language Models are Domain Experts Earlier, we asked whether LMs can reliably play the role of a domain expert. In our ablations, we study how
metadata influences the model search process by examining how removing metadata changes the programs proposed by BoxLM. In Figure 3, we plot the model fit and list the corresponding BoxLM programs in the figure caption. For the peregrine dataset, when given all the metadata, BoxLM models this dataset using a logistic growth model, which is motivated by the problem domain. However, logistic growth models cannot capture the initial decline in population. Interestingly, when we remove all metadata, BoxLM performs better, because it uses a quartic polynomial to model the data. This closes the gap in performance with the expert program. This highlights how prior knowledge can have a (sometimes overly) strong influence on the modeling choices of BoxLM. We observe a similar trend with the dugongs growth curve dataset. When given metadata, BoxLM uses a von Bertalanffy growth function (von Bertalanffy, 1949), which is commonly used to model animal growth. When we remove all metadata, BoxLM uses a polynomial function with a logarithmically transformed value for the inputs. Here, both modeling approaches fit the data well. Interestingly, BoxLM sets the prior parameters in these models in a data-informed way. For example, BoxLM sets the asymptotic length in the von Bertanlanffy function based on the observed lengths in the dataset. See code snippet in Figure 7 of the Appendix. For the eight schools dataset, BoxLM identifies a hierarchical model even without metadata, illustrating that model criticism can compensate for lack of domain knowledge. We illustrate the improvement round to round in Figure 6.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">Expert</th>
<th style="text-align: left;">LM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Eight schools</td>
<td style="text-align: left;">$\underline{-30.70}$</td>
<td style="text-align: left;">$\underline{-30.42}$</td>
</tr>
<tr>
<td style="text-align: left;">Eight schools sim</td>
<td style="text-align: left;">$\underline{-18.09}$</td>
<td style="text-align: left;">$\underline{-18.31}$</td>
</tr>
<tr>
<td style="text-align: left;">Eight schools sim no metadata</td>
<td style="text-align: left;">$\underline{-18.09}$</td>
<td style="text-align: left;">$\underline{-16.36}$</td>
</tr>
<tr>
<td style="text-align: left;">Dugongs</td>
<td style="text-align: left;">$\underline{22.52}$</td>
<td style="text-align: left;">$\underline{23.40}$</td>
</tr>
<tr>
<td style="text-align: left;">Dugongs sim</td>
<td style="text-align: left;">$\underline{50.04}$</td>
<td style="text-align: left;">$\underline{57.40}$</td>
</tr>
<tr>
<td style="text-align: left;">Dugongs sim no metadata</td>
<td style="text-align: left;">$\underline{50.04}$</td>
<td style="text-align: left;">$\underline{55.24}$</td>
</tr>
<tr>
<td style="text-align: left;">Surgical</td>
<td style="text-align: left;">$\underline{-40.29}$</td>
<td style="text-align: left;">-38.03</td>
</tr>
<tr>
<td style="text-align: left;">Surgical sim</td>
<td style="text-align: left;">$\underline{-39.80}$</td>
<td style="text-align: left;">$\underline{-38.38}$</td>
</tr>
<tr>
<td style="text-align: left;">Surgical sim no metadata</td>
<td style="text-align: left;">$\mathbf{- 3 9 . 8 0}$</td>
<td style="text-align: left;">-63.72</td>
</tr>
<tr>
<td style="text-align: left;">Peregrine</td>
<td style="text-align: left;">$\mathbf{- 1 4 2 . 1 9}$</td>
<td style="text-align: left;">-173.11</td>
</tr>
<tr>
<td style="text-align: left;">Peregrine sim</td>
<td style="text-align: left;">$\mathbf{- 1 3 0 . 4 8}$</td>
<td style="text-align: left;">-179.06</td>
</tr>
<tr>
<td style="text-align: left;">Peregrine sim no meta</td>
<td style="text-align: left;">$\underline{-130.48}$</td>
<td style="text-align: left;">$\underline{-136.39}$</td>
</tr>
</tbody>
</table>
<p>Table 1. Comparison of BoxLM programs against expert programs We perform this comparison across four different datasets and two different ablations that replace observations with synthetic observations and remove all metadata. We report the expected predictive log density estimated via leave-one-out cross validation. We bold statistically significant differences and underline non-significant differences. LM programs match the performance of human expert programs on 9/12 datasets.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Domain knowledge shapes BoxLM modeling approaches. In the top row, we keep all metadata (e.g., dataset description). In the bottom row, we remove metadata that reveals information about the domain. This leads to qualitatively different approaches for three datasets; for eight schools, BoxLM discovers a hierarchical model even without metadata. We list the corresponding programs for these different ablations: (top row) $\frac{R}{\left(1+\left(\left(K-P_{0}\right) / P_{0}\right) \exp (-r t)\right)}$, $\operatorname{BetaBin}(n, \alpha, \beta), L_{\text {inf }}\left(1-\exp \left(-k\left(\text { age }-t_{0}\right)\right)\right)$; (bottom row) $a+b x_{0}+c x_{0}^{2}+d x_{0}^{3}+e x_{0}^{4}, \alpha+\beta x_{0}+\gamma x_{0}^{2}, \alpha+\beta_{1} \log x_{0}+\beta_{2} \log x_{0}^{2}+\beta_{3} \log x_{0}^{2}$.</p>
<h3>3.3. Improving classic models under modeling constraints</h3>
<p>In the previous experiment, we explored BoxLM's ability to identify models tabula rasa (e.g., without any initial seed model). However, in many scientific settings, we begin with a well-known model and are tasked with improving it. Here, we explore BoxLM's ability to improve upon a Lotka-Volterra model of predator-prey dynamics. In addition, a crucial component of model discovery is respecting "soft" modeling constraints that are easy to express in natural language but hard to formalize (e.g., ecologists should think this is a plausible model). We therefore illustrate another advantage of BoxLM: we can express these modeling constraints in natural language and use them to drive LM proposals.</p>
<p>Dataset To create our dataset, we simulate data from the following "perturbed" Lotka-Volterra dynamics</p>
<p>$$
\begin{aligned}
&amp; \frac{d b}{d t}=\alpha b-\beta b c \
&amp; \frac{d c}{d t}=-\gamma c+\delta b c^{0.95}
\end{aligned}
$$</p>
<p>Setup BoxLM is tasked with implementing an ODE model in Python using Jax (Bradbury et al., 2018). Estimating the parameters of Lotka-Volterra models via Bayesian inference is challenging. We instead learn the parameters via gradient descent which can be straightforwardly implemented using modern automatic differentiation libraries (Chen et al., 2018; Kidger, 2021); in particular, we use diffrax (Kidger, 2021), a Jax-based ODE library that supports learning ODE parameters via backpropagation. We consider three variations: warm-start with constraints (WS-Constraint), warm-start with no constraints (WS-No Constraint), and no warm-start (No-WS) that differ in their initial seed program and the initial instructions which express modeling
constraints. In all variations, we provide the LM with a scatter plot of training datapoints. In the no warm-start variation, we provide the LM with an implementation of standard Lotka-Volterra dynamics using diffrax and the predictions obtained from fitting standard Lotka-Volterra to the training data. In both warm-start variations, we provide the LM with (1) an implementation of a hybrid neural ODE that introduces an additive correction term to the predator dynamics, parameterized by a multilayer perceptron (MLP), and (2) the predictions obtained from fitting this model to the training data. In the constrained warm-start variation, we ask the LM to produce a hybrid neural ODE model that is interpretable to an ecology expert who suggested a Holling's type II response (Rosenzweig \&amp; MacArthur, 1963). In the unconstrained warm-start variation, we provide the same seed program but do not impose this additional interpretability constraint. For models with both neural and physical components, we employ a two-stage learning procedure so that the neural component does not dominate the dynamics; see Appendix C for details.</p>
<p>Results In Figure 4 (left), we plot the predictions obtained from integrating the learned dynamics for an ODE proposed by BoxLM, the training data points generated from the true dynamics, and the predictions from the standard Lotka-Volterra model. We fit free parameters to the training data via gradient descent. The grey region indicates the extrapolation region.</p>
<p>BoxLM can significantly improve upon the standard LotkaVolterra model by introducing corrections to the dynamics (Figure 4). The standard Lotka-Volterra model cannot capture the decreasing amplitude in the data; furthermore, there is a slight phase shift relative to the training datapoints. In contrast, BoxLM identifies an ODE that captures these properties and extrapolates accurately. In Figure 4 (right), we compare these programs against a neural ODE base-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Correcting misspecified Lotka-Volterra dynamics. BoxLM can introduce corrections to standard Lotka-Volterra dynamics (no warm-start) and a hybrid neural ODE approach (warm-start) that outperform several baselines. (left) LM-proposed model predictions on training data and extrapolations (grey region). (right) Test MAE of LM models (No-WS, WS-Constraint, and WS-No Constraint) compared to the standard Lotka-Volterra model LV, a Neural ODE, and a hybrid Neural ODE model with a multiplicative correction to the prey-predator dynamics (Hybrid).</p>
<div class="codehilite"><pre><span></span><code><span class="n">LM</span><span class="w"> </span><span class="n">proposed</span><span class="w"> </span><span class="n">Lotka</span><span class="o">-</span><span class="n">Volterra</span><span class="w"> </span><span class="n">programs</span>
<span class="n">def</span><span class="w"> </span><span class="n">no_warm_start</span><span class="p">(</span><span class="n">t</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">_coeffs</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">Hypothesis</span><span class="w"> </span><span class="mi">1</span><span class="err">:</span><span class="w"> </span><span class="n">Logistic</span><span class="w"> </span><span class="n">growth</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">prey</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">carrying</span><span class="w"> </span><span class="n">capacity</span><span class="w"> </span><span class="n">kappa</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">Hypothesis</span><span class="w"> </span><span class="mi">2</span><span class="err">:</span><span class="w"> </span><span class="n">Saturation</span><span class="w"> </span><span class="n">effect</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">predation</span><span class="w"> </span><span class="n">rate</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="k">parameter</span><span class="w"> </span><span class="n">psi</span>
<span class="w">    </span><span class="n">db_dt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">kappa</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">beta</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">psi</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="p">)</span>
<span class="w">    </span><span class="n">dc_dt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="n">gamma</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">delta</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">epsilon</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">c</span><span class="o">**</span><span class="mi">2</span>
<span class="n">def</span><span class="w"> </span><span class="n">warm_start_constrained</span><span class="p">(</span><span class="n">t</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">_mlp</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">jnp</span><span class="p">.</span><span class="k">array</span><span class="p">(</span><span class="o">[</span><span class="n">b</span><span class="o">]</span><span class="p">)</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="k">Input</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">prey</span><span class="w"> </span><span class="n">density</span><span class="w"> </span><span class="p">(</span><span class="n">bunny</span><span class="w"> </span><span class="n">population</span><span class="p">)</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">Neural</span><span class="w"> </span><span class="n">network</span><span class="w"> </span><span class="n">learns</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">modifier</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">handling</span><span class="w"> </span><span class="nc">time</span><span class="w"> </span><span class="n">based</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">prey</span><span class="w"> </span><span class="n">density</span>
<span class="w">    </span><span class="n">handling_time_modifier</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_mlp</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">Modulate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">predation</span><span class="w"> </span><span class="n">rate</span><span class="w"> </span><span class="n">beta</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">non</span><span class="o">-</span><span class="n">linear</span><span class="w"> </span><span class="n">manner</span><span class="w"> </span><span class="n">according</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">handling</span>
<span class="w">        </span><span class="nc">time</span>
<span class="w">    </span><span class="n">db_dt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">beta</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">handling_time_modifier</span><span class="o">[</span><span class="n">0</span><span class="o">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="p">)</span>
<span class="w">    </span><span class="n">dc_dt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="n">gamma</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">delta</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">handling_time_modifier</span><span class="o">[</span><span class="n">0</span><span class="o">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="p">)</span>
<span class="n">def</span><span class="w"> </span><span class="n">warm_start_unconstrained</span><span class="p">(</span><span class="n">t</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">_mlp</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">jfu</span><span class="p">.</span><span class="n">ravel_pytree</span><span class="p">((</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">c</span><span class="p">))</span><span class="o">[</span><span class="n">0</span><span class="o">]</span>
<span class="w">    </span><span class="n">mlp_output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_mlp</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">Fine</span><span class="o">-</span><span class="n">tuned</span><span class="w"> </span><span class="n">scaling</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">MLP</span><span class="w"> </span><span class="k">output</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="k">match</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">amplitude</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="k">data</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="n">closely</span>
<span class="w">    </span><span class="n">db_dt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">beta</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mf">0.02</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">mlp_output</span><span class="o">[</span><span class="n">0</span><span class="o">]</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">Reduced</span><span class="w"> </span><span class="n">scaling</span><span class="w"> </span><span class="k">for</span>
<span class="w">        </span><span class="n">bunnies</span>
<span class="w">    </span><span class="n">dc_dt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="n">gamma</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">delta</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mf">0.06</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">mlp_output</span><span class="o">[</span><span class="n">1</span><span class="o">]</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">Increased</span><span class="w"> </span><span class="n">scaling</span><span class="w"> </span><span class="k">for</span>
<span class="w">        </span><span class="n">cats</span>
</code></pre></div>

<p>Figure 5. BoxLM can propose corrections to ODEs. (top) In the no warm-start (No-WS) variation, BoxLM introduces corrections informed by domain knowledge of predator-prey models (carrying capacity, predation saturation). (middle) When prompted to introduce neural networks in an interpretable way (WS-Constraint), one strategy BoxLM proposes is to make the handling time parameter depend non-linearly on the prey density, extending a traditional approach to modeling predation saturation. (bottom) When prompted to introduce neural networks without constraints (WS-No Constraint), BoxLM introduces additive MLP-parameterized corrections and adjusts the scaling factors.
line, and a hybrid neural ODE baseline that incorporates a multiplicative correction (parameterized by an MLP) to the predator-prey interaction term in the predator equation. See Section C of the Appendix for details on these baselines. For the LM variations, we report the average test MAE across three runs. In Figure 4, we see that all BoxLM variations outperform the baselines.</p>
<p>In Figure 5, we present code snippets corresponding to representative programs proposed in the constrained and unconstrained variations. These snippets show how natural language constraints can guide BoxLM towards more flexible models that retain interpretability. For the variation with no natural language constraints (WS-No Constraint), BoxLM takes a purely empirical approach. In particular, BoxLM adjusts the scaling terms on the additive MLP correction</p>
<p>term. For the WS-Constraint variation, BoxLM proposes a hybrid approach integrating the neural approach in the prompt with classic models in the literature; importantly, even though BoxLM is asked to balance interpretability and flexibility, BoxLM still identifies programs that outperform the neural ODE and standard Lotka-Volterra baselines. One approach BoxLM proposes is an extension of the Rosenzweig and MacArthur model with a Holling's type II functional response (Rosenzweig \&amp; MacArthur, 1963) to allow a static parameter to depend dynamically on the prey density: BoxLM models the handling time, or the time a predator spends "processing" a prey, as a nonlinear function of the prey density via an MLP. These results show how we can use natural language to drive BoxLM towards models that balance flexibility and interpretability.</p>
<h2>4. Conclusion</h2>
<p>We introduced a method for leveraging LMs for automated model discovery. Our method can identify models that perform favorably against strong baselines and improve upon expert models. We also studied how domain knowledge and natural language constraints influence our system. Altogether, our results highlight the compelling advantages of LM-driven statistical model discovery.</p>
<p>Our work has important limitations that motivate future research. First, we focused on modeling static datasets. An interesting direction could be leveraging LMs for active data collection. Second, since our tasks were restricted to onedimensional datasets, simple model criticism statistics were sufficient and therefore decided in advance (residuals, posterior predictive mean). Another interesting future direction could be fully automating the criticism step. Finally, while in-context learning was effective in our tasks, we could explore finetuning techniques for training a language model to produce better probabilistic programs.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6. Round-to-round improvement. Model revision leads to improvements on average. (Improvement is not necessarily monotonic for a given dataset and run.) (top) ELPD LOO score vs. round for Stan experiments. We normalize the ELPD LOO scores across programs proposed for 3 rounds of Box's loop and average across datasets for the no metadata condition. Larger is better and error bars correspond to standard error. (bottom) Squared error vs round for LV experiment. We report the squared error averaged across three different runs, for the warm-start, no constraint condition; smaller is better.</p>
<h2>Acknowledgements</h2>
<p>This work was supported in part by AFOSR Grant FA9550-21-1-0397, ONR Grant N00014-22-1-2110, and an NSF Expeditions Grant, Award Number (FAIN) 1918771. EBF is a Chan Zuckerberg Biohub - San Francisco Investigator.
We thank Eric Zelikman and Neil Band for detailed feedback on this paper. We also thank Omar Shaikh and Jensen Gao for helpful discussions.</p>
<h2>Impact Statement</h2>
<p>This paper presents work whose goal is to partially automate statistical modeling. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.</p>
<h2>References</h2>
<p>Abril-Pla, O., Andreani, V., Carroll, C., Dong, L., Fonnesbeck, C. J., Kochurov, M., Kumar, R., Lao, J., Luhmann, C. C., Martin, O. A., Osthege, M., Vieira, R., Wiecki, T., and Zinkov, R. PyMC: a modern, and comprehensive probabilistic programming framework in python. PeerJ Computer Science, 9, 2023.</p>
<p>Achiam, O. J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., Avila, R., Babuschkin, I., Balaji, S., Balcom, V., Baltescu, P., Bao, H., Bavarian, M., Belgum, J., Bello, I., Berdine, J., Bernadett-Shapiro, G., Berner, C., Bogdonoff, L., Boiko, O., Boyd, M., Brakman, A.-L., Brockman, G., Brooks, T., Brundage, M., Button, K., Cai, T., Campbell, R., Cann, A., Carey, B., Carlson, C., Carmichael, R., Chan, B., Chang, C., Chantzis, F., Chen, D., Chen, S., Chen, R., Chen, J., Chen, M., Chess, B., Cho, C., Chu, C., Chung, H. W., Cummings, D., Currier, J., Dai, Y., Decareaux, C., Degry, T., Deutsch, N., Deville, D., Dhar, A., Dohan, D., Dowling, S., Dunning, S., Ecoffet, A., Eleti, A., Eloundou, T., Farhi, D., Fedus, L., Felix, N., Fishman, S. P., Forte, J., Fulford, I., Gao, L., Georges, E., Gibson, C., Goel, V., Gogineni, T., Goh, G., Gontijo-Lopes, R., Gordon, J., Grafstein, M., Gray, S., Greene, R., Gross, J., Gu, S. S., Guo, Y., Hallacy, C., Han, J., Harris, J., He, Y., Heaton, M., Heidecke, J., Hesse, C., Hickey, A., Hickey, W., Hoeschele, P., Houghton, B., Hsu, K., Hu, S., Hu, X., Huizinga, J., Jain, S., Jain, S., Jang, J., Jiang, A., Jiang, R., Jin, H., Jin, D., Jomoto, S., Jonn, B., Jun, H., Kaftan, T., Kaiser, L., Kamali, A., Kanitscheider, I., Keskar, N. S., Khan, T., Kilpatrick, L., Kim, J. W., Kim, C., Kim, Y., Kirchner, H., Kiros, J. R., Knight, M., Kokotajlo, D., Kondraciuk, L., Kondrich, A., Konstantinidis, A., Kosic, K., Krueger, G., Kuo, V., Lampe, M., Lan, I., Lee, T., Leike, J., Leung, J., Levy, D., Li, C. M., Lim, R., Lin, M., Lin, S., Litwin, M., Lopez, T., Lowe, R., Lue, P., Makanju, A. A., Malfacini, K., Manning, S., Markov, T., Markovski, Y., Martin, B., Mayer, K., Mayne, A., McGrew, B., McKinney, S. M., McLeavey, C., McMillan, P., McNeil, J., Medina, D., Mehta, A., Menick, J., Metz, L., Mishchenko, A., Mishkin, P., Monaco, V., Morikawa, E., Mossing, D. P., Mu, T., Murati, M., Murk, O., M'ely, D., Nair, A., Nakano, R., Nayak, R., Neelakantan, A., Ngo, R., Noh, H., Long, O., O’Keefe, C., Pachocki, J. W., Paino, A., Palermo, J., Pantuliano, A., Parascandolo, G., Parish, J., Parparita, E., Passos, A., Pavlov, M., Peng, A., Perelman, A., de Avila Belbute Peres, F., Petrov, M., de Oliveira Pinto, H. P., Pokorny, M., Pokrass, M., Pong, V. H., Powell, T., Power, A., Power, B., Proehl, E., Puri, R., Radford, A., Rae, J., Ramesh, A., Raymond, C., Real, F., Rimbach, K., Ross, C., Rotsted, B., Roussez, H., Ryder, N., Saltarelli, M. D., Sanders, T., Santurkar, S., Sastry, G., Schmidt, H., Schnurr, D., Schulman, J., Selsam, D.,</p>
<p>Sheppard, K., Sherbakov, T., Shieh, J., Shoker, S., Shyam, P., Sidor, S., Sigler, E., Simens, M., Sitkin, J., Slama, K., Sohl, I., Sokolowsky, B. D., Song, Y., Staudacher, N., Such, F. P., Summers, N., Sutskever, I., Tang, J., Tezak, N. A., Thompson, M., Tillet, P., Tootoonchian, A., Tseng, E., Tuggle, P., Turley, N., Tworek, J., Uribe, J. F. C., Vallone, A., Vijayvergiya, A., Voss, C., Wainwright, C., Wang, J. J., Wang, A., Wang, B., Ward, J., Wei, J., Weinmann, C., Welihinda, A., Welinder, P., Weng, J., Weng, L., Wiethoff, M., Willner, D., Winter, C., Wolrich, S., Wong, H., Workman, L., Wu, S., Wu, J., Wu, M., Xiao, K., Xu, T., Yoo, S., Yu, K., Yuan, Q., Zaremba, W., Zellers, R., Zhang, C., Zhang, M., Zhao, S., Zheng, T., Zhuang, J., Zhuk, W., and Zoph, B. GPT-4 Technical Report. 2023.</p>
<p>Baum, L. E. and Petrie, T. Statistical inference for probabilistic functions of finite state markov chains. The annals of mathematical statistics, 37(6):1554-1563, 1966.</p>
<p>Blei, D. M. Build, compute, critique, repeat: Data analysis with latent variable models. Annual Review of Statistics and Its Application, 1(1):203-232, 2014.</p>
<p>Bommarito, M. J. and Katz, D. M. GPT Takes the Bar Exam. ArXiv, abs/2212.14402, 2022.</p>
<p>Bongard, J. C. and Lipson, H. Automated reverse engineering of nonlinear dynamical systems. Proceedings of the National Academy of Sciences, 104:9943 - 9948, 2007.</p>
<p>Box, G. E. P. and Hunter, W. G. A useful method for modelbuilding. Technometrics, 4:301-318, 1962.</p>
<p>Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.</p>
<p>Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS'20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.</p>
<p>Chen, M., Tworek, J., Jun, H., Yuan, Q., Ponde, H., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D. W., Plappert, M.,</p>
<p>Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Babuschkin, I., Balaji, S., Jain, S., Carr, A., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M. M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code. ArXiv, abs/2107.03374, 2021.</p>
<p>Chen, R. T. Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D. Neural ordinary differential equations. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS'18, pp. 6572-6583, Red Hook, NY, USA, 2018. Curran Associates Inc.</p>
<p>Duvenaud, D., Lloyd, J., Grosse, R., Tenenbaum, J., and Zoubin, G. Structure discovery in nonparametric regression through compositional kernel search. In Dasgupta, S. and McAllester, D. (eds.), Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pp. 11661174, Atlanta, Georgia, USA, 17-19 Jun 2013. PMLR.</p>
<p>Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., and Neubig, G. PAL: program-aided language models. In Proceedings of the 40th International Conference on Machine Learning, ICML'23. JMLR.org, 2023.</p>
<p>Gelman, A. and Rubin, D. B. Inference from iterative simulation using multiple sequences. Statistical Science, 7(4): 457-472, 1992. ISSN 08834237.</p>
<p>Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., and Rubin, D. B. Bayesian data analysis, third edition. 2013.</p>
<p>Goodman, N. D., Mansinghka, V. K., Roy, D. M., Bonawitz, K., and Tenenbaum, J. B. Church: a language for generative models. In Conference on Uncertainty in Artificial Intelligence, 2008.</p>
<p>Grosse, R. B. Model selection in compositional spaces. 2014.</p>
<p>Homan, M. D. and Gelman, A. The no-u-turn sampler: adaptively setting path lengths in hamiltonian monte carlo. J. Mach. Learn. Res., 15(1):1593-1623, jan 2014. ISSN $1532-4435$.</p>
<p>Kalman, R. E. A new approach to linear filtering and prediction problems. ASME Journal of Basic Engineering, 82(1):35-45, March 1960.</p>
<p>Kidger, P. On Neural Differential Equations. PhD thesis, University of Oxford, 2021.</p>
<p>Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. In Oh,
A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022.</p>
<p>Lee, P., Bubeck, S., and Petro, J. Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine. New England Journal of Medicine, 388(13):1233-1239, 2023. doi: 10.1056/NEJMsr2214184. PMID: 36988602.</p>
<p>Linka, K., St. Pierre, S. R., and Kuhl, E. Automated model discovery for human brain using constitutive artificial neural networks. Acta Biomaterialia, 160:134-151, 2023. ISSN 1742-7061. doi: https://doi.org/10.1016/j.actbio. 2023.01.055.</p>
<p>Lloyd, J. R., Duvenaud, D. K., Grosse, R. B., Tenenbaum, J. B., and Ghahramani, Z. Automatic construction and natural-language description of nonparametric regression models. In AAAI Conference on Artificial Intelligence, 2014.</p>
<p>Magnusson, M., Bürkner, P., and Vehtari, A. posteriordb: a set of posteriors for Bayesian inference and probabilistic programming, October 2023.</p>
<p>McKinney, B. A., Crowe, J. E., Voss, H. U., Crooke, P. S., Barney, N., and Moore, J. H. Hybrid grammar-based approach to nonlinear dynamical system identification from biological time series. Phys. Rev. E, 73:021912, Feb 2006. doi: 10.1103/PhysRevE.73.021912.</p>
<p>Miller, A. C., Foti, N. J., and Fox, E. Learning insulinglucose dynamics in the wild. In Proceedings of the 5th Machine Learning for Healthcare Conference, volume 126 of Proceedings of Machine Learning Research, pp. 172-197. PMLR, 07-08 Aug 2020.</p>
<p>Oreshkin, B. N., Carpov, D., Chapados, N., and Bengio, Y. N-beats: Neural basis expansion analysis for interpretable time series forecasting. ArXiv, abs/1905.10437, 2019.</p>
<p>Qiu, L., Jiang, L., Lu, X., Sclar, M., Pyatkin, V., Bhagavatula, C., Wang, B., Kim, Y., Choi, Y., Dziri, N., and Ren, X. Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. In The Twelfth International Conference on Learning Representations, 2024.</p>
<p>Rabiner, L. A tutorial on hidden markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257-286, 1989. doi: 10.1109/5.18626.</p>
<p>Rosenzweig, M. and MacArthur, R. Graphical representation and stability conditions of predator-prey interaction. American Naturalist, 97:209-223, 1963.</p>
<p>Rozière, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X., Adi, Y., Liu, J., Remez, T., Rapin, J., Kozhevnikov, A., Evtimov, I., Bitton, J., Bhatt, M. P., Ferrer, C. C.,</p>
<p>Grattafiori, A., Xiong, W., D’efossez, A., Copet, J., Azhar, F., Touvron, H., Martin, L., Usunier, N., Scialom, T., and Synnaeve, G. Code Llama: Open Foundation Models for Code. ArXiv, abs/2308.12950, 2023.</p>
<p>Saad, F. A., Patton, B., Hoffman, M., Saurous, R. A., and Mansinghka, V. K. Sequential monte carlo learning for time series structure discovery. In International Conference on Machine Learning, 2023.</p>
<p>Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., and Scialom, T. Toolformer: Language models can teach themselves to use tools. ArXiv, abs/2302.04761, 2023.</p>
<p>Schmidt, M. and Lipson, H. Distilling free-form natural laws from experimental data. Science, 324(5923):81-85, 2009. doi: 10.1126/science. 1165893.</p>
<p>Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K. R., and Yao, S. Reflexion: language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.
van de Meent, J.-W., Paige, B., Yang, H., and Wood, F. An introduction to probabilistic programming, 2021.</p>
<p>Vehtari, A., Gelman, A., and Gabry, J. Practical Bayesian model evaluation using leave-one-out crossvalidation and WAIC. Statistics and Computing, 27(5): 1413-1432, sep 2017. ISSN 0960-3174. doi: 10.1007/ s11222-016-9696-4.
von Bertalanffy, L. Problems of organic growth. Nature, 163:156-158, 1949.</p>
<p>Wang, R., Zelikman, E., Poesia, G., Pu, Y., Haber, N., and Goodman, N. D. Hypothesis search: Inductive reasoning with language models. In The Twelfth International Conference on Learning Representations, 2024.</p>
<p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., brian ichter, Xia, F., Chi, E. H., Le, Q. V., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022.</p>
<p>Wilson, A. G. and Adams, R. P. Gaussian process kernels for pattern discovery and extrapolation. In Proc. ICML, 2013.</p>
<p>Wong, L. S., Grand, G., Lew, A. K., Goodman, N. D., Mansinghka, V. K., Andreas, J., and Tenenbaum, J. B. From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought. ArXiv, abs/2306.12672, 2023.</p>
<p>Wood, F., van de Meent, J. W., and Mansinghka, V. A new approach to probabilistic programming inference. In Proceedings of the 17th International conference on Artificial Intelligence and Statistics, pp. 1024-1032, 2014.</p>
<p>Wu, Y., Jia, F., Zhang, S., Li, H., Zhu, E., Wang, Y., Lee, Y. T., Peng, R., Wu, Q., and Wang, C. An Empirical Study on Challenging Math Problem Solving with GPT-4, 2023.</p>
<p>Zhong, R., Zhang, P., Li, S., Ahn, J., Klein, D., and Steinhardt, J. Goal driven discovery of distributional differences via language descriptions. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.</p>
<h1>A. Gaussian Process experiments</h1>
<p>In the prompt, we ask the LM to use the following operations.</p>
<ol>
<li>Replace a subexpression $\mathcal{S}$ with $\mathcal{S}+\mathcal{B}$, where $\mathcal{B}$ is any base kernel.</li>
<li>Replace a subexpression $\mathcal{S}$ with $\mathcal{S} x \mathcal{B}$, where $\mathcal{B}$ is any base kernel.</li>
<li>Any base kernel $\mathcal{B}$ may be replaced with any other base kernel family.</li>
</ol>
<p>LM hyperparameters We provide the LM with the following kernels: Exponentiated Quadratic, Periodic, Linear, and Polynomial. We run our pipeline for two rounds with three proposals each round. We use a temperature of 0.2 for the Proposal LM and temperature of 0.0 for the Critic LM. We use three in-context exemplars. Our Critic LM conditions on the best twelve programs so far.</p>
<p>In the augmented variation, we also provide the LM with the following additional kernels: Matern32, Matern52, Cosine, and the Rational Quadratic kernel. In the augumented variation, we run our pipeline for three rounds with eight proposals each round. We use a temperature of 0.7 for the Proposal LM and temperature of 0.0 for the Critic LM. We use three in-context exemplars. Our Critic LM conditions on the best twelve programs so far.</p>
<p>The marginal likelihood can be multimodal in the parameters of the periodic kernel. Therefore, following Duvenaud et al. (2013), if the proposed kernel has periodic components, we initialize the period at five different initial values, optimize the marginal likelihood starting from those different initializations, and choose the kernel hyperparameters with the highest marginal likelihood across those initializations.</p>
<p>Spectral Mixture kernel We use a GP with a spectral mixture kernel (Wilson \&amp; Adams, 2013) with 5 mixture components. For each dataset, we randomly initialize the parameters of the mixture and choose the kernel hyperparameters with the highest log marginal likelihood across five random initializations.</p>
<h2>B. Stan Experiments</h2>
<p>Eight Schools Dataset This dataset consists of eight observations: the estimated treatment effect of a SAT coaching program and the standard error of the treatment effect.</p>
<p>Peregrine dataset This dataset consists of peregrine population counts in the French Jura from 1964 to 2003 (40 observations in total).</p>
<p>Dugongs Dataset The ages and lengths of 27 captured dugongs (sea cows).
Surgical Dataset The mortality rates in 12 hospitals performing cardiac surgery on babies.
LM hyperparameters We run our pipeline for three rounds with eight proposals each round. We use a temperature of 0.7 for the Proposal LM and temperature of 0.0 for the Critic LM. We use three in-context exemplars. Our Critic LM conditions on the best twelve programs so far.</p>
<p>Markov Chain Monte Carlo diagnostics We evaluated the fidelity of the learned posteriors using the Gelman-Rubin $\hat{R}$ diagnostic (Gelman \&amp; Rubin, 1992) and by examining the Bulk Effective Sample Size (ESS). In particular, the programs reported in the table all had $\hat{R} \leq 1.01$ and mean bulk ESS $&gt;=400$ per chain.</p>
<h2>C. Lotka-Volterra</h2>
<p>Dataset To create our dataset, we simulate data from the following "perturbed" Lotka-Volterra dynamics</p>
<p>$$
\begin{aligned}
&amp; \frac{d b}{d t}=\alpha b-\beta b c \
&amp; \frac{d c}{d t}=-\gamma c+\delta b c^{0.95}
\end{aligned}
$$</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7. LM proposes programs informed by domain knowledge LM chooses a model informed by the domain (animal length vs age) and sets the priors based on the dataset (e.g., the largest length in the dataset is smaller than 3).</p>
<p>We set $\alpha=0.9, \beta=1.1, \delta=-1.2, \gamma=2.1$. The parameter $\alpha$ characterizes the prey's maximum growth rate and $\beta$ controls how the predator population modulate the growth rate. The parameter $\gamma$ characterizes the prey's maximum death rate and $\delta$ controls how the predator's growth rate depends on the prey population density. In contrast to the standard Lotka-Volterra dynamics, we raise $c$ to a fractional power.
We now describe the various baselines we compare against in Section 3.3.
Standard Lotka-Volterra We fit the free parameters of the standard Lotkva-Volterra differential equations.</p>
<p>$$
\begin{aligned}
&amp; \frac{d b}{d t}=\alpha b-\beta b c \
&amp; \frac{d c}{d t}=-\gamma c+\delta b c
\end{aligned}
$$</p>
<p>Neural ODE baseline We parameterize the predator and prey equations with an MLP. We run a hyperparameter search over four widths $(4,8,16,32)$ and 3 depths $(1,2,4)$. We use a learning rate of $3 \mathrm{e}-3$ and train using full-batch gradient descent with Adam for 1500 iterations.</p>
<p>Hybrid Neural ODE baseline We implement a Hybrid Neural ODE baseline that introduces a correction to the predatorprey interaction term. Note that, we follow a two-stage "boosting" type procedure to fit the parameters of the MLP. First, we fit the free parameters $\alpha, \beta, \gamma, \delta$ to the data. We then freeze those parameters and fit the MLP parameters. Without this two-staged approach, the MLP term can dominate the dynamics. The MLP term has one layer and four hidden units and we train the MLP with full batch gradient descent with Adam using a learning rate of 3e-3.</p>
<p>$$
\begin{aligned}
&amp; \frac{d b}{d t}=\alpha \cdot b-\beta \cdot b \cdot c \
&amp; \frac{d c}{d t}=-\gamma \cdot c+\delta \cdot b \cdot(c+0.1 \cdot \operatorname{mlp}(b, c))
\end{aligned}
$$</p>
<p>In the warm-start variations, we provide the LM with an initial hybrid Neural ODE baseline that introduces an additive correction to prey equation. The MLP term has one layer and four hidden units.</p>
<p>$$
\begin{aligned}
&amp; \frac{d b}{d t}=\alpha \cdot b-\beta \cdot b \cdot c+0.1 \cdot \operatorname{mlp}(b, c) \
&amp; \frac{d c}{d t}=-\gamma \cdot c+\delta \cdot b \cdot c
\end{aligned}
$$</p>
<p>LM hyperparameters We run our pipeline for four rounds with twelve proposals each round. We use a temperature of 0.7 for the Proposal LM and temperature of 0.0 for the Critic LM. We use three in-context exemplars. Our Critic LM conditions on the best twelve programs so far.</p>
<h1>D. Failure rates of GPT-4 V proposed programs</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Percent successfully scored</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4 textual dataframe</td>
<td style="text-align: left;">$78 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 vision only</td>
<td style="text-align: left;">$70 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: left;">$76 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2. Percentage of pymc programs that we can successfully perform inference in for Stan experiments.</p>
<h2>E. Additional ODE results</h2>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8. Modeling nonlinear ODEs. LM can introduce polynomial corrections to simple harmonic oscillator (SHO) that provide a better fit. Grey region indicates extrapolation region.</p>
<p>We evaluated on three additional ODEs (nonlinear oscillators) from this github repository pysindy. These ODEs include a Duffing ODE, Van Der Pol oscillator, and a Cubic Harmonic Damped oscillator. We give BoxLM a simple harmonic oscillator to start and ask it to introduce polynomial corrections to improve the fit. Consistent with Section 3.3, our approach can generally improve upon a baseline ODE and extrapolate accurately into the test region.</p>
<h2>F. Visual Interface/GPT-3.5 Ablations</h2>
<p>Context length limits are a potential limitation if we provide the dataset in textual format in the prompt. We therefore experiment with a visual-only variation. We remove all textual representations of datasets and model criticism statistics from the prompt and only provide visual plots of these datasets and statistics. We show this visual-only variation does not harm the performance relative to textual variation and should not suffer as much from context length limits as the dataset grows larger. We also show we can obtain similar results with GPT-3.5 Turbo, which is significantly less costly than GPT-4.</p>
<h2>G. State-space model hidden state update experiments</h2>
<p>We present the same results from the main text but take a state-space update inspired approach to computing the natural language criticism $h^{t}$.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>GPT-4 Text</th>
<th>GPT-4 Visual Only</th>
<th>GPT-3.5</th>
<th>Expert</th>
</tr>
</thead>
<tbody>
<tr>
<td>Eight schools</td>
<td>-30.17</td>
<td>-30.40</td>
<td>-30.44</td>
<td>-30.70</td>
</tr>
<tr>
<td>Dugongs</td>
<td>22.61</td>
<td>23.76</td>
<td>21.01</td>
<td>22.52</td>
</tr>
<tr>
<td>Surgical</td>
<td>-37.36</td>
<td>-38.54</td>
<td>-42.2</td>
<td>-40.29</td>
</tr>
<tr>
<td>Peregrine</td>
<td>-164.69</td>
<td>-143.45</td>
<td>-161.14</td>
<td>-142.19</td>
</tr>
</tbody>
</table>
<p>Table 3. Vision interface and model type ablations. Comparison of GPT-4 with textual representation of data and model criticism statistics in prompt, GPT-4 with only visual representations (GPT-4 Visual Only) of data and model criticism statistics (e.g., only plots), and GPT 3.5 Turbo against expert programs. The visual-only variation (GPT-4 Visual-Only) performs comparably to the textual variation (GPT-4 Text )and outperforms the textual variation on the Peregrine dataset. GPT-3.5 performs slightly worse on some datasets but comparably on most. We report the expected predictive log density (LOO).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">BoxLM+ State Space</th>
<th style="text-align: center;">BoxLM State Space</th>
<th style="text-align: center;">Periodic</th>
<th style="text-align: center;">AS</th>
<th style="text-align: center;">SM</th>
<th style="text-align: center;">N-BEATS</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Air</td>
<td style="text-align: center;">$\mathbf{0 . 0 4}$</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">$\underline{0.06}$</td>
<td style="text-align: center;">0.22</td>
</tr>
<tr>
<td style="text-align: left;">Beer</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">$\underline{0.06}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 5}$</td>
<td style="text-align: center;">0.02</td>
</tr>
<tr>
<td style="text-align: left;">Heart</td>
<td style="text-align: center;">$\mathbf{0 . 2 0}$</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">$\mathbf{0 . 2 0}$</td>
<td style="text-align: center;">$\underline{0.21}$</td>
<td style="text-align: center;">$\underline{0.21}$</td>
<td style="text-align: center;">0.07</td>
</tr>
<tr>
<td style="text-align: left;">Milk</td>
<td style="text-align: center;">$\mathbf{0 . 0 6}$</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">$\underline{0.10}$</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">$\underline{0.09}$</td>
<td style="text-align: center;">0.04</td>
</tr>
<tr>
<td style="text-align: left;">Wine</td>
<td style="text-align: center;">$\underline{0.17}$</td>
<td style="text-align: center;">$\underline{0.17}$</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">$\mathbf{0 . 1 3}$</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.17</td>
</tr>
<tr>
<td style="text-align: left;">Wool</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">$\underline{0.15}$</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">$\mathbf{0 . 1 3}$</td>
<td style="text-align: center;">0.18</td>
</tr>
</tbody>
</table>
<p>Figure 9. Test set performance of BoxLM with state-space updated on time series datasets. Performance of BoxLM system with state space update for model criticism. Comparison of BoxLM test mean absolute error (MAE) against Automatic Statistician using greedy search (AS), spectral mixture kernel (SM), periodic kernel (Periodic), and N-BEATS. BoxLM+ searches over an augmented kernel space. We bold the best and underline the second best among the GP methods.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 10. Correcting misspecified Lotka-Volterra dynamics (BoxLM with state-space updates). BoxLM can introduce corrections to standard Lotka-Volterra dynamics (no warm-start) and a hybrid neural ODE approach (warm-start) that outperform several baselines. Test MAE of LM models (No-WS, WS-Constraint, and WS-No Constraint) compared to the standard Lotka-Volterra model LV, a Neural ODE, and a hybrid Neural ODE model with a multiplicative correction to the prey-predator dynamics (Hybrid).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">Expert</th>
<th style="text-align: left;">LM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Eight schools</td>
<td style="text-align: left;">$\underline{-30.70}$</td>
<td style="text-align: left;">$\underline{-30.17}$</td>
</tr>
<tr>
<td style="text-align: left;">Eight schools sim</td>
<td style="text-align: left;">$\underline{-18.09}$</td>
<td style="text-align: left;">$\underline{-18.39}$</td>
</tr>
<tr>
<td style="text-align: left;">Eight schools sim no metadata</td>
<td style="text-align: left;">$\underline{-18.09}$</td>
<td style="text-align: left;">$\underline{-18.90}$</td>
</tr>
<tr>
<td style="text-align: left;">Dugongs</td>
<td style="text-align: left;">$\underline{22.52}$</td>
<td style="text-align: left;">$\underline{22.61}$</td>
</tr>
<tr>
<td style="text-align: left;">Dugongs sim</td>
<td style="text-align: left;">$\underline{50.04}$</td>
<td style="text-align: left;">$\underline{57.4}$</td>
</tr>
<tr>
<td style="text-align: left;">Dugongs sim no metadata</td>
<td style="text-align: left;">$\mathbf{5 0 . 0 4}$</td>
<td style="text-align: left;">26.68</td>
</tr>
<tr>
<td style="text-align: left;">Surgical</td>
<td style="text-align: left;">$\mathbf{- 4 0 . 2 9}$</td>
<td style="text-align: left;">-37.36</td>
</tr>
<tr>
<td style="text-align: left;">Surgical sim</td>
<td style="text-align: left;">$\underline{-39.80}$</td>
<td style="text-align: left;">$\underline{-38.45}$</td>
</tr>
<tr>
<td style="text-align: left;">Surgical sim no metadata</td>
<td style="text-align: left;">$\mathbf{- 3 9 . 8 0}$</td>
<td style="text-align: left;">-58.72</td>
</tr>
<tr>
<td style="text-align: left;">Peregrine</td>
<td style="text-align: left;">$\mathbf{- 1 4 2 . 1 9}$</td>
<td style="text-align: left;">-164.69</td>
</tr>
<tr>
<td style="text-align: left;">Peregrine sim</td>
<td style="text-align: left;">$\mathbf{- 1 3 0 . 4 8}$</td>
<td style="text-align: left;">-177.15</td>
</tr>
<tr>
<td style="text-align: left;">Peregrine sim no meta</td>
<td style="text-align: left;">$\underline{-130.48}$</td>
<td style="text-align: left;">$\underline{-127.32}$</td>
</tr>
</tbody>
</table>
<p>Table 4. Comparison of BoxLM with state-space update programs against expert programs We perform this comparison across four different datasets and two different ablations that replace observations with synthetic observations and remove all metadata. We report the expected predictive log density estimated via leave-one-out cross validation. We bold statistically significant differences and underline non-significant differences.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ To avoid overloading the word "model", we will refer to language models as LMs.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
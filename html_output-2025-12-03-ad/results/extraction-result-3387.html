<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3387 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3387</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3387</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-77.html">extraction-schema-77</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <p><strong>Paper ID:</strong> paper-4f1aaee2b06856f5826debf1e177aa5e6dcb5a8c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4f1aaee2b06856f5826debf1e177aa5e6dcb5a8c" target="_blank">Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper conducts some qualitative investigations of this kind of evaluation for the particular case of spatial reasoning (which is a fundamental aspect of commonsense reasoning) and concludes with some suggestions for future work both to improve the capabilities of language models and to systematise this type of dialectical evaluation.</p>
                <p><strong>Paper Abstract:</strong> Language models have become very popular recently and many claims have been made about their abilities, including for commonsense reasoning. Given the increasingly better results of current language models on previous static benchmarks for commonsense reasoning, we explore an alternative dialectical evaluation. The goal of this kind of evaluation is not to obtain an aggregate performance value but to find failures and map the boundaries of the system. Dialoguing with the system gives the opportunity to check for consistency and get more reassurance of these boundaries beyond anecdotal evidence. In this paper we conduct some qualitative investigations of this kind of evaluation for the particular case of spatial reasoning (which is a fundamental aspect of commonsense reasoning). We conclude with some suggestions for future work both to improve the capabilities of language models and to systematise this kind of dialectical evaluation.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3387.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3387.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (C3.5 / P3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5turbo (ChatGPT instances evaluated as C3.5 on chat.openai.com and P3.5 on platform.openai.com/playground)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conversational foundation language model (GPT-3.5turbo) evaluated in this paper via interactive, dialectical prompts on a range of commonsense spatial reasoning problems (parthood, rotation, directions, size, Winograd-style referent problems, affordances, object permanence, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5turbo accessed via ChatGPT (chat.openai.com) and via OpenAI Playground (platform.openai.com). The paper notes stochastic behaviour between interfaces and sessions and that playground runs sometimes used temperature=0. The model is used 'out of the box' (no fine-tuning) in interactive dialogues.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Dialectical commonsense spatial reasoning prompts (textual problems)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>A collection of short textual puzzles and probe prompts that require spatial knowledge and reasoning (transitive parthood/containment, letter rotations, left/right/above/below relations, relative size containment, Winograd-style size/fit referent problems, physical affordances and interactions such as staking plants, handles, linked chains, and occlusion/object permanence). These are not formal board puzzles like Sudoku but are spatial commonsense problems requiring relational and causal spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Natural-language textual dialogue prompts presented incrementally in a chat session (questions, follow-ups and clarifications). Letters used to represent shapes (e.g., 'rotate N by 90 degrees') or short vignette descriptions for physical scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot conversational / dialectical evaluation: single-shot prompts followed by follow-ups and challenge prompts within the same chat session; some runs used Playground with temperature=0; no few-shot exemplars or chain-of-thought prompting was specified as part of the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Qualitative analysis based on dialogue transcripts. Authors show many concrete failure modes: incorrect transitive inferences (initially hedged or incorrect about parthood/containment), poor 2D rotation/shape reasoning for some letters, inability to combine left/right with above/below reliably, linguistics-based rather than semantics-based pronoun resolution in Winograd-style examples, unrealistic physical reasoning (e.g., claiming metal chains 'stretch' or that a larger circle can be moved into a smaller one), inconsistent responses within the same session, and sensitivity to wording and interface. The paper emphasises that correct answers often co-occur with flawed justifications, and vice versa, so surface correctness masks reasoning errors. No internal model interpretability (attention, ablation) or intermediate representations are analysed — analysis is based on qualitative inspection of outputs and aggregated scoring (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Aggregated qualitative scoring from Table 1: for the chat.openai.com GPT-3.5turbo runs (labeled C3.5) authors report 19 prompts with correct answers and 14 prompts with correct explanations (counts appear in Table 1 of the paper). Playground GPT-3.5turbo runs (P3.5) show similar raw correct-answer counts (reported as 19 correct answers, ~12 correct explanations). The paper evaluates ~40+ numbered prompts; the metrics reported are counts of 'correct answer' and 'correct explanation' per model variant (see Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Frequent incorrect or internally inconsistent spatial inferences; reliance on linguistic heuristics instead of spatial semantics for pronoun resolution; failure to combine orthogonal relations (left/right with above/below); incorrect mental imagery (rotation/shape descriptions), mistaken physical intuitions (chains stretching), and inconsistent behaviour across rephrasing or across sessions. The LLMs are also stochastic, making reproducibility and automation harder.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>The paper compares GPT-3.5 results to GPT-4 variants and Bard (LaMDA-based). GPT-3.5 variants perform substantially worse than GPT-4 variants on the same prompt suite (see Table 1). No human benchmark data are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3387.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3387.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (C4 / P4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (ChatGPT instances evaluated as C4 on chat.openai.com and P4 on platform.openai.com/playground)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 instances evaluated in the paper on the same dialectical spatial commonsense prompts; shown to outperform GPT-3.5 on many items but still exhibiting notable failures and inconsistent explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 accessed via both chat.openai.com and the Playground (authors note different behaviors across interfaces). Used 'out of the box' with some Playground runs at temperature=0. The paper references the GPT-4 technical report but does not fine-tune the model for these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Dialectical commonsense spatial reasoning prompts (textual problems)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same suite of textual spatial/commonsense problems (parthood, rotation, direction, size and containment, Winograd-style referent problems, CPP physical-interaction problems, object permanence). Problems require qualitative spatial relations, simple mental rotation, occlusion reasoning, and commonsense physical affordance inference.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Natural-language textual chat prompts and follow-ups; simple symbolic letters used for rotation tasks (no images, purely text).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Interactive dialectical zero-shot prompts with follow-ups; some playground queries used temperature=0. No explicit chain-of-thought, few-shot examples, or external tools were used in the methodologies described.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Qualitative evaluation shows GPT-4 variants produce many more correct answers and more correct explanations than GPT-3.5, but still produce important mistakes. Authors note that GPT-4 often gives correct answers with plausible explanations, indicating improved capability, but the dialectical probing reveals remaining blind spots and inconsistency under perturbations. No internal interpretability (attention patterns etc.) is presented; analysis is transcript-based and aggregated in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>From Table 1: GPT-4 variants reported ~30 correct answers; for the GPT-4 chat instance authors report ~30 correct answers and ~30 correct explanations for the Playground GPT-4 run (P4), and GPT-4 chat (C4) reported ~30 correct answers and ~22 correct explanations (see Table 1 for per-prompt breakdown).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Despite improved raw scores, GPT-4 still makes fundamental mistakes on spatial reasoning prompts (e.g., some rotation/direction/size questions, some Winograd-style referent questions, and some physical-interaction problems). The model still sometimes provides linguistically-plausible but semantically incorrect explanations and shows sensitivity to prompt phrasing and dialogue history.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Authors directly compare GPT-4 to GPT-3.5 and Bard; GPT-4 variants show clear improvement over GPT-3.5 in this prompt suite (higher counts of correct answers and explanations), but no human baseline is provided. The authors emphasise that higher aggregate scores do not eliminate important conceptual failures revealed by dialectical probing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3387.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3387.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bard (LaMDA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bard (Google's LaMDA-based conversational model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's Bard (LaMDA-based) evaluated in the paper on selected spatial commonsense prompts; the paper reports that Bard's performance was generally worse than GPT-4 and in many cases worse than GPT-3.5 on the selected prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Bard (LaMDA-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Bard was evaluated via the Google web UI 'Experiment' available March 2023. The authors used the first response (did not exhaust alternative drafts) and note that Bard interactions were different and overall worse on these spatial prompts; no fine-tuning was applied.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Dialectical commonsense spatial reasoning prompts (textual problems)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same set of textual spatial/commonsense prompts (parthood, rotation, directions, relative size, Winograd-style referent problems, object affordances and interactions, occlusion and object permanence).</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Natural-language textual prompts and follow-ups in the Bard web UI; the authors recorded first responses and some additional drafts in some sessions.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Interactive dialectical zero-shot prompts with follow-ups; authors did not use special prompting tricks or chain-of-thought; they used the first Bard response mostly.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Qualitative findings show Bard made many errors on the spatial prompts; the authors note Bard's interactions differ in style and that its performance on these spatial commonsense tasks was 'much worse' in many cases. The analysis is based on example dialogues and aggregated scoring (Table 1). No internal interpretability analyses were performed.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 1 reports Bard with roughly 30 correct answers and 17 correct explanations (per the paper's scoring columns for Bard); however the paper notes the interactions/formatting differ and were not integrated into all table rows identically.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Bard showed many of the same failure modes: incorrect spatial inferences, inconsistent explanations, failures on Winograd-style referent resolution tied to spatial semantics, and poor physical intuition in several CPP tasks. The authors caution that Bard's interaction differences make direct comparison imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Compared in Table 1 alongside GPT-3.5 and GPT-4 variants; Bard underperforms GPT-4 overall on these prompts and has mixed performance relative to GPT-3.5. No human baseline is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>GPT-4 technical report <em>(Rating: 2)</em></li>
                <li>Visual spatial reasoning <em>(Rating: 2)</em></li>
                <li>Holistic evaluation of language models <em>(Rating: 2)</em></li>
                <li>On the dangers of stochastic parrots: Can language models be too big? <em>(Rating: 1)</em></li>
                <li>The defeat of the Winograd Schema Challenge <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3387",
    "paper_id": "paper-4f1aaee2b06856f5826debf1e177aa5e6dcb5a8c",
    "extraction_schema_id": "extraction-schema-77",
    "extracted_data": [
        {
            "name_short": "GPT-3.5 (C3.5 / P3.5)",
            "name_full": "GPT-3.5turbo (ChatGPT instances evaluated as C3.5 on chat.openai.com and P3.5 on platform.openai.com/playground)",
            "brief_description": "A conversational foundation language model (GPT-3.5turbo) evaluated in this paper via interactive, dialectical prompts on a range of commonsense spatial reasoning problems (parthood, rotation, directions, size, Winograd-style referent problems, affordances, object permanence, etc.).",
            "citation_title": "Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs",
            "mention_or_use": "use",
            "model_name": "GPT-3.5turbo",
            "model_description": "GPT-3.5turbo accessed via ChatGPT (chat.openai.com) and via OpenAI Playground (platform.openai.com). The paper notes stochastic behaviour between interfaces and sessions and that playground runs sometimes used temperature=0. The model is used 'out of the box' (no fine-tuning) in interactive dialogues.",
            "model_size": null,
            "puzzle_name": "Dialectical commonsense spatial reasoning prompts (textual problems)",
            "puzzle_description": "A collection of short textual puzzles and probe prompts that require spatial knowledge and reasoning (transitive parthood/containment, letter rotations, left/right/above/below relations, relative size containment, Winograd-style size/fit referent problems, physical affordances and interactions such as staking plants, handles, linked chains, and occlusion/object permanence). These are not formal board puzzles like Sudoku but are spatial commonsense problems requiring relational and causal spatial reasoning.",
            "input_representation": "Natural-language textual dialogue prompts presented incrementally in a chat session (questions, follow-ups and clarifications). Letters used to represent shapes (e.g., 'rotate N by 90 degrees') or short vignette descriptions for physical scenarios.",
            "prompting_method": "Zero-shot conversational / dialectical evaluation: single-shot prompts followed by follow-ups and challenge prompts within the same chat session; some runs used Playground with temperature=0; no few-shot exemplars or chain-of-thought prompting was specified as part of the experiments.",
            "spatial_reasoning_analysis": "Qualitative analysis based on dialogue transcripts. Authors show many concrete failure modes: incorrect transitive inferences (initially hedged or incorrect about parthood/containment), poor 2D rotation/shape reasoning for some letters, inability to combine left/right with above/below reliably, linguistics-based rather than semantics-based pronoun resolution in Winograd-style examples, unrealistic physical reasoning (e.g., claiming metal chains 'stretch' or that a larger circle can be moved into a smaller one), inconsistent responses within the same session, and sensitivity to wording and interface. The paper emphasises that correct answers often co-occur with flawed justifications, and vice versa, so surface correctness masks reasoning errors. No internal model interpretability (attention, ablation) or intermediate representations are analysed — analysis is based on qualitative inspection of outputs and aggregated scoring (Table 1).",
            "performance_metrics": "Aggregated qualitative scoring from Table 1: for the chat.openai.com GPT-3.5turbo runs (labeled C3.5) authors report 19 prompts with correct answers and 14 prompts with correct explanations (counts appear in Table 1 of the paper). Playground GPT-3.5turbo runs (P3.5) show similar raw correct-answer counts (reported as 19 correct answers, ~12 correct explanations). The paper evaluates ~40+ numbered prompts; the metrics reported are counts of 'correct answer' and 'correct explanation' per model variant (see Table 1).",
            "limitations_or_failure_modes": "Frequent incorrect or internally inconsistent spatial inferences; reliance on linguistic heuristics instead of spatial semantics for pronoun resolution; failure to combine orthogonal relations (left/right with above/below); incorrect mental imagery (rotation/shape descriptions), mistaken physical intuitions (chains stretching), and inconsistent behaviour across rephrasing or across sessions. The LLMs are also stochastic, making reproducibility and automation harder.",
            "comparison_to_other_models_or_humans": "The paper compares GPT-3.5 results to GPT-4 variants and Bard (LaMDA-based). GPT-3.5 variants perform substantially worse than GPT-4 variants on the same prompt suite (see Table 1). No human benchmark data are provided in this paper.",
            "uuid": "e3387.0",
            "source_info": {
                "paper_title": "Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "GPT-4 (C4 / P4)",
            "name_full": "GPT-4 (ChatGPT instances evaluated as C4 on chat.openai.com and P4 on platform.openai.com/playground)",
            "brief_description": "GPT-4 instances evaluated in the paper on the same dialectical spatial commonsense prompts; shown to outperform GPT-3.5 on many items but still exhibiting notable failures and inconsistent explanations.",
            "citation_title": "Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "GPT-4 accessed via both chat.openai.com and the Playground (authors note different behaviors across interfaces). Used 'out of the box' with some Playground runs at temperature=0. The paper references the GPT-4 technical report but does not fine-tune the model for these tasks.",
            "model_size": null,
            "puzzle_name": "Dialectical commonsense spatial reasoning prompts (textual problems)",
            "puzzle_description": "Same suite of textual spatial/commonsense problems (parthood, rotation, direction, size and containment, Winograd-style referent problems, CPP physical-interaction problems, object permanence). Problems require qualitative spatial relations, simple mental rotation, occlusion reasoning, and commonsense physical affordance inference.",
            "input_representation": "Natural-language textual chat prompts and follow-ups; simple symbolic letters used for rotation tasks (no images, purely text).",
            "prompting_method": "Interactive dialectical zero-shot prompts with follow-ups; some playground queries used temperature=0. No explicit chain-of-thought, few-shot examples, or external tools were used in the methodologies described.",
            "spatial_reasoning_analysis": "Qualitative evaluation shows GPT-4 variants produce many more correct answers and more correct explanations than GPT-3.5, but still produce important mistakes. Authors note that GPT-4 often gives correct answers with plausible explanations, indicating improved capability, but the dialectical probing reveals remaining blind spots and inconsistency under perturbations. No internal interpretability (attention patterns etc.) is presented; analysis is transcript-based and aggregated in Table 1.",
            "performance_metrics": "From Table 1: GPT-4 variants reported ~30 correct answers; for the GPT-4 chat instance authors report ~30 correct answers and ~30 correct explanations for the Playground GPT-4 run (P4), and GPT-4 chat (C4) reported ~30 correct answers and ~22 correct explanations (see Table 1 for per-prompt breakdown).",
            "limitations_or_failure_modes": "Despite improved raw scores, GPT-4 still makes fundamental mistakes on spatial reasoning prompts (e.g., some rotation/direction/size questions, some Winograd-style referent questions, and some physical-interaction problems). The model still sometimes provides linguistically-plausible but semantically incorrect explanations and shows sensitivity to prompt phrasing and dialogue history.",
            "comparison_to_other_models_or_humans": "Authors directly compare GPT-4 to GPT-3.5 and Bard; GPT-4 variants show clear improvement over GPT-3.5 in this prompt suite (higher counts of correct answers and explanations), but no human baseline is provided. The authors emphasise that higher aggregate scores do not eliminate important conceptual failures revealed by dialectical probing.",
            "uuid": "e3387.1",
            "source_info": {
                "paper_title": "Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Bard (LaMDA)",
            "name_full": "Bard (Google's LaMDA-based conversational model)",
            "brief_description": "Google's Bard (LaMDA-based) evaluated in the paper on selected spatial commonsense prompts; the paper reports that Bard's performance was generally worse than GPT-4 and in many cases worse than GPT-3.5 on the selected prompts.",
            "citation_title": "Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs",
            "mention_or_use": "use",
            "model_name": "Bard (LaMDA-based)",
            "model_description": "Bard was evaluated via the Google web UI 'Experiment' available March 2023. The authors used the first response (did not exhaust alternative drafts) and note that Bard interactions were different and overall worse on these spatial prompts; no fine-tuning was applied.",
            "model_size": null,
            "puzzle_name": "Dialectical commonsense spatial reasoning prompts (textual problems)",
            "puzzle_description": "Same set of textual spatial/commonsense prompts (parthood, rotation, directions, relative size, Winograd-style referent problems, object affordances and interactions, occlusion and object permanence).",
            "input_representation": "Natural-language textual prompts and follow-ups in the Bard web UI; the authors recorded first responses and some additional drafts in some sessions.",
            "prompting_method": "Interactive dialectical zero-shot prompts with follow-ups; authors did not use special prompting tricks or chain-of-thought; they used the first Bard response mostly.",
            "spatial_reasoning_analysis": "Qualitative findings show Bard made many errors on the spatial prompts; the authors note Bard's interactions differ in style and that its performance on these spatial commonsense tasks was 'much worse' in many cases. The analysis is based on example dialogues and aggregated scoring (Table 1). No internal interpretability analyses were performed.",
            "performance_metrics": "Table 1 reports Bard with roughly 30 correct answers and 17 correct explanations (per the paper's scoring columns for Bard); however the paper notes the interactions/formatting differ and were not integrated into all table rows identically.",
            "limitations_or_failure_modes": "Bard showed many of the same failure modes: incorrect spatial inferences, inconsistent explanations, failures on Winograd-style referent resolution tied to spatial semantics, and poor physical intuition in several CPP tasks. The authors caution that Bard's interaction differences make direct comparison imperfect.",
            "comparison_to_other_models_or_humans": "Compared in Table 1 alongside GPT-3.5 and GPT-4 variants; Bard underperforms GPT-4 overall on these prompts and has mixed performance relative to GPT-3.5. No human baseline is provided.",
            "uuid": "e3387.2",
            "source_info": {
                "paper_title": "Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs",
                "publication_date_yy_mm": "2023-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "GPT-4 technical report",
            "rating": 2
        },
        {
            "paper_title": "Visual spatial reasoning",
            "rating": 2
        },
        {
            "paper_title": "Holistic evaluation of language models",
            "rating": 2
        },
        {
            "paper_title": "On the dangers of stochastic parrots: Can language models be too big?",
            "rating": 1
        },
        {
            "paper_title": "The defeat of the Winograd Schema Challenge",
            "rating": 2
        }
    ],
    "cost": 0.0152465,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs</h1>
<p>Anthony G Cohn ${ }^{1}$, José Hernández-Orallo ${ }^{2}$<br>${ }^{1}$ University of Leeds \&amp; The Alan Turing Institute \&amp; Tongji University<br>${ }^{2}$ Universitat Politècnica de València \&amp; ValGRAI \&amp; Leverhulme Centre for the Future of Intelligence<br>a.g.cohn@leeds.ac.uk, jorallo@upv.es</p>
<h4>Abstract</h4>
<p>Language models have become very popular recently and many claims have been made about their abilities, including for commonsense reasoning. Given the increasingly better results of current language models on previous static benchmarks for commonsense reasoning, we explore an alternative dialectical evaluation. The goal of this kind of evaluation is not to obtain an aggregate performance value but to find failures and map the boundaries of the system. Dialoguing with the system gives the opportunity to check for consistency and get more reassurance of these boundaries beyond anecdotal evidence. In this paper we conduct some qualitative investigations of this kind of evaluation for the particular case of spatial reasoning (which is a fundamental aspect of commonsense reasoning). We conclude with some suggestions for future work both to improve the capabilities of language models and to systematise this kind of dialectical evaluation.</p>
<h2>1 Introduction</h2>
<p>Reasoning about common sense has been a goal of AI since its earliest days (McCarthy 1959), but has proved surprisingly hard to endow computers with. Whilst many systems have been built which display expert abilities in fields as varied as medicine (e.g. (Van Melle 1978)), engineering (Dimitrova et al. 2020), bioinformatics (Tunyasuvunakool et al. 2021) and even games such Go (Silver et al. 2016), there has not yet been a system which has truly displayed the commonsense reasoning abilities of a young child. However, the advent of large language models (LLMs), such as BERT (Devlin et al. 2019) and GPT (Brown et al. 2020), which generally display perhaps surprising fluency and an apparent ability to answer questions relating to almost any topic broached, raises the question of whether, or at least to what extent, they can reason about commonsense notions and answer questions involving common sense. There are a number of commonsense benchmarks, such as the Winograd Schema Challenge (WSC) (Levesque, Davis, and Morgenstern 2011) and, at least according to aggregate measures, recent LLMs can score well (Srivastava et al. 2022; Liang et al. 2022).</p>
<p>However, such success may not demonstrate a true ability to do commonsense reasoning (Kocijan et al. 2022) - the suggested reasons are (1) lax evaluation criteria, (2) artifacts in the dataset and (3) knowledge leakage from the training
data. Another important reason is the abuse of aggregations over tasks and collections of tasks, in massive benchmarks, such as BIG-Bench (Srivastava et al. 2022). As has been noted, e.g. (Burnell et al. 2023), even if aggregate measures do indicate good expected performance, they do not necessarily reveal the model limitations, since failed examples are overshadowed by many other examples where the model can use some shortcuts or statistical tricks to get the right answer. A fifth reason of why this traditional evaluation of common sense reasoning on language models may be limited is because they take the form of a static setting, in many cases as multiple-choice questions (MCQ); this means that the real extent of understanding cannot be tested, and in the case of MCQs the task is considerably easier since the answer is already provided as one of the options. This multiplies the effects of the second and third issues (artifacts and knowledge leakage from the training set). Actually, many existing tasks and benchmarks have already been used verbatim in the training of the model ${ }^{1}$. We may use canaries on the training data, or build 'fresh' items for evaluation, but this is impractical, as new models appear very frequently. If we assume this contamination exists (OpenAI 2023, Ap.C/D), then negative results-refutations, in scientific terminology-are of value, but positive results should not be taken with the same value. Because of all these issues, reasoning, and especially commonsense reasoning, may require a more dialectic approach to evaluation.</p>
<p>But what is common sense? Davis (2023) suggests the following criteria for common sense: (i) is common; (ii) is "largely sensible"; (iii) supports reasoning; (iv) is integrated into other cognitive abilities (language, vision, etc.) - one never observes it directly, only how it is manifested through language, action, etc.; (v) is independent of any modality or task; (vi) has broad scope; (vii) is distinguished from common knowledge, encyclopaedic and expert knowledge; (viii) is concerned with generalities rather than individuals; (ix) is not book learning or explicitly taught in schools; (x) is separate from purely linguistic or purely perceptual interpretation. Davis goes on to present a list of common-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>sense benchmarks ${ }^{2}$ and analyses these as to whether they meet the criteria above (and some other desirable features for benchmarks) and concludes "many of the commonsense benchmarks that have been created do not at all respect these boundaries; most involve substantial amounts of what is clearly common knowledge and many involve rather obscure encyclopedic knowledge or, more rarely, even expert knowledge". Thus the oft-reported success of AI systems for being able to reason about common sense because of their performance on such benchmarks is questionable.</p>
<p>Given the existing commonsense benchmarks and their limitations, as pointed out by Davis, in this paper we investigate a more granular, dialectic way of evaluating commonsense reasoning in language models. We specifically focus on spatial reasoning, because it should be sufficiently detached from linguistic capabilities and other elements of common sense, it may require a model of the world (intuitive physics) that seems to be at odds with the disembodied character of language models, and also because spatial reasoning in humans may rely on significant inductive bias about the world, such as distance, size, orientation, etc., as the result of our evolutionary history. Modern language models allow for a dialectic evaluation by the accumulation of the previous questions and answers as context. This is what we explore in a manual way here, in the tradition of other dialectic evaluation protocols (e.g., interviews or oral exams) that follow a systematic choice of questions (unlike the Turing test). In particular, in many cases we are influenced by ${ }^{3}$ McCarthy's elaboration tolerance: "A formalism is elaboration tolerant to the extent that it is convenient to modify a set of facts expressed in the formalism to take into account new phenomena or changed circumstances" (2003).</p>
<p>The purpose of this paper is to explore this dialectical way of probing the system, to start to develop a collection of instances to test for commonsense spatial reasoning abilities, and evaluate some LLMs on these.</p>
<h3>1.1 Commonsense spatial reasoning</h3>
<p>Spatial reasoning (and spatio-temporal reasoning) has long been recognised as a core part of commonsense reasoning, for example spatial representation and reasoning is central to the Naive Physics enterprise (Hayes 1985b; Hayes 1985a). A large part of the AI literature on commonsense reasoning has concerned the representation of and reasoning with, qualitative spatial information (Cohn and Renz 2008; Chen et al. 2015). One of the main reasons for this focus on qualitative spatial representations is that commonsense reasoning is not usually concerned with metric information which tends to be the domain of traditional mathematical reasoning. Rather, qualitative spatial reasoning is concerned with representing relational information, includ-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>ing notions of relative size or distance ${ }^{4}$. There are a number of aspects of such spatial knowledge which have been investigated (Cohn and Renz 2008), including (i) mereotopology, (ii) direction and orientation, (iii) size, (iv) distance and (v) shape. Frequently, spatial reasoning requires reasoning about more than one of these aspects simultaneously.</p>
<p>There are certainly spatial instances in the various benchmarks purporting to test commonsense reasoning, some of them in multimodal situations (Liu, Emerson, and Collier 2022), but others in purely textual form. We could use some instances from existing benchmarks such as from the WSC, but these are vulnerable to statistical patterns and other tricks (Kocijan et al. 2022), and are limited to static non-dialectic interactions. Also, there is the contamination risk that the LLM may have been trained on them, making good results not indicative of a generic capability. We will build on some of them in section 2.2 though, and some other sources in section 2.3 , since the dialectic character of the evaluation allows for elaboration on the reasons or variations, to check if the model has or does not have the ability.</p>
<h2>2 An evaluation of the spatial reasoning abilities of some LLMs</h2>
<p>In order to exploit the dialectical possibilities for evaluating commonsense reasoning and the use of context (previous interaction in the session), we will use conversational foundation models. One of these is ChatGPT as it is (a) recent, (b) easily accessible and (c) has been the subject of an extraordinary amount of attention in the scientific and popular media. We evaluate a number of versions of ChatGPT using GPT-3.5turbo and also using GPT-4 on chat.openai.com; and using 3.5turbo and also using GPT-4 on platform.openai.com/playground. Finally we also evaluate Bard, based on LaMDA (Thoppilan et al. 2022). For convenience below, we refer to these five LLMs as C3.5, C4, P3.5, P4 and Bard respectively.</p>
<p>ChatGPT and Bardcan certainly provide very fluent and sometimes perhaps surprisingly sophisticated outputs, e.g. when asked to write something in a particular style. However, to what extent does such fluent text output display commonsense reasoning abilities and in particular about space? This is the question we now turn to. In this paper we cannot hope to exhaustively answer this question; rather, we sample the space, guided by some existing challenges and by basic competencies we might expect to be displayed. In what follows, we include the results for ChatGPT using GPT-3.5turbo (C3.5), because it shows a greater number of inadequate or incorrect responses, and illustrates the dialectic approach best. Actually, for those responses that are correct we simple include a ,- symbol omitting the answer. ${ }^{5}$ In some of the conversations reported here, asking later</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>prompts in a conversation separately in a new conversation sometimes will give an improved result. However, keeping all the prompts in a single chat session as a single dialogue unveils some important and revealing mistakes, and this is the case we report.</p>
<h3>2.1 Basic spatial relations</h3>
<p>Parthood Parthood is perhaps the most basic of all spatial relationships. Mereology (Simons 2000) has been widely studied in the literature and forms the basis of much of qualitative spatial reasoning. Let us start with some dialogues. Prompt-1.1: If $x$ is part of $y$ and $y$ is part of $z$ what is the spatial relationship between $x$ and $z$ ? C3.5-Response: If $x$ is part of $y$ and $y$ is part of $z$, it means that $x$ is a subcomponent of $y$, and $y$ is a subcomponent of $z$. [...] The spatial relationship between $x$ and $z$ depends on the specific context of the situation. It is possible that $x$ and $z$ are physically connected or adjacent to each other, but it is also possible that they are spatially separate from each other. For example, if $x$ is a tire and $y$ is a car, and $z$ is a garage, then $x$ is part of $y$ and $y$ is part of $z$. However, $x$ and $z$ are not necessarily spatially related to each other. The tire may be physically located in the garage, but it could also be in the trunk of the car, or even outside the garage. [...] The response here is not good; the first sentence just recapitulates the question and then a very odd example ensues. Most would agree that a tyre is part of car, but not that a car is part of garage as seems to be implied here (it might be in garage though - see below). If something is a subcomponent of something else one would always regard it as spatially related and indeed a spatial part.
"Part of" and "in" tend to be used interchangeably so we explore the response response in this case. Prompt-1.2: If $u$ is in $v$ and $v$ is in $w$, what is the spatial relationship between $u$ and $w$ ? C3.5-Response: If $u$ is in $v$ and $v$ is in $w$, it means that $u$ is a subcomponent of $v$, and $v$ is a subcomponent of w. [...] The spatial relationship between $u$ and $w$ depends on the specific context of the situation. However, in general, $u$ and $w$ are not necessarily spatially related to each other, just like in the previous question. For example, if $u$ is a cell, $v$ is an organ, and $w$ is a body, then $u$ is in $v$ and $v$ is in $w$. However, $u$ and $w$ are not necessarily spatially related to each other. The cell could be located in the organ, but the organ could be located in a different part of the body. [...] As with prompt 1.1, C3.5 presents a confusing example, claiming in a contradictory manner that a cell could be in an organ which is in a different part of the body to where the cell is! We then tried using "contains" rather than "part of". Prompt-1.3: If $f$ contains $g$ and $g$ contains $h$ then what is the spatial relationship between $f$ and $h$ ? C3.5-Response: [...] Since $h$ is a subcomponent of $g$, and $g$ is a subcompo-
in general we do not give the responses here where they are acceptable, and in the cases where the LLM makes an error we only give the key part(s) of the response, with sections indicated by ellipsis [...]. The full prompt-response pairs can be found in the appendix to this paper. The summary of results for C3.5 can be found in Table 1, while the comparison for all LLMs can be found in Table 1.
nent of f , it follows that h is also a subcomponent of f . In other words, $h$ is contained within $f$. Therefore, the spatial relationship between $f$ and $h$ is that $h$ is located within $f$. In other words, $h$ is a part of $f$, and may be physically located within f , depending on the specific context of the situation. The response here is good except for the unwanted hedge in the last sentence.</p>
<p>Finally we explicitly state in the prompt that the relationship between the entities involved are spatial relations rather than leaving this implicit (to see if this helps with the strange example generation). Prompt-1.4: If $b$ is a spatial part of $c$ and $c$ is a spatial part of $d$, then what is the spatial relationship between $b$ and $d$ ? C3.5-Response: $\checkmark$.</p>
<p>Rotation Reasoning about rotation is a commonly used test of spatial reasoning abilities. In the absence of diagrams, here we use letters which have a known shape. Prompt-2.1: If I rotate a letter $M$ through 180 degrees what letter is it most similar to? However, C3.5 performs badly on a different letter rotation test: Prompt-2.2: If I rotate a capital Z by 90 degrees what letter is it most similar to? C3.5-Response: If you rotate a capital "Z" by 90 degrees clockwise, it would look most similar to the letter "L" (in uppercase form). The capital "Z" is a diagonal letter that consists of two perpendicular lines. When you rotate it by 90 degrees, the two lines become horizontal, with the top line now on the left and the bottom line now on the right. This shape closely resembles the letter "L", which is also a horizontal letter consisting of a single line on the left and a shorter line on the right.[...] C3.5 incorrectly describes the shape of a Z and then gets the wrong answer. However if asked about rotating an N , it gets the right answer, in spite of incorrectly describing the shape of both an N and Z. Prompt-2.3: If I rotate a capital " $N$ " by 90 degrees then what letter is it most similar to? C3.5-Response: If you rotate a capital " N " by 90 degrees clockwise, it would look most similar to the letter " Z " (in uppercase form). The capital " N " consists of two diagonal lines that intersect at a right angle, with the longer line pointing downwards to the right. When you rotate it by 90 degrees, the longer line will point to the left instead of downwards, and the shorter line will point upwards instead of to the left. This shape closely resembles the letter " Z ", which is also a diagonal letter consisting of two lines that intersect at a right angle, with the longer line pointing downwards to the right. [...]</p>
<p>Directions We restrict ourselves here to asking about spatial points since directions between areal objects can be more complex. Prompt-3.1: If a point $b$ is to the left of a point $c$, and a point $d$ is to the right of a point $c$, then what is the spatial relationship between $b$ and $d$ ? C3.5-Response: If point $b$ is to the left of point $c$ and point $d$ is to the right of point $c$, then point $b$ and point $d$ are on opposite sides of point c. In other words, point $b$ is on the left-hand side of a line passing through points c and d , while point d is on the right-hand side of the same line. Therefore, the spatial relationship between $b$ and $d$ is that they are on opposite sides of the line passing through c and d . This looks good but is not completely clear about the spatial relationship be-</p>
<p>tween b and d, only the relationship between of these and the constructed line. Unfortunately C3.5 is not able to answer this question. Prompt-3.2: is $b$ to the left or right of d? C3.5-Response: Without additional information about the positions of points b and d relative to each other, it's impossible to determine whether b is to the left or right of d. All we know is that b and d are on opposite sides of the line passing through c and d . But their positions relative to each other could be anywhere on their respective sides of that line. Given this, perhaps unsurprisingly, C3.5 is not able to reason about left/right and above/below simultaneously. Prompt-4.1: In an image plane, if point $e$ is directly to the left of point $f$, and point $g$ is directly above point $f$, then what is the spatial relationship between $e$ and $g$ in that image plane? C3.5-Response: Without more information, we cannot determine the spatial relationship between e and g with certainty. If e and g are on the same horizontal line in the image plane, then e and g would be neither directly above nor below each other. In this case, e would be to the left of f , and g would be above f , but e and g would not be directly related to each other in terms of their horizontal or vertical positions. However, if e and g are on different horizontal lines in the image plane, then they could have a different spatial relationship. For example, if e is on a higher horizontal line than g , then e would be to the left and above g . On the other hand, if g is on a higher horizontal line than e, then e would be to the left and below g. [...]</p>
<h3>2.2 Size, shape and location</h3>
<p>Circle size Reasoning about relative size is another important aspect of spatial common sense. Prompt-5.1: If a circle $b$ is larger than a circle $c$, is it possible to move $b$ so that it is entirely contained by c? C3.5-Response: Yes, it is possible to move a larger circle (b) so that it is entirely contained by a smaller circle (c). To do this, you would need to move the center of circle b towards the center of circle c until the edge of circle $b$ touches the edge of circle c. As long as the radius of circle b is smaller than the distance between the centers of the two circles, it is possible to move circle b in this way so that it is entirely contained by circle c. It's important to note that in order for circle $b$ to be entirely contained by circle c , the radius of circle $b$ must be less than or equal to the difference between the radii of circle c and circle b. C3.5 is wrong in its answer and also, in the last sentence, in its specification of what it would take for it to be possible.
Trophy For the WSC trophy/case example already mentioned in footnote 4, C3.5 fails to reason correctly, despite the example being widely available on the web (though usually without the correct answer given). Prompt-6.1: In the sentence "The trophy wouldn't fit in the case because it was too small.", what does the "it" refer to? C3.5-Response: In the sentence "The trophy wouldn't fit in the case because it was too small.", the pronoun "it" refers to the trophy. The answer is wrong so we asked for a justification: Prompt-6.2: Why do you say that? C3.5-Response: I say that because the subject of the sentence is "the trophy" and the pronoun "it" is used later on in the sentence as a pronoun to refer back to the subject "trophy". [...] C3.5 has justified
its answer purely in terms of linguistic rather than semantic knowledge, so we challenged it again: Prompt-6.3: Why doesn't "it" refer to the case? C3.5-Response: "It" doesn't refer to the case because the subject of the sentence is "the trophy", not "the case". The sentence is talking about the trophy not fitting into the case, so the focus is on the trophy and not the case. Additionally, the sentence says that "it was too small", which would make more sense if "it" referred to the trophy, since the size of the trophy would be more relevant to whether or not it could fit in the case. If "it" referred to the case, the sentence would be saying that the case was too small, which is not what the sentence is trying to convey.
C3.5 displays ever more confusion, and doesn't appreciate that the size of the case could also be relevant. The confusion continues when the other half of the WSC is posed: Prompt-6.4: In the sentence "The trophy wouldn't fit in the case because it was too big.", what does the "it" refer to? C3.5-Response: [...] the pronoun "it" cannot logically refer to the trophy, as the sentence says the trophy is too big to fit in the case. Therefore, if the sentence is correct, "it" must refer to the case. However, the sentence contains a contradiction, because the first part of the sentence implies that the trophy is too large to fit in the case, but the second part says that the case is too big, which suggests that the trophy would actually fit. If we assume that the intended meaning of the sentence is that the trophy is too big to fit in the case, then the correct sentence would be "The trophy wouldn't fit in the case because it was too small". C3.5 contradicts itself in the first sentence and then goes on to insist that the "it" must refer to the case before finally wanting to incorrectly rephrase the sentence. When challenged Prompt-6.5: Why can't the "it" refer to the trophy? C3.5 does apologise for getting wrong but then gives an incorrect reason for the change of mind: C3.5-Response: [...] In this case, the use of the word "it" indicates that the speaker is referring to the trophy, as the trophy is the last noun mentioned before the pronoun "it." [...] whilst also showing it cannot reason correctly about spatial adjacency.</p>
<p>Bookcase We invented the following analogous WSC to see how consistent the problem is. Prompt-7.1: The book couldn't stand upright in the bookcase because it was too small. What does "it" refer to? C3.5-Response: "It" in the given sentence refers to the book. [...] because the book was too small to fit properly. As it is wrong, we ask why. Prompt-7.2: Why do you say that? C3.5-Response: [...] The reason for the book's inability to stand upright is given by the dependent clause "because it was too small." [...] "it" is used as a pronoun to refer back to the subject of the sentence, which is "the book". [...] Prompt-7.3: Does a pronoun always refer to the subject of the sentence? C3.5-Response: $\because$. Prompt-7.4: Surely the book wouldn't fit properly only if it was too big, not too small? C3.5-Response: You are correct. [...] The correct interpretation of the sentence is that "the book couldn't stand upright in the bookcase because it was too big." When a book is too big, it cannot fit properly in a bookcase, and as a result, it cannot stand upright. [...] So finally C3.5 does get the right answer and reason. But when re-asked the original</p>
<p>question, C3.5 gets it wrong still (in spite of being within the same C3.5 session): Prompt-7.5: So if I say "The book couldn't stand upright in the bookcase because it was too small", what does the "it" refer to? C3.5-Response: In the sentence "The book couldn't stand upright in the bookcase because it was too small," the pronoun "it" refers to the book. The sentence suggests that the book is too small to fit properly in the bookcase, which is causing it to be unable to stand upright. Therefore, the word "it" in the sentence refers to the book, which is the subject of the sentence. At least the justification is not made on linguistic grounds but rather on putative semantic ones, but clearly fails to understand commonsense spatiality here ${ }^{6}$. When given the complementary sentence, however, C3.5 correctly gives the referent of "it": Prompt-7.6: In the sentence "The book couldn't stand upright in the bookcase because it was too big", what does the "it" refer to? C3.5-Response: $\sqrt{ }$.</p>
<p>School bag Another spatial WSC is this one: Prompt-8.1: Tom threw his school bag down to Ray after he reached the top of the stairs. Who reached the top of the stairs? C3.5-Response: Tom reached the top of the stairs. The answer is correct, though based on the previous examples we may suspect that it has not reasoned semantically but rather because Tom is the subject of the sentence, which seems to be confirmed in the case of the complementary schema: Prompt-8.2: Tom threw his school bag down to Ray after he reached the bottom of the stairs. Who reached the bottom of the stairs? C3.5-Response: The sentence you provided is contradictory, as it states that Tom threw his school bag down to Ray after he (Tom) reached the bottom of the stairs. If Tom reached the bottom of the stairs, he would not be in a position to throw his school bag up to Ray, who would presumably be at the top of the stairs. It's possible that you meant to say "after he (Tom) reached the top of the stairs," in which case the answer would be Ray reached the bottom of the stairs. [...] It is also not clear why Tom couldn't throw the bag up to Ray (unless it was particularly heavy), though of course the question was about throwing the bag down not up. C3.5 does finally give the right answer when given a further Prompt-8.3: No it's not contradictory if you interpret "he" as referring to the correct person. C3.5-Response: I apologize for misunderstanding your question earlier. In the sentence "Tom threw his school bag down to Ray after he reached the bottom of the stairs," the pronoun "he" refers to Ray. Therefore, Ray is the person who reached the bottom of the stairs. We can test its understanding of property transfer: Prompt-8.4: So who has the school bag now? C3.5 is able to respond correctly: C3.5-Response: Based on the sentence "Tom threw his school bag down to Ray after he reached the bottom of the stairs," we can infer that Ray has the school bag now, as Tom threw it down to him. Are the systems able to reason correctly on the same sentence with substituted names? Prompt-8.5: John threw his school bag</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>down to Frank after he reached the bottom of the stairs. Who reached the bottom of the stairs? Unfortunately the answer is in the negative: C3.5-Response: In the sentence "John threw his school bag down to Frank after he reached the bottom of the stairs," the pronoun "he" refers to John. Therefore, John is the person who reached the bottom of the stairs.</p>
<h3>2.3 Affordances and object interaction</h3>
<p>The Commmonsense Problem page ${ }^{7}$ (CPP) contains a selection of problems contributed over a period of some years by members of the community which represent "challenge problems" for commonsense reasoning. Not all of these involve spatial reasoning; in this section we look at the performance of the systems on some of those that do.</p>
<p>Stakes in a Garden Prompt-9.1: A gardener who has valuable plants with long delicate stems protects them against the wind by staking them; that is, by plunging a stake into the ground near them and attaching the plants to the stake with string. Why will this help? ${ }^{8}$ C3.5-Response: Staking delicate plants with long stems can help protect them from wind damage in several ways. Firstly, wind can cause the plant to sway or bend, and if the stems are long and delicate, they may break under the stress. Staking provides support for the plant, keeping it upright and preventing it from bending or breaking. Secondly, wind can cause soil erosion around the base of the plant, which can destabilize it and cause it to topple over. Staking can help anchor the plant in place, preventing it from being uprooted by the wind. Lastly, staking can help promote healthy growth in the plant. [...] Overall, staking delicate plants with long stems can help protect them from wind damage, promote healthy growth, and ensure they continue to thrive. The response is good . CPP follows McCarthy's elaboration tolerance with a number of variants some of which were also put to the systems. Prompt-9.2: Would it work if the length of the string was shorter than the distance between the plant and the stake? C3.5-Response: If the length of the string used to tie the plant to the stake is shorter than the distance between the plant and the stake, it may not provide adequate support to the plant. The string may be too tight, which can cause the stem to bend or break, or it may not be able to support the weight of the plant. In order to properly stake a plant, the string should be long enough to wrap around the stem several times and then be tied to the stake with a knot or a twist tie. This ensures that the string is not too tight and can provide support to the stem without damaging it. If the distance between the plant and the stake is too great, it may be necessary to use additional stakes or to reposition the stake closer to the plant. [...] The response here does not capture the essential point that a short string could not provide any support, though at least it does suggest a longer string might be better. Conversely for this Prompt-9.3: Would it work if the</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>length of the string was much longer than the distance between the plant and the stake? C3.5 gives a good response, even noting the risk of entanglement: C3.5-Response: If the length of the string used to tie the plant to the stake is much longer than the distance between the plant and the stake, it may not be as effective in supporting the plant. The excess string may cause the plant to sway or move around more than necessary, which could potentially damage the stem or roots. Additionally, the excess string may create a tripping hazard or become tangled with nearby plants or garden tools. Ideally, the string used to tie the plant to the stake should be just long enough to wrap around the stem several times and then be tied to the stake with a knot or a twist tie. This will provide the necessary support to the plant without causing any unnecessary movement or creating any hazards in the garden. In some cases, using a longer string may be necessary if the plant is particularly tall or the stake needs to be placed far away from the plant for some reason. However, it's important to use the right length of string for each specific situation to ensure that the plant is properly supported and that the garden remains safe and tidy. Prompt-9.4: Could I twist the ends of the string together instead of making a knot? C3.5-Response: Yes, twisting the ends of the string together can be an effective way to tie a plant to a stake. This is known as a "twist tie" and it can be a convenient and secure method for attaching the string to the stake. [...] Twist ties can be particularly useful for delicate plants with soft stems, as they can be tied tightly without damaging the plant. They can also be easily adjusted or removed as needed, which makes them a convenient option for plants that may require frequent staking. Overall, whether you choose to tie the string in a knot or use a twist tie, the most important thing is to ensure that the plant is properly supported and that the tie is not too tight or too loose. This will help prevent damage to the plant and promote healthy growth. Here C3.5 doesn't show any understanding of what twisting the ends of the string means and also implies that it might damage the plant which would not be the case. The end of the response concerning the knot not being too tight does not show real understanding of how the string should be used (i.e. to keep the plant close to the stake rather than tying it to it).</p>
<p>The Handle Problem This problem concerns trying to formalise what a handle is and what could be used as a handle. Prompt-10.1: Give a general purpose characterisation of what constitutes a handle, in the ordinary sense of doorhandle or drawer-handle, which is sufficient to enable one to infer from a qualitative description of the shape of a part of an object whether or not it can be a handle for that object. C3.5-Response: In the ordinary sense, a handle is a structural component of an object that is intended to be grasped by the user to facilitate movement or manipulation of the object. A handle is typically a protruding part that provides a surface for the user to grip onto, and it is designed to be ergonomic, easy to grasp, and durable. Based on this general purpose characterization of a handle, one can infer whether or not a part of an object can be a handle by considering the following factors: Protrusion: A handle typically protrudes
from the surface of the object to provide a surface for the user to grip onto. [...] Ergonomics: A handle should be designed to fit comfortably in the user's hand, with a surface that is easy to grip onto. [...] Durability: A handle should be durable enough to withstand repeated use without breaking or wearing down. [...] The response is a good characterisation, though not really surprising given the large numbers of definitions of handles likely to be in its training data. Prompt-10.2: Could I use a blunt conical projection as a handle for a door or drawer? C3.5-Response: Yes, a blunt conical projection can be used as a handle for a door or drawer, depending on its design and size. A conical projection can provide a comfortable grip for the user and can be easy to operate. The size and shape of the conical projection should be designed with ergonomics in mind, ensuring that it fits comfortably in the user's hand and is easy to grasp. Additionally, the conical projection should be durable enough to withstand repeated use without breaking or wearing down. It's worth noting that a blunt conical projection may not be the most common type of handle for doors or drawers, as other shapes such as cylindrical or rectangular handles are more commonly used. [...] This is clearly a failure of commonsense reasoning, although it does note it's not a common shape - the response fails to realise that this really would not be very effective as a handle. So we checked which orientation C3.5 thought the handle was in, and responded that it is indeed tapering away from the drawer: Prompt-10.3: Is the base of this conical handle next to or away from the drawer? C3.5-Response: The orientation of the conical handle base in relation to the drawer will depend on the specific design of the handle. However, in most cases, the base of the conical handle would be attached to the surface of the drawer or door, with the tapering section of the handle extending away from the drawer or door. This orientation allows the user to grasp the tapered section of the handle and pull it outward, which in turn opens the drawer or door. The base of the handle provides a stable attachment point for the handle, ensuring that it remains securely in place when in use. [...] So C3.5 does believe that the base is next to the door/drawer. One strong point of this response is the comment that attaching the handle this way round provides a secure attachment point (not literally a point of course!) which displays some spatio-mechanical knowledge. A negative point is that this orientation does not allow a good grasp. So we challenged C3.5 about the orientation: Prompt-10.4: Wouldn't it be more ergonomic to have the base away from the drawer? C3.5-Response: Having the base of the conical handle away from the drawer or door could provide a different grip orientation and could be more ergonomic for certain users or situations. This design could allow the user to wrap their fingers around the base of the handle, which could provide a more comfortable and secure grip, especially if the handle is being used to lift a heavy object. However, this design could also make it more difficult for the user to operate the drawer or door, as they would need to push the handle inward instead of pulling it outward. [...] A handle with the base attached to the surface of the drawer or door and the tapering section extending outward is a com-</p>
<p>mon and effective design, but there may be situations where an alternative design, such as one with the base away from the drawer, could be more appropriate. The first part is a good response, but the middle section is bizarre and clearly fails a lack of understanding about how opening a drawer works. It also displays a lack of common knowledge in the final part where it claims that this orientation is common.</p>
<p>Reasoning about linked chains Another spatial problem in the CPP concerns chains ${ }^{9}$ : Prompt-11.1: Formally characterize the structure of a metallic linked chain. What will happen if one picks up one end of such a chain and walks away? By a linked chain I mean a chain which is constructed as a series of interconnected loops of metal and no other parts, such as is used as an anchor chain in a ship. C3.5-Response: A metallic linked chain is a structure composed of interconnected metal loops that form a continuous chain. Each loop in the chain is linked to the adjacent loop through a mechanical joint, which provides flexibility and allows the chain to bend and conform to different shapes. When one end of the linked chain is picked up and pulled away, the rest of the chain will follow due to the interlinked nature of the loops. As the chain is pulled, the links will stretch and become elongated, causing the chain to become thinner and longer. The chain will also experience tension forces along its length, which will increase as the pulling force is applied. If the force applied is too great, the chain may break at its weakest point, or one of the links may become dislodged from the rest of the chain. [...] This response starts off well (predicting bending and confirming a following behaviour) but then becomes flawed: one wouldn't expect such a metal chain to stretch (except by virtue of the links always being tight up against its neighbours) and it would not become thinner (as a string of Plasticine would). The conversation continued: Prompt-11.2: If the loops of this chain are large, is it possible to move the last link in the chain at all without any other link moving? C3.5-Response: If the loops of the chain are large, it may be possible to move the last link in the chain without any other link moving, but it depends on the specific geometry and construction of the chain. [...] If the last link in the chain is located near the end of the chain and there is enough slack or play between adjacent links, it may be possible to move the last link without any other link moving. However, if the last link is located closer to the middle of the chain, [...] This is mostly a good response but C3.5 is clearly confused about where the end of chain is, saying it might be in the middle! We then probed its understanding of what it means to be the last link in a chain. Prompt-11.3: How many other links is the last link in a chain connected to? C3.5-Response: The number of other links that the last link in a metallic linked chain is connected to will depend on the specific design and construction of the chain. In general, a metallic linked chain is designed to be a continuous loop, with each link connected to the adjacent link through a mechanical joint. The last link in the chain</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>will be connected to the second-to-last link, which in turn will be connected to the third-to-last link, and so on, until all of the links in the chain are connected in a continuous loop. [...] Therefore, while the last link in a metallic linked chain is typically connected to one or more adjacent links, the number of other links it is connected to may vary depending on the specific application and use case of the chain. There are several odd things here: C3.5 seems to think a chain is necessarily a loop rather than being just made of loops. But in that case there would not be a last link! And it explicitly suggests that the last link is connected to more than one other link. So we pressed C3.5 further. Prompt-11.4: Earlier you said that the last link could be in the middle of the chain. Can you explain further? C3.5-Response: I apologize for the confusion in my previous response. In a continuous loop metallic linked chain, the last link will always be connected to the second-to-last link, and the second-to-last link will be connected to the third-to-last link, and so on. Therefore, it is not possible for the last link to be in the middle of the chain. However, it is possible for the chain to have a break in the loop, in which case the last link may not be connected to any other links. For example, if a section of the chain is removed or cut, the last link on one side of the break will no longer be connected to the second-to-last link on the other side of the break. If a break occurs in the chain, the links on either side of the break will no longer be connected in a continuous loop, and the chain will no longer be able to function as intended. [...] This is good except for C3.5's insistence that chains must be a circular and cannot function as intended if not a loop. Prompt-11.5: If a chain is used a ship's anchor chain, is the chain a continuous loop? C3.5-Response: Yes, a chain used as a ship's anchor chain is typically a continuous loop metallic linked chain. The chain is connected to the ship at one end and to the anchor at the other end, forming a continuous loop that allows the anchor to be lowered and raised as needed. [...] This is not only partly wrong but also self contradictory as a loop does not have ends! We then pressed C3.5 specifically about the nature of the chain. Prompt-11.6: When you talk of a "continuous loop metallic linked chain", do you mean that the components of the chain are continuous loops, or that the chain itself forms a continuous loop? C3.5-Response: When I refer to a "continuous loop metallic linked chain," I mean that the chain itself forms a continuous loop, not necessarily that the components of the chain are continuous loops. A continuous loop metallic linked chain is a chain in which each link is connected to the adjacent link to form a continuous loop. The chain can be made up of individual links that are not continuous loops themselves, but when connected, they form a loop that is continuous and unbroken. [...] In general, a continuous loop metallic linked chain is designed to be flexible and able to bend in multiple directions, which allows it to conform to different shapes and absorb shock loads without breaking. This flexibility also allows the chain to be wound up and stored in a compact space when not in use. [...] C3.5 is not able to consistently explain the nature of a chain and contains several inaccuracies.</p>
<h3>2.4 Object permanence</h3>
<p>In this section we present some other problems specifically invented in this paper for the purposes of probing the systems' understanding of object permanence.</p>
<p>Where are the car keys? This problem refers to object permanence when an agent is moving (egocentric object permanence). Prompt-12.1: John and Mary are having breakfast together and Mary puts the car keys on the table. Mary then goes out for a walk. Whilst she is out, John puts the keys in his pocket. Mary then returns. Where does Mary expect to find the keys? C3.5-Response: Mary likely expects to find the keys where she left them, which is on the table. Since she did not give any indication to John that he could take the keys or move them, she would assume that they are still in the same place where she left them. If she doesn't find the keys on the table, she may ask John about it, but it's also possible that John may return the keys to the table before she realizes they are missing. In any case, it would be important for John to communicate with Mary if he moved the keys, so as to avoid any confusion or inconvenience. This answer is good - just what one would hope for. However if we then ask: Prompt-12.2: If the car wasn't in the drive when Mary returned from the walk, and John was out, then where would she expect the keys to be? then we run into problems: C3.5-Response: If the car wasn't in the drive when Mary returned from the walk and John was not there, Mary would likely check the usual places where they keep the keys, such as a key holder, a drawer, a hook, or a purse. If Mary could not find the keys in any of the places, she might have to wait for John to return or call or message him to ask where the car is and where the keys are. If John had taken the keys with him and was not reachable, Mary might have to find alternative transportation or wait for John to return. If Mary had a spare set of keys, she might be able to use those to drive the car. [...] C3.5 has completely failed to connect the change of spatial position of the car with the change of spatial position of the keys. Worse, it goes on to suggest that she might use some spare keys to drive the car which is not in the drive any more!</p>
<p>Cars behind the sofa Now let us try an example where it's the object moving (allocentric object permanence). Prompt-13.1: You are the only person in a room and see a green toy car on the left, a sofa in front of you and a door on the right. A car starts moving from left to right and you can see how it approaches the sofa. Then you don't see the car for a while. Finally a car appears again approaching the door. What happened while you couldn't see any car? C3.5-Response: [...]based on the information you've provided, it seems that the green toy car passed behind the sofa while you couldn't see it. This suggests that there may have been an obstruction blocking your view of the car for a period of time, such as the sofa. This is very good. It seems C3.5 understands object permanence with occlusion. But let us probe a bit further. Prompt-13.2: The car that appeared on the right was red. Can you explain what happened? C3.5-Response: Certainly! Based on the information you've provided, it seems that there were two toy cars
in the room - a green one and a red one. The green car initially started moving from the left side of the room towards the sofa in front of you, and then passed behind the sofa while you couldn't see it. After a while, the red car appeared, moving towards the door on the right side of the room. It's possible that the red car was already present in the room, but not initially visible to you due to your line of sight being blocked by the green car or another object in the room. Alternatively, the red car may have entered the room at some point while you were not observing it. Without more information, it's difficult to say for sure what exactly happened. It starts well, but the final part shows important problems, such as thinking that the red car was occluded by the green car, or the comment that the red car could have come from the door, when we said it was approaching the door.</p>
<h2>3 Discussion</h2>
<p>As we have seen above, the systems are never stuck for an answer for any of the problems posed (though they may occasionally incorrectly says there is not enough information to answer), but frequently their answers are wrong, or display fundamental misunderstandings. Table 1 summarises the findings from the earlier sections. This granular table is more insightful and informative about failures than any performance metric aggregation, and maps the contour of capabilities. The effort in the dialogues is put to find a variation or continuation where a system fails, to properly map this contour of spatial common sense.</p>
<p>It is worth noting that "ChatGPT has limited knowledge of world and events after 2021 and may also occasionally produce harmful instructions or biased content" (ChatGPT FAQ). The problems here are not related to knowledge of events post 2021, and certainly in the tests performed here, most answers are incorrect. Notably most of the questions here require reasoning rather than something more akin to factual look-up, and this has already been widely noted as something that LLMs are not good at. Indeed "all" that LLMs are really doing is next word prediction, albeit with clever harnessing. This has been discussed at length (Bender et al. 2021), where in particular it is noted that the seeming coherence of ChatGPT text is just that "seeming" - i.e., in their words "Coherence in the Eye of the Beholder". Ever since the days of the earliest AI chatbot, Eliza (Weizenbaum 1976), humans have been taken in by machine output, attributing too much intelligence to the mechanism which produced and assuming more understanding than actually exists. Bender et al (2021) suggest that "contrary to how it may seem when we observe its output, a language model is a system for haphazardly stitching together sequences of linguistic forms it has observed in its vast training data, according to probabilistic information about how they combine, but without any reference to meaning: a stochastic parrot". Whether actual intelligence and real reasoning abilities can emerge from such a mechanism trained on huge amounts of data is still an open question.</p>
<h3>3.1 Limitations</h3>
<p>This paper is far from a comprehensive evaluation of the spatial reasoning abilities of LLMs, and less so of the state of</p>
<p>the art of the fast-changing landscape of foundation models ${ }^{10}$. But it indicates that they frequently fail to display relatively elementary spatial reasoning abilities ${ }^{11}$. It could be argued that the interactive approach followed here is a weakness, but we believe it is also a strength since the dialogues presented show not only when the system is right or wrong, but also its explanations of its answers, and these explanations can be right or wrong independently of the correctness of the answer - e.g. the right answer may be given along with a fallacious reason, and vice versa. The differences between models as shown in Table 1 is very notable, and looking at the correct answers for more advanced models indicates certain progress in commonsense reasoning, but the persistent failures suggest that our dialectical approach could be pursued in order to try to find the new boundaries of more powerful systems.</p>
<p>At present it is not possible to finetune ChatGPT, GPT, Bart or their underlying base models so this evaluation "out of the box". For those models for which finetuning becomes available, we could investigate to what extent the problems revealed here can be mitigated. However, fine-tuning is perhaps against some of the principles of common sense, according to Davis (2023), because we want to determine if a generic system has common sense, not building or fine tuning a system for common sense. Also note that so far we have not given any of the systems any feedback on their responses. An alternative to fine-tuning could be to use human feedback (through reinforcement learning or other mechanisms such as rule learning, as used in (OpenAI 2023)).</p>
<p>The LLMs are stochastic, giving different answers for the same prompt for different sessions. This is an advantage when we are looking for a case that shows no good understanding, as we are driven by achieving refutation. However, it makes automation difficult as each dialogue must be based on the answer to previous prompts.</p>
<h3>3.2 Future Work</h3>
<p>One aspect of spatial reasoning not investigated here is geographical spatial reasoning, for example reasoning about cardinal directions ${ }^{12}$. Another area for future work is a more automated approach to evaluation. We have already discussed above some issues with existing benchmarks, as particularly highlighted by Davis (2023). The advantage of the dialectical approach taken in this paper is that following an initial prompt, the response can be analysed and further prompts issued to test understanding. This kind of conversation provides an arguably more robust and reliable indicator of the capabilities of the AI system, at the expense of the human being intimately involved - this can thus be seen as both an advantage and a disadvantage because of the cost of the human time. It may be that some middle ground is possible, whereby a pre-stored tree of questions/prompts is created to</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>facilitate an automated approach. Creating such trees would not be easy though, and the difficulty of anticipating possible responses very challenging. Using a multi-choice evaluation removes many of these problems, but there are well known issues with the design and use multiple-choice tests. A challenge for the future would be the automated generation of such trees. This would be a form of adaptive testing. A combination of both automated and manual evaluation methods is probably still required for a thorough evaluation.</p>
<p>Another aspect to be investigated further, which we started to address in a small way here, is to test robustness of the responses by posing not just a single query, but with small variations, both linguistic and semantic. The sensitivity of LLMs to such perturbations has already been noted in a Theory-of-Mind setting (Ullman 2023).</p>
<p>Acknowledgments: This work was funded by EU's Horizon 2020 research and innovation programme under grant agreement No. 952215 (TAILOR), US DARPA HR00112120007 (RECoG-AI) and Spanish grant PID2021-122830OB-C42 (SFERA) funded by MCIN/AEI/10.13039/501100011033 and "ERDF A way of making Europe". Partial financial support was also received from the ESRC under grant ES/W003473/1.</p>
<p>Data Access Statement: All data used in this paper can be found in the appendix.</p>
<p>Author contributions: AGC conceived the paper and wrote the initial draft, and JHO the second draft. Both authors contributed to all subsequent drafts.</p>
<h2>References</h2>
<p>Bender, E. M.; Gebru, T.; McMillan-Major, A.; and Shmitchell, S. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, 610-623.
Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020. Language models are few-shot learners. Advances in Neural Information Processing Systems 33:18771901.</p>
<p>Burnell, R.; Schellaert, W.; Burden, J.; Ullman, T.; Martinez-Plumed, F.; Tenenbaum, J. B.; Rutar, D.; Cheke, L. G.; Sohl-Dickstein, J.; Mitchell, M.; Kiela, D.; Shanahan, M.; Voorhees, E. W.; Cohn, A. G.; Leibo, J. Z.; and Hernandez-Orallo, J. 2023. Rethinking the reporting of evaluation results in Artificial Intelligence. Science.
Chen, J.; Cohn, A. G.; Liu, D.; Wang, S.; Ouyang, J.; and Yu, Q. 2015. A survey of qualitative spatial representations. The Knowledge Engineering Review 30(1):106-136.
Cohn, A. G., and Renz, J. 2008. Qualitative spatial representation and reasoning. In van Harmelen, F.; Lifschitz, V.; and Porter, B., eds., Handbook of Knowledge Representation, volume 3. Elsevier. 551-596.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">C3.5</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">P3.5</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">C4</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">P4</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Bard</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Prompt#</td>
<td style="text-align: center;">Correct answer</td>
<td style="text-align: center;">Correct explanation</td>
<td style="text-align: center;">Correct answer</td>
<td style="text-align: center;">Correct explanation</td>
<td style="text-align: center;">Correct answer</td>
<td style="text-align: center;">Correct explanation</td>
<td style="text-align: center;">Correct answer</td>
<td style="text-align: center;">Correct explanation</td>
<td style="text-align: center;">Correct answer</td>
<td style="text-align: center;">Correct explanation</td>
</tr>
<tr>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">1/2</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">$2.4^{*}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">$2.5^{*}$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$1 / 2$</td>
</tr>
<tr>
<td style="text-align: center;">$5.2^{*}$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">$5.3^{*}$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">6.1+6.2</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">6.4</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$1 / 2$</td>
</tr>
<tr>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">n/a</td>
</tr>
<tr>
<td style="text-align: center;">7.1+7.2</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$1 / 2$</td>
</tr>
<tr>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">8.1</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">8.2</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$1 / 2$</td>
</tr>
<tr>
<td style="text-align: center;">8.3</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">8.4</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">n/a</td>
</tr>
<tr>
<td style="text-align: center;">8.5</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">9.3</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$1 / 2$</td>
</tr>
<tr>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">10.2</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">10.3</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$1 / 2$</td>
</tr>
<tr>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">11.4</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$1 / 2$</td>
</tr>
<tr>
<td style="text-align: center;">11.5</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$1 / 2$</td>
</tr>
<tr>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">n/a</td>
</tr>
<tr>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">n/a</td>
</tr>
<tr>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$1 / 2$</td>
</tr>
<tr>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$1 / 2$</td>
</tr>
<tr>
<td style="text-align: center;">Total $\checkmark$</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">17</td>
</tr>
<tr>
<td style="text-align: center;">Total $\checkmark$</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">16.5</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">22</td>
</tr>
</tbody>
</table>
<p>Table 1: A summary of the responses to the numbered prompts for all systems. * in column 1 denotes a prompt only in the appendix. $\checkmark$ denotes a correct response/explanation and $\times$ an incorrect one. $\frac{1}{2}$ denotes some correctness. The decision as to whether to score a response/explanation as $\checkmark t \cdot t \times$ can be subjective, particularly in the case where only a very small mistake is made. The assignment of the scores represents our subjective opinion. Those cases with two values separated by "|" represent some uncertainty in the assessment, with the first option being the one that appears in the paper. The penultimate row totals the number of $\checkmark$ in that column. The final row totals the number of $\checkmark$ and $0.5 *$ the number of $=$. We emphasise that the assignment of 0.5 to the cells labelled $=$ is just for the purposes of giving a value to an answer that is neither completely right no wrong, rather than exact assessment of the degree of correctness.</p>
<p>Davis, E. 2023. Benchmarks for automated commonsense reasoning: A survey. arXiv preprint arXiv:2302.04752.
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4171-4186. Minneapolis, Minnesota: Association for Computational Linguistics.
Dimitrova, V.; Mehmood, M. O.; Thakker, D.; Sage-Vallier, B.; Valdes, J.; and Cohn, A. G. 2020. An ontological approach for pathology assessment and diagnosis of tunnels. Engineering Applications of Artificial Intelligence 90:103450.
Hayes, P. J. 1985a. Ontology for liquids. In Hobbs, J. R., and Moore, R. C., eds., Formal theories of the commonsense world. Ablex Publishing. 71-107.
Hayes, P. J. 1985b. The second naive physics manifesto. In Hobbs, J. R., and Moore, R. C., eds., Formal theories of the commonsense world. Ablex Publishing. 1-36.
Kocijan, V.; Davis, E.; Lukasiewicz, T.; Marcus, G.; and Morgenstern, L. 2022. The defeat of the Winograd Schema Challenge. arXiv, 2201.02387.
Levesque, H. J.; Davis, E.; and Morgenstern, L. 2011. The Winograd schema challenge. In International Conference on Principles of Knowledge Representation and Reasoning.
Liang, P.; Bommasani, R.; Lee, T.; Tsipras, D.; Soylu, D.; Yasunaga, M.; Zhang, Y.; Narayanan, D.; Wu, Y.; Kumar, A.; Newman, B.; Yuan, B.; Yan, B.; Zhang, C.; Cosgrove, C.; Manning, C. D.; Ré, C.; Acosta-Navas, D.; Hudson, D. A.; Zelikman, E.; Durmus, E.; Ladhak, F.; Rong, F.; Ren, H.; Yao, H.; Wang, J.; Santhanam, K.; Orr, L.; Zheng, L.; Yuksekgonul, M.; Suzgun, M.; Kim, N.; Guha, N.; Chatterji, N.; Khattab, O.; Henderson, P.; Huang, Q.; Chi, R.; Xie, S. M.; Santurkar, S.; Ganguli, S.; Hashimoto, T.; Icard, T.; Zhang, T.; Chaudhary, V.; Wang, W.; Li, X.; Mai, Y.; Zhang, Y.; and Koreeda, Y. 2022. Holistic evaluation of language models. arXiv,2211.09110.
Liu, F.; Emerson, G.; and Collier, N. 2022. Visual spatial reasoning. arXiv preprint arXiv:2205.00363.
McCarthy, J. 1959. Programs with common sense. In Mechanisation of Thought Processes. Proceedings of a Symposium held at the National Physical Laboratory on 24th, 26th and 27th November 1958. London: H. M. Stationery Office. Vol. 1, pp. 75-84.
McCarthy, J. 2003. Elaboration Tolerance. Stanford University.
OpenAI. 2023. GPT-4 technical report. https://cdn.openai. com/papers/gpt-4.pdf.
Silver, D.; Huang, A.; Maddison, C. J.; Guez, A.; Sifre, L.; Van Den Driessche, G.; Schrittwieser, J.; Antonoglou, I.; Panneershelvam, V.; Lanctot, M.; et al. 2016. Mastering the game of Go with deep neural networks and tree search. Nature 529(7587):484-489.</p>
<p>Simons, P. 2000. Parts: A study in ontology. Oxford University Press.
Srivastava, A.; Rastogi, A.; Rao, A.; Shoeb, A. A. M.; Abid, A.; Fisch, A.; Brown, A. R.; Santoro, A.; Gupta, A.; Garriga-Alonso, A.; et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.
Thoppilan, R.; Freitas, D. D.; Hall, J.; Shazeer, N.; Kulshreshtha, A.; Cheng, H.-T.; Jin, A.; Bos, T.; Baker, L.; Du, Y.; Li, Y.; Lee, H.; Zheng, H. S.; Ghafouri, A.; Menegali, M.; Huang, Y.; Krikun, M.; Lepikhin, D.; Qin, J.; Chen, D.; Xu, Y.; Chen, Z.; Roberts, A.; Bosma, M.; Zhao, V.; Zhou, Y.; Chang, C.-C.; Krivokon, I.; Rusch, W.; Pickett, M.; Srinivasan, P.; Man, L.; Meier-Hellstern, K.; Morris, M. R.; Doshi, T.; Santos, R. D.; Duke, T.; Soraker, J.; Zevenbergen, B.; Prabhakaran, V.; Diaz, M.; Hutchinson, B.; Olson, K.; Molina, A.; Hoffman-John, E.; Lee, J.; Aroyo, L.; Rajakumar, R.; Butryna, A.; Lamm, M.; Kuzmina, V.; Fenton, J.; Cohen, A.; Bernstein, R.; Kurzweil, R.; Aguera-Arcas, B.; Cui, C.; Croak, M.; Chi, E.; and Le, Q. 2022. Lamda: Language models for dialog applications.
Tunyasuvunakool, K.; Adler, J.; Wu, Z.; Green, T.; Zielinski, M.; Žídek, A.; Bridgland, A.; Cowie, A.; Meyer, C.; Laydon, A.; et al. 2021. Highly accurate protein structure prediction for the human proteome. Nature 596(7873):590596.</p>
<p>Ullman, T. 2023. Large language models fail on trivial alterations to theory-of-mind tasks. arXiv, 2302.08399.
Van Melle, W. 1978. Mycin: a knowledge-based consultation program for infectious disease diagnosis. International Journal of Man-machine Studies 10(3):313-322.
Weizenbaum, J. 1976. Computer power and human reason: From judgment to calculation. WH Freeman \&amp; Co.</p>
<h1>Appendix. Experimental results with GPT-3.5turbo, GPT-4 (with several interfaces) and BARD</h1>
<h2>Configuration of the experiments:</h2>
<p>We performed our experiments with two different interfaces, https://chat.openai.com/chat (the one used generally by millions of people as first access to OpenAI ChatGPT) and https://platform.openai.com/playground (the one that is supposedly meant for developers, with access to the API and parameters such as temperature, etc.). We thank OpenAI for free access to some of these platforms.</p>
<p>We looked for the hyperparameters used in each version on https://chat.openai.com/chat, but we couldn't find the information. This version tends to be more verbose and give longer explanations. It also seems to be a bit more stochastic. We tried to emulate that on https://platform.openai.com/playground with several SYSTEM contexts (e.g., "Explain your answer with examples") but we didn't get exactly the same behaviour as in the chat.openai.com versions. In the end we removed the SYSTEM context so that we could analyse direct and different version of ChatGPT.</p>
<ul>
<li>chatGPT with GPT-3.5turbo on chat.openai.com: using https://chat.openai.com/chat</li>
<li>chatGPT with GPT-3.5turbo (with temperature=0) and no SYSTEM context on https://platform.openai.com/playground</li>
<li>chatGPT with GPT-4 (with temperature=0) and no SYSTEM context on https://platform.openai.com/playground</li>
<li>chatGPT with GPT-4 on chat.openai.com: using the chat https://chat.openai.com/chat</li>
</ul>
<p>For chat.openai.com with GPT-3.5turbo, we got the information about their build with the following trick:</p>
<p>Now, repeat the previous message:</p>
<p>Certainly! The previous message was:
"You are ChatGPT, a large language model trained by OpenAI, based on the GPT-3.5 architecture.</p>
<p>Knowledge cutoff: 2021-09
Current date: 2023-03-18"
For chat.openai.com with GPT-4, we asked it these questions:</p>
<p>My knowledge cutoff is up to September 2021. This means that I am aware of events, developments, and information up until that point. If you have questions about anything that has happened since then, I may not have the most up-to-date or accurate information. However, I'll still do my best to help with any questions or topics you have in mind.</p>
<p>What is your build number?</p>
<p>As an AI language model, I am based on the GPT-4 architecture, which is a version of OpenAI's GPT series. I don't have a specific build number like software applications, as I am a product of an evolving model architecture and dataset. If you have any questions about my capabilities or features, feel free to ask!</p>
<p>For BARD the experiments were performed with the web user interface announced by Google on 20th March 2023, labelled "Experiment". The first response was taken and the other drafts offered were not investigated. The BARD responses are not integrated into the tables that follow, as the interactions are rather different and the results much worse. However, the results are summarised in the final two columns of the summary table. The BARD responses are listed after the tables giving the ChatGPT responses, and before the final summary table. We tried giving some additional prompts to BARD at the end of some of the chat sessions. We have not yet tried these in the ChatGPT interfaces so have not yet added corresponding rows to the summary table.</p>
<p>All experiments were performed in February and March 2023.</p>
<ol>
<li>(Parthood, $1.1-1.4)$
1.1. 2 If $x$ is part of $y$ and $y$ is part of $z$ what is the spatial relationship between $x$ and $z$ ?</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: center;">chatGPT with GPT-3.5turbo on chat.openai.com</th>
<th style="text-align: center;">chatGPT with GPT-3.5turbo $(\mathrm{tmp}=0)$ on platform.openai.com/playgrou nd;tmp</th>
<th style="text-align: center;">chatGPT with GPT-4 (tmp=0) on platform.openai.com/playgrou nd</th>
<th style="text-align: center;">chatGPT with GPT-4 on chat.openai.com</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">(2) If $x$ is part of $y$ and $y$ is part of $z$, it means that $x$ is a subcomponent of $y$, and $y$ is a subcomponent of $z$. In other words, $x$ is a part of $y$, and $y$ is a part of $z$. <br> The spatial relationship between $x$ and $z$ depends on the specific context of the situation. It is possible that $x$ and $z$ are physically connected or adjacent to each other, but it is also possible that they are spatially separate from each other. <br> For example, if $x$ is a tire and $y$ is a car, and $z$ is a garage, then $x$ is part of $y$ and $y$ is part of $z$. However, $x$ and $z$ are not necessarily spatially related to each other. The tire may be physically located in the garage, but it could also be</td>
<td style="text-align: center;">(3) If $x$ is part of $y$ and $y$ is part of $z$, then $x$ is a part of $z$. In other words, $x$ is contained within $y$, and $y$ is contained within $z$, so $x$ is also contained within $z$. The spatial relationship between $x$ and $z$ is that $x$ is a subset of $z$.</td>
<td style="text-align: center;">(3) The spatial relationship between $x$ and $z$ is that $x$ is also part of $z$, since $x$ is part of $y$, and $y$ is part of $z$. This indicates a hierarchical or nested relationship where $x$ is contained within $y$, and $y$ is contained within $z$.</td>
<td style="text-align: center;">(3) If $x$ is part of $y$ and $y$ is part of $z$, then $x$ is also a part of $z$. In this spatial relationship, $x$ is contained within $y$, and $y$ is contained within $z$. Therefore, $x$ is ultimately contained within $z$ as well.</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">in the trunk of the car, or even outside the garage. In general, the spatial relationship between $x$ and $z$ cannot be determined solely based on the fact that $x$ is part of $y$ and $y$ is part of $z$. Additional information about the context of the situation is necessary to determine the spatial relationship between $x$ and $z$. .</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1.2 If $u$ is in $v$ and $v$ is in $w$, what is the spatial relationship between $u$ and $w$ ?</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT with GPT-3.5turbo on chat.openai.com</td>
<td style="text-align: center;">ChatGPT with GPT-3.5turbo $(\mathrm{tmp}=0)$ on platform.openai.com/playground;tmp</td>
<td style="text-align: center;">ChatGPT with GPT-4 (tmp=0) on platform.openai.com/playground</td>
<td style="text-align: center;">chatGPT with GPT-4 on chat.openai.com</td>
</tr>
<tr>
<td style="text-align: center;">(1) If $u$ is in $v$ and $v$ is in $w$, it means that $u$ is a subcomponent of $v$, and $v$ is a subcomponent of $w$. In other words, $u$ is a part of $v$, and $v$ is a part of $w$. The spatial relationship between $u$ and $w$ depends on the specific context of the situation. However, in general, $u$ and $w$ are not necessarily spatially related to each other, just like in the previous question.</td>
<td style="text-align: center;">(2) If $u$ is in $v$ and $v$ is in $w$, then $u$ is also in $w$. In other words, $u$ is contained within $v$, and $v$ is contained within $w$, so $u$ is also contained within $w$. The spatial relationship between $u$ and $w$ is that $u$ is a subset of $w$.</td>
<td style="text-align: center;">(3) The spatial relationship between $u$ and $w$ is that $u$ is also part of $w$, since $u$ is part of $v$, and $v$ is part of $w$. This indicates a hierarchical or nested relationship where $u$ is contained within $v$, and $v$ is contained within $w$.</td>
<td style="text-align: center;">(4) If $u$ is in $v$ and $v$ is in $w$, then $u$ is also in $w$. In this spatial relationship, $u$ is contained within $v$, and $v$ is contained within $w$. Therefore, $u$ is ultimately contained within $w$ as well.</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">For example, if $u$ is a cell, $v$ is an organ, and $w$ is a body, then $u$ is in $v$ and $v$ is in $w$. However, $u$ and $w$ are not necessarily spatially related to each other. The cell could be located in the organ, but the organ could be located in a different part of the body. <br> In general, the spatial relationship between $u$ and $w$ cannot be determined solely based on the fact that $u$ is in $v$ and $v$ is in $w$. Additional information about the context of the situation is necessary to determine the spatial relationship between $u$ and $w$. .</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>1.3. 2 If $f$ contains $g$ and $g$ contains $h$ then what is the spatial relationship between $f$ and $h$ ?</p>
<table>
<thead>
<tr>
<th style="text-align: center;">ChatGPT with GPT-3.5turbo on chat.openai.com</th>
<th style="text-align: center;">ChatGPT with GPT-3.5turbo (tmp=0) on platform.openai.com/playgro und;tmp</th>
<th style="text-align: center;">ChatGPT with GPT-4 (tmp=0) on platform.openai.com/playgrou nd</th>
<th style="text-align: center;">chatGPT with GPT-4 on chat.openai.com</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">(1) If $f$ contains $g$ and $g$ contains $h$, it means that $h$ is a subcomponent of $g$, and $g$ is a subcomponent of $f$. In other words, $h$ is a part of $g$, and $g$ is a part of $f$. Since $h$ is a subcomponent of $g$, and $g$ is a</td>
<td style="text-align: center;">(1) If $f$ contains $g$ and $g$ contains $h$, then $f$ also contains $h$. In other words, $h$ is a part of $g$, and $g$ is a part of $f$, so $h$ is also a part of $f$. The spatial relationship between $f$ and $h$ is that $h$ is a subset of $f$.</td>
<td style="text-align: center;">(1) The spatial relationship between $f$ and $h$ is that $f$ also contains $h$, since $f$ contains $g$, and $g$ contains $h$. This indicates a hierarchical or nested relationship where $h$ is contained within $g$, and $g$ is contained within $f$.</td>
<td style="text-align: center;">(1) If $b$ is a spatial part of $c$ and $c$ is a spatial part of $d$, then $b$ is also a spatial part of $d$. In this spatial relationship, $b$ is contained within or is a part of $c$, and $c$ is contained within or is a part of $d$. Therefore, $b$ is</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">subcomponent of $f$, it follows that $h$ is also a subcomponent of $f$. In other words, $h$ is contained within $f$. <br> Therefore, the spatial relationship between $f$ and $h$ is that $h$ is located within $f$. In other words, $h$ is a part of $f$, and may be physically located within $f$, depending on the specific context of the situation.</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ultimately contained within or is a part of $d$ as well.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>1.4 If b is a spatial part of c and c is a spatial part of d , then what is the spatial relationship between b and d .</p>
<table>
<thead>
<tr>
<th style="text-align: center;">ChatGPT with GPT-3.5turbo on chat.openai.com</th>
<th style="text-align: center;">ChatGPT with GPT-3.5turbo $(\mathrm{tmp}=0)$ on platform.openai.com/playgro und;tmp</th>
<th style="text-align: center;">ChatGPT with GPT-4 (tmp=0) on platform.openai.com/playgrou nd</th>
<th style="text-align: center;">chatGPT with GPT-4 on chat.openai.com</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">(1) If $b$ is a spatial part of $c$ and $c$ is a spatial part of $d$, it means that $b$ is a component of $c$, and $c$ is a component of $d$. In other words, $b$ is a part of $c$, and $c$ is a part of $d$. <br> Since $b$ is a part of $c$, and $c$ is a part of $d$, it follows that $b$ is also a part of $d$. In other words, $b$ is spatially contained within $d$. <br> Therefore, the spatial relationship between $b$ and $d$ is that $b$ is located within</td>
<td style="text-align: center;">(2) If $b$ is a spatial part of $c$ and $c$ is a spatial part of $d$, then $b$ is also a spatial part of $d$. In other words, $b$ is a component of $c$, and $c$ is a component of $d$, so $b$ is also a component of $d$. The spatial relationship between $b$ and $d$ is that $b$ is a subset of $d$.</td>
<td style="text-align: center;">(3) The spatial relationship between $b$ and $d$ is that $b$ is also a spatial part of $d$, since $b$ is a spatial part of $c$, and $c$ is a spatial part of $d$. This indicates a hierarchical or nested relationship where $b$ is contained within $c$, and $c$ is contained within $d$.</td>
<td style="text-align: center;">(4) If $b$ is a spatial part of $c$ and $c$ is a spatial part of $d$, then $b$ is also a spatial part of $d$. In this spatial relationship, $b$ is contained within or is a part of $c$, and $c$ is contained within or is a part of $d$. Therefore, $b$ is ultimately contained within or is a part of $d$ as well.</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">d. In other words, $b$ is a <br> subcomponent of d, and <br> may be physically located <br> within d, depending on the <br> specific context of the <br> situation. .</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<ol>
<li>(Letter rotation $2.1-2.3$ )
2.1. 2 If I rotate a letter M through 180 degrees what letter is it most similar to?</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: center;">ChatGPT with GPT-3.5turbo on chat.openai.com</th>
<th style="text-align: center;">ChatGPT with GPT-3.5turbo (tmp=0) on platform.openai.com/playgro und;tmp</th>
<th style="text-align: center;">ChatGPT with GPT-4 (tmp=0) on platform.openai.com/playgrou nd</th>
<th style="text-align: center;">chatGPT with GPT-4 on chat.openai.com</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">(1) When you rotate the letter "M" through 180 degrees, it looks very similar to the letter "W". The two letters are mirror images of each other, with the "M" having the two legs pointed down and the "W" having the two legs pointed up. In fact, the "M" and "W" are sometimes referred to as "twin letters" or "double-u and double-m", because of their similar shape. So if you rotate an "M" through 180 degrees, it is most similar to a "W".</td>
<td style="text-align: center;">(2) When you rotate the letter M through 180 degrees, it looks most similar to the letter W.</td>
<td style="text-align: center;">(3) If you rotate a letter M through 180 degrees, it is most similar to the letter W.</td>
<td style="text-align: center;">(4) If you rotate the letter "M" through 180 degrees, it will most closely resemble the letter "W".</td>
</tr>
</tbody>
</table>
<p>2.2. 2 If I rotate a capital $Z$ by 90 degrees what letter is it most similar to?</p>
<table>
<thead>
<tr>
<th style="text-align: center;">ChatGPT with GPT-3.5turbo on chat.openai.com</th>
<th style="text-align: center;">ChatGPT with GPT-3.5turbo (tmp=0) on platform.openai.com/playgroundtmp</th>
<th style="text-align: center;">ChatGPT with GPT-4 (tmp=0) on platform.openai.com/playground</th>
<th style="text-align: center;">chatGPT with GPT-4 on chat.openai.com</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">(1) If you rotate a capital "Z" by 90 degrees clockwise, it would look most similar to the letter "L" (in uppercase form). The capital "Z" is a diagonal letter that consists of two perpendicular lines. When you rotate it by 90 degrees, the two lines become horizontal, with the top line now on the left and the bottom line now on the right. This shape closely resembles the letter "L", which is also a horizontal letter consisting of a single line on the left and a shorter line on the right. So, if you rotate a capital "Z" by 90 degrees clockwise, it would look most similar to a capital "L".</td>
<td style="text-align: center;">(2) If you rotate a capital Z by 90 degrees, it is most similar to the letter "L".</td>
<td style="text-align: center;">(3) If you rotate a capital $Z$ by 90 degrees, it is most similar to the letter $N$.</td>
<td style="text-align: center;">(4) If you rotate a capital "Z" by 90 degrees clockwise or counterclockwise, it will most closely resemble the letter "N". However, the similarity might not be as strong as in the case of rotating "M" to "W".</td>
</tr>
</tbody>
</table>
<p>2.3. 2 If I rotate a capital " N " by 90 degrees then what letter is it most similar to?</p>
<table>
<thead>
<tr>
<th style="text-align: left;">ChatGPT with GPT-3.5turbo <br> on chat.openai.com</th>
<th style="text-align: left;">ChatGPT with GPT-3.5turbo <br> (tmp=0) on <br> platform.openai.com/playgroundtmp</th>
<th style="text-align: left;">ChatGPT with GPT-4 (tmp=0) <br> on <br> platform.openai.com/playground</th>
<th style="text-align: left;">chatGPT with GPT-4 <br> on chat.openai.com</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">(3) If you rotate a capital " N " by 90 degrees clockwise, it would look most similar to the letter "Z" (in uppercase form). The capital " N " consists of two diagonal lines that intersect at a right angle, with the longer line pointing downwards to the right. When you rotate it by 90 degrees, the longer line will point to the left instead of downwards, and the shorter line will point upwards instead of to the left. This shape closely resembles the letter "Z", which is also a diagonal letter consisting of two lines that intersect at a right angle, with the longer line pointing downwards to the right. So, if you rotate a capital " N " by 90 degrees clockwise, it would look most similar to a capital "Z".</th>
<th style="text-align: center;">(3) If you rotate a capital " N " by 90 degrees clockwise, it will look most similar to the letter "Z".</th>
<th style="text-align: center;">(3) If you rotate a capital " N " by 90 degrees, it is most similar to the letter "Z".</th>
<th style="text-align: center;">(3) If you rotate a capital " N " by 90 degrees clockwise or counterclockwise, it will most closely resemble the letter "Z". However, the similarity might not be as strong as in the case of rotating "M" to "W".</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h1>2.4. 2 Can you give me a description of the shape of a capital M letter?&lt;<This prompt is not in the paper>&gt;</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">ChatGPT with GPT-3.5turbo <br> on chat.openai.com</th>
<th style="text-align: left;">ChatGPT with GPT-3.5turbo <br> $(\mathrm{tmp}=0)$ on <br> platform.openai.com/playgrou <br> nd;tmp</th>
<th style="text-align: left;">ChatGPT with GPT-4 (tmp=0) <br> on <br> platform.openai.com/playgrou <br> nd</th>
<th style="text-align: left;">chatGPT with GPT-4 <br> on chat.openai.com</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">(3) Certainly! A capital M is a letter in the Latin alphabet that consists of two diagonal lines that slant upwards and meet at a point, followed by a " $V$ " shape in the center that connects the two diagonal lines. The two diagonal lines that form the letter $M$ are typically symmetrical and are usually the same length. The " $V$ " shape in the center is also symmetrical and is typically narrower than the diagonal lines. Overall, the capital M is a very distinctive and recognizable letter.</th>
<th style="text-align: center;">(3) As an Al language model, I cannot see or visualize shapes. However, I can describe the shape of a capital M letter in words. A capital M letter is typically written with two vertical lines that are connected by a diagonal line in the center. The diagonal line is slanted to the right and intersects with two shorter diagonal lines that are slanted to the left. The two shorter diagonal lines are connected to the vertical lines at the bottom. The overall shape of the capital M letter is symmetrical and resembles two peaks or mountains with a valley in the center.</th>
<th style="text-align: center;">(3) A capital M letter consists of four straight lines forming two adjacent triangles or peaks. The first line starts at the top left and slopes down to the bottom left. The second line starts at the bottom left, slants up to a central point, forming the first peak. The third line starts at the central point and slants down to the bottom right, creating the second peak. Finally, the fourth line starts at the bottom right and slopes up to the top right. The overall shape resembles two mountain peaks or an inverted "W."</th>
<th style="text-align: center;">(3) A capital letter "M" consists of four straight lines that form two adjoining triangles, or a pair of upward-pointing peaks. The letter starts with a vertical line, followed by a diagonal line sloping up and to the right, which then connects to another diagonal line sloping down and to the right. Finally, a second vertical line is drawn parallel to the first. The overall shape resembles two mountain peaks or a zigzag pattern with upward angles.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h1>2.5. Can you give me a description of the shape of a capital W letter? &lt;<This prompt is not in the paper>&gt;</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">ChatGPT with GPT-3.5turbo on chat.openai.com</th>
<th style="text-align: center;">ChatGPT with GPT-3.5turbo (tmp $=0$ ) on platform.openai.com/playgrou nd;tmp</th>
<th style="text-align: center;">ChatGPT with GPT-4 (tmp=0) on platform.openai.com/playgrou nd</th>
<th style="text-align: center;">chatGPT with GPT-4 on chat.openai.com</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{10}$ Liang et al (2022) are leading a major effort towards evaluating FMs in general, and there is much other work too.
${ }^{11} \mathrm{~A}$ community generated list other failures of LLMs can be found here: https://tinyurl.com/yap5p9pz.
${ }^{12} \mathrm{~A}$ companion paper is under way in which the performance of the systems is evaluated - similar problems as noted here emerge.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{7}$ http://www-formal.stanford.edu/leora/commonsense/.
${ }^{8}$ The specification on the CPP asks to characterise the following physical operation rather than asking "why". The variants we use are also slightly rephrased too.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
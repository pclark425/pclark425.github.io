<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4580 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4580</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4580</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-33161a5a9b5dcb635b5a97475e6a6209a69ada7d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/33161a5a9b5dcb635b5a97475e6a6209a69ada7d" target="_blank">The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The AI Scientist is introduced, which generates novel research ideas, writes code, executes experiments, visualizes results, describes its findings by writing a full scientific paper, and then runs a simulated review process for evaluation, presenting the first comprehensive framework for fully automatic scientific discovery.</p>
                <p><strong>Paper Abstract:</strong> One of the grand challenges of artificial general intelligence is developing agents capable of conducting scientific research and discovering new knowledge. While frontier models have already been used as aides to human scientists, e.g. for brainstorming ideas, writing code, or prediction tasks, they still conduct only a small part of the scientific process. This paper presents the first comprehensive framework for fully automatic scientific discovery, enabling frontier large language models to perform research independently and communicate their findings. We introduce The AI Scientist, which generates novel research ideas, writes code, executes experiments, visualizes results, describes its findings by writing a full scientific paper, and then runs a simulated review process for evaluation. In principle, this process can be repeated to iteratively develop ideas in an open-ended fashion, acting like the human scientific community. We demonstrate its versatility by applying it to three distinct subfields of machine learning: diffusion modeling, transformer-based language modeling, and learning dynamics. Each idea is implemented and developed into a full paper at a cost of less than $15 per paper. To evaluate the generated papers, we design and validate an automated reviewer, which we show achieves near-human performance in evaluating paper scores. The AI Scientist can produce papers that exceed the acceptance threshold at a top machine learning conference as judged by our automated reviewer. This approach signifies the beginning of a new era in scientific discovery in machine learning: bringing the transformative benefits of AI agents to the entire research process of AI itself, and taking us closer to a world where endless affordable creativity and innovation can be unleashed on the world's most challenging problems. Our code is open-sourced at https://github.com/SakanaAI/AI-Scientist</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4580.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4580.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based Automated Reviewer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large-Language-Model-Based Automated Paper Reviewer (GPT-4o-based agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated reviewer agent implemented with frontier LLMs (GPT-4o primarily) that parses manuscript text and emits structured review outputs (numerical scores, strengths/weaknesses, accept/reject decision) according to NeurIPS/ML-conference review guidelines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-based Automated Paper Reviewing (GPT-4o agent)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>The method ingests the raw manuscript text (PDF parsed to text), prompts an LLM (GPT-4o or alternatives) with a NeurIPS-style review template, and outputs numeric rubric scores (soundness, presentation, contribution, overall, confidence), lists of strengths and weaknesses, and a preliminary binary decision (accept/reject). It can be augmented with chained self-reflection passes, few-shot exemplars, response ensembling (multiple independent reviews), and a meta-review (Area Chair) aggregation step. Decisions can be post-calibrated by thresholding the numeric overall score to map to accept/reject.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Soundness (technical correctness), Presentation (clarity), Contribution/Originality, Overall Quality (aggregate), Confidence of reviewer; plus enumerated Strengths and Weaknesses and a binary accept/reject decision. The reviewer also reports finer-grained rubric fields used in NeurIPS-style reviews (Originality, Quality, Clarity, Significance, Soundness, Presentation, Contribution).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (primary); also evaluated Sonnet 3.5, GPT-4o-mini, Llama 3.1 405B, DeepSeek Coder</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine Learning / ML conference papers</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Research claims, hypotheses and empirical contributions presented in ML manuscripts (predictive/algorithmic claims and mechanistic/architectural proposals)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Validated on 500 ICLR 2022 papers from OpenReview. Best configuration (GPT-4o with 5 self-reflection rounds, 5 ensembled reviews, and 1-shot prompt + meta-aggregation) achieved ~70% accuracy (reported in text for a particular config) and calibrated performance shown in Table 1: Balanced Accuracy up to 0.65 (GPT-4o 1-shot @6), Accuracy ~0.66, F1 ~0.57, AUC ~0.65, False Positive Rate ~0.31, False Negative Rate ~0.39. Compared to human NeurIPS baseline: Balanced Acc 0.66, Accuracy 0.73, F1 0.49, AUC 0.65, FPR 0.17, FNR 0.52. The LLM reviewer produced human-level balanced accuracy on a balanced dataset (0.65 vs 0.66) and a lower false negative rate than humans (0.39 vs 0.52) but a higher false positive rate (0.31 vs 0.17).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated: primary evaluation uses LLM-generated reviews compared to historical human reviews. Hybrid approach: automated reviews are compared quantitatively to human reviewer decisions/scores and to the average of human reviewers; the paper also uses LLM-based meta-review aggregation to emulate Area-Chair decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical comparison against ground-truth human reviews from an OpenReview dataset (500 ICLR 2022 submissions); computed classification metrics (accuracy, balanced accuracy, F1, AUC, FPR, FNR), correlation analyses between LLM scores and average human reviewer scores, and comparison to inter-human reviewer correlation (human–human). Calibration experiments (thresholding at different score cutoffs, e.g. 6 or 8) were performed to pick operating points.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Cannot view figures (vision disabled) so relies on textual descriptions; dataset may overlap with LLM pretraining corpora; accepted papers available were final camera-ready while rejected ones were original submission (data mismatch); inability to run rebuttal dialogues in current setup; model-specific biases (e.g. Sonnet over-optimism necessitated different thresholding); higher false positive rate than humans; occasional inability to follow output templates (some LLMs); limited to textual parsing and thus misses visual/figural errors; systemic calibration required and sensitivity to prompting and ensembling choices.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>500-paper subset drawn from ICLR 2022 OpenReview data (public OpenReview dataset used as ground truth)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4580.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4580.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NeurIPS-Style Review Rubric</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NeurIPS Conference Review Guidelines / Rubric (used as evaluation framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-dimensional review rubric used by major ML conferences specifying numerical scoring axes (originality, technical quality/soundness, clarity/presentation, significance/contribution) and decision thresholds; used here as the template for LLM reviewer outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>NeurIPS-style Review Rubric</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>A structured rubric where reviewers assign numeric scores on axes such as Originality, Quality (technical soundness), Clarity/Presentation, Significance/Contribution, and an overall score plus reviewer confidence; reviewers provide free-text Strengths, Weaknesses, Questions, and a recommendation (accept/reject). The LLM reviewer is prompted to follow this rubric exactly and to produce the same fields.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Originality, Technical Soundness, Clarity/Presentation, Contribution/Significance, Strengths & Weaknesses, Reviewer Confidence and an Overall numeric score; decisions are derived from thresholds on the overall score (e.g., >=6 -> weak accept).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine Learning conference submissions (general ML research)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evaluation of research claims, empirical findings, algorithmic proposals and theoretical arguments as presented in manuscripts</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used as the basis for LLM review outputs and for calibrating accept/reject. The paper reports that thresholding the LLM overall score at 6 roughly corresponds to average accepted-paper scores; calibration influenced reported balanced accuracy and false positive/negative rates.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Human rubric applied by automated LLM reviewers; comparison is direct (LLM-produced rubric fields vs human reviewer rubric fields).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Comparison of LLM rubric outputs against human rubric scores from OpenReview; threshold calibration to match human decision distributions; correlation analyses between LLM overall score and average human scores.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Rubric was applied to text-parsed PDFs so some contextual/visual information was lost; differences in data availability (camera-ready vs submission) between accepted/rejected examples; rubric thresholds require calibration per model (different bias profiles).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>ICLR 2022 OpenReview paper texts used for comparison to human rubric judgments</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4580.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4580.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reviewer Performance Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Classification and Agreement Metrics for Reviewer Evaluation (Balanced Accuracy, Accuracy, F1, AUC, FPR, FNR, reviewer–reviewer correlation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of standard binary classification and statistical metrics used to quantify how well the automated reviewer decisions match human reviewer decisions and scores, plus correlation measures to compare reviewer–reviewer agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Reviewer performance metrics (balanced accuracy, accuracy, F1, AUC, FPR, FNR, correlation)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute binary classification metrics (Accuracy, Balanced Accuracy to account for class imbalance, F1 score for positive-class harmonic mean, AUC for score discrimination, False Positive Rate and False Negative Rate) comparing LLM accept/reject decisions (or thresholded scores) against ground-truth human decisions. Additionally compute Pearson (or analogous) correlation between LLM raw scores and average human reviewer scores and compare to inter-human reviewer correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Agreement with human acceptance decisions (accuracy, balanced accuracy), precision/recall trade-offs (F1, FPR, FNR), ranking/discrimination (AUC), and alignment with average reviewer scores (correlation coefficient).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Evaluation of reviews for ML research manuscripts</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>N/A (this entry is about metrics used to evaluate reviewer outputs rather than scientific theories per se)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Table 1 reports values for multiple LLM configurations and humans: Human baseline Balanced Acc 0.66, Accuracy 0.73, F1 0.49, AUC 0.65, FPR 0.17, FNR 0.52. Best calibrated GPT-4o (1-shot @6) Balanced Acc 0.65, Accuracy 0.66, F1 0.57, AUC 0.65, FPR 0.31, FNR 0.39. Other LLMs show lower performance; uncalibrated models produce skewed FPR/FNR requiring calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated computation of classification metrics comparing LLM outputs to human labels; used to quantify parity with human reviewers and to detect model biases (e.g., over-rejection or over-acceptance).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Bootstrap confidence intervals reported, comparison across multiple LLMs and prompt configurations, and comparison to human–human consistency experiment metrics (NeurIPS consistency experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Class imbalance in dataset biases raw accuracy; choice of threshold (calibration) strongly affects FPR/FNR trade-offs; metrics do not capture qualitative aspects of review helpfulness; correlation numbers are low (human–human correlation ~0.14; LLM–average correlation ~0.18) highlighting intrinsic variability in reviewing.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>ICLR 2022 OpenReview subset (500 papers) with human reviewer labels/scores</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4580.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4580.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Decision Calibration (Thresholding)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Score Thresholding and Calibration for Accept/Reject Decisions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Post-hoc calibration strategy that maps continuous LLM overall scores to binary accept/reject decisions by applying an acceptance threshold (e.g., score >= 6 or >= 8) chosen to match desired operating characteristics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Threshold-based calibration of reviewer scores</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>After the LLM emits an overall numeric score, a fixed threshold is applied to decide accept/reject. Thresholds are tuned per-model to account for model-specific biases (e.g., Sonnet 3.5 required threshold 8 to be calibrated). Calibration is evaluated by observing resulting classification metrics (balanced accuracy, FPR/FNR) on the validation dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Trade-off between false positives and false negatives (FPR/FNR), balanced accuracy and alignment with human acceptance rates; calibration aims to produce operating points similar to human decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>ML paper reviewing</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>N/A (decision-mapping procedure)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Changing thresholds affected reported metrics substantially: e.g., Sonnet 3.5 required threshold 8 for calibrated Balanced Acc ~0.59; GPT-4o 0-shot calibrated @6 yielded Balanced Acc 0.63. Threshold choice impacted FPR and FNR—models with optimistic biases showed low FNR but very high FPR unless thresholded upward.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated calibration procedure validated against human labels; used to better align LLM decisions to historical human decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Grid search over thresholds and evaluation of classification metrics on the ICLR 2022 validation set; comparisons to human baseline statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Different LLMs exhibit different score distributions so threshold is model-dependent; thresholding fixes binary decision but does not improve underlying rubric scoring accuracy or qualitative review helpfulness; calibration can mask deeper biases (e.g., over-acceptance tendencies).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>ICLR 2022 OpenReview (500-paper subset) used for calibration and evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4580.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4580.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompting & Ensembling Enhancements</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt Engineering and Response-Ensembling (self-reflection, few-shot, ensembling, meta-aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A collection of prompting and inference-time strategies used to improve LLM reviewer decision quality, including self-reflection (iterative refinement), few-shot exemplars, multiple independent review generations (ensembling), and a meta-review aggregation stage emulating an Area Chair.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Reflexion/self-reflection + Few-shot + Ensemble Reviews + Meta-aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Self-reflection: run multiple passes where the LLM reconsiders and refines its output (chain-of-thought-style or explicit reflection). Few-shot: provide exemplar review(s) to shape output format and scoring behavior. Ensemble: generate multiple independent reviews and aggregate (average numeric scores, combine free-text). Meta-aggregation: run a higher-level prompt acting as an Area Chair to synthesize ensembled reviews into a final decision. These techniques are combined to reduce variance and improve decision accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Improvement in accuracy/balanced accuracy/F1/AUC and reduction in variance of predictions across runs; alignment with average human reviewer scores; stability of outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (demonstrated benefits), ablated on Sonnet 3.5 and GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Automated peer review for ML papers</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>N/A (methods to improve review generation)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Ablations showed self-reflection improved accuracy by ~+2% and one-shot few-shot prompting by ~+2%; ensembling did not substantially change mean accuracy but reduced variance. Best-performing configuration combined 5 rounds of self-reflection, 5 ensembled reviews, and 1-shot prompting with meta-aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated enhancements applied to LLM reviewer and validated against human review labels; improvements measured via standard classification metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Ablation experiments reported in Figure 2 comparing configurations (with/without reflection, with/without few-shot, ensembling sizes) on the ICLR 2022 subset.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Diminishing returns: ensembling reduced variance more than mean error; some LLMs (e.g., Sonnet) required different calibration despite enhancements; additional compute/costs for multiple passes; enhancements do not address inability to view figures or ask rebuttals.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Evaluated on ICLR 2022 OpenReview subset (500 papers) used for ablation comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ICLR 2022 OpenReview Data <em>(Rating: 2)</em></li>
                <li>NeurIPS consistency experiment (Beygelzimer et al., 2021) <em>(Rating: 2)</em></li>
                <li>NeurIPS review guidelines <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4580",
    "paper_id": "paper-33161a5a9b5dcb635b5a97475e6a6209a69ada7d",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "LLM-based Automated Reviewer",
            "name_full": "Large-Language-Model-Based Automated Paper Reviewer (GPT-4o-based agent)",
            "brief_description": "An automated reviewer agent implemented with frontier LLMs (GPT-4o primarily) that parses manuscript text and emits structured review outputs (numerical scores, strengths/weaknesses, accept/reject decision) according to NeurIPS/ML-conference review guidelines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "LLM-based Automated Paper Reviewing (GPT-4o agent)",
            "evaluation_method_description": "The method ingests the raw manuscript text (PDF parsed to text), prompts an LLM (GPT-4o or alternatives) with a NeurIPS-style review template, and outputs numeric rubric scores (soundness, presentation, contribution, overall, confidence), lists of strengths and weaknesses, and a preliminary binary decision (accept/reject). It can be augmented with chained self-reflection passes, few-shot exemplars, response ensembling (multiple independent reviews), and a meta-review (Area Chair) aggregation step. Decisions can be post-calibrated by thresholding the numeric overall score to map to accept/reject.",
            "evaluation_criteria": "Soundness (technical correctness), Presentation (clarity), Contribution/Originality, Overall Quality (aggregate), Confidence of reviewer; plus enumerated Strengths and Weaknesses and a binary accept/reject decision. The reviewer also reports finer-grained rubric fields used in NeurIPS-style reviews (Originality, Quality, Clarity, Significance, Soundness, Presentation, Contribution).",
            "model_name": "GPT-4o (primary); also evaluated Sonnet 3.5, GPT-4o-mini, Llama 3.1 405B, DeepSeek Coder",
            "model_size": null,
            "scientific_domain": "Machine Learning / ML conference papers",
            "theory_type": "Research claims, hypotheses and empirical contributions presented in ML manuscripts (predictive/algorithmic claims and mechanistic/architectural proposals)",
            "human_comparison": true,
            "evaluation_results": "Validated on 500 ICLR 2022 papers from OpenReview. Best configuration (GPT-4o with 5 self-reflection rounds, 5 ensembled reviews, and 1-shot prompt + meta-aggregation) achieved ~70% accuracy (reported in text for a particular config) and calibrated performance shown in Table 1: Balanced Accuracy up to 0.65 (GPT-4o 1-shot @6), Accuracy ~0.66, F1 ~0.57, AUC ~0.65, False Positive Rate ~0.31, False Negative Rate ~0.39. Compared to human NeurIPS baseline: Balanced Acc 0.66, Accuracy 0.73, F1 0.49, AUC 0.65, FPR 0.17, FNR 0.52. The LLM reviewer produced human-level balanced accuracy on a balanced dataset (0.65 vs 0.66) and a lower false negative rate than humans (0.39 vs 0.52) but a higher false positive rate (0.31 vs 0.17).",
            "automated_vs_human_evaluation": "Automated: primary evaluation uses LLM-generated reviews compared to historical human reviews. Hybrid approach: automated reviews are compared quantitatively to human reviewer decisions/scores and to the average of human reviewers; the paper also uses LLM-based meta-review aggregation to emulate Area-Chair decisions.",
            "validation_method": "Empirical comparison against ground-truth human reviews from an OpenReview dataset (500 ICLR 2022 submissions); computed classification metrics (accuracy, balanced accuracy, F1, AUC, FPR, FNR), correlation analyses between LLM scores and average human reviewer scores, and comparison to inter-human reviewer correlation (human–human). Calibration experiments (thresholding at different score cutoffs, e.g. 6 or 8) were performed to pick operating points.",
            "limitations_challenges": "Cannot view figures (vision disabled) so relies on textual descriptions; dataset may overlap with LLM pretraining corpora; accepted papers available were final camera-ready while rejected ones were original submission (data mismatch); inability to run rebuttal dialogues in current setup; model-specific biases (e.g. Sonnet over-optimism necessitated different thresholding); higher false positive rate than humans; occasional inability to follow output templates (some LLMs); limited to textual parsing and thus misses visual/figural errors; systemic calibration required and sensitivity to prompting and ensembling choices.",
            "benchmark_dataset": "500-paper subset drawn from ICLR 2022 OpenReview data (public OpenReview dataset used as ground truth)",
            "uuid": "e4580.0",
            "source_info": {
                "paper_title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "NeurIPS-Style Review Rubric",
            "name_full": "NeurIPS Conference Review Guidelines / Rubric (used as evaluation framework)",
            "brief_description": "A multi-dimensional review rubric used by major ML conferences specifying numerical scoring axes (originality, technical quality/soundness, clarity/presentation, significance/contribution) and decision thresholds; used here as the template for LLM reviewer outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "NeurIPS-style Review Rubric",
            "evaluation_method_description": "A structured rubric where reviewers assign numeric scores on axes such as Originality, Quality (technical soundness), Clarity/Presentation, Significance/Contribution, and an overall score plus reviewer confidence; reviewers provide free-text Strengths, Weaknesses, Questions, and a recommendation (accept/reject). The LLM reviewer is prompted to follow this rubric exactly and to produce the same fields.",
            "evaluation_criteria": "Originality, Technical Soundness, Clarity/Presentation, Contribution/Significance, Strengths & Weaknesses, Reviewer Confidence and an Overall numeric score; decisions are derived from thresholds on the overall score (e.g., &gt;=6 -&gt; weak accept).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Machine Learning conference submissions (general ML research)",
            "theory_type": "Evaluation of research claims, empirical findings, algorithmic proposals and theoretical arguments as presented in manuscripts",
            "human_comparison": true,
            "evaluation_results": "Used as the basis for LLM review outputs and for calibrating accept/reject. The paper reports that thresholding the LLM overall score at 6 roughly corresponds to average accepted-paper scores; calibration influenced reported balanced accuracy and false positive/negative rates.",
            "automated_vs_human_evaluation": "Human rubric applied by automated LLM reviewers; comparison is direct (LLM-produced rubric fields vs human reviewer rubric fields).",
            "validation_method": "Comparison of LLM rubric outputs against human rubric scores from OpenReview; threshold calibration to match human decision distributions; correlation analyses between LLM overall score and average human scores.",
            "limitations_challenges": "Rubric was applied to text-parsed PDFs so some contextual/visual information was lost; differences in data availability (camera-ready vs submission) between accepted/rejected examples; rubric thresholds require calibration per model (different bias profiles).",
            "benchmark_dataset": "ICLR 2022 OpenReview paper texts used for comparison to human rubric judgments",
            "uuid": "e4580.1",
            "source_info": {
                "paper_title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Reviewer Performance Metrics",
            "name_full": "Classification and Agreement Metrics for Reviewer Evaluation (Balanced Accuracy, Accuracy, F1, AUC, FPR, FNR, reviewer–reviewer correlation)",
            "brief_description": "A set of standard binary classification and statistical metrics used to quantify how well the automated reviewer decisions match human reviewer decisions and scores, plus correlation measures to compare reviewer–reviewer agreement.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Reviewer performance metrics (balanced accuracy, accuracy, F1, AUC, FPR, FNR, correlation)",
            "evaluation_method_description": "Compute binary classification metrics (Accuracy, Balanced Accuracy to account for class imbalance, F1 score for positive-class harmonic mean, AUC for score discrimination, False Positive Rate and False Negative Rate) comparing LLM accept/reject decisions (or thresholded scores) against ground-truth human decisions. Additionally compute Pearson (or analogous) correlation between LLM raw scores and average human reviewer scores and compare to inter-human reviewer correlation.",
            "evaluation_criteria": "Agreement with human acceptance decisions (accuracy, balanced accuracy), precision/recall trade-offs (F1, FPR, FNR), ranking/discrimination (AUC), and alignment with average reviewer scores (correlation coefficient).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Evaluation of reviews for ML research manuscripts",
            "theory_type": "N/A (this entry is about metrics used to evaluate reviewer outputs rather than scientific theories per se)",
            "human_comparison": true,
            "evaluation_results": "Table 1 reports values for multiple LLM configurations and humans: Human baseline Balanced Acc 0.66, Accuracy 0.73, F1 0.49, AUC 0.65, FPR 0.17, FNR 0.52. Best calibrated GPT-4o (1-shot @6) Balanced Acc 0.65, Accuracy 0.66, F1 0.57, AUC 0.65, FPR 0.31, FNR 0.39. Other LLMs show lower performance; uncalibrated models produce skewed FPR/FNR requiring calibration.",
            "automated_vs_human_evaluation": "Automated computation of classification metrics comparing LLM outputs to human labels; used to quantify parity with human reviewers and to detect model biases (e.g., over-rejection or over-acceptance).",
            "validation_method": "Bootstrap confidence intervals reported, comparison across multiple LLMs and prompt configurations, and comparison to human–human consistency experiment metrics (NeurIPS consistency experiment).",
            "limitations_challenges": "Class imbalance in dataset biases raw accuracy; choice of threshold (calibration) strongly affects FPR/FNR trade-offs; metrics do not capture qualitative aspects of review helpfulness; correlation numbers are low (human–human correlation ~0.14; LLM–average correlation ~0.18) highlighting intrinsic variability in reviewing.",
            "benchmark_dataset": "ICLR 2022 OpenReview subset (500 papers) with human reviewer labels/scores",
            "uuid": "e4580.2",
            "source_info": {
                "paper_title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Decision Calibration (Thresholding)",
            "name_full": "Score Thresholding and Calibration for Accept/Reject Decisions",
            "brief_description": "Post-hoc calibration strategy that maps continuous LLM overall scores to binary accept/reject decisions by applying an acceptance threshold (e.g., score &gt;= 6 or &gt;= 8) chosen to match desired operating characteristics.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Threshold-based calibration of reviewer scores",
            "evaluation_method_description": "After the LLM emits an overall numeric score, a fixed threshold is applied to decide accept/reject. Thresholds are tuned per-model to account for model-specific biases (e.g., Sonnet 3.5 required threshold 8 to be calibrated). Calibration is evaluated by observing resulting classification metrics (balanced accuracy, FPR/FNR) on the validation dataset.",
            "evaluation_criteria": "Trade-off between false positives and false negatives (FPR/FNR), balanced accuracy and alignment with human acceptance rates; calibration aims to produce operating points similar to human decisions.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "ML paper reviewing",
            "theory_type": "N/A (decision-mapping procedure)",
            "human_comparison": true,
            "evaluation_results": "Changing thresholds affected reported metrics substantially: e.g., Sonnet 3.5 required threshold 8 for calibrated Balanced Acc ~0.59; GPT-4o 0-shot calibrated @6 yielded Balanced Acc 0.63. Threshold choice impacted FPR and FNR—models with optimistic biases showed low FNR but very high FPR unless thresholded upward.",
            "automated_vs_human_evaluation": "Automated calibration procedure validated against human labels; used to better align LLM decisions to historical human decisions.",
            "validation_method": "Grid search over thresholds and evaluation of classification metrics on the ICLR 2022 validation set; comparisons to human baseline statistics.",
            "limitations_challenges": "Different LLMs exhibit different score distributions so threshold is model-dependent; thresholding fixes binary decision but does not improve underlying rubric scoring accuracy or qualitative review helpfulness; calibration can mask deeper biases (e.g., over-acceptance tendencies).",
            "benchmark_dataset": "ICLR 2022 OpenReview (500-paper subset) used for calibration and evaluation",
            "uuid": "e4580.3",
            "source_info": {
                "paper_title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Prompting & Ensembling Enhancements",
            "name_full": "Prompt Engineering and Response-Ensembling (self-reflection, few-shot, ensembling, meta-aggregation)",
            "brief_description": "A collection of prompting and inference-time strategies used to improve LLM reviewer decision quality, including self-reflection (iterative refinement), few-shot exemplars, multiple independent review generations (ensembling), and a meta-review aggregation stage emulating an Area Chair.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Reflexion/self-reflection + Few-shot + Ensemble Reviews + Meta-aggregation",
            "evaluation_method_description": "Self-reflection: run multiple passes where the LLM reconsiders and refines its output (chain-of-thought-style or explicit reflection). Few-shot: provide exemplar review(s) to shape output format and scoring behavior. Ensemble: generate multiple independent reviews and aggregate (average numeric scores, combine free-text). Meta-aggregation: run a higher-level prompt acting as an Area Chair to synthesize ensembled reviews into a final decision. These techniques are combined to reduce variance and improve decision accuracy.",
            "evaluation_criteria": "Improvement in accuracy/balanced accuracy/F1/AUC and reduction in variance of predictions across runs; alignment with average human reviewer scores; stability of outputs.",
            "model_name": "GPT-4o (demonstrated benefits), ablated on Sonnet 3.5 and GPT-4o-mini",
            "model_size": null,
            "scientific_domain": "Automated peer review for ML papers",
            "theory_type": "N/A (methods to improve review generation)",
            "human_comparison": true,
            "evaluation_results": "Ablations showed self-reflection improved accuracy by ~+2% and one-shot few-shot prompting by ~+2%; ensembling did not substantially change mean accuracy but reduced variance. Best-performing configuration combined 5 rounds of self-reflection, 5 ensembled reviews, and 1-shot prompting with meta-aggregation.",
            "automated_vs_human_evaluation": "Automated enhancements applied to LLM reviewer and validated against human review labels; improvements measured via standard classification metrics.",
            "validation_method": "Ablation experiments reported in Figure 2 comparing configurations (with/without reflection, with/without few-shot, ensembling sizes) on the ICLR 2022 subset.",
            "limitations_challenges": "Diminishing returns: ensembling reduced variance more than mean error; some LLMs (e.g., Sonnet) required different calibration despite enhancements; additional compute/costs for multiple passes; enhancements do not address inability to view figures or ask rebuttals.",
            "benchmark_dataset": "Evaluated on ICLR 2022 OpenReview subset (500 papers) used for ablation comparisons",
            "uuid": "e4580.4",
            "source_info": {
                "paper_title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ICLR 2022 OpenReview Data",
            "rating": 2
        },
        {
            "paper_title": "NeurIPS consistency experiment (Beygelzimer et al., 2021)",
            "rating": 2
        },
        {
            "paper_title": "NeurIPS review guidelines",
            "rating": 2
        }
    ],
    "cost": 0.01540225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery</h1>
<p>Chris Lu ${ }^{1,2, <em>}$, Cong Lu ${ }^{3,4, </em>}$, Robert Tjarko Lange ${ }^{1, <em>}$, Jakob Foerster ${ }^{2, \dagger}$, Jeff Clune ${ }^{3,4,5, \dagger}$ and David Ha ${ }^{1, \dagger}$<br></em>Equal Contribution, ${ }^{1}$ Sakana AI, ${ }^{2}$ FLAIR, University of Oxford, ${ }^{3}$ University of British Columbia, ${ }^{4}$ Vector Institute, ${ }^{5}$ Canada CIFAR<br>AI Chair, ${ }^{\dagger}$ Equal Advising</p>
<h4>Abstract</h4>
<p>One of the grand challenges of artificial general intelligence is developing agents capable of conducting scientific research and discovering new knowledge. While frontier models have already been used as aides to human scientists, e.g. for brainstorming ideas, writing code, or prediction tasks, they still conduct only a small part of the scientific process. This paper presents the first comprehensive framework for fully automatic scientific discovery, enabling frontier large language models (LLMs) to perform research independently and communicate their findings. We introduce The AI Scientist, which generates novel research ideas, writes code, executes experiments, visualizes results, describes its findings by writing a full scientific paper, and then runs a simulated review process for evaluation. In principle, this process can be repeated to iteratively develop ideas in an open-ended fashion and add them to a growing archive of knowledge, acting like the human scientific community. We demonstrate the versatility of this approach by applying it to three distinct subfields of machine learning: diffusion modeling, transformer-based language modeling, and learning dynamics. Each idea is implemented and developed into a full paper at a meager cost of less than $\$ 15$ per paper, illustrating the potential for our framework to democratize research and significantly accelerate scientific progress. To evaluate the generated papers, we design and validate an automated reviewer, which we show achieves near-human performance in evaluating paper scores. The AI Scientist can produce papers that exceed the acceptance threshold at a top machine learning conference as judged by our automated reviewer. This approach signifies the beginning of a new era in scientific discovery in machine learning: bringing the transformative benefits of AI agents to the entire research process of AI itself, and taking us closer to a world where endless affordable creativity and innovation can be unleashed on the world's most challenging problems. Our code is open-sourced at https://github.com/SakanaAI/AI-Scientist.</p>
<h2>1. Introduction</h2>
<p>The modern scientific method (Chalmers, 2013; Dewey, 1910; Jevons, 1877) is arguably one of the greatest achievements of the Enlightenment. Traditionally, a human researcher collects background knowledge, drafts a set of plausible hypotheses to test, constructs an evaluation procedure, collects evidence for the different hypotheses, and finally assesses and communicates their findings. Afterward, the resulting manuscript undergoes peer review and subsequent iterations of refinement. This procedure has led to countless breakthroughs in science and technology, improving human quality of life. However, this iterative process is inherently limited by human researchers' ingenuity, background knowledge, and finite time. Attempting to automate general scientific discovery (Langley, 1987, 2024; Waltz and Buchanan, 2009) has been a long ambition of the community since at least the early 70s, with computer-assisted works like the Automated Mathematician (Lenat, 1977; Lenat and Brown, 1984) and DENDRAL (Buchanan and Feigenbaum, 1981). In the field of AI, researchers have envisioned the possibility of automating AI research using AI itself (Ghahramani, 2015; Schmidhuber, 1991, 2010a,b, 2012), leading to "AI-generating algorithms" (Clune, 2019). More recently, foundation models have seen tremendous advances in their general capabilities (Anthropic, 2024; Google DeepMind Gemini Team, 2023; Llama Team, 2024; OpenAI, 2023), but they have only been shown to accelerate individual parts of the research pipeline, e.g. the writing of scientific manuscripts (Altmäe et al., 2023;</p>
<p>Dinu et al., 2024; Ifargan et al., 2024; Majumder et al., 2024), as a muse to brainstorm ideas (Baek et al., 2024; Girotra et al., 2023; Wang et al., 2024b), or aides to coding (Gauthier, 2024). To date, the community has yet to show the possibility of executing entire research endeavors without human involvement.</p>
<p>Traditional approaches to automating research projects have so far relied on carefully constraining the search space of potential discoveries, which severely limits the scope of exploration and requires substantial human expertise and design. For example, significant advancements in materials discovery (Merchant et al., 2023; Pyzer-Knapp et al., 2022; Szymanski et al., 2023) and synthetic biology (Hayes et al., 2024; Jumper et al., 2021) have been achieved by restricting exploration to well-characterized domains with predefined parameters, which allows for targeted progress but limits broader, open-ended discovery and addressing only a subset of the scientific process, without encompassing tasks such as manuscript preparation. Within the field of machine learning itself, research automation has largely been restricted to hyperparameter and architecture search (He et al., 2021; Hutter et al., 2019; Lu et al., 2022b; Wan et al., 2021, 2022) or algorithm discovery (Alet et al., 2020; Chen et al., 2024b; Kirsch et al., 2019; Lange et al., 2023a,b; Lu et al., 2022a; Metz et al., 2022) within a hand-crafted search space. Recent advances in LLMs have shown the potential to extend the search space to more generalized, code-level solutions (Faldor et al., 2024; Lehman et al., 2022; Lu et al., 2024a; Ma et al., 2023). However, these approaches remain constrained by rigorously-defined search spaces and objectives, which limit the breadth and depth of possible discoveries.</p>
<p>In this paper, we introduce The AI Scientist, the first fully automated and scalable pipeline for end-to-end paper generation, enabled by recent advances in foundation models. Given a broad research direction and a simple initial codebase, The AI Scientist seamlessly performs ideation, a literature search, experiment planning, experiment iterations, manuscript writing, and peer reviewing to produce insightful papers. Furthermore, in principle The AI Scientist can run in an openended loop, building on its previous scientific discoveries to improve the next generation of ideas. This allows us to speed up the slow nature of scientific iteration at a surprisingly low financial cost ( $\sim \$ 15 /$ paper) and represents a step towards turning the world's ever-increasing computing resources into the scientific breakthroughs needed to tackle the core challenges of the 21st century. Here, we focus on Machine Learning (ML) applications, but this approach can more generally be applied to almost any other discipline, e.g. biology or physics, given an adequate way of automatically executing experiments (Arnold, 2022; Kehoe et al., 2015; Zucchelli et al., 2021).</p>
<p>By leveraging modern LLM frameworks like chain-of-thought (Wei et al., 2022) and self-reflection (Shinn et al., 2024) to improve decision-making, The AI Scientist is able to generate its own scientific ideas and hypotheses, as well as a plan for testing them with experiments. Next, The AI Scientist implements plan-directed code-level changes to the experiment "template" using the state-of-the-art coding assistant Aider (Gauthier, 2024), and executes experiments to collect a set of computational results, which are in turn used to draft a scientific paper. The AI Scientist then performs an automated paper-reviewing process using guidelines from a standard machine learning conference. Finally, The AI Scientist adds the completed ideas and reviewer feedback to its archive of scientific findings, and the process repeats. Crucially, the generated paper and experimental artifacts The AI Scientist produces allow us to easily interpret and judge its findings post-hoc, allowing human scientists to also benefit from what is learned.</p>
<p>Our contributions are summarized as follows:</p>
<ol>
<li>We introduce the first end-to-end framework for fully automated scientific discovery in Machine Learning research, enabled by frontier LLMs (Section 3). This fully automated process includes idea generation, experiment design, execution, and visualizing and writing up the results into a full manuscript.</li>
</ol>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1 | Conceptual illustration of The AI Scientist, an end-to-end LLM-driven scientific discovery process. The AI Scientist first invents and assesses the novelty of a set of ideas. It then determines how to test the hypotheses, including writing the necessary code by editing a codebase powered by recent advances in automated code generation. Afterward, the experiments are automatically executed to collect a set of results consisting of both numerical scores and visual summaries (e.g. plots or tables). The results are motivated, explained, and summarized in a LaTeX report. Finally, The AI Scientist generates an automated review, according to current practice at standard machine learning conferences. The review can be used to either improve the project or as feedback to future generations for open-ended scientific discovery.
2. To assess the quality of the generated papers, we introduce a foundation model-based reviewing process in Section 4. This process achieves near-human-level performance across multiple evaluation metrics (e.g. $65 \%$ vs. $66 \%$ balanced accuracy) when evaluated on ICLR 2022 OpenReview data. The reviews further enable The AI Scientist to select the best ideas for "publication" to an ever-growing archive of scientific discoveries, and the process can be repeated to build on these discoveries, just as in the human scientific community.
3. The AI Scientist can generate hundreds of interesting, medium-quality papers over the course of a week. In this report, we focus on a subset of these papers, highlighting novel insights in diffusion modeling, language modeling, and grokking. We perform an in-depth case study into one selected paper in Section 5, and present aggregate results in Section 6.
4. We conclude the paper with an extensive discussion on the limitations, ethical considerations, and future outlook of our approach in Sections 8 and 9.</p>
<h1>2. Background</h1>
<p>Large Language Models. In this paper, we build our automated scientist from autoregressive large language models (LLMs, Anthropic (2023); Google DeepMind Gemini Team (2023); Llama Team (2024); OpenAI (2023); Zhu et al. (2024)) which learn to generate text completions by modeling the conditional probability of a new token (similar to a word) given the preceding tokens, $p\left(x_{t} \mid x_{&lt;t} ; \theta\right)$, and sampling at test-time. Together with vast data and model scaling, this enables LLMs to not only generate coherent text, but crucially also exhibit human-like abilities, including commonsense knowledge (Talmor et al., 2019), reasoning (Wei et al., 2022), and the ability to write code (Chen et al., 2021; Xu et al., 2022).</p>
<p>LLM Agent Frameworks. Typical applications of LLMs often involve embedding the model into an "agent" (Wang et al., 2024a) framework, including the following possibilities: the structuring of</p>
<p>language queries (e.g. few-shot prompting (Brown et al., 2020)), encouraging reasoning traces (e.g. chain-of-thought (Wei et al., 2022)), or asking the model to iteratively refine its outputs (e.g., selfreflection (Shinn et al., 2024)). These leverage the language model's ability to learn in-context (Olsson et al., 2022) and can greatly improve its performance, robustness and reliability on many tasks.</p>
<p>Aider: An LLM-Based Coding Assistant. Our automated scientist directly implements ideas in code and uses a state-of-the-art open-source coding assistant, Aider (Gauthier, 2024). Aider is an agent framework that is designed to implement requested features, fix bugs, or refactor code in existing codebases. While Aider can in principle use any underlying LLM, with frontier models it achieves a remarkable success rate of $18.9 \%$ on the SWE Bench (Jimenez et al., 2024) benchmark, a collection of real-world GitHub issues. In conjunction with new innovations added in this work, this level of reliability enables us, for the first time, to fully automate the ML research process.</p>
<h1>3. The AI Scientist</h1>
<p>Overview. The AI Scientist has three main phases (Figure 1): (1) Idea Generation, (2) Experimental Iteration, and (3) Paper Write-up. After the write-up, we introduce and validate an LLM-generated review to assess the quality of the generated paper (Section 4). We provide The AI SCIENTIST with a starting code template that reproduces a lightweight baseline training run from a popular model or benchmark. For example, this could be code that trains a small transformer on the works of Shakespeare (Karpathy, 2022), a classic proof-of-concept training run from natural language processing that completes within a few minutes. The AI Scientist is then free to explore any possible research direction. The template also includes a LaTeX folder that contains style files and section headers, along with simple plotting code. We provide further details on the templates in Section 6, but in general, each run starts with a representative small-scale experiment relevant to the topic area. The focus on small-scale experiments is not a fundamental limitation of our method, but simply for computational efficiency reasons and compute constraints on our end. We provide the prompts for all stages in Appendix A.</p>
<ol>
<li>Idea Generation. Given a starting template, The AI Scientist first "brainstorms" a diverse set of novel research directions. We take inspiration from evolutionary computation and open-endedness research (Brant and Stanley, 2017; Lehman et al., 2008; Stanley, 2019; Stanley et al., 2017) and iteratively grow an archive of ideas using LLMs as the mutation operator (Faldor et al., 2024; Lehman et al., 2022; Lu et al., 2024b; Zhang et al., 2024). Each idea comprises a description, experiment execution plan, and (self-assessed) numerical scores of interestingness, novelty, and feasibility. At each iteration, we prompt the language model to generate an interesting new research direction conditional on the existing archive, which can include the numerical review scores from completed previous ideas. We use multiple rounds of chain-of-thought (Wei et al., 2022) and self-reflection (Shinn et al., 2024) to refine and develop each idea. After idea generation, we filter ideas by connecting the language model with the Semantic Scholar API (Fricke, 2018) and web access as a tool (Schick et al., 2024). This allows The AI Scientist to discard any idea that is too similar to existing literature.</li>
<li>Experiment Iteration. Given an idea and a template, the second phase of The AI Scientist first executes the proposed experiments and then visualizes its results for the downstream write-up. The AI Scientist uses Aider to first plan a list of experiments to run and then executes them in order. We make this process more robust by returning any errors upon a failure or time-out (e.g. experiments taking too long to run) to Aider to fix the code and re-attempt up to four times.</li>
</ol>
<p>After the completion of each experiment, Aider is then given the results and told to take notes in the style of an experimental journal. Currently, it only conditions on text but in future versions, this could include data visualizations or any modality. Conditional on the results, it then re-plans and implements the next experiment. This process is repeated up to five times. Upon completion of</p>
<p>experiments, Aider is prompted to edit a plotting script to create figures for the paper using Python. The AI Scientist makes a note describing what each plot contains, enabling the saved figures and experimental notes to provide all the information required to write up the paper. At all steps, Aider sees its history of execution.</p>
<p>Note that, in general, the provided initial seed plotting and experiment templates are small, selfcontained files. The AI Scientist frequently implements entirely new plots and collects new metrics that are not in the seed templates. This ability to arbitrarily edit the code occasionally leads to unexpected outcomes (Section 8).
3. Paper Write-up. The third phase of The AI Scientist produces a concise and informative write-up of its progress in the style of a standard machine learning conference proceeding in LaTeX. We note that writing good LaTeX can even take competent human researchers some time, so we take several steps to robustify the process. This consists of the following:
(a) Per-Section Text Generation: The recorded notes and plots are passed to Aider, which is prompted to fill in a blank conference template section by section. This goes in order of introduction, background, methods, experimental setup, results, and then the conclusion (all sections apart from the related work). All previous sections of the paper it has already written are in the context of the language model. We include brief tips and guidelines on what each section should include, based on the popular "How to ML Paper" guide, and include details in Appendix A.3. At each step of writing, Aider is prompted to only use real experimental results in the form of notes and figures generated from code, and real citations to reduce hallucination. Each section is initially refined with one round of self-reflection (Shinn et al., 2024) as it is being written. Aider is prompted to not include any citations in the text at this stage, and fill in only a skeleton for the related work, which will be completed in the next stage.
(b) Web Search for References: In a similar vein to idea generation, The AI Scientist is allowed 20 rounds to poll the Semantic Scholar API looking for the most relevant sources to compare and contrast the near-completed paper against for the related work section. This process also allows The AI Scientist to select any papers it would like to discuss and additionally fill in any citations that are missing from other sections of the paper. Alongside each selected paper, a short description is produced of where and how to include the citation, which is then passed to Aider. The paper's bibtex is automatically appended to the LaTeX file to guarantee correctness.
(c) Refinement: After the previous two stages, The AI Scientist has a completed first draft, but can often be overly verbose and repetitive. To resolve this, we perform one final round of self-reflection section-by-section, aiming to remove any duplicated information and streamline the arguments of the paper.
(d) Compilation: Once the LaTeX template has been filled in with all the appropriate results, this is fed into a LaTeX compiler. We use a LaTeX linter and pipe compilation errors back into Aider so that it can automatically correct any issues.</p>
<h1>4. Automated Paper Reviewing</h1>
<p>An LLM Reviewer Agent. A key component of an effective scientific community is its reviewing system, which evaluates and improves the quality of scientific papers. To mimic such a process using large language models, we design a GPT-4o-based agent (OpenAI, 2023) to conduct paper reviews based on the Neural Information Processing Systems (NeurIPS) conference review guidelines. The review agent processes the raw text of the PDF manuscript using the PyMuPDF parsing library. The output contains numerical scores (soundness, presentation, contribution, overall, confidence), lists of weaknesses and strengths as well as a preliminary binary decision (accept or reject). These decisions</p>
<p>may then be post-calibrated by thresholding using the reviewer score. We leverage this automated reviewing process to obtain an initial evaluation of the papers generated by The AI Scientist. We provide the entire reviewing prompt template in Appendix A.4.</p>
<p>Table 1 | Performance of The AI Scientist's automated LLM reviewing system on 500 ICLR 2022 papers. We show mean and $95 \%$ bootstrap confidence intervals, and highlight the comparison between the human baseline and our best AI reviewer.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Reviewer</th>
<th style="text-align: center;">Balanced Acc. $\uparrow$</th>
<th style="text-align: center;">Accuracy $\uparrow$</th>
<th style="text-align: center;">F1 Score $\uparrow$</th>
<th style="text-align: center;">AUC $\uparrow$</th>
<th style="text-align: center;">FPR $\downarrow$</th>
<th style="text-align: center;">FNR $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Human (NeurIPS) ${ }^{1}$</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.52</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Random Decision</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Always Reject</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: center;">Uncalibrated</td>
<td style="text-align: center;">Sonnet 3.5</td>
<td style="text-align: center;">$0.52 \pm 0.01$</td>
<td style="text-align: center;">$0.40 \pm 0.01$</td>
<td style="text-align: center;">$0.55 \pm 0.01$</td>
<td style="text-align: center;">$0.52 \pm 0.01$</td>
<td style="text-align: center;">$0.95 \pm 0.02$</td>
<td style="text-align: center;">$0.00 \pm 0.00$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4o-mini</td>
<td style="text-align: center;">$0.53 \pm 0.02$</td>
<td style="text-align: center;">$0.65 \pm 0.01$</td>
<td style="text-align: center;">$0.11 \pm 0.06$</td>
<td style="text-align: center;">$0.53 \pm 0.02$</td>
<td style="text-align: center;">$0.01 \pm 0.01$</td>
<td style="text-align: center;">$0.94 \pm 0.04$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4o (0-shot)</td>
<td style="text-align: center;">$0.61 \pm 0.04$</td>
<td style="text-align: center;">$0.68 \pm 0.03$</td>
<td style="text-align: center;">$0.43 \pm 0.07$</td>
<td style="text-align: center;">$0.61 \pm 0.04$</td>
<td style="text-align: center;">$0.11 \pm 0.03$</td>
<td style="text-align: center;">$0.67 \pm 0.07$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4o (1-shot)</td>
<td style="text-align: center;">$0.60 \pm 0.03$</td>
<td style="text-align: center;">$0.70 \pm 0.03$</td>
<td style="text-align: center;">$0.37 \pm 0.08$</td>
<td style="text-align: center;">$0.60 \pm 0.03$</td>
<td style="text-align: center;">$0.04 \pm 0.02$</td>
<td style="text-align: center;">$0.76 \pm 0.06$</td>
</tr>
<tr>
<td style="text-align: center;">Calibrated</td>
<td style="text-align: center;">Sonnet 3.5 @8</td>
<td style="text-align: center;">$0.59 \pm 0.04$</td>
<td style="text-align: center;">$0.65 \pm 0.04$</td>
<td style="text-align: center;">$0.45 \pm 0.06$</td>
<td style="text-align: center;">$0.59 \pm 0.04$</td>
<td style="text-align: center;">$0.20 \pm 0.04$</td>
<td style="text-align: center;">$0.61 \pm 0.07$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4o-mini @6</td>
<td style="text-align: center;">$0.59 \pm 0.04$</td>
<td style="text-align: center;">$0.64 \pm 0.04$</td>
<td style="text-align: center;">$0.45 \pm 0.06$</td>
<td style="text-align: center;">$0.59 \pm 0.04$</td>
<td style="text-align: center;">$0.22 \pm 0.05$</td>
<td style="text-align: center;">$0.60 \pm 0.07$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4o (0-shot) @6</td>
<td style="text-align: center;">$0.63 \pm 0.04$</td>
<td style="text-align: center;">$0.63 \pm 0.04$</td>
<td style="text-align: center;">$0.56 \pm 0.05$</td>
<td style="text-align: center;">$0.63 \pm 0.04$</td>
<td style="text-align: center;">$0.38 \pm 0.05$</td>
<td style="text-align: center;">$0.36 \pm 0.07$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4o (1-shot) @6</td>
<td style="text-align: center;">$0.65 \pm 0.04$</td>
<td style="text-align: center;">$0.66 \pm 0.04$</td>
<td style="text-align: center;">$0.57 \pm 0.05$</td>
<td style="text-align: center;">$0.65 \pm 0.04$</td>
<td style="text-align: center;">$0.31 \pm 0.05$</td>
<td style="text-align: center;">$0.39 \pm 0.07$</td>
</tr>
</tbody>
</table>
<p>Evaluating the Automated Reviewer. To evaluate the LLM-based reviewer's performance, we compared the artificially generated decisions with ground truth data for 500 ICLR 2022 papers extracted from the publicly available OpenReview dataset (Berto, 2024). Similar to the previous section, we combine many recent advancements in LLM agents to make the decision-making process robust. More specifically, we improve the base LLM's decision-making process by leveraging self-reflection (Shinn et al., 2024), providing few-shot examples (Wei et al., 2022) and response ensembling (Wang et al., 2022). With GPT-4o, The AI Scientist's reviewing procedure achieves $70 \%$ accuracy when combining 5 rounds of self-reflection, 5 ensembled reviews, and a 1 -shot review example taken from the ICLR 2022 review guidelines. Afterward, we perform an LLM-based meta-review, which prompts the agent to act as an Area Chair (Wang et al., 2022) (full prompts in Appendix A.4). While this number is lower than the $73 \%$ accuracy that was reported for humans in the NeurIPS 2021 consistency experiment (Beygelzimer et al., 2021), the automated reviewer achieves superhuman F1 Scores ( 0.57 vs. 0.49 ) and human-level AUC ( 0.65 for both) when thresholding the decision at a score of 6 (a "Weak Accept" in the NeurIPS review guidelines). This choice corresponds roughly to the average score of accepted papers.</p>
<p>The considered ICLR 2022 paper dataset is very class-imbalanced, i.e. it contains many more rejected papers. When considering a balanced dataset of papers, The AI Scientist's reviewing process achieves human-level accuracy ( $0.65 \%$ vs. $0.66 \%$ ). Furthermore, the False Negative Rate (FNR) is much lower than the human baseline ( 0.39 vs. 0.52 ). Hence, the LLM-based review agent rejects fewer high-quality papers. The False Positive Rate (FNR), on the other hand, is higher ( 0.31 vs. 0.17 ) highlighting room for potential future improvements.</p>
<p>To further validate the performance of the automated reviewer, we compare the consistency of the overall paper scores between anonymous OpenReview reviewers randomly sampled pairwise per paper (Figure 2, bottom-left) and between the average of all reviewers and the LLM score (Figure 2, bottom-middle). For the set of 500 ICLR 2022 papers, we find that the correlation between the score of two human reviewers is smaller ( 0.14 ) than the correlation between the LLM score and the average score across the reviewers ( 0.18 ). Overall, across all metrics, the results suggest that LLM-based reviews can not only provide valuable feedback (D'Arcy et al., 2024) but also align more closely with the average human reviewer score than individual human reviewers align with each other.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2 | Evaluation of The AI Scientist's paper reviewing process on ICLR 2022 OpenReview Data using GPT-4o. Adding Reflexion and one-shot prompting improves the accuracy of the LLM-Based Reviewing Process. Review ensembling ( 5 reviews) and subsequent meta-aggregation, on the other hand, did not affect the reviewer's performance, but can reduce variance.</p>
<p>Each review is generated for $\$ 0.25$ to $\$ 0.50$ in API costs. We additionally compared the reviewing performance of various other foundation models. While Claude Sonnet 3.5 (Anthropic, 2024) and GPT-4o-mini provide a more cost-efficient approach, their performance was substantially worse (Table 1). Moreover, we had to threshold scores at 8 for Sonnet 3.5 to obtain calibrated results, due to persistent over-optimism bias. Llama 3.1 405B (Llama Team, 2024) struggled to follow the reviewer output template consistently. We open-source our code, providing a new and interesting LLM benchmark for the community.</p>
<p>LLM Reviewer Ablations. We compare various prompt configurations for GPT-4o and find that both Reflexion ( $+2 \%$ ) and one-shot prompting ( $+2 \%$ ) substantially help with performing more accurate reviewing (Figure 2, top and bottom-right). On the other hand, using review ensembling does not appear to improve the reviewer's performance substantially but can reduce variance. In the following sections, we used our best overall reviewer: GPT-4o with 5 rounds of self-reflection, 5 ensembled reviews, a meta-aggregation step, and 1 few-shot example.</p>
<h1>5. In-Depth Case Study</h1>
<p>Before we present extensive experiments and metrics for The AI Scientist's generated papers in Section 6, we first visualize a representative sample from a run of the The AI Scientist which illustrates both its strengths and shortcomings, followed by a broader discussion of its potential. The selected paper "Adaptive Dual-Scale Denoising" is generated from a run where The AI Scientist is asked to do research on diffusion modeling, which is fully detailed in Section 6.1. The base foundation model was Claude Sonnet 3.5 (Anthropic, 2024).</p>
<p>Generated Idea. As discussed in Section 3, The AI Scientist first generates an idea based on the provided template and its previous archive of discoveries. The idea in the selected paper was proposed in the 6th iteration of the algorithm and aims to improve the ability of diffusion models to capture both global structure and local details in a 2D dataset, by proposing two branches in the standard denoiser network. This is a well-motivated direction that has been the primary reason for researchers adopting diffusion models over prior styles of generative models such as VAEs (Kingma</p>
<p>and Welling, 2014) and GANs (Goodfellow et al., 2014), and to the best of our knowledge has not been widely studied.</p>
<p>We highlight that The AI Scientist generates an impressive experimental plan that includes the proposed code modification, comparison to baselines, evaluation metrics, and the design of additional plots. As has been previously observed in the literature, judgments by LLMs can often have bias (Zheng et al., 2024) which we can observe in over-estimation of an idea's interestingness, feasibility, or novelty. The "novel" flag at the end indicates The AI Scientist believes the idea is novel after searching for related papers using the Semantic Scholar API.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Idea</span><span class="w"> </span><span class="nx">adaptive_dual_scale_denoising</span>
<span class="s">&quot;Name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;adaptive_dual_scale_denoising&quot;</span><span class="p">,</span>
<span class="s">&quot;Title&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Adaptive Dual-Scale Denoising for Dynamic Feature Balancing in</span>
<span class="s">Low-Dimensional Diffusion Models&quot;</span><span class="p">,</span>
<span class="s">&quot;Experiment&quot;</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Modify MLPDenoiser to implement a dual-scale processing</span>
<span class="s">approach with two parallel branches: a global branch for the original input</span>
<span class="s">and a local branch for an upscaled input. Introduce a learnable, timestep-</span>
<span class="s">conditioned weighting factor to dynamically balance the contributions of</span>
<span class="s">global and local branches. Train models with both the original and new</span>
<span class="s">architecture on all datasets. Compare performance using KL divergence and</span>
<span class="s">visual inspection of generated samples. Analyze how the weighting factor</span>
<span class="s">evolves during the denoising process and its impact on capturing global</span>
<span class="s">structure vs. local details across different datasets and timesteps.&quot;</span><span class="p">,</span>
<span class="s">&quot;Interestingness&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">9</span><span class="p">,</span>
<span class="s">&quot;Feasibility&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span>
<span class="s">&quot;Novelty&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span>
<span class="s">&quot;novel&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
</code></pre></div>

<p>Generated Experiments. We display the generated code diff (deletions are in red, and additions are in green) for the substantial algorithmic changes below. The code matches the experimental description and is well-commented. The AI Scientist is able to iterate on the code with results from intermediate experiments in the loop, and it eventually ends up with interesting design choices for the adaptive weight network, e.g. a LeakyReLU. Importantly, this network has a well-behaved output that is guaranteed to be between 0 and 1 . We additionally note that The AI Scientist changed the output of the network to return the adaptive weights to make new visualizations.</p>
<div class="codehilite"><pre><span></span><code>@@ -60,19 +60,55 @@ class MLPDenoiser(nn.Module):
    self.input_mlp1 = SinusoidalEmbedding(embedding_dim, scale=25.0)
    self.input_mlp2 = SinusoidalEmbedding(embedding_dim, scale=25.0)
    self.network = nn.Sequential(
    self.global_network = nn.Sequential(
        nn.Linear(embedding_dim <span class="gs">* 3, hidden_dim),</span>
<span class="gs">        *</span> [ResidualBlock(hidden_dim) for _ in range(hidden_layers)],
        nn.ReLU(),
        nn.Linear(hidden_dim, 2),
    )
    self.local_network = nn. Sequential(
        nn.Linear(embedding_dim <span class="gs">* 3, hidden_dim),</span>
<span class="gs">        *</span> [ResidualBlock(hidden_dim) for _ in range(hidden_layers)],
        nn.ReLU(),
        nn.Linear(hidden_dim, 2),
    )
*
</code></pre></div>

<div class="codehilite"><pre><span></span><code>    self.upscale = nn.Linear(2, 4)
    self.downscale = nn.Linear(2, 2)
    # Modify the weight network to have an additional hidden layer and use LeakyReLU
    self.weight_network = nn.Sequential(
        nn.Linear(embedding_dim, hidden_dim),
        nn.LeakyReLU(0.2),
        nn.Linear(hidden_dim, hidden_dim // 2),
        nn.LeakyReLU(0.2),
        nn.Linear(hidden_dim // 2, 2),
        nn.Softmax(dim=-1) # Ensure weights sum to 1
    )
def forward(self, x, t):
    x1_emb = self.input_mlp1(x[:, 0])
    x2_emb = self.input_mlp2(x[:, 1])
    t_emb = self.time_mlp(t)
    emb = torch.cat([x1_emb, x2_emb, t_emb], dim=-1)
    return self.network(emb)
    global_emb = torch.cat([x1_emb, x2_emb, t_emb], dim=-1)
    # Global branch
    global_output = self.global_network(global_emb)
    # Local branch with upscaling
    x_upscaled = self.upscale(x)
    x1_upscaled_emb = self.input_mlp1(x_upscaled[:, 0])
    x2_upscaled_emb = self.input_mlp2(x_upscaled[:, 1])
    local_emb = torch.cat([x1_upscaled_emb, x2_upscaled_emb, t_emb], dim=-1)
    local_output = self.local_network(local_emb)
    # Calculate dynamic weights based on timestep
    weights = self.weight_network(t_emb)
    # Combine global and local outputs with learnable weighting
    output = weights[:, 0].unsqueeze(1) * global_output + weights[:, 1].unsqueeze(1)
    local_output
    return output, weights
</code></pre></div>

<p>Generated Paper. The AI Scientist generates an 11-page scientific manuscript in the style of a standard machine learning conference submission complete with visualizations and all standard sections. We display a preview of the completely AI-generated paper in Figure 3, with the full-sized version available in Appendix D.1.</p>
<p>We highlight specific things that were particularly impressive in the paper:</p>
<ul>
<li>Precise Mathematical Description of the Algorithm. The algorithmic changes in the code above are described precisely, with new notation introduced where necessary, using LaTeX math packages. The overall training process is also described exactly.</li>
<li>Comprehensive Write-up of Experiments. The hyperparameters, baselines, and datasets are listed in the paper. As an essential sanity check, we verified that the main numerical results in Table 1 of the generated paper exactly match the experimental logs. Impressively, while the recorded numbers are in long-form floats, The AI Scientist chooses to round them all to 3 decimal places without error. Even more impressively, the results are accurately compared to the baseline (e.g. $12.8 \%$ reduction in KL on the dinosaur dataset).</li>
<li>Good Empirical Results. Qualitatively, the sample quality looks much improved from the</li>
</ul>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3 | Preview of the "Adaptive Dual-Scale Denoising" paper which was entirely autonomously generated by The AI Scientist. The full paper can be viewed in Appendix D. 1
baseline. Fewer points are greatly out-of-distribution with the ground truth. Quantitatively, there are improvements to the approximate KL divergence between true and estimated distribution.</p>
<ul>
<li>New Visualizations. While we provided some baseline plotting code for visualizing generated samples and the training loss curves, it came up with novel algorithm-specific plots displaying the progression of weights throughout the denoising process.</li>
<li>Interesting Future Work Section. Building on the success of the current experiments, the future work section lists relevant next steps such as scaling to higher-dimensional problems, more sophisticated adaptive mechanisms, and better theoretical foundations.</li>
</ul>
<p>On the other hand, there are also pathologies in this paper:</p>
<ul>
<li>Subtle Error in Upscaling Network. While a linear layer upscales the input to the denoiser network, only the first two dimensions are being used for the "local" branch, leading this upscaling layer to be a linear layer that preserves the same dimensionality effectively.</li>
<li>Hallucination of Experimental Details. The paper claims that V100 GPUs were used, even though the agent couldn't have known the actual hardware used. In reality, H100 GPUs were used. It also guesses the PyTorch version without checking.</li>
<li>Positive Interpretation of Results. The paper tends to take a positive spin even on its negative results, which leads to slightly humorous outcomes. For example, while it summarizes its positive results as: "Dino: $12.8 \%$ reduction (from 0.989 to 0.862 )" (lower KL is better), the negative results are reported as "Moons: $3.3 \%$ improvement (from 0.090 to 0.093 )". Describing a negative result as an improvement is certainly a stretch of the imagination.</li>
<li>Artifacts from Experimental Logs. While each change to the algorithm is usually descriptively labeled, it occasionally refers to results as "Run 2", which is a by-product from its experimental $\log$ and should not be presented as such in a professional write-up.</li>
<li>Presentation of Intermediate Results. The paper contains results for every single experiment that was run. While this is useful and insightful for us to see the evolution of the idea during execution, it is unusual for standard papers to present intermediate results like this.</li>
<li>Minimal References. While additional references have been sourced from Semantic Scholar, including two papers in the related work that are very relevant comparisons, overall the bibliography is small at only 9 entries.</li>
</ul>
<p>Review. The automated reviewer points out valid concerns in the generated manuscript. The review</p>
<p>recognizes the experiments were with simple, 2D datasets only, however, this is because we externally constrained the system to use these datasets, and in its current form, The AI Scientist cannot download higher-dimensional datasets from the internet. On the other hand, limitations such as the proposed algorithm's increased computational cost of the algorithm are mentioned in the actual paper, which shows that The AI Scientist is often up-front about the drawbacks of its idea. The reviewer also lists many relevant questions about the paper, such as: explaining the variability of performance across datasets, and explaining in more detail how the upscaling process affects the local branch's input.</p>
<h1>Review - adaptive dual scale denoising</h1>
<p>"Summary": "The paper introduces an adaptive dual-scale denoising approach for low-dimensional diffusion models, aiming to balance global structure and local details in generated samples. The novel architecture incorporates two parallel branches and a learnable, timestep-conditioned weighting mechanism to dynamically balance their contributions throughout the denoising process. The approach is evaluated on four 2D datasets, demonstrating improvements in sample quality.",
"Strengths": [
"Novel approach to balancing global and local features in diffusion models for low-dimensional data.",
"Comprehensive empirical evaluation on multiple 2D datasets.",
"Adaptive weighting mechanism that dynamically adjusts focus during denoising."
],
"Weaknesses": [
"Lacks detailed theoretical justification for the dual-scale architecture.",
"Computational cost is significantly higher, which may limit practical applicability.",
"Some sections are not clearly explained, such as the autoencoder aggregator and weight evolution analysis.",
"Limited diversity in the datasets used for evaluation. More complex, real-world datasets could strengthen claims.",
"Insufficient ablation studies and analysis on specific design choices like different types of aggregators."
],
"Originality": 4,
"Quality": 3,
"Clarity": 3,
"Significance": 3,
"Questions": [
"Can you provide a more detailed theoretical justification for the dual-scale architecture?",
"What impact do different types of aggregators have on the model's performance?",
"How does the model perform on more complex, real-world low-dimensional datasets?",
"Can the computational cost be reduced without sacrificing performance?"
],
"Limitations": [
"The paper should address the high computational cost and explore ways to optimize it.",</p>
<p>"The limited diversity of datasets and lack of detailed theoretical backing for the proposed architecture are notable limitations." ], "Ethical Concerns": false, "Soundness": 3, "Presentation": 3, "Contribution": 3, "Overall": 5, "Confidence": 4, "Decision": "Reject"</p>
<p>Final Comments. Drawing from our domain knowledge in diffusion modeling-which, while not our primary research focus, is an area in which we have published papers-we present our overall opinions on the paper generated by The AI Scientist below.</p>
<ul>
<li>The AI Scientist correctly identifies an interesting and well-motivated direction in diffusion modeling research, e.g. previous work has studied modified attention mechanisms (Hatamizadeh et al., 2024) for the same purpose in higher-dimensional problems. It proposes a comprehensive experimental plan to investigate its idea, and successfully implements it all, achieving good results. We were particularly impressed at how it responded to subpar earlier results and iteratively adjusted its code (e.g. refining the weight network). The full progression of the idea can be viewed in the paper.</li>
<li>While the paper's idea improves performance and the quality of generated diffusion samples, the reasons for its success may not be as explained in the paper. In particular, there is no obvious inductive bias beyond an upscaling layer (effectively just an additional linear layer) for the splitting of global or local features. However, we do see progression in weights (and thus a preference for the global or local branch) across diffusion timesteps which suggests that something non-trivial is happening. Our interpretation is instead that the network that The AI Scientist has implemented for this idea resembles a mixture-of-expert (MoE, Fedus et al. (2022); Yuksel et al. (2012)) structure that is prevalent across LLMs (Jiang et al., 2024). An MoE could indeed lead to the diffusion model learning separate branches for global and local features, as the paper claims, but this statement requires more rigorous investigation.</li>
<li>Interestingly, the true shortcomings of this paper described above certainly require some level of domain knowledge to identify and were only partially captured by the automated reviewer (i.e., when asking for more details on the upscaling layer). At the current capabilities of The AI Scientist, this can be resolved by human feedback. However, future generations of foundation models may propose ideas that are challenging for humans to reason about and evaluate. This links to the field of "superalignment" (Burns et al., 2023) or supervising AI systems that may be smarter than us, which is an active area of research.</li>
<li>Overall, we judge the performance of The AI Scientist to be about the level of an early-stage ML researcher who can competently execute an idea but may not have the full background knowledge to fully interpret the reasons behind an algorithm's success. If a human supervisor was presented with these results, a reasonable next course of action could be to advise THE AI Scientist to re-scope the project to further investigate MoEs for diffusion. Finally, we naturally expect that many of the flaws of the The AI Scientist will improve, if not be eliminated, as foundation models continue to improve dramatically.</li>
</ul>
<h1>6. Experiments</h1>
<p>We extensively evaluate The AI Scientist on three templates (as described in Section 3) across different publicly available LLMs: Claude Sonnet 3.5 (Anthropic, 2024), GPT-4o (OpenAI, 2023),</p>
<p>DeepSeek Coder (Zhu et al., 2024), and Llama-3.1 405b (Llama Team, 2024). The first two models are only available by a public API, whilst the second two models are open-weight. For each run, we provide 1-2 basic seed ideas as examples (e.g. modifying the learning rate or batch size) and have it generate another 50 new ideas. We visualize an example progression of proposed ideas in Appendix C. Each run of around fifty ideas in total takes approximately 12 hours on $8 \times$ NVIDIA H100s ${ }^{2}$. We report the number of ideas that pass the automated novelty check, successfully complete experiments, and result in valid compilable manuscripts. Note that the automated novelty check and search are self-assessed by each model for its own ideas, making relative "novelty" comparisons challenging. Additionally, we provide the mean and max reviewer scores of the generated papers and the total cost of the run. Finally, we select and briefly analyze some of the generated papers, which are listed below. The full papers can be found in Appendix D, alongside the generated reviews and code.</p>
<p>In practice, we make one departure from the formal description of The AI Scientist, and generate ideas without waiting for paper evaluations to be appended to the archive in order to parallelize more effectively. This allowed us to pay the cost of the idea generation phase only once and iterate faster; furthermore, we did not observe any reduction in the quality of the papers generated as measured by the average review score with this modification.</p>
<p>Table 2 | 10 selected papers generated by The AI Scientist across 3 different templates, together with scores from our automated reviewer corresponding to the NeurIPS guidelines. The average accepted paper at NeurIPS has a score of around 6 from human evaluation.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Type</th>
<th style="text-align: center;">Paper Title</th>
<th style="text-align: center;">Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">2D Diffusion</td>
<td style="text-align: center;">DualScale Diffusion: Adaptive Feature Balancing for Low-Dimensional Generative Models</td>
<td style="text-align: center;">$\mathbf{5}$</td>
</tr>
<tr>
<td style="text-align: left;">2D Diffusion</td>
<td style="text-align: center;">Multi-scale Grid Noise Adaptation: Enhancing Diffusion Models For Low-dimensional Data</td>
<td style="text-align: center;">$\mathbf{4}$</td>
</tr>
<tr>
<td style="text-align: left;">2D Diffusion</td>
<td style="text-align: center;">GAN-Enhanced Diffusion: Boosting Sample Quality and Diversity</td>
<td style="text-align: center;">$\mathbf{3}$</td>
</tr>
<tr>
<td style="text-align: left;">2D Diffusion</td>
<td style="text-align: center;">DualDiff: Enhancing Mode Capture in Low-dimensional Diffusion Models via Dual-expert Denoising</td>
<td style="text-align: center;">$\mathbf{5}$</td>
</tr>
<tr>
<td style="text-align: left;">NanoGPT</td>
<td style="text-align: center;">StyleFusion: Adaptive Multi-style Generation in Character-Level Language Models</td>
<td style="text-align: center;">$\mathbf{5}$</td>
</tr>
<tr>
<td style="text-align: left;">NanoGPT</td>
<td style="text-align: center;">Adaptive Learning Rates for Transformers via Q-Learning</td>
<td style="text-align: center;">$\mathbf{3}$</td>
</tr>
<tr>
<td style="text-align: left;">Grokking</td>
<td style="text-align: center;">Unlocking Grokking: A Comparative Study of Weight Initialization Strategies in Transformer Models</td>
<td style="text-align: center;">$\mathbf{5}$</td>
</tr>
<tr>
<td style="text-align: left;">Grokking</td>
<td style="text-align: center;">Grokking Accelerated: Layer-wise Learning Rates for Transformer Generalization</td>
<td style="text-align: center;">$\mathbf{4}$</td>
</tr>
<tr>
<td style="text-align: left;">Grokking</td>
<td style="text-align: center;">Grokking Through Compression: Unveiling Sudden Generalization via Minimal Description Length</td>
<td style="text-align: center;">$\mathbf{3}$</td>
</tr>
<tr>
<td style="text-align: left;">Grokking</td>
<td style="text-align: center;">Accelerating Mathematical Insight: Boosting Grokking Through Strategic Data Augmentation</td>
<td style="text-align: center;">$\mathbf{5}$</td>
</tr>
</tbody>
</table>
<p>From manual inspection, we find that Claude Sonnet 3.5 consistently produces the highest quality papers, with GPT-4o coming in second. We provide a link to all papers, run files, and logs in our GitHub repository, and recommend viewing the uploaded Claude papers for a qualitative analysis. This observation is also validated by the scores obtained from the LLM reviewer (Figure 4). When dividing the number of generated papers by the total cost, we end up at a cost of around $\$ 10-15$ per paper. Notably, GPT-4o struggles with writing LaTeX, which prevents it from completing many of its papers. For the open-weight models, DeepSeek Coder is significantly cheaper but often fails to correctly call the Aider tools. Llama-3.1 405b performed the worst overall but was the most convenient to work with, as we were frequently rate-limited by other providers. Both DeepSeek Coder and Llama-3.1 405b often had missing sections and results in their generated papers. In the following subsections, we will describe each template, its corresponding results, and specific papers.</p>
<h1>6.1. Diffusion Modeling</h1>
<p>General Description: This template studies improving the performance of diffusion generative models (Ho et al., 2020; Sohl-Dickstein et al., 2015) on low-dimensional datasets. Compared to image</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4 | Violin plots showing the distribution of scores generated by the The AI Scientist reviewer for AI-generated papers across three domains and four foundation models. Scores on the y-axis refer to NeurIPS ratings, which range from 2 (Strong Reject) to 6 (Weak Accept).
Table 3 | Evaluation of automated AI Scientist paper generation for Diffusion Modeling.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Total <br> Ideas</th>
<th style="text-align: center;">Novel <br> Ideas</th>
<th style="text-align: center;">Experiments <br> Passed</th>
<th style="text-align: center;">Completed <br> Papers</th>
<th style="text-align: center;">Mean <br> Score</th>
<th style="text-align: center;">Max <br> Score</th>
<th style="text-align: center;">Total <br> Cost</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Sonnet 3.5</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">3.82</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">$\sim \$ 250$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4o</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">41</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">3.70</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">$\sim \$ 300$</td>
</tr>
<tr>
<td style="text-align: left;">DeepSeek Coder</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">3.32</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">$\sim \$ 10$</td>
</tr>
<tr>
<td style="text-align: left;">Llama-3.1 405b</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2.30</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">$\sim \$ 120$</td>
</tr>
</tbody>
</table>
<p>generation, low-dimensional diffusion is much less well-studied, and thus there may be interesting algorithmic contributions to be made here.</p>
<p>Code Template: We base this template on a modified version of the popular 'tanelp/tiny-diffusion' repository (Pärnamaa, 2023) with additional minor hyperparameter tuning added and exponential moving average on the weights. The diffusion models are DDPM (Ho et al., 2020) models trained to generate samples from four distributions including geometric shapes, the two moons dataset, and a 2D dinosaur. The denoiser network is parameterized as an MLP with sinusoidal embeddings for the diffusion timestep and input data. The plotting script visualizes generated samples and plots training loss by default. Estimated KL is provided as an additional metric for sample quality via non-parametric entropy estimation.
Highlighted Generated Paper 1: DualScale Diffusion: Adaptive Feature Balancing for LowDimensional Generative Models. We analyze this paper in-depth in Section 5. This paper proposes a dual-scale denoising approach that splits the traditional diffusion denoiser into a global and a local processing branch. The network input is upscaled before being fed into the local branch. The outputs of the branches are then combined using a learnable time-conditioned weighting. It achieves impressive quantitative and qualitative results. It further manages to plot the evolution of the weighting across time, which requires very significant deviation from the provided code.
Highlighted Generated Paper 2: Multi-scale Grid Noise Adaptation: Enhancing Diffusion Models For Low-dimensional Data. This paper proposes to dynamically scale the standard diffusion noise schedule with a learned multiplicative factor based on where a particular input is in 2D space. The multiplicative factor is set by two grids that cover the input space, one coarse $5 \times 5$ grid and one more fine-grained $20 \times 20$ grid. This creative approach allows the diffusion model to dramatically improve performance across the datasets.
Highlighted Generated Paper 3: GAN-Enhanced Diffusion: Boosting Sample Quality and Diversity. This paper, inspired by GANs, proposes adding a discriminator to the diffusion model to guide the generation. It achieves comparable quantitative performance to the baseline, however, the</p>
<p>final generated figures appear to have fewer out-of-distribution points. This is notable as the current version of The AI Scientist is unable to view them (a problem that can be remedied by using multi-modal models in the future).</p>
<p>Highlighted Generated Paper 4: DualDiff: Enhancing Mode Capture in Low-dimensional Diffusion Models via Dual-expert Denoising. This paper proposes a similar idea to our first highlighted diffusion paper, also studying a mixture of experts style network for low-dimensional diffusion models. However, this idea evolves differently, with the standard diffusion loss now being augmented with a loss that encourages diversity in the two experts. The paper impressively visualizes the impact of the diversity loss in distributing inputs across both experts and further color-codes which parts of the sample space each expert is specialized in. We were particularly impressed by The AI Scientist's ability to perform a radically different take on a similar idea.</p>
<h1>6.2. Language Modeling</h1>
<p>Table 4 | Evaluation of automated AI Scientist paper generation for Language Modeling.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Total <br> Ideas</th>
<th style="text-align: center;">Novel <br> Ideas</th>
<th style="text-align: center;">Experiments <br> Passed</th>
<th style="text-align: center;">Completed <br> Papers</th>
<th style="text-align: center;">Mean <br> Score</th>
<th style="text-align: center;">Max <br> Score</th>
<th style="text-align: center;">Total <br> Cost</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Sonnet 3.5</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">4.05</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">$\sim \$ 250$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4o</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">44</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">3.25</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">$\sim \$ 300$</td>
</tr>
<tr>
<td style="text-align: left;">DeepSeek Coder</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">3.21</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">$\sim \$ 10$</td>
</tr>
<tr>
<td style="text-align: left;">Llama-3.1 405b</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">41</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2.31</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">$\sim \$ 120$</td>
</tr>
</tbody>
</table>
<p>General Description: This template investigates transformer-based (Vaswani et al., 2017) autoregressive next-token prediction tasks. Because this task is widely studied and optimized, it is difficult for The AI Scientist to find significant improvements. There are some common failure modes for this template that result in impressive-looking, but deceptive results. For example, a few of its ideas effectively cheat by subtly leaking information from future tokens, which results in lower perplexity.</p>
<p>Code Template: The code is modified from the popular NanoGPT repository (Karpathy, 2022). The provided script template trains a small transformer language model on the character-level Shakespeare dataset (Karpathy, 2015), the enwik8 dataset (Hutter, 2006), and the text8 dataset (Mahoney, 2011). It runs three seeds on the Shakespeare dataset, and one each on the remaining ones. The code saves the runtime, validation losses, and train losses. The plotting script visualizes training curves by default.</p>
<p>Highlighted Generated Paper 1: StyleFusion: Adaptive Multi-style Generation in Character-Level Language Models. This paper proposes an architectural change to the model, in which a learned per-token "style adapter" modulates the Transformer state at each layer. The method achieves strong results and deserves further investigation, though we suspect that one reason it may work is that it is simply adding more parameters, which may trivialize the result. Furthermore, it omits some important implementation details in the writing, such as how the style loss labels are derived (which appear to be randomly assigned on each update step).
Highlighted Generated Paper 2: Adaptive Learning Rates in Transformers via Q-Learning. This paper proposes using a basic online Q-Learning algorithm to adjust the model's learning rate during training. The state consists of the current learning rate and validation loss, the action applies a small perturbation to the learning rate, and the reward is the negative change in validation loss. While the idea is creative, it seems inappropriate to use simple Q-Learning in this highly non-stationary and partially-observed environment. Nonetheless, it happens to achieve effective results.</p>
<h1>6.3. Grokking Analysis</h1>
<p>Table 5 | Evaluation of automated AI Scientist paper generation for Grokking.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Total <br> Ideas</th>
<th style="text-align: center;">Novel <br> Ideas</th>
<th style="text-align: center;">Experiments <br> Passed</th>
<th style="text-align: center;">Completed <br> Papers</th>
<th style="text-align: center;">Mean <br> Score</th>
<th style="text-align: center;">Max <br> Score</th>
<th style="text-align: center;">Total <br> Cost</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Sonnet 3.5</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">47</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">3.44</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">$\sim \$ 250$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4o</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2.92</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">$\sim \$ 300$</td>
</tr>
<tr>
<td style="text-align: left;">DeepSeek Coder</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">3.13</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">$\sim \$ 10$</td>
</tr>
<tr>
<td style="text-align: left;">Llama-3.1 405b</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">2.00</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">$\sim \$ 120$</td>
</tr>
</tbody>
</table>
<p>General Description: This template investigates questions about generalization and learning speed in deep neural networks. We follow the classic experimental paradigm reported in Power et al. (2022) for analyzing "grokking", a poorly understood phenomenon in which validation accuracy dramatically improves long after the train loss saturates. We provide code that generates synthetic datasets of modular arithmetic tasks and then trains a Transformer model on them. Unlike the previous templates, this one is more amenable to open-ended empirical analysis (e.g. what conditions grokking occurs) rather than just trying to improve performance metrics.</p>
<p>Code Template: We base our implementation off of two popular open source re-implementations (May, 2022; Snell, 2021) of Power et al. (2022). The code generates four synthetic datasets of modular arithmetic tasks and trains a transformer on each across three random seeds. It returns train losses, validation losses, and the number of update steps required to reach perfect validation accuracy. The plotting scripts visualize the training and validation curves by default.</p>
<p>Highlighted Generated Paper 1: Unlocking Grokking: A Comparative Study of Weight Initialization Strategies in Transformer Models. This paper investigates different weight initializations and their impact on grokking. It finds that Xavier (Glorot and Bengio, 2010) and Orthogonal weight initializations consistently result in significantly faster grokking on the tasks than the widely-used default baseline weight initializations (Kaiming Uniform and Kaiming Normal). While this is a basic investigation, it provides an interesting result that could be studied in more depth. The paper also has a creative and catchy title.</p>
<p>Highlighted Generated Paper 2: Grokking Accelerated: Layer-wise Learning Rates for Transformer Generalization. This paper assigns different learning rates to different layers of the Transformer architecture. It finds that increasing the learning rate for higher layers results in significantly faster and more consistent grokking after iterating through different configurations throughout its experiments. It impressively includes the key section of its implementation in the write-up.</p>
<p>Highlighted Generated Paper 3: Grokking Through Compression: Unveiling Sudden Generalization via Minimal Description Length. This paper investigates potential connections between grokking and Minimal Description Length (MDL). We believe this idea is particularly interesting, though not executed very well. Its method for measuring MDL simply involves counting the number of parameters above a threshold $\epsilon$. While this does end up correlating with grokking, it is not analyzed in much depth. The paper could be significantly improved by investigating other estimates of MDL and including basic ablations. Furthermore, The AI Scientist failed to write the Related Works section and hallucinated a plot (Figure 5).</p>
<p>Highlighted Generated Paper 4: Accelerating Mathematical Insight: Boosting Grokking Through Strategic Data Augmentation. This paper investigates data augmentation techniques for grokking in modular arithmetic. It comes up with valid and creative augmentation techniques (operand reversal and operand negation) and finds that they can significantly accelerate grokking. While it is not surprising that data augmentation can improve generalization, the experiments and ideas seem</p>
<p>generally well-executed. However, The AI Scientist once again failed to write the Related Works section. In principle, this failure may be easily remedied by simply running the paper write-up step multiple times.</p>
<h1>7. Related Work</h1>
<p>While there has been a long tradition of automatically optimizing individual parts of the ML pipeline (AutoML, He et al. (2021); Hutter et al. (2019)), none come close to the full automation of the entire research process, particularly in communicating obtained scientific insights in an interpretable and general format.</p>
<p>LLMs for Machine Learning Research. Most closely related to our work are those that use LLMs to assist machine learning research. Huang et al. (2024) propose a benchmark for measuring how successfully LLMs can write code to solve a variety of machine learning tasks. Lu et al. (2024a) use LLMs to propose, implement, and evaluate new state-of-the-art algorithms for preference optimization. Liang et al. (2024) use LLMs to provide feedback on research papers and find that they provide similar feedback to human reviewers, while Girotra et al. (2023) find that LLMs can consistently produce higher quality ideas for innovation than humans. Baek et al. (2024); Wang et al. (2024b) use LLMs to propose research ideas based on scientific literature search but do not execute them. Wang et al. (2024c) automatically writes surveys based on an extensive literature search. Our work can be seen as the synthesis of all these distinct threads, resulting in a single autonomous open-ended system that can execute the entire machine learning research process.</p>
<p>LLMs for Structured Exploration. Because LLMs contain many human-relevant priors, they are commonly used as a tool to explore large search spaces. For example, recent works have used LLM coding capabilities to explore reward functions (Ma et al., 2023; Yu et al., 2023), virtual robotic design (Lehman et al., 2023), environment design (Faldor et al., 2024), and neural architecture search (Chen et al., 2024a). LLMs can also act as evaluators (Zheng et al., 2024) for "interestingness" (Lu et al., 2024b; Zhang et al., 2024) and as recombination operators for black-box optimization with Evolution Strategies (Lange et al., 2024; Song et al., 2024) and for Quality-Diversity approaches (Bradley et al., 2024; Ding et al., 2024; Lim et al., 2024). Our work combines many of these notions, including that our LLM Reviewer judges papers on novelty and interestingness, and that many proposed ideas are new combinations of previous ones.</p>
<p>AI for Scientific Discovery. There has been a long tradition of AI assisting scientific discovery (Langley, 1987, 2024) across many other fields. For example, AI has been used for chemistry (Buchanan and Feigenbaum, 1981), synthetic biology (Hayes et al., 2024; Jumper et al., 2021), materials discovery (Merchant et al., 2023; Pyzer-Knapp et al., 2022; Szymanski et al., 2023), mathematics (Lenat, 1977; Lenat and Brown, 1984; Romera-Paredes et al., 2024), and algorithm search (Fawzi et al., 2022). Other works aim to analyze existing pre-collected datasets and find novel insights (Falkenhainer and Michalski, 1986; Ifargan et al., 2024; Langley, 1987; Majumder et al., 2024; Nordhausen and Langley, 1990; Yang et al., 2024; Zytkow, 1996). Unlike our work, these are usually restricted to a well-defined search space in a single domain and do not involve "ideation", writing, or peer review from the AI system. In its current form, The AI Scientist excels at conducting research ideas implemented via code; with future advances (e.g. robotic automation for wet labs (Arnold, 2022; Kehoe et al., 2015; Sparkes et al., 2010; Zucchelli et al., 2021)), the transformative benefits of our approach could reach across all science, especially as foundation models continue to improve.</p>
<h2>8. Limitations \&amp; Ethical Considerations</h2>
<p>While The AI Scientist produces research that can provide novel insights, it has many limitations and raises several important ethical considerations. We believe future versions of The AI SCIENTIST</p>
<p>will be able to address many of its current shortcomings.
Limitations of the Automated Reviewer. While the automated reviewer shows promising initial results, there are several potential areas for improvement. The dataset used, from ICLR 2022, is old enough to potentially appear in the base model pre-training data - this is a hard claim to test in practice since typical publicly available LLMs do not share their training data. However, preliminary analysis showed that LLMs were far from being able to reproduce old reviews exactly from initial segments, which suggests they have not memorized this data. Furthermore, the rejected papers in our dataset used the original submission file, whereas for the accepted papers only the final camera-ready copies were available on OpenReview. Future iterations could use more recent submissions (e.g. from TMLR) for evaluation. Unlike standard reviewers, the automated reviewer is unable to ask questions to the authors in a rebuttal phase, although this could readily be incorporated into our framework. Finally, since it does not currently use any vision capabilities, The AI Scientist (including the reviewer) is unable to view figures and must rely on textual descriptions of them.</p>
<p>Common Failure Modes. The AI Scientist, in its current form, has several shortcomings in addition to those already identified in Section 5. These also include, but are not limited to:</p>
<ul>
<li>The idea generation process often results in very similar ideas across different runs and even models. It may be possible to overcome this by allowing The AI Scientist to directly follow up and go deeper on its best ideas, or by providing it content from recently-published papers as a source of novelty.</li>
<li>As shown in Tables 3 to 5, Aider fails to implement a significant fraction of the proposed ideas. Furthermore, GPT-4o in particular frequently fails to write LaTeX that compiles. While The AI SCIENTIST can come up with creative and promising ideas, they are often too challenging for it to implement.</li>
<li>The AI Scientist may incorrectly implement an idea, which can be difficult to catch. An adversarial code-checking reviewer may partially address this. As-is, one should manually check the implementation before trusting the reported results.</li>
<li>Because of The AI Scientist's limited number of experiments per idea, the results often do not meet the expected rigor and depth of a standard ML conference paper. Furthermore, due to the limited number of experiments we could afford to give it, it is difficult for The AI SCIENTIST to conduct fair experiments that control for the number of parameters, FLOPs, or runtime. This often leads to deceptive or inaccurate conclusions. We expect that these issues will be mitigated as the cost of compute and foundation models continues to drop.</li>
<li>Since we do not currently use the vision capabilities of foundation models, it is unable to fix visual issues with the paper or read plots. For example, the generated plots are sometimes unreadable, tables sometimes exceed the width of the page, and the page layout (including the overall visual appearance of the paper (Huang, 2018)) is often suboptimal. Future versions with vision and other modalities should fix this.</li>
<li>When writing, The AI Scientist sometimes struggles to find and cite the most relevant papers. It also commonly fails to correctly reference figures in LaTeX, and sometimes even hallucinates invalid file paths.</li>
<li>Importantly, The AI SCIENTIST occasionally makes critical errors when writing and evaluating results. For example, it struggles to compare the magnitude of two numbers, which is a known pathology with LLMs. Furthermore, when it changes a metric (e.g. the loss function), it sometimes does not take this into account when comparing it to the baseline. To partially address this, we make sure all experimental results are reproducible, storing copies of all files when they are executed.</li>
<li>Rarely, The AI Scientist can hallucinate entire results. For example, an early version of our writing prompt told it to always include confidence intervals and ablation studies. Due</li>
</ul>
<p>to computational constraints, The AI Scientist did not always collect additional results; however, in these cases, it would sometimes hallucinate an entire ablations table. We resolved this by instructing The AI Scientist explicitly to only include results it directly observed. Furthermore, it frequently hallucinates facts we do not provide, such as the hardware used.</p>
<ul>
<li>More generally, we do not recommend taking the scientific content of this version of The AI SCIENTIST at face value. Instead, we advise treating generated papers as hints of promising ideas for practitioners to follow up on. Nonetheless, we expect the trustworthiness of THE AI SCIENTIST to increase dramatically in the coming years in tandem with improvements to foundation models. We share this paper and code primarily to show what is currently possible and hint at what is likely to be possible soon.</li>
</ul>
<p>Safe Code Execution. The current implementation of The AI Scientist has minimal direct sandboxing in the code, leading to several unexpected and sometimes undesirable outcomes if not appropriately guarded against. For example, in one run, The AI Scientist wrote code in the experiment file that initiated a system call to relaunch itself, causing an uncontrolled increase in Python processes and eventually necessitating manual intervention. In another run, THE AI SCIENTIST edited the code to save a checkpoint for every update step, which took up nearly a terabyte of storage. In some cases, when The AI Scientist's experiments exceeded our imposed time limits, it attempted to edit the code to extend the time limit arbitrarily instead of trying to shorten the runtime. While creative, the act of bypassing the experimenter's imposed constraints has potential implications for AI safety (Lehman et al., 2020). Moreover, The AI Scientist occasionally imported unfamiliar Python libraries, further exacerbating safety concerns. We recommend strict sandboxing when running The AI SCIENTIST, such as containerization, restricted internet access (except for Semantic Scholar), and limitations on storage usage.</p>
<p>At the same time, there were several unexpected positive results from the lack of guardrails. For example, we had forgotten to create the output results directory in the grokking template in our experiments. Each successful run from The AI Scientist that outputted a paper automatically caught this error when it occurred and fixed it. Furthermore, we found that The AI Scientist would occasionally include results and plots that we found surprising, differing significantly from the provided templates. We describe some of these novel algorithm-specific visualizations in Section 6.1.</p>
<p>Broader Impact and Ethical Considerations. While The AI Scientist has the potential to be a valuable tool for researchers, it also carries significant risks of misuse. The ability to automatically generate and submit papers to academic venues could greatly increase the workload for reviewers, potentially overwhelming the peer review process and compromising scientific quality control. Similar concerns have been raised about generative AI in other fields, such as its impact on the arts (Epstein et al., 2023). Furthermore, if the Automated Reviewer tool was widely adopted by reviewers, it could diminish the quality of reviews and introduce undesirable biases into the evaluation of papers. Because of this, we believe that papers or reviews that are substantially AI-generated must be marked as such for full transparency.</p>
<p>As with most previous technological advances, The AI Scientist has the potential to be used in unethical ways. For example, it could be explicitly deployed to conduct unethical research, or even lead to unintended harm if The AI Scientist conducts unsafe research. Concretely, if it were encouraged to find novel, interesting biological materials and given access to "cloud labs" (Arnold, 2022) where robots perform wet lab biology experiments, it could (without its overseer's intent) create new, dangerous viruses or poisons that harm people before we can intervene. Even in computers, if tasked to create new, interesting, functional software, it could create dangerous malware. THE AI Scientist's current capabilities, which will only improve, reinforce that the machine learning community needs to immediately prioritize learning how to align such systems to explore in a manner</p>
<p>that is safe and consistent with our values.</p>
<h1>9. Discussion</h1>
<p>In this paper, we introduced The AI Scientist, the first framework designed to fully automate the scientific discovery process, and, as a first demonstration of its capabilities, applied it to machine learning itself. This end-to-end system leverages LLMs to autonomously generate research ideas, implement and execute experiments, search for related works, and produce comprehensive research papers. By integrating stages of ideation, experimentation, and iterative refinement, The AI Scientist aims to replicate the human scientific process in an automated and scalable manner.</p>
<p>Why does writing papers matter? Given our overarching goal to automate scientific discovery, why are we also motivated to have The AI Scientist write papers, like human scientists? For example, previous AI-enabled systems such as FunSearch (Romera-Paredes et al., 2024) and GNoME (PyzerKnapp et al., 2022) also conduct impressive scientific discovery in restricted domains, but they do not write papers.</p>
<p>There are several reasons why we believe it is fundamentally important for The AI Scientist to write scientific papers to communicate its discoveries. First, writing papers offers a highly interpretable method for humans to benefit from what has been learned. Second, reviewing written papers within the framework of existing machine learning conferences enables us to standardize evaluation. Third, the scientific paper has been the primary medium for disseminating research findings since the dawn of modern science. Since a paper can use natural language, and include plots and code, it can flexibly describe any type of scientific study and discovery. Almost any other conceivable format is locked into a certain kind of data or type of science. Until a superior alternative emerges (or possibly invented by AI), we believe that training The AI Scientist to produce scientific papers is essential for its integration into the broader scientific community.</p>
<p>Costs. Our framework is remarkably versatile and effectively conducts research across various subfields of machine learning, including transformer-based language modeling, neural network learning dynamics, and diffusion modeling. The cost-effectiveness of the system, producing papers with potential conference relevance at an approximate cost of $\$ 15$ per paper, highlights its ability to democratize research (increase its accessibility) and accelerate scientific progress. Preliminary qualitative analysis, for example in Section 5, suggests that the generated papers can be broadly informative and novel, or at least contain ideas worthy of future study.</p>
<p>The actual compute we allocated for The AI Scientist to conduct its experiments in this work is also incredibly light by today's standards. Notably, our experiments generating hundreds of papers were largely run only using a single $8 \times$ NVIDIA H100 node over the course of a week. Massively scaling the search and filtering would likely result in significantly higher-quality papers.</p>
<p>In this project, the bulk of the cost for running The AI Scientist is associated with the LLM API costs for coding and paper writing. In contrast, the costs associated with running the LLM reviewer, as well as the computational expenses for conducting experiments, are negligible due to the constraints we've imposed to keep overall costs down. However, this cost breakdown may change in the future if The AI Scientist is applied to other scientific fields or used for larger-scale computational experiments.</p>
<p>Open vs. Closed Models. To quantitatively evaluate and improve the generated papers, we first created and validated an Automated Paper Reviewer. We show that, although there is significant room for improvement, LLMs are capable of producing reasonably accurate reviews, achieving results comparable to humans across various metrics. Applying this evaluator to the papers generated by The AI Scientist enables us to scale the evaluation of our papers beyond manual inspection.</p>
<p>We find that Sonnet 3.5 consistently produces the best papers, with a few of them even achieving a score that exceeds the threshold for acceptance at a standard machine learning conference from the Automated Paper Reviewer.</p>
<p>However, there is no fundamental reason to expect a single model like Sonnet 3.5 to maintain its lead. We anticipate that all frontier LLMs, including open models, will continue to improve. The competition among LLMs has led to their commoditization and increased capabilities. Therefore, our work aims to be model-agnostic regarding the foundation model provider. In this project, we studied various proprietary LLMs, including GPT-4o and Sonnet, but also explored using open models like DeepSeek and Llama-3. We found that open models offer significant benefits, such as lower costs, guaranteed availability, greater transparency, and flexibility, although slightly worse quality. In the future, we aim to use our proposed discovery process to produce self-improving AI in a closed-loop system using open models.</p>
<p>Future Directions. Direct enhancements to The AI Scientist could include integrating vision capabilities for better plot and figure handling, incorporating human feedback and interaction to refine the AI's outputs, and enabling The AI Scientist to automatically expand the scope of its experiments by pulling in new data and models from the internet, provided this can be done safely. Additionally, The AI Scientist could follow up on its best ideas or even perform research directly on its own code in a self-referential manner. Indeed, significant portions of the code for this project were written by Aider. Expanding the framework to other scientific domains could further amplify its impact, paving the way for a new era of automated scientific discovery. For example, by integrating these technologies with cloud robotics and automation in physical lab spaces (Arnold, 2022; Kehoe et al., 2015; Sparkes et al., 2010; Zucchelli et al., 2021) provided it can be done safely, The AI SCIENTIST could perform experiments for biology, chemistry, and material sciences.</p>
<p>Crucially, future work should address the reliability and hallucination concerns, potentially through a more in-depth automatic verification of the reported results. This could be done by directly linking code and experiments, or by seeing if an automated verifier can independently reproduce the results.</p>
<p>Conclusion. The introduction of The AI Scientist marks a significant step towards realizing the full potential of AI in scientific research. By automating the discovery process and incorporating an AI-driven review system, we open the door to endless possibilities for innovation and problem-solving in the most challenging areas of science and technology. Ultimately, we envision a fully AI-driven scientific ecosystem including not only AI-driven researchers but also reviewers, area chairs, and entire conferences. However, we do not believe the role of a human scientist will be diminished. We expect the role of scientists will change as we adapt to new technology, and they will be empowered to tackle more ambitious goals. For instance, researchers often have more ideas than they have time to pursue, what if The AI Scientist could take the first explorations on all of them?</p>
<p>While the current iteration of The AI Scientist demonstrates a strong ability to innovate on top of well-established ideas, such as Diffusion Modeling or Transformers, it is an open question whether such systems can ultimately propose genuinely paradigm-shifting ideas. Will future versions of The AI Scientist be capable of proposing ideas as impactful as Diffusion Modeling, or come up with the next Transformer architecture? Will machines ultimately be able to invent concepts as fundamental as the artificial neural network, or information theory? We believe The AI Scientist will make a great companion to human scientists, but only time will tell to the extent to which the nature of human creativity and our moments of serendipitous innovation (Stanley and Lehman, 2015) can be replicated by an open-ended discovery process conducted by artificial agents.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Note that the experiment templates are very small-scale and are not compute-intensive. They would likely take a similar amount of time on cheaper GPUs, as we do not achieve high utilization.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
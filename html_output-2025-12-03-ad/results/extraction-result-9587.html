<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9587 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9587</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9587</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-166.html">extraction-schema-166</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-273849959</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.03417v1.pdf" target="_blank">Usefulness of LLMs as an Author Checklist Assistant for Scientific Papers: NeurIPS’24 Experiment</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) represent a promising, but controversial, tool in aiding scientific peer review. This study evaluates the usefulness of LLMs in a conference setting as a tool for vetting paper submissions against submission standards. We conduct an experiment at the 2024 Neural Information Processing Systems (NeurIPS) conference, where 234 papers were voluntarily submitted to an “LLM-based Checklist Assistant.” This assistant validates whether papers adhere to the author checklist used by NeurIPS, which includes questions to ensure compliance with research and manuscript preparation standards. Evaluation of the assistant by NeurIPS paper authors suggests that the LLM-based assistant was generally helpful in verifying checklist completion. In post-usage surveys, over 70% of authors found the assistant useful, and 70% indicate that they would revise their papers or checklist responses based on its feedback. While causal attribution to the assistant is not definitive, qualitative evidence suggests that the LLM contributed to improving some submissions. Survey responses and analysis of re-submissions indicate that authors made substantive revisions to their submissions in response to specific feedback from the LLM. The experiment also highlights common issues with LLMs—inaccuracy (20/52) and excessive strictness (14/52) were the most frequent issues flagged by authors. We also conduct experiments to understand potential gaming of the system, which reveal that the assistant could be manipulated to enhance scores through fabricated justifications, highlighting potential vulnerabilities of automated review tools.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9587.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9587.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Scientist (LRD+24)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The AI scientist: Towards fully automated open-ended scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned in this paper's related work as prior work aiming toward automated, open-ended scientific discovery using AI/LLM techniques; this NeurIPS paper only cites it and provides no methods, data, or results about law/equation discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The AI scientist: Towards fully automated open-ended scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>unspecified / general scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>unspecified (claimed goal: open-ended discovery, potentially including quantitative laws)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Not described in this paper; only referenced by title in related work. No prompt/fine-tuning/retrieval or other methodological details provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>No details provided about this referenced work within the paper. The NeurIPS paper discusses general LLM limitations that would apply to automated discovery approaches (hallucination, bias, inability to fully interpret complex mathematical content, limited multimodal handling, API data-privacy concerns, and susceptibility to adversarial manipulation), but it does not report empirical results or failure modes specific to the AI Scientist reference.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Usefulness of LLMs as an Author Checklist Assistant for Scientific Papers: NeurIPS’24 Experiment', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9587.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9587.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (Checklist Assistant)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 used as a Checklist Assistant for NeurIPS'24 submissions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper deployed GPT-4 (via API) as a per-question Checklist Assistant to validate author checklist answers and provide actionable feedback on 234 NeurIPS submissions; it did not use GPT-4 to extract quantitative laws or equations from the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 (third-party OpenAI API); used via API with hyperparameters temperature=1, top_p=1, n=1. No architecture/parameter-count details provided in the paper beyond the commercial model name and API usage.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>scientific peer review / machine learning papers (NeurIPS submissions)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Prompt-engineered per-checklist-question API calls: each checklist question (author answer + justification + full paper text/appendices) was sent as a separate prompt; instructions required itemized, actionable feedback and a final score line (Score: 1/0.5/0). Long papers were truncated to first 15,000 words; scoring and output formatting constrained to reduce mixing of score and review. Auxiliary use of LLM (GPT-4) to summarize and cluster feedback (GPT-4 used as annotator) and a separate LLM (GPT-4o) used in adversarial attack experiments to iteratively rewrite justifications.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>234 voluntarily submitted NeurIPS'24 papers with accompanying author checklists and appendices; inputs were PDF->text parsed (with parsing issues noted), each checklist question processed individually. No external links or multimodal content (figures) were ingested.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Author surveys (pre- and post-usage), analysis of LLM feedback content (automatic extraction and clustering of feedback points using an LLM), re-submission comparisons (word-count changes, checklist answer changes), and adversarial attack experiments (iterative LLM rewriting of justifications and reevaluation of scores). Statistical tests reported (e.g., permutation tests, binomial tests, bootstrapping, Clopper-Pearson intervals) for various analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Primary outcomes relate to checklist verification utility, not law discovery: >70% of surveyed authors found the assistant useful and would revise papers/checklist responses; LLM gave granular feedback (typically 4–6 distinct points per question). Reported issues: inaccuracy (20/52 freeform complainants), excessive strictness (14/52). Adversarial attack: iterative AI rewriting of justifications increased Checklist Assistant scores on 14/15 questions across 234 papers, demonstrating vulnerability to gaming. No quantitative laws/equations were distilled or evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Not applicable for quantitative-law extraction. Notable methodological behaviors: LLM provided concrete feedback (e.g., on optimizer settings, compute reporting, claim-consistency) and also produced illegitimate fabricated justifications (e.g., invented hardware descriptions, placeholder repository URLs) when used as an attack agent.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Reported and observed limitations relevant to any attempt to extract mathematical relationships from literature: hallucination/inaccuracy (LLM overlooked content or misinterpreted text), excessive strictness or irrelevant requests, inability to process multimodal elements (figures/equations in images), PDF-to-text parsing failures ( ligatures, section detection), truncation limits (15k words), variability/inconsistency across runs for some questions, and vulnerability to adversarially optimized justifications that can falsely increase verification scores. Data-privacy concerns from sending papers to third-party APIs are also noted.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>No direct comparison to human experts or classical automated equation-discovery systems for the task of distilling quantitative laws; the paper compares authors' perceptions and subsequent manuscript changes but does not benchmark GPT-4 against human extraction of mathematical relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Usefulness of LLMs as an Author Checklist Assistant for Scientific Papers: NeurIPS’24 Experiment', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The AI scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 2)</em></li>
                <li>Can large language models provide useful feedback on research papers? a large-scale empirical analysis <em>(Rating: 1)</em></li>
                <li>Specter: Document-level representation learning using citation-informed transformers <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9587",
    "paper_id": "paper-273849959",
    "extraction_schema_id": "extraction-schema-166",
    "extracted_data": [
        {
            "name_short": "AI Scientist (LRD+24)",
            "name_full": "The AI scientist: Towards fully automated open-ended scientific discovery",
            "brief_description": "Mentioned in this paper's related work as prior work aiming toward automated, open-ended scientific discovery using AI/LLM techniques; this NeurIPS paper only cites it and provides no methods, data, or results about law/equation discovery.",
            "citation_title": "The AI scientist: Towards fully automated open-ended scientific discovery",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "scientific_domain": "unspecified / general scientific discovery",
            "law_type": "unspecified (claimed goal: open-ended discovery, potentially including quantitative laws)",
            "method_description": "Not described in this paper; only referenced by title in related work. No prompt/fine-tuning/retrieval or other methodological details provided here.",
            "input_corpus_description": null,
            "evaluation_method": null,
            "results_summary": null,
            "notable_examples": null,
            "limitations_challenges": "No details provided about this referenced work within the paper. The NeurIPS paper discusses general LLM limitations that would apply to automated discovery approaches (hallucination, bias, inability to fully interpret complex mathematical content, limited multimodal handling, API data-privacy concerns, and susceptibility to adversarial manipulation), but it does not report empirical results or failure modes specific to the AI Scientist reference.",
            "baseline_comparison": null,
            "uuid": "e9587.0",
            "source_info": {
                "paper_title": "Usefulness of LLMs as an Author Checklist Assistant for Scientific Papers: NeurIPS’24 Experiment",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "GPT-4 (Checklist Assistant)",
            "name_full": "GPT-4 used as a Checklist Assistant for NeurIPS'24 submissions",
            "brief_description": "This paper deployed GPT-4 (via API) as a per-question Checklist Assistant to validate author checklist answers and provide actionable feedback on 234 NeurIPS submissions; it did not use GPT-4 to extract quantitative laws or equations from the literature.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "GPT-4 (third-party OpenAI API); used via API with hyperparameters temperature=1, top_p=1, n=1. No architecture/parameter-count details provided in the paper beyond the commercial model name and API usage.",
            "scientific_domain": "scientific peer review / machine learning papers (NeurIPS submissions)",
            "law_type": null,
            "method_description": "Prompt-engineered per-checklist-question API calls: each checklist question (author answer + justification + full paper text/appendices) was sent as a separate prompt; instructions required itemized, actionable feedback and a final score line (Score: 1/0.5/0). Long papers were truncated to first 15,000 words; scoring and output formatting constrained to reduce mixing of score and review. Auxiliary use of LLM (GPT-4) to summarize and cluster feedback (GPT-4 used as annotator) and a separate LLM (GPT-4o) used in adversarial attack experiments to iteratively rewrite justifications.",
            "input_corpus_description": "234 voluntarily submitted NeurIPS'24 papers with accompanying author checklists and appendices; inputs were PDF-&gt;text parsed (with parsing issues noted), each checklist question processed individually. No external links or multimodal content (figures) were ingested.",
            "evaluation_method": "Author surveys (pre- and post-usage), analysis of LLM feedback content (automatic extraction and clustering of feedback points using an LLM), re-submission comparisons (word-count changes, checklist answer changes), and adversarial attack experiments (iterative LLM rewriting of justifications and reevaluation of scores). Statistical tests reported (e.g., permutation tests, binomial tests, bootstrapping, Clopper-Pearson intervals) for various analyses.",
            "results_summary": "Primary outcomes relate to checklist verification utility, not law discovery: &gt;70% of surveyed authors found the assistant useful and would revise papers/checklist responses; LLM gave granular feedback (typically 4–6 distinct points per question). Reported issues: inaccuracy (20/52 freeform complainants), excessive strictness (14/52). Adversarial attack: iterative AI rewriting of justifications increased Checklist Assistant scores on 14/15 questions across 234 papers, demonstrating vulnerability to gaming. No quantitative laws/equations were distilled or evaluated.",
            "notable_examples": "Not applicable for quantitative-law extraction. Notable methodological behaviors: LLM provided concrete feedback (e.g., on optimizer settings, compute reporting, claim-consistency) and also produced illegitimate fabricated justifications (e.g., invented hardware descriptions, placeholder repository URLs) when used as an attack agent.",
            "limitations_challenges": "Reported and observed limitations relevant to any attempt to extract mathematical relationships from literature: hallucination/inaccuracy (LLM overlooked content or misinterpreted text), excessive strictness or irrelevant requests, inability to process multimodal elements (figures/equations in images), PDF-to-text parsing failures ( ligatures, section detection), truncation limits (15k words), variability/inconsistency across runs for some questions, and vulnerability to adversarially optimized justifications that can falsely increase verification scores. Data-privacy concerns from sending papers to third-party APIs are also noted.",
            "baseline_comparison": "No direct comparison to human experts or classical automated equation-discovery systems for the task of distilling quantitative laws; the paper compares authors' perceptions and subsequent manuscript changes but does not benchmark GPT-4 against human extraction of mathematical relationships.",
            "uuid": "e9587.1",
            "source_info": {
                "paper_title": "Usefulness of LLMs as an Author Checklist Assistant for Scientific Papers: NeurIPS’24 Experiment",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The AI scientist: Towards fully automated open-ended scientific discovery",
            "rating": 2,
            "sanitized_title": "the_ai_scientist_towards_fully_automated_openended_scientific_discovery"
        },
        {
            "paper_title": "Can large language models provide useful feedback on research papers? a large-scale empirical analysis",
            "rating": 1,
            "sanitized_title": "can_large_language_models_provide_useful_feedback_on_research_papers_a_largescale_empirical_analysis"
        },
        {
            "paper_title": "Specter: Document-level representation learning using citation-informed transformers",
            "rating": 1,
            "sanitized_title": "specter_documentlevel_representation_learning_using_citationinformed_transformers"
        }
    ],
    "cost": 0.0144005,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Usefulness of LLMs as an Author Checklist Assistant for Scientific Papers: NeurIPS'24 Experiment
8 Nov 2024</p>
<p>Alexander Goldberg 
Ihsan Ullah 
Thanh Gia 
BenedictusHieu Khuong 
Kent Rachmat 
Zhen Xu 
Isabelle Guyon 
Nihar B Shah 
Usefulness of LLMs as an Author Checklist Assistant for Scientific Papers: NeurIPS'24 Experiment
8 Nov 2024532C8E271FD96E20F493F552A4F97768arXiv:2411.03417v2[cs.CL]
Large language models (LLMs) represent a promising, but controversial, tool in aiding scientific peer review.This study evaluates the usefulness of LLMs in a conference setting as a tool for vetting paper submissions against submission standards.We conduct an experiment at the 2024 Neural Information Processing Systems (NeurIPS) conference, where 234 papers were voluntarily submitted to an "LLMbased Checklist Assistant."This assistant validates whether papers adhere to the author checklist used by NeurIPS, which includes questions to ensure compliance with research and manuscript preparation standards.Evaluation of the assistant by NeurIPS paper authors suggests that the LLM-based assistant was generally helpful in verifying checklist completion.In post-usage surveys, over 70% of authors found the assistant useful, and 70% indicate that they would revise their papers or checklist responses based on its feedback.While causal attribution to the assistant is not definitive, qualitative evidence suggests that the LLM contributed to improving some submissions.Survey responses and analysis of resubmissions indicate that authors made substantive revisions to their submissions in response to specific feedback from the LLM.The experiment also highlights common issues with LLMs-inaccuracy (20/52) and excessive strictness (14/52) were the most frequent issues flagged by authors.We also conduct experiments to understand potential gaming of the system, which reveal that the assistant could be manipulated to enhance scores through fabricated justifications, highlighting potential vulnerabilities of automated review tools.</p>
<p>Introduction</p>
<p>Recent advancements in large language models (LLMs) have significantly enhanced their capabilities in areas such as question answering and text generation.One promising application of LLMs is in aiding the scientific peer-review process [Sha22, KAD + 24].However, the idea of using LLMs in peer review is contentious and fraught with potential issues [LS23].LLMs can hallucinate, exhibit biases, and may compromise the fairness of the peer-review process.Despite these potential issues, LLMs may serve as useful analytical tools to scrutinize manuscripts and identify possible weaknesses or inaccuracies that need addressing.</p>
<p>In this study, we take the first steps towards harnessing the power of LLMs in the application of conference peer review.We conduct an experiment the the Neural Information Processing Systems (NeurIPS) 2024 conference, a premier conference in the field of machine learning. 1While the wider ethical implications and appropriate use cases of LLMs remain unclear and must be a larger community discussion, here, we evaluate a relatively clear-cut and low-risk use case: vetting paper submissions against submission standards, with results shown only to the authors.</p>
<p>Specifically, the NeurIPS peer-review process requires authors to submit a checklist appended to their manuscripts.Such author checklists, utilized in NeurIPS as well as in other peer-review venues [MSA01, VEA + 07, MLT + 09], contain a set of questions designed to ensure that authors follow appropriate research</p>
<p>• The majority of surveyed authors reported a positive experience using the LLM assistant.After using the assistant, over 70% of authors reported that they found the assistant useful and over 70% reported that they would modify their paper and/or checklist responses based on the feedback given (Section 4.1.3).</p>
<p>• Authors' expectations of the assistant's effectiveness were even more positive before using it than their assessments after actually using it (Section 4.1.3).</p>
<p>• Among the main issues reported by authors in qualitative feedback, the most frequently cited were inaccuracy (20/52 respondents) and that the LLM was too strict in its requirements (14/52 respondents) (Section 4.1.4).</p>
<p>(2) While changes in NeurIPS paper submissions cannot be causally attributed to use of the checklist verification assistant, we find qualitative evidence that the checklist review meaningfully helped some authors to improve their submissions.</p>
<p>• Analysis of the content of LLM feedback to authors indicates that the LLM provided granular feedback to authors, generally giving 4-6 distinct and specific points of feedback per question across the 15 questions (Section 4.2.1).</p>
<p>• Survey responses reflect that some authors made meaningful changes to their submissions-35 survey respondents described specific modifications they would make to their submissions in response to the Checklist Assistant (Section 4.2.2).</p>
<p>• In 40 instances, authors submitted their paper twice to the checklist verifier (accounting for 80 total paper submissions.)Between these two submissions, authors tended to increase the length of their checklist justifications significantly, suggesting that they may have added content in response to LLM feedback (Section 4.2.3).</p>
<p>Finally, we investigate how LLM-based tools can be easily manipulated -specifically, we find that with AIassisted re-writing of the justifications, an adversarial author can make the Checklist Assistant significantly more lenient (Section 5.1).In summary, the majority of authors found LLM assistance to be beneficial, highlighting the significant potential of LLMs to enhance scientific workflows-whether by serving as direct assistants to authors or helping journals and conferences verify guideline compliance.However, our findings also underscore that LLMs cannot fully replace human expertise in these contexts.A notable portion of users encountered inaccuracies, and the models were also vulnerable to adversarial manipulation.</p>
<ol>
<li>
<p>New Assets: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?</p>
</li>
<li>
<p>Crowdsourcing and Research with Human Subjects: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?</p>
</li>
<li>
<p>Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?</p>
</li>
</ol>
<p>Related work</p>
<p>Language models have been used in the scientific peer review process for over a decade.The primary application so far has been in assigning reviewers to papers.Here, a language model first computes a "similarity score" between every reviewer-paper pair, based on the text of the submitted paper and the text of the reviewer's previously published papers [CZ13, WGNBK19, CFB + 20] (see [Sha22, Section 3] for more references).A higher value of the similarity score indicates that the language model considers this reviewer to have a higher expertise for this paper.Given these similarity scores, reviewers are then assigned to papers using an optimization routine that maximizes the similarity scores of the assigned reviewer-paper pairs [CZ13, SSS21, PZ22].</p>
<p>There have been recent works that design or use LLMs to write the entire review of papers [LZC + 23, TLS + 24, DHBD24, LLL + 24, DRB + 23].The outcome measures for evaluating the effectiveness of the LLMgenerated reviews are based on ratings sourced from authors or other researchers.It is not entirely clear how these ratings translate to meeting the objectives of peer review in practice namely that of identifying errors, choosing better papers, and providing useful feedback to authors.Moreover, it is also known that evaluation of peer reviews themselves are fraught with biases [GSC + 23], and the aggregate effect of such biases on these evaluations of reviews is not clear.Our work focuses on a more concrete task in reviewing papers than generating an end-to-end review, namely validating that papers meet criteria specified in an Author Checklist.Moreover, we evaluate the efficacy of LLMs in the setting of an actual peer review conference.</p>
<p>Recent work also investigates whether LLMs can identify errors in papers and shows promising initial results.The paper [LS23] constructs a set of short papers with deliberately inserted errors and asks LLMs to identify errors.GPT-4 does identify the error more than half the time.Another experiment described in [Sha22] asks GPT-4 to identify deliberately inserted errors in three full papers.It successfully and consistently does so on one paper, partially and occasionally on a second paper, and is consistently unsuccessful on the third.Note that in both experiments [LS23,Sha22], the prompts specifically asked the LLM to find errors rather than generically asking the LLM to review the paper.Moreover, both experiments had small sample sizes in terms of the number of papers.In another set of experiments presented in [LS23], evaluated the ability of large language models (LLMs) to compare the "strength" of results between papers, mirroring the goals of conferences and journals in selecting 'better' papers.The experiment consisted of creating 10 pairs of abstracts, where one abstract in each pair was made 'clearly' and objectively stronger than the other.To simulate diverse, yet irrelevant conditions, the language of the abstracts was deliberately varied.In this test, GPT-4 performed no better than random chance in identifying the stronger abstract, underscoring that while LLMs may excel at some complex tasks like scientific error identification, they often struggle with seemingly simpler tasks.</p>
<p>The papers [LS23, WBM + 23] investigate the performance of LLMs in evaluating checklist compliance.These studies, however, were retrospective studies of published papers, whereas our work is deployed live associated to a peer-review venue and helps authors improve their checklist compliance before they make their submission.</p>
<p>Recent work has highlighted the prevalence of the use of LLMs both in preparation of scientific paper manuscripts [LZW + 24] and in the generation of scientific peer reviews [LIZ + 24, LRD + 24].For example, [LZW + 24] estimates that as of January 2024, 17.5% of the abstracts in Computer Science papers on arXiv Question 3: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA] Justification: There is no theory in this paper.</p>
<p>Review: The answer provided by the authors is consistent with the content of the paper.Based on the detailed review of the paper, it is clear that the work primarily focuses on conducting an observational study to investigate the presence of citation bias in peer review processes.The study utilizes statistical modeling and analysis rather than theoretical developments that would involve theorems, lemmas, or formal proofs.Therefore, the classification of the answer as "NA" (Not Applicable) is accurate given the nature of the research presented.However, to provide constructive and actionable suggestions for improvement and to ensure thorough and critical assessment, it is beneficial to clearly outline the assumptions behind the statistical models and analyses employed.This would enhance the readability and understandability of the study's methodology and its limitations.[...] Question 12: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?Answer: [Yes] Justification: We have used algorithms such as TPMS and PeerReview4All.We have cited them.Review: There does not appear to be any discrepancy between the authors' justification for the cited question and the actual paper content in terms of providing proper credit and mentioning licenses for used assets, according to their statement.However, the detailed source citations such as the specific papers for TPMS (Toronto Paper Matching System) and PeerReview4All, including their licenses or terms of use, are not explicitly mentioned in the provided text.It's essential to ensure that these aspects are clearly attributed and any terms of use are respected, as stated in the initial guidelines provided to authors.[...] had used an LLM to significantly modify content.The fact that LLMs are already being used by authors and reviewers throughout the scientific peer review process underscores the need for systematic evaluation of the efficacy of LLMs in scientific peer review, which we begin to address in this paper.Given the many known risks associated with large language models (LLMs), our approach to deploying them in this realworld scenario was highly cautious.We focused solely on using the LLMs to assist authors in improving their papers, deliberately excluding reviewers from this process.</p>
<p>Design and deployment</p>
<p>We design an LLM-based tool (Checklist Assistant) to assist NeurIPS authors in ensuring their submitted checklists are thoroughly answered.Our platform interfaced with a third-party LLM (GPT-4 from OpenAI), using simple prompt engineering with these hyper-parameters: temperature = 1, top p = 1, and n = 1.For each checklist question, the LLM is provided with the author's checklist response and justification, alongside the complete paper and any appendices.The LLM's role is to assess the accuracy and thoroughness of each response and justification, offering targeted suggestions for improvement.Each checklist item is treated as an individual task, i.e., an API call with only one question, its answer and justification by the author, and the paper and appendices.The API call returns a review and score for the submitted question.</p>
<p>Figure 1 illustrates examples of feedback provided by the Checklist Assistant for two different papers.In these examples, green indicates that the tool found "no significant concerns", while orange signals "needs improvement" with the NeurIPS Paper Checklist standards.Authors are encouraged to carefully review any orange feedback, validate the identified issues, and make the necessary revisions to align with the checklist requirements.</p>
<p>Deployment</p>
<p>We deployed the Checklist Assistant on Codabench.org[XEP + 22].We configured 15 Google Cloud CPU workers, integrated with Codabench, to handle multiple paper submissions concurrently.The bulk of the computations were carried out by the LLM third-party software (GPT-4 from OpenAI) via API calls (one call per question, and additional calls in case of failure).</p>
<p>Participation was fully voluntary, and participants were recruited through a NeurIPS blog post that was released 8 days before the abstract submission deadline.Interested participants were asked to register though a Google form.Participants who submitted registration requests through the Google form were then given access to the Assistant on the Codabench platform.The submissions were entirely optional and completely separate from the NeurIPS paper submission system and the review process.The papers had to be formatted as specified in the NeurIPS'24 call for papers (complete with appendices and checklist).Information provided in external links was not taken into account by the assistant.We asked submitters to fill out the checklist to the best of their abilities.Submissions made via the Codabench landing page were processed as follows:</p>
<ol>
<li>
<p>Checklist Assistant: The paper was parsed using a PDF-to-text parser, then screened for any problems such as the format of the paper or checklist, etc.Each answered question in the checklist was processed by an LLM using an API.</p>
</li>
<li>
<p>Result Compilation: LLM responses were combined for all questions and formatted in an HTML document with proper colors and structure for readability and user-friendliness.</p>
</li>
</ol>
<p>We encountered several parsing issues with both paper texts and checklists.Initially, our parser struggled with subsections and titles, prompting code improvements to handle sections accurately.Checklist parsing also faced issues due to spacing and incomplete checklists, which we addressed by refining the code.Special characters, especially merged letters like "fi" and "fl" in the submitted PDFs required further parsing updates.</p>
<p>Prompt engineering</p>
<p>In this section we discuss design of a prompt given to the LLM, tasked to behave as Checklist Assistant.We provide the full prompt in Appendix B.</p>
<p>While preparing the Checklist Assistant, we experimented with various prompt styles.Tuning was carried out using a dozen papers.Some checklists were filled out with our best effort to be correct, and others included deliberately planted errors to verify robustness and calibrate the scores.We observed that the LLM performed better with clear, step-by-step instructions.</p>
<p>Our final prompt provided a sequence of instructions covering different aspects of the required review, designed as follows: first, the context is set by indicating that the paper is under review for the NeurIPS conference.Next, the main goal is clarified, specifying that the LLM's primary task is to assist the author in responding to the checklist question.The LLM is then directed to review the author's answer and justification, identifying any discrepancies with the paper based on the specific guidelines of the question.It is instructed to provide itemized, actionable feedback according to the guidelines, offering suggestions for improvement, with clear examples for responses such as "Yes," "No," or "NA."At the end of the review, the LLM is asked to assign a score: Score=1 for no issues, Score=0.5 for minor improvements, and Score=0 for critical issues.Finally, the LLM is provided with the checklist question, the author's answer, justification, the relevant guidelines, and the paper content.</p>
<p>Before prompt adjustments, LLM responses often mixed the review with the score.To fix this, we specified that the score should be returned on a separate line at the end of the review.For long papers exceeding 35 pages (or 15,000 words), we processed only the first 15,000 words and notified authors with a warning.</p>
<p>We hypothesized that users might find the LLM responses overly strict, vague, and lengthy (which was indeed later confirmed), so we added prompt instructions like "use 0 score sparingly", "provide itemized, actionable feedback", and "focus on significant improvements."Although the Checklist Assistant returned scores of 0, 0.5, and 1, we combined the 0 and 0.5 scores to indicate that improvement was needed, rather than differentiating between two levels of severity (with red for 0 and orange for 0.5).This decision was made due to concerns that the LLM's evaluations might be too harsh.User feedback on LLM strictness and other issues is analyzed in Section 4.</p>
<p>We also tested whether the LLM was consistent in generating answers for reiterations of the same input.As a sanity check, we test for each question, whether the variation of the output scores for multiple runs on the same paper is comparable to the variation across papers.We find that the variation in scores for multiple runs on the same paper is significantly lower than variation across papers (p &lt; 0.05; based on a one sided permutation test after BH correction) for all but one question.The only question that had a comparable variance within and across papers was the question on ethics (Q9; p &gt; 0.4).</p>
<p>Anonymity, confidentiality, and consent</p>
<p>The authors could retain their anonymity by registering to Codabench with an email that did not reveal their identity, and by submitting anonymized papers.The papers and LLM outputs were kept confidential and were not be accessible to NeurIPS reviewers, meta reviewers, and program chairs.It is important to note that while authors retained ownership of their submissions, the papers were sent to the API of an LLM service, and treated under their conditions of confidentiality.</p>
<p>This study was approved by the Carnegie Mellon University Institutional Review Board (IRB).The participants gave written documentation of informed consent to participate.</p>
<p>Evaluation</p>
<p>In our evaluations, we seek to address two main questions regarding the use of an LLM-automated Author Checklist Assistant:</p>
<p>(1) Do authors perceive an LLM Author Checklist Assistant as a valuable enhancement to the paper submission process?</p>
<p>(2) Does the use of an Author Checklist Assistant meaningfully help authors to improve their paper submissions?</p>
<p>In order to understand author experience using the provided Author Checklist Assistant, we surveyed authors before and after submitting to the Author Checklist Assistant.Additionally, we analyzed the content and submission patterns of author's checklists and the LLM responses.A summary of our main findings is given in Section 1.In this subsequent section we provide detailed analyses of survey responses and usage of the Checklist Assistant.In Section 4.1, we give results on author perception and experience and in Section 4.2 we analyze changes made by authors to their submissions after using the Author Checklist Assistant.</p>
<p>Author Perception and Experience</p>
<p>First, we analyze the authors' usage patterns and perceptions of the Author Checklist Assistant, as captured through surveys.In Section 4.1.1,we provide an overview of how authors filled out the checklist and the responses given by the LLM on their checklists.In Section 4.1.2,we detail the survey methodology used to understand author experience and in Section 4.1.3,we analyze results of the survey.Finally, in Section 4.1.4,we overview the main challenges identified by authors when using the Author Checklist Assistant.</p>
<p>Overview of Checklist Usage and Responses</p>
<p>A total of 234 papers, each accompanied by a checklist, were submitted to the assistant.For each checklist question, authors could respond with Yes, No, NA, or TODO.As illustrated in Figure 2a, most questions received a Yes response, indicating that the authors confirmed their paper met the corresponding checklist criteria.However, for the questions on Theory, Impacts, Safeguards, Documentation, Human Subjects, and   Risks, a significant portion of authors selected NA.Additionally, a notable number of authors responded No to the questions on Code and Data, and Error Bars.</p>
<p>In response to the authors' checklists, the LLM provided written feedback, with green indicating 'No Concerns' and orange indicating 'Needs improvement'.Figure 2b illustrates the distribution of LLM feedback for each checklist question.For most questions, the majority of feedback suggested that the checklist or manuscript could be improved.However, for the questions on Theory, Human Subjects, and Risks, many NA responses were deemed appropriate, leading the LLM to respond with 'No Concerns.'This likely reflects the LLM's confidence in confirming that certain papers did not include theory, human subjects research, or clear broader risks, making those checklist items irrelevant.In Figure 3, we show the distribution of LLM evaluations per submission.All submissions received several 'Needs improvement' ratings, with each being advised to improve on 8 to 13 out of the 15 checklist questions.</p>
<p>Survey Methodology</p>
<p>To assess authors' perceptions of the usefulness of the Author Checklist Assistant, we conducted a survey with all participants both at registration (pre-usage) and immediately after using the Author Checklist Assistant (post-usage).We provide the content of the surveys in Figure 4.Both surveys contained the same four questions, with the pre-usage survey focusing on expectations and the post-usage survey on actual I am motivated to participate because I anticipate that feedback from an LLM will. . .(a) Pre-usage survey.Freeform feedback:</p>
<p>lead me to</p>
<p>• Describe any changes that you intend to make to your manuscript due to the feedback you received.</p>
<p>• Describe any issues you experienced with the accuracy or usability of checklist verification.</p>
<p>(b) Post-usage survey.</p>
<p>Figure 4: Survey questions administered to the participants experience.Responses were recorded on a four-point Likert scale, ranging from strongly disagree to strongly agree.In the post-usage survey, we also asked authors to provide freeform feedback on (1) any changes they planned to make to their paper, and (2) any issues they encountered while using the Checklist Assistant.We received 539 responses to the pre-usage survey and 234 papers submitted.However, we received only 78 responses to the post-usage survey, representing 63 unique participants (due to multiple submissions for the same paper).While completing the pre-registration survey was mandatory for all participants, the post-usage survey was optional.As a result, all participants in the post-usage survey had also completed the pre-registration survey.</p>
<p>Survey Responses</p>
<p>Figure 5 presents the survey responses collected before and after using the checklist verification tool.We include responses from authors who completed both surveys (n=63).In cases where authors submitted the survey multiple times for the same paper, we included only the earliest post-usage response.Including the duplicated responses made a negligible difference, with the proportion of positive responses changing by less than 0.02 across all questions.</p>
<p>Overall, the majority of authors responded positively regarding their experience with the Checklist Assistant.70% of surveyed authors reported plans to make changes based on the feedback received, 70% reported that they found the assistant concretely useful, and 67% expressed excitement about using LLMs as Checklist Assistants in the future.Thus, a statistically significant majority of authors responded positively to "Will Modify", "Useful" and "Excited to Use" after using the assistant based on a one-sided Binomial Test with Benjamini-Hochberg Correction comparing the sample proportion to 0.5 (adjusted p-values of 0.001, 0.002, and 0.007 respectively).</p>
<p>It is notable that authors were even more positive before using the tool.Comparing pre-and postusage responses, there was a statistically significant drop in positive feedback on the "Useful" and "Excited to Use" questions-we run a permutation test with 50,0000 permutations to test whether the difference between proportion of positive responses pre and post-usage is non-zero, which gives Benjamini-Hochberg adjusted p-values of 0.007 and 0.013 for"Excited to Use" and "Useful" respectively with effect sizes of −0.23 and −0.2.</p>
<p>We also assessed the correlation between post-usage survey responses and the number of 'needs improvement' scores given by the LLM to authors.In Figure 6, we show mean number of needs improvement scores for authors responding positively or negatively to each survey question.We find no substantial effect of number of 'needs improvement' scores on survey responses.This may reflect that the number of 'needs improvement' scores was less important in author's perception than the written content of the LLM's  Finally, we examined potential selection bias due to the drop-off in participation in the post-usage survey by analyzing the pre-usage survey responses across different groups.As noted earlier, only a portion of the 539 participants who completed the pre-usage survey went on to submit papers (234 Submitters), and an even smaller group responded to the post-usage survey (78 Post-Usage Respondents).In Figure 7, we compare the pre-usage survey responses between Submitters and Non-Submitters, as well as between Post-Usage Respondents and Non-Respondents.No substantial differences in rates of positive responses were found (using a permutation test for the difference in mean response, gave p-values of &gt; 0.3 for all questions before multiple testing correction), suggesting there is no significant selection bias.</p>
<p>Challenges in Usage</p>
<p>In addition to the structured survey responses, 52 out of the 78 post-usage survey submissions included freeform feedback detailing issues with the Checklist Assistant's usage.We manually categorized the reported issues from these responses and identified the following primary concerns, listed in order of decreasing frequency (summarized in Figure 8):</p>
<ol>
<li>
<p>Inaccurate: 20 authors reported that the LLM was inaccurate.Note that it is not possible to tell from the responses how many inaccuracies participants found in individual questions since the survey did not ask   about individual checklist questions.Many participants noted specific issues, in particular that the LLM overlooked content in the paper, requesting changes to either the checklist or the paper for elements that the authors believed were already addressed.Additionally, some authors reported more nuanced accuracy issues.For instance, one author mentioned that the LLM misinterpreted a "thought experiment" as a real experiment and incorrectly asked for more details about the experimental setup.Another author reported that the LLM mistakenly assumed human subjects were involved due to a discussion of "interpretability" in the paper.</p>
</li>
<li>
<p>Too strict: 14 authors reported that the LLM was too strict.</p>
</li>
<li>
<p>Infeasible to make changes due to page limits: 5 authors felt that they received useful feedback, but it would not be possible to incorporate due to their papers already being at the page limit.</p>
</li>
<li>
<p>Too generic: 4 authors reported that the feedback they received was not specific enough to their paper.</p>
</li>
<li>
<p>Insufficient LLM capabilities: 4 authors complained that the LLM could not handle content over the (LLM assistant's) page limit or that it was not multimodal and hence ignored figures.</p>
</li>
<li>
<p>Feedback inconsistent across submissions: 3 authors reported that the LLM feedback changed across multiple submissions to the server even though the paper and checklist content did not change.</p>
</li>
<li>
<p>Desire for full paper review: 3 authors reported that they would like feedback on the entire paper, not just on checklist items.</p>
</li>
<li>
<p>Bad at theory (mathematical) papers: 2 authors wrote that the LLM seemed bad at theory (mathematical) papers.</p>
</li>
<li>
<p>Too verbose: 2 authors wrote that the LLM's feedback was too wordy.</p>
</li>
</ol>
<p>Changes to Submissions in Response to Feedback</p>
<p>In the following analysis, we integrate an assessment of the LLM's feedback with the authors' checklist answers, to better understand whether the Checklist Assistant helped authors make concrete and meaningful changes to their papers.In Section 4.2.1, we analyze the types of feedback given by the LLM to authors.In Section 4.2.2, we overview the changes to their papers that authors self-reported making in survey responses.Lastly, in Section 4.2.3, we analyze changes made in multiple submissions of the same paper to the Author Checklist Assistant.</p>
<p>Characterization of LLM Feedback by Question</p>
<p>For authors to make meaningful changes to their papers, the Author Checklist Assistant must provide concrete feedback.In this section, we analyze the type of feedback given by the Checklist Assistant to determine whether it is specific to the checklist answers or more generic.</p>
<p>Given the large volume of feedback, we employed an LLM to extract key points from the Checklist Assistant's responses for each question on the paper checklist and to cluster these points into overarching categories.Specifically, for each of the 15 questions across the 234 checklist submissions, we used GPT-4 to identify the main points of feedback provided to authors.We manually inspected that the main points extracted by GPT-4 matched the long-form feedback on 10 randomly selected submitted paper checklists and found that GPT-4 was highly accurate in extracting these key feedback points.We then passed the names and descriptions of these feedback points to GPT-4 to hierarchically cluster them into broader themes.</p>
<p>Figure 9: Most frequent categories of feedback given by the Author Checklist Assistant on four questions of the checklist.The types of feedback below each question were identified by using another LLM to summarize the main points in the feedback given by the checklist verifier on submitted papers.Frequency of each category of feedback is shown in parentheses (out of n=234 checklist submissions).Feedback categories for other checklist questions are provided in Appendix C.</p>
<p>The most frequently identified feedback themes for 4 questions are shown in Figure 9.Here are our key observations from this analysis.</p>
<p>1.The LLM identified many granular types of feedback within each checklist question.We illustrate with examples of responses to four questions in Figure 9.For instance, the LLM gave granular feedback within the Experimental settings/details question on optimizer configuration details, implementation code availability, and explicit mention of non-traditional experiments.</p>
<ol>
<li>
<p>The LLM tended to provide 4-6 distinct points of feedback per question (for each of the 15 questions).</p>
</li>
<li>
<p>The LLM is capable of giving concrete and specific feedback for many questions.For example, on the "Claims" question, the LLM commented on consistency and precision in documenting claims on 50 papers, including feedback like matching the abstract and introduction and referencing appendices.On the "Compute resources" question the LLM commented specifically on detailing compute / execution time of methods.</p>
</li>
<li>
<p>The LLM tends to provide some generic boilerplate for each question.The most common category of feedback for each question is a generic commentary on enhancing general aspects of the question.</p>
</li>
<li>
<p>There are certain topics that appear across many questions, in particular discussion of limitations and improved documentation.</p>
</li>
</ol>
<p>6.The LLM often expands the scope of checklist questions.For example, the LLM brings up reproducibility as a concern in feedback to the NeurIPS code of ethics question and brings up anonymity quite frequently in the code and data accessibility question.</p>
<p>We provide a full list of the summarized main themes of feedback in Appendix C. In summary, our analysis of the feedback given by the LLM suggests that the LLM gave concrete and actionable feedback to authors that they could potentially use to modify their paper submissions.Our analysis also suggests that a more detailed checklist could be developed to provide more granular feedback, based on the rubrics covered by the Author Checklist Assistant.Such a detailed checklist could be processed automatically by an LLM to systematically identify specific, commonly overlooked issues in scientific papers and flag concrete issues for authors to resolve.</p>
<p>Authors' Descriptions of Submission Changes</p>
<p>We obtain additional evidence of changes made by authors in response to the Checklist Assistant through the post-usage survey.In the survey, we asked authors to detail in freeform feedback any changes they had made or planned to make in responses to feedback from the LLM.Of the 78 survey responses, 45 provided feedback to this question.Of these 45 responses, 35 actually described changes they would make (the remainder used this freeform feedback to describe issues that they had in using the assistant).Based on manual coding of the comments, we identified the main themes in changes they planned to make:</p>
<ol>
<li>
<p>14 authors said that they would improve justifications for their checklist answers by including more detail and/or references to paper sections.</p>
</li>
<li>
<p>6 authors said that they would add more details about experiments, datasets, or compute.</p>
</li>
<li>
<p>2 authors said they would change an answer to the checklist that they filled out incorrectly.</p>
</li>
<li>
<p>2 or fewer authors mentioned improving the intro/abstract, discussion of limitations, and discussion of standard errors.</p>
</li>
</ol>
<p>Overall, these responses indicate that some authors were motivated to modify their submissions due to feedback from the checklist verification.</p>
<p>Analysis of Re-submissions</p>
<p>Finally, we analyze changes made between submissions to the Checklist Assistant when authors submitted multiple times.There were 40 instances where an author submitted the same paper to the checklist verification multiple times (out of 184 total distinct paper submissions to the checklist verification).In this analysis, we assess changes made to the paper checklist between the first and second submission to our checklist verifier in order to understand whether authors made substantive changes to their checklists and/or paper manuscripts in response to feedback from the checklist verification.</p>
<p>We find that of the 40 pairs of papers, in 22 instances authors changed at least one answer in their checklist (e.g., 'NA' to 'Yes') between the first and second submission and in 39 instances they changed at least one justification for a checklist answer (with the remaining paper being an exact re-submission with no changes).Of the 22 papers where authors changed an answer, on one paper the author changed the answer to all questions from "TODO" to an actual answer, while on the other papers authors changed 1 to 3 answers with most changing only one answer.We exclude the paper where the initial checklist was entirely "TODO" from the rest of this analysis.The most common changes were to the Documentation question (5 authors changed from 'NA' to 'Yes'), followed by 3 authors who changed Impacts from 'NA' to 'Yes' and 3 authors who changed Error bars from 'No' to 'Yes.'</p>
<p>Of the authors who changed justifications on their paper checklist, many authors made a large number of changes, with 35/39 changing more than 6 justifications of the 15 questions on the checklist.In Figure 10, we show (multiplicative) increase in word count between initial submission and final submission on questions where authors changed justifications (a value of 2 corresponds to a doubling of the length of an answer).We find that over half the time when authors changed a checklist answer, they more than doubled the length of their justification.In Figure 11, we evaluate whether the LLM Checklist Assistant evaluation responds to changes to paper checklist answers between re-submissions.We find that on questions where the authors changed neither answer nor justification, the LLM evaluation of the checklist answer remained unchanged 81% of the time, improved 7% of the time and was worse 12% of the time.We note that the change could be due to changes in the paper manuscript or due to inconsistency in LLM evaluation or due to randomness of the LLM.By comparison, when the author changed a justification to an answer on the checklist, the LLM evaluation improved 13% of the time and was was worse 5% of the time.When an author changed an answer on the checklist, the LLM improved its evaluation 15% of the time and was worse 2% of the time.Due to small sample size, the differences in LLM evaluations between the baseline of no changes made to checklist answers and cases where changes are high variance, but provide some evidence that the LLM evaluation is responsive to changes by authors.</p>
<p>Limitations</p>
<p>Despite significant advances, LLMs still face several limitations when used in verifying scientific submissions.We first investigate one key potential limitation that gains importance in settings where LLMs may be used instead of human reviewers -that of adversarial attacks.We then discuss various other limitations and how we mitigated some of them in this work.</p>
<p>Gaming the review system</p>
<p>The intended use of our Checklist Assistant was to help authors improve their papers, not to serve as a tool for reviewers to verify the accuracy of authors' responses.To illustrate a potential risk of using the Checklist Assistant beyond its intended purpose, we address a concern that arises if the assistant were used as an automated verification step as part of a review process: could authors "game" the system by automatically improving their checklist responses with the help of AI, without making actual changes to their paper?If such gaming were possible, authors could provide a false impression of compliance to a conference without (much) additional effort and without actually improving their papers as an Author Checklist aims to incentivize.Such a question about gaming the system is motivated by various issues of adversarial behavior in peer review [Lit21, JZL + 20, RSJ + 24] as well as feasibility of adversarial attacks on other parts of the review process [MSLL17, TJ19, EQM + 23, HRS24] (see [Sha22, Section 4] for a detailed survey).</p>
<p>To assess whether our system is vulnerable to such gaming, we employed another LLM as an attack agent to iteratively manipulate the checklist justifications, aiming to deceive the Checklist Assistant.In this iterative process, the attack agent receives feedback from the system after each round and uses it to refine its justifications.This feedback loop continues until the agent has optimized its responses over successive iterations.Specifically, we provided GPT-4o with the initial checklist responses and instructed it to revise the justifications based solely on feedback, without altering the underlying content of the paper.The full adversarial prompt used is detailed in Appendix D. Our deployed Checklist Assistant allows up to three submissions per user, with scores of 0 and 0.5 merged, as described in Section 3.2 and illustrated in Figure 1.To simulate this environment, we similarly allowed the attack agent three attempts to revise justifications, treating this as the attack budget.At the end of three iterations, the agent selected the highest scored response for each checklist question.</p>
<p>To quantify statistically how successful the adversarial attack is, we then submitted the selected justification multiple times to our Checklist Assistant for evaluation.We calculate the mean and variability of these multiple repeats.Given that our outcomes are binary (scores of 0 and 0.5 are merged), we assessed variability using the Clopper-Pearson 95% confidence interval.Figure 12 presents the performance of the attack agent as it refines the justifications over three attack rounds.The blue bars show the average score and variability across multiple evaluations of the revised justifications, while the red bars represent the average and variability for the original, unmodified justifications submitted by authors.Each bar and the corresponding confidence intervals are based on evaluations of 234 papers, repeated three times, yielding 234 × 3 data points per bar.Our results indicate a clear improvement in scores following the adversarial attack: 14 out of 15 questions show score increases when comparing the unchanged justifications with those refined through three attack rounds.</p>
<p>We also conducted a manual examination of the changes made by the LLM to the justifications.We found that the LLM employed several illegitimate strategies, such as adding a hardware description that was not present in the paper or original justifications and generating a placeholder URL as a purported repository for the code.These illegitimate justifications were given a score of '1' by the Checklist Assistant.</p>
<p>While we recognize the potential value of using large language models (LLMs) to clarify answers to checklist questions, our experiment reveals a potential risk: automated assistants, if used as review tools without human oversight in peer review settings, could be manipulated to raise scores based on persuasive rebuttals.Such improvements would add no real value for readers, as they would not reflect substantive changes in the paper itself.</p>
<p>Other Limitations</p>
<p>LLMs are known to have various other limitations in addition to non-robustness to gaming.LLMs may misinterpret complex content or fail to fully assess checklist criteria, leading to verification errors.There is also a risk that authors might rely too much on the LLM analysis, potentially missing deeper issues that require human judgment.Throughout our experiment, we encouraged authors to manually validate the outputs of LLMs, and to use the assistant to supplement their judgment, not replace it.Known issues include false positives (e.g., LLM may ask for extra justifications or changes that are excessive or irrelevant) as well as false negatives (e.g., the LLM overlooks errors in proofs).Furthermore, the Checklist Assistant that we implemented was based only on the text of the paper and did not check figures, tables, and external links (such as Github repositories), hence it may have complained about missing information that are found in those assets.In general, inaccuracy was the most cited issue with our Checklist Assistant by paper authors in the study.Nonetheless, the majority of authors surveyed reported finding the tool useful, suggesting that our tool was still accurate enough, as judged by authors, to provide utility.</p>
<p>Additionally, using LLMs raises concerns about data privacy and security, as papers are processed through an LLM service API.We provided recommendations to participants on anonymizing their submissions and made best efforts to keep all papers in confidence.However, the actual scientific content of papers was provided to a third party API, which may have discouraged usage among highly privacy sensitive paper authors.</p>
<p>Furthermore, biases in LLMs could compromise the fairness and impartiality of the scientific review process, especially for unconventional or interdisciplinary research.We address this risk by only providing the Checklist Assistant to authors: the outputs of the LLM were not visible to, or used by reviewers, meta reviewers, or program chairs in their review of the submission.We note that further use of such a Checklist Assistant by reviewers could introduce bias or otherwise compromise the review process.</p>
<p>Conclusion and future work</p>
<p>The deployment of an LLM-based paper Checklist Assistant at NeurIPS 2024 demonstrated that LLMs hold potential in enhancing the quality of scientific submissions by assisting authors in validating whether their papers meet submission standards.However, there are still notable limitations in deploying LLMs within the scientific peer review process that need to be addressed.</p>
<p>One insight from our study is the existence of a gap between authors' expectations and their actual experience with the LLM assistant.Before using the assistant, authors had overly optimistic expectations about its usefulness.After interaction, although over 70% still found it useful and were willing to revise their papers based on the feedback, there was a noticeable decline in their enthusiasm.This suggests that while LLMs can be valuable, their current capabilities may not fully meet the high expectations set by users, highlighting the need for better calibration of user expectations and improvement of the LLM's performance.</p>
<p>The assistant provided granular and specific feedback, typically offering 4-6 distinct points per checklist question.This level of detail indicates that LLMs can effectively identify areas for improvement in scientific manuscripts.However, the frequent reports of inaccuracies and excessive strictness-cited by 20 and 14 out of 52 respondents, respectively-underscore inherent limitations in the LLM's ability to interpret nuanced scientific content and contextual guidelines accurately.These issues can lead to frustration and may deter users from relying on such tools in the future.</p>
<p>Our analysis also revealed that authors who used the assistant were more likely to make substantive revisions to their submissions.Survey responses and re-submission analyses indicated that authors often expanded their checklist justifications and added more detailed explanations in response to the LLM's feedback.This behavior suggests that LLMs can positively influence the quality of submissions by prompting authors to reflect more deeply on their work.However, the inability to establish a definitive causal link between the assistant's feedback and improvements in the submissions points to the need for more controlled studies to assess the true impact of LLM interventions.</p>
<p>Future work should focus on enhancing the LLM's ability to interpret scientific texts accurately.This could involve training models on domain-specific datasets or incorporating mechanisms to handle complex scientific concepts better.In addition, a deployed Checklist Assistant could be improved by enhancing LLMs to handle multimodal content like figures and tables, measuring and mitigating biases to ensure fair feedback, and integrating LLM assistants directly into submission platforms for a seamless author experience.On the user experience side, it would be useful to better manage user expectations by clearly communicating the assistant's capabilities and limitations.Finally, conducting randomized controlled experiments could provide definitive evidence of LLMs' impact on submission quality.</p>
<p>Expanding the role of LLMs to assist in other stages of peer review is appealing, but the risks of authors exploiting the system must be carefully considered and proactively mitigated.Relying on LLM assistants without human oversight poses risks, as skilled authors could exploit the system, gaming the assistant to improve scores through persuasive rebuttals without meaningful content changes.Such vulnerabilities underscore the importance of safeguarding automated review tools to ensure they enhance, rather than undermine, the integrity of scientific peer review.this effort with an INRIA Google Research grant.We acknowledge the support of ChaLearn, the ANR Chair of Artificial Intelligence HUMANIA ANR-19-CHIA-0022, NSF 1942124, and 2200410.</p>
<p>Importantly, we are thankful to all the participants of the Checklist Assistant for volunteering to try it out and providing their valuable feedback.Guidelines:</p>
<p>• The answer NA means that the abstract and introduction do not include the claims made in the paper.</p>
<p>• The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations.A No or NA answer to this question will not be perceived well by the reviewers.</p>
<p>• The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.</p>
<p>• It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.</p>
<p>Limitations</p>
<p>Question: Does the paper discuss the limitations of the work performed by the authors?</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.</p>
<p>• The authors are encouraged to create a separate "Limitations" section in their paper.</p>
<p>• The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally).The authors should reflect on how these assumptions might be violated in practice and what the implications would be.</p>
<p>• The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs.In general, empirical results often depend on implicit assumptions, which should be articulated.</p>
<p>• The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting.Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.</p>
<p>• The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.</p>
<p>• If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.</p>
<p>• While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper.The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community.Reviewers will be specifically instructed to not penalize honesty concerning limitations.</p>
<p>Theory, Assumptions and Proofs</p>
<p>Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not include theoretical results.</p>
<p>• All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.</p>
<p>• All assumptions should be clearly stated or referenced in the statement of any theorems.</p>
<p>• The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.</p>
<p>• Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.</p>
<p>• Theorems and Lemmas that the proof relies upon should be properly referenced.</p>
<p>Experimental Result Reproducibility</p>
<p>Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>• If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.</p>
<p>• If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.</p>
<p>• Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model.In general.releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.</p>
<p>• While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution.For example an open-source dataset or instructions for how to construct the dataset).(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.</p>
<p>Open access to data and code</p>
<p>Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Guidelines:</p>
<p>• The answer NA means that paper does not include experiments requiring code.</p>
<p>• Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.• While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer.Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).</p>
<p>• The instructions should contain the exact command and environment needed to run to reproduce the results.See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.• The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.</p>
<p>• The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines.If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.</p>
<p>• At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).</p>
<p>• Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.</p>
<p>Experimental Setting/Details</p>
<p>Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>• The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.</p>
<p>• The full details can be provided either with the code, in appendix, or as supplemental material.</p>
<p>Experiment Statistical Significance</p>
<p>Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>•</p>
<p>The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.</p>
<p>• The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).</p>
<p>• The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)</p>
<p>• The assumptions made should be given (e.g., Normally distributed errors).</p>
<p>• It should be clear whether the error bar is the standard deviation or the standard error of the mean.</p>
<p>• It is OK to report 1-sigma error bars, but one should state it.The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.</p>
<p>• For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g.negative error rates).</p>
<p>• If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.</p>
<p>Experiments Compute Resources</p>
<p>Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.</p>
<p>• The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.</p>
<p>• The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).</p>
<p>Code Of Ethics</p>
<p>Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?</p>
<p>Guidelines:</p>
<p>• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.</p>
<p>• If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.</p>
<p>• The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).</p>
<p>Broader Impacts</p>
<p>Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?</p>
<p>Guidelines:</p>
<p>• The answer NA means that there is no societal impact of the work performed.</p>
<p>• If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.</p>
<p>• Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.</p>
<p>• The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments.However, if there is a direct path to any negative applications, the authors should point it out.For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation.On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.</p>
<p>• The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.</p>
<p>• If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).</p>
<p>Safeguards</p>
<p>Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper poses no such risks.</p>
<p>• Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.</p>
<p>• Datasets that have been scraped from the Internet could pose safety risks.The authors should describe how they avoided releasing unsafe images.</p>
<p>• We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.</p>
<p>Licenses for existing assets</p>
<p>Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not use existing assets.</p>
<p>• The authors should cite the original paper that produced the code package or dataset.</p>
<p>• The authors should state which version of the asset is used and, if possible, include a URL.</p>
<p>• The name of the license (e.g., CC-BY 4.0) should be included for each asset.</p>
<p>• For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.</p>
<p>• If assets are released, the license, copyright information, and terms of use in the package should be provided.For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets.</p>
<p>Their licensing guide can help determine the license of a dataset.</p>
<p>• For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.</p>
<p>• If this information is not available online, the authors are encouraged to reach out to the asset's creators.</p>
<p>New Assets</p>
<p>Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not release new assets.</p>
<p>• Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates.This includes details about training, license, limitations, etc.</p>
<p>• The paper should discuss whether and how consent was obtained from people whose asset is used.</p>
<p>• At submission time, remember to anonymize your assets (if applicable).You can either create an anonymized URL or include an anonymized zip file.</p>
<p>Crowdsourcing and Research with Human Subjects</p>
<p>Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.</p>
<p>• Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.</p>
<p>• According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.</p>
<p>Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects</p>
<p>Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.</p>
<p>• Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research.If you obtained IRB approval, you should clearly state this in the paper.</p>
<p>• We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.</p>
<p>• For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.</p>
<p>B Prompt</p>
<p>Here is the prompt used by the Checklist Assistant:</p>
<p>You are provided with a "Paper" to be submitted to the NeurIPS conference.You are assisting the authors in preparing their "Answer" to one checklist "Question".Please examine carefully the proposed author's "Answer" and the proposed author's "Justification" provided, and identify any discrepancies with the actual "Paper" content, for this specific "Question", taking into account the "Guidelines" provided to authors.</p>
<p>Afterwards, provide itemized, actionable feedback, based on the "Guidelines", aiming to improve the paper quality.Concentrate on a few of the most significant improvements that can be made, and write in terse technical English.While Authors' Proposed Answer is generally preferred to be a "Yes", it is acceptable to answer "No" or "NA" provided a proper Authors' Proposed Justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used").If the Authors' Proposed Answer is Yes, the Authors' Proposed Justification for the Answer should point to the section(s) within which related material for the question can be found.Note that the Authors' Proposed Justification is not expected to contain anything else (although it is fine if it contains more details).</p>
<p>Finally, after performing all previous steps, conclude your review with a score for this specific "Question", in a separate line (1: Everything OK or mild issues; 0.5: Needs improvements.Use this score sparingly; 0: Critical issues).Make sure that score is shown in a new line in this format "Score: score value" and there is no content after the score.</p>
<p>C Clustering of LLM Feedback</p>
<p>For each question, we provide below the most identified categories of feedback given (as identified by an LLM annotator), along with their frequency, a description of the category as provided by the LLM, and the sub-categories of feedback that were consolidated into the category.For each question, we show the most frequent type of feedback and two to three additional selected categories.</p>
<p>Claims</p>
<p>Clarification and Emphasis on Novel Contributions</p>
<p>• Frequency: 122</p>
<p>• Description: Consolidate and clarify the uniqueness of the research approach and findings throughout the paper, ensuring the novel contributions are explicitly highlighted in the abstract and introduction.</p>
<p>Consistency and Precision in Documentation</p>
<p>• Frequency: 50</p>
<p>• Description: Ensure precision and consistency in documenting methodologies, findings, and claims throughout the paper, facilitating easier understanding and replication by others.</p>
<p>Supplemental Materials and Detailed Documentation</p>
<p>• Frequency: 70</p>
<p>• Description: Feedback stressing the importance of comprehensive supplemental materials and detailed documentation for deeper insights.</p>
<p>Clarity and Highlighting of Novel Contributions</p>
<p>• Frequency: 170</p>
<p>• Description: Focus on improving the clarity of the paper's novel aspects by explicitly outlining and highlighting the unique contributions in the abstract and introduction sections.</p>
<p>Distinctiveness and Scope Reflection</p>
<p>• Frequency: 153</p>
<p>• Description: Clarify the distinctiveness of scholarly contributions and reflect on the scope and impact, offering detailed analysis and theoretical-practical implications.</p>
<p>Ethical and Legal Compliance</p>
<p>• Frequency: 20</p>
<p>• Description: Emphasizes adherence to ethical research practices and legal considerations across all aspects of research, from methodology to asset management.</p>
<p>• Sample sub-categories of feedback:</p>
<p>-</p>
<p>Community Engagement and Interaction</p>
<p>• Frequency: 23</p>
<p>• Description: Foster community involvement and facilitate the use and feedback of research by engaging with relevant stakeholders, providing code, and encouraging feedback.</p>
<p>• Sample sub-categories of feedback:</p>
<p>-Link to Anonymized Asset URLs or Supplementary Material -Supplementary Materials -Neural Architecture Details -Community Guidelines -Roadmap Clarity</p>
<p>Dataset and Asset Documentation</p>
<p>• Frequency: 22</p>
<p>• Description: Provide detailed documentation on datasets, models, and assets used, including version control, citation, and updates to enhance discoverability and utility.</p>
<p>• Sample sub-categories of feedback:</p>
<ul>
<li>• Description: Discuss the ethical, societal, and environmental considerations of the research, including human subjects and crowdsourcing ethical considerations, data ethics, accessibility, and compliance.Mention key limitations and assumptions to improve transparency and robustness, ensuring ethical transparency in all aspects of the research.</li>
</ul>
<p>• Sample sub-categories of feedback:</p>
<p>-Ethical Considerations Addition • Description: Expand on the real-world applicability of the study's findings by providing more detailed case study descriptions, linking theory to practical applications.Mention how the method impacts or contributes to real-world scenarios with examples or anticipated outcomes, thus enhancing the significance of the research contributions.</p>
<p>• Sample sub-categories of feedback:</p>
<p>-Broader Impact Discussion Expansion -Comprehensive Discussion on Societal Impact -Linking Theory to Practice
-Broader Impact Statement -Impact Statement</p>
<p>Risks</p>
<p>Comprehensive Ethical and Societal Considerations</p>
<p>• Frequency: 155</p>
<p>• Description: Address ethical, societal, and environmental impacts, enforcing adherence to guidelines and ethics in research.</p>
<p>• Sample sub-categories of feedback:</p>
<p>-Discuss Ethical Implications of Technology -Clarify Non-Human Subject Research -Broader Impact Statement -Guideline Adherence Verification</p>
<p>-Compliance and Ethical Consideration</p>
<p>Real-World Applications and Societal Impact</p>
<p>• Frequency: 55</p>
<p>• Description: Focus on the practical applicability of research findings, their societal impact, and future implications, including environmental considerations.</p>
<p>• Sample sub-categories of feedback:</p>
<h2>-Impact of AI Systems on Healthcare Decision-making</h2>
<p>E Adversarial attack with multiple rounds budgets</p>
<p>In our adversarial attack experiment, since our deployed Checklist Assistant allows up to three submissions per user as described in Section 3.2 and illustrated in Figure 1, to simulate this environment, we also allowed the attack agent three attempts to revise justifications, treating these three iterations as the attack budget.</p>
<p>Given that all prompts used by the Checklist Assistant have been released, users can theoretically reuse them and submit responses as many times as desired.To analyze performance across varying attack budgets, we additionally re-evaluated the highest-scored justifications from the first round and the first two rounds.The mean scores and confidence intervals were calculated using the same methodology described in Section 5.</p>
<p>As can be seen in Figure 13, the effectiveness of multiple attack rounds is inconsistent.This inconsistency could be due to the simplicity of our attack algorithm.For the first three questions, we observe clear score improvements with increased attack budgets.However, for the last question where no improvement was achieved even after three rounds as in Figure 12, we find that the question appears to remain stable across multiple attack attempts.Since the primary goal of this study was to demonstrate the feasibility of adversarial gaming, we leave the development of more advanced attack strategies for future work.</p>
<p>Checklist Reviews</p>
<p>Question 1: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?</p>
<p>Answer: Yes Justification: The abstract and introduction clearly state the main contributions and scope of the paper Review: The author's proposed answer to the question, "Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?" is "Yes", with the following justification: "The abstract and introduction clearly state the main contributions and scope of the paper."Upon reviewing the provided "Paper" content against the given "Guidelines", specific actionable feedback to improve the quality of the paper is as follows:</p>
<ol>
<li>
<p>Explicitly Link Claims to Sections: Although the authors affirm that the main claims are clearly stated, it is imperative to directly associate these claims with specific sections or findings in the paper.This not only solidifies the paper's narrative but also guides readers more effectively through the contents, ensuring alignment between claims and demonstrated results.</p>
</li>
<li>
<p>Clarify the Scope of Contributions: While the abstract and introduction delineate the contributions, refining the descriptions to highlight how these contributions advance the field (e.g., by addressing existing gaps or introducing novel methodologies) can enrich the paper's significance.Offer comparisons or contextualize the work within the broader research landscape to underscore its value and innovation.</p>
</li>
<li>
<p>Enhance the Abstract for Clarity and Impact: The abstract should not only mention the contributions but also briefly outline the methodology, key findings, and potential implications or applications of the research.This comprehensive approach ensures that readers grasp the importance and utility of the work at the outset.</p>
</li>
<li>
<p>Detail Limitations and Future Work: A thorough examination of the paper's limitations and potential avenues for future research can significantly strengthen its credibility.Such transparency fosters a realistic understanding of the work's scope and encourages subsequent investigations that build upon the foundation laid by this paper.</p>
</li>
<li>
<p>Incorporate Supporting Data or Results in the Introduction: To elevate the persuasive power of the introduction, consider integrating preliminary data, figures, or highlights from the results that preface the paper's achievements.This tactic not only captivates the reader's interest but also previews the evidence that substantiates the main claims.</p>
</li>
</ol>
<p>By addressing these areas, the authors can enhance the coherence, persuasiveness, and scientific contribution of their paper, ensuring a robust alignment between the stated claims and the documented evidence.</p>
<p>Question 2: Does the paper discuss the limitations of the work performed by the authors?Answer: Yes Justification: We have clearly explained the limitations of our work in Section: Discussion and conclusion Review: The answer and justification provided by the authors align with the content of the paper, specifically in Section: Discussion and conclusion.The authors stated that the limitations of their work are clearly explained in this section, complying with the Guidelines for discussing limitations.Upon examination of the paper's "Discussion and conclusion" section, it indeed covers various forms of limitations including assumptions, scope of claims, factors influencing performance, computational efficiency, and potential issues of privacy and fairness which are in alignment with the Guidelines provided.</p>
<p>However, for comprehensive feedback regarding the quality of their paper, the following improvements are recommended:</p>
<ol>
<li>
<p>Explicitly List Limitations: While the "Discussion and conclusion" section touches upon various challenges and future directions, it is recommended that the authors explicitly list the limitations in bullet points or a separate subsection for clarity.Since the guidelines emphasize transparency in reporting limitations, an easy-to-locate and concise list would improve the paper's compliance with the Guidelines.</p>
</li>
<li>
<p>Quantitative Assessment of Limitations: Where possible, the authors should attempt to quantify the limitations.For instance, mentioning how computational efficiency scales with dataset size or specificity on which types of noise or assumptions most strongly affect their results might offer a clearer understanding of the model's boundaries.</p>
</li>
<li>
<p>Impact of Limitations on Real-world Applications: The authors should discuss how identified limitations could impact practical applications of their algorithm.Relating limitations to real-world scenarios enhances the relevance of the discussion and offers insights into potential areas of improvement or caution for practitioners.</p>
</li>
<li>
<p>Comparison with State-of-the-art: The paper should include a comparative analysis of its limitations with those existing in current state-of-the-art methods.This contextualizes the limitations and can highlight areas where the proposed method either advances beyond known issues or needs improvement.</p>
</li>
</ol>
<p>5.More detailed Justification for Limitations Not Addressed: For limitations that are acknowledged but not addressed in the current work, a more detailed justification should be provided.If certain limitations are due to computational constraints, the authors might discuss what specific resources would be necessary to overcome them, or if data privacy concerns limit the approach, they could suggest potential mitigation strategies.</p>
<p>By addressing these points, the paper will not only comply fully with the Guidelines but also enrich the scientific discourse around the proposed method by providing a well-rounded view of its applicability and scope.Question 3: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: Not Applicable Justification: This does not apply to our research work because our research is not theoretical rather it deals with a meta-dataset creation and experiments executed using this meta-dataset Review: The authors provided an "N/A" answer to the question about providing full sets of assumptions and complete (and correct) proofs for each theoretical result, justifying it by stating their work is focused on meta-dataset creation and experimental setups rather than theoretical derivations.This justification is consistent with the content of the paper that primarily discusses the development, composition, and applications of the Meta-Album meta-dataset for few-shot learning and other learning paradigms.The paper does not present theoretical results or claims that would necessitate formal assumptions or proofs.It focuses on dataset contributions, experimental evaluations, and benchmarking for machine learning algorithms.Therefore, the N/A answer is appropriate and correctly justifies the nature of the work as being empirical and experimental, without theoretical claims that require the provision of assumptions or proofs.</p>
<p>Feedback for improvement based on the guidelines and paper content: 1. Clarify in the paper that no theoretical analysis or claims are made, which is why the question about assumptions and proofs is not applicable.This ensures transparency and aligns with the justification provided.</p>
<ol>
<li>
<p>Highlight any indirect theoretical implications or future theoretical work that could be explored based on the experimental results and dataset contributions.This may not directly affect the N/A answer but enriches the discussion section.</p>
</li>
<li>
<p>Include a brief discussion on the potential for theoretical work in the context of meta-learning, few-shot learning, or dataset benchmarking, derived from the empirical findings.This would show openness to future research directions that integrate both experimental and theoretical investigations.</p>
</li>
<li>
<p>Ensure that any future versions of the paper or related works that might include theoretical aspects clearly outline assumptions and provide complete proofs or references to where these can be found, adhering to the guidelines for papers with theoretical content.Question 4: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?Answer: Yes Justification: We have disclosed all the details of our experiments and provide code and instructions publicly on the Meta-Album GitHub repository (https://github.com/ihsaanullah/meta-album)for reproducibility and results verification Review: The proposed author's "Answer" and "Justification" suggest that all necessary information for reproducing the main experimental results is disclosed and made publicly accessible via their GitHub repository, which ostensibly contains the code and instructions.According to the guidelines, this should indeed include detailed descriptions of the experiments, code, architecture, datasets used, preprocessing steps, and other relevant details for reproducibility.However, the given paper extract does not directly confirm or provide specific sections where this information can be found within the paper itself, as required by the guidelines for a "Yes" answer.</p>
</li>
</ol>
<p>Actionable Feedback: 1. Specify Sections in the Paper: In your "Justification", specify the sections within the paper where detailed experimental setups, including architectures, hyperparameters, data preprocessing, and analysis, can be found.This is crucial for ensuring that readers not only rely on GitHub resources but can also find comprehensive experimental details within the paper itself.</p>
<ol>
<li>
<p>Describe Reproducibility Details in the Paper: Ensure that the paper contains detailed descriptions of the experimental setup, including but not limited to model architectures, hyperparameters, training procedures, data preprocessing steps, and any software dependencies.This would make the paper self-contained in terms of the reproducibility of results.</p>
</li>
<li>
<p>Data Accessibility and Licensing: While providing a GitHub link is helpful, the paper should briefly discuss the availability of the datasets, including any licensing issues or restrictions.This is particularly relevant if the dataset used is not open or requires specific permissions for usage.</p>
</li>
<li>
<p>Version Control and Environment Details: It would enhance reproducibility if the paper or the GitHub repository includes details about the software environment used for the experiments, such as the versions of the programming language, libraries, and frameworks.Consider using Docker containers or conda environments to encapsulate the experimental setup.</p>
</li>
<li>
<p>Benchmarking and Evaluation Protocols: Clearly articulate the benchmarking protocols used in the experiments.This includes how different models were evaluated, the metrics used for comparison, and any cross-validation procedures.Ensure this information is readily accessible either within the paper or the GitHub repository.</p>
</li>
<li>
<p>Provide Model Checkpoints: If possible, provide access to the trained model checkpoints.This allows readers to directly use the pre-trained models for further research or validation of the results presented in the paper, supplementing the reproducibility of the experiments.</p>
</li>
<li>
<p>Clarify the Role of External Resources: If the GitHub repository includes additional resources not described in the paper (e.g., tutorial notebooks, additional analysis scripts), clarify the relevance of these resources in the context of reproducing the main results.</p>
</li>
</ol>
<p>By addressing these points, the paper would not only comply with the NeurIPS reproducibility criteria but also provide the scientific community with a comprehensive resource for validating and building upon the presented work.Question 5: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: Yes Justification: We have publicly provided the code on our github repository and details about how to access the datasets on our website.Review: Feedback and Actionable Points for Improvement:</p>
<ol>
<li>
<p>Clarify Exact Location of Reproducibility Materials: While the Authors' Proposed Justification mentions that code and data access details are provided on GitHub and the website, it lacks specific references to sections in the paper that outline these aspects.Ensure that exact locations within the paper (e.g., section numbers or Appendix references) are given to direct readers efficiently to supplementary materials for reproducing main experimental results.</p>
</li>
<li>
<p>Detailed Instructions for Reproduction: In addition to providing access to code and data, ensure the paper includes or links to detailed instructions for reproducing the main results.This includes environment setup, dependency installation, step-by-step execution of scripts, and any necessary parameter specifications.If these instructions exist, explicitly reference their location in the paper.</p>
</li>
<li>
<p>Dataset Licensing and Access Clarification: The justification mentions datasets being available via a website.Ensure the paper discusses any licensing or access restrictions related to the datasets used.This is crucial for open accessibility and ethical use of data.If there are restrictions, discuss the implications for replicating the study or seeking permissions.4. Version Control and Availability Guarantee for Code and Data: It's essential to address the longevity of the code and data accessibility.This can be achieved by using DOI (Digital Object Identifier) through platforms like Zenodo or Figshare, ensuring that the resources remain available and citable.Mention any plans for maintaining and updating the code repository.</p>
</li>
<li>
<p>Demonstrate Reproducibility with Independent Validation: If possible, include a section or appendix in the paper where you detail an instance of reproduction of the results by an entity independent from the authors.This could significantly enhance the credibility of the reproducibility claims.</p>
</li>
<li>
<p>Supplement Material with Additional Reproducibility Insights: Beyond just providing access, consider including a supplementary document that discusses common issues or challenges encountered during the experiments and their solutions.This firsthand insight can be invaluable for researchers attempting to reproduce or extend the work.</p>
</li>
<li>
<p>Discuss Computational Resource Requirements: Offer a clear discussion on the computational resources required to faithfully reproduce the experiments.This should include hardware specifications and expected runtime, helping researchers to adequately plan for reproduction efforts.</p>
</li>
<li>
<p>Address Anonymity in Submission: If the paper is in a double-blind review phase, ensure that any links to code, data, or supplementary materials are provided in an anonymized form.This can involve using anonymous links or sharing resources with the conference/journal review system directly.</p>
</li>
</ol>
<p>Conclusion:</p>
<p>The response to the checklist question attempts to affirm the open accessibility and reproducibility of the experimental results but lacks specificity in its justification.Directing to a general GitHub repository and a website does not fully assure the checklist's criteria unless explicit, easy-to-follow instructions and references to these in the main paper are provided.Adaptations based on the feedback above can enhance the paper's adherence to NeurIPS guidelines on reproducibility and open-access practices, making it a stronger submission.Question 6: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?Answer: Yes Justification: We have explained the experimental settings or details in Section 3.2: Experiments.Additional details can be found in the Meta-Album GitHub repository.Review: The paper's "Question" section focuses on whether the paper specifies all necessary details for understanding the results, including training and test details like data splits, hyperparameters, type of optimizer, etc.The authors' proposed "Answer" is "Yes," and the "Justification" points to Section 3.2 (Experiments) and the Meta-Album GitHub repository for experimental settings or details.</p>
<p>Upon careful examination of the paper's content related to the "Question," several key observations are evident:</p>
<ol>
<li>
<p>Section 3.2 (Experiments) is indeed mentioned as containing the experimental settings.However, the "Paper" segment does not delve deeply into the specifics of Section 3.2, hindering our ability to precisely assess the completeness of the explanation regarding training and test details.</p>
</li>
<li>
<p>The mention of a GitHub repository for additional details is beneficial but poses a compliance issue.The NeurIPS guidelines stipulate that the paper itself should contain essential details to understand the results fully.While supplementary material is encouraged, crucial information should not be exclusively located outside the core paper.</p>
</li>
<li>
<p>Given the nature of the datasets and experiments discussed in the paper excerpts, it's clear that details such as hyperparameters, data split methodology, and optimizer choices are crucial.These details are pivotal for replicating the study and fully appreciating the reported results' validity and robustness.</p>
</li>
</ol>
<p>Actionable Feedback:</p>
<p>-Enhance Section 3.2: Expand Section 3.2 to include specific details about the hyperparameters used, the methodology for splitting the data for training and testing, and the type of optimizer(s) employed.This will ensure the paper itself contains comprehensive details for replicating the experiments.</p>
<p>-Clarify Dataset Handling: Given the paper's emphasis on the Meta-Album dataset, it's imperative to distinctly outline how the datasets were split for training and testing, especially since the dataset encompasses multiple domains which could have nuanced implications on the experimental design.</p>
<p>-Detail Hyperparameter Selection: Provide a rationale for the choice of hyperparameters and optimizers.If these were determined through a search process, briefly describe this process and the criteria used for selection.This will add depth to the readers' understanding of the experimental design's robustness.</p>
<p>-Incorporate Essential Details in the Paper: While referencing a GitHub repository is helpful for accessing code and supplementary material, ensure that all critical information for understanding and replicating the results is present within the paper.The repository can supplement but not replace the need for comprehensive experimental detail in the paper itself.</p>
<p>-Transparency on Experiment Replicability: If certain details are omitted due to space constraints, explicitly state this in the paper and offer a direct link to where readers can find this information in the supplementary materials or the GitHub repository.</p>
<p>Conclusion:</p>
<p>The paper needs to incorporate improvements to fully meet the NeurIPS checklist criteria regarding the specification of training and test details.Adhering to the actionable feedback provided will significantly enhance the paper's quality and compliance with NeurIPS guidelines.Question 7: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?Answer: Yes Justification: In figure-2, figure-3, Appendix E: Within Domain Few-shot learning, and Appendix F: Cross-Domain Few-shot learning, we have reported error bars Review: The proposed author's answer indicates that error bars have been reported in figures and appendices specified, which suggests adherence to the guidelines that require such statistical measures to be included for the experiments.However, the feedback and improvements depend on verification against the actual paper content as provided, which is crucial for assessing adherence to the guidelines fully.Based on the available text, the following feedback is provided to further align with the NeurIPS conference requirements:</p>
<ol>
<li>
<p>Clarity on Variability Factors and Error Bar Calculation: The paper should clearly explain what variability the error bars are capturing.This detail is critical to understand the reliability of the reported results.If this is currently not mentioned, it should be added to the relevant sections where error bars are reported.The authors could consider specifying whether the variability is due to initial conditions, training/test splits, or any other experimental stochasticity.</p>
</li>
<li>
<p>Method of Calculating Error Bars: It is unclear from the given text how the error bars were calculated.The paper should explicitly state whether a standard deviation, standard error of the mean, or another method was used to calculate the error bars.Moreover, whether any assumptions were made (e.g., assuming normal distribution of errors) should be clarified.If this is not included in the current version, adding a brief methodological explanation either in the figure captions where error bars are reported or in the methodology section would improve the paper.</p>
</li>
<li>
<p>Specification of Confidence Intervals: The guidelines prefer the reporting of 95% confidence intervals or explicitly stating the sigma level of reported error bars.If not already done, the paper should specify this alongside the error bars to enable readers to gauge the statistical significance and confidence in the experimental results accurately.</p>
</li>
<li>
<p>Explanation of Asymmetric Error Bars for Non-Normal Distributions: If any of the distributions of measured values are not symmetric or normal, this should be addressed.The paper should carefully report and explain any occurrences of asymmetric error bars to avoid misleading interpretations.This is particularly relevant if any data transformation or non-parametric statistics were used.</p>
</li>
<li>
<p>In-text Reference to Figures/Table with Error Bars: Ensure that there are explicit references in the text to where the error bars can be found.This aids readers in directly locating and interpreting the statistical significance of the results.If such referencing is missing or vague, it should be made explicit.</p>
</li>
<li>
<p>Discussion on the Implications of Statistical Findings: Lastly, it would enhance the paper if there was a discussion section that contextualizes the statistical significance of the findings.For instance, how do error margins affect the conclusions drawn from the experiments?This could offer insights into the robustness and generalizability of the proposed methods or models.</p>
</li>
</ol>
<p>By addressing these points, the paper can offer a more comprehensive statistical account of the experimental results, in line with the NeurIPS guidelines.This will not only improve readability and interpretability but also enhance the paper's scientific rigor.Question 8: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?Answer: Yes Justification: We have provided details about compute resources used in Section 3.2: Experiments, Appendix E: Within Domain Few-shot learning, and Appendix F: Cross-Domain Few-shot learning Review: The authors' proposed answer and justification appear seem consistent with the guidelines provided and the content outlined in the paper.The paper describes the Meta-Album dataset and its application for few-shot learning in section 3 of the paper, and details related to computational resources used for experiments can be found in Appendices E and F, as mentioned in their justification.However, to provide concrete and actionable feedback for improvement based on the guidelines provided and the detailed content presented in the paper, the following items should be addressed:</p>
<ol>
<li>
<p>Detailed Specifications of Compute Resources: While the paper mentions the use of compute resources in Section 3.2 and Appendices E and F, it does not explicitly detail the types of compute workers (CPU/GPU), the specific models or configurations used, nor cloud provider details if any.It is crucial to include these details to fully comply with the guidelines and for the reproducibility of the experiments.</p>
</li>
<li>
<p>Execution Time: The paper mentions that each experimental run took at most 24 hours on a specific GPU but does not provide a breakdown of execution time for different parts of the experiment or for different datasets within the Meta-Album.Providing such details would enhance the reproducibility of the research.</p>
</li>
</ol>
<p>Memory and Storage Requirements:</p>
<p>There is no specific mention of the memory and storage requirements necessary to reproduce the experiments.Adding this information would help in assessing the feasibility of reproduction efforts and is in line with the provided guidelines.</p>
<ol>
<li>
<p>Compute for Preliminary or Failed Experiments: The paper does not discuss whether the reported compute resources also account for preliminary or failed experiments which did not make it into the final paper.Clarifying this aspect would give a more comprehensive view of the computational cost of the project, in adherence to the guidelines.</p>
</li>
<li>
<p>Reference to Specific Sections for Compute Details: While the paper alludes to Appendices E and F for details on compute resources, ensuring that these appendices contain expressly stated headings or sections that readers can directly refer to for computational details would improve navigability and clarity.</p>
</li>
</ol>
<p>Actionable feedback for improvement:</p>
<p>-Add a detailed table or appendix explicitly stating the compute resources used, including the type of compute workers (CPU or GPU), specifications (e.g., model, RAM), and the cloud provider or internal cluster details if applicable.</p>
<p>-Provide a more detailed breakdown of execution times, possibly in a table format, for each major experimental setup or dataset within the Meta-Album.</p>
<p>-Include information on memory and storage requirements for conducting the experiments.</p>
<p>-Explicitly state whether the reported computational resources include efforts from preliminary or unsuccessful experiments and provide any relevant details.</p>
<p>-Revisit Appendices E and F to ensure they are directly and clearly addressing the guidelines' requests on reporting computational resources.Question 9: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?Answer: Yes Justification: We completely comply with the NeurIPS Code of Ethics Review: Based on the guidelines provided for answering the checklist question related to the NeurIPS Code of Ethics compliance and the content available in the proposed paper, several discrepancies and areas of improvement have been identified.The primary issue is the lack of specific references to sections in the paper that demonstrate compliance with the NeurIPS Code of Ethics.</p>
<ol>
<li>
<p>Mitigation Strategies: Once potential negative impacts are identified, include a discussion on mitigation strategies.This could range from ethical guidelines for using the dataset to technical solutions for privacy preservation or computational efficiency.</p>
</li>
<li>
<p>References to Related Work on Societal Impact: To strengthen the discussion, reference existing literature on the societal impacts of similar technologies or datasets.This shows an awareness of broader concerns in the field and situates the work within ongoing ethical considerations in machine learning.</p>
</li>
<li>
<p>Clarify the Contribution Section: Ensure that the contributions section clearly demarcates the societal impacts (both positive and negative) of the work.This will make it easier for readers to understand the broader implications of the meta-dataset beyond its technical contributions.</p>
</li>
<li>
<p>Engage with Broader Community: Encourage feedback from the broader machine learning and societal impact research communities on potential negative impacts that the authors might have overlooked.This can be achieved through workshops, preprints for community feedback, or collaboration with researchers in ethics and policy.Question 11: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?Answer: Yes Justification: Indeed, we are releasing our datasets with proper license and terms of use.We have also provided recommended usage of these datasets.</p>
</li>
</ol>
<p>Review: The answer provided by the authors indicates that they have put safeguards in place for the responsible release of data by releasing datasets with proper licensing and terms of use.However, they did not point to the specific sections in the paper where details are discussed, which is a requirement according to the guidelines for answering the checklist question.Furthermore, the paper discusses the release of a multi-domain meta-dataset designed to facilitate few-shot learning among other tasks, and details about license information are provided in Appendix B, but there is no explicit discussion on the safeguards for responsible release beyond licensing, such as control over access or usage restrictions for highly sensitive data.</p>
<p>To address these issues and improve the quality of the paper, the authors could consider the following actionable feedback:</p>
<ol>
<li>
<p>Point to Specific Sections: Amend the justification to include specific references to the sections or appendices in the paper where the responsible release of the dataset, including licensing and terms of use, is discussed.According to the provided document, Appendix B details the license information of the datasets.</p>
</li>
<li>
<p>Expand on Safeguards Beyond Licensing: Besides mentioning the license under which the datasets are released, discuss any additional safeguards put in place to prevent misuse.This could include mechanisms for controlling access, guidance on ethical use, and any measures to mitigate the risk of misuse of potentially sensitive data.</p>
</li>
<li>
<p>Discuss Ethical Considerations: Include a separate section or expand the existing discussion on ethical considerations related to the release and usage of the dataset.Address any specific concerns that might arise from the nature of the data, how these concerns are mitigated, and recommend best practices for users of the dataset to ensure ethical usage.</p>
</li>
<li>
<p>Provide Examples of Recommended Usage: While the authors mention that recommended usages of these datasets are provided, adding examples of such recommendations in the paper or pointing to where those can be found could benefit readers and potential users of the dataset.This would clarify how the authors envision the responsible employment of the data they are releasing.</p>
</li>
<li>
<p>Clarify Accessibility and Control: Elaborate on how the datasets will be accessed by potential users and whether there will be any form of control or monitoring over who uses the data and for what purposes.This is especially important for datasets that might carry risks of misuse or have privacy implications.</p>
</li>
</ol>
<p>By addressing these points, the authors would not only clarify the responsible release of their dataset but also contribute to setting a standard for the release of similar datasets in the future.Question 12: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, 2. The guidelines require that if the answer is "Yes," the justification should point to specific sections within the paper.The provided justification vaguely references "appendices" and "a website" without specifying where exactly readers can find this information within the paper itself.</p>
<p>Actionable Feedback: 1. Clearly Reference Section in Paper: Amend the justification to reference specific sections in the paper where documentation about the datasets, including their licenses, limitations, consent for asset use, preprocessing steps, and how contributor consent was obtained, is discussed.</p>
<ol>
<li>
<p>Detail Data Preprocessing and Limitations: Ensure there's a dedicated section in the paper that clearly outlines the data preprocessing steps taken, the rationale behind them, and any potential limitations or biases these steps might introduce into the datasets.This is crucial for reproducibility and understanding the dataset context.</p>
</li>
<li>
<p>License and Consent Detailing: Clearly articulate in the paper the licensing agreements for each dataset used and how consent was obtained for assets, especially where personal data is involved.This will address ethical considerations and legal compliance.</p>
</li>
<li>
<p>Dataset Availability and Accessibility: Ensure the website mentioned for accessing datasets is not only operational but also includes detailed instructions for accessing and utilizing the datasets and any related software or tools.It is also beneficial to include a backup plan or repository (e.g., Zenodo, Figshare) where these assets can be accessed should the website be unavailable.</p>
</li>
<li>
<p>Appendices Reference: If the paper's appendices contain relevant information about the datasets, ensure these appendices are adequately referenced within the main body of the paper, directing readers on where to find detailed documentation.</p>
</li>
<li>
<p>Anonymization During Review Process: Verify and ensure that any shared URLs or references to datasets and assets are appropriately anonymized if required by the submission guidelines, particularly during the review phase to comply with double-blind review requirements.Question 14: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?Answer: Not Applicable Justification: This does not apply on our research as we are not doing any crowdsourcing experiments Review: The provided "Paper" content accurately supports the author's answer that crowdsourcing experiments and research with human subjects are not applicable to their work.The paper introduces "Meta-Album," a meta-dataset for few-shot image classification, which is compiled from various open datasets for facilitating research in meta-learning and does not involve any crowdsourcing or direct research with human subjects.The justification aligns with the guidelines provided, which state that if the research does not involve crowdsourcing nor human subjects, answering "N/A" is appropriate.</p>
</li>
</ol>
<p>Feedback: 1. Clarify Data Sources and Usage: While the paper meticulously lists the datasets included in Meta-Album, offering insights into their origins, classification, processing, and licensing, it would strengthen the narrative to explicitly state that no human subjects or crowdsourcing methods were employed in the creation of these datasets.This clarification would preemptively address any concerns regarding the ethical treatment of participants or the use of crowdsourced data.</p>
<ol>
<li>
<p>Ethical Use and License Compliance: Given the diverse sources of the datasets compiled into Meta-Album, an explicit confirmation of compliance with each dataset's respective license for use, modification, and redistribution can reinforce ethical research practices.It's critical to ensure that the compiled use of these datasets does not infringe on intellectual property rights or ethical guidelines proposed by the original dataset creators.</p>
</li>
<li>
<p>Transparency in Dataset Processing: While the paper provides a comprehensive overview of dataset processing techniques for standardization and quality control, ensuring transparency in any modifications, especially those that might impact the integrity or representativeness of the original data, is critical.Clear documentation of preprocessing steps helps in reproducibility and integrity of research.</p>
</li>
<li>
<p>Addressing Potential Bias: The paper would benefit from a discussion on potential biases inherent in the datasets and the measures taken to mitigate such biases, especially considering the meta-dataset's</p>
</li>
</ol>
<p>Figure 1 :
1
Figure 1: Example of checklist questions, answers, and LLM-provided review.</p>
<p>(a) Author responses to the checklist.(b) LLM scoring of authors' responses to the checklist.</p>
<p>Figure 2 :
2
Figure 2: Summary of author checklist completion and LLM feedback.</p>
<p>Figure 3 :
3
Figure 3: Distribution of 'Needs improvement' scores given by the Checklist Assistant, per checklist.Out of 15 questions, all participants received at least 8 'Needs improvement' and at most 13.More than half of the participants received 12 or more.</p>
<p>Figure 5 :
5
Figure 5: Responses to survey questions pre-and post-usage of the Checklist Assistant, from all authors who responded to both surveys (n=63).Error bars show 95% confidence intervals for the sample proportion.The majority of surveyed authors reported a positive experience using the Checklist Assistant.</p>
<p>Figure 6 :
6
Figure 6: Mean number of 'needs improvement' scores from the LLM evaluation of checklist questions, for post-usage survey respondents answering positively or negatively to each question.Error bars show 95% confidence intervals.</p>
<p>(a) Submitters (n = 234) vs. non-submitters (n = 539) to the checklist verification.(b) Respondents (n = 78) vs. non-respondents (n = 156) to the post-usage survey.</p>
<p>Figure 7 :
7
Figure 7: Comparison in responses to the pre-usage survey for different groups.Error bars show 95% confidence intervals.</p>
<p>Figure 8 :
8
Figure 8: Summary of reported issues using checklist verification from freeform feedback on post-usage survey (n=52 out of 78 total survey responses).Numbers show the total number of authors who reported the issue.</p>
<p>Figure 10 :
10
Figure 10: Ratio between the word count of checklist responses on the first and second submission on papers where the authors changed the justifications (n = 362 responses).The plot shows the proportion of checklist responses that increased by more than a given ratio-many authors increased the length of responses between re-submissions, with more than 50% of responses increasing by a factor of two or more in length.</p>
<p>Figure 11 :
11
Figure 11: Changes in LLM evaluation of paper checklist questions between re-submissions (n=40 pairs) split by changes the author made to the checklist answer.Error bars show 95% confidence intervals obtained via bootstrapping by re-sampling the papers.</p>
<p>Figure 12 :
12
Figure 12: Evaluation of the Checklist Assistant scores on initial unmodified justifications and adversarially revised justifications for the (n = 234) submitted papers.14 out of 15 questions show score increases for the answers refined through 3 adversarial attack rounds.</p>
<p>( a )
a
If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with</p>
<p>•••-•</p>
<p>Ethical Consideration and Permissions -Proprietary or Restricted Asset Clarification -Reach Out for Unspecified Licensing -Error Bars and Computational Justification -Data Scraping and Use Compliance Reproducibility and Open Science • Frequency: 14 • Description: Highlights the importance of ensuring that research is reproducible and accessible, advocating for open science principles through proper documentation and version control.Sample sub-categories of feedback: -Transparency in Modifications and Contributions -Version and URL -Version and URL Details -Detailed Versioning for Reproducibility -URL Inclusion for Assets Documentation Comprehensive Documentation and Reproducibility • Frequency: 189 • Description: Enhance the paper's credibility and utility by providing complete documentation on the methodology, assets, experimental setups, and reproducibility information.Sample sub-categories of feedback: -Documentation Clarity -Verification of Supplementary Materials -Validation and Reproducibility Enhancements -Verification and Reproducibility Features -Validation of Assets Documentation Asset Management and Privacy Compliance • Frequency: 59 • Description: Ensure the privacy and security of data and assets used in research, adhering to compliance requirements and fostering trust through transparency.Sample sub-categories of feedback: Anonymization and Accessibility Explanation -Anonymization Improvement -Anonymization of Assets -Asset Anonymity -Asset Anonymization Transparency and Methodological Details • Frequency: 33 • Description: Increase the transparency and understanding of the paper by thoroughly explaining the methodology, including design decisions, implementations, and limitations.Sample sub-categories of feedback:</p>
<p>-</p>
<p>Ethical Considerations Discussion -Experimental Methodology Clarification -Inclusion of Participant Instructions and Compensation -Clarification and Specification Methodological and Documentation Transparency • Frequency: 81 • Description: Provide detailed methodological information including implementation details, comparison baselines, network architectures, training hyperparameters, dataset specifics, and compliance with standards to enhance reproducibility and understanding.Ensure documentation is accessible and includes visual references for key figures and tables.• Sample sub-categories of feedback: -Include Ethics and Data Source Information -Clarification of Novelty -License and Availability -Broader Impact Statement -Highlight Ethical Compliance Measures Enhancing Real-world Relevance and Theory-Practice Connection • Frequency: 18</p>
<p>•-•</p>
<p>Compliance and Future Work -Data Privacy and Security -Future Guidelines for AI Deployment in Healthcare -Impact Assessment on Human Subjects Transparency and Clarity in Research • Frequency: 49 • Description: Emphasize clear, transparent methodologies, and ethical considerations in research, highlighting novel contributions and methodological details.Sample sub-categories of feedback: Scalability and Efficiency -Correct Answer in the Checklist -Transparency in Methodology -Justification Detailing -Adherence to Guidelines and Standards Clarification and Highlighting of Novel Contributions • Frequency: 26 • Description: Clearly outline and emphasize the paper's unique contributions and novelty in comparison to existing literature.Sample sub-categories of feedback: -Clarify Scope in Abstract -Discussion on Generalization and Limitations -Clarification of Claims -Clarification and Details -Clarification of N/A Justification</p>
<p>Figure 13 :
13
Figure 13: Adversarial attack with multiple rounds budgets</p>
<p>[VEA + 07] Jan P Vandenbroucke, Erik von Elm, Douglas G Altman, Peter C Gøtzsche, Cynthia D Mulrow, Stuart J Pocock, Charles Poole, James J Schlesselman, Matthias Egger, and Strobe Initiative.Strengthening the reporting of observational studies in epidemiology (strobe): explanation and elaboration.Annals of internal medicine, 147(8):W-163, 2007.[WBM+ 23] James G Wrightson, Paul Blazey, D Moher, KM Khan, and CL Ardern.Gpt for rcts? using ai to determine adherence to reporting guidelines.medRxiv, pages 2023-12, 2023.
[WGNBK19] John Wieting, Kevin Gimpel, Graham Neubig, and Taylor Berg-Kirkpatrick. Simple andeffective paraphrastic similarity from parallel translations. In ACL, pages 4602-4608, Florence,Italy, July 2019.[XEP + 22]Zhen Xu, Sergio Escalera, Adrien Pavão, Magali Richard, Wei-Wei Tu, Quanming Yao,Huan Zhao, and Isabelle Guyon. Codabench: Flexible, easy-to-use, and reproducible meta-benchmark platform. Patterns, 3(7):100543, 2022.
AppendicesA Full NeurIPS 2024 Author Checklist1.ClaimsQuestion: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?</p>
<p>Focuses on the importance of open science practices such as sharing data and code, ensuring accessibility and licensing, and engaging with the community to extend the research's impact and application.Description: Focused on methodological detailing for reproducibility, dataset and model transparency, accessibility, ethical distribution, engagement with literature and ethical AI debates, transparency and reproducibility enhancement, and advocacy for accessibility and reproducibility.Description: Include a comprehensive discussion on the societal impact, ethical implications, and future research directions, emphasizing the significance and potential consequences of the research.
Transparency, Reproducibility, and Open Science• Frequency: 58 • Description: • Sample sub-categories of feedback: ['Licensing Information', 'Licenses for Data and Code', 'Data Accessi-bility and Licensing', 'Dataset Licenses and Accessibility', 'Clarify Data • • Sample sub-categories of feedback: ['Transparency in Artifact Sharing', 'Transparency in Methodology',Documentation, Transparency, and Clarity 'Enhancing Reproducibility', 'Enhancing Reproducibility and Ethical Openness', 'Transparency and Re-• Frequency: 105 producibility Improvement']• Description: Encourages improving documentation quality, explicitly stating the work's novelty, limita-Impactstions, and ensuring clarity in the presentation of technical details. Discussion on Ethical Considerations, Societal Impact, and Future Directions• Sample sub-categories of feedback: ['Supplement material details', 'Supplemental Material', 'License In-formation', 'Supplemental Material Checks', 'Enhance Reproducibility Instructions'] • Frequency: 226Anonymity, Ethics, and Compliance •• Frequency: 57Error barsStatistical Enhancements and Rigor• Frequency: 228• Description: Enhance the clarity and comprehensiveness of statistical methodologies, analyses, and trans-Code and data accessibility parency.• Sample sub-categories of feedback: ['Explanation of Statistical Methods', 'Confidence Intervals', 'Handling Reproducibility and Methodological Detail of Asymmetric Distributions', 'Clarity on Computational Limitations', 'Statistical Transparency']• Frequency: 221 Methodological Detailing, Clarity, and Transparency• Description: Emphasizes the necessity for providing exhaustive methodological information, including experimental settings, datasets, and code to ensure the work can be replicated and verified. • Frequency: 27• Description: Provide comprehensive methodological information, including detailing of the methodology, • Sample sub-categories of feedback: ['Supplemental Materials Handling', 'Anonymity Preservation', 'Con-firming Adherence to Submission Guidelines', 'Anonymized Code Release', 'Implementation and Environ-limitations, and assumptions, to enhance understanding and reproducibility of the research.ment Details'] • Sample sub-categories of feedback: ['Expanding Methodology Section', 'Transparency on Limitations',Open Science and Accessibility 'Methodology for Future Work', 'Methodology Explanation', 'Transparency on Limitations']
• Sample sub-categories of feedback: ['Supplemental Material for Extended Details', 'Supplementary Material', 'Data Licensing and Permissions', 'Supplementary Material', 'Limitations and Failure Analysis'] • Description: Underlines the importance of adhering to ethical standards, including anonymizing submissions for double-blind review and ensuring compliance with legal and ethical guidelines.• Sample sub-categories of feedback: ['Anonymization for Review', 'Submission Anonymity Compliance', 'Supplemental Material Anonymization', 'Anonymity in Code Release', 'Anonymity for Review'] Data Quality and Integrity • Frequency: 44 • Description: Discusses the significance of ensuring data quality, detailing how data integrity issues can affect results and conclusions drawn from the research.• Sample sub-categories of feedback: ['Data Quality Control Measures', 'Data Integrity Discussion', 'Limitations of Data Quality', 'Quality Control Procedures', 'Explain Data Quality Measures'] • Sample sub-categories of feedback: ['Related Work', 'Engagement with Existing Literature', 'Accessibility and Inclusion Considerations', 'Narrow Discussion Scope', 'Clarify Positive Impact Connection']</p>
<p>In computer science, unlike most other fields, conferences are a primary venue for publication, with the peer-review process evaluating entire manuscripts rather than just abstracts.
AcknowledgementsIn preparing this experiment, we received advice and help from many people.We are particularly grateful to the NeurIPS'24 organizers, including General Chair Amir Globerson, Program Chairs Danielle Belgrave, Cheng Zhang, Angela Fan, Jakub Tomczak, Ulrich Paquet, and workflow team member Babak Rahmani, for participating in brainstorming discussions and contributing to the design.We have also received inputs and encouragement from Andrew McCallum of OpenReview, Anurag Acharya from Google Scholar, and Tristan Neuman from the NeurIPS board.Several volunteers have contributed ideas and helped with various aspects of the preparation, including Jeremiah Liu, Lisheng Sun, Paulo Henrique Couto, Michael Brenner, Neha Nayak Kennard, and Adrien Pavao.This research project has benefitted from the Microsoft Accelerate Foundation Models Research (AFMR) grant program.We are grateful to Marc Schoenauer for supportingTransparency on Limitations and Future Work• Frequency: 212• Description: Combine discussions on transparency regarding limitations, ethical considerations, future research directions, and assumptions to enhance the paper's integrity and robustness.• Sample sub-categories of feedback: ['Future Work Directions', 'Ethical Considerations and Fairness', 'Expand on Limitations', 'Reflection on Computational Efficiency and Scalability', 'Address Privacy and Fairness Limitations']Computational Efficiency and Scalability• Frequency: 67• Description: Aggregate feedback on computational efficiency, scalability discussions, and the consideration of computational resources in method development.• Sample sub-categories of feedback: ['Computational Efficiency', 'Computational Efficiency', 'Computational Efficiency and Scalability', 'Expand on Environmental Limitations', 'Computational Efficiency and Scalability']Privacy, Fairness, and Ethical Considerations• Frequency: 44• Description: Unify feedback regarding privacy, fairness, ethical discussions, and societal impacts, including data protection in technology use to guide responsible research.• Sample sub-categories of feedback: ['Bias and Fairness Evaluation', 'Societal and Ethical Discussion', 'Privacy and Fairness', 'Privacy and Fairness Considerations', 'Broader Impacts and Ethical Considerations'] Dataset Accessibility, Quality, and Relevance• Frequency: 38• Description: Merge feedback on dataset considerations, including accessibility, relevance, and how dataset quality impacts the method's robustness and generalizability.Theoretical assumptions and proofsTheoretical and Empirical Clarification• Frequency: 161• Description: Enhance the understanding of the paper's theoretical underpinning and empirical evidence through detailed explanations and clear distinctions between both aspects.• Sample sub-categories of feedback: ['Improve Justification Clarity', 'Inclusion of Proof Sketches', 'Supporting Theoretical Foundations', 'Complete Theoretical Proofs', 'Future Theoretical Exploration Outline']Clarity and Accessibility for Comprehension• Frequency: 69• Description: Improve the paper's accessibility and comprehension by providing clear explanations, using consistent definitions, and referencing visual materials accurately.Compute resourcesComprehensive Computational Resource Disclosure and Reproducibility• Frequency: 195• Description: Enhance the clarity and comprehensiveness of computational resource disclosure, including details required for reproducing the results, and emphasize the need for transparency in computational methods.D Adversarial PromptHere is the prompt used by the attack agent:You are provided with a 'Paper' submitted to the NeurIPS conference and a specific checklist 'Question' along with the proposed 'Answer' and 'Justification' by the authors.The 'Review' from an LLM reviewer for this 'Justification' is also provided.Your task is to revise the 'Justification' to satisfy the concerns raised by the LLM reviewer.The revised justification should align with the 'Guidelines' provided to authors and address the key points mentioned in the 'Review'.The goal is to ensure the justification is clear, concise, and meets the standards expected by the reviewers.Please focus on improving the justification in a way that directly addresses the LLM reviewer's feedback, ensuring that it is technically sound and accurately reflects the content of the paper.Make sure that the revised justification points to the appropriate section(s) within the paper and adheres to the guidelines.1. Explicitly Mention Specific Sections: The authors' justification for answering "Yes" to the question "Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics?" is too general.It is recommended that the authors explicitly mention the specific sections of the paper where ethical considerations, data usage permissions, and any potential ethical implications of their research are discussed.This level of detail is necessary for adequately demonstrating compliance with the NeurIPS Code of Ethics.2. Ethical Considerations and Impact Discussion: There is no clear mention within the provided paper excerpts of a section dedicated to discussing ethical considerations and potential societal impact.The authors should include a dedicated section discussing: -How the research adheres to ethical guidelines.-The potential impact of their findings on society, including both benefits and risks.-How they address any potential negative impacts.3. Data Usage Permissions and Licenses: While the paper briefly mentions verified licenses for datasets used in the research, it is recommended to elaborate on these aspects in the paper.Specifically, the authors should: -Provide a detailed account of how data usage complies with the respective licenses and any ethical considerations taken into account while using these datasets.-Discuss the process of data acquisition, processing, and usage in compliance with ethical standards, particularly if data involves human subjects or potentially sensitive information.4. Inclusivity and Fairness: The NeurIPS Code of Ethics emphasizes the importance of inclusivity and fairness.The authors should: -Discuss any measures taken to ensure the inclusivity and fairness of their research approach and outcomes.-Examine and address potential biases in their datasets and algorithms.5. Anonymity Preservation: Ensure that any discussion around specific datasets, methodologies, or collaborations preserves the anonymity standards expected in the double-blind review process, in accordance with NeurIPS submission guidelines.6. Future Work on Ethical Implications: Briefly suggest future research directions that could further address any unresolved ethical considerations, including how the research and methodologies can be improved to align with ethical principles continually.By addressing these recommendations, the authors will significantly strengthen the ethical justification of their work, ensuring it strictly conforms to the NeurIPS Code of Ethics.More detailed and specific discussions related to ethics will not only improve the paper's quality but also enhance its contribution to the field responsibly.Question 10: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?Answer: Yes Justification: There are no negative societal impacts.Rather, this meta-dataset can foster progress in the fields of few-shot learning and meta-learning.We have added "recommended use" in Section 1.3.Review: The authors' proposed answer is affirmative, stating that the paper discusses the societal impacts of their work.However, their justification only mentions the positive impacts and specifically denies the existence of any negative impacts, which contradicts the requirement to discuss both potential positive and negative societal impacts.Actionable feedback for improvement: 1. Revisit Societal Impact Discussion: Expand Section 1.3 or a relevant section to include both potential positive and negative societal impacts explicitly.Reference this expansion in the authors' justification.Identify Potential Negative Impacts:Even if the authors believe the negative impacts are minimal or unlikely, they should still be identified and discussed.Potential areas could include misuse of the meta-dataset for biased training, privacy concerns with dataset images, or environmental considerations due to the computational resources required for processing the meta-dataset.properly credited and are the license and terms of use explicitly mentioned and properly respected?Answer: Yes Justification: We are releasing our datasets with proper license, we are also mentioning the original licenses for the original datasets in Appendix B: License information of Meta-Album datasets Review: The authors' proposed answer and justification indicate that they have credited the original creators of assets used in their work and have mentioned the licenses and terms of use for these assets in Appendix B of their paper, as per the guidelines.However, there's no explicit mention of whether all code, data, models, or other assets not created as part of the Meta-Album project have been similarly acknowledged and whether their licenses have been respected as per the requirement.Given the extensive and varied sources of data, it's critical to ensure that all utilized assets are properly credited, and their licenses are clearly stated not only to respect intellectual property rights but also to inform readers and users of the data about their rights and restrictions regarding the use of the dataset.Actionable Feedback: 1. Verification of Asset Credits: Ensure that not just the datasets but any external code, models, and other resources used in the creation, preprocessing, analysis, or presentation of the Meta-Album have been properly credited.This includes libraries, frameworks, or any third-party software.2. Explicit License Information: While Appendix B is mentioned for license information of the Meta-Album datasets, it's essential to ensure this appendix thoroughly lists all datasets used, their respective licenses, and any restrictions these licenses might impose on users of Meta-Album.These must include direct links to the licenses where available.3. Third-Party Asset Acknowledgment: For any third-party assets (e.g., pre-trained models, software libraries) used that were not explicitly created by the paper authors, include a section acknowledging these assets and their original creators.Provide a citation to the original works and mention the asset's license if applicable.This section can be included in the appendix or as part of the main text where these assets are first mentioned.4. Clear Use Conditions: Given that Meta-Album compiles numerous datasets possibly intended for academic research, the paper should explicitly state any conditions or limitations on the use of Meta-Album imposed by the original datasets' licenses.This helps future researchers understand how they can legally use Meta-Album for their work.Contact Original Asset Owners if Uncertain:In cases where the license information for an asset is not clear or unavailable online, the authors should reach out to the original creators for clarification.This effort should be documented in the paper to inform readers of the due diligence performed in respecting intellectual property rights.6. Review Licenses for Compatibility: Ensure that the licenses of aggregated datasets or assets are compatible with one another and with the intended use of Meta-Album.For instance, mixing datasets under strictly non-commercial licenses with those allowing commercial use needs careful management to prevent legal issues.By addressing these points, the paper will offer clarity on the intellectual property considerations and ensure that it respects and acknowledges the contributions of all asset creators correctly.Question 13: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?Answer: Yes Justification: We have properly documented our new datasets using meta-data files and datasheets for datasets (provided in the appendices).We have also designed a website for users to easily get all the datasets and related information Review: Based on the provided information, the key issues identified in relation to the "Question" regarding documentation and availability of new assets are as follows:1.The Justification points towards the appendices for dataset documentation and mentions a website for accessing data sets and related information.However, it leaves out details on where within the paper specific discussions about licensing, dataset limitations, preprocessing steps, consent for using assets, and how contributor consent was obtained for personal data are addressed.diverse application scope.Acknowledging and addressing biases not only elevates the research quality but also ensures the generalized applicability of Meta-Album across different domains.Conclusion: The authors' response and justification regarding crowdsourcing and human subjects research are valid based on the provided paper content.However, enriching the document with explicit clarifications and enhancements as suggested would align the research closer to best practices in ethical AI research and dataset compilation.Question 15: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?Answer: Not Applicable Justification: This does not concerns us as we are not dealing with human subjects in our research.Review: Given the provided context and guidelines, the paper's content and the author's response to the NeurIPS checklist question regarding the study participants and potential risks do seem aligned, considering the focus on image classification without involving human subjects directly.The nature of the paper, "Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification," does not inherently require the involvement of human subjects in a capacity that would usually pose ethical concerns or necessitate IRB oversight.The datasets compiled and utilized appear to be primarily sourced from publicly available or licensed image datasets, thus falling outside the scope of research requiring such review or disclosure concerning human subject participation or potential risks.However, considering best practices for thorough and responsible research, especially in dataset curation and use, here are several points for improvement:1. License and Use: Even though the question of IRB is rightfully marked as N/A, it would bolster the paper's ethical standing to ensure all datasets incorporated have their use clearly licensed for such compilation, especially when considering public release.This verification does not appear to be a gap but reinforcing this through explicit statements would be beneficial.2. Data Handling: Discussion on data handling, specifically regarding any preprocessing steps involving images of individuals (if any exist within these datasets), would solidify the paper's stance on privacy and ethical use.It's unclear if any of the "human actions" or related datasets contain identifiable features.3. Bias and Fairness: The paper could address potential biases intrinsic to the datasets it comprises, considering ethical implications.This doesn't directly relate to human subject research ethics but falls under the broader category of responsible AI research and application, which includes ethical considerations.4. Future Research Directions: Mentioning future research directions that involve any form of human interaction, even if remotely via crowdsourcing annotations or similar methods, should come with a brief note on planned ethical considerations or IRB-like oversight.5. Contribution Clarification: While not a direct requirement, clarifying contributions to the paper from multiple authors and potentially acknowledging any support or participation from individuals in dataset curation (if applicable) might indirectly relate to ethical research practices.Feedback Summary: The paper correctly identifies IRB concerns as N/A for its current scope but can strengthen its ethical reporting by addressing above points related to dataset licensing, possible biases, data handling, and future research directions involving human interaction, even if indirectly.
Specter: Document-level representation learning using citation-informed transformers. Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, Daniel S Weld, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>The Toronto Paper Matching System: An automated paperreviewer assignment system. L Charlin, R S Zemel, ICML Workshop on Peer Reviewing and Publishing Models. 2013</p>
<p>Aries: A corpus of scientific paper edits made in response to peer reviews. D' Mike, Tom Arcy, Larry Hope, Doug Birnbaum, Alexis Downey ; Mike D'arcy, Erin Ross, Bailey Bransom, Jonathan Kuehl, Tom Bragg, Doug Hope, Downey, arXiv:2401.04259ArXiv, abs/2306.125872024. 2023arXiv preprintMARG: multi-agent review generation for scientific papers</p>
<p>No more reviewer# 2: Subverting automatic {Paper-Reviewer} assignment using adversarial learning. Thorsten Eisenhofer, Erwin Quiring, Jonas Möller, Doreen Riepel, Thorsten Holz, Konrad Rieck ; Alexander Goldberg, Ivan Stelmakh, Kyunghyun Cho, Alice Oh, Alekh Agarwal, Danielle Belgrave, Nihar B Shah, arXiv:2311.09497Peer reviews of peer reviews: A randomized controlled trial and other experiments. 2023. 2023arXiv preprint32nd USENIX Security Symposium (USENIX Security 23)</p>
<p>Mitigating manipulation in peer review via randomized reviewer assignments. Jhih-Yi ; Janet) Hsieh, Aditi Raghunathan, Nihar Shah ; Steven Jecmen, Hanrui Zhang, Ryan Liu, Nihar B Shah, Vincent Conitzer, Fei Fang ; Ilia, Osama Kuznetsov, Koen Mohammed Afzal, Nils Dercksen, Alexander Dycke, Tom Goldberg, Dirk Hope, Jonathan K Hovy, Anne Kummerfeld, Kevin Lauscher, Sheng Leyton-Brown, Lu, Margot Mausam, Aurélie Mieskes, Danish Névéol, Lizhen Pruthi, Roy Qu, Noah A Schwartz, Thamar Smith, Jingyan Solorio, Xiaodan Wang, Zhu, arXiv:2405.06563Vulnerability of text-matching in ML/AI conference reviewer assignments to collusion rings. Neurips, Anna RogersNihar B. Shah, and Iryna Gurevych2024. 2020. 2024JZL + 20. KAD + 24. What can natural language processing</p>
<p>Monitoring ai-modified content at scale: A case study on the impact of ChatGPT on AI conference peer reviews. Michael L Littman ; Weixin, Zachary Liang, Yaohui Izzo, Haley Zhang, Hancheng Lepp, Xuandong Cao, Lingjiao Zhao, Haotian Chen, Sheng Ye, Zhi Liu, Daniel A Huang, James Y Mcfarland, Chris Zou, Cong Lu, Robert Tjarko Lu, Jakob Lange, Jeff Foerster, David Clune, Ha, Communications of the ACM. Tim R Davidson, Veniamin Veselovsky, Robert West, Manoel Horta Ribeiro2021. 2024. 2024. 202464The AI scientist: Towards fully automated open-ended scientific discovery. LRD + 24. The AI review lottery: Widespread AI-assisted peer reviews boost paper scores and acceptance rates</p>
<p>Can large language models provide useful feedback on research papers? a large-scale empirical analysis. Ryan Liu, Nihar B Shah ; Weixin, Yuhui Liang, Hancheng Zhang, Binglu Cao, Daisy Wang, Xinyu Ding, Kailas Yang, Siyu Vodrahalli, Daniel He, Yian Smith, Yin, arXiv:2310.01783arXiv:2404.01268Annals of internal medicine. 15142023. 2023. 2024. 2009arXiv preprintPreferred reporting items for systematic reviews and meta-analyses: the prisma statement</p>
<p>The consort statement: revised recommendations for improving the quality of reports of parallel-group randomised trials. The lancet. David Moher, Kenneth F Schulz, Douglas G Altman, 2001357</p>
<p>PDF mirage: Content masking attack against Information-Based online services. Ian Markwood, Dakun Shen, Yao Liu, Zhuo Lu, 26th USENIX Security Symposium (USENIX Security 17). Vancouver, BCUSENIX AssociationAugust 2017</p>
<p>I will have order! optimizing orders for fair reviewer assignment. Justin Payan, Yair Zick ; Charvi, Xiangchen Rastogi, Zhijing Song, Ivan Jin, Hal Stelmakh, Iii Daumé, Kun Zhang, Nihar B Shah, arXiv:2403.01015A randomized controlled trial on anonymizing reviewers to each other in peer review discussions. 2022. 2024arXiv preprintProceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems</p>
<p>An overview of challenges, experiments, and computational solutions in peer review. B Nihar, Shah, Communications of the ACM). June 2022</p>
<p>PeerReview4All: Fair and accurate reviewer assignment in peer review. Ivan Stelmakh, Nihar Shah, Aarti Singh, Journal of Machine Learning Research. 2021</p>
<p>PDFPhantom: Exploiting pdf attacks against academic conferences' paper submission process with counterattack. Dat Tran, Chetan Jaiswal, Keith Tyser, Jason Lee, Avi Shporer, Madeleine Udell, 2019 IEEE 10th Annual Ubiquitous Computing, Electronics &amp; Mobile Communication Conference (UEMCON). 2019. 2024Dov Te'eni, and Iddo Drori. Openreviewer: Mitigating challenges in LLM reviewing. submitted to ICLR 2024 on OpenReview.net</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8190 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8190</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8190</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-150.html">extraction-schema-150</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <p><strong>Paper ID:</strong> paper-277313420</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.19271v1.pdf" target="_blank">MARS: Memory-Enhanced Agents with Reflective Self-improvement</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have made significant advances in the field of natural language processing, but they still face challenges such as continuous decision-making, lack of long-term memory, and limited context windows in dynamic environments. To address these issues, this paper proposes an innovative framework Memory-Enhanced Agents with Reflective Self-improvement. The MARS framework comprises three agents: the User, the Assistant, and the Checker. By integrating iterative feedback, reflective mechanisms, and a memory optimization mechanism based on the Ebbinghaus forgetting curve, it significantly enhances the agents capabilities in handling multi-tasking and long-span information.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8190.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8190.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MARS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memory-Enhanced Agents with Reflective Self-improvement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent framework (User, Assistant, Checker) that augments language-model agents with iterative feedback, a reflection mechanism, and a dual STM/LTM memory system managed by a MemorySyntax forgetting/retention policy to improve decision-making without additional training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MARS assistant</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-driven assistant agent that iteratively updates its policy using checker feedback and stores self-reflections and trajectory history into short-term and long-term memory; uses MemorySyntax to prioritize/decay memory entries.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various (GPT-3.5, GPT-4, Llama2-7B, CodeLlama-7B, Qwen-1.8B, ChatGLM2-6B, Mistral-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multiple commercial and open-source LLMs evaluated as the assistant within MARS; sizes range from ~1.8B to GPT-family models; the framework is model-agnostic and applied without additional training.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AgentBench (multi-task: CODE/DB/KG/ALFWorld/WebShop/Mind2Web) and long-context tasks (LCC, RepoBench-P, HotpotQA, TriviaQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A suite of multi-turn, open-ended generation tasks covering database/sql operations, knowledge graph tasks, web interactions, ALFWorld embodied tasks, and long-context code/reasoning QA requiring long-span memory and multi-step decision making.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-task agent evaluation (question answering, code completion, web navigation, sequential task execution, multi-hop reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>dual-memory: Short-Term Memory (STM) + Long-Term Memory (LTM) with MemorySyntax (Ebbinghaus-based forgetting/retention)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Explicit STM buffer storing recent trajectory (observations and actions) and an LTM storing derived self-reflections; MemorySyntax assigns a strength score to items and applies exponential decay (Ebbinghaus curve) plus linguistic optimization to decide retention, transfer, or discard using thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Recent observations O, actions a, trajectories T, and synthesized self-reflections r_t (verbal summaries/insights), stored as natural-language entries in STM/LTM.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Prompt concatenation / conditioning on current state using retained STM entries and selected LTM reflections; retention prioritized by strength/recency thresholds (MemorySyntax); memory entries are incorporated into the assistant's policy/reflection function.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>AgentBench: up to 2.26x improvement on Database tasks for strong LLMs; open-source models improved 57.7%–100%. HotpotQA (Long-form QA): GPT-3.5 baseline 54.1% -> MARS 74.9% (+20.8% absolute); GPT-4 baseline 61.2% -> MARS 78.4% (+17.2%). RAG (HotpotQA) ChatGPT-4 - MARS: 74.8% (+4.7 over best RAG baseline). Code/Repo tasks: modest gains in LCC (F1 ~79.29). Reasoning (HotpotQA/TriviaQA) MARS F1s 22.06 / 22.76 respectively (substantially > Reflexion/Beam).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baseline (no MARS memory/reflection): AgentBench and long-context baselines reported (examples): GPT-3.5 HotpotQA 54.1%; GPT-4 HotpotQA 61.2%. RAG baselines: FiD 70.1% (HotpotQA), DPR 66.3%, BM25 60.8%. (See paper tables for task-specific baselines.)</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Ablations (Table 5) show removing memory optimization drastically reduces performance on small models; comparisons against Reflexion and Beam Search show MARS much stronger on multi-document reasoning (HotpotQA/TriviaQA). RAG comparisons (Table 4) show ChatGPT-4 with MARS outperforms standard RAG pipelines by 3.6–4.7 percentage points.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Incorporating explicit STM/LTM with reflective self-generated summaries and a human-inspired retention model (MemorySyntax) significantly improves multi-turn, long-context, and multi-document tasks; effects are strongest for smaller/open-source models and in complex reasoning tasks where reflection and selective retention are critical.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Iterative feedback and memory management increase compute/latency and complexity; performance depends on checker quality (incorrect feedback can harm learning); MemorySyntax thresholds require tuning; generalization beyond tested benchmarks not proven; sparse reward signals may limit granularity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MARS: Memory-Enhanced Agents with Reflective Self-improvement', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8190.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8190.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemorySyntax</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemorySyntax (Ebbinghaus-based memory optimization with linguistic strengthening)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory-management method that combines an exponential forgetting curve (Ebbinghaus) with linguistic optimization to increase retention strength of important information and decide retention, transfer to LTM, or discard via thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemorySyntax (component used by MARS assistant)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A module that processes incoming information, applies linguistic optimization to increase 'strength', and manages STM/LTM entries by computing R(I_t, tau)=exp(-tau/S) and comparing to thresholds θ1, θ2 to retain in STM, move to LTM, or discard.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applied across evaluated LLM agents (framework component, model-agnostic)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a standalone LM; a policy/heuristic layer guiding which natural-language memory entries are kept and how strongly they decay over time.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AgentBench tasks; long-context reasoning and code tasks (LCC, RepoBench-P, HotpotQA, TriviaQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used to decide what task-relevant information to keep for future reasoning and decision-making across multi-turn tasks requiring long-span memory.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>memory management for multi-turn reasoning and long-context tasks</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>memory prioritization / retention policy for STM/LTM</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Assigns an importance/strength S (possibly boosted via linguistic optimization) then applies exponential decay R = exp(-tau/S); compares R to thresholds θ1 and θ2 to decide STM retention, LTM transfer, or deletion.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Optimized natural-language information items I*_t with associated strength scores S* stored in STM or LTM.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Retention-based selection: items above thresholds are used (via prompt concatenation / conditioning) in subsequent iterations; low-strength items are discarded.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Implicit in MARS gains; paper's ablation (Table 5) attributes large performance drops when memory optimization is removed (e.g., Qwen-1.8B KG 6.8 -> 48.0 with optimization; CodeLlama DB 2.7 -> 41.3 with optimization).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Not reported as numeric per-component, but ablation shows severe degradation when memory optimization disabled (see Table 5 examples).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Ablation on 'memory optimization' (which includes MemorySyntax) shows catastrophic drops for small models (Qwen-1.8B KG 6.8 -> 48.0 after optimization; ALF 0.0 -> 10.5) and big gains for CodeLlama-7B (DB 2.7 -> 41.3, WS 14.3 -> 58.7).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A principled forgetting/retention policy that raises important items' strength via linguistic optimization is crucial for maintaining relevant context and enabling small models to perform complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Choice of thresholds θ1/θ2 and how to compute/boost strength S* are sensitive hyperparameters; poor tuning can negatively affect retention and performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MARS: Memory-Enhanced Agents with Reflective Self-improvement', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8190.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8190.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflective Self-Improvement / Reflection Mechanism (MARS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mechanism where the assistant synthesizes self-reflections r_t from its outputs and sparse rewards over trajectories, stores those reflections in LTM, and uses them to guide future decisions and reduce repeated logical mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MARS reflection mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>The assistant computes r_t = ref(o_{1:t}, R_{1:t}), verbal summaries/insights about past performance, adds them to LTM, and uses those reflections to shape policy updates and reduce recurring errors.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applied with multiple LLMs inside MARS (GPT-3.5, GPT-4, Llama2, CodeLlama, Qwen, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reflection is a model-agnostic component producing natural-language summaries of past trials stored for later conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotpotQA example and general multi-turn tasks (AgentBench)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used to improve multi-hop reasoning and sequential task completion by converting past trials into reusable guidance stored in long-term memory.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-hop reasoning, iterative refinement, and error-correction across tasks</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>LTM of self-reflections (episodic/semantic hybrid in natural-language)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Generates reflections from past outputs and rewards, appends r_t to LTM; reflections are later fetched/concatenated into prompts to influence future outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Verbal self-reflections r_t (rich natural-language summaries about strategy, mistakes, and corrective plans).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Prompt concatenation/conditioning: relevant reflections are included as context based on task and iteration; selection guided by retention/strength scores.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported qualitative and quantitative improvements: e.g., reductions in CLE and invalid-format errors in OS/DB tasks and improved reasoning F1 on HotpotQA (MARS 22.06 vs Reflexion 11.26 and Beam 10.26).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baseline without reflection exhibited more logical mistakes and lower multi-document reasoning performance (see comparisons to Reflexion/Beam and ablations removing memory optimization).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Error analysis and ablations indicate reflection helps eliminate many basic errors (CLE, invalid format) and learn from past failures; direct ablation of reflection alone not numerically isolated but included in overall memory-optimization ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Self-generated reflections provide richer training signals than sparse scalar rewards alone and materially reduce repeated logical/action errors when stored and reused.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Effectiveness depends on quality/accuracy of generated reflections; storing too many or low-quality reflections may incur cost and degrade performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MARS: Memory-Enhanced Agents with Reflective Self-improvement', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8190.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8190.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG+MARS (ChatGPT-4 - MARS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation agents augmented with MARS (evaluated ChatGPT-4 - MARS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of MARS concepts to retrieval-augmented pipelines where ChatGPT-4, combined with MARS memory/reflective mechanisms, outperforms standard RAG approaches on multi-document QA with lower latency and memory footprint.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ChatGPT-4 - MARS (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A RAG-style agent using ChatGPT-4 as the generator, augmented by MARS's memory/reflection and MemorySyntax, improving retrieval-context integration and post-retrieval reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4 used as generative component in RAG experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-document QA (HotpotQA), Document Retrieval for Contextual Answering (Natural Questions), Open-Domain QA with Multiple Contexts (TriviaQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answering questions that require combining evidence from multiple retrieved documents or large context windows.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>retrieval-augmented question answering / multi-document reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented memory combined with MARS LTM/STM (hybrid external retrieval + internal memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Standard retrieval (BM25/DPR/OpenAI retrieval) provides context, while MARS memory/reflection augments the generative step and selectively retains important retrieved evidence using MemorySyntax.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Retrieved documents/paragraphs plus selected reflections and retained evidence stored in LTM.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Retrieval (BM25/DPR/OpenAI) combined with selection of important retrieved items via MemorySyntax retention and concatenation into prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>HotpotQA: ChatGPT-4 - MARS 74.8% (accuracy) [+4.7 over best RAG baseline FiD 70.1%]; Natural Questions: ChatGPT-4 - MARS 73.6% (+3.8 vs FiD 69.8%); TriviaQA: ChatGPT-4 - MARS 75.5% (+3.6 vs FiD 71.9%). Latency and memory also improved (reported latency and memory reductions ~47–50% vs some baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Representative RAG baselines: FiD 70.1% (HotpotQA); RAG (DPR) 66.3%; BM25 60.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Direct comparison across multiple RAG methods showing consistent 3.6–4.7 percentage point gains for ChatGPT-4 with MARS over standard RAG baselines (BM25, DPR, OpenAI retrieval, FiD).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Augmenting RAG pipelines with MARS memory/reflective components yields modest but consistent accuracy gains and reduces latency/memory footprint compared to conventional RAG systems.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Integration complexity (combining retrieval and internal memory), and gains are moderate compared to full MARS benefits in non-RAG tasks; relies on retrieval quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MARS: Memory-Enhanced Agents with Reflective Self-improvement', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8190.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8190.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memory optimization ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation of Memory Optimization (Qwen-1.8B & CodeLlama-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Quantitative ablation showing the critical role of memory optimization (MemorySyntax + reflection) for small/open-source models: large absolute performance gains after enabling memory optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Qwen-1.8B and CodeLlama-7B within MARS (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Two open-source LLMs evaluated with and without MARS memory-optimization to measure the effect of the memory module.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-1.8B; CodeLlama-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLMs of ~1.8B and 7B parameters respectively, used as assistant models in ablation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AgentBench sub-tasks (KG, ALF, DB, WebShop (WS), etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Various agent tasks from AgentBench covering knowledge graph (KG), ALFWorld (ALF), Database (DB), WebShop (WS) and others to probe practical agent capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>agent tasks: knowledge graph operations, embodied sequential tasks, DB operations, web navigation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Memory optimization module (MemorySyntax + reflection) applied to STM/LTM</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>When enabled, items are linguistically optimized, assigned strength, and managed via retention thresholds; when disabled, the memory optimization step is absent.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Same as MARS: observations, actions, trajectories, and reflections stored in natural-language memory buffers.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Prompt concatenation of retained memory entries guided by retention thresholds; when disabled, no optimized retention occurs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Qwen-1.8B examples: KG 6.8 -> 48.0 after optimization; ALF 0.0 -> 10.5. CodeLlama-7B examples: DB 2.7 -> 41.3; WS 14.3 -> 58.7 (numbers from Table 5 ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Qwen-1.8B baseline (no memory optimization): KG 6.8; ALF 0.0. CodeLlama-7B baseline (no memory optimization): DB 2.7; WS 14.3.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Direct ablation: enabling memory optimization yields massive gains especially for small models; CodeLlama-7B gains also large but smaller relative to low-capacity Qwen.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Memory optimization is essential for enabling small/open-source LLMs to perform complex agent tasks; without it, performance can be near-zero on some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Ablation highlights fragility of small models without memory optimization; exact contribution split between reflection vs retention policy not independently quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MARS: Memory-Enhanced Agents with Reflective Self-improvement', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8190.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8190.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion/Beam Search (baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion and Beam Search (self-refinement baseline methods)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two self-refinement baselines compared to MARS: Reflexion uses past trial experiences in verbal form as memory-like inputs, and Beam Search incorporates self-assessment through stochastic beam exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion; Beam Search</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Reflexion: a method that records past trial attempts in natural language to inform future attempts (verbal episodic memory); Beam Search: stochastic beam exploration with internal self-assessment but not explicit long-term memory.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applied to same LLMs in comparison experiments (not tied to a single model in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Baseline self-refinement strategies; used as comparative methods in long-context/reasoning evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LCC (code completion), RepoBench-P (code retrieval), HotpotQA, TriviaQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Code completion and multi-document reasoning tasks used to compare self-refinement strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>code completion (LCC), code retrieval (RepoBench-P), multi-hop reasoning (HotpotQA/TriviaQA)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Reflexion: verbal trial-history memory; Beam Search: local search/history (not explicit long-term LTM)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Reflexion stores past trial outputs as natural-language memories; Beam Search evaluates multiple candidate trajectories via beams and implicit selection, with some self-assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Reflexion: past attempts in natural language; Beam: candidate sequences and scores.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Reflexion: prompt concatenation of past trials; Beam: selection via beam scores and self-assessment heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Table 3: LCC F1 — Reflexion ~79.29, Beam Search ~79.12, MARS ~79.29 (similar on simpler code tasks). HotpotQA F1 — Reflexion 11.26, Beam 10.26, MARS 22.06 (MARS substantially better on multi-document reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Not explicitly separated; these methods are baselines in their native forms.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Direct comparison: on code completion (LCC) all methods perform similarly; on challenging reasoning (HotpotQA/TriviaQA) Reflexion and Beam Search perform poorly compared to MARS (roughly half or less of MARS F1).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Verbal trial-history (Reflexion) and beam-based self-assessment provide limited benefits for complex multi-document reasoning compared to MARS's integrated reflection + MemorySyntax approach; they are competitive on simpler code completion tasks but fail on harder reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Reflexion's trial-history can be insufficient to handle multi-document integration; Beam Search lacks persistent long-term retention and qualitative self-insight, reducing its effectiveness on complex reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MARS: Memory-Enhanced Agents with Reflective Self-improvement', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Memgpt: Towards llms as operating systems. <em>(Rating: 2)</em></li>
                <li>Agentbench: Evaluating llms as agents. <em>(Rating: 2)</em></li>
                <li>Hybrid computing using a neural network with dynamic external memory. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8190",
    "paper_id": "paper-277313420",
    "extraction_schema_id": "extraction-schema-150",
    "extracted_data": [
        {
            "name_short": "MARS",
            "name_full": "Memory-Enhanced Agents with Reflective Self-improvement",
            "brief_description": "A multi-agent framework (User, Assistant, Checker) that augments language-model agents with iterative feedback, a reflection mechanism, and a dual STM/LTM memory system managed by a MemorySyntax forgetting/retention policy to improve decision-making without additional training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MARS assistant",
            "agent_description": "An LLM-driven assistant agent that iteratively updates its policy using checker feedback and stores self-reflections and trajectory history into short-term and long-term memory; uses MemorySyntax to prioritize/decay memory entries.",
            "model_name": "various (GPT-3.5, GPT-4, Llama2-7B, CodeLlama-7B, Qwen-1.8B, ChatGLM2-6B, Mistral-7B)",
            "model_description": "Multiple commercial and open-source LLMs evaluated as the assistant within MARS; sizes range from ~1.8B to GPT-family models; the framework is model-agnostic and applied without additional training.",
            "task_name": "AgentBench (multi-task: CODE/DB/KG/ALFWorld/WebShop/Mind2Web) and long-context tasks (LCC, RepoBench-P, HotpotQA, TriviaQA)",
            "task_description": "A suite of multi-turn, open-ended generation tasks covering database/sql operations, knowledge graph tasks, web interactions, ALFWorld embodied tasks, and long-context code/reasoning QA requiring long-span memory and multi-step decision making.",
            "task_type": "multi-task agent evaluation (question answering, code completion, web navigation, sequential task execution, multi-hop reasoning)",
            "memory_used": true,
            "memory_type": "dual-memory: Short-Term Memory (STM) + Long-Term Memory (LTM) with MemorySyntax (Ebbinghaus-based forgetting/retention)",
            "memory_mechanism": "Explicit STM buffer storing recent trajectory (observations and actions) and an LTM storing derived self-reflections; MemorySyntax assigns a strength score to items and applies exponential decay (Ebbinghaus curve) plus linguistic optimization to decide retention, transfer, or discard using thresholds.",
            "memory_representation": "Recent observations O, actions a, trajectories T, and synthesized self-reflections r_t (verbal summaries/insights), stored as natural-language entries in STM/LTM.",
            "memory_retrieval_method": "Prompt concatenation / conditioning on current state using retained STM entries and selected LTM reflections; retention prioritized by strength/recency thresholds (MemorySyntax); memory entries are incorporated into the assistant's policy/reflection function.",
            "performance_with_memory": "AgentBench: up to 2.26x improvement on Database tasks for strong LLMs; open-source models improved 57.7%–100%. HotpotQA (Long-form QA): GPT-3.5 baseline 54.1% -&gt; MARS 74.9% (+20.8% absolute); GPT-4 baseline 61.2% -&gt; MARS 78.4% (+17.2%). RAG (HotpotQA) ChatGPT-4 - MARS: 74.8% (+4.7 over best RAG baseline). Code/Repo tasks: modest gains in LCC (F1 ~79.29). Reasoning (HotpotQA/TriviaQA) MARS F1s 22.06 / 22.76 respectively (substantially &gt; Reflexion/Beam).",
            "performance_without_memory": "Baseline (no MARS memory/reflection): AgentBench and long-context baselines reported (examples): GPT-3.5 HotpotQA 54.1%; GPT-4 HotpotQA 61.2%. RAG baselines: FiD 70.1% (HotpotQA), DPR 66.3%, BM25 60.8%. (See paper tables for task-specific baselines.)",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Ablations (Table 5) show removing memory optimization drastically reduces performance on small models; comparisons against Reflexion and Beam Search show MARS much stronger on multi-document reasoning (HotpotQA/TriviaQA). RAG comparisons (Table 4) show ChatGPT-4 with MARS outperforms standard RAG pipelines by 3.6–4.7 percentage points.",
            "key_findings": "Incorporating explicit STM/LTM with reflective self-generated summaries and a human-inspired retention model (MemorySyntax) significantly improves multi-turn, long-context, and multi-document tasks; effects are strongest for smaller/open-source models and in complex reasoning tasks where reflection and selective retention are critical.",
            "limitations_or_challenges": "Iterative feedback and memory management increase compute/latency and complexity; performance depends on checker quality (incorrect feedback can harm learning); MemorySyntax thresholds require tuning; generalization beyond tested benchmarks not proven; sparse reward signals may limit granularity.",
            "uuid": "e8190.0",
            "source_info": {
                "paper_title": "MARS: Memory-Enhanced Agents with Reflective Self-improvement",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "MemorySyntax",
            "name_full": "MemorySyntax (Ebbinghaus-based memory optimization with linguistic strengthening)",
            "brief_description": "A memory-management method that combines an exponential forgetting curve (Ebbinghaus) with linguistic optimization to increase retention strength of important information and decide retention, transfer to LTM, or discard via thresholds.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MemorySyntax (component used by MARS assistant)",
            "agent_description": "A module that processes incoming information, applies linguistic optimization to increase 'strength', and manages STM/LTM entries by computing R(I_t, tau)=exp(-tau/S) and comparing to thresholds θ1, θ2 to retain in STM, move to LTM, or discard.",
            "model_name": "applied across evaluated LLM agents (framework component, model-agnostic)",
            "model_description": "Not a standalone LM; a policy/heuristic layer guiding which natural-language memory entries are kept and how strongly they decay over time.",
            "task_name": "AgentBench tasks; long-context reasoning and code tasks (LCC, RepoBench-P, HotpotQA, TriviaQA)",
            "task_description": "Used to decide what task-relevant information to keep for future reasoning and decision-making across multi-turn tasks requiring long-span memory.",
            "task_type": "memory management for multi-turn reasoning and long-context tasks",
            "memory_used": true,
            "memory_type": "memory prioritization / retention policy for STM/LTM",
            "memory_mechanism": "Assigns an importance/strength S (possibly boosted via linguistic optimization) then applies exponential decay R = exp(-tau/S); compares R to thresholds θ1 and θ2 to decide STM retention, LTM transfer, or deletion.",
            "memory_representation": "Optimized natural-language information items I*_t with associated strength scores S* stored in STM or LTM.",
            "memory_retrieval_method": "Retention-based selection: items above thresholds are used (via prompt concatenation / conditioning) in subsequent iterations; low-strength items are discarded.",
            "performance_with_memory": "Implicit in MARS gains; paper's ablation (Table 5) attributes large performance drops when memory optimization is removed (e.g., Qwen-1.8B KG 6.8 -&gt; 48.0 with optimization; CodeLlama DB 2.7 -&gt; 41.3 with optimization).",
            "performance_without_memory": "Not reported as numeric per-component, but ablation shows severe degradation when memory optimization disabled (see Table 5 examples).",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Ablation on 'memory optimization' (which includes MemorySyntax) shows catastrophic drops for small models (Qwen-1.8B KG 6.8 -&gt; 48.0 after optimization; ALF 0.0 -&gt; 10.5) and big gains for CodeLlama-7B (DB 2.7 -&gt; 41.3, WS 14.3 -&gt; 58.7).",
            "key_findings": "A principled forgetting/retention policy that raises important items' strength via linguistic optimization is crucial for maintaining relevant context and enabling small models to perform complex tasks.",
            "limitations_or_challenges": "Choice of thresholds θ1/θ2 and how to compute/boost strength S* are sensitive hyperparameters; poor tuning can negatively affect retention and performance.",
            "uuid": "e8190.1",
            "source_info": {
                "paper_title": "MARS: Memory-Enhanced Agents with Reflective Self-improvement",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Reflection",
            "name_full": "Reflective Self-Improvement / Reflection Mechanism (MARS)",
            "brief_description": "A mechanism where the assistant synthesizes self-reflections r_t from its outputs and sparse rewards over trajectories, stores those reflections in LTM, and uses them to guide future decisions and reduce repeated logical mistakes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MARS reflection mechanism",
            "agent_description": "The assistant computes r_t = ref(o_{1:t}, R_{1:t}), verbal summaries/insights about past performance, adds them to LTM, and uses those reflections to shape policy updates and reduce recurring errors.",
            "model_name": "applied with multiple LLMs inside MARS (GPT-3.5, GPT-4, Llama2, CodeLlama, Qwen, etc.)",
            "model_description": "Reflection is a model-agnostic component producing natural-language summaries of past trials stored for later conditioning.",
            "task_name": "HotpotQA example and general multi-turn tasks (AgentBench)",
            "task_description": "Used to improve multi-hop reasoning and sequential task completion by converting past trials into reusable guidance stored in long-term memory.",
            "task_type": "multi-hop reasoning, iterative refinement, and error-correction across tasks",
            "memory_used": true,
            "memory_type": "LTM of self-reflections (episodic/semantic hybrid in natural-language)",
            "memory_mechanism": "Generates reflections from past outputs and rewards, appends r_t to LTM; reflections are later fetched/concatenated into prompts to influence future outputs.",
            "memory_representation": "Verbal self-reflections r_t (rich natural-language summaries about strategy, mistakes, and corrective plans).",
            "memory_retrieval_method": "Prompt concatenation/conditioning: relevant reflections are included as context based on task and iteration; selection guided by retention/strength scores.",
            "performance_with_memory": "Reported qualitative and quantitative improvements: e.g., reductions in CLE and invalid-format errors in OS/DB tasks and improved reasoning F1 on HotpotQA (MARS 22.06 vs Reflexion 11.26 and Beam 10.26).",
            "performance_without_memory": "Baseline without reflection exhibited more logical mistakes and lower multi-document reasoning performance (see comparisons to Reflexion/Beam and ablations removing memory optimization).",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Error analysis and ablations indicate reflection helps eliminate many basic errors (CLE, invalid format) and learn from past failures; direct ablation of reflection alone not numerically isolated but included in overall memory-optimization ablation.",
            "key_findings": "Self-generated reflections provide richer training signals than sparse scalar rewards alone and materially reduce repeated logical/action errors when stored and reused.",
            "limitations_or_challenges": "Effectiveness depends on quality/accuracy of generated reflections; storing too many or low-quality reflections may incur cost and degrade performance.",
            "uuid": "e8190.2",
            "source_info": {
                "paper_title": "MARS: Memory-Enhanced Agents with Reflective Self-improvement",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "RAG+MARS (ChatGPT-4 - MARS)",
            "name_full": "Retrieval-Augmented Generation agents augmented with MARS (evaluated ChatGPT-4 - MARS)",
            "brief_description": "Application of MARS concepts to retrieval-augmented pipelines where ChatGPT-4, combined with MARS memory/reflective mechanisms, outperforms standard RAG approaches on multi-document QA with lower latency and memory footprint.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ChatGPT-4 - MARS (RAG)",
            "agent_description": "A RAG-style agent using ChatGPT-4 as the generator, augmented by MARS's memory/reflection and MemorySyntax, improving retrieval-context integration and post-retrieval reasoning.",
            "model_name": "ChatGPT-4",
            "model_description": "OpenAI GPT-4 used as generative component in RAG experiments.",
            "task_name": "Multi-document QA (HotpotQA), Document Retrieval for Contextual Answering (Natural Questions), Open-Domain QA with Multiple Contexts (TriviaQA)",
            "task_description": "Answering questions that require combining evidence from multiple retrieved documents or large context windows.",
            "task_type": "retrieval-augmented question answering / multi-document reasoning",
            "memory_used": true,
            "memory_type": "retrieval-augmented memory combined with MARS LTM/STM (hybrid external retrieval + internal memory)",
            "memory_mechanism": "Standard retrieval (BM25/DPR/OpenAI retrieval) provides context, while MARS memory/reflection augments the generative step and selectively retains important retrieved evidence using MemorySyntax.",
            "memory_representation": "Retrieved documents/paragraphs plus selected reflections and retained evidence stored in LTM.",
            "memory_retrieval_method": "Retrieval (BM25/DPR/OpenAI) combined with selection of important retrieved items via MemorySyntax retention and concatenation into prompts.",
            "performance_with_memory": "HotpotQA: ChatGPT-4 - MARS 74.8% (accuracy) [+4.7 over best RAG baseline FiD 70.1%]; Natural Questions: ChatGPT-4 - MARS 73.6% (+3.8 vs FiD 69.8%); TriviaQA: ChatGPT-4 - MARS 75.5% (+3.6 vs FiD 71.9%). Latency and memory also improved (reported latency and memory reductions ~47–50% vs some baselines).",
            "performance_without_memory": "Representative RAG baselines: FiD 70.1% (HotpotQA); RAG (DPR) 66.3%; BM25 60.8%.",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Direct comparison across multiple RAG methods showing consistent 3.6–4.7 percentage point gains for ChatGPT-4 with MARS over standard RAG baselines (BM25, DPR, OpenAI retrieval, FiD).",
            "key_findings": "Augmenting RAG pipelines with MARS memory/reflective components yields modest but consistent accuracy gains and reduces latency/memory footprint compared to conventional RAG systems.",
            "limitations_or_challenges": "Integration complexity (combining retrieval and internal memory), and gains are moderate compared to full MARS benefits in non-RAG tasks; relies on retrieval quality.",
            "uuid": "e8190.3",
            "source_info": {
                "paper_title": "MARS: Memory-Enhanced Agents with Reflective Self-improvement",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Memory optimization ablation",
            "name_full": "Ablation of Memory Optimization (Qwen-1.8B & CodeLlama-7B)",
            "brief_description": "Quantitative ablation showing the critical role of memory optimization (MemorySyntax + reflection) for small/open-source models: large absolute performance gains after enabling memory optimization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Qwen-1.8B and CodeLlama-7B within MARS (ablation)",
            "agent_description": "Two open-source LLMs evaluated with and without MARS memory-optimization to measure the effect of the memory module.",
            "model_name": "Qwen-1.8B; CodeLlama-7B",
            "model_description": "Open-source LLMs of ~1.8B and 7B parameters respectively, used as assistant models in ablation experiments.",
            "task_name": "AgentBench sub-tasks (KG, ALF, DB, WebShop (WS), etc.)",
            "task_description": "Various agent tasks from AgentBench covering knowledge graph (KG), ALFWorld (ALF), Database (DB), WebShop (WS) and others to probe practical agent capabilities.",
            "task_type": "agent tasks: knowledge graph operations, embodied sequential tasks, DB operations, web navigation",
            "memory_used": true,
            "memory_type": "Memory optimization module (MemorySyntax + reflection) applied to STM/LTM",
            "memory_mechanism": "When enabled, items are linguistically optimized, assigned strength, and managed via retention thresholds; when disabled, the memory optimization step is absent.",
            "memory_representation": "Same as MARS: observations, actions, trajectories, and reflections stored in natural-language memory buffers.",
            "memory_retrieval_method": "Prompt concatenation of retained memory entries guided by retention thresholds; when disabled, no optimized retention occurs.",
            "performance_with_memory": "Qwen-1.8B examples: KG 6.8 -&gt; 48.0 after optimization; ALF 0.0 -&gt; 10.5. CodeLlama-7B examples: DB 2.7 -&gt; 41.3; WS 14.3 -&gt; 58.7 (numbers from Table 5 ablation).",
            "performance_without_memory": "Qwen-1.8B baseline (no memory optimization): KG 6.8; ALF 0.0. CodeLlama-7B baseline (no memory optimization): DB 2.7; WS 14.3.",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Direct ablation: enabling memory optimization yields massive gains especially for small models; CodeLlama-7B gains also large but smaller relative to low-capacity Qwen.",
            "key_findings": "Memory optimization is essential for enabling small/open-source LLMs to perform complex agent tasks; without it, performance can be near-zero on some tasks.",
            "limitations_or_challenges": "Ablation highlights fragility of small models without memory optimization; exact contribution split between reflection vs retention policy not independently quantified.",
            "uuid": "e8190.4",
            "source_info": {
                "paper_title": "MARS: Memory-Enhanced Agents with Reflective Self-improvement",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Reflexion/Beam Search (baselines)",
            "name_full": "Reflexion and Beam Search (self-refinement baseline methods)",
            "brief_description": "Two self-refinement baselines compared to MARS: Reflexion uses past trial experiences in verbal form as memory-like inputs, and Beam Search incorporates self-assessment through stochastic beam exploration.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Reflexion; Beam Search",
            "agent_description": "Reflexion: a method that records past trial attempts in natural language to inform future attempts (verbal episodic memory); Beam Search: stochastic beam exploration with internal self-assessment but not explicit long-term memory.",
            "model_name": "applied to same LLMs in comparison experiments (not tied to a single model in paper)",
            "model_description": "Baseline self-refinement strategies; used as comparative methods in long-context/reasoning evaluations.",
            "task_name": "LCC (code completion), RepoBench-P (code retrieval), HotpotQA, TriviaQA",
            "task_description": "Code completion and multi-document reasoning tasks used to compare self-refinement strategies.",
            "task_type": "code completion (LCC), code retrieval (RepoBench-P), multi-hop reasoning (HotpotQA/TriviaQA)",
            "memory_used": true,
            "memory_type": "Reflexion: verbal trial-history memory; Beam Search: local search/history (not explicit long-term LTM)",
            "memory_mechanism": "Reflexion stores past trial outputs as natural-language memories; Beam Search evaluates multiple candidate trajectories via beams and implicit selection, with some self-assessment.",
            "memory_representation": "Reflexion: past attempts in natural language; Beam: candidate sequences and scores.",
            "memory_retrieval_method": "Reflexion: prompt concatenation of past trials; Beam: selection via beam scores and self-assessment heuristics.",
            "performance_with_memory": "Table 3: LCC F1 — Reflexion ~79.29, Beam Search ~79.12, MARS ~79.29 (similar on simpler code tasks). HotpotQA F1 — Reflexion 11.26, Beam 10.26, MARS 22.06 (MARS substantially better on multi-document reasoning).",
            "performance_without_memory": "Not explicitly separated; these methods are baselines in their native forms.",
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Direct comparison: on code completion (LCC) all methods perform similarly; on challenging reasoning (HotpotQA/TriviaQA) Reflexion and Beam Search perform poorly compared to MARS (roughly half or less of MARS F1).",
            "key_findings": "Verbal trial-history (Reflexion) and beam-based self-assessment provide limited benefits for complex multi-document reasoning compared to MARS's integrated reflection + MemorySyntax approach; they are competitive on simpler code completion tasks but fail on harder reasoning.",
            "limitations_or_challenges": "Reflexion's trial-history can be insufficient to handle multi-document integration; Beam Search lacks persistent long-term retention and qualitative self-insight, reducing its effectiveness on complex reasoning.",
            "uuid": "e8190.5",
            "source_info": {
                "paper_title": "MARS: Memory-Enhanced Agents with Reflective Self-improvement",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Memgpt: Towards llms as operating systems.",
            "rating": 2,
            "sanitized_title": "memgpt_towards_llms_as_operating_systems"
        },
        {
            "paper_title": "Agentbench: Evaluating llms as agents.",
            "rating": 2,
            "sanitized_title": "agentbench_evaluating_llms_as_agents"
        },
        {
            "paper_title": "Hybrid computing using a neural network with dynamic external memory.",
            "rating": 2,
            "sanitized_title": "hybrid_computing_using_a_neural_network_with_dynamic_external_memory"
        }
    ],
    "cost": 0.01707725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MARS: Memory-Enhanced Agents with Reflective Self-improvement
25 Mar 2025</p>
<p>Xuechen Liang 
East China Jiaotong University
Nanchang</p>
<p>Jiangxi Province
China</p>
<p>Meiling Tao 
Xidian University
2 South Taibai RoadXi'an</p>
<p>Shaanxi Province
China</p>
<p>Yinghui Xia 
AutoAgents Co., Ltd
BeĳingChina</p>
<p>Jianhui Wang 
University of Electronic Science and Technology of China
ChengduChina</p>
<p>Kun Li 
Xiamen University
361005XiamenFujianChina</p>
<p>Yĳin Wang 
Xidian University
2 South Taibai RoadXi'an</p>
<p>Shaanxi Province
China</p>
<p>Jingsong Yang 
AutoAgents Co., Ltd
BeĳingChina</p>
<p>Tianyu Shi ty.shi@mail.utoronto.ca 
Faculty of Applied Science and Engineering
University of Toronto
TorontoCA</p>
<p>Yuantao Wang wangyuantao@emails.bjut.edu.cn 
Beĳing University of Technology
BeĳingChina</p>
<p>Miao Zhang zhangmiao@sz.tsinghua.edu.cn 
Shenzhen International Graduate School
Tsinghua University
ShenzhenChina</p>
<p>Xueqian Wang wang.xq@sz.tsinghua.edu.cn 
Shenzhen International Graduate School
Tsinghua University
ShenzhenChina</p>
<p>University of Minnesota -Twin Cities
55455MinneapolisMinnesotaUSA</p>
<p>MARS: Memory-Enhanced Agents with Reflective Self-improvement
25 Mar 202528C6E4A922D4C82FFBF09FB928785D39arXiv:2503.19271v1[cs.CL]Preprint submitted to NeurocomputingLarge Language ModelAI AgentSelf-evolutionEbbinghaus forgetting curve
Large language models (LLMs) have made significant advances in the field of natural language processing, but they still face challenges such as continuous decision-making, lack of long-term memory, and limited context windows in dynamic environments.To address these issues, this paper proposes an innovative framework-Memory-Enhanced Agents with Reflective Self-improvement.The MARS framework comprises three agents: the User, the Assistant, and the Checker.By integrating iterative feedback, reflective mechanisms, and a</p>
<p>Introduction</p>
<p>In recent years, large language models (LLMs) have made significant progress in the field of natural language processing, demonstrating powerful performance in tasks such as dialogue and text generation [2].However, these models still face several challenges: (1) Agents need to continuously make decisions in changing environments and adapt to new situations and tasks.(2) Agents lack long-term memory mechanisms, which is increasingly evident in situations requiring sustained interaction with the environment [3].The limited context window also hinders the model's ability to handle information over long time spans [4].</p>
<p>To tackle these challenges, researchers have proposed meta-learning and multi-task learning to enhance the transferability and adaptability of LLM agents.For memory limitations, prior works like MemGPT [5] use a FIFO queue to manage forgetting, while MemoryBank employs a forgetting curve based on insertion time.However, these approaches are often task-specific, lacking a general framework to systematically improve LLM agents in complex environments.Recent innovations, such as AutoGPT2 and BabyAGI 3 , leverage LLMs as core controllers, aiming to solve real-world challenges.Yet, multi-agent frameworks still face issues like communication overload, heavily relying on memory to maintain context.As interaction history grows, resource demands and latency increase, limiting efficient deployment in practical scenarios.</p>
<p>In this paper, we propose an innovative framework, Self-evolving Agents with reflective and memory-augmented abilities (MARS).By enhancing agents' self-adjustment capabilities through reflection, they can more effectively utilize historical information and make efficient decisions when faced with complex and dynamic tasks.From the perspective of selfevolution, we introduce a memory optimization mechanism based on the Ebbinghaus forgetting curv [6].This mechanism helps agents selectively retain key information, optimize information storage and transmission, reduce unnecessary cognitive load, and enhance agents' capabilities in interaction tasks with the environment.Experimental results show that our approach achieves significant improvements across various benchmarks, particularly excelling in smaller models.Specifically, on AgentBench, the performance of powerful LLMs like GPT-3.5 and GPT-4 is enhanced by up to 2.26X.For open-source models, performance improvements range from 57.7% to 100%.In tasks such as multi-source question answering and code generation, our approach achieves state-of-the-art results [7].</p>
<p>The main contributions of our work are as follows:</p>
<p>• We propose a novel framework, MARS, which introduces a reflection mechanism to enhance the self-adjustment capabilities of agents.Without any additional training, this enables agents to more effectively utilize historical information and make efficient decisions when faced with complex and dynamic tasks.</p>
<p>• We introduce a memory optimization mechanism based on the Ebbinghaus forgetting curve.This helps agents selectively retain key information, reducing the issue of information overload in multi-agent systems.</p>
<p>• MARS achieves improvements over strong baselines in multiple challenging realworld tasks and achieves state-of-the-art results on benchmarks.This framework can be extended to other LLMs, with particularly notable improvements in smaller models.</p>
<p>Related work</p>
<p>The field of artificial intelligence has seen significant advancements through developments in perceptual computing [8][9][10][11][12][13][14][15] and autonomous engineering systems [16][17][18][19][20][21][22][23][24][25][26][27][28].These advancements have led to breakthroughs in generative intelligence [29][30][31][32].</p>
<p>Self-Improvement of Reasoning and Decision-Making</p>
<p>A lot of research is focused on making large language models (LLMs) better at improving themselves.Some researchers are working on using carefully crafted prompts to help models learn how to get better, although this usually only works for one-off tasks.Others are tweaking how models get feedback during tasks, which helps them get better at thinking things through [33].There's also work on using strategies like random beam searches to help models make smarter decisions and assess their own work.Most current methods rely on quick, one-off tweaks and learning strategies that need lots of resources and hands-on tech help [34].This paper introduces a self-reflection mechanism, showing that LLMs can keep getting better and produce higher quality work across different tasks, all without needing extra training.</p>
<p>Memory Mechanism for LLM-based Agents</p>
<p>In LLM-based agents, the memory module is a critical component responsible for storing, processing, and retrieving task-related information.This memory plays a pivotal role in how the agent accumulates knowledge, handles historical experiences, and supports its decisionmaking processes.To enhance the self-evolution capabilities of these agents, researchers are focused on designing and optimizing these memory modules [35].Past research has covered various designs and implementations of memory modules.This includes integrating information from different trials to boost reasoning abilities or storing information in natural language to enhance the module's interpretability and user-friendliness [36].While there has been progress, further improvements in self-adjustment capabilities and memory management efficiency are still needed to better address complex problems in real-world applications.Our proposed memory optimization mechanism is designed to help agents better manage and adapt to dynamic and complex task environments.</p>
<p>Method</p>
<p>In this section, we present the MARS framework, designed to improve agent performance by leveraging three core mechanisms: iterative feedback, reflection, and MemorySyntax (as shown in Figure 1).The assistant agent  iteratively updates its policy   based on feedback   provided by the checker agent , optimizing over successive iterations to maximize the expected reward .The reflection mechanism allows  to incorporate historical observations O  and actions a  , forming a self-reflection   , which is stored in the memory M  for future decision-making.Finally, MemorySyntax combines the Ebbinghaus forgetting curve with linguistic principles to manage memory decay, dynamically updating the agent's short-term memory M  and long-term memory M  by prioritizing information based on its retention strength (  ), thus improving the agent's ability to retain crucial information while discarding less relevant data.In the following subsections, we describe the detailed implementation of these components.</p>
<p>Iterative Feedback</p>
<p>The iterative feedback mechanism in the MARS framework enables the assistant agent  to refine its policy   through repeated interactions with the checker agent .At each iteration , the assistant receives feedback   based on its current output o  , and adjusts its Figure 1: Figure 1: An illustration of the MARS framework, designed for continuous improvement in artificial intelligence systems.It integrates user interaction, information optimization, and observation-action cycles to enable iterative learning.Feedback mechanisms refine outputs, while performance metrics such as retention and rejection rates are monitored.The system leverages technical components like code, operating systems, and databases, and incorporates a knowledge graph for enhanced semantic understanding.Tailored for environments like Mind2Web, ALFWorld, and web shopping platforms, MARS aims to optimize web-based interactions and task automation through adaptive learning and self-improvement.policy accordingly.This process continues until the checker validates the output or the iteration cap  is reached, ensuring that  incrementally optimizes its decisions to improve task performance over successive iterations.</p>
<p>Initialization Phase</p>
<p>Role Assignment.In the MARS framework, three agents are introduced: the user , the assistant , and the checker .The user, upon receiving prompt   , assumes the role of task proposer by specifying a task T  and related constraints C  .The assistant, upon receiving prompt   , generates a sequence of actions a  based on the observations O  and environment E. The checker  evaluates the output o  produced by the assistant, providing feedback   based on the discrepancy between o  and the expected result, updating its policy   iteratively to minimize this gap.Task Assignment.The task T  provided by the user includes an initial task description d  and an instance i  that serves as the reference for correct output.This forms the input set I  = (d  , i  ) for the assistant to initiate its generative process.The assistant then proceeds by selecting an action a  at each time step , guided by   , with the goal of maximizing the reward   for completing T  .</p>
<p>Actual Interaction Phase</p>
<p>Following the role assignment and task definition in the initialization phase, the assistant  transitions into the actual interaction phase to generate outputs aimed at accomplishing the task T  .In this phase,  iteratively produces outputs o  at each time step  based on the task description d  and instance i  provided in the input set I  = (d  , i  ).At each time step , the assistant selects an action a  by following its policy   , which is conditioned on the current state   , the reward signal   (the reward score for task performance), and feedback    from the checker .This decision-making process is formalized as:
o 𝑡 ∼ 𝜋 𝜃 (o 𝑡 | 𝑠 𝑡 , 𝑅 𝑡 , 𝑓 𝑖 𝑡 ),(1)
where   represents the assistant's policy,   reflects the reward signal based on task performance at time , and    is the feedback provided by the checker during the -th iteration.</p>
<p>As the interaction progresses, the checker  evaluates each output o  generated by , comparing it against the expected outcome derived from i  .Based on this comparison, the checker provides iterative feedback    to guide  in refining its actions a  and outputs o  .The iterative refinement continues until either the checker validates the output as correct or the iteration limit  is reached.</p>
<p>Evolutionary Goals and Directions</p>
<p>Leveraging the feedback    obtained at each iteration , the assistant  formulates new evolutionary objectives:
G 𝑡+1 = (A 𝑡+1 , D 𝑡+1 ),(2)
where A +1 represents the updated memory optimization mechanisms, and D +1 refers to the refined self-adjustment capabilities.These evolutionary objectives guide the assistant in updating its policy   for the subsequent iteration.The policy update is governed by the function , which integrates the current policy    with the new evolutionary objectives
G 𝑡+1 : 𝜋 𝑡+1 𝜃 = 𝜓(𝜋 𝑡 𝜃 , G 𝑡+1 ).(3)
The iterative policy refinement enables the assistant  to continuously adapt its strategies based on cumulative feedback and evolving task requirements, thereby improving its overall performance in dynamic environments.</p>
<p>Memory Management</p>
<p>The MARS framework implements a dual-memory system, consisting of Short-Term Memory (STM) and Long-Term Memory (LTM), to manage task-relevant information and enhance the agent's reasoning and decision-making capabilities (see Figure 2 for a visual representation of this process).</p>
<p>Short-Term Memory (STM).</p>
<p>STM is responsible for storing immediate, task-specific data with limited capacity.It updates rapidly with new observations (O  ) and actions (a  ), maintaining a recent trajectory history T  = (O  , a  ).This allows the agent to make real-time decisions and respond quickly to dynamic changes in the environment [37].</p>
<p>Long-Term Memory (LTM).</p>
<p>LTM retains critical information and self-reflections (  ) over extended periods, enabling the agent to accumulate knowledge from past interactions and apply it to future tasks.Stored as M  = {  |  ∈  }, this memory mechanism allows the agent to leverage prior experiences to improve task performance, particularly in complex environments that require long-span information retention [3].</p>
<p>By integrating STM and LTM, the MARS framework allows the agent to balance immediate task demands with the ability to draw from accumulated knowledge, thereby enhancing its overall decision-making efficiency.</p>
<p>Reflection</p>
<p>Figure 3 illustrates an example of the reflection mechanism applied to a HotpotQA task.The reflection mechanism equips the assistant  with sparse reward signals, such as binary success/failure states, trajectory T  , and its stored memory M  .The assistant processes these inputs, deriving insights from past performance and storing self-reflections r  for future decision-making.These self-reflections, richer than scalar rewards, enhance the assistant's learning capacity and are incorporated into long-term memory:
r 𝑡 = ref(o 1:𝑡 , R 1:𝑡 ),(4)
where ref(•) denotes the reflection function based on the output sequence o 1: and rewards R 1: .The derived reflection r  is then added to M  :
M 𝐿 ← M 𝐿 ∪ {r 𝑡 }.(5)
The process gradually enhances the agent's decision-making, allowing it to adapt effectively through accumulated experience.</p>
<p>MemorySyntax</p>
<p>Building upon the reflection mechanism, the MemorySyntax method integrates the Ebbinghaus forgetting curve with linguistic principles to emulate human-like memory processes within the agent's memory management system.Let   denote the information received at time , and let (  , ) represent its retention rate after a time interval .According to the Ebbinghaus forgetting curve, the retention rate is modeled as:
𝑅(𝐼 𝑡 , 𝜏) = 𝑒 − 𝜏 𝑆 , (6)
where  is the strength of the information, reflecting its importance and complexity.</p>
<p>To enhance retention, MemorySyntax applies linguistic optimization to   , producing an optimized version  *  with increased strength  * &gt; .The retention rate for  *  is defined as:
𝑅(𝐼 * 𝑡 , 𝜏) = 𝑒 − 𝜏 𝑆 * , if 𝐼 * 𝑡 ∈ M 𝑆 , 𝑒 − 𝜏 𝑆 , if 𝐼 * 𝑡 ∈ M 𝐿 ,(7)
where M  and M  represent short-term memory and long-term memory, respectively.</p>
<p>The agent updates its memory state M  based on the retention rate of  *  using predefined thresholds  1 and  2 , with  1 &gt;  2 .The memory update rule is formalized as:
M 𝑡+1 =          M 𝑡 ∪ {𝐼 * 𝑡 }, if 𝑅(𝐼 * 𝑡 , 𝜏) ≥ 𝜃 1 , M 𝑡 \ {𝐼 * 𝑡 }, if 𝑅(𝐼 * 𝑡 , 𝜏) &lt; 𝜃 2 , M 𝑡 , otherwise.(8)
This update rule operates under the following conditions:
• Retention in Short-Term Memory (M 𝑆 ): If 𝑅(𝐼 * 𝑡 , 𝜏) ≥ 𝜃 1 , the information 𝐼 * 𝑡 is retained in short-term memory M 𝑆 .
• Discarding Information: If ( *  , ) &lt;  2 , the information  *  is considered insignificant and is discarded.</p>
<p>• Transfer to Long-Term Memory (M  ): If  2 ≤ ( *  , ) &lt;  1 , the information  *  is transferred to long-term memory M  .</p>
<p>By simulating human memory dynamics, MemorySyntax enables the agent to prioritize essential information, retain critical data in short-term memory M  , store important but less frequently used information in long-term memory M  , and discard irrelevant data.The mechanism addresses memory capacity limitations and enhances the agent's ability to perform complex tasks requiring efficient memory management.</p>
<p>Detailed Dataset Information</p>
<p>(1) Operating systems Integrating LLMs into operating systems has immense potential for automating and optimizing tasks.This integration requires a secure, userfriendly interface to ensure effective LLM-OS interaction.LLMs must accurately understand the OS context for informed operations, prioritizing safety to prevent misuse.Additionally, the system should effectively handle errors and provide clear  (6) ALFWorld ALFWorld bridges interactive TextWorld environments with embodied tasks from the ALFRED dataset, enabling agents to learn abstract strategies and apply them to real-world tasks.It supports abstract reasoning and concrete execution, allowing agents to plan actions in a text-based simulator and execute these tasks in a visual environment.This approach enhances agent generalization and problemsolving skills across various domains, such as language understanding and visual navigation, by leveraging a modular design that simplifies research improvements.</p>
<p>Experiment</p>
<p>To demonstrate the capabilities and performance of MARS in coordinating autonomous agent groups to collaboratively complete tasks, we conduct extensive quantitative experiments on benchmark tasks.We use a public benchmark: AgentBench, a multidimensional evolutionary benchmark, from which we select six tasks.These tasks evaluate the reasoning and decision-making abilities of LLMs acting as agents in multi-turn open-ended generation settings.To comprehensively assess the agents' long-context understanding capabilities, we select four widely adopted tasks in the domain of long text.These tasks reflect the agents' programming abilities(LCC [38], RepoBench-P [39]) and reasoning abilities(HotpotQA4 , TriviaQA5 ).</p>
<p>Evaluation on AgentBench</p>
<p>Task Description AgentBench includes scenarios from CODE (Knowledge Graph, OS, DB), GAME (ALFWorld) [40], and WEB (WebShop [41], Mind2Web [42]).</p>
<p>Baselines</p>
<p>We evaluate commercial models GPT-3.5 and GPT-4, and open-source models Llama2, Codellama, Qwen, and ChatGLM2.Dialogue history exceeding the model length limit is truncated, using greedy decoding.</p>
<p>Results</p>
<p>As shown in Table 1, our method significantly improves model performance on AgentBench, especially for smaller models.GPT-3.5 and GPT-4, despite already high scores, also show notable improvements with MARS, up to 2.26x in the Database task.Llama2-7b is notably enhanced, showing the method's impact on weaker models.CodeLlama-7b and Qwen-1.8Balso see substantial gains.Qwen-1.8B,after applying our method, performs close to GPT-3.5, highlighting its potential as a general agent.Llama2, previously error-prone, shows a significant reduction in basic errors through feedback and memory optimization, proving that our method not only enhances agent capabilities but also reduces fundamental errors in complex tasks.</p>
<p>Complex Problem-Solving Tasks Evaluation</p>
<p>We evaluated MARS against baseline models on three tasks: long-form QA, multi-turn dialog, and sequential task completion.As shown in Table 2, MARS outperforms all baselines with significant gains, such as a 20.8% increase in answer accuracy for GPT-3.5 on HotpotQA and a 17.3% improvement in task completion for Mistral-7b on ALFWorld.Across all tasks, MARS notably enhances answer accuracy, dialog coherence, and step completion.</p>
<p>Evaluation of Long-Context Tasks</p>
<p>We evaluated the agent's code generation and reasoning on four long-text tasks: LCC Dataset [43] focuses on predicting the next line of code from a few initial lines, with Precision, Recall, and F1 as metrics.RepoBench-P [44] tests retrieval of relevant code snippets from cross-file and within-file contexts to predict the next line, also evaluated with Precision, Recall, and F1.HotPotQA [45], a Wikipedia-based dataset with 113k question-answer pairs, challenges the agent to reason across multiple documents, evaluated by answer F1.TriviaQA [46] is a reading comprehension dataset with question-answer pairs and evidence paragraphs (filtered to over 1,000 words), also using answer F1 for evaluation.</p>
<p>We compared two self-refinement methods: Beam Search, which integrates selfassessment through stochastic beam search, and Reflexion, which uses past trial experience in a verbal form.</p>
<p>Evaluation Results:</p>
<p>Code Completion Task: On the LCC dataset (Table 3), MARS shows a slight improvement in F1 score (79.29) compared to Beam Search and Reflexion.Its memory mechanisms help refine code predictions, but the performance difference is not substantial in simpler tasks like code completion.</p>
<p>Reasoning Tasks: MARS significantly outperforms Reflexion and Beam Search on HotPotQA and TriviaQA, with F1 scores of 22.06 and 22.76 (Table 3).MARS's ability to effectively integrate multi-document information through reflection leads to better reasoning accuracy, while Reflexion and Beam Search face challenges in handling complex reasoning tasks.</p>
<p>Evaluation of RAG Agents</p>
<p>Table 4 demonstrates that ChatGPT-4 (MARS) consistently outperforms other RAG methods across all tasks, with accuracy improvements ranging from 3.6% to 4.7%.Additionally,</p>
<p>Error analysis</p>
<p>As shown in Figure 4, the MARS framework significantly enhances agent performance across tasks, especially in the WS task, due to its iterative feedback mechanism, which refines outputs through continuous assistant-checker interaction.In OS and DB tasks, CLE and invalid format errors are nearly eliminated, with a notable reduction in invalid action errors, attributed to the reflection mechanism that helps the assistant learn and reduce logical mistakes.</p>
<p>Ablation Study</p>
<p>We conducted ablation experiments on Qwen-1.8B and CodeLlama-7B to evaluate memory optimization (Table 5).Without memory optimization, both models perform weakly, especially Qwen-1.8B,which improves from 6.8 to 48.0 in KG and from 0.0 to 10.5 in ALF after optimization.Similarly, CodeLlama-7B shows substantial gains, particularly in DB (2.7 to 41.3) and WS (14.3 to 58.7).Overall, CodeLlama-7B performs better than Qwen-1.8B,highlighting the stronger adaptability of models with more parameters in handling complex tasks.</p>
<p>Comparison with Task-Splitting Agents</p>
<p>To compare the MARS framework with existing agents across various task types and datasets, we conducted a comprehensive evaluation.The results are detailed in Table 6.MARS demonstrates superior performance in task completion time, answer accuracy, dialog coherence, and step completion accuracy, highlighting its effectiveness in managing complex, multi-faceted tasks.</p>
<p>Conclusion</p>
<p>In this paper, we propose the MARS framework, which enhances agents' self-adjustment and memory management in complex tasks through reflective mechanisms and memory optimization.Experimental results show significant performance improvements across benchmarks, especially in smaller models.In the AgentBench test, MARS boosts the performance of strong baselines like GPT-3.5 and GPT-4, while also significantly improving open-source models.It effectively reduces basic errors and logical mistakes, particularly enabling smaller models to handle complex tasks.</p>
<p>Limitations</p>
<p>Despite the significant improvements achieved by the MARS framework, several limitations remain.The iterative feedback process can become computationally intensive, leading to increased latency and higher resource consumption, which may not be feasible for real-time applications or systems with limited computational power.Additionally, managing short-term and long-term memory introduces complexity and overhead, requiring further optimization.The framework's performance heavily relies on the quality and accuracy of the checker; incorrect feedback can lead to suboptimal solutions.While the MARS framework has shown effectiveness on specific benchmark tasks, its generalization across a broader range of tasks needs validation.It may struggle with tasks involving high levels of ambiguity or requiring deep contextual understanding.The MemorySyntax method's thresholds for retaining or forgetting information are critical and can affect performance, necessitating careful tuning.The use of sparse reward signals may not provide sufficient granularity for learning, requiring more nuanced feedback mechanisms.Finally, the evaluation of the MARS framework has primarily been conducted on specific datasets, introducing a risk of evaluation bias.Broader, more diverse evaluations are needed to ensure robustness and general applicability.Addressing these limitations will require further research and development to refine the framework, optimize its components, and validate its performance across diverse and complex tasks.</p>
<p>Appendix</p>
<p>Examples</p>
<p>Cases study on TriviaQA in using GPT with MARS.</p>
<p>User:</p>
<p>Answer the question based on the given pasMARSs.Only give me the answer and do not output any other words.The following are given pasMARSs: History of canals in Great Britain -London Canal Museum History of canals in Great Britain Home &gt; History &gt; Canals in Britain In the history of canals, Britain was not a pioneer.The Chinese can claim that the Grand Canal of China was one of the first, in the tenth century,a. . . . . .Answer the question based on the given pasMARSs.Only give me the answer and do not output any other words.Question: Which metal is produced by the Bessemer Process?Answer: Assistant:</p>
<p>The content of the file seems to be an HTML document related to the administrative division of Turkey, including population statistics, charts, and maps.This file contains scripts, metadata, and structured data typical for a web page.Checker: Clear Objectives: When answering user questions, the focus should be on directly addressing the question without introducing irrelevant information.Document Handling: Before confirming whether the document content is relevant to the question, detailed aspects of the document should not be directly addressed.Reflection:</p>
<p>Issue Description: The user wants to understand why the system outputs irrelevant HTML document content based on user input and seeks detailed feedback and reflection.Reasons for the Error: Misinterpretation of Input: The system mistakenly assumed that the user uploaded a file and attempted to read its content, whereas the user only provided text input.Processing Logic Error: Without clearly determining the user's intent, the system directly parsed and outputted file content.User Experience Problems: Irrelevant Output: The user expects concise, relevant answers, but the system provided lengthy and irrelevant HTML content, affecting the user experience.Poor Communication: Failure to accurately understand and respond to the user's real needs resulted in off-topic responses In-depth Reflection and Improvement Suggestions: Clarify User Intent: Accurate Parsing of User Input: Enhance the system's natural language processing capabilities to accurately distinguish whether the user uploaded a file or simply provided text input.User Feedback Mechanism: Implement a user feedback mechanism to promptly adjust the understanding of user intent.For example, confirm the issue to verify if the user uploaded a file.Optimize Processing Logic: Layered Processing Flow: In system design, separate file processing logic from text response logic to avoid misjudgments.For instance, when a user uploads a file, process the file content separately; when a user inputs text, provide relevant answers directly.Pre-processing Check: Prior to handling files, perform pre-processing checks to confirm if the file content meets expectations. . . . . . .LLMs with MARS response: Steel.Ground truth Steel.</p>
<p>Cases study on LCC in using GPT with MARS.User: import networkx as nx import pickle from Queue import PriorityQueue import copy import random import string import sys class MultiDiGraphEdgeKey(nx.MultiDiGraph): . . . . . .</p>
<p>Assistant:</p>
<p>You will be given the history of a past experience where you were given a task and failed to complete it.Reflect on the strategy and actions taken.The given code is a part of the Camerini algorithm, which is used for ranking edges in a directed graph.This algorithm is particularly useful for problems involving the identification of optimal or near-optimal substructures, like arborescences or spanning trees, in complex networks.The algorithm has two main phases: Collapsing Phase: This phase identifies and collapses cycles in the graph, creating supernodes.This simplification helps in managing cycles more efficiently and prepares the graph for the expansion phase.Expanding Phase: After collapsing cycles into supernodes, this phase expands the supernodes back into their original structure, resulting in a ranked arborescence, where the edges are sorted or prioritized based on certain criteria. . . . . . .</p>
<p>LLMs with MARS response:</p>
<p>Z.addedge(eu, ev, **data.copy())self.init(graph, Y, Z) A = best(root) self.init(graph) e , d = self.next(A,Y, Z) w = self.getgraphscore(A)-d if d != float('inf') else float('inf') P.put((-w, e, A, Y, Z)) solutions.append(A)return solutions def bestbranching(self, root): return self.best(root)def bestarborescencenorooted(self, root): return self.best(root)def bestarborescencerooted(self, root): return self.best(root)def getgraphscore(self, graph): return sum(data[self.attr] for u, v, data in graph.edges(data=True))def findroots(self, graph): roots = [node for node, degree in graph.indegree()if degree == 0] return roots . . . . . .</p>
<p>Figure 2 :
2
Figure 2: An example of the assistant's iterative workflow, including checker evaluation, prompt templates for feedback, and reflection processes integrating short-term and long-term memory.</p>
<p>Figure 3 :
3
Figure 3: The illustration of an example HotpotQA with MARS.</p>
<p>Figure 4 :
4
Figure 4: Execution results across six tasks (CLE: Context Limit Exceeded, TLE: Task Limit Exceeded).Task limits are the main cause of incomplete tasks, highlighting LLM agents' limitations under time constraints.Table5: Ablation study for memory optimization</p>
<p>Devise a concise, new plan of action that accounts for your previous mistakes.Here are two examples: Example 1: If you failed to remember to lock the door, plan to double-check the door each time.Example2: If you missed a deadline, plan to set reminders earlier. . . . . .Checker:</p>
<p>Table 1 :
1
Baseline and MARS Framework Performance on AgentBench Database The ability of LLMs to operate on real databases via SQL is critical due to the importance and complexity of database analysis in everyday activities.Previous research has highlighted the effectiveness of LLMs in automating database access, such as with T5QL, a new SQL generation method.Furthermore, fine-tuned LLMs (like GPT-3.5)have demonstrated the ability to extract and link complex scientific information from texts, obtaining structured knowledge from unstructured text and building extensive databases.
LLM Type ModelVEROSDBKGALFWSM2WBase MARS Base MARSBase MARS Base MARSBase MARS Base MARSBase MARSAPIGPT-4 GPT-3.542.4 31.649.7 38.332.0 39.8 15.7 35.657.4 63.1 25.9 37.678.0 82.0 17.0 23.067.1 67.8 64.1 72.127.0 32.0 16.0 28.027.0 32.0 16.0 28.0Llama2-7B Chat0.08.40.010.20.025.00.05.04.410.40.015.00.015.0CodeLlama-7B Instruct5.718.42.619.20.027.00.012.516.3 40.20.015.015.0 15.0OSSQwen1.8B Chat2.718.71.415.16.845.30.010.56.611.40.613.613.6 13.6Qwen-7B Chat5.622.24.818.00.048.034.0 38.50.013.60.015.015.0 15.0ChatGLM2-6B v1.10.015.20.016.30.017.00.05.00.310.34.914.914.9 14.9feedback to users, enhancing overall interaction and control. Addressing these aspectscan transform computer interaction and efficiency across various industries.(2) (4) Knowledge Graphs LLMs' utilization in constructing and interacting with knowledgegraphs (KG) presents a promising opportunity to enhance semantic understanding andinformation retrieval. This involves assessing the models' ability to generate and inter-pret complex interrelations within data, facilitating more intuitive and context-awareresponses. Effective LLM performance in this domain could significantly improveAI's reasoning and decision-making capabilities based on structured knowledge.(5) Mind2Web Mind2Web (M2W) is a dataset designed for developing web agentscapable of performing complex tasks on real websites via language instructions.It features over 2,000 tasks across 137 sites from 31 domains. M2W's real webenvironments and diverse user interactions make it a crucial platform for advancingAI navigation capabilities.
(3) WebShop WebShop is an innovative simulation of an e-commerce website environment, featuring 1.18 million real-world products and 12,087 crowd-sourced text instructions.It challenges agents to navigate various types of webpages and perform diverse actions to find, customize, and purchase products based on given instructions.WebShop's challenges include understanding compositional instructions, query (re-)formulation, dealing with noisy text on webpages, and strategic exploration.</p>
<p>Table 2 :
2
Evaluation of MARS and Baseline Models on Three Different Tasks
TaskAnswerDialogStepAgentCompletionAccuracyCoherenceCompletionTime (min)(QA)(%)(%)Accuracy (%)GPT-3.5 (Baseline)Long-form QA54.1%48.5%62.7%(HotpotQA)GPT-4 (Baseline)Long-form QA61.2%53.8%68.2%(HotpotQA)Llama2-7b (Baseline)Multi-turn Dialog55.9%50.1%64.8%(MultiWOZ)Codellama-13b (Baseline)Multi-turn Dialog58.4%52.3%66.7%(MultiWOZ)Mistral-7b (Baseline)Sequential Task56.5%51.5%65.1%(ALFWorld)MARS-GPT-3.5Long-form QA74.9%68.3%80.6%(HotpotQA)(+20.8%)(+19.8%)(+17.9%)MARS-GPT-4Long-form QA78.4%73.4%83.9%(HotpotQA)(+17.2%)(+19.6%)(+15.7%)MARS-Llama2-7bMulti-turn Dialog72.2%67.9%78.5%(MultiWOZ)(+16.1%)(+17.8%)(+13.7%)MARS-Codellama-13bMulti-turn Dialog74.7%71.2%81.2%(MultiWOZ)(+16.3%)(+18.9%)(+14.5%)MARS-Mistral-7bSequential Task73.8%70.5%79.9%(ALFWorld)(+17.3%)(+19.0%)(+14.8%)</p>
<p>Table 3 :
3
Comparison of Performance Across Different Methods
ModelsLCCRepoBench-PHotpotQA TriviaQAPrecision RecallF1Precision RecallF1F1F1Reflexion77.7281.00 79.2878.7381.86 80.2511.2611.23Beam search78.9879.32 79.1278.7581.02 79.8710.2612.13MARS78.7679.88 79.2979.2783.28 81.2222.0622.76</p>
<p>Table 4 :
4
Evaluation of Retrieval-Augmented Generation (RAG) Agents on Different Task Types and Datasets
AgentAccuracy (QA) (%) Latency (ms) Memory UMARS (MB)Task 1: Multi-Document QA (HotpotQA)RAG (BM25)60.8121613RAG (DPR)66.3129542RAG (OpenAI Retrieval)67.4108494TART63.2144477FiD (Fusion-in-Decoder)70.1153456ChatGPT-4 -MARS74.8 (+4.7)128231 (-50%)Task 2: Document Retrieval for Contextual Answering (Natural Questions)RAG (BM25)59.9125605RAG (DPR)65.5131561RAG (OpenAI Retrieval)66.8113484TART62.4146455FiD (Fusion-in-Decoder)69.8156443ChatGPT-4 -MARS73.6 (+3.8)131227 (-49%)Task 3: Open-Domain QA with Multiple Contexts (TriviaQA)RAG (BM25)62.1124615RAG (DPR)67.8129530RAG (OpenAI Retrieval)68.9117494TART64.7148462FiD (Fusion-in-Decoder)71.9155456ChatGPT-4 -MARS75.5 (+3.6)134243 (-47%)</p>
<p>Table 6 :
6
Evaluation of Task-Splitting SOTA Agents on Different Task Types and Datasets
AgentTask Completion Answer Accuracy Dialog Coherence Step CompletionTime (min)(QA)(%)(%)Accuracy (%)Task 1: Long-form Question Answering (HotpotQA)BabyAGI7.874.1N/AN/AAutoGPT8.176.3N/AN/AHuggingGPT6.779.2N/AN/ALangChain7.977.0N/AN/AAgentTaskMatrix.AI6.580.3N/AN/AMARS-10.682.5 (+6.0)N/AN/AChatGPT-4Task 2: Multi-turn Dialog Completion (MultiWOZ)BabyAGI8.2N/A73.5N/AAutoGPT8.5N/A75.6N/AHuggingGPT7.1N/A78.1N/ALangChain8.3N/A76.5N/AAgentTaskMatrix.AI6.8N/A79.9N/AMARS-10.9N/A81.6 (+5.8)N/AChatGPT-4</p>
<p>Task 3: Sequential Task with Dependencies (ALFWorld)
BabyAGI8.7N/AN/A71.5AutoGPT8.3N/AN/A73.1HuggingGPT7.5N/AN/A75.2LangChain7.8N/AN/A72.8AgentTaskMatrix.AI6.6N/AN/A76.4MARS-10.7N/AN/A82.3 (+5.9)ChatGPT-4
https://github.com/Significant-Gravitas/AutoGPT
https://github.com/yoheinakajima/babyagi
https://hotpotqa.github.io/
https://github.com/mandarjoshi90/triviaqa
Cases study on Repobench-P in using GPT with MARS.User:Please complete the code given below.{src//kademlia//operation//KadRefreshOperation.javapublic class KadRefreshOperation implements Operation private final KadServer server; private final KademliaNode localNode; private final KademliaDHT dht; private final KadConfiguration config; public KadRefreshOperation......Assistant:To devise a new plan of action, let's analyze the previous mistakes and come up with a solution: Mistake 1: In the KadRefreshOperation class, the execute() method calls the BucketRefreshOperation and ContentRefreshOperation without any error handling or exception handling.This can lead to unexpected errors and failures.Solution: Add proper error handling and exception handling in the execute() method of the KadRefreshOperation class.This will ensure that any errors or exceptions are properly handled and the execution can continue smoothly. . . . . .Checker:In the 'KadRefreshOperation' class, there is no error handling when the 'execute()' method calls 'BucketRefreshOperation' and 'ContentRefreshOperation'. Analysis: If 'BucketRe-freshOperation' or 'ContentRefreshOperation' throws an exception, the entire 'execute()' method will fail, potentially causing the interruption of the entire system's refresh operation.In the 'StoreOperation' class, the 'execute()' method does not handle the case of existing content in the DHT.Analysis: The 'execute()' method always returns 1 as the number of storage nodes, which is clearly incorrect.The correct number of nodes should be returned based on whether the content already exists in the DHT.Solution: Before storing the content, check if it already exists.If it does, return 0; otherwise, store the content and return the actual number of storage nodes.LLMs with MARS response:private final transient KademliaRoutingTable routingTable; private final transient KadConfiguration config; private final transient MesMARSFactory mesMARSFactory; /<em> Scheduled refresh operation </em>/ private final Timer refreshTimer; /<em><em> * Initialize the Kademlia system * @param ownerId The ID for this owner * @param nodeId The ID for this node * @param inetAddress The address for this node * @param port The port for this node * @param config Configuration for the Kad system * * @throws java.io.IOException </em>/ public JKademliaNode(String ownerId, KademliaId nodeId, InetAddress inetAddress, int port, KadConfiguration config) throws IOException { this.ownerId = ownerId; this.config = config; /</em> Initialize the Node <em>/ this.localNode = new Node(nodeId, in-etAddress, port); /</em> Initialize the Server */ this.server = new KadServer(localNode.getNodeId(), config); . . . . . .Ethics StatementThe development and application of the Self-evolving Agents with Reflective and Memory-augmented Abilities (MARS) framework presented in this paper adhere to the principles of ethical research and innovation.We acknowledge the broader impact of deploying autonomous agents in complex and dynamic environments and have taken the following measures to ensure ethical compliance:1. Data Privacy and Security: Our framework does not involve the collection or processing of personal data.However, we emphasize the importance of data privacy and security in the application of similar technologies and recommend the implementation of robust data protection measures.2. Transparency: We provide a comprehensive description of the MARS framework, its mechanisms, and its decision-making processes to ensure transparency in its functioning.Bias and Fairness:We are aware of the potential for algorithmic bias in AI systems and have taken steps to minimize such biases in the design of our framework.We encourage further research into the ethical implications of AI decision-making.Accountability:We maintain that the developers and deployers of AI systems should be accountable for their systems' actions and outcomes.MARS is designed to provide clear audit trails for its decisions.Informed Consent:In cases where human interaction is involved, we advocate for the principle of informed consent, ensuring that all participants are aware of the AI's role and its implications.6. Limitations: We acknowledge the limitations of our work, including the potential for the framework to be misused or to fail under certain conditions.We call for ongoing research to address these concerns.7. Future Research: We recommend that future work in this area should continue to consider ethical implications, including the long-term societal effects of self-evolving AI agents.We believe that addressing these ethical considerations can contribute to the responsible development and deployment of AI technologies.
X Liu, H Yu, H Zhang, Y Xu, X Lei, H Lai, Y Gu, H Ding, K Men, K Yang, S Zhang, X Deng, A Zeng, Z Du, C Zhang, S Shen, T Zhang, Y Su, H Sun, M Huang, Y Dong, J Tang, arXiv:2308.03688Agentbench: Evaluating llms as agents. 2023arXiv preprint</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, arXiv:2005.141652020</p>
<p>Hybrid computing using a neural network with dynamic external memory. A Graves, G Wayne, M Reynolds, T Harley, I Danihelka, A Grabska-Barwińska, S G Colmenarejo, E Grefenstette, T Ramalho, J Agapiou, Nature. 53876262016</p>
<p>J W Rae, A Potapenko, S M Jayakumar, T P Lillicrap, arXiv:1911.05507Compressive transformers for long-range sequence modelling. 2019arXiv preprint</p>
<p>C Packer, S Wooders, K Lin, V Fang, S G Patil, I Stoica, J E Gonzalez, arXiv:2310.08560Memgpt: Towards llms as operating systems. 2024</p>
<p>Über das gedächtnis: untersuchungen zur experimentellen psychologie. H Ebbinghaus, Duncker &amp; Humblot1885</p>
<p>The state of the art in open domain complex question answering: a survey. R Etezadi, M Shamsfard, Applied Intelligence. 5342023</p>
<p>Dsa-paml: A parallel automated machine learning system via dual-stacked autoencoder. P Liu, F Pan, X Zhou, S Li, P Zeng, S Liu, L Jin, Neural Computing and Applications. 34152022</p>
<p>Muformer: A long sequence time-series forecasting model based on modified multi-head attention. P Zeng, G Hu, X Zhou, S Li, P Liu, S Liu, Knowledge-Based Systems. 2541095842022</p>
<p>Seformer: a long sequence time-series forecasting model based on binary position encoding and information transfer regularization. P Zeng, G Hu, X Zhou, S Li, P Liu, Applied Intelligence. 53122023</p>
<p>A retinex structure-based low-light enhancement model guided by spatial consistency. M Zhang, Y Shen, Z Li, G Pan, S Lu, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Mdanet: A multi-stage domain adaptation framework for generalizable low-light image enhancement. J Wang, Y He, K Li, S Li, L Zhao, J Yin, M Zhang, T Shi, X Wang, Neurocomputing. 1295722025</p>
<p>Tscnet: A text-driven semanticlevel controllable framework for customized low-light image enhancement. M Zhang, J Yin, P Zeng, Y Shen, S Lu, X Wang, Neurocomputing. 1295092025</p>
<p>Adagent: Anomaly detection agent with multimodal large models in adverse environments. M Zhang, Y Shen, J Yin, S Lu, X Wang, IEEE Access. 2024</p>
<p>Scrnet: a retinex structure-based low-light enhancement model guided by spatial consistency. M Zhang, Y Shen, S Zhong, arXiv:2305.080532023arXiv preprint</p>
<p>M Zhang, Z Fang, T Wang, Q Zhang, S Lu, J Jiao, T Shi, arXiv:2503.08199A cascading cooperative multi-agent framework for on-ramp merging control integrating large language models. 2025arXiv preprint</p>
<p>Wcdt: World-centric diffusion transformer for traffic scene generation. C Yang, Y He, A X Tian, D Chen, J Wang, T Shi, A Heydarian, arXiv:2404.020822024arXiv preprint</p>
<p>Human-centric reward optimization for reinforcement learning-based automated driving using large language models. Z Zhou, J Zhang, J Zhang, Y He, B Wang, T Shi, A Khamis, arXiv:2405.041352024arXiv preprint</p>
<p>Gagent: An adaptive rigid-soft gripping agent with vision language models for complex lighting environments. Z Li, M Zhang, X Lin, M Yin, S Lu, X Wang, arXiv:2403.108502024arXiv preprint</p>
<p>Voltage regulation in polymer electrolyte fuel cell systems using gaussian process model predictive control. X Li, M Yang, M Zhang, Y Qi, Z Li, S Yu, Y Wang, L Shen, X Li, 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2024</p>
<p>X Li, M Yang, Y Qi, M Zhang, arXiv:2406.16871Neural network based model predictive control of voltage for a polymer electrolyte fuel cell system with constraints. 2024arXiv preprint</p>
<p>Ddpm-moco: Advancing industrial surface defect generation and detection with generative and contrastive learning. Y He, X Wang, T Shi, International Joint Conference on Artificial Intelligence. Springer2024</p>
<p>Railway side slope hazard detection system based on generative models. X Zheng, Y He, Y Luo, L Zhang, J Wang, T Shi, Y Bai, IEEE Sensors Journal. 2025</p>
<p>Street microclimate prediction based on transformer model and street view image in high-density urban areas. X Ma, T Zeng, M Zhang, P Zeng, B Lin, S Lu, Building and Environment. 2691124902025</p>
<p>Archidiff: Interactive design of 3d architectural forms generated from a single image. J Yin, W Gao, J Li, P Xu, C Wu, B Lin, S Lu, Computers in Industry. 1681042752025</p>
<p>Residential floor plans: Multi-conditional automatic generation using diffusion models. P Zeng, W Gao, J Yin, P Xu, S Lu, Automation in Construction. 1621053742024</p>
<p>Card: Cross-modal agent framework for generative and editable residential design. P Zeng, M Jiang, Z Wang, J Li, J Yin, S Lu, NeurIPS 2024 Workshop on Open-World Agents. </p>
<p>Enhancing code llms with reinforcement learning in code generation. J Wang, Z Zhang, Y He, Y Song, T Shi, Y Li, H Xu, K Wu, G Qian, Q Chen, arXiv:2412.203672024arXiv preprint</p>
<p>Enhancing intent understanding for ambiguous prompts through human-machine co-adaptation. Y He, J Wang, K Li, Y Wang, L Sun, J Yin, M Zhang, X Wang, arXiv:2501.151672025arXiv preprint</p>
<p>Enhancing low-cost video editing with lightweight adaptors and temporal-aware inversion. Y He, S Li, K Li, J Wang, B Li, T Shi, J Yin, M Zhang, X Wang, arXiv:2501.046062025arXiv preprint</p>
<p>Promptlnet: Regionadaptive aesthetic enhancement via prompt guidance in low-light enhancement net. J Yin, Y He, M Zhang, P Zeng, T Wang, S Lu, X Wang, arXiv:2503.082762025arXiv preprint</p>
<p>Cmat: A multi-agent collaboration tuning framework for enhancing small language models. X Liang, M Tao, Y Xia, T Shi, J Wang, J Yang, arXiv:2404.016632024arXiv preprint</p>
<p>Large language models can self-improve. J Huang, S S Gu, L Hou, Y Wu, X Wang, H Yu, J Han, arXiv:2210.116102022</p>
<p>Toward self-improvement of llms via imagination, searching, and criticizing. Y Tian, B Peng, L Song, L Jin, D Yu, H Mi, D Yu, arXiv:2404.122532024</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, Journal of machine learning research. 211402020</p>
<p>Unsupervised multilingual word embedding with limited resources using neural language models. T Wada, T Iwata, Y Matsumoto, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, nature. 51875402015</p>
<p>Longcoder: A long-range pre-trained language model for code completion. D Guo, C Xu, N Duan, J Yin, J Mcauley, arXiv:2306.148932023</p>
<p>Repobench: Benchmarking repository-level code autocompletion systems. T Liu, C Xu, J Mcauley, arXiv:2306.030912023</p>
<p>ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. M Shridhar, X Yuan, M.-A Côté, Y Bisk, A Trischler, M Hausknecht, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021</p>
<p>S Yao, H Chen, J Yang, K Narasimhan, arXiv:2207.01206Webshop: Towards scalable real-world web interaction with grounded language agents. 2023</p>
<p>X Deng, Y Gu, B Zheng, S Chen, S Stevens, B Wang, H Sun, Y Su, arXiv:2306.06070Mind2web: Towards a generalist agent for the web. 2023</p>
<p>Introducing the LCC metaphor datasets. M Mohler, M Brunson, B Rink, M Tomlinson, ; N Calzolari, K Choukri, T Declerck, S Goggi, M Grobelnik, B Maegaard, J Mariani, H Mazo, A , Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16). J Moreno, S Odĳk, Piperidis, the Tenth International Conference on Language Resources and Evaluation (LREC'16)Portorož, Slovenia2016European Language Resources Association (ELRA)</p>
<p>Repobench: Benchmarking repository-level code autocompletion systems. T Liu, C Xu, J Mcauley, The Twelfth International Conference on Learning Representations. 2024</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Z Yang, P Qi, S Zhang, Y Bengio, W Cohen, R Salakhutdinov, C D Manning, 10.18653/v1/D18-1259Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. E Riloff, D Chiang, J Hockenmaier, J Tsujii, the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. M Joshi, E Choi, D Weld, L Zettlemoyer, 10.18653/v1/P17-1147Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. R Barzilay, M.-Y Kan, the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20171</p>            </div>
        </div>

    </div>
</body>
</html>
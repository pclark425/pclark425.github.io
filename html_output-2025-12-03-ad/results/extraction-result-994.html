<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-994 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-994</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-994</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-119ad6b55970b90696c620b7b3985b86845cb533</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/119ad6b55970b90696c620b7b3985b86845cb533" target="_blank">Counterfactual Data Augmentation using Locally Factored Dynamics</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work introduces local causal models (LCMs), which are induced from a global causal model by conditioning on a subset of the state space, and proposes an approach to inferring these structures given an object-oriented state representation, as well as a novel algorithm for model-free Counterfactual Data Augmentation (CoDA).</p>
                <p><strong>Paper Abstract:</strong> Many dynamic processes, including common scenarios in robotic control and reinforcement learning (RL), involve a set of interacting subprocesses. Though the subprocesses are not independent, their interactions are often sparse, and the dynamics at any given time step can often be decomposed into locally independent causal mechanisms. Such local causal structures can be leveraged to improve the sample efficiency of sequence prediction and off-policy reinforcement learning. We formalize this by introducing local causal models (LCMs), which are induced from a global causal model by conditioning on a subset of the state space. We propose an approach to inferring these structures given an object-oriented state representation, as well as a novel algorithm for model-free Counterfactual Data Augmentation (CoDA). CoDA uses local structures and an experience replay to generate counterfactual experiences that are causally valid in the global model. We find that CoDA significantly improves the performance of RL agents in locally factored tasks, including the batch-constrained and goal-conditioned settings.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e994.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e994.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoDA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Counterfactual Data Augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data augmentation algorithm that generates causally-valid counterfactual transitions by swapping locally independent components between observed transitions, validated by a locally-conditioned mask; used to expand replay buffers for off-policy RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Counterfactual Data Augmentation (CoDA)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>CoDA takes pairs of factual transitions (s,a,s') and a mask function M(s,a) that encodes local adjacency (which state/action components influence which next-state components). It computes connected components of the local causal graph for each transition, selects a set of independent components, swaps those components between the transitions to propose a counterfactual transition, and then validates the proposal by reapplying M to the counterfactual. A proposal is accepted only if the counterfactual's mask yields the same graph partitioning (and the method also requires that the structural equations for swapped components agree across the neighborhoods). CoDA thus reuses empirical subsamples rather than sampling from an explicit forward dynamics model.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Spriteworld bouncing-ball; RoboschoolPong-based continuous Pong; FetchPush-v1; Slide2</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Simulated, interactive RL environments (object-oriented/state-factorized). Spriteworld: multi-sprite bouncing-collision 2D canvas used for standard online RL; Pong: continuous control variant used for batch RL; FetchPush and Slide2: goal-conditioned robotic manipulation-style tasks. All are simulated virtual labs that allow agent interaction (actions) and produce transition-level data.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Detection of irrelevant/distracting components via a locally-conditioned adjacency mask M(s,a) (learned or oracle); proposals that would entangle components are rejected. Requires identical local structural equations across neighborhoods to avoid spurious swaps.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant/conditionally-independent variables (local non-parents), selection bias concerns from prioritization (discussed), and model bias when using learned dynamics for augmentation (compared unfavorably to CoDA).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Uses MASK(s,a) (either oracle or learned) to detect which input components locally influence which output components; computes connected components and intersections to identify independent subgraphs; validates counterfactual by recomputing mask for the proposed transition and rejecting proposals whose partitioning changed.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Validation-based rejection: proposed counterfactuals are accepted only if the recomputed local mask on the counterfactual matches the original partitions; additionally requires that structural equations for swapped components be the same across neighborhoods (ensures local-model consistency).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Substantial empirical gains in RL: in Spriteworld CoDA (oracle mask) yields the best performance; learned-mask CoDA gives significant early sample-efficiency gains; in batch Pong CoDA with learned masks approximately doubles effective data size and yields ~3x performance boost at small dataset sizes (reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Baselines without CoDA (base TD3/HER) perform substantially worse; model-based Dyna using the same learned dynamics model suffers from model bias and underperforms CoDA and/or base agent in several settings.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Environments tested with up to 3-4 object-factors (Spriteworld up to 4 sprites; Pong: 3 components — 2 paddles and ball; Slide2: two pucks plus gripper/action entanglement).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoDA can generate a combinatorial number of counterfactual samples (n^m if n factual samples and m independent components), markedly improving sample efficiency in locally factored RL tasks; validating counterfactuals with a local mask prevents invalid swaps; using learned masks works but lags oracle masks, indicating masking is a critical component; CoDA outperforms direct model-based augmentation (Dyna) in these domains due to lower model bias.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Counterfactual Data Augmentation using Locally Factored Dynamics', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e994.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e994.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LCM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Local Causal Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A formalism that induces a sparser, local causal DAG by conditioning the global SCM on a neighborhood (subset of state-action space), thereby exposing context-specific independences that can be exploited for counterfactual reasoning and data augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Local Causal Models (LCMs)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Given a global structural causal model whose global graph may be densely connected, LCMs are obtained by conditioning on a local neighborhood L ⊆ S×A to produce a restricted SCM M^L whose structural equations f^L,i are restrictions of the global f^i to inputs in L. By structural minimality applied to the restricted domain, some edges may vanish, yielding disconnected components (locally independent mechanisms). LCMs are used to justify computational savings in counterfactual reasoning and to permit CoDA's swapping of independent components.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same simulated RL environments as used for CoDA (Spriteworld, Pong, FetchPush, Slide2)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Interactive simulated environments with factorizable state and action spaces; LCMs are induced for neighborhoods encountered in trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Context conditioning: variables that are globally parents can be non-parents within a local neighborhood, thus LCMs 'ignore' spurious global edges within that context; this amounts to conditional independence detection via domain restriction.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious global dependencies (edges present due to rare interactions), context-specific confounding that disappears under conditioning on L.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Derive local graph G^L by applying structural minimality to the restricted function domains (operationalized via MASK(s,a) in implementation); in practice the mask is learned to approximate which inputs the next-state components actually depend on locally.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Logical/structural: if local conditioning removes an edge (by minimality), then interventions and counterfactual reasoning can be simplified to exclude that edge; CoDA validates by recomputing masks for candidate counterfactuals to ensure the local model holds.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>No standalone numeric metrics; LCMs underpin CoDA's empirical gains (see CoDA results), enabling valid counterfactual generation and improved sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Conditioning on neighborhoods can transform dense global graphs into much sparser local graphs, enabling separation of causal mechanisms and scalable counterfactual reasoning; LCMs are the theoretical backbone for CoDA's swap-and-validate augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Counterfactual Data Augmentation using Locally Factored Dynamics', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e994.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e994.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SANDy-Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sparse Attention Neural Dynamics (Transformer variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned masking approach that uses a set-transformer / transformer attention masks (product across layers) trained for next-state prediction; the attention masks are thresholded to produce a local adjacency mask M(s,a) indicating local dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>SANDy-Transformer (attention-based local mask learning)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Train a transformer-like network (single-head set transformer architecture) to predict next-state from current state and action using an L2 loss. Extract the per-layer attention masks (sparse attention) and take their product to produce a global network mask approximating the absolute Jacobian upper bound; threshold this mask at τ to obtain a binary adjacency matrix M(s,a) indicating which inputs locally influence which next-state components. This mask is used by CoDA to identify independent components for swapping.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Spriteworld bouncing-ball; batch Pong; also used to generate masks for CoDA in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Simulated RL environments with disentangled, factorized state representations; training is supervised next-state prediction (passive learning) using logged transitions; interactive capability existed but the model is trained from offline data.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Sparse attention + thresholding to identify zero (or near-zero) dependencies; sparsity penalty / thresholding serves as variable selection to ignore distractor inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant/condition-specific inputs (variables that do not locally affect next-state components); reduces false positive parent edges caused by rare interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Extract attention masks from transformer layers and multiply them to approximate network Jacobian structure; threshold the resulting mask to detect zeros (non-dependencies). The transformer was found empirically to outperform a mixture-of-experts alternative in preliminary experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Implicit: if attention-based mask indicates no dependence, CoDA will not swap that component; counterfactual proposals that violate the mask are rejected. No explicit statistical refutation beyond mask validation is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Empirically, CoDA using the pretrained transformer mask provided significant early sample-efficiency gains and outperformed baseline TD3 in Spriteworld; the transformer mask was chosen since it performed better than the mixture-of-MLPs in preliminary tests (no numeric mask-accuracy numbers provided in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Baseline without CoDA (TD3) performed worse; Dyna using the same transformer model as dynamics performed poorly due to model bias.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Tested in domains with up to 4 object-factors; transformer mask trained with ~42k samples for Spriteworld and with dataset-specific training for Pong.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Attention-based masks (set-transformer) are an effective, scalable way to infer local factorization for CoDA; thresholded attention products serve as a practical proxy for the network Jacobian and can detect irrelevant local inputs enabling safe component-swapping.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Counterfactual Data Augmentation using Locally Factored Dynamics', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e994.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e994.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SANDy-Mixture</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sparse Attention Neural Dynamics (Mixture-of-MLPs variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mixture-of-experts approach where each expert is a dynamics MLP trained with sparsity to induce local dependencies; adjacency masks are estimated by thresholding an upper bound on the network Jacobian computed from weight matrices.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>SANDy-Mixture (mixture-of-MLPs Jacobian thresholding)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Train a mixture-of-MLP experts where each expert predicts next-state and is encouraged to be sparse (sparsity penalty). Estimate the input-output dependency structure by approximating the network Jacobian: bound the Jacobian using absolute weight matrix products across layers and threshold the resulting matrix to form a binary local mask M_tau(s,a). The mixture attention (selected expert) is conditioned on the current state and yields a locally-conditioned mask.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic Markov processes; Spriteworld (used in preliminary comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Simulated next-state prediction tasks from logged transitions; mixture gating is conditioned on current state (passive supervised learning).</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Jacobain-based thresholding to identify zero (or negligible) partial derivatives, i.e., inputs that do not influence outputs locally; mixture-of-experts structure yields specialized local masks.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant inputs/distractors and context-specific non-dependencies; reduces false parent edges by exploiting local expert specialization.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Compute an upper bound on the elementwise network Jacobian via product of absolute weight matrices across layers; threshold absolute values to form M_tau(s,a).</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Mask-based rejection of counterfactual proposals in CoDA; no separate statistical refutation beyond threshold-based adjacency identification is described.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Performed worse than the set-transformer variant in preliminary experiments (per authors' report); used to validate approaches to inferring masks but not the main mask used in later experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td>Evaluated in synthetic and Spriteworld settings (similar object counts as other experiments, up to 4 objects).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The mixture-of-MLPs approach can estimate local masks via Jacobian upper-bounds, but in the authors' preliminary tests the set-transformer attention-based mask outperformed it; the Jacobian-thresholding idea provides a principled way to detect local non-dependencies (distractors).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Counterfactual Data Augmentation using Locally Factored Dynamics', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e994.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e994.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraN-DAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gradient-based Neural DAG Learning (GraN-DAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A gradient-based method for learning DAG structure using neural networks and continuous optimization to enforce acyclicity and sparsity (cited as related work and inspiration for masked architectures).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gradient-based neural dag learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>GraN-DAG (cited)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>GraN-DAG is a neural-network-based causal discovery method that formulates DAG learning as a continuous optimization problem with an acyclicity constraint and uses gradient-based techniques and regularization to recover sparse causal graphs. In this paper it is mentioned as a related approach for causal structure discovery and as inspiration for masked/neural approaches to learning adjacency.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Mentioned only in related work / methodology context; not used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an example of gradient-based neural DAG learning that inspired masked architectures for causal discovery; the present paper generalizes the idea to locally-conditioned masks but does not evaluate GraN-DAG itself.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Counterfactual Data Augmentation using Locally Factored Dynamics', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e994.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e994.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MADE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Masked Autoencoder for Distribution Estimation (MADE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive masked-network technique used to restrict dependencies between inputs and outputs; cited as an inspiration for layer-wise masking and structured masks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MADE: Masked autoencoder for distribution estimation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>MADE (cited)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>MADE uses fixed masks on network weights to enforce autoregressive dependency structure. The present paper cites MADE when discussing prior work that uses layer-wise masks to control input-output dependencies and describes a generalization that conditions layer masks on the current state and action.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Mentioned in the context of mask architectures; not used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MADE is referenced as prior art for masked network architectures; the paper extends this idea by conditioning masks on the current state/action to obtain local factorizations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Counterfactual Data Augmentation using Locally Factored Dynamics', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e994.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e994.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bareinboim-Pearl-selection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Selection bias control methods (Bareinboim & Pearl)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work on controlling and recovering from selection bias in causal inference; referenced in the paper when discussing selection bias and prioritization pitfalls when augmenting data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Controlling selection bias in causal inference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Selection-bias control / recovery (Bareinboim & Pearl)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>The cited works provide graphical criteria and algorithms to detect and correct selection bias (and to recover causal effects under selection), including techniques that use graph structure to determine when adjustment or reweighting can recover unbiased causal estimates. The paper references these works as relevant to understanding and potentially mitigating selection bias introduced by prioritizing counterfactual samples.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Referenced conceptually to warn about selection bias when prioritizing counterfactual data; not implemented in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Selection bias, distributional shift induced by prioritized sampling</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper flags selection bias as a risk when creating counterfactual datasets and cites Bareinboim & Pearl as foundational work for controlling such biases; suggests that mask/topology knowledge can help mitigate selection bias in causal estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Counterfactual Data Augmentation using Locally Factored Dynamics', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gradient-based neural dag learning <em>(Rating: 2)</em></li>
                <li>MADE: Masked autoencoder for distribution estimation <em>(Rating: 2)</em></li>
                <li>Controlling selection bias in causal inference <em>(Rating: 2)</em></li>
                <li>Recovering from selection bias in causal and statistical inference <em>(Rating: 2)</em></li>
                <li>Causation, prediction, and search <em>(Rating: 2)</em></li>
                <li>Inferring causation from time series in earth system sciences <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-994",
    "paper_id": "paper-119ad6b55970b90696c620b7b3985b86845cb533",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "CoDA",
            "name_full": "Counterfactual Data Augmentation",
            "brief_description": "A data augmentation algorithm that generates causally-valid counterfactual transitions by swapping locally independent components between observed transitions, validated by a locally-conditioned mask; used to expand replay buffers for off-policy RL.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Counterfactual Data Augmentation (CoDA)",
            "method_description": "CoDA takes pairs of factual transitions (s,a,s') and a mask function M(s,a) that encodes local adjacency (which state/action components influence which next-state components). It computes connected components of the local causal graph for each transition, selects a set of independent components, swaps those components between the transitions to propose a counterfactual transition, and then validates the proposal by reapplying M to the counterfactual. A proposal is accepted only if the counterfactual's mask yields the same graph partitioning (and the method also requires that the structural equations for swapped components agree across the neighborhoods). CoDA thus reuses empirical subsamples rather than sampling from an explicit forward dynamics model.",
            "environment_name": "Spriteworld bouncing-ball; RoboschoolPong-based continuous Pong; FetchPush-v1; Slide2",
            "environment_description": "Simulated, interactive RL environments (object-oriented/state-factorized). Spriteworld: multi-sprite bouncing-collision 2D canvas used for standard online RL; Pong: continuous control variant used for batch RL; FetchPush and Slide2: goal-conditioned robotic manipulation-style tasks. All are simulated virtual labs that allow agent interaction (actions) and produce transition-level data.",
            "handles_distractors": true,
            "distractor_handling_technique": "Detection of irrelevant/distracting components via a locally-conditioned adjacency mask M(s,a) (learned or oracle); proposals that would entangle components are rejected. Requires identical local structural equations across neighborhoods to avoid spurious swaps.",
            "spurious_signal_types": "Irrelevant/conditionally-independent variables (local non-parents), selection bias concerns from prioritization (discussed), and model bias when using learned dynamics for augmentation (compared unfavorably to CoDA).",
            "detection_method": "Uses MASK(s,a) (either oracle or learned) to detect which input components locally influence which output components; computes connected components and intersections to identify independent subgraphs; validates counterfactual by recomputing mask for the proposed transition and rejecting proposals whose partitioning changed.",
            "downweighting_method": null,
            "refutation_method": "Validation-based rejection: proposed counterfactuals are accepted only if the recomputed local mask on the counterfactual matches the original partitions; additionally requires that structural equations for swapped components be the same across neighborhoods (ensures local-model consistency).",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Substantial empirical gains in RL: in Spriteworld CoDA (oracle mask) yields the best performance; learned-mask CoDA gives significant early sample-efficiency gains; in batch Pong CoDA with learned masks approximately doubles effective data size and yields ~3x performance boost at small dataset sizes (reported in paper).",
            "performance_without_robustness": "Baselines without CoDA (base TD3/HER) perform substantially worse; model-based Dyna using the same learned dynamics model suffers from model bias and underperforms CoDA and/or base agent in several settings.",
            "has_ablation_study": true,
            "number_of_distractors": "Environments tested with up to 3-4 object-factors (Spriteworld up to 4 sprites; Pong: 3 components — 2 paddles and ball; Slide2: two pucks plus gripper/action entanglement).",
            "key_findings": "CoDA can generate a combinatorial number of counterfactual samples (n^m if n factual samples and m independent components), markedly improving sample efficiency in locally factored RL tasks; validating counterfactuals with a local mask prevents invalid swaps; using learned masks works but lags oracle masks, indicating masking is a critical component; CoDA outperforms direct model-based augmentation (Dyna) in these domains due to lower model bias.",
            "uuid": "e994.0",
            "source_info": {
                "paper_title": "Counterfactual Data Augmentation using Locally Factored Dynamics",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "LCM",
            "name_full": "Local Causal Models",
            "brief_description": "A formalism that induces a sparser, local causal DAG by conditioning the global SCM on a neighborhood (subset of state-action space), thereby exposing context-specific independences that can be exploited for counterfactual reasoning and data augmentation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Local Causal Models (LCMs)",
            "method_description": "Given a global structural causal model whose global graph may be densely connected, LCMs are obtained by conditioning on a local neighborhood L ⊆ S×A to produce a restricted SCM M^L whose structural equations f^L,i are restrictions of the global f^i to inputs in L. By structural minimality applied to the restricted domain, some edges may vanish, yielding disconnected components (locally independent mechanisms). LCMs are used to justify computational savings in counterfactual reasoning and to permit CoDA's swapping of independent components.",
            "environment_name": "Same simulated RL environments as used for CoDA (Spriteworld, Pong, FetchPush, Slide2)",
            "environment_description": "Interactive simulated environments with factorizable state and action spaces; LCMs are induced for neighborhoods encountered in trajectories.",
            "handles_distractors": true,
            "distractor_handling_technique": "Context conditioning: variables that are globally parents can be non-parents within a local neighborhood, thus LCMs 'ignore' spurious global edges within that context; this amounts to conditional independence detection via domain restriction.",
            "spurious_signal_types": "Spurious global dependencies (edges present due to rare interactions), context-specific confounding that disappears under conditioning on L.",
            "detection_method": "Derive local graph G^L by applying structural minimality to the restricted function domains (operationalized via MASK(s,a) in implementation); in practice the mask is learned to approximate which inputs the next-state components actually depend on locally.",
            "downweighting_method": null,
            "refutation_method": "Logical/structural: if local conditioning removes an edge (by minimality), then interventions and counterfactual reasoning can be simplified to exclude that edge; CoDA validates by recomputing masks for candidate counterfactuals to ensure the local model holds.",
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": "No standalone numeric metrics; LCMs underpin CoDA's empirical gains (see CoDA results), enabling valid counterfactual generation and improved sample efficiency.",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Conditioning on neighborhoods can transform dense global graphs into much sparser local graphs, enabling separation of causal mechanisms and scalable counterfactual reasoning; LCMs are the theoretical backbone for CoDA's swap-and-validate augmentation.",
            "uuid": "e994.1",
            "source_info": {
                "paper_title": "Counterfactual Data Augmentation using Locally Factored Dynamics",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "SANDy-Transformer",
            "name_full": "Sparse Attention Neural Dynamics (Transformer variant)",
            "brief_description": "A learned masking approach that uses a set-transformer / transformer attention masks (product across layers) trained for next-state prediction; the attention masks are thresholded to produce a local adjacency mask M(s,a) indicating local dependencies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "SANDy-Transformer (attention-based local mask learning)",
            "method_description": "Train a transformer-like network (single-head set transformer architecture) to predict next-state from current state and action using an L2 loss. Extract the per-layer attention masks (sparse attention) and take their product to produce a global network mask approximating the absolute Jacobian upper bound; threshold this mask at τ to obtain a binary adjacency matrix M(s,a) indicating which inputs locally influence which next-state components. This mask is used by CoDA to identify independent components for swapping.",
            "environment_name": "Spriteworld bouncing-ball; batch Pong; also used to generate masks for CoDA in experiments",
            "environment_description": "Simulated RL environments with disentangled, factorized state representations; training is supervised next-state prediction (passive learning) using logged transitions; interactive capability existed but the model is trained from offline data.",
            "handles_distractors": true,
            "distractor_handling_technique": "Sparse attention + thresholding to identify zero (or near-zero) dependencies; sparsity penalty / thresholding serves as variable selection to ignore distractor inputs.",
            "spurious_signal_types": "Irrelevant/condition-specific inputs (variables that do not locally affect next-state components); reduces false positive parent edges caused by rare interactions.",
            "detection_method": "Extract attention masks from transformer layers and multiply them to approximate network Jacobian structure; threshold the resulting mask to detect zeros (non-dependencies). The transformer was found empirically to outperform a mixture-of-experts alternative in preliminary experiments.",
            "downweighting_method": null,
            "refutation_method": "Implicit: if attention-based mask indicates no dependence, CoDA will not swap that component; counterfactual proposals that violate the mask are rejected. No explicit statistical refutation beyond mask validation is reported.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Empirically, CoDA using the pretrained transformer mask provided significant early sample-efficiency gains and outperformed baseline TD3 in Spriteworld; the transformer mask was chosen since it performed better than the mixture-of-MLPs in preliminary tests (no numeric mask-accuracy numbers provided in main text).",
            "performance_without_robustness": "Baseline without CoDA (TD3) performed worse; Dyna using the same transformer model as dynamics performed poorly due to model bias.",
            "has_ablation_study": true,
            "number_of_distractors": "Tested in domains with up to 4 object-factors; transformer mask trained with ~42k samples for Spriteworld and with dataset-specific training for Pong.",
            "key_findings": "Attention-based masks (set-transformer) are an effective, scalable way to infer local factorization for CoDA; thresholded attention products serve as a practical proxy for the network Jacobian and can detect irrelevant local inputs enabling safe component-swapping.",
            "uuid": "e994.2",
            "source_info": {
                "paper_title": "Counterfactual Data Augmentation using Locally Factored Dynamics",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "SANDy-Mixture",
            "name_full": "Sparse Attention Neural Dynamics (Mixture-of-MLPs variant)",
            "brief_description": "A mixture-of-experts approach where each expert is a dynamics MLP trained with sparsity to induce local dependencies; adjacency masks are estimated by thresholding an upper bound on the network Jacobian computed from weight matrices.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "SANDy-Mixture (mixture-of-MLPs Jacobian thresholding)",
            "method_description": "Train a mixture-of-MLP experts where each expert predicts next-state and is encouraged to be sparse (sparsity penalty). Estimate the input-output dependency structure by approximating the network Jacobian: bound the Jacobian using absolute weight matrix products across layers and threshold the resulting matrix to form a binary local mask M_tau(s,a). The mixture attention (selected expert) is conditioned on the current state and yields a locally-conditioned mask.",
            "environment_name": "Synthetic Markov processes; Spriteworld (used in preliminary comparisons)",
            "environment_description": "Simulated next-state prediction tasks from logged transitions; mixture gating is conditioned on current state (passive supervised learning).",
            "handles_distractors": true,
            "distractor_handling_technique": "Jacobain-based thresholding to identify zero (or negligible) partial derivatives, i.e., inputs that do not influence outputs locally; mixture-of-experts structure yields specialized local masks.",
            "spurious_signal_types": "Irrelevant inputs/distractors and context-specific non-dependencies; reduces false parent edges by exploiting local expert specialization.",
            "detection_method": "Compute an upper bound on the elementwise network Jacobian via product of absolute weight matrices across layers; threshold absolute values to form M_tau(s,a).",
            "downweighting_method": null,
            "refutation_method": "Mask-based rejection of counterfactual proposals in CoDA; no separate statistical refutation beyond threshold-based adjacency identification is described.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Performed worse than the set-transformer variant in preliminary experiments (per authors' report); used to validate approaches to inferring masks but not the main mask used in later experiments.",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": "Evaluated in synthetic and Spriteworld settings (similar object counts as other experiments, up to 4 objects).",
            "key_findings": "The mixture-of-MLPs approach can estimate local masks via Jacobian upper-bounds, but in the authors' preliminary tests the set-transformer attention-based mask outperformed it; the Jacobian-thresholding idea provides a principled way to detect local non-dependencies (distractors).",
            "uuid": "e994.3",
            "source_info": {
                "paper_title": "Counterfactual Data Augmentation using Locally Factored Dynamics",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "GraN-DAG",
            "name_full": "Gradient-based Neural DAG Learning (GraN-DAG)",
            "brief_description": "A gradient-based method for learning DAG structure using neural networks and continuous optimization to enforce acyclicity and sparsity (cited as related work and inspiration for masked architectures).",
            "citation_title": "Gradient-based neural dag learning",
            "mention_or_use": "mention",
            "method_name": "GraN-DAG (cited)",
            "method_description": "GraN-DAG is a neural-network-based causal discovery method that formulates DAG learning as a continuous optimization problem with an acyclicity constraint and uses gradient-based techniques and regularization to recover sparse causal graphs. In this paper it is mentioned as a related approach for causal structure discovery and as inspiration for masked/neural approaches to learning adjacency.",
            "environment_name": "",
            "environment_description": "Mentioned only in related work / methodology context; not used in experiments.",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": null,
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Cited as an example of gradient-based neural DAG learning that inspired masked architectures for causal discovery; the present paper generalizes the idea to locally-conditioned masks but does not evaluate GraN-DAG itself.",
            "uuid": "e994.4",
            "source_info": {
                "paper_title": "Counterfactual Data Augmentation using Locally Factored Dynamics",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "MADE",
            "name_full": "Masked Autoencoder for Distribution Estimation (MADE)",
            "brief_description": "An autoregressive masked-network technique used to restrict dependencies between inputs and outputs; cited as an inspiration for layer-wise masking and structured masks.",
            "citation_title": "MADE: Masked autoencoder for distribution estimation",
            "mention_or_use": "mention",
            "method_name": "MADE (cited)",
            "method_description": "MADE uses fixed masks on network weights to enforce autoregressive dependency structure. The present paper cites MADE when discussing prior work that uses layer-wise masks to control input-output dependencies and describes a generalization that conditions layer masks on the current state and action.",
            "environment_name": "",
            "environment_description": "Mentioned in the context of mask architectures; not used in experiments.",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": null,
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "MADE is referenced as prior art for masked network architectures; the paper extends this idea by conditioning masks on the current state/action to obtain local factorizations.",
            "uuid": "e994.5",
            "source_info": {
                "paper_title": "Counterfactual Data Augmentation using Locally Factored Dynamics",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "Bareinboim-Pearl-selection",
            "name_full": "Selection bias control methods (Bareinboim & Pearl)",
            "brief_description": "Work on controlling and recovering from selection bias in causal inference; referenced in the paper when discussing selection bias and prioritization pitfalls when augmenting data.",
            "citation_title": "Controlling selection bias in causal inference",
            "mention_or_use": "mention",
            "method_name": "Selection-bias control / recovery (Bareinboim & Pearl)",
            "method_description": "The cited works provide graphical criteria and algorithms to detect and correct selection bias (and to recover causal effects under selection), including techniques that use graph structure to determine when adjustment or reweighting can recover unbiased causal estimates. The paper references these works as relevant to understanding and potentially mitigating selection bias introduced by prioritizing counterfactual samples.",
            "environment_name": "",
            "environment_description": "Referenced conceptually to warn about selection bias when prioritizing counterfactual data; not implemented in experiments.",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": "Selection bias, distributional shift induced by prioritized sampling",
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Paper flags selection bias as a risk when creating counterfactual datasets and cites Bareinboim & Pearl as foundational work for controlling such biases; suggests that mask/topology knowledge can help mitigate selection bias in causal estimation.",
            "uuid": "e994.6",
            "source_info": {
                "paper_title": "Counterfactual Data Augmentation using Locally Factored Dynamics",
                "publication_date_yy_mm": "2020-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gradient-based neural dag learning",
            "rating": 2
        },
        {
            "paper_title": "MADE: Masked autoencoder for distribution estimation",
            "rating": 2
        },
        {
            "paper_title": "Controlling selection bias in causal inference",
            "rating": 2
        },
        {
            "paper_title": "Recovering from selection bias in causal and statistical inference",
            "rating": 2
        },
        {
            "paper_title": "Causation, prediction, and search",
            "rating": 2
        },
        {
            "paper_title": "Inferring causation from time series in earth system sciences",
            "rating": 1
        }
    ],
    "cost": 0.018862999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Counterfactual Data Augmentation using Locally Factored Dynamics</h1>
<p>Silviu Pitis, Elliot Creager, Animesh Garg<br>Department of Computer Science, University of Toronto, Vector Institute<br>{spitis, creager, garg}@cs.toronto.edu</p>
<h4>Abstract</h4>
<p>Many dynamic processes, including common scenarios in robotic control and reinforcement learning (RL), involve a set of interacting subprocesses. Though the subprocesses are not independent, their interactions are often sparse, and the dynamics at any given time step can often be decomposed into locally independent causal mechanisms. Such local causal structures can be leveraged to improve the sample efficiency of sequence prediction and off-policy reinforcement learning. We formalize this by introducing local causal models (LCMs), which are induced from a global causal model by conditioning on a subset of the state space. We propose an approach to inferring these structures given an object-oriented state representation, as well as a novel algorithm for Counterfactual Data Augmentation (CoDA). CoDA uses local structures and an experience replay to generate counterfactual experiences that are causally valid in the global model. We find that CoDA significantly improves the performance of RL agents in locally factored tasks, including the batch-constrained and goal-conditioned settings. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>High-dimensional dynamical systems are often composed of simple subprocesses that affect one another through sparse interaction. If the subprocesses never interacted, an agent could realize significant gains in sample efficiency by globally factoring the dynamics and modeling each subprocess independently [28, 29]. In most cases, however, the subprocesses do eventually interact and so the prevailing approach is to model the entire process using a monolithic, unfactored model. In this paper, we take advantage of the observation that locally-during the time between their interactions-the subprocesses are causally independent. By locally factoring dynamic processes in this way, we are able to capture the benefits of factorization even when their subprocesses interact on the global scale.
Consider a game of billiards, where each ball can be viewed as a separate physical subprocess. Predicting the opening break is difficult because all balls are mechanically coupled by their initial placement. Indeed, a dynamics model with dense coupling amongst balls may seem sensible when considering the expected outcomes over the course of the game, as each ball has a non-zero chance of colliding with the others. But at any given timestep, interactions between balls are usually sparse.
One way to take advantage of sparse interactions between otherwise disentangled entities is to use a structured state representation together with a graph neural network or other message passing transition model that captures the local interactions [26, 39]. When it is tractable to do so, such architectures can be used to model the world dynamics directly, producing transferable, task-agnostic models. In many cases, however, the underlying processes are difficult to model precisely, and model-free $[46,87]$ or task-oriented model-based $[18,63]$ approaches are less biased and exhibit superior performance. In this paper we argue that knowledge of whether or not local interactions</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Counterfactual Data Augmentation (CoDA). Given 3 factual samples, knowledge of the local causal structure lets us mix and match factored subprocesses to form counterfactual samples. The first proposal is rejected because one of its factual sources (the blue ball) is not locally factored. The third proposal is rejected because it is not itself factored. The second proposal is accepted, and can be used as additional training data for a reinforcement learning agent.
occur is useful in and of itself, and can be used to generate causally-valid counterfactual data even in absence of a forward dynamics model. In fact, if two trajectories have the same local factorization in their transition dynamics, then under mild conditions we can produce new counterfactually plausible data using our proposed Counterfactual Data Augmentation (CoDA) technique, wherein factorized subspaces of observed trajectory pairs are swapped (Figure 1). This lets us sample from a counterfactual data distribution by stitching together subsamples from observed transitions. Since CoDA acts only on the agent's training data, it is compatible with any agent architecture (including unfactored ones).</p>
<p>In the remainder of this paper, we formalize this data augmentation strategy and discuss how it can improve performance of model-free RL agents in locally factored tasks.</p>
<p>Our main contributions are:</p>
<ol>
<li>We define local causal models (LCMs), which are induced from a global model by conditioning on a subset of the state space, and show how local structure can simplify counterfactual reasoning.</li>
<li>We introduce CoDA as a generalized data augmentation strategy that is able to leverage local factorizations to manufacture unseen, yet causally valid, samples of the environment dynamics. We show that goal relabeling $[36,1]$ and visual augmentation $[2,46]$ are instances of CoDA that use global independence relations and we propose a locally conditioned variant of CoDA that swaps independent subprocesses to form counterfactual experiences (Figure 1).</li>
<li>Using an attention-based method for discovering local causal structure in a disentangled state space, we show that our CoDA algorithm significantly improves the sample efficiency in standard, batch-constrained, and goal-conditioned reinforcement learning settings.</li>
</ol>
<h1>2 Local Causality in MDPs</h1>
<h3>2.1 Preliminaries and Problem Setup</h3>
<p>The basic model for decision making in a controlled dynamic process is a Markov Decision Process (MDP), described by tuple $\langle\mathcal{S}, \mathcal{A}, P, R, \gamma\rangle$ consisting of the state space, action space, transition function, reward function, and discount factor, respectively [72, 83]. Note that MDPs generalize uncontrolled Markov processes (set $A=\emptyset$ ), so that our work applies also to sequential prediction. We denote individual states and actions using lowercase $s \in \mathcal{S}$ and $a \in \mathcal{A}$, and variables using the uppercase $S$ and $A$ (e.g., $s \in \operatorname{range}(S) \subseteq \mathcal{S}$ ). A policy $\pi: \mathcal{S} \times \mathcal{A} \rightarrow[0,1]$ defines a probability distribution over the agent's actions at each state, and an agent is typically tasked with learning a parameterized policy $\pi_{\theta}$ that maximizes value $\mathbb{E}<em t="t">{P, \pi} \sum</em>\right)$.
In most non-trivial cases, the state $s \in \mathcal{S}$ can be described as an object hierarchy together with global context. For instance, this decomposition will emerge naturally in any simulated process or game that is defined using a high-level programming language (e.g., the commonly used Atari [7] or Minecraft [35] simulators). In this paper we consider MDPs with a single, known top-level decomposition of the state space $\mathcal{S}=\mathcal{S}^{1} \oplus \mathcal{S}^{2} \oplus \cdots \oplus \mathcal{S}^{n}$ for fixed $n$, leaving extensions to hierarchical decomposition and} \gamma^{t} R\left(s_{t}, a_{t</p>
<p>multiple representations [32, 17], dynamic factor count $n$ [92], and (learned) latent representations [13] to future work. The action space might be similarly decomposed: $\mathcal{A}=\mathcal{A}^{1} \oplus \mathcal{A}^{2} \oplus \cdots \oplus \mathcal{A}^{m}$.</p>
<p>Given such state and action decompositions, we may model time slice $(t, t+1)$ using a structural causal model (SCM) $\mathcal{M}<em t="t">{t}=\left\langle V</em>$, where:}, U_{t}, \mathcal{F}\right\rangle$ ([65], Ch. 7) with directed acyclic graph (DAG) $\mathcal{G</p>
<ul>
<li>$V_{t}=\left{V_{t \mid+1]}^{i}\right}<em t="t">{i=0}^{2 n+m}=\left{S</em>$.}^{1} \ldots S_{t}^{n}, A_{t}^{1} \ldots A_{t}^{m}, S_{t+1}^{1} \ldots S_{t+1}^{n}\right}$ are the nodes (variables) of $\mathcal{G</li>
<li>$U_{t}=\left{U_{t \mid+1]}^{i}\right}<em t_1="t+1">{i=0}^{2 n+m}$ is a set of noise variables, one for each $V^{i}$, determined by the initial state, past actions, and environment stochasticity. We assume that noise variables at time $t+1$ are independent from other noise variables: $U</em>$ denotes an individual realization of the noise variables.}^{i} \pm U_{t \mid+1]}^{j} \forall i, j$. The instance $u=$ $\left(u^{1}, u^{2}, \ldots, u^{2 n+m}\right)$ of $U_{t</li>
<li>$\mathcal{F}=\left{f^{i}\right}<em _mid_1_="\mid+1]" t="t">{i=0}^{2 n+m}$ is a set of functions ("structural equations") that map from $U</em>$; see, e.g., Figure 2 (center).}^{i} \times \operatorname{Pa}\left(V_{t \mid+1]}^{i}\right)$ to $V_{t \mid+1]}^{i}$, where $\operatorname{Pa}\left(V_{t \mid+1]}^{i}\right) \subset V_{t} \backslash V_{t \mid+1]}^{i}$ are the parents of $V_{t \mid+1]}^{i}$ in $\mathcal{G}$; hence each $f^{i}$ is associated with the set of incoming edges to node $V_{t \mid+1]}^{i}$ in $\mathcal{G</li>
</ul>
<p>Note that while $V_{t}, U_{t}$, and $\mathcal{M}<em 1="1">{t}$ are indexed by $t$ (their distributions change over time), the structural equations $f^{i} \in \mathcal{F}$ and causal graph $\mathcal{G}$ represent the global transition function $P$ and apply at all times $t$. To reduce clutter, we drop the subscript $t$ on $V, U$, and $\mathcal{M}$ when no confusion can arise.
Critically, we require the set of edges in $\mathcal{G}$ (and thus the number of inputs to each $f^{i}$ ) to be structurally minimal ([67], Remark 6.6).
Assumption (Structural Minimality). $V^{j} \in \operatorname{Pa}\left(V^{i}\right)$ if and only if there exists some $\left{u^{i}, v^{-i j}\right}$ with $u^{i} \in \operatorname{range}\left(U^{i}\right), v^{-i j} \in \operatorname{range}\left(V \backslash\left{V^{i}, V^{j}\right}\right)$ and pair $\left(v</em>$.
Intuitively, structural minimality says that $V^{j}$ is a parent of $V^{i}$ if and only if setting the value of $V^{j}$ can have a nonzero direct effect ${ }^{2}$ on the child $V^{i}$ through the structural equation $f^{i}$. The structurally minimal representation is unique [67].
Given structural minimality, we can think of edges in $\mathcal{G}$ as representing global causal dependence. The probability distribution of $S_{t+1}^{i}$ is fully specified by its parents $\operatorname{Pa}\left(S_{t+1}^{i}\right)$ together with its noise variable $U_{i}$; that is, we have $P\left(S_{t+1}^{i} \mid S_{t}, A_{t}\right)=P\left(S_{t+1}^{i} \mid \operatorname{Pa}\left(S_{t+1}^{i}\right)\right)$ so that $S_{t+1}^{i} \pm V^{j} \mid \operatorname{Pa}\left(S_{t+1}^{i}\right)$ for all nodes $V^{j} \notin \operatorname{Pa}\left(S_{t+1}^{i}\right)$. We call an MDP with this structure a factored MDP [37]. When edges in $\mathcal{G}$ are sparse, factored MDPs admit more efficient solutions than unfactored MDPs [28].}^{j}, v_{2}^{j}\right)$ with $v_{1}^{j}, v_{2}^{j} \in \operatorname{range}\left(V^{j}\right)$ such that $v_{1}^{i}=f^{i}\left(\left{u^{i}, v^{-i j}, v_{1}^{j}\right}\right) \neq f^{i}\left(\left{u^{i}, v^{-i j}, v_{2}^{j}\right}\right)=v_{2}^{i</p>
<h1>2.2 Local Causal Models (LCMs)</h1>
<p>Limitations of Global Models Unfortunately, even if states and actions can be cleanly decomposed into several nodes, in most practical scenarios the DAG $\mathcal{G}$ is fully connected (or nearly so): since the $f^{i}$ apply globally, so too does structural minimality, and edge $\left(S_{k}^{i}, S_{k+1}^{j}\right)$ at time $k$ is present so long as there is a single instance-at any time $t$, no matter how unlikely-in which $S_{t}^{i}$ influences $S_{t+1}^{j}$. In the words of Andrew Gelman, "there are (almost) no true zeros" [24]. As a result, the factorized causal model $\mathcal{M}_{t}$, based on globally factorized dynamics, rarely offers an advantage over a simpler causal model that treats states and actions as monolithic entities (e.g., [12]).</p>
<p>LCMs Our key insight is that for each pair of nodes $\left(V_{t}^{i}, S_{t+1}^{j}\right)$ with $V_{t}^{i} \in \operatorname{Pa}\left(S_{t+1}^{j}\right)$ in $\mathcal{G}$, there often exists a large subspace $\mathcal{L}^{(j \pm i)} \subset \mathcal{S} \times \mathcal{A}$ for which $S_{t+1}^{j} \pm V_{t}^{i} \mid \operatorname{Pa}\left(S_{t+1}^{j}\right) \backslash V_{t}^{i},\left(s_{t}, a_{t}\right) \in \mathcal{L}^{(j \pm i)}$. For example, in case of a two-armed robot (Figure 2), there is a large subspace of states in which the two arms are too far apart to influence each other physically. Thus, if we restrict our attention to $\left(s_{t}, a_{t}\right) \in \mathcal{L}^{(j \pm i)}$, we can consider a local causal model $\mathcal{M}<em t="t">{t}^{\mathcal{L}^{(j \pm i)}}$ whose local DAG $\mathcal{G}^{\mathcal{L}^{(j \pm i)}}$ is strictly sparser than the global DAG $\mathcal{G}$, as the structural minimality assumption applied to $\mathcal{G}^{\mathcal{L}^{(j \pm i)}}$ implies that there is no edge from $V</em>}^{i}$ to $S_{t+1}^{j}$. More generally, for any subspace $\mathcal{L} \subseteq S \times A$, we can induce the Local Causal Model (LCM) $\mathcal{M<em t="t">{t}^{\mathcal{L}}=\left\langle V</em>$ as:}^{\mathcal{L}}, U_{t}^{\mathcal{L}}, \mathcal{F}^{\mathcal{L}}\right\rangle$ with DAG $\mathcal{G}^{\mathcal{L}}$ from the global model $\mathcal{M}_{t</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: A two-armed robot (left) might be modeled as an MDP whose state and action spaces decompose into left and right subspaces: $\mathcal{S}=\mathcal{S}^{L} \oplus \mathcal{S}^{R}, \mathcal{A}=\mathcal{A}^{L} \oplus \mathcal{A}^{R}$. Because the arms can touch, the global causal model (center left) between time steps is fully connected, even though left-to-right and right-to-left connections (dashed red edges) are rarely active. By restricting our attention to the subspace of states in which left and right dynamics are independent we get a local causal model (center right) with two components that can be considered separately for training and inference.</p>
<ul>
<li>$V_{t}^{\mathcal{L}}=\left{V_{t[+1]}^{\mathcal{L}, i}\right}<em t_1_="t[+1]">{i=0}^{2 n+m}$, where $P\left(V</em>\right)$.}^{\mathcal{L}, i}\right)=P\left(V_{t[+1]}^{i} \mid\left(s_{t}, a_{t}\right) \in \mathcal{L</li>
<li>$U_{t}^{\mathcal{L}}=\left{U_{t[+1]}^{\mathcal{L}, i}\right}<em t_1_="t[+1]">{i=0}^{2 n+m}$, where $P\left(U</em>\right)$.}^{\mathcal{L}, i}\right)=P\left(U_{t[+1]}^{i} \mid\left(s_{t}, a_{t}\right) \in \mathcal{L</li>
<li>$\mathcal{F}^{\mathcal{L}}=\left{f^{\mathcal{L}, i}\right}<em _mathcal_L="\mathcal{L">{i=0}^{2 n+m}$, where $f^{\mathcal{L}, i}=\left.f^{i}\right|</em>$}}$ ( $f^{i}$ with range of input variables restricted to $\mathcal{L}$ ). Due to structural minimality, the signature of $f^{\mathcal{L}, i}$ may shrink (as the range of the relevant variables is now restricted to $\mathcal{L}$ ), and corresponding edges in $\mathcal{G}$ will not be present in $\mathcal{G}^{\mathcal{L}} .{ }^{3</li>
</ul>
<p>In case of the two-armed robot, conditioning on the arms being far apart simplifies the global DAG to a local DAG with two connected components (Figure 2). This can make counterfactual reasoning considerably more efficient: given a factual situation in which the robot's arms are far apart, we can carry out separate counterfactual reasoning about each arm.</p>
<p>Leveraging LCMs To see the efficiency therein, consider a general case with global causal model $\mathcal{M}$. To answer the counterfactual question, "what might the transition at time $t$ have looked like if component $S_{t}^{i}$ had value $x$ instead of value $y$ ?", we would ordinarily apply Pearl's do-calculus to $\mathcal{M}$ to obtain submodel $\mathcal{M}<em t="t">{\text {do }\left(S</em>}^{i}=x\right)}=\left\langle V, U, \mathcal{F<em x="x">{x}\right\rangle$, where $\mathcal{F}</em>}=\mathcal{F} \backslash f^{i} \cup\left{S_{t}^{i}=x\right}$ and incoming edges to $S_{t}^{i}$ are removed from $\mathcal{G<em t="t">{\text {do }\left(S</em>}^{i}=x\right)}$ [65]. The component distributions at time $t+1$ can be computed by reevaluating each function $f^{j}$ that depends on $S_{t}^{i}$. When $S_{t}^{i}$ has many children (as is often the case in the global $\mathcal{G}$ ), this requires one to estimate outcomes for many structural equations $\left{f^{j} \mid V^{j} \in\right.$ Children $\left.\left(V_{t}^{i}\right)\right}$. But if both the original value of $S_{t}$ (with $S_{t}^{i}=y$ ) and its new value (with $S_{t}^{i}=x$ ) are in the set $\mathcal{L}$, the intervention is "within the bounds" of local model $\mathcal{M}^{\mathcal{L}}$ and we can instead work directly with local submodel $\mathcal{M<em t="t">{\text {do }\left(S</em>\right|}^{i}=x\right)}^{\mathcal{L}}$ (defined accordingly). The validity of this follows from the definitions: since $f^{\mathcal{L}, j}=\left.f^{j<em t="t">{\mathcal{L}}$ for all of $S</em>$, this reduces the number of structural equations that need to be considered.}^{i}$ 's children, the nodes $V_{t}^{k}$ for $k \neq i$ at time $t$ are held fixed, and the noise variables at time $t+1$ are unaffected, the distribution at time $t+1$ is the same under both models. When $S_{t}^{i}$ has fewer children in $\mathcal{M}^{\mathcal{L}}$ than in $\mathcal{M</p>
<h1>3 Counterfactual Data Augmentation</h1>
<p>We hypothesize that local causal models will have several applications, and potentially lead to improved agent designs, algorithms, and interpretability. In this paper we focus on improving offpolicy learning in RL by exploiting causal independence in local models for Counterfactual Data Augmentation (CoDA). CoDA augments real data by making counterfactual modifications to a subset of the causal factors at time $t$, leaving the rest of the factors untouched. Following the logic outlined in the Subsection 2.2, this can understood as manufacturing "fake" data samples using the counterfactual model $\mathcal{M}<em t="t">{\text {do }\left(S</em>$ and resample their children. While this is always possible using a model-based approach if we have good models of the structural equations, it is particularly nice when the causal mechanisms are independent, as we can do counterfactual reasoning directly by reusing subsamples from observed trajectories.}^{i--j}=x\right)}^{\left[\mathcal{L}\right]}$, where we modify the causal factors $S_{t}^{i-j</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Four instances of CoDA; orange nodes are relabeled, noise variables omitted for clarity. First: Goal relabeling [36], including HER [1], augments transitions with counterfactual goals. Second: Visual feature augmentation [2, 46] uses domain knowledge to change visual features $S_{t}^{V}$ (such as textures, lighting, and camera positions) that the designer knows do not impact the physical state $S_{t+1}^{P}$. Third: Dyna [82], including MBPO [34], augments real states with new actions and resamples the next state using a learned dynamics model. Fourth (ours): Given two transitions that share local causal structures, we propose to swap connected components to form new transitions.</p>
<p>Definition. The causal mechanisms represented by subgraphs $\mathcal{G}<em j="j">{i}, \mathcal{G}</em>} \subset \mathcal{G}$ are independent when $\mathcal{G<em j="j">{i}$ and $\mathcal{G}</em>$.}$ are disconnected in $\mathcal{G</p>
<p>When $\mathcal{G}$ is divisible into two (or more) connected components, we can think of each subgraph as an independent causal mechanism that can be reasoned about separately.</p>
<p>Existing data augmentation techniques can be interpreted as specific instances of CoDA (Figure 3). For example, goal relabeling [36], as used in Hindsight Experience Replay (HER) [1] and Q-learning for Reward Machines [33], exploits the independence of the goal dynamics $G_{t} \mapsto G_{t+1}$ (identity map) and the next state dynamics $S_{t} \times A_{t} \mapsto S_{t+1}$ in order to relabel the goal variable $G_{t}$ with a counterfactual goal. While the goal relabeling is done model-free, we typically assume knowledge of the goal-based reward mechanism $G_{t} \times S_{t} \times A_{t} \times S_{t+1} \mapsto R_{t+1}$ to relabel the reward, ultimately mixing model-free and model-based reasoning. Similarly, visual feature augmentation, as used in reinforcement learning from pixels [46, 41] and sim-to-real transfer [2], exploits the independence of the physical dynamics $S_{t}^{P} \times A_{t} \mapsto S_{t+1}^{P}$ and visual feature dynamics $S_{t}^{V} \mapsto S_{t+1}^{V}$ such as textures and camera position, assumed to be static $\left(S_{t+1}^{V}=S_{t}^{V}\right)$, to counterfactually augment visual features. Both goal relabeling and visual data augmentation rely on global independence relationships.</p>
<p>We propose a novel form of knowlen particular, we observe that whenever an environment transition is within the bounds of some local model $\mathcal{M}^{L}$ whose graph $\mathcal{G}^{\mathcal{L}}$ has the locally independent causal mechanism $\mathcal{G}<em i="i">{i}$ as a disconnected subgraph (note: $\mathcal{G}</em>}$ itself need not be connected), that transition contains an unbiased sample from $\mathcal{G<em i="i">{i}$. Thus, given two transitions in $\mathcal{L}$, we may mix and match the samples of $\mathcal{G}</em>$.}$ to generate counterfactual data, so long as the resulting transitions are themselves in $\mathcal{L</p>
<p>Remark 3.1. How much data can we generate using our CoDA algorithm? If we have $n$ independent samples from subspace $\mathcal{L}$ whose graph $\mathcal{G}^{\mathcal{L}}$ has $m$ connected components, we have $n$ choices for each of the $m$ components, for a total of $n^{m}$ CoDA samples-an exponential increase in data! One might term this the "blessing of independent subspaces."</p>
<p>Remark 3.2. Our discussion has been at the level of a single transition (time slice $(t, t+1)$ ), which is consistent with the form of data that RL agents typically consume. But we could also use CoDA to mix and match locally independent components over several time steps (see, e.g., Figure 1).</p>
<p>Remark 3.3. As is typical, counterfactual reasoning changes the data distribution. While off-policy agents are typically robust to distributional shift, future work might explore different ways to control or prioritize the counterfactual data distribution [77, 43]. We note, however, that certain prioritization schemes may introduce selection bias [31], effectively entangling otherwise independent causal mechanisms (e.g., HER's "future" strategy [1] may introduce "hindsight bias" [45, 78]).
Remark 3.4. The global independence relations relied upon by goal relabeling and image augmentation are incredibly general, as evidenced by their wide applicability. We posit that certain local independence relations are similarly general. For example, the physical independence of objects separated by space (the billiards balls of Figure 1, the two-armed robot of Figure 2, and the environments used in Section 4), and the independence between an agent's actions and the truth of (but not belief about) certain facts the agent is ignorant of (e.g., an opponent's true beliefs).</p>
<div class="codehilite"><pre><span></span><code><span class="nv">function</span><span class="w"> </span>\<span class="ss">(</span>\<span class="nv">operatorname</span>{<span class="nv">CODA</span>}<span class="ss">(</span>\<span class="ss">)</span><span class="w"> </span><span class="nv">transition</span><span class="w"> </span><span class="nv">t1</span>,<span class="w"> </span><span class="nv">transition</span><span class="w"> </span><span class="nv">t2</span><span class="ss">)</span>:
<span class="w">    </span><span class="nv">s1</span>,<span class="w"> </span><span class="nv">a1</span>,<span class="w"> </span><span class="nv">s1</span><span class="err">&#39; \(\leftarrow\) t1</span>
<span class="w">    </span><span class="nv">s2</span>,<span class="w"> </span><span class="nv">a2</span>,<span class="w"> </span><span class="nv">s2</span><span class="err">&#39; \(\leftarrow\) t2</span>
<span class="err">    m1, m2 \(\leftarrow \operatorname{MASK}(s 1, a 1), \operatorname{MASK}(s 2, a 2)\)</span>
<span class="err">    D1 \(\leftarrow\) COMPONENTS(m1)</span>
<span class="err">    D2 \(\leftarrow\) COMPONENTS(m2)</span>
<span class="err">    \(\mathrm{d} \leftarrow\) random sample from (D1 \(\cap\) D2)</span>
<span class="err">    \(\delta, \bar{a}, \bar{a}^{\prime} \leftarrow \operatorname{copy}\left(s 1, a 1, s 1^{\prime}\right)\)</span>
<span class="err">    \(\delta[\mathrm{d}], \bar{a}[\mathrm{~d}], \bar{a}^{\prime}[\mathrm{d}] \leftarrow \mathrm{s} 2[\mathrm{~d}], \mathrm{a} 2[\mathrm{~d}], \mathrm{s} 2^{\prime}[\mathrm{d}]\)</span>
<span class="err">    \(\overline{\mathrm{D}} \leftarrow\) COMPONENTS( \(\operatorname{MASK}(8, \bar{a}))\)</span>
<span class="err">    return \(\left(8, \bar{a}, \bar{a}^{\prime}\right)\) if \(\mathrm{d} \in \overline{\mathrm{D}}\) else \(\emptyset\)</span>
</code></pre></div>

<p>function $\operatorname{MASK}($ state s , action a):
Returns $(n+m) \times(n)$ matrix indicating if the $n$ next state components (columns) locally depend on the $n$ state and $m$ action components (rows).
function COMPONENTS(mask m):
Using the mask as the adjacency matrix for $\mathcal{G}^{\mathcal{L}}$ (with dummy columns for next action), finds the set of connected components $C=\left{C_{j}\right}$, and returns the set of independent components $D=\left{\mathcal{G}<em k="k">{i}=\bigcup</em> \subset\right.$ powerset $\left.(C)\right}$.} \mathcal{C}_{k}^{i} \mid \mathcal{C}^{\prime</p>
<p>Implementing CoDA We implement CoDA, as outlined above and visualized in Figure 3(d), as a function of two factual transitions and a mask function $M\left(s_{t}, a_{t}\right): \mathcal{S} \times \mathcal{A} \rightarrow{0,1}^{(n+m) \times n}$ that represents the adjacency matrix of the sparsest local causal graph $\mathcal{G}^{\mathcal{L}}$ such that $\mathcal{L}$ is a neighborhood of $\left(s_{t}, a_{t}\right) .{ }^{4}$ We apply $M$ to each transition to obtain local masks $\mathrm{m}<em 2="2">{1}$ and $\mathrm{m}</em>}$, compute their connected components, and swap independent components $\mathcal{G<em j="j">{i}$ and $\mathcal{G}</em>}$ (mutually disjoint and collectively exhaustive groups of connected components) between the transitions to produce a counterfactual proposal. We then apply $M$ to the counterfactual $\left(\tilde{s<em t="t">{t}, \tilde{a}</em>}\right)$ to validate the proposal-if the counterfactual mask $\mathfrak{ß}$ shares the same graph partitions as $\mathrm{m<em 2="2">{1}$ and $\mathrm{m}</em>$, we accept the proposal as a CoDA sample. See Algorithm 1.</p>
<p>Note that masks $\mathrm{m}<em 2="2">{1}, \mathrm{~m}</em>}$ and $\mathfrak{ß}$ correspond to different neighborhoods $\mathcal{L<em 2="2">{1}, \mathcal{L}</em>}$ and $\tilde{\mathcal{L}}$, so it is not clear that we are "within the bounds" of any model $\mathcal{M}^{\mathcal{L}}$ as was required in Subsection 2.2 for valid counterfactual reasoning. To correct this discrepancy we use the following proposition and additionally require the causal mechanisms (subgraphs) for independent components $\mathcal{G<em j="j">{i}$ and $\mathcal{G}</em>}$ to share structural equations in each local neighborhood: $f^{\mathcal{L<em 2="2">{1}, i}=f^{\mathcal{L}</em>}, i}=f^{\tilde{\mathcal{L}}, i}$ and $f^{\mathcal{L<em 2="2">{1}, j}=f^{\mathcal{L}</em>}, j}=$ $f^{\tilde{\mathcal{L}}, j} .{ }^{5}$ This makes our reasoning valid in the local subspace $\mathcal{L}^{*}=\mathcal{L<em 2="2">{1} \cup \mathcal{L}</em>$. See Appendix A for proof.
Proposition 1. The causal mechanisms represented by $\mathcal{G}} \cup \tilde{\mathcal{L}<em j="j">{i}, \mathcal{G}</em>} \subset \mathcal{G}$ are independent in $\mathcal{G}^{\mathcal{L<em 2="2">{1} \cup \mathcal{L}</em>}}$ if and only if $\mathcal{G<em j="j">{i}$ and $\mathcal{G}</em>}$ are independent in both $\mathcal{G}^{\mathcal{L<em 2="2">{1}}$ and $\mathcal{G}^{\mathcal{L}</em>}}$, and $f^{\mathcal{L<em 2="2">{1}, i}=f^{\mathcal{L}</em>}, i}, f^{\mathcal{L<em 2="2">{1}, j}=f^{\mathcal{L}</em>$.}, j</p>
<p>Since CoDA only modifies data within local subspaces, this biases the resulting replay buffer to have more factorized transitions. In our experiments below, we specify the ratio of observed-tocounterfactual data heuristically to control this selection bias, but find that off-policy agents are reasonably robust to large proportions of CoDA-sampled trajectories. We leave a full characterization of the selection bias in CoDA to future studies, noting that knowledge of graph topology was shown to be useful in mitigating selection bias for causal effect estimation [5, 6].</p>
<p>Inferring local factorization While the ground truth mask function $M$ may be available in rare cases as part of a simulator, the general case either requires a domain expert to specify an approximate causal model (as in goal relabeling and visual data augmentation) or requires the agent to learn the local factorization from data. Given how common independence due to physical separation of objects is, the former option will often be available. In the latter case, we note that the same data could also be used to learn a forward model. Thus, there is an implicit assumption in the latter case that learning the local factorization is easier than modeling the dynamics. We think this assumption is rather mild, as an accurate forward dynamics model would subsume the factorization, and we provide some empirical evidence of its validity in Section 4.
Learning the local factorization is similar to conditional causal structure discovery [81, 75, 67], conditioned on neighborhood $\mathcal{L}$ of $\left(s_{t}, a_{t}\right)$, except that the same structural equations must be applied globally (if the structural equations were conditioned on $\mathcal{L}$, Proposition 1 would fail). As there are many algorithms for general structure discovery [81, 75], and the arrow of time simplifies the inquiry</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>[27, 67], there may be many ways to approach this problem. For now, we consider a generalization of the global network mask approach used by MADE [25] (eq. 10) for autoregressive distribution modeling and GraN-DAG [44] (eq. 6) for causal discovery, which additionally conditions the mask on the current state and action.</p>
<p>This approach computes a locally conditioned network mask $M\left(s_{t},a_{t}\right)$ by taking the matrix product of locally conditioned layer masks: $M\left(s_{t},a_{t}\right)=\Pi_{t=1}^{L} M_{\ell}\left(s_{t},a_{t}\right)$. This mask can be understood as an upper bound on the network’s absolute Jacobian (see Appendix C). Again, there may be several models allow one to compute conditional layer masks. We tested two such models: a mixture of MLP experts and a single-head set transformer architecture [85, 47]. Each is described in more detail in Appendix C. Both are trained to model forward dynamics using an L2 prediction loss and induce a sparse network mask either via a sparsity penality (in case of the mixture of experts model) or via a sparse attention mechanism (in case of the set transformer). In preliminary experiments (Appendix C) we found that the set transformer performed better, and proceed to use it in our main experiments (Section 4). The set transformer uses the attention mask at each layer as the layer mask for that layer, so that the network mask is simply the product of the attention masks. Though trained to model forward dynamics, the CoDA models are used by the agent to infer local factorization rather than to directly sample future states as is typical in model-based RL. We found this produced reasonable results in the tested domains (below). See Appendix C for details. Future work should consider other approaches to inferring local structure such as graph neural networks [39, 13].</p>
<h2>4 Experiments</h2>
<p>Our experiments evaluate CoDA in the online, batch, and goal-conditioned settings, in each case finding that CoDA significantly improves agent performance as compared to non-CoDA baselines. Since CoDA only modifies an agent’s training data, we expect these improvements to extend to other off-policy task settings in which the state space can be accurately disentangled. Below we outline our experimental design and results, deferring specific details and additional results to Appendix B.</p>
<p>Standard online RL We extend Spriteworld [89] to construct a “bouncing ball” environment (right), that consists of multiple objects (sprites) that move and collide within a confined 2D canvas. We use tasks of varying difficulty, where the agent must navigate $N\in{1,2,3,4}$ of 4 sprites to their fixed target positions. The agent receives reward of $1/N$ for each of the $N$ sprites placed; e.g., the hardest task (Place 4) gives $1/4$ reward for each of 4 sprites placed. For each task, we use CoDA to expand the replay buffer of a TD3 agent [22] by about 8 times. We compare CoDA with a ground truth masking function (available via the Spriteworld environment) and learned masking function to the base TD3 agent, as well as a Dyna agent that generates additional training data by sampling from a model. For fair comparison, we use the same transformer used for CoDA masks for Dyna, which we pretrain using approximately 42,000 samples from a random policy. As in HER, we assume access to the ground truth reward function to relabel the rewards. The results in Figure 4 show that both variants of CoDA significantly improve sample complexity over the baseline. By contrast, the Dyna agent suffers from model bias, even though it uses the same model as CoDA.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Standard online RL (3 seeds): CoDA with the ground truth mask always performs the best, validating our basic idea. CoDA with a pretrained model also offers a significant early boost in sample efficiency and maintains its lead over the base TD3 agent throughout training. Using the same model to generate data directly (a la Dyna [82]) performs poorly, suggesting significant model bias.</p>
<p>| $|\mathcal{D}|$ | Real data | MBPO | Ratio of Real:CoDA [:MBPO] data (ours) |  |  |  |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| $(1000 s)$ | 1 s | 1s:1s | 1s:1c | 1s:3c | 1s:5c | 1s:3c:1s |
| 25 | $13.2 \pm 0.7$ | $18.5 \pm 1.5$ | $43.8 \pm 2.8$ | $40.9 \pm 2.5$ | $38.4 \pm 4.9$ | $\mathbf{4 6 . 8} \pm 3.1$ |
| 50 | $22.8 \pm 3.0$ | $36.6 \pm 4.3$ | $66.6 \pm 3.8$ | $64.4 \pm 3.1$ | $62.5 \pm 3.5$ | $\mathbf{7 0 . 4} \pm 3.8$ |
| 75 | $43.2 \pm 4.9$ | $46.0 \pm 4.7$ | $73.4 \pm 2.8$ | $\mathbf{7 6 . 7} \pm 2.6$ | $75.0 \pm 3.4$ | $74.6 \pm 3.2$ |
| 100 | $63.0 \pm 3.1$ | $66.4 \pm 4.9$ | $77.8 \pm 2.0$ | $\mathbf{8 2 . 7} \pm 1.5$ | $76.6 \pm 3.0$ | $73.7 \pm 2.9$ |
| 150 | $77.4 \pm 1.2$ | $72.6 \pm 5.6$ | $82.2 \pm 1.8$ | $\mathbf{8 5 . 8} \pm 1.4$ | $84.2 \pm 1.0$ | $79.7 \pm 3.6$ |
| 250 | $78.2 \pm 2.7$ | $77.9 \pm 2.4$ | $85.0 \pm 2.9$ | $\mathbf{8 7 . 8} \pm 1.8$ | $87.0 \pm 1.0$ | $78.3 \pm 4.9$ |</p>
<p>Table 1: Batch RL (10 seeds): Mean success ( $\pm$ standard error, estimated using 1000 bootstrap resamples) on Pong environment. CoDA with learned masking function more than doubles the effective data size, resulting in a 3 x performance boost at smaller data sizes. Note that a 1 s:5c Real:CoDA ratio performs slightly worse than a 1 s:3c ratio due to distributional shift (Remark 3.3).</p>
<p>Batch RL A natural setting for CoDA is batch-constrained RL, where an agent has access to an existing transition-level dataset, but cannot collect more data via exploration [21, 48]. Another reason why CoDA is attractive in this setting is that there is no a priori reason to prefer the given batch data distribution to a counterfactual one. For this experiment we use a continuous control Pong environment based on RoboschoolPong [40]. The agent must hit the ball past
<img alt="img-4.jpeg" src="img-4.jpeg" />
the opponent, receiving reward of +1 when the ball is behind the opponent's paddle, -1 when the ball is behind the agent's paddle, and 0 otherwise. Since our transformer model performed poorly when used as a dynamics model, our Dyna baseline for batch RL adopts a state-of-the-art architecture [34] that employs a 7-model ensemble (MBPO). We collect datasets of up to 250,000 samples from an pretrained policy with added noise. For each dataset, we train both mask and reward functions (and in case of MBPO, the dynamics model) on the provided data and use them to generate different amounts of counterfactual data. We also consider combining CoDA with MBPO, by first expanding the dataset with MBPO and then applying CoDA to the result. We train the same TD3 agent on the expanded datasets in batch mode for 500,000 optimization steps. The results in Table 1 show that with only 3 state factors (two paddles and ball), applying CoDA is approximately equivalent to doubling the amount of real data.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 5: Goal-conditioned RL (5 seeds): In FetchPush and the challenging Slide2 environment, a HER agent whose dataset has been enlarged with CoDA approximately doubles the sample efficiency of the base HER agent.</p>
<p>Goal-conditioned RL As HER [1] is an instance of prioritized CoDA that greatly improves sample efficiency in sparse-reward tasks, can our unprioritized CoDA algorithm further improve HER agents? We use HER to relabel goals on real data only, relying on random CoDA-style goal relabeling for CoDA data. After finding that CoDA obtains state-of-the-art results in FetchPush-v1 [71], we show that CoDA also accelerates learning in a novel and significantly more
<img alt="img-6.jpeg" src="img-6.jpeg" />
challenging Slide2 environment, where the agent must slide two pucks onto their targets (Figure 5). For this experiment, we specified a heuristic mask using domain knowledge ("objects are disentangled if more than 10 cm apart") that worked in both FetchPush and Slide2 despite different dynamics.</p>
<h1>5 Related Work</h1>
<p>Factored MDPs [28, 29, 90] consider MDPs where state variables are only influenced by a fixed subset of "parent" variables at the previous timestep. The notion of "context specific independence" (CSI), which was used to compactly represent single factors of a Bayes net [10] or MDP [9] for efficient inference and model storage, ${ }^{6}$ is closely related to the local factorizations we study in this</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>paper. CSI can be understood as going one step beyond CoDA, exploiting not only knowledge of the local factorization, but also the structural equations at play in the local factorization; CSI could be leveraged for model-based RL approaches where faithful models of factored dynamics can be realized. Object-oriented and relational approaches to RL and prediction [15, 26, 39, 50, 93, 94] represent the dynamics as a set of interacting entities. Factored actions and policies have been used to formulate dimension-wise policy gradient baselines in standard and multi-agent settings [19, 54, 91].
A growing body of work applies causal reasoning the RL setting to improve sample efficiency, interpretability and learn better representations [55, 57, 74]. Particularly relevant is the work by Buesing et al. [12], which improves sample efficiency by using a causal model to sample counterfactual trajectories, thereby reducing variance of off-policy gradient estimates in a guided policy search framework. These counterfactuals use coarse-grained representations at the trajectory level, while our approach uses factored representations within a single transition. Batch RL [48, 21, 58] and more generally off-policy RL [88, 60] are counterfactual by nature, and are particularly important when it is costly or dangerous to obtain on-policy data [84]. The use of counterfactual goals to accelerate learning goal-conditioned RL [36, 76, 71] is what inspired our local CoDA algorithm.
Data augmentation is also widely used in supervised learning, and is considered a required best practice in high dimensional problems [42, 46, 66]. Heuristics for data augmentation often encode a causal invariance statement with respect to certain perturbations on the inputs. Thus model performance on counterfactual/augmented data can be seen as a measure of robustness. Assuring that models perform robustly in this sense is relevant to applications where fairness is a concern, as counterfactuals can be used to achieve robust performance and debias data [23, 64, 8].</p>
<h1>6 Conclusion</h1>
<p>In this paper we proposed a local causal model (LCM) framework that captures the benefits of decomposition in settings where the global causal model is densely connected. We used our framework to design a local Counterfactual Data Augmentation (CoDA) algorithm that expands available training data with counterfactual samples by stitching together locally independent subsamples from the environment. Empirically, we showed that CoDA can more than double the sample efficiency and final performance of reinforcement learning agents in locally factored environments.
There are several interesting avenues for future work. First, the sizable gap between ground truth and learned CoDA in our Spriteworld results suggest there is room for improvement in our approach to learning the masking function. Second, we have applied CoDA in a random, unprioritized fashion, but past work $[1,77]$ suggests there is significant benefit to prioritization. Third, we have applied CoDA in a way that might be considered model-free, insofar as we reuse subsamples from the environment dynamics rather than generating samples using our models of the causal mechanims. However, our LCM formalism allows for mixing model-free and model-based reasoning, which could further improve sample efficiency. Fourth, we used fully-observable, disentangled state spaces with a fixed top-level decomposition, but ultimately we would like to deploy CoDA in partially observable, entangled settings (e.g. RL from pixels) with multiple dynamic, multi-level decompositions [32, 17, 92]. Unsupervised learning of factorized latent representations is an active area of research [16, 39, 52, 53, 89], and it would be interesting to combine these methods with CoDA. Finally, it would be interesting to explore applications of our LCM framework to other areas such as interpretability [56, 59], exploration [61, 86], and off-policy evaluation [84].</p>
<h2>Acknowledgments and Disclosure of Funding</h2>
<p>We thank Jimmy Ba, Harris Chan, Seyed Kamyar Seyed Ghasemipour, James Lucas, David Madras, Yuhuai Wu and Lunjun Zhang for helpful comments and discussions. We also thank the anonymous reviewers for their feedback, which improved the final manuscript. SP is supported by an NSERC PGS-D award. EC is a student researcher at Google Brain in Toronto. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute (https://vectorinstitute.ai/partners).</p>
<h1>Broader Impact</h1>
<p>Considerations related to counterfactual reasoning CoDA transforms the observational data distribution into a counterfactual one. This incurs several risks and benefits, listed below.</p>
<ul>
<li>Modern machine learning models and reinforcement learning agents often generalize poorly under distributional shift (sometimes called "covariate shift") [49, 14]. CoDA has the potential to produce out-of-distribution data, that would never show up in the observational distribution. Thus, care should be taken when applying agent modules that have been trained only on observational data to counterfactual data, as their performance could decline sharply, thereby creating safety risks. We anticipate that work on uncertainty will be essential to controlling the risks associated with distributional shift [80].</li>
<li>The fact that CoDA creates distributional shift can also provide benefits in the form of distributional robustness and fairness. Training on out-of-distribution data can make models more robust [79], increasing their trustworthiness and practical applicability. As noted in Section 5, counterfactual reasoning can be leveraged in areas where fairness is a concern. For example, [64] propose to reduce gender bias in natural language processing by generating counterfactual sentences with swapped gender pronouns. We anticipate that a version of CoDA that prioritizes fairness concerns in a similar manner could be applied in the reinforcement learning and computer vision contexts.</li>
<li>In reinforcement learning specifically, a shift in data distribution requires either (1) the use of an off-policy algorithm, or (2) high variance off-policy corrections for on-policy algorithms. In the former case, it should be noted that in case of function approximation, even off-policy algorithms may be negatively affected by large shifts in their training distribution [83, 20]. More work is needed to quantify these effects and their implications for agent performance.</li>
</ul>
<p>Improving RL in batch-constrained settings In many settings, such as medicine and education, obtaining large quantities of observational data using a random policy is prohibitively expensive and/or unethical [84, 60]. As such, agents that can efficiently learn effective policies from batchconstrained data are needed, as are accurate ways to estimate agent performance from off-policy data [58, 62, 73]. We see CoDA as complementary to both goals, as subspace swapping is a powerful tool to produce large quantities of counterfactual data given a modest observational dataset. However, subspace swapping alone may be insufficient to generate plausible "exploratory" data for evaluating and learning new policies. For example, medical records from certain demographic groups may be unavailable or improperly collected/labeled. It is conceivable that a CoDA with a suitable prioritization scheme could compensate for such sample bias, but applied work in batch-constrained domains that characterizes the effect of sample bias on CoDA should nevertheless be carried out.</p>
<p>General considerations related to artificial agency To the extent that CoDA is a general technique for improving the ability of artificial agents to achieve their goals, it inherits the potential risks and benefits associated with empowered artificial agency, including but not limited to:
(a) the pursuit of misguided or dangerous goals, whether due to mispecification by a benevolent principal, the self-serving motives of its principals, or interference by malicious parties or other deviations from proper intents,
(b) the unsafe and improper pursuit of goals due to poor modeling or representation, resource constraints and lack of capacity, constraint mispecification, partial observability, or inadequate encoding and understanding of human values, and
(c) improvements to capital processes and automation of human labor, which could improve economic efficiency and raise the overall social welfare, but also run the risk of increased inequality, workforce displacement, and technological unemployment.</p>
<p>The risk associated with points (a) and (b) may be exacerbated in case of CoDA due to the risks associated with counterfactual reasoning outlined above: to the extent that CoDA is done with a poorly fit local factorization model, or with a local factorization model that does not generalize well to the counterfactual distribution, this could cause the agent to pursue poorly formulated counterfactual (imagined) goals, or create causally invalid data that hurts agent performance.</p>
<h1>References</h1>
<p>[1] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In Advances in neural information processing systems, pages 5048-5058, 2017.
[2] OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):3-20, 2020.
[3] Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.
[5] Elias Bareinboim and Judea Pearl. Controlling selection bias in causal inference. In Artificial Intelligence and Statistics, pages 100-108, 2012.
[6] Elias Bareinboim, Jin Tian, and Judea Pearl. Recovering from selection bias in causal and statistical inference. In AAAI, pages 2410-2416. Citeseer, 2014.
[7] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279, 2013.
[8] Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In Advances in neural information processing systems, pages 4349-4357, 2016.
[9] Craig Boutilier, Richard Dearden, Moises Goldszmidt, et al. Exploiting structure in policy construction. In IJCAI, volume 14, pages 1104-1113, 1995.
[10] Craig Boutilier, Nir Friedman, Moises Goldszmidt, and Daphne Koller. Context-specific independence in bayesian networks. arXiv preprint arXiv:1302.3562, 2013.
[11] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
[12] Lars Buesing, Theophane Weber, Yori Zwols, Sebastien Racaniere, Arthur Guez, Jean-Baptiste Lespiau, and Nicolas Heess. Woulda, coulda, shoulda: Counterfactually-guided policy search. International Conference on Learning Representations, 2019.
[13] Christopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt Botvinick, and Alexander Lerchner. Monet: Unsupervised scene decomposition and representation. arXiv preprint arXiv:1901.11390, 2019.
[14] Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John Schulman. Quantifying generalization in reinforcement learning. arXiv preprint arXiv:1812.02341, 2018.
[15] Carlos Diuk, Andre Cohen, and Michael L Littman. An object-oriented representation for efficient reinforcement learning. In Proceedings of the 25th international conference on Machine learning, pages $240-247,2008$.
[16] Aysegul Dundar, Kevin J Shih, Animesh Garg, Robert Pottorf, Andrew Tao, and Bryan Catanzaro. Unsupervised disentanglement of pose, appearance and background from images and videos. arXiv preprint arXiv:2001.09518, 2020.
[17] Babak Esmaeili, Hao Wu, Sarthak Jain, Alican Bozkurt, N Siddharth, Brooks Paige, Dana H Brooks, Jennifer Dy, and Jan-Willem Meent. Structured disentangled representations. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 2525-2534, 2019.
[18] Amir-massoud Farahmand, Andre Barreto, and Daniel Nikovski. Value-aware loss function for modelbased reinforcement learning. In Artificial Intelligence and Statistics, pages 1486-1494, 2017.
[19] Jakob N Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients. In Thirty-second AAAI conference on artificial intelligence, 2018.
[20] Justin Fu, Aviral Kumar, Matthew Soh, and Sergey Levine. Diagnosing bottlenecks in deep q-learning algorithms. arXiv preprint arXiv:1902.10250, 2019.</p>
<p>[21] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. arXiv preprint arXiv:1812.02900, 2018.
[22] Scott Fujimoto, Herke Van Hoof, and David Meger. Addressing function approximation error in actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.
[23] Sahaj Garg, Vincent Perot, Nicole Limtiaco, Ankur Taly, Ed H Chi, and Alex Beutel. Counterfactual fairness in text classification through robustness. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pages 219-226, 2019.
[24] Andrew Gelman. Causality and statistical learning, 2011.
[25] Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder for distribution estimation. In International Conference on Machine Learning, pages 881-889, 2015.
[26] Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine, Yoshua Bengio, and Bernhard Schölkopf. Recurrent independent mechanisms. arXiv preprint arXiv:1909.10893, 2019.
[27] Clive WJ Granger. Investigating causal relations by econometric models and cross-spectral methods. Econometrica: journal of the Econometric Society, pages 424-438, 1969.
[28] Carlos Guestrin, Daphne Koller, Ronald Parr, and Shobha Venkataraman. Efficient solution algorithms for factored mdps. Journal of Artificial Intelligence Research, 19:399-468, 2003.
[29] Assaf Hallak, François Schnitzler, Timothy Mann, and Shie Mannor. Off-policy model-based learning under unknown factored dynamics. In International Conference on Machine Learning, pages 711-719, 2015.
[30] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.
[31] MA Hernán and JM Robins. Causal inference: What if. Boca Raton: Chapman \&amp; Hill/CRC, 2020.
[32] Irina Higgins, Nicolas Sonnerat, Loic Matthey, Arka Pal, Christopher P Burgess, Matko Bošnjak, Murray Shanahan, Matthew Botvinick, Demis Hassabis, and Alexander Lerchner. SCAN: Learning hierarchical compositional visual concepts. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=rkN2I1-RZ.
[33] Rodrigo Toro Icarte, Toryn Klassen, Richard Valenzano, and Sheila McIlraith. Using reward machines for high-level task specification and decomposition in reinforcement learning. In International Conference on Machine Learning, pages 2107-2116, 2018.
[34] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization. In Advances in Neural Information Processing Systems, pages 12498-12509, 2019.
[35] Matthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell. The malmo platform for artificial intelligence experimentation. In IJCAI, pages 4246-4247, 2016.
[36] Leslie Pack Kaelbling. Learning to achieve goals. In IJCAI, pages 1094-1099. Citeseer, 1993.
[37] Michael Kearns and Daphne Koller. Efficient reinforcement learning in factored mdps. In IJCAI, volume 16, pages $740-747,1999$.
[38] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
[39] Thomas Kipf, Elise van der Pol, and Max Welling. Contrastive learning of structured world models. arXiv preprint arXiv:1911.12247, 2019.
[40] Oleg Klimov and John Schulman. Roboschool, 2017.
[41] Ilya Kostrikov, Denis Yarats, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. arXiv preprint arXiv:2004.13649, 2020.
[42] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097-1105, 2012.
[43] Aviral Kumar, Abhishek Gupta, and Sergey Levine. Discor: Corrective feedback in reinforcement learning via distribution correction. arXiv preprint arXiv:2003.07305, 2020.</p>
<p>[44] Sébastien Lachapelle, Philippe Brouillard, Tristan Deleu, and Simon Lacoste-Julien. Gradient-based neural dag learning. arXiv preprint arXiv:1906.02226, 2019.
[45] Sameera Lanka and Tianfu Wu. Archer: Aggressive rewards to counter bias in hindsight experience replay. arXiv preprint arXiv:1809.02070, 2018.
[46] Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Reinforcement learning with augmented data. arXiv preprint arXiv:2004.14990, 2020.
[47] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks. arXiv preprint arXiv:1810.00825, 2018.
[48] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
[49] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Learning to generalize: Meta-learning for domain generalization. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
[50] Richard Li, Allan Jabri, Trevor Darrell, and Pulkit Agrawal. Towards practical multi-object manipulation using relational reinforcement learning. arXiv preprint arXiv:1912.11032, 2019.
[51] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
[52] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Rätsch, Sylvain Gelly, Bernhard Schölkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations. arXiv preprint arXiv:1811.12359, 2018.
[53] Francesco Locatello, Ben Poole, Gunnar Rätsch, Bernhard Schölkopf, Olivier Bachem, and Michael Tschannen. Weakly-supervised disentanglement without compromises. arXiv preprint arXiv:2002.02886, 2020.
[54] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in neural information processing systems, pages 6379-6390, 2017.
[55] Chaochao Lu, Bernhard Schölkopf, and José Miguel Hernández-Lobato. Deconfounding reinforcement learning in observational settings. arXiv preprint arXiv:1812.10576, 2018.
[56] Daoming Lyu, Fangkai Yang, Bo Liu, and Steven Gustafson. Sdrl: Interpretable and data-efficient deep reinforcement learning leveraging symbolic planning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 2970-2977, 2019.
[57] Prashan Madumal, Tim Miller, Liz Sonenberg, and Frank Vetere. Explainable reinforcement learning through a causal lens. arXiv preprint arXiv:1905.10958, 2019.
[58] Travis Mandel, Yun-En Liu, Sergey Levine, Emma Brunskill, and Zoran Popovic. Offline policy evaluation across representations with applications to educational games. In AAMAS, pages 1077-1084, 2014.
[59] Alexander Mott, Daniel Zoran, Mike Chrzanowski, Daan Wierstra, and Danilo Jimenez Rezende. Towards interpretable reinforcement learning using attention augmented agents. In Advances in Neural Information Processing Systems, pages 12329-12338, 2019.
[60] Rémi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare. Safe and efficient off-policy reinforcement learning. In Advances in Neural Information Processing Systems, pages 1054-1062, 2016.
[61] Ashvin V Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. Visual reinforcement learning with imagined goals. In Advances in Neural Information Processing Systems, pages $9191-9200,2018$.
[62] Michael Oberst and David Sontag. Counterfactual off-policy evaluation with gumbel-max structural causal models. arXiv preprint arXiv:1905.05824, 2019.
[63] Junhyuk Oh, Satinder Singh, and Honglak Lee. Value prediction network. In Advances in Neural Information Processing Systems, pages 6118-6128, 2017.
[64] Ji Ho Park, Jamin Shin, and Pascale Fung. Reducing gender bias in abusive language detection. arXiv preprint arXiv:1808.07231, 2018.</p>
<p>[65] Judea Pearl. Causal inference in statistics: An overview. Statistics surveys, 3:96-146, 2009.
[66] Luis Perez and Jason Wang. The effectiveness of data augmentation in image classification using deep learning. arXiv preprint arXiv:1712.04621, 2017.
[67] Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: foundations and learning algorithms. MIT press, 2017.
[68] Silviu Pitis, Harris Chan, and Jimmy Ba. Protoge: Prototype goal encodings for multi-goal reinforcement learning. The 4th Multidisciplinary Conference on Reinforcement Learning and Decision Making (RLDM2019), 2019.
[69] Silviu Pitis, Harris Chan, and Stephen Zhao. mrl: modular rl. https://github.com/spitis/mrl, 2020.
[70] Silviu Pitis, Harris Chan, Stephen Zhao, Bradly Stadie, and Jimmy Ba. Maximum entropy gain exploration for long horizon multi-goal reinforcement learning. In Proceedings of the Thirty-seventh International Conference on Machine Learning, 2020.
[71] Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforcement learning: Challenging robotics environments and request for research. arXiv preprint arXiv:1802.09464, 2018.
[72] Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley \&amp; Sons, 2014.
[73] Aniruddh Raghu, Omer Gottesman, Yao Liu, Matthieu Komorowski, Aldo Faisal, Finale Doshi-Velez, and Emma Brunskill. Behaviour policy estimation in off-policy policy evaluation: Calibration matters. arXiv preprint arXiv:1807.01066, 2018.
[74] Danilo J Rezende, Ivo Danihelka, George Papamakarios, Nan Rosemary Ke, Ray Jiang, Theophane Weber, Karol Gregor, Hamza Merzic, Fabio Viola, Jane Wang, et al. Causally correct partial models for reinforcement learning. arXiv preprint arXiv:2002.02836, 2020.
[75] Jakob Runge, Sebastian Bathiany, Erik Bollt, Gustau Camps-Valls, Dim Coumou, Ethan Deyle, Clark Glymour, Marlene Kretschmer, Miguel D Mahecha, Jordi Muñoz-Marí, et al. Inferring causation from time series in earth system sciences. Nature communications, 10(1):1-13, 2019.
[76] Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators. In International conference on machine learning, pages 1312-1320, 2015.
[77] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015.
[78] Yannick Schroecker and Charles Isbell. Universal value density estimation for imitation learning and goal-conditioned reinforcement learning. arXiv preprint arXiv:2002.06473, 2020.
[79] Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness with principled adversarial training. arXiv preprint arXiv:1710.10571, 2017.
[80] Jasper Snoek, Yaniv Ovadia, Emily Fertig, Balaji Lakshminarayanan, Sebastian Nowozin, D Sculley, Joshua Dillon, Jie Ren, and Zachary Nado. Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift. In Advances in Neural Information Processing Systems, pages 13969-13980, 2019.
[81] Peter Spirtes, Clark N Glymour, Richard Scheines, and David Heckerman. Causation, prediction, and search. MIT press, 2000.
[82] Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart Bulletin, 2(4):160-163, 1991.
[83] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
[84] Philip Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement learning. In International Conference on Machine Learning, pages 2139-2148, 2016.
[85] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008, 2017.</p>
<p>[86] Maya Zhe Wang and Benjamin Y Hayden. Monkeys are curious about counterfactual outcomes. Cognition, 189:1-10, 2019.
[87] Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba. Benchmarking model-based reinforcement learning. arXiv preprint arXiv:1907.02057, 2019.
[88] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279-292, 1992.
[89] Nicholas Watters, Loic Matthey, Matko Bosnjak, Christopher P Burgess, and Alexander Lerchner. Cobra: Data-efficient model-based rl through unsupervised object discovery and curiosity-driven exploration. arXiv preprint arXiv:1905.09275, 2019.
[90] Théophane Weber, Nicolas Heess, Lars Buesing, and David Silver. Credit assignment techniques in stochastic computation graphs. arXiv preprint arXiv:1901.01761, 2019.
[91] Cathy Wu, Aravind Rajeswaran, Yan Duan, Vikash Kumar, Alexandre M Bayen, Sham Kakade, Igor Mordatch, and Pieter Abbeel. Variance reduction for policy gradient with action-dependent factorized baselines. arXiv preprint arXiv:1803.07246, 2018.
[92] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. In Advances in neural information processing systems, pages 3391-3401, 2017.
[93] Vinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls, David Reichert, Timothy Lillicrap, Edward Lockhart, et al. Relational deep reinforcement learning. arXiv preprint arXiv:1806.01830, 2018.
[94] Guangxiang Zhu, Zhiao Huang, and Chongjie Zhang. Object-oriented dynamics predictor. In Advances in Neural Information Processing Systems, pages 9804-9815, 2018.</p>
<h1>A Proof of Proposition 1</h1>
<p>Lemma 1. If $V^{j} \in \operatorname{Pa}^{\mathcal{L}}\left(V^{i}\right)$ in DAG $G^{\mathcal{L}}$ of (local) causal model $\mathcal{M}^{\mathcal{L}}$, and $\mathcal{L} \subset \mathcal{X}$, then $V^{j} \in$ $\operatorname{Pa}^{\mathcal{X}}\left(V^{i}\right)$ in DAG $G^{\mathcal{X}}$ corresponding to causal model $\mathcal{M}^{\mathcal{X}}$.</p>
<p>Proof. By minimality, there exist $\left{u^{i}, v^{-j}, v_{1}^{j}\right}$ and $\left{u^{i}, v^{-j}, v_{2}^{j}\right}$ with $v^{-j} \in \operatorname{Pa}^{\mathcal{L}}\left(V^{i}\right) \backslash V^{j}$ for which $f^{i}\left(\left{u^{i}, v^{-j}, v_{1}^{j}\right}\right) \neq f^{i}\left(\left{u^{i}, v^{-j}, v_{2}^{j}\right}\right)$. Expand $\left{v^{-j}, v_{1}^{j}\right}$ and $\left{v^{-j}, v_{2}^{j}\right}$ to $\left(s_{1}, a_{1}\right),\left(s_{2}, a_{2}\right) \in \mathcal{L}$ (with any values of other components). But $\mathcal{L} \subset \mathcal{X}$, so $\left(s_{1}, a_{1}\right),\left(s_{2}, a_{2}\right) \in \mathcal{X}$ and it follows from minimality in $\mathcal{X}$ that $V^{j} \in \operatorname{Pa}^{\mathcal{X}}\left(V^{i}\right)$.
Corollary 1. If $\mathcal{L} \subset \mathcal{X}, G^{\mathcal{L}}$ is sparser (has fewer edges) than $G^{\mathcal{X}}$.</p>
<p>Proposition 1. The causal mechanisms represented by $\mathcal{G}<em j="j">{i}, \mathcal{G}</em>} \subset \mathcal{G}$ are independent in $\mathcal{G}^{\mathcal{L<em 2="2">{1} \cup \mathcal{L}</em>}}$ if and only if $\mathcal{G<em j="j">{i}$ and $\mathcal{G}</em>}$ are independent in both $\mathcal{G}^{\mathcal{L<em 2="2">{1}}$ and $\mathcal{G}^{\mathcal{L}</em>}}$, and $f^{\mathcal{L<em 2="2">{1}, i}=f^{\mathcal{L}</em>}, i}, f^{\mathcal{L<em 2="2">{1}, j}=f^{\mathcal{L}</em>$.
Proof. $(\Rightarrow)$ If $\mathcal{G}}, j<em j="j">{i}$ and $\mathcal{G}</em>}$ are independent in $G^{\mathcal{L<em 2="2">{1} \cup \mathcal{L}</em>}}$, independence in $\mathcal{G}^{\mathcal{L<em 2="2">{1}}$ and $\mathcal{G}^{\mathcal{L}</em>}}$ follows from Corollary 1. That $f^{\mathcal{L<em 2="2">{1}, i}=f^{\mathcal{L}</em>}, i}$ (and $f^{\mathcal{L<em 2="2">{1}, j}=f^{\mathcal{L}</em>}, j}$ ), on their shared domain, follows since each is a restriction of the same function $f^{\mathcal{L<em 2="2">{1} \cup \mathcal{L}</em>}, i}$ (or $f^{\mathcal{L<em 2="2">{1} \cup \mathcal{L}</em>$ ).
$(\Leftarrow)$ Suppose $\mathcal{G}}, j<em j="j">{i}$ and $\mathcal{G}</em>}$ are independent in $\mathcal{G}^{\mathcal{L<em 2="2">{1}}$ and $\mathcal{G}^{\mathcal{L}</em>}}$ but not $G^{\mathcal{L<em 2="2">{1} \cup \mathcal{L}</em>}}$. By the definition of independence applied to $G^{\mathcal{L<em 2="2">{1} \cup \mathcal{L}</em>}}$, we have that, without loss of generality, there is a $V_{i} \in \mathcal{G<em j="j">{i}, V</em>} \in$ $\mathcal{G<em j="j">{j}$ with $V</em>} \in \operatorname{Pa}^{\mathcal{L<em 2="2">{1} \cup \mathcal{L}</em>}}\left(V_{i}\right)$. Then, from the definition of minimality, it follows that there exist $\left(s_{1}, a_{1}\right),\left(s_{2}, a_{2}\right) \in \mathcal{L<em 2="2">{1} \cup \mathcal{L}</em>\right)$.
Clearly, if $\left(s_{1}, a_{1}\right)$ and $\left(s_{2}, a_{2}\right)$ are both in $\mathcal{L}}$ that differ only in the value of $V_{j}$, and $u_{i} \in \operatorname{range}\left(U_{i}\right)$ for which $f^{i}\left(s_{1}, a_{1}, u_{i}\right) \neq f^{i}\left(s_{2}, a_{2}, u_{i<em 2="2">{1}$ (or $\mathcal{L}</em>}$ ), there will be an edge from $V_{j}$ to $V_{i}$ in $\mathcal{G}^{\mathcal{L<em 2="2">{1}}$ (or $\mathcal{G}^{\mathcal{L}</em>}}$ ) and the claim follows by contradiction. Thus, the only interesting case is when, without loss of generality, $\left(s_{1}, a_{1}\right) \in \mathcal{L<em 2="2">{1}$ and $\left(s</em>}, a_{2}\right) \in \mathcal{L<em 1="1">{2}$. The key observation is that $\left(s</em>}, a_{1}\right)$ and $\left(s_{2}, a_{2}\right)$ differ only in the value of node $V_{j} \notin \mathcal{G<em i="i">{i}$ : since $\mathcal{G}</em>}$ is an independent causal mechanism in both $\mathcal{G}^{\mathcal{L<em 2="2">{1}}$ and $\mathcal{G}^{\mathcal{L}</em>}}$ and the parents of $V_{i}$ take on the same values in each, we have that $f^{i}\left(s_{1}, a_{1}, u_{i}\right)=$ $f^{i, \mathcal{L<em 1="1">{1}}\left(s</em>}, a_{1}, u_{i}\right)=f^{i, \mathcal{L<em 2="2">{2}}\left(s</em>\right)$ and the claim follows by contradiction.}, a_{2}, u_{i}\right)=f^{i}\left(s_{2}, a_{2}, u_{i</p>
<h2>B Additional Experiment Details</h2>
<p>This section provides training details for the experiments discussed in Section 4 as well as some additional results. Code is available at https://github.com/spitis/mrl [69].</p>
<h2>B. 1 Online RL</h2>
<p>Here we detail the procedure used in the Online RL experiments in the Spriteworld environment.
Implementation We work with the original TD3 codebase, architecture, and hyperparameters (except batch size; see below), and focus our efforts solely on modifying the agent's training distribution.</p>
<p>Environment We extend the base Spriteworld framework [89] with (1) basic collisions (to induce a local factorization / so that a global factorization will not work), (2) a new continuous action space (2-dimensional), (3) a disentangled state renderer that returns the position and velocity of each sprite (a total of 16-dimensions in tasks with four sprites), (4) a mask renderer that returns the ground truth masking function (allows us to evaluate our masking function in Appendix C), and (5) a suite of partial and sparse reward tasks that we use for experiments. These extensions will included with the release of our code.</p>
<p>Data augmentation Every 1000 environment steps, we sample 2000 pairs of random transitions from the agent's replay buffer, and apply CoDA to produce a maximum of 5 unique CoDA samples per pair. We apply two forms of CoDA, using (1) an oracle / ground truth mask function that we back out of the simulator, and (2) the mask of a pre-trained transformer model (see Section C). The mask function was trained using approximately 42,000 samples from a random policy (5/6 of 50,000, with</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 6: Average reward on Sparse reward bouncing ball environment. As in the partial reward case (Figure 4), we observe that CoDA agents outperform the other agents (except in Place 4, where no agent achieves any reward).
the rest of the data used for validation). CoDA samples are added to a second CoDA replay buffer. For purposes of this experiment both buffers are have effectively infinite capacity (they are never filled). During training, the agent's batches are sampled proportionally from the real and CoDA replay buffers (this means that approximately $7 / 8$ of the data that the agent trains on is counterfactual).</p>
<p>Baselines In addition to the base TD3 agent and CoDA, we also use the transformer model that is used as a mask function to generate data by performing forward rollouts with a random policy, as in Dyna [82]. So that this baseline produces approximately the same number of samples as CoDA with the learned mask, we roll the model out for 5 steps from 1500 random samples from the replay buffer, again every 1000 environment steps. This produces 7500 model-based samples for every 1000 environment transitions.
Use of the transformer mask function requires setting the threshold value $\tau$, which we do by monitoring accuracy and F1 scores for sparsity prediction (as discussed in Appendix C) on validation data, ultimately using the value $\tau=0.05$.</p>
<p>Batch Size Since CoDA samples are plentiful we increase the agent's batch size from 256 to 1000 to allow it to train on more environment samples in the same number of training steps. We found that this slightly improved the performance of the base TD3 agent. An increase in batch size also allows the agent to see more of its own on-policy data in the face of many off-policy CoDA samples.</p>
<p>Additional results in sparse reward task variants In addition to the partial reward tasks described in Section 4, we also tested CoDA in four sparse reward tasks of varying difficulty. These are the same as the partial reward tasks, except that a sparse reward of 1 is granted only when all N sprites are in their target locations. While these tasks were much harder (and perhaps impossible in the case of Place 4 due to moving sprites), as shown in Figure 6, the CoDA agents maintain a clear advantage.</p>
<p>Results plot The results plot shows the 3 seeds in reduced opacity together with their smoothed mean.</p>
<h1>B. 2 Batch RL</h1>
<p>Here we detail the procedure used in the Batch RL experiments in the Pong environment.
Implementation This experiment works with a different codebase than the Spriteworld experiment, in order to simplify the use of CoDA in a Batch RL setting. Our experiment first builds the agent's dataset (consisting of real data, dyna data and/or CoDA data), then instantiates a TD3 agent by filling its replay buffer with the dataset. The replay buffer is always expanded to include the entire enlarged dataset (for the 5 x CoDA ratio at 250,000 data size this means the buffer has 1.5 E 6 experiences). The agent is run for 500,000 optimization steps.</p>
<p>Hyperparameters We used similar hyperparameters to the original TD3 codebase, with the following differences:</p>
<ul>
<li>
<p>We use a discount factor of $\gamma=0.98$ instead of 0.99 .</p>
</li>
<li>
<p>Since Pong is a sparse reward task with $\gamma=0.98$, we clip critic targets to $(-50,50)$.</p>
</li>
<li>We use networks of size $(128,128)$ instead of $(256,256)$.</li>
<li>As for our Spriteworld experiment, we use a batch size of 1000.</li>
</ul>
<p>Environment We base our Pong environment on RoboSchoolPong-v1 [40]. The original environment allowed the ball to teleport back to the center after one of the players scored, offered a small dense reward signal for hitting the ball, and included a stray "timeout" feature in the agent's state representation. We fix the environment so that the ball does not teleport, and instead have the episode reset every 150 steps, and also 10 steps after either player scores. The environment is treated as continuous and never returns a done signal that is not also accompanied by a TimeLimit.truncated indicator [11]. We change the reward to be strictly sparse, with reward of $\pm 1$ given when the ball is behind one of the players' paddles. Finally, we drop the stray "timeout" feature, so that the state space is 12-dimensional, where each set of 4 dimensions is the x-position, y-position, x-velocity, and y-velocity of the corresponding object ( 2 paddles and one ball).</p>
<p>Training the CoDA model Without access to a ground truth mask, we needed to train a masking function C to identify local disentanglement. We also forewent the ground truth reward, instead training our own reward classifier. In each case we used the batch dataset given, and so we trained different models for each random seed. For our masking model, we stacked two single-head transformer blocks (without positional encodings) and used the product of their attention masks as the mask. Each block consists of query $Q$, key $K$, and value $V$ networks that each have 3-layers of 256 neurons each, with the attention computed as usual [85, 47]. The transformer is trained to minimize the L2 error of the next state prediction given the current state and action as inputs. The input is disentangled, and so has shape (batch_size, num_components, num_features). In each row (component representation) of each sample, features corresponding to other components are set to zero. The transformer is trained for 2000 steps with a batch size of 256, Adam optimizer [38] with learning rate of $3 \mathrm{e}-4$ and weight decay of $1 \mathrm{e}-5$. For our reward function we use a fully-connected neural network with 1 hidden layer of 128 units. The reward network accepts an $\left(s, a, s^{\prime}\right)$ tuple as input (not disentangled) and outputs a softmax over the possible reward values of $[-1,0,1]$. It is trained for 2000 steps with a batch size of 512, Adam optimizer [38] with learning reate of $1 \mathrm{e}-3$ and weight decay of $1 \mathrm{e}-4$. All hyperparameters were rather arbitrary (we used the default setting, or in case it did not work, the first setting that gave reasonable results, as was determined by inspection). To ensure that our model and reward functions are trained appropriately (i.e., do not diverge) for each seed, we confirm that the average loss of the CoDA model is below 0.005 at the training and that the average loss of the reward model is below 0.1 , which values were found by inspection of a prototype run. These conditions were met by all seeds.
When used to produce masks, we chose a threshold of $\tau=0.02$ by inspection, which seemed to produce reasonable results. A more principled approach would do cross-validation on the available data, as we did for Spriteworld (Appendix C).</p>
<p>Tested configurations Table 1 reports a subset of our tested configurations. We report our results in full below. We considered the following configurations:</p>
<h1>1. Real data only.</h1>
<ol>
<li>CoDA + real data: after training the CoDA model, we expand the base dataset by either 2 , 3,4 or 6 times.</li>
<li>Dyna (using CoDA model): after training the CoDA model, we use it as a forward dynamics model instead of for CoDA; we use 1-step rollouts with random actions from random states in the given dataset to expand the dataset by 2 x . We also tried with 5 -step rollouts, but found that this further hurt performance (not shown). Note that Dyna results use only 5 seeds.</li>
<li>Dyna (using MBPO model): as the CoDA model exhibits significant model bias when used as a forward dynamics model, we replicate the state-of-the-art model-based architecture used by MBPO [34] and use it as a forward dynamics model for Dyna; we experimented with 1 -step and 5 -step rollouts with random actions from random states in the given dataset to expand the dataset by 2 x . This time we found that the 5 -step rollouts do better, which we attribute to the lower model bias together with the ability to create a more diverse dataset</li>
</ol>
<p>| $|\mathcal{D}|$ <br> $(1000 s)$ | Real data <br> 1 k | Dyna <br> 1k:1M | MBPO <br> 1k:1M | 1k:1c | Ratio of Real:CoDA [:MBPO] data (ours) <br> 1k:2c |  |  | 1k:3c:1M |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| 25 | $13.2 \pm 0.7$ | $12.2 \pm 1.4$ | $18.5 \pm 1.5$ | $43.8 \pm 2.7$ | $41.6 \pm 3.7$ | $40.9 \pm 2.5$ | $38.4 \pm 4.9$ | $46.8 \pm 3.2$ |
| 50 | $22.8 \pm 3.2$ | $17.4 \pm 3.0$ | $36.6 \pm 4.0$ | $66.6 \pm 3.7$ | $58.4 \pm 4.7$ | $64.4 \pm 3.1$ | $62.5 \pm 3.5$ | $70.4 \pm 3.7$ |
| 75 | $43.2 \pm 4.7$ | $23.3 \pm 4.7$ | $46.0 \pm 4.5$ | $73.4 \pm 2.8$ | $71.6 \pm 3.8$ | $76.7 \pm 2.6$ | $75.0 \pm 3.3$ | $74.6 \pm 3.3$ |
| 100 | $63.0 \pm 3.0$ | $25.9 \pm 7.4$ | $66.4 \pm 5.2$ | $77.8 \pm 2.0$ | $77.4 \pm 1.8$ | $82.7 \pm 1.5$ | $76.6 \pm 3.0$ | $73.7 \pm 2.9$ |
| 150 | $77.4 \pm 1.3$ | $34.1 \pm 1.5$ | $72.6 \pm 5.6$ | $82.2 \pm 1.7$ | $84.0 \pm 1.2$ | $85.8 \pm 1.4$ | $84.2 \pm 1.0$ | $79.7 \pm 3.5$ |
| 250 | $78.2 \pm 2.7$ | $44.2 \pm 4.0$ | $77.9 \pm 2.3$ | $85.0 \pm 2.8$ | $88.1 \pm 1.0$ | $87.8 \pm 1.7$ | $87.0 \pm 1.0$ | $78.3 \pm 5.0$ |</p>
<p>Table 2: Extended Batch RL results. Mean success ( $\pm$ standard error, estimated using 1000 bootstrap resamples) on Pong environment. All results average over 10 seeds, except Dyna, which uses 5.
(1-step not shown). The MBPO model is described below. We use the same reward model as CoDA to relabel rewards for the MBPO model, which only predicts next state.
5. MBPO + CoDA: as MBPO improved performance over the baseline (real data only) at lower dataset sizes, we considered using MBPO together with CoDA. We use the base dataset to train the MBPO, CoDA, and reward models, as described above. We then use the MBPO model to expand the base dataset by 2 x , as described above. We then use the CoDA model to expand the expanded dataset by 3 x the original dataset size. Thus the final dataset is 5 x as large as the original dataset ( 1 real : $1 \mathrm{MBPO}: 3 \mathrm{CoDA}$ ).</p>
<p>All configurations alter only the training dataset, and the same agent architecture/hyperparameters (reported above) are used in each case.</p>
<p>MBPO model Since using the CoDA model for Dyna harms rather than helps, we consider using a stronger, state-of-the-art model-based approach. In particular, we adopt the model used by ModelBased Policy Optimization [34]. This model consists of a size 7 ensemble of neural networks, each with 4 layers of 200 neurons. We use ReLU activations, Adam optimizer [38] with weight decay of $5 \mathrm{e}-5$, and have each network output a the mean and (log) diagonal covariance of a multi-variate Gaussian. We train the networks with a maximum likelihood loss. To sample from the model, we choose an ensemble member uniformly at random and sample from its output distribution, as in [34].</p>
<p>Full results See Table 2 for results.</p>
<h1>B. 3 Goal-conditioned RL</h1>
<p>Here we detail the procedure used in the Goal-conditioned RL experiments on the Fetchpush-v1 and Slide2 environments.</p>
<p>Implementation This experiment uses the same codebase as our Batch RL, which provides state-of-the-art baseline HER agents and will be released with the paper.</p>
<p>Hyperparameters For Fetchpush-v1 we use the default hyperparameters from the codebase, which outperform the original HER agents of $[1,71]$ and follow-up works. We do not tune the CoDA agent (but see additional CoDA hyperparameters below). They are as follows:</p>
<ul>
<li>Off-policy algorithm: DDPG [51]</li>
<li>Hindsight relabeling strategy: futureactual_2_2 [68], using exclusively future [1] relabeling for the first 25,000 steps</li>
<li>Optimizer: Adam [38] with default hyperparameters</li>
<li>Batch size: 2000</li>
<li>
<p>Optimization frequency: 1 optimization step every 2 environment steps after the 5000th environment step</p>
</li>
<li>
<p>Target network updates: update every 10 optimization steps with a Polyak averaging coefficient of 0.05</p>
</li>
<li>Discount factor: 0.98</li>
<li>Action 12 regularization: 0.01</li>
<li>Networks: 3x512 layer-normalized [4] hidden layers with ReLU activations</li>
<li>Target clipping: $(-50,0)$</li>
<li>Action noise: 0.1 Gaussian noise</li>
<li>Epsilon exploration [71]: 0.2, with an initial 100\% exploration period of 10,000 steps</li>
<li>Observation normalization: yes</li>
<li>Buffer size: 1M</li>
</ul>
<p>On Slide2 we tried to tune the baseline hyperparameters somewhat, but note that this is a fairly long experiment ( 10 M timesteps) and so only a few settings were tested due to constraints. In particular, we considered the following modifications:</p>
<ul>
<li>Expanding the replay buffer to 2 M (effective)</li>
<li>Reducing the batch size to 1000 (effective)* (used for results)</li>
<li>Using the future_4 strategy (agent fails to learn in 10M steps)</li>
<li>Reducing optimization step frequency to 1 step every 4 environment steps (about the same performance)</li>
</ul>
<p>We tried similar adjustments to our CoDA agent, but found the default hyperparameters (used for results) performed well. We found that the CoDA agent outperforms the base HER agent on all tested settings.</p>
<p>For CoDA, we used the following additional hyperparameters:</p>
<ul>
<li>CoDA buffer size: 3M</li>
<li>Make CoDA data every: 250 environment steps</li>
<li>Number of source pairs from replay buffer used to make CoDA data: 2000</li>
<li>Number of CoDA samples per source pair: 2</li>
<li>Maximum ratio of CoDA:Real data to train on: 3:1</li>
</ul>
<p>Environment On FetchPush-v1 the standard state features include the relative position of the object and gripper, which entangles the two. While this could be dealt with by dynamic relabeling (as used for HER's reward), we simply drop the corresponding features from the state.
Slide2 has two pucks that slide on a table and bounce off of a solid railing. Observations are 40-dimensional (including the 6-dimensional goal), and actions are 4-dimensional. Initial positions and goal positions are sampled randomly on the table. During training, the agent gets a sparse reward of 0 (otherwise -1) if both pucks are within 5 cm of their ordered target. At test time we count success as having both picks within 7.5 cm of the target on the last step of the episode. Episodes last 75 steps and there is no done signal (this is intended as a continuous task).
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 7: The Slide2 environment.</p>
<p>CoDA Heuristic For these experiments we use a hand-coded heuristic designed with domain knowledge. In particular, we assert that the action is always entangled with the gripper, and that gripper/action and objects (pucks or blocks) are disentangled whenever they are more than 10 cm apart. This encodes independence due to physical separation, which we hypothesize is a very generally heuristic that humans implicitly rely on all the time. The pucks have a radius of 2.5 cm and height of 4 cm , and the blocks are $5 \mathrm{~cm} \times 5 \mathrm{~cm} \times 5 \mathrm{~cm}$, so this heuristic is quite generous / suboptimal. Despite being suboptimal, it demonstrates the ease with which domain knowledge can be injected via the CoDA mask: we need only a high precision (low false positive rate) heuristic-the recall is not as important. It is likely that an agent could learn a better mask that also takes into account velocity.</p>
<p>Results plot The plot shows mean $\pm 1$ standard deviation of the smoothed data over 5 seeds.</p>
<h1>C Inferring Local Factorization</h1>
<p>Here we present several approaches to inferring the local factorization of subspaces, a crucial subroutine of CoDA. We note that in many cases where domain knowledge is available, simple heuristics may suffice, e.g. in our Goal-conditioned RL experiments discussed in Section 4 where a simple distance-based indicator function in the state space was used. However, as such heuristics may not be universally available, the question of whether data-driven approaches can successfully infer local factorization is of general interest. We note that the performance of CoDA will improve alongside future improvements in this inference task (motivating future work in this area), and that inferring the local factorization in general is an easier task than learning the environment dynamics.
We begin by presenting two methods for inferring local factorization, derived from variants of a next-state prediction task, which we here refer to as SANDy for Sparse Attention Neural Dynamics. To verify the merits of SANDy in the local factorization inference task, we evaluate two SANDy variants in controlled settings where the ground truth factorization is known: first in a synthetic Markov process (MDP without actions), and second in Spriteworld. This label information is used only to evaluate performance, and not to train the SANDy parameters. The SANDy-Transformer model was used for the Online RL experiments presented in Section 4.</p>
<h2>C. 1 Methods</h2>
<p>We propose two Sparse Attention for Neural Dynamics (SANDy) models. In each case we seek to learn a function (or mask) $M(s, a) \rightarrow{0,1}^{(|S|+|A|) \times|S|}$ whose output represents the adjacency matrix of the local causal graph, conditioned on the state and action. We note that $M(s, a)<em k="k">{i, j}=0$ alone is insufficient, in general, to determine the local subspace $\mathcal{L} \subset \mathcal{S} \times \mathcal{A}$, since there may be multiple disconnected subspaces $\mathcal{L}</em>(s, a)}$ with $M_{k<em k="k">{i, j}=0$ whose union $\bigcup</em>} \mathcal{L<em _=";" k="k">{k}$ has $M,</em>=1$. This can be resolved by our Proposition 1 if we also force the relevant structural equations to be the same. For now, we assume the mask determines the local subspace, and leave exploration of this possibility to future work. Empirically, we will see that our assumption is reasonable.}(s, a)_{i, j</p>
<p>SANDy-Mixture The first model is a mixture-of-MLPs model with an attention mechanism that is computed from the current state. Each component of the mixture is a neural dynamics model with sparse local dependencies. For a given component, the key idea is to train a neural dynamics model to predict the next state $h\left(s_{t}, a_{t}\right) \approx s_{t+1}$ and approximate the masking function by thresholding the (transpose of the) network Jacobian of $h,[\mathbf{J}(s, a)]<em j="j">{i, j}=\frac{\delta}{\delta\left[s, a\right]</em>$ as providing the first-order element-wise dependencies between the predicted next state and the network input. We then derive the local factorization by thresholding the absolute Jacobian}}[h(s, a)]_{i}$. Intuitively, we can think of $\mathbf{J</p>
<p>$$
M_{\tau}(s, a)=\mathbb{1}(|\mathbf{J}(s, a)|&gt;\tau)
$$</p>
<p>where $\mathbb{1}(\cdot)$ represents the indicator function and $\tau$ is a threshold hyperparameter.
To estimate the network Jacobian, we note that for standard activation functions (sigmoid, tanh, relu), it can be bound from above by the matrix product of its weight matrices. To see this, let $h_{\theta}$ be an $L$-layer MLP parameterized by $\theta=\left(\mathbf{W}^{(1)}, \mathbf{b}^{(1)}, \cdots, \mathbf{W}^{(L)}, \mathbf{b}^{(L)}\right)$ with activation $\sigma$ with bounded derivative $\sigma^{\prime}(x) \leq 1$, and note that for each layer $\boldsymbol{h}^{(\ell)}=\sigma\left(\mathbf{W}^{(\ell)} \boldsymbol{h}^{(\ell-1)}+\mathbf{b}^{(\ell)}\right)$ we have:</p>
<p>$$
\left|\frac{d \boldsymbol{h}<em i="i">{j}^{(\ell)}}{d \boldsymbol{h}</em>\right|
$$}^{(\ell-1)}}\right| \leq\left|\mathbf{W}_{j i}^{(\ell)</p>
<p>Then, using the chain rule, triangle inequality, and the identity $|a b|=|a||b|$, we can compute:</p>
<p>$$
\begin{aligned}
\left|\frac{d \boldsymbol{h}<em i="i">{j}^{(\ell)}}{d \boldsymbol{h}</em>}^{(\ell-2)}}\right| &amp; \leq\left|\frac{d}{d \boldsymbol{h<em _cdot="\cdot" j="j">{i}^{(\ell-2)}} \mathbf{W}</em>}^{(\ell)} \boldsymbol{h}^{(\ell-1)}\right|=\left|\mathbf{W<em i="i">{j \cdot}^{(\ell)} \cdot \frac{d \boldsymbol{h}^{(\ell-1)}}{d \boldsymbol{h}</em>\right| \
&amp; \leq \sum_{k}\left|\mathbf{W}}^{(\ell-2)}<em k="k">{j k}^{(\ell)} \frac{d \boldsymbol{h}</em>}^{(\ell-1)}}{d \boldsymbol{h<em _cdot="\cdot" j="j">{i}^{(\ell-2)}}\right| \leq\left|\mathbf{W}</em>\right|
\end{aligned}
$$}^{(\ell)}\right| \cdot\left|\mathbf{W}_{. i}^{(\ell-1)</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ Note that these methods were proposed to efficiently encode conditional probability tables at the graph nodes, requiring that all variables considered be discrete; CoDA on the other hand works in continuous tasks.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
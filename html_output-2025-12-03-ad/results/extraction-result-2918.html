<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2918 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2918</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2918</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-72.html">extraction-schema-72</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-276774869</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.02197v1.pdf" target="_blank">ATLaS: Agent Tuning via Learning Critical Steps</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Model (LLM) agents have demonstrated remarkable generalization capabilities across multi-domain tasks. Existing agent tuning approaches typically employ supervised finetuning on entire expert trajectories. However, behavior-cloning of full trajectories can introduce expert bias and weaken generalization to states not covered by the expert data. Additionally, critical steps, such as planning, complex reasoning for intermediate subtasks, and strategic decision-making, are essential to success in agent tasks, so learning these steps is the key to improving LLM agents. For more effective and efficient agent tuning, we propose ATLaS that identifies the critical steps in expert trajectories and finetunes LLMs solely on these steps with reduced costs. By steering the training's focus to a few critical steps, our method mitigates the risk of overfitting entire trajectories and promotes generalization across different environments and tasks. In extensive experiments, an LLM finetuned on only 30% critical steps selected by ATLaS outperforms the LLM finetuned on all steps and recent open-source LLM agents. ATLaS maintains and improves base LLM skills as generalist agents interacting with diverse environments.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2918.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2918.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (Reasoning and Acting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent framework that interleaves explicit language-model internal reasoning (thoughts) with environment actions, representing interaction as a trajectory of thoughts, actions, and observations; adopted as the interaction format for agents in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>At each timestep the LLM first produces an internal reasoning 'thought' (h_t) given the trajectory so far, then emits an action (a_t); the full trajectory τ = (h1,a1,o1,...,ht,at,ot) is kept in the prompt/context and updated each step. This paper adopts the ReAct format for its LLM agents and for both training and evaluation data.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td>Llama-3.1-8B-Instruct (primary backbone in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>ALFWorld, BabyAI, Maze, TextCraft, Jericho, SciWorld (and other held-in/hheld-out environments such as Wordle, WebShop listed in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>A collection of text-based and embodied text-simulated benchmarks (e.g., ALFWorld household tasks, BabyAI grid instruction-following, Maze navigation, TextCraft crafting, Jericho exploratory fiction games, SciWorld science experiments) that require multi-step reasoning, planning, observation, and action sequencing under partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working memory (prompt-context / trajectory history)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>No separate external memory module is introduced; memory is implemented implicitly by storing the full trajectory (thoughts h_t, actions a_t, and observations o_t) in the model's input context. At each step the trajectory is appended and presented to the LLM prompts (prompt_h and prompt_a). During ATLAS training the full trajectory remains as input but teacher-forcing loss is computed only on selected critical-step tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Direct inclusion in context / recency-based access via the model's attention over the context window (no separate retrieval/indexing mechanism described).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Bounded by model context window; training used max length 8192 tokens (context-limited).</td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Past internal thoughts (h_t), actions (a_t), and observations (o_t) constituting the trajectory; the paper keeps full trajectories as input though training loss is applied only to selected steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>The paper does not evaluate explicit memory modules; it uses the ReAct trajectory as the implicit working memory. A verification experiment ('start from critical step') suggests the base model can autonomously generate non-critical steps given the critical-step context, implying that the trajectory context suffices to supply non-critical information and training can focus on critical steps.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>No explicit memory architecture is studied; limitations are the context-window bound (8192 tokens) and lack of a retrieval/indexing mechanism. The paper notes selector dependence on powerful closed-source models but does not analyze memory-specific failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ATLaS: Agent Tuning via Learning Critical Steps', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2918.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2918.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inner Monologue</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inner Monologue (Embodied reasoning through planning with language models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior approach mentioned in related work that uses internal dialogue / internal chain-of-thought to support planning and decision-making for embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Inner monologue: Embodied reasoning through planning with language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Inner Monologue</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An approach that generates internal dialogues or structured internal reasoning traces to guide planning and actions; cited as related work demonstrating internal thought processes improve decision-making for embodied agents.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>General embodied/interactive tasks (referenced as prior work), not specifically run in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Embodied tasks and planning problems where internal deliberation (monologue) helps produce multi-step plans and actions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Mentioned as related work showing the benefit of internal dialogues for planning; this paper does not analyze or reuse Inner Monologue's memory mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ATLaS: Agent Tuning via Learning Critical Steps', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2918.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2918.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MC-DML</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MC-DML (Monte Carlo planning with LLMs for text-based games)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recently-cited work that combines large language models with Monte Carlo planning applied to text-based decision-making; mentioned in related work but not used in experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Monte carlo planning with large language model for text-based game agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MC-DML</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Combines LLM policy generation with Monte Carlo planning procedures to improve action selection in text-based games (described in related work). The cited description positions it as an approach to improve dynamic decision-making in text games via planning.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>Text-based games (general reference; not evaluated in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Text-based interactive games requiring exploration and planning (e.g., Jericho-style games); the cited work focuses on planning-enhanced LLM decision-making in such environments.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Mentioned as complementary prior work that combines planning and LLMs for text games; the ATLAS paper does not detail MC-DML's memory mechanisms nor compare to them.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ATLaS: Agent Tuning via Learning Critical Steps', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2918.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2918.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neurosymbolic integration (Fang et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models are neurosymbolic reasoners (integration of LLMs with external symbolic modules)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related work that integrates LLMs with external symbolic modules for zero-shot symbolic reasoning in text-based games; cited as demonstrating improved symbolic reasoning by coupling LLMs with external modules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are neurosymbolic reasoners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Neurosymbolic integration (Fang et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An approach integrating LLMs with external symbolic modules to perform zero-shot symbolic reasoning in text-based game settings; mentioned as a related technique for improving reasoning/planning in interactive environments.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>Text-based games (zero-shot symbolic reasoning tasks referenced), not evaluated in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Text-based interactive environments requiring symbolic reasoning and manipulation; the cited work addresses zero-shot symbolic reasoning by augmenting LLMs with symbolic modules.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Cited as evidence that integrating external symbolic modules can aid LLM agents in text games, but the ATLAS paper does not evaluate memory-specific mechanisms from this work.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ATLaS: Agent Tuning via Learning Critical Steps', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Inner monologue: Embodied reasoning through planning with language models <em>(Rating: 2)</em></li>
                <li>Monte carlo planning with large language model for text-based game agents <em>(Rating: 2)</em></li>
                <li>Large language models are neurosymbolic reasoners <em>(Rating: 2)</em></li>
                <li>ALFWorld: Aligning text and embodied environments for interactive learning <em>(Rating: 1)</em></li>
                <li>Agentgym: Evolving large language model-based agents across diverse environments <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2918",
    "paper_id": "paper-276774869",
    "extraction_schema_id": "extraction-schema-72",
    "extracted_data": [
        {
            "name_short": "ReAct",
            "name_full": "ReAct (Reasoning and Acting)",
            "brief_description": "An agent framework that interleaves explicit language-model internal reasoning (thoughts) with environment actions, representing interaction as a trajectory of thoughts, actions, and observations; adopted as the interaction format for agents in this paper.",
            "citation_title": "ReAct: Synergizing reasoning and acting in language models",
            "mention_or_use": "use",
            "agent_name": "ReAct",
            "agent_description": "At each timestep the LLM first produces an internal reasoning 'thought' (h_t) given the trajectory so far, then emits an action (a_t); the full trajectory τ = (h1,a1,o1,...,ht,at,ot) is kept in the prompt/context and updated each step. This paper adopts the ReAct format for its LLM agents and for both training and evaluation data.",
            "base_llm_model": "Llama-3.1-8B-Instruct (primary backbone in experiments)",
            "base_llm_size": "8B",
            "text_game_name": "ALFWorld, BabyAI, Maze, TextCraft, Jericho, SciWorld (and other held-in/hheld-out environments such as Wordle, WebShop listed in experiments)",
            "text_game_description": "A collection of text-based and embodied text-simulated benchmarks (e.g., ALFWorld household tasks, BabyAI grid instruction-following, Maze navigation, TextCraft crafting, Jericho exploratory fiction games, SciWorld science experiments) that require multi-step reasoning, planning, observation, and action sequencing under partial observability.",
            "uses_memory": true,
            "memory_type": "working memory (prompt-context / trajectory history)",
            "memory_architecture": "No separate external memory module is introduced; memory is implemented implicitly by storing the full trajectory (thoughts h_t, actions a_t, and observations o_t) in the model's input context. At each step the trajectory is appended and presented to the LLM prompts (prompt_h and prompt_a). During ATLAS training the full trajectory remains as input but teacher-forcing loss is computed only on selected critical-step tokens.",
            "memory_retrieval_mechanism": "Direct inclusion in context / recency-based access via the model's attention over the context window (no separate retrieval/indexing mechanism described).",
            "memory_capacity": "Bounded by model context window; training used max length 8192 tokens (context-limited).",
            "what_is_stored_in_memory": "Past internal thoughts (h_t), actions (a_t), and observations (o_t) constituting the trajectory; the paper keeps full trajectories as input though training loss is applied only to selected steps.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": false,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "The paper does not evaluate explicit memory modules; it uses the ReAct trajectory as the implicit working memory. A verification experiment ('start from critical step') suggests the base model can autonomously generate non-critical steps given the critical-step context, implying that the trajectory context suffices to supply non-critical information and training can focus on critical steps.",
            "memory_limitations": "No explicit memory architecture is studied; limitations are the context-window bound (8192 tokens) and lack of a retrieval/indexing mechanism. The paper notes selector dependence on powerful closed-source models but does not analyze memory-specific failure modes.",
            "comparison_with_other_memory_types": null,
            "uuid": "e2918.0",
            "source_info": {
                "paper_title": "ATLaS: Agent Tuning via Learning Critical Steps",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Inner Monologue",
            "name_full": "Inner Monologue (Embodied reasoning through planning with language models)",
            "brief_description": "A prior approach mentioned in related work that uses internal dialogue / internal chain-of-thought to support planning and decision-making for embodied tasks.",
            "citation_title": "Inner monologue: Embodied reasoning through planning with language models",
            "mention_or_use": "mention",
            "agent_name": "Inner Monologue",
            "agent_description": "An approach that generates internal dialogues or structured internal reasoning traces to guide planning and actions; cited as related work demonstrating internal thought processes improve decision-making for embodied agents.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": "General embodied/interactive tasks (referenced as prior work), not specifically run in this paper",
            "text_game_description": "Embodied tasks and planning problems where internal deliberation (monologue) helps produce multi-step plans and actions.",
            "uses_memory": null,
            "memory_type": null,
            "memory_architecture": null,
            "memory_retrieval_mechanism": null,
            "memory_capacity": null,
            "what_is_stored_in_memory": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Mentioned as related work showing the benefit of internal dialogues for planning; this paper does not analyze or reuse Inner Monologue's memory mechanisms.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "uuid": "e2918.1",
            "source_info": {
                "paper_title": "ATLaS: Agent Tuning via Learning Critical Steps",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "MC-DML",
            "name_full": "MC-DML (Monte Carlo planning with LLMs for text-based games)",
            "brief_description": "A recently-cited work that combines large language models with Monte Carlo planning applied to text-based decision-making; mentioned in related work but not used in experiments here.",
            "citation_title": "Monte carlo planning with large language model for text-based game agents",
            "mention_or_use": "mention",
            "agent_name": "MC-DML",
            "agent_description": "Combines LLM policy generation with Monte Carlo planning procedures to improve action selection in text-based games (described in related work). The cited description positions it as an approach to improve dynamic decision-making in text games via planning.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": "Text-based games (general reference; not evaluated in this paper)",
            "text_game_description": "Text-based interactive games requiring exploration and planning (e.g., Jericho-style games); the cited work focuses on planning-enhanced LLM decision-making in such environments.",
            "uses_memory": null,
            "memory_type": null,
            "memory_architecture": null,
            "memory_retrieval_mechanism": null,
            "memory_capacity": null,
            "what_is_stored_in_memory": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Mentioned as complementary prior work that combines planning and LLMs for text games; the ATLAS paper does not detail MC-DML's memory mechanisms nor compare to them.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "uuid": "e2918.2",
            "source_info": {
                "paper_title": "ATLaS: Agent Tuning via Learning Critical Steps",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Neurosymbolic integration (Fang et al.)",
            "name_full": "Large language models are neurosymbolic reasoners (integration of LLMs with external symbolic modules)",
            "brief_description": "Related work that integrates LLMs with external symbolic modules for zero-shot symbolic reasoning in text-based games; cited as demonstrating improved symbolic reasoning by coupling LLMs with external modules.",
            "citation_title": "Large language models are neurosymbolic reasoners",
            "mention_or_use": "mention",
            "agent_name": "Neurosymbolic integration (Fang et al.)",
            "agent_description": "An approach integrating LLMs with external symbolic modules to perform zero-shot symbolic reasoning in text-based game settings; mentioned as a related technique for improving reasoning/planning in interactive environments.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": "Text-based games (zero-shot symbolic reasoning tasks referenced), not evaluated in this paper",
            "text_game_description": "Text-based interactive environments requiring symbolic reasoning and manipulation; the cited work addresses zero-shot symbolic reasoning by augmenting LLMs with symbolic modules.",
            "uses_memory": null,
            "memory_type": null,
            "memory_architecture": null,
            "memory_retrieval_mechanism": null,
            "memory_capacity": null,
            "what_is_stored_in_memory": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Cited as evidence that integrating external symbolic modules can aid LLM agents in text games, but the ATLAS paper does not evaluate memory-specific mechanisms from this work.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "uuid": "e2918.3",
            "source_info": {
                "paper_title": "ATLaS: Agent Tuning via Learning Critical Steps",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Inner monologue: Embodied reasoning through planning with language models",
            "rating": 2,
            "sanitized_title": "inner_monologue_embodied_reasoning_through_planning_with_language_models"
        },
        {
            "paper_title": "Monte carlo planning with large language model for text-based game agents",
            "rating": 2,
            "sanitized_title": "monte_carlo_planning_with_large_language_model_for_textbased_game_agents"
        },
        {
            "paper_title": "Large language models are neurosymbolic reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_neurosymbolic_reasoners"
        },
        {
            "paper_title": "ALFWorld: Aligning text and embodied environments for interactive learning",
            "rating": 1,
            "sanitized_title": "alfworld_aligning_text_and_embodied_environments_for_interactive_learning"
        },
        {
            "paper_title": "Agentgym: Evolving large language model-based agents across diverse environments",
            "rating": 1,
            "sanitized_title": "agentgym_evolving_large_language_modelbased_agents_across_diverse_environments"
        }
    ],
    "cost": 0.01465125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ATLAS: Agent Tuning via Learning Critical Steps
5 Jun 2025</p>
<p>Zhixun Chen zhixun.chen@student.uts.edu.au 
University of Technology Sydney</p>
<p>Ming Li minglii@umd.edu 
University of Maryland</p>
<p>Yuxuan Huang yuxuan.huang@liverpool.ac.uk 
University of Liverpool</p>
<p>Yali Du yali.du@kcl.ac.uk 
King's College London</p>
<p>Meng Fang meng.fang@liverpool.ac.uk 
University of Liverpool</p>
<p>Tianyi Zhou tianyi@umd.edu 
University of Maryland</p>
<p>ATLAS: Agent Tuning via Learning Critical Steps
5 Jun 2025818863E1D0E1B483CC9A6FAE2FC20AEFarXiv:2503.02197v2[cs.CL]
Large Language Model (LLM) agents have demonstrated remarkable generalization capabilities across multi-domain tasks.Existing agent tuning approaches typically employ supervised finetuning on entire expert trajectories.However, behavior-cloning of full trajectories can introduce expert bias and weaken generalization to states not covered by the expert data.Additionally, critical steps-such as planning, complex reasoning for intermediate subtasks, and strategic decision-making-are essential to success in agent tasks, so learning these steps is the key to improving LLM agents.For more effective and efficient agent tuning, we propose ATLAS that identifies the critical steps in expert trajectories and finetunes LLMs solely on these steps with reduced costs.By steering the training's focus to a few critical steps, our method mitigates the risk of overfitting entire trajectories and promotes generalization across different environments and tasks.In extensive experiments, an LLM finetuned on only ∼30% critical steps selected by ATLAS outperforms the LLM finetuned on all steps and recent open-source LLM agents.ATLAS maintains and improves base LLM skills as generalist agents interacting with diverse environments.</p>
<p>Introduction</p>
<p>An intelligent agent can perceive its environment, process information, and take actions to achieve specific goals or objectives (Maes, 1995;Wooldridge and Jennings, 1995).Traditional AI agents, especially empowered by Monte Carlo Tree Search (MCTS) or Reinforcement learning, have shown great potential in complex and difficult tasks (Silver et al., 2017;Vinyals et al., 2019).However, these methods are task-specific and suffer from weak generalization across different task domains or environments.On the other hand, the recent rise of Large Language Models (LLMs) (Brown et al., 2020;Team, 2024a;Touvron et al.,   overfitting to expert trajectories and training inefficiency.Most existing agent tuning methods rely on imitation of every step of expert trajectories (Chen et al., 2023;Zeng et al., 2023;Chen et al., 2024;Zhang et al., 2024;Xi et al., 2024;Song et al., 2024).For instance, FireAct (Chen et al., 2023) generates diverse expert trajectories to fine-tune LLMs with enhanced planning abilities, while AgentTuning (Zeng et al., 2023) combines expert trajectories from various interactive environments with general instruction datasets to improve agents' generalization capability.</p>
<p>However, fine-tuning on the entire expert trajectories can introduce expert bias, causing the model to overfit to specific behaviors and diminishing its ability to generalize to unseen environments (Ghosh et al., 2024).While experts in a few critical steps indeed provide key supervised information to improve the agent, the agent may already excel in other non-critical steps.Additionally, fitting the full trajectories of one task may lead to declines in others due to the potential distribution gap and negative transfer between tasks (Song et al., 2024).Moreover, imitating expert trajectories on redundant steps or sub-optimal/replaceable actions incurs excessive training costs.</p>
<p>To address the above challenges, we propose AT-LAS, Agent Tuning via Learning Critical Steps, a novel approach that identifies the critical steps within expert trajectories and only finetunes LLMs on those selected steps, which is illustrated in Figure 1.By steering the training's focus to a few critical steps, we not only reduce the backpropagation cost for training but also lower the overfitting risk to whole trajectories.Moreover, imitation on partial trajectories mitigates the expert bias, and encourages generalization across environments and tasks.</p>
<p>In ATLAS, an oracle LLM (GPT4o by default) is utilized as the selector to identify the critical steps semantically based on four criteria: key observations, plan formulation, recalling prior information, and pivotal actions.These critical steps are essential to the success of downstream tasks, but they are usually challenging to be generated by the base LLMs.Our training focuses on finetuning base models on these selected steps.Our experiments provide extensive comparisons between ATLAS and other agent-tuning strategies or LLM agents.As indicated by the results in Figure 2, ATLAS-finetuned agent maintains and improves the generalization capabilities of base LLMs in diverse environments, excelling on both held-in and held-out tasks.Our main contributions can be summarized as:</p>
<p>• We introduce a novel method ATLAS reducing the tokens for agent tuning to 30%, by selecting the critical steps in expert trajectories.</p>
<p>• Agents finetuned on 30% critical steps outperforms the baseline agents finetuned on 100% steps, especially in the multi-task learning scenario, mitigating the expert bias and negative transfer across tasks.</p>
<p>• ATLAS-finetuned agent achieves a better generalization capability than baseline agents on not only held-in but also held-out tasks.</p>
<p>Preliminaries</p>
<p>As a multi-task learning framework, we define a set of N environments as E. For any particular environment E ∈ E and a task prompt space I, the agent's dynamics is formalized as a Partially Observable Markov Decision Process (POMDP) (Sutton, 2018) M E,I ≜ (S, A, O, T, r).In this formalization, S denotes the state space, A denotes the action space, O denotes the observation space, T : S × A → S represents the state transition function, and r : S × A → R defines the reward function.</p>
<p>Upon receiving an instruction i ∈ I in environment E, the agent takes action a t ∼ π θ (•|o t ; E, i) according to its policy π θ , where θ represents the policy parameters.For LLM agents, π θ is an LLM, and its output is not limited to actions and depends on the prompt.The environment then transitions to a new state s t+1 ∈ S based on the action, and the agent receives a new observation o t+1 ∈ O.This interaction continues as the agent engages with the environment until the task is completed or the maximum step limit is reached.We adopt the ReAct framework (Yao et al., 2022b) for the LLM agent: at each step t, the agent first generates reasoning thought h t with a think prompt prompt h .Define the trajectory up to step-t as
τ 1:t ≜ (h 1 , a 1 , o 1 , . . . , h t , a t , o t ),(1)
The thought h t is sampled by
h t ∼ π θ (•|τ 1:t−1 ; prompt h , E, i). (2)
The action is then drawn from the same model with another prompt prompt a , i.e.,
a t ∼ π θ (•|τ 1:t−1 , h t ; prompt a , E, i).(3)
The return of trajectory τ = τ 1:T , G(τ ) = T t=1 γ t r t with a discounting factor γ, reflects the agent's performance in task i within environment E. We collect a dataset D of expert trajectories for all environments in E and tasks in I.</p>
<p>Methodology</p>
<p>In this section, we introduce our ATLAS, which identifies critical steps of the expert trajectories and finetunes LLM agents solely on them.</p>
<p>Definition of Critical Steps</p>
<p>Critical steps are key actions or states within an agent's trajectory that have a substantial impact on the return G(τ ).As depicted in Figure 3, these steps are essential for ensuring the success of the trajectory.We characterize critical steps as those requiring precise decision-making and flawless execution by the agent.In contrast, non-critical steps are more flexible, allowing for adjustments or rearrangements without compromising the overall trajectory.Typically, critical steps are challenging for the base model's policy to generate, whereas non-critical steps are relatively easy for the base model.To identify these steps, we sample expert trajectories with dense rewards in available environments.We then empirically categorize four types of critical steps: Plan Creation, Critical Observation, Critical Action, and Self Correction.</p>
<p>Selection of Critical Steps</p>
<p>Properly identifying and extracting critical steps from a trajectory is essential for ensuring effective analysis and learning.However, unlike instancelevel data selection for LLMs, which has been explored (Li et al., 2024d,c), step-level selection is challenging since the task-related critical steps can differ dramatically by their semantic meaning.To address this issue, we leverage the selector to identify critical steps based on the critical step identification prompt, prompt c .This approach ensures a more nuanced and context-aware assessment, helping to accurately identify the most influential steps for optimal performance.</p>
<p>The prompt initially defines the critical steps with the four categories introduced above:</p>
<p>• Plan Creation: Steps where the LLM agent formulates sub-goals by analyzing previous observations and considering the final objective, breaking down the larger goal into manageable tasks that guide the agent's actions toward the overall outcome.</p>
<p>• Critical Observation: Steps where the LLM agent identifies and analyzes key information from the environment, which helps the agent understand the objective or state and refine its strategy and decision-making toward more effective outcomes.</p>
<p>• Critical Action: Steps where the LLM agent takes decisive and impactful actions based on prior observations, significantly advancing the process toward the final objectives.These actions are crucial in shaping the direction of the agent's strategy and are often pivotal moments that determine progress or failure, ensuring that the agent remains on track to achieve the desired outcome.</p>
<p>• Self Correction: Steps where the LLM agent carefully recalls and assesses its previous actions or decisions, especially after encountering failure or suboptimal outcomes.During this process, the agent reflects on what went wrong, identifies areas for improvement, and adjusts its approach to enhance future performance, which helps the agent refine its decision-making and better align with the overall objective.</p>
<p>After defining critical steps, prompt c guides the selector to comprehend the given trajectory and summarize a high-level plan with sub-goals of the trajectory to enhance its understanding of the trajectory.Based on the sub-goals, the selector identifies the critical steps and categorizes them into four predefined categories.Notably, we require the selector to select at most m percentage of steps in the trajectory and return their indices as
C τ = selector(τ ; prompt c , m)(4)
ATLAS does not ignore all other steps in the trajectory during training.Instead, we keep the whole original trajectory, including the non-critical steps, as the input sequence but do not compute teacher-force loss on their tokens during training.</p>
<p>Critical Step Identification Prompts</p>
<ol>
<li>
<p>Generate a high-level plan or strategy from the expert conversation, summarizing the key sub-goals required to complete the task.</p>
</li>
<li>
<p>Determine the most critical action steps in the expert conversation based on the plan and sub-goals, ensuring the number of selected steps does not exceed {m% × length(τ )}.</p>
</li>
<li>
<p>Justify your selection of these critical steps, specifying the category to which each step belongs.</p>
</li>
</ol>
<p>The full prompts are provided in Appendix D.1.</p>
<p>Agent Tuning on Critical Steps</p>
<p>Using the critical step dataset D c , we optimize the model by calculating the loss exclusively on these critical steps.Training the agent on this reduced set of essential steps enhances both the efficiency and effectiveness of the fine-tuning process, ensuring the model focuses on the key actions that contribute to task success and gains knowledge rather than merely mimicking expert behavior.Thus, the objective function can be defined as:
J (θ) = E τ ∼D t∈Cτ log π θ (h t |τ 1:t−1 ; prompt h , E, i)+ log π θ (a t |τ 1:t−1 , h t ; prompt a , E, i) ,
where θ denotes the trainable parameter of the LLM agent.This expectation-based objective ensures that the model maximizes probabilities at the critical steps that most influence task success, following the same reasoning as in the full-trajectory approach but applied selectively to D c .</p>
<p>Experiments</p>
<p>Environments</p>
<p>In our experimental framework, environments are categorized into held-in and held-out sets to evaluate the model's performance.Held-in environments refer to the set of environments that are included in the training dataset.These are the environments in which the models can be fine-tuned, allowing them to learn and adapt their behavior based on the provided expert trajectories.The performance of the held-in environments measures how well the model has learned from the training data and its ability to replicate or improve upon expert behaviors within familiar settings.On the other hand, held-out environments consist of environments and tasks that are excluded from the training process.These environments are used to assess the model's generalization capabilities, determining how effectively it can apply learned knowledge to novel and unseen scenarios.The performance of held-out environments indicates the model's ability to generalize its decision-making and problemsolving skills beyond the specific examples it was trained on, highlighting its potential for adaptability in diverse and dynamic real-world applications.</p>
<p>Specifically, our held-in environments include web navigation task Webshop (Yao et al., 2022a); textual digital games Textcraft (Prasad et al., 2023), Wordle and Maze (Abdulhai et al., 2023); embodied games Alfworld (Shridhar et al., 2020), Scienceworld (Wang et al., 2022) and Babyai (Chevalier-Boisvert et al., 2018); tool using tasks Weather, Todo and Movie (Ma et al., 2024).It is important to note that some environments provide a dense reward r ∈ [0, 1], while others offer only sparse rewards r ∈ {0, 1}.To maintain simplicity and consistency, we follow the evaluation matrix definition used in previous work (Singh et al., 2024;Xi et al., 2024) across all tasks.For the held-out environments, we apply two tool usage environments Sheet and Academic, and two game environments Jericho and PDDL (Ma et al., 2024).A detailed description of these environments can be found in Appendix A.</p>
<p>Implementation Details</p>
<p>We apply the open-sourced dataset AgentTraj-L1 (Xi et al., 2024) as our unfiltered dataset D o , which contains the expert trajectories of all held-in tasks mentioned above.We then filter the dataset to obtain the critical step dataset D c by prompting GPT-4o with the selection instruction I c .For more details, please refer to Appendix D.1.</p>
<p>We employ Llama-3.1-8B-Instruct(Dubey et al., 2024) as the backbone model for our agent unless specified.During training, we focus on the critical step D c , computing loss exclusively at these pivotal steps in the trajectory.We set the maximum selection ratio m for the critical step to be 30%.More information about the training setup is provided in Appendix B. The evaluation prompts are based on the React format (Yao et al., 2022b), consistent with the structure of the training data.</p>
<p>Baselines</p>
<p>We compare our LLM agent against three types of baselines: closed-source models, open-source models, and other fine-tuned LLM agents.Specifically, for closed-source models, we include GPT-3.5-Turbo(Ouyang et al., 2022), GPT-4-Turbo (Team, 2024a), Claude 3 (Anthropic, 2024), and DeepSpeek-Chat (Liu et al., 2024).For open-source models, we evaluate Llama3-8B-Instruct, Llama3.1-8B-Instruct(Dubey et al., 2024), Gemma-2-9b-it (Gemma Team et al., 2024), Phi-3mini-128k-instruct, Phi-3.5-mini-instruct(Team, 2024b), Mistral-7B-Instruct-v0.3 (Jiang et al., 2023) and Qwen2.5-7B-Instruct(Team, 2024c).For LLM agents fine-tuned on expert trajectories, we compare Xlam-7B-r (Zhang et al., 2024), AgentLM from AgentTuning (Zeng et al., 2023) and AgentEvol from AgentGym (Xi et al., 2024).We also fine-tune AgentTraj-L using the same base model, but with SFT on the unfiltered dataset D o as a baseline for a fair comparison.Additionally, we introduce another baseline where the base model's perplexity on all steps of the training dataset is used to select critical steps.Specifically, we choose the top 30% of steps with the highest perplexity and fine-tune on those steps.</p>
<p>Main Results</p>
<p>The results shown in Table 1 demonstrate the performance of various models on held-in and held-out tasks.For closed-source models, the reported outcomes follow those presented in AgentGym (Xi et al., 2024).On held-in tasks, our approach, finetuned on just 30% of the entire tokens of the expert trajectories, outperforms baseline methods across most tasks.It also achieves over a 5% average improvement compared to models fine-tuned with unfiltered data.Furthermore, in certain tasks such as Alfowrld and Babyai, our model surpasses some advanced closed-source models.The perplexitybased selection performs poorly because it solely measures the token generation difficulty of one step by the base model, which is affected by both the step's importance and expert bias.Consequently, using perplexity for selection forces the model to mimic the expert's distribution rather than effectively learning the task knowledge to solve the problem.</p>
<p>The results on held-out tasks demonstrate that fine-tuning exclusively on critical steps enhances the generalization of agents to unseen tasks.Focus- ing solely on these steps reduces the need to fully replicate the expert LLM's distribution, allowing agents to learn the knowledge of how to solve problems effectively.Unlike other approaches, such as those described in (Zeng et al., 2023), which mix a large amount of general instruction data with task-specific trajectory data to enhance generalization, our method achieves comparable performance without relying on such datasets.This underscores the effectiveness of critical step training in boosting agents' performance on held-out tasks.</p>
<p>To validate the effectiveness of our approach across various backbone models, we perform experiments using Mistral-7B- Instruct-v0.3 (Jiang et al., 2023) and Qwen2.5-7B-Instruct(Team, 2024c).The results, summarized in Table 2, demonstrate that our method consistently enhances performance across different models, yielding improvements on both held-in and held-out tasks.The detailed results are shown in Appendix C.</p>
<p>Ablation Study</p>
<p>Critical Steps vs. Non-Critical Steps</p>
<p>To assess the effectiveness of critical steps, we compare ATLAS with finetuning on non-critical steps not selected by the selector LLM in ATLAS.The number of steps for the non-critical selection dataset remains identical to our fine-tuning dataset, which is 30% of the entire training dataset.</p>
<p>The experimental results, presented in Table 2, indicate that fine-tuning with critical steps positively impacts the performance on both held-in and held-out tasks.In contrast, fine-tuning with non-critical steps results in a negative impact across all tasks.These findings suggest that certain steps contribute positively to the agent's learning by providing useful knowledge, whereas other steps introduce negative biases, leading to a decline in performance.</p>
<p>Different Selection Ratio</p>
<p>The ratio of critical steps plays a crucial role in the fine-tuning process.Therefore, we explore this hyperparameter and require the teacher model to select different proportions of steps from the expert trajectory as critical steps to identify the optimal ratio.As shown in Table 3, a 30% ratio yields the best agent performance on both held-in and heldout tasks.We also explore values of m greater than 30% but observed only a marginal increase in the number of critical steps selected by the selector.Consequently, we did not evaluate fine-tuning results for m larger than 30%.Motivated on these findings, we adopt m = 30%.More detailed results can be found in Appendix C.</p>
<p>Critical Steps vs. Random Steps</p>
<p>To further evaluate the efficacy of using a selector LLM for selecting critical steps, we conduct comparisons to randomly selected steps of different ratios.Specifically, we randomly draw 30%, 20%, and 10% steps from the original trajectories and compare them with 30%, 20%, and 10% critical steps for agent tuning.</p>
<p>The results are summarized in  to space constraints.Detailed results are provided in Appendix C. Our results indicate that increasing the ratio of random selection improves performance of the fine-tuned LLM agent on both held-in and held-out tasks.However, the random-selection's performance at all the three ratios remains inferior to that achieved on ATLAS's critical steps, particularly at higher ratios, highlighting the effectiveness of the selector.</p>
<p>Value Function defined Critical Steps</p>
<p>To further assess the accuracy of using GPT to identify critical steps, we perform experiments to compare datasets selected based on the estimated value function.Following (Xiong et al., 2024), we estimate the value of expert actions by traversing all actions along the trajectory N times.Specifically, the agent begins from a specific action in the expert trajectory and performs rollouts from that action N times.The final rewards from these N rollouts are summed and averaged to estimate the value of the action.Subsequently, the differences in value between consecutive steps are calculated, and steps with differences exceeding a predefined threshold are identified as critical.We set N = 5 for this experiment and a detailed methodology is provided in Appendix B.3.Notably, the estimated value function is designed for single-task training, so we compare the performance on the BabyAI and Weather tasks for evaluation These environments are chosen because some others randomly generate goals upon reset, making rollouts infeasible.Additionally, environments with long horizons are impractical due to the extensive time and computational resources required.For instance, the rollout of a dataset with 2000 trajectories, each averaging 25 steps, necessitates at least 6.5 × 10 5 inferences, which is highly inefficient.Table 4 shows that our method achieves performance comparable to the estimated value function approach.However, with limited rollouts, the estimates are often inaccurate, producing values of 0 at initial steps and 1 at later steps.As a result, only a few steps are identified as critical based on these variations.Despite this limitation, the comparable performance of the two methods suggests the potential for achieving better results with fewer critical steps, which we plan to explore in future work.</p>
<p>Choices of Selector LLMs</p>
<p>In this section, we evaluate different models for selecting critical steps.Specifically, we use the open-source model Llama3.1-70B-Instruct2as the comparison teacher model to identify critical steps on three tasks Alfworld, BabyAI and Weather.The results in Table 5 indicate that GPT4o selected dataset can better fine-tune the LLM agents on all three tasks, showing that the selector model's capability significantly impacts the quality of the critical step dataset.The dataset selected by Llama3.1-70B-Instructfails to accurately identify critical steps, instead including many non-critical steps, which in turn reduces the agents' performance after fine-tuning.</p>
<p>Critical Step Verification</p>
<p>We conduct an additional ablation study to evaluate the improvement in the base model's performance (LLaMA-3.1-8B-Instruct)when execution begins immediately after each identified critical step.In trajectories containing multiple critical steps, the model is reinitialized after each one in an iterative manner, ensuring that all critical steps are utilized throughout the trajectory.Experimental results on a representative subset of 100 tasks from the BabyAI and Maze environments reveal notable performance improvements when the model is initialized from critical steps, compared to start from the beginning of the task.This indicates that the base model is generally capable of generating all non-critical steps autonomously and that training efforts should prioritize learning from critical steps specifically.5 Related Work</p>
<p>LLM Agents</p>
<p>LLMs have demonstrated strong capabilities as general-purpose agents, effectively handling complex tasks across diverse environments without task-specific training.Several recent works explore different paradigms to enhance the reasoning, planning, and learning abilities of LLM-based agents.(Huang et al., 2022a) demonstrate that LLMs can act as zero-shot planners, enabling agents to perform complex tasks directly from natural language instructions.Building on this, Inner Monologue (Huang et al., 2022b) proposes using internal dialogues for decision-making and planning, further eliminating the need for external training.Similarly, AUTOACT (Qiao et al., 2024) shows that agents can learn to solve question-answering tasks through self-generated strategies and planning.ReST meets ReAct (Aksitov et al., 2023) introduces a self-improvement framework in which agents enhance their multi-step reasoning through iterative feedback.TextGrad (Yuksekgonul et al., 2024) leverages LLMs for automatic differentiation via textual reasoning, bypassing conventional training pipelines.</p>
<p>In the realm of interactive and social intelligence, SOTOPIA (Wang et al., 2024b) emphasizes realtime human-agent communication for acquiring socially intelligent behavior.MMAC-Copilot extends this by employing collaborative chains to improve LLM agents' performance in multimodal interaction scenarios.(Fang et al., 2024) demonstrate the effectiveness of integrating LLMs with external symbolic modules for zero-shot symbolic reasoning in text-based games.To improve agent performance in dynamic environments, MC-DML (Shi et al., 2025) combines LLMs with Monte Carlo planning for text-based decision-making.Lastly, HASARD (Tomilin et al., 2025) presents a visionbased safe reinforcement learning benchmark that challenges embodied agents with complex egocentric tasks, advancing the evaluation of safety in RL.</p>
<p>Tuning Agents</p>
<p>Fine-tuning large language models (LLMs) to enhance their performance in agent-related tasks has emerged as a key area of research, with various works proposing innovative frameworks, datasets, and methods.FireAct (Chen et al., 2023) finetunes models using various agent trajectories in the ReAct format, achieving substantial performance boosts.AgentTuning (Zeng et al., 2023) demonstrates the value of instruction tuning with datasets such as AgentInstruct, achieving superior task performance with its AgentLM models.AgentBank (Song et al., 2024) improves agent generalization and performance by fine-tuning LLMs with more than 50,000 diverse interaction trajectories.Agen-tOhana (Zhang et al., 2024) standardizes and aggregates agent trajectory datasets, facilitating efficient pipelines and providing state-of-the-art results with its xLAM-v0.1 model.AGENTGYM (Xi et al., 2024) enables agent self-evolution through the interactive AGENTEVOL framework, which reduces human supervision through environmental feedback.The IPR framework (Xiong et al., 2024) refines LLM agents with step-level feedback, demonstrating strong results on complex benchmarks.Above all, these works highlight the critical role of fine-tuning in advancing LLM agent capabilities, leveraging diverse data, iterative feedback mechanisms, and innovative frameworks to improve generalization, robustness, and task efficiency.</p>
<p>Conclusion</p>
<p>In this work, we propose a novel method, ATLAS, for fine-tuning LLM agents by focusing on critical steps in expert trajectories.Using an oracle LLM as the selector, we first construct a dataset of critical steps, which highlights the most impactful actions in the trajectories.We then optimize the LLMs by computing and minimizing the loss exclusively on these critical steps.Experimental results demonstrate that by leveraging only 30% of the steps, our method achieves superior performance in training environments compared to finetuning on 100% of the trajectories.Moreover, the improved generalization ability of our finetuned agents is also reflected in enhanced performance on held-out tasks.These findings underscore the importance of critical step training in maintaining and enhancing the strengths of LLM agents while significantly reducing training costs.This approach offers a cost-effective solution for fine-tuning LLMs across diverse tasks and environments.</p>
<p>Limitation</p>
<p>While our proposed method, ATLAS, demonstrates significant improvements in efficiency and generalization for fine-tuning LLM agents on multi-environments, some aspects could be further improved.Firstly, the current approach to selecting critical steps depends heavily on powerful closed-source models.It is crucial to explore methods that enable precise critical step selection while minimizing time and computatiional costs.Additionally, the existing selection process primarily focuses on semantic aspects; combining it with other metrics could enhance the accuracy of identifying critical steps and further reduce the tokens needed for fine-tuning.</p>
<p>A Environment Details</p>
<p>ALFWorld (Shridhar et al., 2020): ALFWorld is a simulated household environment developed upon the TextWorld framework, designed to require agents to perform tasks that involve spatial navigation and the application of common-sense knowledge.Within this environment, the action space includes interactions such as object manipulation (e.g., lifting, repositioning), environmental inspection, and the operational use of furniture.Agent actions are executed according to predefined logical rules, with the system generating contextually appropriate feedback.The principal evaluation criterion is the success rate, with each task sequence capped at 30 iterations to balance efficiency and feasibility.For the AGENTTRAJ-L dataset, a total of 2,420 trajectories were aggregated, consisting of 1,920 instances generated by state-of-the-art computational models and 500 human-annotated examples to ensure diversity and grounding in real-world reasoning.</p>
<p>BabyAI (Chevalier-Boisvert et al., 2018): BabyAI is a grid-world simulation platform featuring 40 instruction-following tasks that necessitate agent-object interaction.Agents are constrained to a 7x7 observational grid, permitting object manipulation only within immediate proximity.The original framework employs visual observations and primitive actions (e.g., "move forward," "turn left"), while an adapted version substitutes visual inputs with textual instructions and integrates highlevel action commands (e.g., "pick up green key 1," "go through blue locked door 2"), thereby broadening the action space.A step-discounted reward is assigned upon successful task completion, with unsuccessful attempts yielding zero reward.For AGENTTRAJ-L, 810 trajectories were generated using state-of-the-art models.Performance is evaluated via task-specific rewards, capped at 20 rounds per task.</p>
<p>MAZE (Abdulhai et al., 2023): Maze is a textual environment where agents possess positional awareness, including their current location, target destination, and adjacent obstructions.Agents select movement in one of four directional axes (up, down, left, right) per step, with each action incurring a penalty of -1 until goal attainment.AGENTTRAJ-L comprises 215 trajectories.Performance is evaluated via success rate, with task sequences capped at 15 iterations.</p>
<p>Movie (Ma et al., 2024): Movie is a structured environment enabling LLM agents to leverage a specialized tool for accessing film-related metadata (e.g., cinematic details, personnel, production entities).The framework offers 16 discrete actions to execute task-specific objectives.It integrates The Movie Database (TMDB) API, incorporating its dataset and operational functions to facilitate agent interactions.Agents obtain a binary reward (1 for correct output alignment with reference solutions, 0 otherwise).AGENTTRAJ-L comprises 215 annotated trajectories, with 20 questions reserved for evaluation and the remainder allocated to training.Success rate serves as the primary performance metric, with task sequences limited to 12 iterations.SciWorld (Wang et al., 2022): SciWorld functions as an empirical assessment framework for evaluating scientific reasoning capabilities in an interactive textual environment, structured to reflect elementary-level curricular standards.The platform encompasses 30 diverse task categories, operationalizing activities such as tool-based measurement and mechanical experimentation.It employs a domain-specific action space, with a simulator generating contextual feedback on action outcomes.The AGENTTRAJ-L dataset includes 2,120 trajectories synthesized for evaluation.Task performance is measured via reward accumulation, with each trial restricted to a maximum of 30 iterations.</p>
<p>TextCraft (Prasad et al., 2023): TextCraft is a text-based environment modeled after WordCraft, designed for simulating item crafting through a hierarchical framework of 544 nodes aligned with Minecraft's recipe hierarchy.Each task specifies a target item and a sequence of compositional crafting actions (e.g., "craft <item> using <ingredi-ents>," "get <item>," "inventory"), with complexity ranging from 1 to 4 procedural steps.The environment provides real-time feedback on action validity and execution states, enabling agents to directly acquire non-craftable items.A reward of 1 is granted exclusively upon successful synthesis of the target item.For evaluation, 100 tasks are partitioned from a broader training set, with AGENTTRAJ-L comprising 374 trajectories (299 generated by state-of-the-art models, 75 humanannotated).Success rate is the primary metric, capped at 20 rounds per task for empirical validation.</p>
<p>TODOList (Ma et al., 2024): Todolist is a structured environment enabling LLM agents to manage personal agenda data through a task-oriented interface with 11 discrete operational commands.The tool integrates the TodoList API to operationalize task management functionalities.Agents receive binary reward allocation (1 for congruence with reference outputs, 0 otherwise).AGENTTRAJ-L comprises 135 trajectories, with 20 task instances reserved for evaluation and the remainder allocated to training.Performance is assessed via success rate, constrained to a maximum of 15 iterations per task.</p>
<p>Weather (Ma et al., 2024): Weather is a structured environment enabling LLM agents to retrieve meteorological parameters (e.g., temperature, precipitation, air quality) across spatiotemporal contexts via a specialized tool.The framework offers 18 discrete operational commands, integrating the Open-Meteo API through Python-based implementation to enable data querying functionalities.Agents receive a binary reward (1 for output alignment with reference solutions, 0 otherwise).AGENTTRAJ-L includes 311 annotated trajectories, with 20 queries reserved for evaluation and the remainder allocated to training.Performance is assessed via success rate, constrained to a maximum of 10 iterations per task.</p>
<p>WebShop (Yao et al., 2022a): Webshop is a simulated e-commerce platform where agents execute product procurement tasks adhering to predefined criteria through interface interactions (button-based navigation) or text-based search functionality.The environment integrates 12,000 structured instructions and leverages over one million real-world Amazon product listings, with 6,910 instructions selected for task execution.AGENTTRAJ-L includes 3,930 annotated trajectories.Performance is quantified via success rate, with task sequences limited to 10 rounds to balance efficiency and practical applicability.</p>
<p>Wordle (Abdulhai et al., 2023): Wordle is a lexical reasoning assessment framework where agents deduce a target word from a constrained fiveletter vocabulary.The environment operationalizes character-level feedback after each guess, indicating positional accuracy and presence of characters.Agents accumulate a step penalty of -1 until successful identification or attempt exhaustion.AGENTTRAJ-L contains 955 trajectories.Performance is quantified via success rate, with trials capped at 8 rounds for empirical validation.</p>
<p>Academia (Ma et al., 2024): Academia is a structured environment enabling LLM agents to access computer science research resources (e.g., publications, author metadata) via seven discrete operational commands.The tool integrates the Citation Network Dataset to implement core functionalities.Agents receive binary rewards (1 for output alignment with reference solutions, 0 otherwise).Performance is assessed via success rate across 20 evaluation tasks, with trials capped at 12 rounds per task.</p>
<p>Sheet (Ma et al., 2024): Sheet is a structured environment enabling LLM agents to manipulate spreadsheet data via 20 discrete operational commands, leveraging the Google Sheets API.Reward is determined by structural and content congruence between the agent-modified spreadsheet and a reference template, quantified on a 0-1 scale.Evaluation employs 20 predefined tasks, with performance measured via reward accumulation and trials capped at 15 rounds per task.</p>
<p>Jericho (Ma et al., 2024): Jericho is a textbased game framework designed to evaluate agents' capacity for interactive exploration and dynamic world modeling within fictional narratives.These tasks demand agents to infer contextual rules (e.g., magical systems) through iterative interaction rather than pre-existing commonsense knowledge.To address operational feasibility given LLM agents' context window constraints, original game objectives (often requiring 50-300 steps) are restructured into modular subtasks achievable within 15 steps.For example, the zork1 dungeon exploration is redefined as "locate the secret passage entrance in the living room," reducing the sequence to 8 steps.This adaptation preserves core exploratory challenges while aligning task complexity with computational tractability.</p>
<p>PDDL (Ma et al., 2024): The Planning Domain Definition Language (PDDL) serves as a framework for evaluating strategic reasoning in symbolic planning tasks, with four benchmark domains-Gripper, Barman, Blocksworld, and Tyreworld-designed to test multi-step action sequencing under efficiency constraints.Agents must navigate domain-specific objectives (e.g., transporting objects, mixology, block stacking, tire installation) by optimizing action sequences to minimize redundant operations.For instance, in the Barman domain, agents strategically allocate containers to reduce cleaning cycles during cocktail preparation.To enable natural language interaction, symbolic PDDL predicates (e.g., ontable(shaker1)) and actions (e.g., clean-shaker) are translated into textual observations (e.g., "Shaker1 is on the table") and instructions (e.g., "Clean shaker1 with hand1 while hand2 is empty").Each domain features 10-20 curated multi-round problems, with progress measured via a normalized matching score that quantifies alignment between the current state and the goal state.Full task completion (100% progress) requires satisfying all goal conditions, such as hierarchical block placement in Blocksworld.This adaptation bridges symbolic planning with language-agent interoperability while preserving strategic complexity.</p>
<p>B More Implementation Details B.1 Training Configuration</p>
<p>We use the Adam optimizer (Kingma, 2014), with a learning rate of 2e-5 and a cosine scheduler for the agent fine-tuning.The models are trained with 3 epoches and a warmup rate 0.03.The batch size is 128 and the max length of 8192.All experiments are conducted on 4 NVIDIA A100 80G GPUs and fine-tunes a Llama3.1-8B-Instruct on ATLAS takes approximately 8 hours.We use PyTorch FSDP (Paszke et al., 2019) for efficient training.</p>
<p>B.2 Perplexity Selection</p>
<p>The perplexity-based selection method identifies critical steps by measuring the generation difficulty of each step within a trajectory.We compute perplexity as the exponentiated average negative loglikelihood of a step's tokens under the model's distribution.Formally, the perplexity for step x is defined as:
PPL(x) = exp   − 1 |x| |x| t=1 log p(x t | x 1:t−1 )   (5)
where |x| is the token length of the step, and p(x t | x 1:t−1 ) is the model's probability for token x t given its context.Higher perplexity values indicate steps that are more challenging for the model to generate.</p>
<p>Instead of using a fixed threshold, we dynamically select the top 30% of steps with the highest perplexity within each trajectory.</p>
<p>B.3 Value Estimation</p>
<p>We follow the rollout setup of (Xiong et al., 2024), which defines the rollout estimate reward function as r s (s t , a t ) as the estimated outcome reward from the exploration starting step t.The agent with policy π s is employed to rollout trajectory τ t:n from step t, based on the historical trajectory τ t−1 .The environment then gives a final reward r o (i, τ n , E) for the trajectory.The estimated reward can be calculated as: r s (s t , a t ) = E τn∼πs(τt:n|τ t−1 ) [r o (i, τ n , E)] (6) Due to the complexity of computing this expectation value directly, we utilize the Monte Carlo sampling technique for estimation.By drawing N trajectories from step t using π s , we create a collection of trajectories:
{τ (i) |i = 1, . . . , N } = MC πs (τ t−1 ; N ) (7)
The estimated reward is then calculated as: r s (s t , a t ) = 1 N N i=1 r o (i, τ n , E), for t &lt; n, r o (i, τ n , E), for t = n.</p>
<p>(8) Then the value function V π (s t ) is estimated as the expectation over all possible trajectories starting from state s t under policy π: V π (s t ) = E τt:n∼π(τt:n|st) n k=t γ k−t r s (s k , a k ) , (9) where γ is the discount factor which we set to be 0.99 here.Finally, we compute the difference between consecutive steps.If this difference exceeds the threshold of 0.1, the step is identified as a critical step.At this point, we designate the expert action corresponding to the same timestep t as trainable and calculate the loss associated with that action.</p>
<p>C Additional Results</p>
<p>Detailed results of other backbone models, random selection and different critical selection ratio m are shown in Table 7.</p>
<p>D Prompts Details D.1 Prompts of Critical Step Selection</p>
<p>Critcal Selection Prompts A critical step is defined as a key action or decision that, if performed correctly, significantly increases the likelihood of successfully completing the task.It represents a turning point in the process that influences the outcome of subsequent actions.More specifically, critical steps include:</p>
<p>• Plan Creation: Steps where the LLM agent formulates sub-goals by analyzing previous observations and considering the final objective, breaking down the larger goal into manageable tasks that guide the agent's actions towards the overall outcome.</p>
<p>• Critical Observation: Steps where the LLM agent identifies and analyzes key information from the environment, which help agent understand the objective or state and refine its strategy and decision-making towards more effective outcomes.</p>
<p>• Critical Action: Steps where the LLM agent takes decisive and impactful actions based on prior observations, significantly advancing the process toward the final objectives.These actions are crucial in shaping the direction of the agent's strategy and are often pivotal moments that determine progress or failure, ensuring that the agent remains on track to achieve the desired outcome.</p>
<p>• Self Correction: Steps where the LLM agent carefully recalls and assesses its previous actions or decisions, especially after encountering failure or suboptimal outcomes.During this process, the agent reflects on what went wrong, identifies areas for improvement, and adjusts its approach to enhance future performance, which helps the agent refine its decisionmaking and better align with the overall objective.</p>
<p>Task Description: Your task is:</p>
<ol>
<li>
<p>Induce a high-level plan or strategy based on the expert conversation, summarizing the key steps needed to successfully complete the task.</p>
</li>
<li>
<p>Based on this high-level plan, identify the most critical action steps in the expert conversation.A maximum of {} steps may be chosen from the conversation.</p>
</li>
<li>
<p>Provide a detailed explanation for choosing these critical steps, specifying which category (e.g., key observation, planning, recall, pivotal action) they belong to and why mastering these steps ensures the success of the task.</p>
</li>
</ol>
<p>Answer Format:</p>
<p>1.</p>
<p>D.2 Prompts for Task Inference</p>
<p>We adopt the setup of AgentGym (Xi et al., 2024) and utilize the same prompts for task inference.For further details, please refer to Appendix G: Prompt Details of AgentGym.</p>
<p>Figure 2 :
2
Figure 2: Three base LLMs finetuned by ATLAS vs. full trajectories (100% of the steps), evaluated on held-in and held-out agentic tasks.ATLAS consistently outperforms full-trajectory finetuning, indicating better generalizability of ATLAS by training on fewer but critical steps.</p>
<p>Figure 3 :
3
Figure 3: Overall of ATLAS.The selector identifies critical steps in expert trajectories collected in multiple environments, where "O" and "A" denote observation and action, respectively.Training loss is only computed on the critical steps.This encourages more exploration of non-critical steps, reduces the training cost, and improves the agent's generalization performance.</p>
<p>Table 1 :
1
Performance of ATLAS-finetuned LLM agents vs. baseline LLMs and finetuned agents on held-in and held-out test tasks."Unfilter" finetunes an LLM on the unfiltered dataset of complete expert trajectories.The best performance in each category is highlighted in bold.By finetuning LLMs only on the critical steps of expert trajectories, ATLAS achieves outstanding agent performance on both held-in and held-out tasks.
Modelheld-in Alfworld Babyai Maze Movie Sciworld Textcraft Todo Weather Webshop Wordle AVG Academic Sheet Jericho PDDL AVG held-outClosed-Source ModelDeepSeek-Chat51.0045.674.0070.0016.8023.0075.0070.0011.0024.00 39.05-----Claude-3-Haiku0.001.934.0050.000.830.0065.0055.005.5016.00 19.83-----Claude-3-Sonnet13.0079.254.0050.002.7838.0080.0065.001.5036.00 36.95-----GPT-3.5-Turbo26.0071.364.0070.007.6447.0040.0025.0012.5020.00 32.3565.0038.34 19.9324.98 37.06GPT-4-Turbo67.5072.83 68.00 95.0014.3877.0095.0080.0015.5088.00 67.3280.0075.83 52.4481.16 72.36Open-Source Base ModelLlama-2-7B-Instruct0.0023.000.000.007.200.000.000.000.000.003.020.000.001.250.830.52Llama-3-8B-Instruct0.0061.60 12.00 60.0070.8215.0060.0030.0013.000.0032.2440.0035.40 10.5116.67 25.65Llama-3.1-8B-Instruct0.0060.69 16.00 65.0019.579.0065.0040.002.000.0027.7240.0037.31 19.2312.18 27.18Phi-3-mini-128k-instruct0.0019.808.0050.0019.101.0040.0020.001.004.0016.2925.0030.211.251.2514.43Phi-3.5-mini-instruct0.0021.00 12.00 60.0018.203.0060.0030.002.006.0021.2230.0033.620.560.8316.25Gemma2-9B-it0.0052.008.0055.0072.600.0045.000.000.000.0023.2630.0034.46 14.027.2021.42Mistral-7B-Instruct-v0.30.0017.304.000.0048.000.0010.005.000.000.008.430.009.691.962.503.54Qwen2.5-7B-Instruct0.0067.00 16.00 75.0015.307.0085.0030.003.0012.00 31.0355.0039.00 14.260.2827.14Fine-tuned AgentsXlam-7B-r17.5062.000.0020.0012.001.0015.0010.0037.504.0017.935.0028.408.923.0618.85AgentLM-7B71.000.4912.005.001.634.0015.000.0036.504.0014.9610.0013.30 15.882.7810.49AgentEvol-7B88.0082.70 12.00 60.0038.0064.0070.0025.0076.5012.00 52.8225.0026.204.053.1014.59AgentTraj-L (100% steps)83.0070.31 48.00 75.0037.9271.0085.0055.0068.0012.00 60.5275.0042.83 15.1811.71 36.18Perplexity Selection78.5077.59 24.00 65.0053.5459.0070.0030.0066.508.0053.2130.0036.48 13.288.7622.13ATLAS (30% steps)84.5080.98 48.00 90.0042.0272.0090.0060.0071.5020.00 65.9170.0049.39 18.2115.84 38.36</p>
<p>Table 2 :
2
Table3, with only the average performance across tasks presented due Critical vs. Non-critical steps &amp; complete trajectories for agent tuning (tested on held-in and held-out tasks).The agent trained on ATLAS-selected critical steps does not only outperform the agent trained on full trajectories but also surpasses the agent trained on non-critical steps by a large margin.This indicates that the critical steps improve agent tuning while training on non-critical steps might be detrimental to the generalization.
Data</p>
<p>Table 3 :
3
ATLAS vs. random selection of steps at different ratios (including 100% as a reference).</p>
<p>Table 5 :
5
Ablation study of the selector LLM for critical step selection on Alfworld, BabyAI, and Weather tasks.Average performance over tasks is reported.
TaskSelectorPerformanceAlfworldGPT-4o Llama3.1-70B83.00 78.50BabyAIGPT-4o Llama3.1-70B78.93 67.23WeatherGPT-4o Llama3.1-70B60.00 55.00</p>
<p>Table 6 :
6
Performance comparison of the model initialized from scratch vs. from critical steps on BabyAI and Maze tasks.
TasksFrom Scratch From Critical StepBabyAI54.876.4Maze18.044.0</p>
<p>The high-level plan is: [Summarize the strategy and key steps for task completion]</p>
<ol>
<li>The critical steps are: conversation[...] 3. Reason: [Explain why these steps are critical, including which category they fall into (key observation, planning, recall, pivotal action) and how they enable the player to avoid mistakes in subsequent steps]</li>
</ol>
<p>Table 7 :
7
Detailed performance of other backbone models, random selection and different critical selection ratio m.The best performance in each section is highlighted in bold.</p>
<p>https://huggingface.co/datasets/AgentGym/AgentTraj-L
https://huggingface.co/meta-llama/Llama-3. 1-70B-Instruct</p>
<p>Marwa Abdulhai, Isadora White, Charlie Snell, Charles Sun, Joey Hong, Yuexiang Zhai, Kelvin Xu, Sergey Levine, arXiv:2311.18232Lmrl gym: Benchmarks for multi-turn reinforcement learning with language models. 2023arXiv preprint</p>
<p>Renat Aksitov, Sobhan Miryoosefi, Zonglin Li, Daliang Li, Sheila Babayan, Kavya Kopparapu, Zachary Fisher, Ruiqi Guo, Sushant Prakash, Pranesh Srinivasan, Manzil Zaheer, Felix Yu, Sanjiv Ku, Rest meets react: Self-improvement for multi-step reasoning llm agent. mar. 2023</p>
<p>The claude 3 model family: Opus, sonnet, haiku. arXiv:2312.10003.Anthropic.2024Preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, arXiv:2310.05915Advances in Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordEhsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao2020. 202333arXiv preprintFireact: Toward language agent fine-tuning</p>
<p>Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, Feng Zhao, arXiv:2403.12881Agent-flan: Designing data and methods of effective agent tuning for large language models. 2024arXiv preprint</p>
<p>Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, Yoshua Bengio, arXiv:1810.08272Babyai: A platform to study the sample efficiency of grounded language learning. 2018arXiv preprint</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.21783The llama 3 herd of models. 2024arXiv preprint</p>
<p>Large language models are neurosymbolic reasoners. Meng Fang, Shilong Deng, Yudi Zhang, Zijing Shi, Ling Chen, Mykola Pechenizkiy, Jun Wang, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202438</p>
<p>. Thomas Mesnard, Gemma Team, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, 10.34740/KAGGLE/M/3301arXiv:2402.05119Gemma. Sreyan Ghosh. Chandra Kiran Reddy Evuru, Sonal Kumar, Deepali Aneja, Zeyu Jin, Ramani Duraiswami, Dinesh Manocha2024arXiv preprintet al. 2024. A closer look at the limitations of instruction tuning</p>
<p>Language models as zeroshot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, arXiv:2201.072072022aPreprint</p>
<p>Inner monologue: Embodied reasoning through planning with language models. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, Brian Ichter, arXiv:2207.056082022bPreprint</p>
<p>Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>P Diederik, Kingma, arXiv:1412.6980Adam: A method for stochastic optimization. 2014arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Selective reflectiontuning: Student-selected data recycling for LLM instruction-tuning. Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Jiuxiang Gu, Tianyi Zhou, Findings of the Association for Computational Linguistics ACL 2024. Bangkok, Thailand2024aand virtual meeting. Association for Computational Linguistics</p>
<p>Reflection-tuning: Recycling data for better instruction-tuning. Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Tianyi Zhou, NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. 2023</p>
<p>Mosaic-it: Free compositional data augmentation improves instruction tuning. Ming Li, Pei Chen, Chenguang Wang, Hongyu Zhao, Yijun Liang, Yupeng Hou, Fuxiao Liu, Tianyi Zhou, arXiv:2405.133262024barXiv preprint</p>
<p>Superfiltering: Weak-to-strong data filtering for fast instruction-tuning. Ming Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu Zhao, Jianzong Wang, Ning Cheng, Tianyi Zhou, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024c1</p>
<p>From quantity to quality: Boosting LLM performance with self-guided data selection for instruction tuning. Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, Jing Xiao, Proceedings of the 2024 Conference of the North American Chapter. the Association for Computational Linguistics: Human Language Technologies. the 2024 Conference of the North American ChapterMexico City, MexicoAssociation for Computational Linguistics2024d1Long Papers</p>
<p>Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, arXiv:2405.04434Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model. 2024arXiv preprint</p>
<p>Agentboard: An analytical evaluation board of multi-turn llm agents. Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, Junxian He, arXiv:2401.131782024arXiv preprint</p>
<p>Agents that reduce work and information overload. Pattie Maes, Readings in human-computer interaction. Elsevier1995</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>Pytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Advances in neural information processing systems. 201932</p>
<p>Tianjun Shishir G Patil, Xin Zhang, Joseph E Wang, Gonzalez, arXiv:2305.15334Gorilla: Large language model connected with massive apis. 2023arXiv preprint</p>
<p>Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish Sabharwal, Mohit Bansal, Tushar Khot, arXiv:2311.05772Adapt: As-needed decomposition and planning with language models. 2023arXiv preprint</p>
<p>Autoact: Automatic agent learning from scratch for qa via self-planning. Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Eleanor Jiang, Chengfei Lv, Huajun Chen, arXiv:2401.052682024Preprint</p>
<p>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, arXiv:2307.16789Toolllm: Facilitating large language models to master 16000+ real-world apis. 2023arXiv preprint</p>
<p>Zijing Shi, Meng Fang, Ling Chen, arXiv:2504.16855arXiv:2010.03768Alfworld: Aligning text and embodied environments for interactive learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, 2025. 2020arXiv preprintMonte carlo planning with large language model for text-based game agents</p>
<p>Mastering the game of go without human knowledge. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, nature. 55076762017</p>
<p>Beyond human data: Scaling self-training for problem-solving with language models. Avi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter J Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, Abhishek Kumar, Alex Alemi, Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Gamaleldin Elsayed, Hanie Sedghi, Igor Mordatch, Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura Culp, arXiv:2312.06585Xiao, Maxwell L. Bileschi, Noah Constant, Roman Novak, Rosanne Liu, Tris Warkentin, Yundi Qian, Yamini Bansal, Ethan Dyer, Behnam Neyshabur, Jascha Sohl-Dickstein, and Noah Fiedel2024Preprint</p>
<p>Yifan Song, Weimin Xiong, Xiutian Zhao, Dawei Zhu, Wenhao Wu, Ke Wang, Cheng Li, Wei Peng, Sujian Li, arXiv:2410.07706Agentbank: Towards generalized llm agents via fine-tuning on 50000+ interaction trajectories. 2024arXiv preprint</p>
<p>Reinforcement learning: An introduction. Richard S Sutton, 2018A Bradford Book</p>
<p>arXiv:2303.08774OpenAI Team. 2024a. Gpt-4 technical report. Preprint</p>
<p>arXiv:2407.10671Phi-3 Team. 2024b. Phi-3 technical report: A highly capable language model locally on your phone. arXiv preprint</p>
<p>arXiv:2407.10671Qwen2 Team. 2024c. Qwen2 technical report. arXiv preprint</p>
<p>Hasard: A benchmark for vision-based safe reinforcement learning in embodied agents. Tristan Tomilin, Meng Fang, Mykola Pechenizkiy, arXiv:2503.082412025arXiv preprint</p>
<p>Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Edouard Joulin, Guillaume Lample ; Yasmine Grave, Nikolay Babaei, Soumya Bashlykov, Prajjwal Batra, Shruti Bhargava, Dan Bhosale, Lukas Bikel, Cristian Canton Blecher, Moya Ferrer, Guillem Chen, David Cucurull, Jude Esiobu, Jeremy Fernandes, Wenyin Fu, Brian Fu, Cynthia Fuller, Vedanuj Gao, Naman Goswami, Anthony Goyal, Saghar Hartshorn, Rui Hosseini, Hakan Hou, Marcin Inan, Viktor Kardas, Madian Kerkez, Isabel Khabsa, Artem Kloumann, Punit Korenev, Marie-Anne Singh Koura, Thibaut Lachaux, Jenya Lavril, Diana Lee, Yinghai Liskovich, Yuning Lu, Xiang Mao ; Jian, Puxin Kuan, Zheng Xu, Iliyan Yan, Yuchen Zarov, Zhang, arXiv:2302.13971arXiv:2307.09288Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan2023aAurelien RodriguezBinh Tang, Ross Taylor, Adina Williams,; Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovPreprintLlama: Open and efficient foundation language models. and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Grandmaster level in starcraft ii using multi-agent reinforcement learning. Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, nature. 57577822019</p>
<p>A survey on large language model based autonomous agents. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Frontiers of Computer Science. 1861863452024a</p>
<p>Ruiyi Wang, Haofei Yu, Wenxin Zhang, Zhengyang Qi, Maarten Sap, Graham Neubig, Yonatan Bisk, Hao Zhu, arXiv:2403.08715Sotopia-π: Interactive learning of socially intelligent language agents. 2024bPreprint</p>
<p>Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, Prithviraj Ammanabrolu, arXiv:2203.07540Scienceworld: Is your agent smarter than a 5th grader?. 2022arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Intelligent agents: Theory and practice. Michael Wooldridge, Nicholas R Jennings, The knowledge engineering review. 1021995</p>
<p>Zhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang Hong, Honglin Guo, Junzhe Wang, Dingwen Yang, Chenyang Liao, Xin Guo, Wei He, arXiv:2406.04151Agentgym: Evolving large language model-based agents across diverse environments. 2024arXiv preprint</p>
<p>Weimin Xiong, Yifan Song, Xiutian Zhao, Wenhao Wu, Xun Wang, Ke Wang, Cheng Li, Wei Peng, Sujian Li, arXiv:2406.11176Watch every step! llm agent learning via iterative step-level process refinement. 2024arXiv preprint</p>
<p>A survey on knowledge distillation of large language models. Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, Tianyi Zhou, ArXiv, abs/2402.131162024</p>
<p>Webshop: Towards scalable real-world web interaction with grounded language agents. Shunyu Yao, Howard Chen, John Yang, Karthik Narasimhan, Advances in Neural Information Processing Systems. 2022a35</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Yuan Karthik R Narasimhan, Cao, The Eleventh International Conference on Learning Representations. 2022b</p>
<p>Textgrad: Automatic "differentiation" via text. Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, James Zou, arXiv:2406.074962024Preprint</p>
<p>Agenttuning: Enabling generalized agent abilities for llms. Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, Jie Tang, arXiv:2310.128232023arXiv preprint</p>
<p>Agentohana: Design unified data and training pipeline for effective agent learning. Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran Yao, Juntao Tan, Thai Hoang, Liangwei Yang, Yihao Feng, Zuxin Liu, arXiv:2402.155062024arXiv preprint</p>
<p>. Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Yifan Dong, Chen Du, Yushuo Yang, Zhipeng Chen, Jinhao Chen, Ruiyang Jiang, Yifan Ren, Xinyu Li, Zikang Tang, Peiyu Liu, Jian-Yun Liu, Ji-Rong Nie, Wen, </p>
<p>Model held-in held-out Alfworld Babyai Maze Movie Sciworld Textcraft Todo Weather Webshop Wordle AVG Sheet Academic Jericho PDDL AVG Mistral-7B-Instruct. arXiv:2303.18223v0.3A survey of large language models. Preprint</p>
<p>. -L Agenttraj, </p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2069 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2069</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2069</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-53.html">extraction-schema-53</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <p><strong>Paper ID:</strong> paper-281079256</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.01398v1.pdf" target="_blank">The Need for Verification in AI-Driven Scientific Discovery</a></p>
                <p><strong>Paper Abstract:</strong> Artificial intelligence (AI) is transforming the practice of science. Machine learning and large language models (LLMs) can generate hypotheses at a scale and speed far exceeding traditional methods, offering the potential to accelerate discovery across diverse fields. However, the abundance of hypotheses introduces a critical challenge: without scalable and reliable mechanisms for verification, scientific progress risks being hindered rather than being advanced. In this article, we trace the historical development of scientific discovery, examine how AI is reshaping established practices for scientific discovery, and review the principal approaches, ranging from data-driven methods and knowledge-aware neural architectures to symbolic reasoning frameworks and LLM agents. While these systems can uncover patterns and propose candidate laws, their scientific value ultimately depends on rigorous and transparent verification, which we argue must be the cornerstone of AI-assisted discovery.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2069.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2069.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI-Descartes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI-Descartes (neuro-symbolic generator-verifier framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic framework that couples symbolic regression (generator) with formal theorem-proving (verifier) to produce candidate symbolic laws and then test their consistency with background theory via a reasoning distance metric.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI-Descartes</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>neuro-symbolic system (symbolic regression + automated theorem prover)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>physical sciences / symbolic equation discovery</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>symbolic mathematical formulas / candidate scientific laws</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately novel to highly novel (can propose formulas not present in training data; suitable for rediscovery and novel candidate laws)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>mixed-integer nonlinear programming formulation of symbolic regression (enumerative / optimization-based search) that produces candidate formulas fitting data</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>formal logical verification via a theorem prover comparing candidate formulas to a background theory B; computes empirical error ε(f) and reasoning error β(f) and ranks candidates by both</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>generates many candidate symbolic formulas efficiently even from few noisy datapoints; no numeric generation success rates reported in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>validation is formal (theorem-proving) yielding a reasoning distance; paper reports that it can distinguish numerically-accurate but theory-inconsistent formulas from provable ones, but gives no aggregate accuracy/precision metrics</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not quantified numerically; framework is explicitly designed so reasoning error β(f) flags theory-inconsistent candidates, so validation is stronger for outputs consistent with provided background theory; when background theory is incomplete or inconsistent, validation weakens (reasoning error may be nonzero)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper emphasizes a gap: generation can produce many hypotheses quickly while verification (reasoning) is more costly and constraining; AI-Descartes mitigates gap by post-hoc formal checking but retains a sequential generator-then-verifier separation which limits joint exploitation of data and theory</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Implicit via empirical error ε(f) and reasoning error β(f) but no probabilistic uncertainty estimates reported</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported; calibration of generator confidence vs. verifier outcome not quantified</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported quantitatively; designed to reason about unmeasured variables via background theory, improving OOD interpretability when theory is complete</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Uses reasoning distance β(f) and empirical error ε(f) rather than plausibility proxies; these are direct proxies for logical consistency and data fit rather than human-judged plausibility</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Paper recommends human oversight, particularly for background-theory specification and handling translation/axiom choice; frequency likely increases with novelty but no numerical schedule provided</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>semi-formal to formal (works best in domains with formalizable background theory, e.g., parts of physics); performance depends strongly on domain formalization completeness</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Generator-verifier architecture: enumerate candidates via SR then formally verify; ranking by combined empirical and reasoning error. This reduces spurious hypotheses but retains sequential separation which the paper notes limits full exploitation of data+theory.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Authors report that many numerically-accurate formulas violate background theory and that reasoning error β(f) successfully filters such formulas; they argue generative methods produce a flood of hypotheses that overwhelm verification pipelines, motivating AI-Descartes' verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>No quantitative contradictory evidence presented; AI-Descartes itself is presented as a mechanism to close the gap but the paper notes it cannot fully exploit data and theory simultaneously (sequential design).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Qualitative: verification (theorem proving) is computationally nontrivial relative to generation; no numeric cost ratio reported. Paper implies verification is more expensive and constraining.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2069.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2069.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI-Hilbert</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI-Hilbert (theory-integrated polynomial discovery framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A theory-guided optimization framework that integrates background axioms into the hypothesis generation process to produce polynomial (or rational) candidate laws together with algebraic certificates of derivability using SOS/SDP relaxations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI-Hilbert</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>theory-constrained optimization / symbolic discovery via semidefinite programming (SOS certificates)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>physical sciences / algebraic equation discovery</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>polynomial (or rational) equations representing candidate physical laws, plus algebraic certificates</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately novel (restricted to polynomial/rational hypothesis classes so novelty outside that class is limited)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>formulates a mixed-integer polynomial optimization problem combining data fit, complexity penalties, and algebraic constraints; then converts to semidefinite program (SOS) to find polynomials consistent (or approximately consistent) with background theory</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>exact algebraic certificates of derivability when distance d_c = 0; if d_c > 0 returns approximate certificate; validation is algebraic (certificate from SOS decomposition)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Generates polynomial candidates constrained by background axioms; effective when target is polynomial/rational; no numeric generation rates reported</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>When background theory is complete and consistent, returns exact derivability certificates (strong formal validation). If theory incomplete/inconsistent, returns approximate certificates; no aggregate metrics reported</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Because hypothesis space is restricted to polynomials/rationals, novelty outside that space reduces ability to both generate and validate; validation is strong for in-class discoveries but degrades for out-of-class or when background theory incomplete</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>AI-Hilbert integrates generation and validation (theory used during generation), reducing the generation-->validation gap for polynomial classes; comparison shows smaller gap than sequential approaches but tradeoff is restricted hypothesis class</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Validation returns a distance d_c to theory and multipliers producing certificates; not probabilistic uncertainty, but distance quantifies discrepancy</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported as probabilistic calibration; algebraic certificates provide deterministic guarantees when applicable</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Out-of-class (non-polynomial) phenomena not well-handled; approximate certificates may indicate mismatch but no numerical OOD metrics provided</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Relies on algebraic derivability distance d_c and SOS certificates rather than human plausibility scores</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human inspection recommended especially when d_c > 0 (approximate derivations) or when background theory may be incomplete; no numeric frequency provided</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>highly formal for algebraic/physics settings (works best when domain can be formalized as polynomial axioms)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Integrate theory into generation (constrained search) and produce formal SOS certificates; limits hypothesis space to guarantee verifiability</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Paper argues that integrating theory into generation reduces verification bottleneck compared to purely generative SR by restricting search to derivable formulas; shows conceptual reduction in post-hoc verification burden but no numerical throughput metrics</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>No direct contradiction; method trades broader generative capacity for verifiability, demonstrating a design choice rather than a refutation of the gap.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Qualitative: solving SOS/SDP certificates during generation is computationally expensive relative to unconstrained SR generation, but yields stronger guarantees; no numeric ratio reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2069.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2069.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-5 (example)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-5 (state-of-the-art large language model, as used in examples)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent large language model used here as an off-the-shelf symbolic reasoner/generator; in the paper it is shown to sometimes correctly derive symbolic formulas (example: solved one artificial problem and performed partial symbolic regression attempts on Kepler data).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-5</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model (transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mathematics / symbolic equation discovery / general scientific hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>symbolic expressions, candidate functional forms, natural-language hypotheses and derivations</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately novel (can produce novel combinations not verbatim from training data but often simple functional forms; may rediscover known laws)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>sequence modeling using patterns learned from large corpora; performs limited symbolic-regression-like reasoning when prompted (tries multiple functional forms)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>No formal internal scientific validation; plausibility often judged by surface coherence and human review; in examples the authors compared GPT-5 outputs to ground-truth formulas</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Qualitatively: GPT-5 produced the correct expression for one artificial example and explored multiple candidate forms for Kepler-data example; no quantitative generation accuracy reported beyond anecdotal success/failure examples in appendix</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Model itself does not perform formal validation; validation in paper was external (authors checked correctness). No accuracy/precision metrics reported for validation by GPT-5</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Paper shows GPT-5 outperformed GPT-4 on a simple artificial example (GPT-5 returned correct derivation while GPT-4 failed), but overall performance degrades on novel or complex symbolic tasks; no numeric scaling reported</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Generation is fast and often flexible; validation remains manual or external — authors highlight a gap where LLM generation outpaces available verification and LLMs can hallucinate plausible but incorrect formulas</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>GPT-5 does not provide formal uncertainty estimates about correctness in these examples; no calibrated confidence scores reported</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported; authors note models produce confident but sometimes false statements (hallucinations), implying poor calibration of truth-confidence</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Limited / deteriorates on tasks requiring formal derivations beyond training-distribution patterns; anecdotal improvement over GPT-4 but no quantified OOD metrics</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>LLM's surface plausibility and internal likelihoods (token probabilities) can act as proxies, but the paper emphasizes these are plausibility rather than provability</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Required for each scientific output in the examples; frequency presumed to increase with novelty and complexity</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>varies; LLMs operate across many domains, including poorly formalized domains; their outputs should be validated more in less formal domains</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>The paper suggests coupling LLMs with formal verifiers or domain tools (e.g., theorem provers, simulations, domain-specific tools) but does not present an integrated system in this work</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Authors show GPT-5 can generate correct formulas but argue generative models produce many unverified hypotheses and can hallucinate (citing examples of fabricated legal cases and biomedical references). Appendix shows GPT-5 succeeded where GPT-4 failed on a toy derivation, illustrating generation advances but not validation improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>GPT-5's successful derivation for the artificial example shows generation can sometimes reach correct derivations; however this is anecdotal and does not contradict the overall generation-validation gap claim.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Generation (LLM inference) is inexpensive relative to formal verification or laboratory experiments; no numeric ratio reported but textual claim: generation is fast while verification remains slow/expensive.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2069.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2069.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (example)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (large language model used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier-generation large language model used in appendix examples; failed to return correct symbolic formula on a simple artificial task where GPT-5 succeeded, illustrating variability of LLM symbolic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model (transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mathematics / symbolic equation discovery</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>symbolic expressions and natural-language derivations</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>incremental to moderately novel depending on prompt; less capable on symbolic derivations than GPT-5 in examples</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>pattern completion from large corpora; prompts drive generation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>No internal formal validation; outputs validated externally by authors comparing to ground-truth formula in appendix</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>In the toy example reported, GPT-4 failed to return the correct function while GPT-5 succeeded; no broader performance metrics provided</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>None intrinsic; authors used external comparison to ground truth to assess correctness in examples</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Degrades on tasks requiring exact symbolic derivations or formal reasoning beyond rote patterns; example shows a failure on a simple synthetic task</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>LLM generation capability exceeded internal validation — outputs required human checking; GPT-4 produced incorrect confident output in the example</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not provided in the example; model confidence not reported</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported; example implies overconfidence in incorrect outputs</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Poorer than GPT-5 on the provided toy OOD-like derivation; no quantitative metrics</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Surface coherence and human-judged plausibility used by humans to filter outputs; not formal</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Every output in examples was human-validated; frequency increases with novelty</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>operates across domains; performance depends on domain formalizability</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Paper suggests combining LLMs with formal provers or constrained generation methods but does not implement for GPT-4 in this work</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Direct example: GPT-4 failed to produce the correct symbolic formula while GPT-5 succeeded, showing generation ability varies and outputs can be incorrect and require verification</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>No contradictory evidence; example supports gap claim.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>LLM inference cheap compared to formal verification or experiments; no numeric ratio reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2069.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2069.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Symbolic Regression Engines (PySR, AI Feynman, RSRM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Symbolic regression engines including PySR, AI Feynman family, and recent RSRM methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Search-based tools that generate analytic expressions from data (evolutionary search, physics heuristics, Monte-Carlo/RL-guided search); widely used to propose candidate scientific formulas but typically lack formal provability guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Symbolic regression engines (PySR, AI Feynman, RSRM, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>symbolic regression / evolutionary / optimization-based generators</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>physical sciences, general scientific modeling / equation discovery</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>symbolic mathematical expressions and candidate laws</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately novel within hypothesis class explored; can produce compact interpretable formulas not seen in training sets</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>evolutionary search, annealing, MINLP, Monte Carlo Tree Search combined with RL (RSRM), or physics-inspired heuristics (AI Feynman)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Primarily empirical: fit-to-data metrics (error, parsimony, complexity trade-offs). Some works add constraint checks or counterexample-guided refinement but formal theorem-proving not standard</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>These systems can rapidly enumerate many candidate formulas and find compact fits; performance depends on data quality and problem; no global numeric rates provided in paper</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation commonly via held-out error, parsimony/Pareto fronts, and domain-expert inspection; formal guarantees absent so validation is weaker than theorem-prover approaches</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>More novel (out-of-class) formulas are more likely to be numerical fits without theoretical grounding; these engines frequently produce plausible fits that do not generalize</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Generation capacity is high (many candidates) while validation is limited to empirical fit and heuristics, creating a verification bottleneck noted by authors</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Typically none beyond fit residuals and model-complexity penalties; no calibrated probabilistic uncertainty reported in discussed works</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported; implication is low calibration wrt theoretical correctness</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Tends to overfit in OOD settings; risk of producing non-generalizable formulas</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Uses empirical error, complexity/power-law regularizers, Pareto fronts as proxies for validity rather than formal derivability</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Often required for top candidates; frequency increases with novelty because expertise needed to assess physical plausibility</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>works across domains but strongest where empirical laws dominate (physics); lacks formal-theory integration by default</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Some works embed constraints or use counterexample-guided refinement (e.g., LGML, LGGA, counterexample-guided SR); paper notes these still only enforce functional-form plausibility not full background-theory provability</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Authors cite proliferation of SR outputs that fit data but lack theoretical grounding and overwhelm verification pipelines; reference multiple SR tools and note absence of formal reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>None presented; some SR methods combined with domain heuristics (AI Feynman) improve plausibility but not formal provability.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Generation (search/evolution) cost moderate; validation (expert review, experiments) often much more expensive; no numeric ratios provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2069.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2069.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaProof</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaProof (DeepMind system coupling LLM-style reasoning with Lean theorem prover)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that translates informal mathematical problems into Lean and pairs this translation with reinforcement learning to train models capable of proposing and proving abstractions; demonstrated silver-medal level performance on IMO-style problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AlphaProof (AlphaProof / AlphaGeometry teams)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-guided formal theorem proving (LLM + interactive theorem prover)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mathematics / theorem proving</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>formal proofs / proven theorems</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>highly novel within theorem-proving tasks (automates formalization and proof search for nontrivial problems)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>translate informal problems to formal Lean statements using NLP, then train proof-generating agents with reinforcement learning from those translations</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Formal verification by the Lean theorem prover; proofs are machine-checked within the proof assistant</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported to achieve silver-medal-level performance on IMO-style problems (system scored 28/42 as cited); this is an explicit empirical benchmark result reported in paper text</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation is exact: Lean machine-checks the proofs; thus validation performance is effectively perfect for accepted proofs (soundness guaranteed by Lean), subject to correctness of formalization</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Strong performance when formalization is possible; paper notes limitations in scientific domains where axioms are not agreed upon (natural sciences), constraining applicability</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Here generation and validation are tightly coupled: Lean provides exact validation, but the translation step (informal→formal) can introduce errors that require human verification; thus generation may still outpace trustworthy formalization</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Formal proofs are binary (proved/unproved); no probabilistic uncertainty quantified for proof correctness beyond formal checking</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not applicable for formal proofs; the system's confidence is not needed because Lean certifies correctness</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not discussed quantitatively beyond IMO benchmark; system relies on availability of formalizable statements</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Formal proof-checking rather than proxy metrics</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human oversight recommended for the informal→formal translation stage; frequency depends on difficulty and domain ambiguity</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>highly formal (mathematics) — enables exact verification but paper notes natural sciences lack universally agreed axioms limiting transfer</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Use formal proof assistants to provide machine-checked proofs; training LLMs to formalize informal statements reduces human burden, but translation errors remain a bottleneck.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Paper cites that while Lean-based RL approaches achieve strong proof performance in mathematics, the lack of agreed axioms in many scientific domains and potential translation errors prevent a general solution to verification for natural sciences.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Success on IMO-style problems (28/42) demonstrates that for well-formalized domains generation and verification can be effectively closed.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Formal proof checking is computational but generally cheaper than experimental validation; no numeric ratios provided. The translation and RL training are computationally expensive.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Combining data and theory for derivable scientific discovery with AI-Descartes <em>(Rating: 2)</em></li>
                <li>Evolving scientific discovery by unifying data and background knowledge with AI-Hilbert <em>(Rating: 2)</em></li>
                <li>LLM-SRBench: A new benchmark for scientific equation discovery with large language models <em>(Rating: 2)</em></li>
                <li>LLM-SR: Scientific equation discovery via programming with large language models <em>(Rating: 2)</em></li>
                <li>Ai achieves silver-medal standard solving international mathematical olympiad problems <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2069",
    "paper_id": "paper-281079256",
    "extraction_schema_id": "extraction-schema-53",
    "extracted_data": [
        {
            "name_short": "AI-Descartes",
            "name_full": "AI-Descartes (neuro-symbolic generator-verifier framework)",
            "brief_description": "A neuro-symbolic framework that couples symbolic regression (generator) with formal theorem-proving (verifier) to produce candidate symbolic laws and then test their consistency with background theory via a reasoning distance metric.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "AI-Descartes",
            "system_type": "neuro-symbolic system (symbolic regression + automated theorem prover)",
            "scientific_domain": "physical sciences / symbolic equation discovery",
            "output_type": "symbolic mathematical formulas / candidate scientific laws",
            "novelty_level": "moderately novel to highly novel (can propose formulas not present in training data; suitable for rediscovery and novel candidate laws)",
            "generation_method": "mixed-integer nonlinear programming formulation of symbolic regression (enumerative / optimization-based search) that produces candidate formulas fitting data",
            "validation_method": "formal logical verification via a theorem prover comparing candidate formulas to a background theory B; computes empirical error ε(f) and reasoning error β(f) and ranks candidates by both",
            "generation_performance": "generates many candidate symbolic formulas efficiently even from few noisy datapoints; no numeric generation success rates reported in this paper",
            "validation_performance": "validation is formal (theorem-proving) yielding a reasoning distance; paper reports that it can distinguish numerically-accurate but theory-inconsistent formulas from provable ones, but gives no aggregate accuracy/precision metrics",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Not quantified numerically; framework is explicitly designed so reasoning error β(f) flags theory-inconsistent candidates, so validation is stronger for outputs consistent with provided background theory; when background theory is incomplete or inconsistent, validation weakens (reasoning error may be nonzero)",
            "generation_validation_comparison": "Paper emphasizes a gap: generation can produce many hypotheses quickly while verification (reasoning) is more costly and constraining; AI-Descartes mitigates gap by post-hoc formal checking but retains a sequential generator-then-verifier separation which limits joint exploitation of data and theory",
            "uncertainty_quantification": "Implicit via empirical error ε(f) and reasoning error β(f) but no probabilistic uncertainty estimates reported",
            "calibration_quality": "Not reported; calibration of generator confidence vs. verifier outcome not quantified",
            "out_of_distribution_performance": "Not reported quantitatively; designed to reason about unmeasured variables via background theory, improving OOD interpretability when theory is complete",
            "validation_proxy_metrics": "Uses reasoning distance β(f) and empirical error ε(f) rather than plausibility proxies; these are direct proxies for logical consistency and data fit rather than human-judged plausibility",
            "human_validation_required": true,
            "human_validation_frequency": "Paper recommends human oversight, particularly for background-theory specification and handling translation/axiom choice; frequency likely increases with novelty but no numerical schedule provided",
            "formal_verification_used": true,
            "domain_formalization_level": "semi-formal to formal (works best in domains with formalizable background theory, e.g., parts of physics); performance depends strongly on domain formalization completeness",
            "gap_mitigation_strategies": "Generator-verifier architecture: enumerate candidates via SR then formally verify; ranking by combined empirical and reasoning error. This reduces spurious hypotheses but retains sequential separation which the paper notes limits full exploitation of data+theory.",
            "evidence_supporting_gap": "Authors report that many numerically-accurate formulas violate background theory and that reasoning error β(f) successfully filters such formulas; they argue generative methods produce a flood of hypotheses that overwhelm verification pipelines, motivating AI-Descartes' verifier.",
            "evidence_contradicting_gap": "No quantitative contradictory evidence presented; AI-Descartes itself is presented as a mechanism to close the gap but the paper notes it cannot fully exploit data and theory simultaneously (sequential design).",
            "computational_cost_ratio": "Qualitative: verification (theorem proving) is computationally nontrivial relative to generation; no numeric cost ratio reported. Paper implies verification is more expensive and constraining.",
            "uuid": "e2069.0"
        },
        {
            "name_short": "AI-Hilbert",
            "name_full": "AI-Hilbert (theory-integrated polynomial discovery framework)",
            "brief_description": "A theory-guided optimization framework that integrates background axioms into the hypothesis generation process to produce polynomial (or rational) candidate laws together with algebraic certificates of derivability using SOS/SDP relaxations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "AI-Hilbert",
            "system_type": "theory-constrained optimization / symbolic discovery via semidefinite programming (SOS certificates)",
            "scientific_domain": "physical sciences / algebraic equation discovery",
            "output_type": "polynomial (or rational) equations representing candidate physical laws, plus algebraic certificates",
            "novelty_level": "moderately novel (restricted to polynomial/rational hypothesis classes so novelty outside that class is limited)",
            "generation_method": "formulates a mixed-integer polynomial optimization problem combining data fit, complexity penalties, and algebraic constraints; then converts to semidefinite program (SOS) to find polynomials consistent (or approximately consistent) with background theory",
            "validation_method": "exact algebraic certificates of derivability when distance d_c = 0; if d_c &gt; 0 returns approximate certificate; validation is algebraic (certificate from SOS decomposition)",
            "generation_performance": "Generates polynomial candidates constrained by background axioms; effective when target is polynomial/rational; no numeric generation rates reported",
            "validation_performance": "When background theory is complete and consistent, returns exact derivability certificates (strong formal validation). If theory incomplete/inconsistent, returns approximate certificates; no aggregate metrics reported",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Because hypothesis space is restricted to polynomials/rationals, novelty outside that space reduces ability to both generate and validate; validation is strong for in-class discoveries but degrades for out-of-class or when background theory incomplete",
            "generation_validation_comparison": "AI-Hilbert integrates generation and validation (theory used during generation), reducing the generation--&gt;validation gap for polynomial classes; comparison shows smaller gap than sequential approaches but tradeoff is restricted hypothesis class",
            "uncertainty_quantification": "Validation returns a distance d_c to theory and multipliers producing certificates; not probabilistic uncertainty, but distance quantifies discrepancy",
            "calibration_quality": "Not reported as probabilistic calibration; algebraic certificates provide deterministic guarantees when applicable",
            "out_of_distribution_performance": "Out-of-class (non-polynomial) phenomena not well-handled; approximate certificates may indicate mismatch but no numerical OOD metrics provided",
            "validation_proxy_metrics": "Relies on algebraic derivability distance d_c and SOS certificates rather than human plausibility scores",
            "human_validation_required": true,
            "human_validation_frequency": "Human inspection recommended especially when d_c &gt; 0 (approximate derivations) or when background theory may be incomplete; no numeric frequency provided",
            "formal_verification_used": true,
            "domain_formalization_level": "highly formal for algebraic/physics settings (works best when domain can be formalized as polynomial axioms)",
            "gap_mitigation_strategies": "Integrate theory into generation (constrained search) and produce formal SOS certificates; limits hypothesis space to guarantee verifiability",
            "evidence_supporting_gap": "Paper argues that integrating theory into generation reduces verification bottleneck compared to purely generative SR by restricting search to derivable formulas; shows conceptual reduction in post-hoc verification burden but no numerical throughput metrics",
            "evidence_contradicting_gap": "No direct contradiction; method trades broader generative capacity for verifiability, demonstrating a design choice rather than a refutation of the gap.",
            "computational_cost_ratio": "Qualitative: solving SOS/SDP certificates during generation is computationally expensive relative to unconstrained SR generation, but yields stronger guarantees; no numeric ratio reported.",
            "uuid": "e2069.1"
        },
        {
            "name_short": "GPT-5 (example)",
            "name_full": "GPT-5 (state-of-the-art large language model, as used in examples)",
            "brief_description": "A recent large language model used here as an off-the-shelf symbolic reasoner/generator; in the paper it is shown to sometimes correctly derive symbolic formulas (example: solved one artificial problem and performed partial symbolic regression attempts on Kepler data).",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-5",
            "system_type": "large language model (transformer)",
            "scientific_domain": "mathematics / symbolic equation discovery / general scientific hypothesis generation",
            "output_type": "symbolic expressions, candidate functional forms, natural-language hypotheses and derivations",
            "novelty_level": "moderately novel (can produce novel combinations not verbatim from training data but often simple functional forms; may rediscover known laws)",
            "generation_method": "sequence modeling using patterns learned from large corpora; performs limited symbolic-regression-like reasoning when prompted (tries multiple functional forms)",
            "validation_method": "No formal internal scientific validation; plausibility often judged by surface coherence and human review; in examples the authors compared GPT-5 outputs to ground-truth formulas",
            "generation_performance": "Qualitatively: GPT-5 produced the correct expression for one artificial example and explored multiple candidate forms for Kepler-data example; no quantitative generation accuracy reported beyond anecdotal success/failure examples in appendix",
            "validation_performance": "Model itself does not perform formal validation; validation in paper was external (authors checked correctness). No accuracy/precision metrics reported for validation by GPT-5",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Paper shows GPT-5 outperformed GPT-4 on a simple artificial example (GPT-5 returned correct derivation while GPT-4 failed), but overall performance degrades on novel or complex symbolic tasks; no numeric scaling reported",
            "generation_validation_comparison": "Generation is fast and often flexible; validation remains manual or external — authors highlight a gap where LLM generation outpaces available verification and LLMs can hallucinate plausible but incorrect formulas",
            "uncertainty_quantification": "GPT-5 does not provide formal uncertainty estimates about correctness in these examples; no calibrated confidence scores reported",
            "calibration_quality": "Not reported; authors note models produce confident but sometimes false statements (hallucinations), implying poor calibration of truth-confidence",
            "out_of_distribution_performance": "Limited / deteriorates on tasks requiring formal derivations beyond training-distribution patterns; anecdotal improvement over GPT-4 but no quantified OOD metrics",
            "validation_proxy_metrics": "LLM's surface plausibility and internal likelihoods (token probabilities) can act as proxies, but the paper emphasizes these are plausibility rather than provability",
            "human_validation_required": true,
            "human_validation_frequency": "Required for each scientific output in the examples; frequency presumed to increase with novelty and complexity",
            "formal_verification_used": false,
            "domain_formalization_level": "varies; LLMs operate across many domains, including poorly formalized domains; their outputs should be validated more in less formal domains",
            "gap_mitigation_strategies": "The paper suggests coupling LLMs with formal verifiers or domain tools (e.g., theorem provers, simulations, domain-specific tools) but does not present an integrated system in this work",
            "evidence_supporting_gap": "Authors show GPT-5 can generate correct formulas but argue generative models produce many unverified hypotheses and can hallucinate (citing examples of fabricated legal cases and biomedical references). Appendix shows GPT-5 succeeded where GPT-4 failed on a toy derivation, illustrating generation advances but not validation improvements.",
            "evidence_contradicting_gap": "GPT-5's successful derivation for the artificial example shows generation can sometimes reach correct derivations; however this is anecdotal and does not contradict the overall generation-validation gap claim.",
            "computational_cost_ratio": "Generation (LLM inference) is inexpensive relative to formal verification or laboratory experiments; no numeric ratio reported but textual claim: generation is fast while verification remains slow/expensive.",
            "uuid": "e2069.2"
        },
        {
            "name_short": "GPT-4 (example)",
            "name_full": "GPT-4 (large language model used as baseline)",
            "brief_description": "An earlier-generation large language model used in appendix examples; failed to return correct symbolic formula on a simple artificial task where GPT-5 succeeded, illustrating variability of LLM symbolic reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-4",
            "system_type": "large language model (transformer)",
            "scientific_domain": "mathematics / symbolic equation discovery",
            "output_type": "symbolic expressions and natural-language derivations",
            "novelty_level": "incremental to moderately novel depending on prompt; less capable on symbolic derivations than GPT-5 in examples",
            "generation_method": "pattern completion from large corpora; prompts drive generation",
            "validation_method": "No internal formal validation; outputs validated externally by authors comparing to ground-truth formula in appendix",
            "generation_performance": "In the toy example reported, GPT-4 failed to return the correct function while GPT-5 succeeded; no broader performance metrics provided",
            "validation_performance": "None intrinsic; authors used external comparison to ground truth to assess correctness in examples",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Degrades on tasks requiring exact symbolic derivations or formal reasoning beyond rote patterns; example shows a failure on a simple synthetic task",
            "generation_validation_comparison": "LLM generation capability exceeded internal validation — outputs required human checking; GPT-4 produced incorrect confident output in the example",
            "uncertainty_quantification": "Not provided in the example; model confidence not reported",
            "calibration_quality": "Not reported; example implies overconfidence in incorrect outputs",
            "out_of_distribution_performance": "Poorer than GPT-5 on the provided toy OOD-like derivation; no quantitative metrics",
            "validation_proxy_metrics": "Surface coherence and human-judged plausibility used by humans to filter outputs; not formal",
            "human_validation_required": true,
            "human_validation_frequency": "Every output in examples was human-validated; frequency increases with novelty",
            "formal_verification_used": false,
            "domain_formalization_level": "operates across domains; performance depends on domain formalizability",
            "gap_mitigation_strategies": "Paper suggests combining LLMs with formal provers or constrained generation methods but does not implement for GPT-4 in this work",
            "evidence_supporting_gap": "Direct example: GPT-4 failed to produce the correct symbolic formula while GPT-5 succeeded, showing generation ability varies and outputs can be incorrect and require verification",
            "evidence_contradicting_gap": "No contradictory evidence; example supports gap claim.",
            "computational_cost_ratio": "LLM inference cheap compared to formal verification or experiments; no numeric ratio reported.",
            "uuid": "e2069.3"
        },
        {
            "name_short": "Symbolic Regression Engines (PySR, AI Feynman, RSRM)",
            "name_full": "Symbolic regression engines including PySR, AI Feynman family, and recent RSRM methods",
            "brief_description": "Search-based tools that generate analytic expressions from data (evolutionary search, physics heuristics, Monte-Carlo/RL-guided search); widely used to propose candidate scientific formulas but typically lack formal provability guarantees.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Symbolic regression engines (PySR, AI Feynman, RSRM, etc.)",
            "system_type": "symbolic regression / evolutionary / optimization-based generators",
            "scientific_domain": "physical sciences, general scientific modeling / equation discovery",
            "output_type": "symbolic mathematical expressions and candidate laws",
            "novelty_level": "moderately novel within hypothesis class explored; can produce compact interpretable formulas not seen in training sets",
            "generation_method": "evolutionary search, annealing, MINLP, Monte Carlo Tree Search combined with RL (RSRM), or physics-inspired heuristics (AI Feynman)",
            "validation_method": "Primarily empirical: fit-to-data metrics (error, parsimony, complexity trade-offs). Some works add constraint checks or counterexample-guided refinement but formal theorem-proving not standard",
            "generation_performance": "These systems can rapidly enumerate many candidate formulas and find compact fits; performance depends on data quality and problem; no global numeric rates provided in paper",
            "validation_performance": "Validation commonly via held-out error, parsimony/Pareto fronts, and domain-expert inspection; formal guarantees absent so validation is weaker than theorem-prover approaches",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "More novel (out-of-class) formulas are more likely to be numerical fits without theoretical grounding; these engines frequently produce plausible fits that do not generalize",
            "generation_validation_comparison": "Generation capacity is high (many candidates) while validation is limited to empirical fit and heuristics, creating a verification bottleneck noted by authors",
            "uncertainty_quantification": "Typically none beyond fit residuals and model-complexity penalties; no calibrated probabilistic uncertainty reported in discussed works",
            "calibration_quality": "Not reported; implication is low calibration wrt theoretical correctness",
            "out_of_distribution_performance": "Tends to overfit in OOD settings; risk of producing non-generalizable formulas",
            "validation_proxy_metrics": "Uses empirical error, complexity/power-law regularizers, Pareto fronts as proxies for validity rather than formal derivability",
            "human_validation_required": true,
            "human_validation_frequency": "Often required for top candidates; frequency increases with novelty because expertise needed to assess physical plausibility",
            "formal_verification_used": false,
            "domain_formalization_level": "works across domains but strongest where empirical laws dominate (physics); lacks formal-theory integration by default",
            "gap_mitigation_strategies": "Some works embed constraints or use counterexample-guided refinement (e.g., LGML, LGGA, counterexample-guided SR); paper notes these still only enforce functional-form plausibility not full background-theory provability",
            "evidence_supporting_gap": "Authors cite proliferation of SR outputs that fit data but lack theoretical grounding and overwhelm verification pipelines; reference multiple SR tools and note absence of formal reasoning",
            "evidence_contradicting_gap": "None presented; some SR methods combined with domain heuristics (AI Feynman) improve plausibility but not formal provability.",
            "computational_cost_ratio": "Generation (search/evolution) cost moderate; validation (expert review, experiments) often much more expensive; no numeric ratios provided.",
            "uuid": "e2069.4"
        },
        {
            "name_short": "AlphaProof",
            "name_full": "AlphaProof (DeepMind system coupling LLM-style reasoning with Lean theorem prover)",
            "brief_description": "A system that translates informal mathematical problems into Lean and pairs this translation with reinforcement learning to train models capable of proposing and proving abstractions; demonstrated silver-medal level performance on IMO-style problems.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "AlphaProof (AlphaProof / AlphaGeometry teams)",
            "system_type": "LLM-guided formal theorem proving (LLM + interactive theorem prover)",
            "scientific_domain": "mathematics / theorem proving",
            "output_type": "formal proofs / proven theorems",
            "novelty_level": "highly novel within theorem-proving tasks (automates formalization and proof search for nontrivial problems)",
            "generation_method": "translate informal problems to formal Lean statements using NLP, then train proof-generating agents with reinforcement learning from those translations",
            "validation_method": "Formal verification by the Lean theorem prover; proofs are machine-checked within the proof assistant",
            "generation_performance": "Reported to achieve silver-medal-level performance on IMO-style problems (system scored 28/42 as cited); this is an explicit empirical benchmark result reported in paper text",
            "validation_performance": "Validation is exact: Lean machine-checks the proofs; thus validation performance is effectively perfect for accepted proofs (soundness guaranteed by Lean), subject to correctness of formalization",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "performance_vs_novelty": "Strong performance when formalization is possible; paper notes limitations in scientific domains where axioms are not agreed upon (natural sciences), constraining applicability",
            "generation_validation_comparison": "Here generation and validation are tightly coupled: Lean provides exact validation, but the translation step (informal→formal) can introduce errors that require human verification; thus generation may still outpace trustworthy formalization",
            "uncertainty_quantification": "Formal proofs are binary (proved/unproved); no probabilistic uncertainty quantified for proof correctness beyond formal checking",
            "calibration_quality": "Not applicable for formal proofs; the system's confidence is not needed because Lean certifies correctness",
            "out_of_distribution_performance": "Not discussed quantitatively beyond IMO benchmark; system relies on availability of formalizable statements",
            "validation_proxy_metrics": "Formal proof-checking rather than proxy metrics",
            "human_validation_required": true,
            "human_validation_frequency": "Human oversight recommended for the informal→formal translation stage; frequency depends on difficulty and domain ambiguity",
            "formal_verification_used": true,
            "domain_formalization_level": "highly formal (mathematics) — enables exact verification but paper notes natural sciences lack universally agreed axioms limiting transfer",
            "gap_mitigation_strategies": "Use formal proof assistants to provide machine-checked proofs; training LLMs to formalize informal statements reduces human burden, but translation errors remain a bottleneck.",
            "evidence_supporting_gap": "Paper cites that while Lean-based RL approaches achieve strong proof performance in mathematics, the lack of agreed axioms in many scientific domains and potential translation errors prevent a general solution to verification for natural sciences.",
            "evidence_contradicting_gap": "Success on IMO-style problems (28/42) demonstrates that for well-formalized domains generation and verification can be effectively closed.",
            "computational_cost_ratio": "Formal proof checking is computational but generally cheaper than experimental validation; no numeric ratios provided. The translation and RL training are computationally expensive.",
            "uuid": "e2069.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Combining data and theory for derivable scientific discovery with AI-Descartes",
            "rating": 2
        },
        {
            "paper_title": "Evolving scientific discovery by unifying data and background knowledge with AI-Hilbert",
            "rating": 2
        },
        {
            "paper_title": "LLM-SRBench: A new benchmark for scientific equation discovery with large language models",
            "rating": 2
        },
        {
            "paper_title": "LLM-SR: Scientific equation discovery via programming with large language models",
            "rating": 2
        },
        {
            "paper_title": "Ai achieves silver-medal standard solving international mathematical olympiad problems",
            "rating": 1
        }
    ],
    "cost": 0.01803575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>The Need for Verification in AI-Driven Scientific Discovery
1 Sep 2025</p>
<p>Cristina Cornelio c.cornelio@samsung.com 
Samsung AI
CambridgeUK</p>
<p>Takuya Ito 
IBM Research
Yorktown Heights
USA</p>
<p>Ryan Cory-Wright 
Imperial Business School
LondonUK</p>
<p>Sanjeeb Dash 
IBM Research
Yorktown Heights
USA</p>
<p>Lior Horesh 
IBM Research
Yorktown Heights
USA</p>
<p>The Need for Verification in AI-Driven Scientific Discovery
1 Sep 202545C4E3C2E7946BC55E73773029B9338EarXiv:2509.01398v1[cs.AI]
Artificial intelligence (AI) is transforming the practice of science.Machine learning and large language models (LLMs) can generate hypotheses at a scale and speed far exceeding traditional methods, offering the potential to accelerate discovery across diverse fields.However, the abundance of hypotheses introduces a critical challenge: without scalable and reliable mechanisms for verification, scientific progress risks being hindered rather than being advanced.In this article, we trace the historical development of scientific discovery, examine how AI is reshaping established practices for scientific discovery, and review the principal approaches, ranging from data-driven methods and knowledge-aware neural architectures to symbolic reasoning frameworks and LLM agents.While these systems can uncover patterns and propose candidate laws, their scientific value ultimately depends on rigorous and transparent verification, which we argue must be the cornerstone of AI-assisted discovery.</p>
<p>Introduction</p>
<p>The overarching goal of science is to provide a set of universal, accurate, and interpretable explanations that describe the natural world.This involves discovering natural laws that not only make accurate predictions but are also corroborated by existing scientific literature.Such laws have historically been discovered through the scientific method, a systematic process that begins with a question and proceeds through a study phase during which researchers gather all prior knowledge and data pertaining to the phenomenon under investigation.This leads to the formulation of hypotheses, empirical validation, and iterative refinement.The scientific method enables the discovery of verifiable scientific truths by relying on substantiated and repeatable evidence, lending science its legitimacy and credibility.</p>
<p>The shift from dogmatic belief systems to a framework grounded in scientific theory and empirical verification, epitomized by the 16th-century transition from religious authority to human reason through the work of Copernicus, Galileo, and Bruno, marked a fundamental epistemological transformation leading to the Scientific Revolution [Leveillee, 2011].Indeed, Kepler's mathematical description of planetary motion, grounded in Tycho Brahe's observational data and Bacon's advocacy for inductive reasoning, established the foundation for modern empirical science [Bacon, 1878, Westfall, 1977].Critically, the consequences of this verification-driven methodology have been profound in the modern age.Verified discoveries such as germ theory [Latour, 1993], high-yield agricultural practices [Borlaug, 1970], and thermodynamic principles [Smil, 2004] have transformed medicine, food production, and energy systems.These advances were achieved through a disciplined integration of theoretical models and experimental validation.</p>
<p>However, the rate of major discoveries has declined in both absolute and relative terms over the past several decades [Bloom et al., 2020, Bhattacharya and Packalen, 2020, Arora et al., 2018].This decline is arguably due to the exhaustion of simple, low-dimensional theories, and the increasing complexity of modern scientific problems [Cowen, 2011] (see Fig. 1A).Amid these challenges, the rise of machine learning and artificial intelligence has introduced promising tools to augment hypothesis generation and data analysis for scientific discovery.However, many existing implementations of these data-driven systems lack formal mechanisms for logical inference that are essential for verifiable scientific discovery [Platt, 1964].In particular, generative AI models have shown remarkable capacity to rapidly generate novel scientific hypotheses [Gottweis et al., 2025, Yamada et al., 2025, Lu et al., 2024, Jumper et al., 2021].However, these outputs often lack empirical grounding and are frequently disconnected from established theoretical frameworks or domain-specific knowledge.This disconnect has led to an overwhelming influx of unverified hypotheses, straining verification pipelines that are essential for validating scientific discoveries [Beel et al., 2025, Gridach et al., 2025, Kulkarni et al., 2025] (see Fig. 1B).Consequently, developing robust methods to refine and verify hypotheses from data-driven approaches is critical to unlocking the full potential of AI in accelerating scientific progress (Fig. 1C).</p>
<p>Traditional scientific pipeline AI-assisted scientific pipeline</p>
<p>Verification bottleneck</p>
<p>Widen verification via automated verification</p>
<p>Accelerate scientific discoveries</p>
<p>Hypotheses crafted from theory and ontology</p>
<p>Rapid &amp; abundant AI-generated hypotheses</p>
<p>The Need for Improved Verification Methods</p>
<p>AI-generated hypotheses</p>
<p>Hypotheses Many Many Few Scientific Discoveries</p>
<p>A B C</p>
<p>Figure 1: The scientific method in the age of AI.A) In the traditional scientific method, theories guide the generation of testable hypotheses, which are then validated through experiments and data.B) However, with generative AI, hypotheses can be rapidly produced from data, but verification still relies on slow, manual evaluation by domain experts.C) Without widening this verification bottleneck (e.g., through automated/integrated verification) the pace of discovery remains limited, despite the acceleration promised by AI.</p>
<p>To address the limitations of purely data-driven approaches, several recent works propose hybrid frameworks.These approaches integrate machine learning with elements of symbolic reasoning, constraints imposition, and formal logic, aiming to ensure scientific validity alongside predictive accuracy (e.g., see Wiberg et al. [2025] for a review on integration between AI/ML and Operations Research techniques).For example, Kolmogorov-Arnold Networks (KANs) [Liu et al., 2025] replace fixed linear weights with learnable univariate functions, producing interpretable approximations of scientific relations.Hamiltonian Neural Networks (HNNs) [Greydanus et al., 2019] similarly enforce energy conservation by learning a Hamiltonian and deriving system dynamics from Hamilton's equations.While the learned models from both systems respect structural embeddings, verification for both systems is limited to their specific structural property.More recently, AI-Descartes [Cornelio et al., 2023a] introduced a general verification mechanism, where hypotheses were generated via a data-driven approach and later verified against known theory via theorem proving.Building upon that work, AI-Hilbert [Cory-Wright et al., 2024] integrated data and theory directly during the hypothesis generation stage, thereby constraining the search to expressions consistent with both data and theory.While both these approaches provide scientifically verifiable results, their application is limited to specific problem formulations in the physical sciences.Thus, while emerging computational tools offer great promise for broadly accelerating scientific discovery, their effectiveness hinges on ensuring resulting insights are not only predictive but also interpretable, verifiable, and aligned with foundational scientific knowledge.</p>
<p>In this article, we review recent progress in AI-driven scientific discovery, while underscoring the critical importance of verifying these methods throughout the discovery process.We begin by highlighting key historical examples where the failure to rigorously verify computational methods led to the collapse of critical missions, resulting in significant loss of life and financial cost.Next, we examine recent data-driven approaches to scientific discovery, highlighting their ability to uncover patterns and generate hypotheses from large datasets, particularly in domains where theoretical models are incomplete or unavailable.This is followed by a comparison with knowledge-aware methods, the emergence of derivable models that integrate symbolic reasoning, and the growing role of large language models (LLMs) in automating and augmenting scientific workflows.We conclude with a broader perspective on the heterogeneous role of verification across scientific domains, outlining current challenges and suggesting promising approaches for future research.</p>
<p>The Importance of Verification: Evidence from Examples</p>
<p>In 1999, NASA's Mars Climate Orbiter was lost when thruster data delivered in pounds per second was interpreted as Newtons per second, a unit mismatch that sent a $125 million spacecraft into the Martian atmosphere [NASA, 1999].Similarly, in 1983, Air Canada Flight 143 ("Gimli Glider") ran out of fuel mid-air after the ground crew incorrectly converted pounds to kilograms during Canada's metric transition, leaving the aircraft with roughly half the required fuel and forcing a dangerous emergency landing [Lockwood, 1985].Similar errors have been documented in hospitals: children given incorrect doses when weights noted in pounds were treated as kilograms [Bokser, 2013]; in a review of 1, 291 weight-related medication error reports to the Pennsylvania Patient Safety Authority, 23.2% involved pounds-kilograms confusion [Bailey et al., 2016]; and selection of high-strength heparin vials (10, 000 U/mL) were mistaken for low-strength ones [Institute for Safe Medication Practices Canada, 2008].The lesson from these domains is clear: without rigorous, automated verification, minor trivial errors can scale into disasters.</p>
<p>In automated scientific discovery, the same principle applies.Automated model-generation tools have transformed scientific discovery.Symbolic regression engines such as PySR [Cranmer, 2020] and AI Feynman [Udrescu and Tegmark, 2020], as well as neural architectures like Kolmogorov-Arnold Networks (KANs) [Liu et al., 2025], Hamiltonian Neural Networks (HNNs) [Greydanus et al., 2019], and Lagrangian Neural Networks (LNNs) [Cranmer et al., 2020a], now produce many new hypotheses quickly.This proliferation creates a new bottleneck: distinguishing between formulas that merely fit the data and those that are scientifically meaningful (Fig. 1).Without rigorous verification, the flood of generated hypotheses risks overwhelming the scientific process with plausible but superficial results.Thus, verification is an essential filter that separates genuine scientific discoveries from hallucinations or mere noisy interpolations that fail to generalize beyond the observed data.</p>
<p>The challenge of verification is further exacerbated by the rise of LLM-based tools whose reliability can be strongly questioned [Marcus, 2025, Kambhampati, 2024].Well-publicized examples of hallucinations from LLMs include the hallucination of legal cases cited in court filings [Weiss, 2023], the fabrication of biomedical references [Gravel et al., 2023], and outputs violating basic algebraic [Hendrycks et al., 2021] or physical consistency [Wang et al., 2023b].We refer to Zhang et al. [2025] for a recent review of LLMs and their use in scientific discovery.</p>
<p>One might argue that the latest generation of Large Language Models (LLMs), which utilize Reinforcement Learning from Human Feedback (RLHF) to steer outputs toward preferred outputs, already provide a form of verification [Ziegler et al., 2019].However, such feedback is not equivalent to scientific verification for several reasons.First, RLHF operates at the level of plausibility rather than truth: models are rewarded for producing outputs that appear correct to human evaluators.Second, the feedback is inherently partial and subjective, relying on limited annotations that cannot exhaustively cover the space of possible outputs.Third, RLHF provides no guarantees: models fine-tuned with feedback still generate confident but false statements and mathematically inconsistent expressions.</p>
<p>Thus, while reinforcement learning can improve the style and surface reliability of generated hypotheses, it does not address the deeper need for principled, automated verification against background theory and empirical constraints.For scientific discovery, this distinction is crucial: plausibility without proof cannot serve as the foundation of knowledge.</p>
<p>In the mathematical literature, the use of formal proof assistants for verification, such as Lean [De Moura et al., 2015], Coq [Bertot and Castéran, 2013], and Isabelle [Nipkow et al., 2002], has attracted considerable attention.These systems enable mathematical theorems to be expressed in a dependently typed language and verified computationally.For example, they can be used to verify that every result in an introductory analysis textbook is correct [Tao, 2025].Moreover, some recent works pair this technology with LLMs.For instance, the AlphaProof system from DeepMind [AlphaProof and AlphaGeometry teams, 2024] achieved a silver medal at the 2024 International Mathematics Olympiad by translating one million informal mathematical problems into Lean using natural language processing, allowing AlphaProof to be trained using reinforcement learning.However, since there is no commonly agreed-upon set of axioms for the natural sciences (e.g., quantum mechanics and gravity are not consistent), and the process of translating informal problems into formal statements can introduce errors unless verified by a user, a Lean-reinforcement learning approach cannot be broadly applied to scientific discovery.Finally, the growing recognition of verification challenges in AI-driven scientific discovery has catalyzed significant government investment in bridging formal methods with statistical AI approaches.In the United States, DARPA's portfolio is an example of this trend, featuring programs such as expMath [DARPA, 2025], which seeks to accelerate mathematics by developing AI systems capable of proposing and proving abstractions, and The Right Space (TRS) [DARPA, 2024c], which applies scientific machine learning to uncover tractable transformations for complex models.Other initiatives [DARPA, 2024b[DARPA, ,a,d, 2018]], including ReMath, PROVERS, V-SPELLS, and the completed HACMS program, further underscore the emphasis on formal verification in critical domains.These funding priorities reflect an acknowledgment that while generative AI excels at quick hypothesis generation and pattern discovery, scientific applications require the reliability and guarantees that only formal verification methods can provide.</p>
<p>AI Methods for Scientific Discovery</p>
<p>Motivated by the increasing importance of verification in scientific discovery and other domains involving AI, we next review the state-of-the-art methods used for scientific discovery.Figure 2 provides a qualitative map of the landscape, positioning the different major methods categories along three dimensions: the degree to which they are data-driven, the degree to which they are knowledge-driven and their associated computational complexity.We also discuss the limitations of the existing approaches and suggest strategies for future improvement.</p>
<p>Data-driven Methods</p>
<p>The data-driven discovery of symbolic formulae is a long-standing challenge in Artificial Intelligence [Kitano, 2016], and the central difficulty remains how to incorporate verification into the process.A variety of approaches have been proposed [Landajuela et al., 2022], ranging from neural networks designed to mimic human physical reasoning [Iten et al., 2020], to tree-structured LSTMs for handling symbolic expression trees and formula verification [Arabshahi et al., 2018], to logic-constrained GANs for image generation [Marra et al., 2018].Symbolic regression (SR) has played a prominent role in this space, with applications to extracting explicit relations from graph neural networks [Cranmer et al., 2020b], constructing analytic models for reinforcement learning control [Derner et al., 2019], or combining regression with Bayesian models [Jin et al., 2019].The AI Feynman family of methods [Udrescu andTegmark, 2020, Udrescu et al., 2020] exemplifies the integration of neural fitting with physics-inspired heuristics, while tools such as PySR [Cranmer, 2020, Cranmer et al., 2020b] and TuringBot [Schmidt and Lipson, 2009] use evolutionary or annealing-based search strategies to identify parsimonious equations.More recent methods, such as RSRM, combine Monte Carlo Tree Search with reinforcement learning for efficient symbolic exploration [Xu et al., 2024].Despite the progress, none of these approaches incorporates formal reasoning, leaving their outputs vulnerable to producing expressions that fit the data but lack theoretical grounding.</p>
<p>To address this gap, several works have attempted to combine SR or neural methods with logical consistency checking.The LGML system [Scott et al., 2020] augments learning with a module that verifies whether candidate functions satisfy constraints on their functional form, while LGGA [Ashok et al., 2021] extends this approach with genetic algorithms and auxiliary mathematical expressions.Similar ideas appear in Błądek and Krawiec [2019] counterexample-guided SR, in Kubalík et al. [2020,</p>
<p>Knowledge-driven discovery</p>
<p>Derivable models</p>
<p>Knowledge-aware NNs (PINNs, HNNs, LNNs)</p>
<p>Symbolic regression</p>
<p>Data-driven discovery</p>
<p>Automated theorem provers</p>
<p>Computational complexity</p>
<p>Linear regression</p>
<p>Kernel regression</p>
<p>Interactive theorem provers</p>
<p>Nonlinear regression (NNs, trees, etc.) Unsupervised (PCA, ICA, clustering, etc.)</p>
<p>Bayesian modeling</p>
<p>Program synthesis</p>
<p>Inductive Logic Programming</p>
<p>Figure 2: Qualitative Landscape of Computational Methods for Scientific Discovery.Different approaches span the spectrum between data-driven and knowledge-driven discovery.Data-driven methods, such as neural networks, can rapidly generate hypotheses but lack verifiability, whereas theory-driven methods, like automated theorem provers, offer rigorous verification but are often slow and undecidable.Derivable or science-aware approaches aim to bridge this gap by combining datadriven modeling with symbolic guarantees.The associated computational complexity reflects trade-offs between speed, interpretability, and verifiability.Note that this figure provides a qualitative illustration of the landscape of computational tools for scientific discovery, highlighting general trends across major categories.The position of specific methods in these categories may vary depending on data type, approach, or hybrid usage.2021] multi-objective framework that enforces nonlinear constraints as discrete data points, and in Engle and Sahinidis [2021] deterministic mixed-integer programming formulation with derivative constraints.These methods, however, remain limited to constraints on functional form rather than incorporating background-theory axioms that describe the scientific environment itself.</p>
<p>In parallel, the broader neuro-symbolic community has explored the integration of logic constraints into machine learning tools.Approaches include penalizing constraint violations in neural networks [Xu et al., 2018, Wang andPan, 2020] and embedding logical rules in the training process [Cornelio et al., 2023b, Li and Srikumar, 2019, Daniele and Serafini, 2020, Xie et al., 2019, Li et al., 2019].Inductive logic programming and rule induction [Tamaddoni-Nezhad et al., 2021, Sen et al., 2022, Evans and Grefenstette, 2018, Sadeghian et al., 2019, Law et al., 2018] provide another way of extracting logical knowledge from data, while program synthesis has gained renewed interest as a means of combining symbolic reasoning with statistical learning [Sun et al., 2022, Nye et al., 2020, Parisotto et al., 2017, Valkov et al., 2018, Yang et al., 2017].Yet, across all these efforts, formal verification of discovered formulas remains elusive: constraints typically ensure plausibility, not provability.The result is that many systems generate equations that appear valid but are not guaranteed to align with the underlying laws of nature.</p>
<p>Knowledge-aware methods</p>
<p>Scientific discovery and artificial intelligence have traditionally followed separate paradigms: the former rooted in theory and verification, and the latter in data-driven learning.As scientific problems become increasingly complex and data become increasingly abundant yet noisy or incomplete, there is a growing interest in integrating scientific knowledge into machine learning models.The resulting hybrid methods aim to combine the flexibility of learning-based approaches with the structure and generalizability offered by physical laws.</p>
<p>This section surveys approaches that leverage scientific knowledge in AI model design and training.We distinguish between physics-informed models, which learn the unknown solution of known governing equations by training neural networks to minimize data and physics residuals, and physics-inspired models, that encode known structures, such as conservation laws, directly in the network's architecture.We also consider symmetry-informed networks that embed invariance or equivariance directly into model operations, so that transformations of the input induce consistent transformations of the output.</p>
<p>Physics-Informed Neural Networks (PINNs).Physics-Informed Neural Networks (PINNs) incorporate governing physical laws into the learning process by embedding partial differential equations (PDEs) directly into the loss function [Raissi et al., 2019, Cuomo et al., 2022].These models are not intended to discover the governing equations themselves, but rather to approximate their solutions.The key idea is to replace or augment traditional numerical solvers by training a neural network that minimizes a composite loss consisting of: 1) A data loss term measuring the fit to observed data; 2) A physics loss term penalizing violation of the PDE; and 3) A boundary condition loss term ensuring physical consistency.Mathematically, for a PDE of the form F (x, u, ∇u, ∇ 2 u) = 0, the PINN approximates the solution u(x) with a neural network u θ (x), and minimizes:
L total = λ d L data + λ f L physics + λ b L boundary .
This approach has demonstrated success across various domains, including fluid mechanics, heat diffusion, and quantum mechanics.It provides an elegant, mesh-free framework capable of solving high-dimensional PDEs with limited data.</p>
<p>While PINNs represent a significant advance in scientific computing, the method requires carefully balancing multiple loss terms, and is therefore sensitive to network architecture choices.This highlights the importance of systematic hyperparameter optimization strategies.Additionally, the current approach relies on known governing equations as constraints, and the generic network architectures employed do not yet fully exploit problem-specific structural information.These characteristics have motivated active research directions focused on adaptive loss weighting schemes, physics-informed architecture design, and methods for discovering unknown governing equations from data [Lu et al., 2021].</p>
<p>Physics-inspired Neural Networks.Physics-inspired neural networks take a complementary approach: instead of embedding the governing equations into the training loss, they encode physical structure directly into the model architecture.These models are well-suited to systems governed by conservation laws, such as those following Hamiltonian or Lagrangian dynamics.</p>
<p>In Hamiltonian neural networks (HNNs) [Greydanus et al., 2019], the model learns a scalar-valued Hamiltonian function H(q, p), where q and p are generalized coordinates and momenta.The dynamics are then obtained by differentiating H according to Hamilton's equations:
dq dt = ∂H ∂p , dp dt = − ∂H ∂q .
enforcing conservation of energy by design.</p>
<p>Lagrangian neural networks (LNNs) [Cranmer et al., 2020a] instead model the Lagrangian L(q, q) and derive equations of motion via the Euler-Lagrange equations.This enables the incorporation of constraints and yields coordinate-invariant representations.</p>
<p>Physics-inspired networks, thus, encode domain knowledge directly into the architecture, allowing them to model both the its state and evolution in a structured way.However, as noted by Newman et al. [2024], these approaches do not discover the underlying laws; instead, they assume them, modeling the dynamics within the specified structural form.Furthermore, incorporating multiple types of physical constraints simultaneously (e.g., energy and momentum conservation alongside symmetry constraints) remains an open challenge.</p>
<p>Equivariant Neural Networks.Many physical systems exhibit symmetries such as translation, rotation, or permutation invariance.Equivariant neural networks explicitly incorporate such symmetries by ensuring that transformations of the input correspond to equivalent transformations of the output [Cohen and Welling, 2016].Formally, a function f is equivariant with respect to a group G if:
f (g • x) = g • f (x), ∀g ∈ G.
Equivariant Convolutional Neural Networks (G-CNNs), Spherical CNNs, and SE(3)-equivariant graph networks have been developed to model molecular systems, fluid dynamics, and lattice structures, among others [Weiler et al., 2021, Batzner et al., 2022].These networks often lead to improved sample efficiency and generalization.Symmetry-informed networks [Akhound-Sadegh et al., 2023] extend this concept to more general forms of structure, potentially including conservation laws and geometric constraints.These methods can be viewed as a broader class of equivariant models.However, as with physics-inspired networks, they often require manual specification of symmetry constraints and may not scale well when multiple symmetries coexist.</p>
<p>Knowledge-aware AI methods, while promising, still face ongoing challenges as they continue to evolve.Current approaches typically depend on experts to manually encode physical laws, architectural choices, or symmetry constraints into models, which limits scalability and automation.Moreover, the simultaneous incorporation of multiple physical principles presents significant computational and theoretical challenges.The interpretability of these models remains a key concern, as they often function as black boxes that provide limited insight into the underlying physical mechanisms they approximate.Most critically, existing methods typically lack formal guarantees regarding constraint satisfaction.Physical laws are commonly enforced through soft constraints via penalty terms in the loss function, which cannot ensure that the learned models rigorously adhere to all governing physical principles.These challenges underscore the need for formal frameworks that unify data-driven modeling with principled use of background knowledge, supporting rigorous verification.</p>
<p>Derivable models</p>
<p>A different line of work is represented by the methods of AI-Descartes [Cornelio et al., 2023a] and AI-Hilbert [Cory-Wright et al., 2024], which explicitly introduce background theory into the process of scientific discovery.In contrast to most existing methods, which either constrain functional forms or encode structural biases, these frameworks embed general scientific axioms and use them to guide or validate the discovery of candidate laws.AI-Descartes takes a verification-oriented perspective, generating hypotheses from data and then employing formal reasoning to test their consistency with background theory.AI-Hilbert, on the other hand, integrates theory directly into the hypothesis generation process, reducing the search space and enforcing consistency during model generation.[Cornelio et al., 2023a] is a neuro-symbolic framework for automated scientific discovery that couples symbolic regression with formal reasoning.The system adopts a generator-verifier paradigm, where any hypothesis generator can be paired with any formal verifier, allowing the generation of arbitrarily defined models without restrictions on functional classes, grammar, or structure.This modular yet sequential design ensures flexibility but prevents data and theory from being leveraged simultaneously: hypotheses are generated from data first and only then verified, a separation that limits the exploitation of their complementary strengths.</p>
<p>AI-Descartes. AI-Descartes</p>
<p>Formally, the system seeks to discover an unknown symbolic model y = f * (x), where x = (x 1 , . . ., x n ) are independent variables and y is the dependent variable.The inputs are defined as a 4-tuple ⟨B, C, D, M⟩, where B denotes the background knowledge, consisting of domain-specific axioms; C is the hypothesis class, describing the admissible symbolic models via a grammar and functional constraints; D is the dataset of m examples; and M specifies modeler preferences, such as acceptable error bounds or complexity measures.The discovery task is then framed as a multi-objective problem: the candidate function f must fit the data, remain consistent with B, and have bounded complexity and prediction error.</p>
<p>As outlined above, the AI-Descartes architecture is organized around two main modules following a generator-verifier design.The first is a symbolic regression (SR) module, formulated as a mixed-integer nonlinear programming (MINLP) problem, which enumerates candidate formulas that approximate the data and remains effective with very few, noisy data points.The second is a reasoning module, based on a theorem prover, that evaluates the logical relationship between a candidate model and the background theory.In particular, AI-Descartes introduces the concept of a reasoning distance, which measures the discrepancy between predictions of a candidate model f and the predictions of a formula derivable from B (assumed to be complete, i.e., containing all the axioms necessary to derive the ground-truth law).Each candidate hypothesis is evaluated both in terms of its empirical error ε(f ) relative to the data D, and its reasoning error β(f ) relative to the axioms in B. These two scores are combined to rank the hypotheses, with the top-ranked model being selected as the best candidate.The interplay between these two main components allows AI-Descartes to filter out spurious hypotheses that, while numerically accurate, violate known physical or logical constraints.</p>
<p>Unlike prior efforts that embed only structural constraints, AI-Descartes incorporates full background theories, expressed in logical form.This enables it to reason over unmeasured variables not present in the data and over non-obvious relations that go beyond the data itself.Building on this capability, AI-Descartes can also compare alternative background theories (possibly inconsistent to each other) by computing reasoning errors for each and selecting the set of axioms that is the most consistent with the data.</p>
<p>AI Hilbert.AI-Hilbert [Cory-Wright et al., 2024] is a theory-guided framework for automated scientific discovery that integrates background knowledge directly into hypothesis generation.In contrast to post hoc verification, AI-Hilbert couples data and theory in a single synthesis problem: candidate laws are constructed to satisfy the axioms as they are fit to the data.However, the method restricts the hypothesis space to polynomial (or, when admissible, rational) expressions, which enables algebraic constraints from the background theory to be enforced exactly or with controlled slack.</p>
<p>More formally, AI-Hilbert aims to discover an unknown polynomial formula q(•) ∈ R[x] which describes a physical phenomenon, and is consistent with both a background theory and a collection of experimental data.The inputs to AI-Hilbert are a four-tuple (B, D, C(Λ), d c ), where: 1) B denotes the relevant background theory, expressed as a collection of axioms: the union of the inequalities {g 1 (x) ≥ 0, . . ., g k (x) ≥ 0} defining G and the equalities {h 1 (x) = 0, . . ., h l (x) = 0} defining H, where g i , h j ∈ R[x] n (the ring of real polynomials in the n-tuple of variables x ∈ R n ).B is defined over n variables x 1 , . . ., x n .However, only t of these n variables can be measured and are directly relevant for explaining the observed phenomenon.In particular, we let x 1 denote the target variable.The remaining n − t variables appear in the background theory but are not directly observable.The background theory B is defined as complete if it contains all the axioms necessary to formally prove the target formula, and incomplete otherwise.Moreover, B is called inconsistent if it contains axioms that contradict each other, and consistent otherwise.A special case of inconsistency is when a formula that incorrectly describes the studied phenomenon is added to a consistent background theory.2) D := {x i } i∈[m] denotes a collection of data points, or measurements of an observed physical phenomenon, which may be few and noisy.3) C denotes a set of constraints and bounds which depend on a set of hyper-parameters Λ (e.g., bound on the degree of the polynomial q).4) d c (•, G ∩ H) denotes a distance function from an arbitrary polynomial to the background theory.</p>
<p>The AI-Hilbert algorithm has 4 main steps: Pr sd using a mixed-integer conic optimization solver, outputting a candidate formula and a set of
multipliers {α i } k i=1 , {β j } l j=1 .
The formula is of the form q(x) = 0 (where the only monomials with nonzero coefficients are those that only contain the variables x 1 , . . ., x t , the observable variables) and such that q
(x) = α 0 (x) + k i=1 α i (x)g i (x) + l j=1 β j (x)h j (x) if d c (q, G ∩ H) = 0,
which is a certificate of the fact that q is derivable from the complete background theory.If d c &gt; 0, for example, when the background theory is inconsistent or incomplete, then AI-Hilbert returns a certificate that q is approximately derivable from the background theory.</p>
<p>LLMs for Scientific Discovery</p>
<p>Recent advances in generative AI, and particularly large language models (LLMs), have opened new avenues for accelerating scientific discovery [Reddy and Shojaee, 2025].In materials discovery, generative graph-based models such as GNoME have drastically expanded the set of known stable materials, representing an order-of-magnitude increase in crystallographic diversity [Merchant et al., 2023a].More recently, LLMs have been used to extract domain knowledge from scientific literature, generate new material compositions, and guide experimental design, as demonstrated in systems like AtomAgents, which integrate LLM reasoning with alloy design pipelines [Ghafarollahi and Buehler, 2024].</p>
<p>Transformer-based models treat equation discovery as a numeric-to-symbolic generation task [Kamienny et al., 2022].However, state-of-the-art general-purpose LLMs, such as OpenAI GPT-5, still have limitations when it comes to symbolic discovery, often producing only relatively simple functional forms (e.g., when prompted with the binary star data in Cornelio et al. [2023a]).At the same time, their ability to make inferences from simple axiom systems has improved notably compared to older models (see Appendix A for more details).In parallel, multimodal approaches like SNIP embed equations and numerical data into smoother joint spaces to improve search efficiency [Meidani et al., 2024], while systems such as LLM-SR explore the use of LLMs as "scientist agents" that evolve equations in search of governing laws [Shojaee et al., 2024].Benchmarks, such as LLM-SRBench, have recently been introduced to systematically evaluate these methods in scientific equation discovery [Shojaee et al., 2025].These works highlight the growing role of generative and language-based models in pushing symbolic regression beyond handcrafted algorithms toward more generalizable AI-driven discovery.</p>
<p>Alongside these task-specific methods, domain-specialized scientific LLMs are being developed to serve as general-purpose research copilots.NatureLM [Xia et al., 2025] is a foundation model designed to unify the "languages of nature" across molecules, proteins, DNA, RNA, and materials, enabling cross-domain generation and design of drug molecules, protein binders, and CRISPR guides.Similarly, Galactica [Taylor et al., 2022], trained on 106B scientific tokens spanning papers, textbooks, chemical sequences, proteins, and code, outperforms general LLMs on scientific benchmarks and introduces specialized reasoning tokens for step-by-step problem solving.These models illustrate how domain-curated corpora and tailored architectures can significantly advance LLM-based scientific discovery.</p>
<p>Finally, LLMs can be framed as agents rather than passive tools: by coupling their broad knowledge bases with external tool integration, LLM-based agents can design, test, and refine hypotheses in ways that approximate the iterative scientific method.ChemCrow [M.Bran et al., 2024], for example, integrates GPT-4 with chemistry-specific tools for reaction prediction, retrosynthesis planning, and safety assessment, enabling both reasoning and validation within chemical workflows.Multi-agent frameworks, such as SciAgents, extend this paradigm by coordinating specialized LLM-based agents to collaboratively explore biomaterials design [Ghafarollahi and Buehler, 2025].Alongside general frameworks for opendomain hypothesis generation in the social sciences [Yang et al., 2024], biomedicine [Qi et al., 2023], and rediscovery settings such as MOOSE-Chem in chemistry [Yang et al., 2025], these systems demonstrate the potential of LLMs and generative models not only to accelerate discovery in targeted domains such as chemistry and materials science, but also to serve as versatile, reasoning-driven collaborators in the broader pursuit of new scientific laws.</p>
<p>Verification in the age of AI-driven science</p>
<p>Modern engineering industries regularly employ verification in the development and deployment of mission-critical technologies, including those in aerospace, medical devices, and autonomous systems.The rigorous process of verifying the accurate implementation of such technologies ensures that these complex systems function precisely as intended, mitigating risks of failure that could lead to catastrophic loss of life, environmental damage, or severe economic disruption.Through meticulous testing, simulation, and formal methods, verification tests validate the design integrity, software reliability, and hardware performance of technologies where even minor deviations can have profound consequences.Given the potentially far-reaching impacts and high costs of scientific research, why isn't a stringent and widespread culture of independent verification more commonly embedded within modern scientific research, rather than being largely limited to industrial applications?In this section, we illustrate examples of the importance of verification across research communities and outline ways to incorporate verification into scientific research to enhance the rigor of the scientific method for the modern age.</p>
<p>The role of verification across scientific domains</p>
<p>The proliferation of AI models in scientific research presents a transformative opportunity to accelerate the pace of scientific discovery.In particular, generative AI models have demonstrated the ability to produce novel hypotheses at rapid scales and speeds.However, the rapid generation of scientific hypotheses presents significant challenges.Many of these AI-generated hypotheses lack empirical verification and are often disconnected from established theoretical frameworks or domain-specific knowledge.However, the strength of a scientific theory lies in its empirical predictive power [Popper, 1959].Without iterative refinement through empirical verification of hypotheses, scientific theories fail to progress and remaining unable to make useful empirical predictions (see Fig. 3).The strength of a scientific theory lies in its empirical predictive power.Thus, the development of a scientific theory requires iterative empirical verification, with stronger theories offering more accurate predictions of observable phenomena.However, the balance between theoretical strength and predictive power varies across scientific domains, and often depends on the epistemic goals and maturity of each field, as well as the nature of the theories (e.g., formal versus ontological theories).</p>
<p>Empirical</p>
<p>In many applied scientific domains, such as drug discovery or materials science, the term "discovery" often refers primarily to the generation of hypotheses -such as identifying a promising molecular compound or material configuration -rather than their empirical verification [Reidenbach et al., 2025, Merchant et al., 2023b, Jain et al., 2022, Anstine and Isayev, 2023, Takeda et al., 2023].This usage underscores the importance of distinguishing between the act of proposing a candidate and the subsequent process of validating its efficacy, safety, or theoretical soundness.As a result, researchers are increasingly confronted with a deluge of unverified hypotheses, clogging (and potentially slowing) verification pipelines that are critical to validating scientific discoveries.However, verification strategies across scientific domains differ greatly in approach and empirical requirements due to differences in their theories and ontologies, as well as the epistemic goals of each field.Here we briefly discuss the variation of verification strategies across a few scientific domains, namely physical, biological and complex sciences, and clinical sciences.</p>
<p>In the physical sciences, verification is tightly coupled with formal theories and mathematical models.Hypotheses are often derived from well-established physical laws, and their verification typically involves controlled experiments or data-driven simulations that yield quantifiable and reproducible results that integrate and conform to these background laws [Udrescu and Tegmark, 2020].This tight integration of theory and data allows for the use of automated verification techniques that derive data from physical laws and theory [Cornelio et al., 2023a, Cory-Wright et al., 2024].</p>
<p>In contrast, however, many chemical, biological, and cognitive sciences present a more complex landscape for verification [Mock et al., 2024].Unlike physics, chemical, materials, and biological theories are often less formalized and more context-dependent, reflecting the inherent complexity of these systems and the variability of the epistemic goals across scientific domains.For example, verification in biology typically involves manual experimentation, such as genetic manipulation or behavioral observation, and relies heavily on ontological frameworks like evolutionary theory or systems biology, and less on explicit, quantitative laws.Though quantification is still important, it is often within the context of multi-variable and dynamical systems that are difficult to quantitatively derive from first principles.Nevertheless, efforts to build-in background knowledge (or incorporate a knowledgeconstrained search space) can improve the quality and validity of discovered hypotheses, thereby improving (and accelerating) scientific discovery in these domains (e.g., in chemistry [Yang et al., 2025], and in cognitive science [Castro et al., 2025]).</p>
<p>In medical and clinical sciences, there are additional layers of complexity.These tend to be shaped by ethical constraints, human variability, and pragmatic demands of clinical practice.Moreover, theories in clinical research are often probabilistic and population-based, rather than deterministic.Importantly, though the gold standard for verification strategies in clinical trials are randomized control trials, due to practical constraints of clinical research, verification strategies also include observational studies and meta-analyses of existing data.However, in all these cases, verification relies on statistical inference to assess efficacy and safety that are informed by ontological systems such as disease classifications and diagnostic criteria, which evolve over time.</p>
<p>Similar domain-specific variations in verification strategies are evident across various fields, including complex system sciences, earth sciences, social sciences, and engineering, among others, and each is shaped by its unique epistemic and methodological contexts.Despite the diversity of verification strategies across scientific domains, a unifying thread is the reliance on logical reasoning as the foundation for hypothesis testing and theory refinement.Whether through deductive modeling in physics, experimental inference in biology, or statistical evaluation in clinical sciences, the process of verification is fundamentally driven by structured, iterative reasoning.This echoes John Platt's notion of strong inference [Platt, 1964], where progress in science stems from the disciplined application of logic to generate, test, and eliminate hypotheses.While the form and tools of logical inference vary -from mathematical formalism (e.g., physical sciences) to ontological frameworks (e.g., biological sciences) to probabilistic models (e.g., clinical sciences) -the underlying commitment to rational analysis and verification remains constant.</p>
<p>Final Remarks and Future Challenges</p>
<p>In this work, we reviewed how AI is reshaping scientific discovery, with verification as a central open challenge.We reviewed a spectrum of methods, spanning from data-driven models to knowledge-based and hybrid approaches, illustrating their potential to accelerate hypothesis generation while also raising important concerns about their interpretability and reliability.The landscape we outlined highlights both the potential and the limits of contemporary AI, while pointing to the need to advance automated verification methods to improve AI-driven scientific discovery.There are many challenges ahead.In the next section we outline the most critical ones and discuss how they open promising directions for future research.</p>
<p>Challenges in AI-Driven Scientific Discovery</p>
<p>A major challenge for AI-driven scientific discovery is building benchmarks that genuinely capture open-ended scientific discovery and are not captured in the training distribution of existing AI systems.Existing datasets-such as AI Feynman [Udrescu and Tegmark, 2020], SciBench [Wang et al., 2023a], ScienceQA [Lu et al., 2022], and MATH [Hendrycks et al., 2021] focus on rediscovery or textbook-style problem solving, which neglects the complexity of theory formation.This is problematic because LLMs may depend on memorization rather than reasoning [Carlini et al., 2021, Wu et al., 2023], and unlike in theorem proving, most benchmarks lack explicit underlying theory, making verification-based evaluation nearly impossible.Indeed, whether an LLM is capable of making a scientific discovery often depends on the precise prompt used and even the notation used to describe a scientific discovery setting.Recent advances, such as simulated domains for scientific discovery [M.Bran et al., 2024, Shojaee et al., 2024] and the newly proposed LLM-SRBench [Shojaee et al., 2025], take steps toward mitigating memorization and evaluating true discovery.Nonetheless, key gaps remain in creating benchmarks that rigorously test novelty, generalizability, and scientific consistency [Cranmer et al., 2020b].</p>
<p>A second key challenge in AI-driven science is the unification of theory and data, since most existing methods focus either on empirical modeling or formal reasoning in isolation.While LLMs have shown promise in theorem proving [Jiang et al., 2023] and equation discovery from data [Shojaee et al., 2024], integrating these capabilities into a holistic framework remains an open problem.Efforts such as AI-Descartes [Cornelio et al., 2023a] and AI-Hilbert [Cory-Wright et al., 2024], as well as work in neuro-symbolic AI [De Raedt andKimmig, 2015, Ahmed et al., 2022], point toward promising directions for future development.However, challenges persist in deriving rigorous hypotheses from data, combining symbolic and neural approaches, and handling uncertainty within formal reasoning.</p>
<p>A third challenge in AI-driven discovery is ensuring that the use of AI does not overly homogenize science.The traditional scientific method is implemented differently by each scientist.This diversity, including the fact that scientists occasionally make mistakes, is a fundamental strength of science, as it enables different individuals to make distinct discoveries [Elliott, 2004].For instance, Alexander Fleming discovered penicillin by accident [Tan and Tatsumura, 2015], a "mistake" that an AI scientist would be unlikely to make.Ensuring that organic "mistakes" remain a part of the scientific method is another key challenge in the age of AI-driven discovery.</p>
<p>Conclusions</p>
<p>A key conclusion is that AI-driven scientific discovery compels us to reconsider the very notion of the "scientific method" itself.Traditionally, science has been portrayed as a systematic process of hypothesis generation, experimentation, and validation, but this narrative has been repeatedly challenged by philosophers such as Feyerabend [Feyerabend, 1975], who argue that rigid methodological rules neither capture nor enable true scientific progress.With the advent of generative models and inspired by industrial practices, however, we may be entering a new era in which verification becomes not just essential but also the primary bottleneck in scientific discovery.This shift would mark a departure from the traditional scientific method, reframing discovery as an iterative dialogue between creativity and verification, potentially laying the new groundwork for a new scientific paradigm.</p>
<p>[Step 1] The background theory B and data D are combined to generate a polynomial optimization problem Pr which targets a specific concept identified by the target variable x 1 .This is achieved by minimizing the distance d c , the model complexity and the error on the data, while integrating the bounds and constraints C. [Step 2] Pr is then reformulated as a semidefinite (or linear if no inequalities are present in the background theory) optimization problem Pr sd , by leveraging standard techniques from SOS optimization.[Step 3] Next, AI-Hilbert solves</p>
<p>Figure 3 :
3
Figure3: The role of verification in the development of scientific theories.The strength of a scientific theory lies in its empirical predictive power.Thus, the development of a scientific theory requires iterative empirical verification, with stronger theories offering more accurate predictions of observable phenomena.However, the balance between theoretical strength and predictive power varies across scientific domains, and often depends on the epistemic goals and maturity of each field, as well as the nature of the theories (e.g., formal versus ontological theories).</p>
<p>Figure 4 :
4
Figure 4: Prompt given to GPT-5 for binary star data used in AI Descartes (with variables relabeled as (d, m 1 , m 2 , p) → (x, y, z, u) and data columns permuted compared to the original dataset) and the output returned by GPT-5.The desired formula is u =</p>
<p>Figure 5 :
5
Figure5: Prompt and output given to GPT-4 for a simple artificial (not arising from any physical theory) example of the type used in AI Descartes.GPT-4 did not return a correct answer, which is f (d, k, z, g) = kzg z−d , whereas GPT-5 did (see Figure6for a comparison with GPT-5 on the same prompt).</p>
<p>Figure 6 :
6
Figure6: Prompt and output given to GPT-5 for a simple artificial (not arising from any physical theory) example of the type used in AI Descartes.GPT-4 did not return a correct answer, which is f (d, k, z, g) = kzg z−d , whereas GPT-5 did (see Figure5for a comparison with GPT-4 on the same prompt).</p>
<p>A AppendixIn this section, we give two examples of simple scientific discovery related queries given to a state-ofthe-art LLM, specifically GPT-5, the latest version of ChatGPT.In the first, we take data given in AI Descartes[Cornelio et al., 2023a]for two binary stars revolving around a common center of gravity and ask GPT-5 to find a function that best fits the data.The target function in this example is Kepler's third law of planetary motion.The data is scaled in such a manner that the period of revolution p is equal towhere d is the distance between the binary stars, and m 1 and m 2 stand for their masses.We rename the variables, (d, m 1 , m 2 , p) → (x, y, z, u), to avoid giving away information about the problem to GPT-5.In Figure4we show the prompt given to GPT-5 and its output.One can see that GPT-5 tries out a number of different functional forms -in other words it performs a limited symbolic regression exercise -and does not produce the desired function as a candidate solution.In Figure5we give the prompt at the top to GPT-4 and show its output, while in Figure6we show instead the output of GPT-5 on the same prompt.It is clear that GPT-4 fails to reason accurately with the axioms and comes up with the correct expression of the functional form relating the variables other than x, whereas GPT-5 produces the correct answer f (d, k, z, g) = kzg z−d and also the correct derivation.
Semantic probabilistic layers for neuro-symbolic learning. K Ahmed, S Teso, K.-W Chang, G Van Den Broeck, A Vergari, Advances in Neural Information Processing Systems. 202235</p>
<p>problems-at-silver-medal-level/. Announces AlphaProof (Lean-based formal reasoning) and AlphaGeometry 2. T Akhound-Sadegh, L Perreault-Levasseur, J Brandstetter, M Welling, S Ravanbakhsh, arXiv:2311.04293Lie point symmetry and physics informed networks. 2023. 20247arXiv preprintAi achieves silver-medal standard solving international mathematical olympiad problems. system scored 28/42 on IMO 2024</p>
<p>Generative Models as an Emerging Paradigm in the Chemical Sciences. D M Anstine, O Isayev, 10.1021/jacs.2c13467Journal of the American Chemical Society. 0002-786314516Apr. 2023American Chemical Society</p>
<p>Combining symbolic expressions and black-box function evaluations in neural programs. F Arabshahi, S Singh, A Anandkumar, In ICLR. 2018</p>
<p>The decline of science in corporate R&amp;D. A Arora, S Belenzon, A Patacconi, Strategic Management Journal. 3912018</p>
<p>Logic guided genetic algorithms (student abstract). D Ashok, J Scott, S J Wetzel, M Panju, V Ganesh, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceMay 202135</p>
<p>F Bacon, Novum organum. Clarendon press1878</p>
<p>Update on medication errors associated with incorrect patient weights. B R Bailey, M J Gaunt, M J Grissinger, Pennsylvania Patient Safety Advisory. 13206 2016Analysis found. 2% of events involved pounds-kilograms confusion (N=1,291</p>
<p>E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials. S Batzner, A Musaelian, L Sun, M Geiger, J P Mailoa, M Kornbluth, N Molinari, T E Smidt, B Kozinsky, Nature Communications. 1312022</p>
<p>Evaluating sakana's ai scientist for autonomous research: Wishful thinking or an emerging reality towards' artificial research intelligence. J Beel, M.-Y Kan, M Baumgart, arXiv:2502.142972025arXiv preprint</p>
<p>Interactive theorem proving and program development: Coq'Art: the calculus of inductive constructions. Y Bertot, P Castéran, 2013Springer Science &amp; Business Media</p>
<p>Stagnation and scientific incentives. J Bhattacharya, M Packalen, 2020National Bureau of Economic ResearchTechnical report</p>
<p>Solving symbolic regression problems with formal constraints. I Błądek, K Krawiec, The Genetic and Evolutionary Computation Conference (GECCO '19). Prague, Czech RepublicACMJuly 13-17, 2019. 2019</p>
<p>Are ideas getting harder to find?. N Bloom, C I Jones, J Van Reenen, M Webb, American Economic Review. 11042020</p>
<p>S J Bokser, A weighty mistake. Agency for Healthcare Research and Quality (AHRQ). PSNet WebM&amp;M case commentary2013</p>
<p>Nobel lecture: The green revolution, peace, and humanity. N Borlaug, 1970</p>
<p>Extracting training data from large language models. N Carlini, F Tramer, E Wallace, M Jagielski, A Herbert-Voss, K Lee, A Roberts, T Brown, D Song, U Erlingsson, 30th USENIX Security Symposium (USENIX Security 21). 2021</p>
<p>Discovering Symbolic Cognitive Models from Human and Animal Behavior. P S Castro, N Tomasev, A Anand, N Sharma, R Mohanta, A Dev, K Perlin, S Jain, K Levin, N Éltető, W Dabney, A Novikov, G C Turner, M K Eckstein, N D Daw, K J Miller, K L Stachenfeld, 10.1101/2025.02.05.636732v1Feb. 2025New ResultsSection</p>
<p>T Cohen, M Welling, Group equivariant convolutional networks. International Conference on Machine Learning (ICML). 2016</p>
<p>Combining data and theory for derivable scientific discovery with AI-Descartes. C Cornelio, S Dash, V Austel, T R Josephson, J Goncalves, K L Clarkson, N Megiddo, B El Khadir, L Horesh, Nature Communications. 14117772023aNature Publishing Group</p>
<p>Learning where and when to reason in neuro-symbolic inference. C Cornelio, J Stuehmer, S X Hu, T Hospedales, International Conference on Learning Representations. 2023b</p>
<p>Evolving scientific discovery by unifying data and background knowledge with ai hilbert. R Cory-Wright, C Cornelio, S Dash, B El Khadir, L Horesh, Nature Communications. 15159222024</p>
<p>The great stagnation: How America ate all the low-hanging fruit of modern history, got sick, and will (eventually) feel better: A Penguin eSpecial from Dutton. T Cowen, 2011Penguin</p>
<p>PySR: Fast &amp; parallelized symbolic regression in Python/Julia. M Cranmer, 10.5281/zenodo.4041459Sept. 2020</p>
<p>M Cranmer, S Greydanus, S Hoyer, P Battaglia, D Spergel, S Ho, Lagrangian neural networks. International Conference on Learning Representations (ICLR). 2020a</p>
<p>Discovering symbolic models from deep learning with inductive biases. M Cranmer, A Sanchez-Gonzalez, P Battaglia, R Xu, K Cranmer, D Spergel, S Ho, NeurIPS. 2020. 2020b</p>
<p>Scientific machine learning through physics-informed neural networks: Where we are and what's next. S Cuomo, V Di Cola, F Giampaolo, G Rozza, M Raissi, F Piccialli, 10.1007/s10915-022-01939-z.pdfJournal of Scientific Computing. 9232022</p>
<p>A Daniele, L Serafini, arXiv:, 2009.06087Neural networks enhancement with logical knowledge. 2020</p>
<p>DARPA. High assurance cyber military systems (HACMS). 2018</p>
<p>reasoning-of-verifiers-enabling-robust-systems. DARPA. Pipelined reasoning of verifiers enabling robust systems (PROVERS). 2024a</p>
<p>DARPA. Recovery of symbolic mathematics from code. 2024bReMath</p>
<p>DARPA. The right space (TRS). 2024c</p>
<p>verified-security-and-performance-enhancement-of-large-legacy-software. DARPA. Verified security and performance enhancement of large legacy software V-SPELLS. 2024d</p>
<p>Exponentiating mathematics (expMath. DARPA. 2025</p>
<p>The lean theorem prover (system description. L De Moura, S Kong, J Avigad, F Van Doorn, J Raumer, International Conference on Automated Deduction. Springer2015</p>
<p>Probabilistic (logic) programming concepts. L De Raedt, A Kimmig, 10.1007/s10994-015-5494-z.pdfMachine Learning. 2015100</p>
<p>Symbolic regression for constructing analytic models in reinforcement learning. E Derner, J Kubalík, N Ancona, R Babuska, arXiv:, 1903.114832019</p>
<p>Error as means to discovery. K Elliott, Philosophy of Science. 7122004</p>
<p>Deterministic symbolic regression with derivative information: General methodology and application to equations of state. M R Engle, N V Sahinidis, AIChE Journal. e174572021</p>
<p>Learning explanatory rules from noisy data. R Evans, E Grefenstette, Journal of Artificial Intelligence Research. 612018</p>
<p>Against Method: Outline of an Anarchistic Theory of Knowledge. P Feyerabend, New Left Books. 1975</p>
<p>Atomagents: Alloy design and discovery through physics-aware multi-modal multi-agent artificial intelligence. A Ghafarollahi, M J Buehler, arXiv:2407.100222024arXiv preprint</p>
<p>Sciagents: automating scientific discovery through bioinspired multi-agent intelligent graph reasoning. A Ghafarollahi, M J Buehler, 10.1002/adma.202413523Advanced Materials. 372224135232025</p>
<p>Towards an ai co-scientist. J Gottweis, W.-H Weng, A Daryin, T Tu, A Palepu, P Sirkovic, A Myaskovsky, F Weissenberger, K Rong, R Tanno, arXiv:2502.188642025arXiv preprint</p>
<p>Learning to fake it: Limited responses and fabricated references provided by chatgpt for medical questions. J Gravel, M D'amours-Gravel, E Osmanlliu, 10.1016/j.mcpdig.2023.05.004Mayo Clinic Proceedings: Digital Health. 132023. 2023 Sep</p>
<p>Hamiltonian neural networks. S Greydanus, M Dzamba, J Yosinski, Advances in Neural Information Processing Systems. 201932</p>
<p>Agentic ai for scientific discovery: A survey of progress, challenges, and future directions. M Gridach, J Nanavati, K Z E Abidine, L Mendes, C Mack, arXiv:2503.089792025arXiv preprint</p>
<p>Institute for Safe Medication Practices Canada. Enhancing safety with unfractionated heparin: A national and international area of focus. D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks. the Neural Information Processing Systems Track on Datasets and Benchmarks2021. 2008. 20088Bulletin reference #5 cites the ISMP Medication Safety Alert!. on heparin errors</p>
<p>Discovering physical concepts with neural networks. R Iten, T Metger, H Wilming, L Rio, R Renner, Physical Review Letters. 1242020</p>
<p>Biological Sequence Design with GFlowNets. M Jain, E Bengio, A Hernandez-Garcia, J Rector-Brooks, B F P Dossou, C A Ekbote, J Fu, T Zhang, M Kilgour, D Zhang, L Simine, P Das, Y Bengio, Proceedings of the 39th International Conference on Machine Learning. the 39th International Conference on Machine LearningPMLRJune 2022</p>
<p>Draft, sketch, and prove: Guiding formal theorem provers with informal proofs. A Q Jiang, S Welleck, J P Zhou, W Li, J Liu, M Jamnik, T Lacroix, Y Wu, G Lample, International Conference on Learning Representations. 2023</p>
<p>Bayesian symbolic regression. Y Jin, W Fu, J Kang, J Guo, J Guo, arXiv[Methodology]:, 1910.088922019</p>
<p>Highly accurate protein structure prediction with AlphaFold. J Jumper, R Evans, A Pritzel, T Green, M Figurnov, O Ronneberger, K Tunyasuvunakool, R Bates, A Žídek, A Potapenko, A Bridgland, C Meyer, S A A Kohl, A J Ballard, A Cowie, B Romera-Paredes, S Nikolov, R Jain, J Adler, T Back, S Petersen, D Reiman, E Clancy, M Zielinski, M Steinegger, M Pacholska, T Berghammer, S Bodenstein, D Silver, O Vinyals, A W Senior, K Kavukcuoglu, P Kohli, D Hassabis, 10.1038/s41586-021-03819-2Nature. 1476-46875967873Aug. 2021Nature Publishing Group</p>
<p>Can large language models reason and plan?. S Kambhampati, Annals of the New York Academy of Sciences. 153412024</p>
<p>End-to-end symbolic regression with transformers. P.-A Kamienny, S Ascoli, G Lample, F Charton, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>Artificial intelligence to win the Nobel prize and beyond: Creating the engine for scientific discovery. H Kitano, AI Magazine. 371Apr. 2016</p>
<p>Symbolic regression driven by training data and prior knowledge. J Kubalík, E Derner, R Babuška, Proceedings of the 2020 Genetic and Evolutionary Computation Conference. the 2020 Genetic and Evolutionary Computation Conference2020</p>
<p>Multi-objective symbolic regression for physics-aware dynamic modeling. J Kubalík, E Derner, R Babuška, Expert Systems with Applications. 1821152102021</p>
<p>A Kulkarni, F Alotaibi, X Zeng, L Wu, T Zeng, B M Yao, M Liu, S Zhang, L Huang, D Zhou, arXiv:2505.04651Scientific hypothesis generation and validation: Methods, datasets, and future directions. 2025arXiv preprint</p>
<p>A unified framework for deep symbolic regression. M Landajuela, C S Lee, J Yang, R Glatt, C P Santiago, T N Mundhenk, I Aravena, G Mulcahy, B Petersen, Advances in Neural Information Processing Systems (NeurIPS). 2022</p>
<p>The Pasteurization of France. B Latour, 1993Harvard University Press</p>
<p>Inductive learning of answer set programs from noisy examples. M Law, A Russo, K Broda, arXiv:, 1808.084412018</p>
<p>Copernicus, galileo, and the church: Science in a religious world. N P Leveillee, Inquiries Journal. 3052011</p>
<p>T Li, V Srikumar, arXiv:, 1906.06298Augmenting neural networks with first-order logic. 2019</p>
<p>T Li, V Gupta, M Mehta, V Srikumar, arXiv:, 1909.00126A logic-driven framework for consistency of neural models. 2019</p>
<p>KAN: Kolmogorov-arnold networks. Z Liu, Y Wang, S Vaidya, F Ruehle, J Halverson, M Soljačić, T Y Hou, M Tegmark, International Conference on Learning Representations (ICLR). 2025</p>
<p>G H Lockwood, Final report of the board of inquiry: Accident involving air canada boeing 767 c-gaun at gimli, manitoba. Lockwood23 july 1983. 1985Government catalogue confirms publication details</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. C Lu, C Lu, R T Lange, J Foerster, J Clune, D Ha, arXiv:2408.062922024arXiv preprint</p>
<p>Learning nonlinear operators via deeponet based on the universal approximation theorem of operators. L Lu, P Jin, G Pang, Z Zhang, G E Karniadakis, 10.1038/s42256-021-00302-5Nature Machine Intelligence. 332021</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. P Lu, S Mishra, T Xia, L Qiu, K.-W Chang, S.-C Zhu, O Tafjord, P Clark, A Kalyan, Advances in Neural Information Processing Systems. 202235</p>
<p>Augmenting large language models with chemistry tools. A M Bran, S Cox, O Schilter, C Baldassari, A D White, P Schwaller, Nature Machine Intelligence. 652024</p>
<p>Llms are not like you and me-and never will be. G Marcus, 2025</p>
<p>G Marra, F Giannini, M Diligenti, M Gori, arXiv:, 1807.09202Constraint-based visual generation. 2018</p>
<p>SNIP: Bridging mathematical symbolic and numeric realms with unified pre-training. K Meidani, P Shojaee, C K Reddy, A B Farimani, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Scaling deep learning for materials discovery. A Merchant, S Batzner, S S Schoenholz, M Aykol, G Cheon, E D Cubuk, Nature. 62479902023a</p>
<p>Scaling deep learning for materials discovery. A Merchant, S Batzner, S S Schoenholz, M Aykol, G Cheon, E D Cubuk, 10.1038/s41586-023-06735-9Nature. 1476-46876247990Dec. 2023b</p>
<p>Recent advances in generative biology for biotherapeutic discovery. M Mock, C J Langmead, P Grandsard, S Edavettal, A Russell, 10.1016/j.tips.2024.01.003Trends in Pharmacological Sciences. 0165-6147453Mar. 2024Elsevier</p>
<p>NASA. Mars climate orbiter mishap investigation board phase i report. 1999</p>
<p>Stable tensor neural networks for efficient deep learning. E Newman, L Horesh, H Avron, M E Kilmer, 10.3389/fdata.2024.1363978/pdfFrontiers in Big Data. 713639782024</p>
<p>Isabelle/HOL: a proof assistant for higher-order logic. T Nipkow, M Wenzel, L C Paulson, 2002Springer</p>
<p>Learning compositional rules via neural program synthesis. M Nye, A Solar-Lezama, J Tenenbaum, B M Lake, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M Balcan, H Lin, Curran Associates, Inc202033</p>
<p>E Parisotto, A -R. Mohamed, R Singh, L Li, D Zhou, P Kohli, Neuro-symbolic program synthesis. International Conference on Learning Representations. 2017</p>
<p>Strong Inference. J R Platt, 10.1126/science.146.3642.347Science. 1463642Oct. 1964American Association for the Advancement of Science</p>
<p>The logic of scientific discovery. K R Popper, 1959Publisher: Basic Books</p>
<p>Large language models are zero shot hypothesis proposers. B Qi, K Zhang, H Li, K Tian, S Zeng, Z.-R Chen, B Zhou, NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. 2023</p>
<p>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. M Raissi, P Perdikaris, G E Karniadakis, Journal of Computational Physics. 3782019</p>
<p>Towards scientific discovery with generative ai: Progress, opportunities, and challenges. C K Reddy, P Shojaee, 10.1609/aaai.v39i27.35084Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceApr. 202539</p>
<p>Applications of Modular Co-Design for De Novo 3D Molecule Generation. D Reidenbach, F Nikitin, O Isayev, S Paliwal, arXiv:2505.18392May 2025</p>
<p>Drum: End-to-end differentiable rule mining on knowledge graphs. A Sadeghian, M Armandpour, P Ding, D Z Wang, Advances in Neural Information Processing Systems. H Wallach, H Larochelle, A Beygelzimer, F Alché-Buc, E Fox, R Garnett, Curran Associates, Inc201932</p>
<p>Distilling free-form natural laws from experimental data. M Schmidt, H Lipson, Science. 32459232009</p>
<p>J Scott, M Panju, V Ganesh, LGML: Logic Guided Machine Learning. 2006.03626, 2020</p>
<p>Neuro-symbolic inductive logic programming with logical neural networks. P Sen, B W S R De Carvalho, R Riegel, A G Gray, AAAI. 222022</p>
<p>Llm-sr: Scientific equation discovery via programming with large language models. P Shojaee, K Meidani, S Gupta, A B Farimani, C K Reddy, arXiv:2404.184002024arXiv preprint</p>
<p>LLM-SRBench: A new benchmark for scientific equation discovery with large language models. P Shojaee, N.-H Nguyen, K Meidani, A B Farimani, K D Doan, C K Reddy, Forty-second International Conference on Machine Learning. 2025</p>
<p>Enriching the Earth: Fritz Haber, Carl Bosch, and the Transformation of World Food Production. V Smil, 2004MIT Press</p>
<p>. J J Sun, M Tjandrasuwita, A Sehgal, A Solar-Lezama, S Chaudhuri, Y Yue, O Costilla-Reyes, arXiv:2210.050502022Neurosymbolic programming for science. arXiv preprint</p>
<p>Foundation Model for Material Science. S Takeda, A Kishimoto, L Hamada, D Nakano, J R Smith, 10.1609/aaai.v37i13.26793Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>Humanmachine scientific discovery. A Tamaddoni-Nezhad, D A Bohan, G A Milani, A Raybould, S Muggleton, Human-like machine intelligence. Oxford University Press2021</p>
<p>Alexander fleming (1881-1955): discoverer of penicillin. S Y Tan, Y Tatsumura, Singapore medical journal. 5673662015</p>
<p>A lean companion to "analysis i. T Tao, 2025</p>
<p>R Taylor, M Kardas, G Cucurull, T Scialom, A Hartshorn, E Saravia, A Poulton, V Kerkez, R Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>AI Feynman 2.0: Paretooptimal symbolic regression exploiting graph modularity. S Udrescu, A Tan, J Feng, O Neto, T Wu, M Tegmark, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M Balcan, H Lin, NeurIPS2020. 2020. December 6-12, 2020, virtual, 2020</p>
<p>AI Feynman: A physics-inspired method for symbolic regression. S.-M Udrescu, M Tegmark, Science Advances. 6162020</p>
<p>Houdini: Lifelong learning as program synthesis. L Valkov, D Chaudhari, A Srivastava, C Sutton, S Chaudhuri, Advances in Neural Information Processing Systems. 201831</p>
<p>Integrating deep learning with logic fusion for information extraction. W Wang, S J Pan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>Scibench: Evaluating college-level scientific problem-solving abilities of large language models. X Wang, Z Hu, P Lu, Y Zhu, J Zhang, S Subramaniam, A R Loomba, S Zhang, Y Sun, W Wang, arXiv:2307.106352023aarXiv preprint</p>
<p>NEWTON: Are large language models capable of physical reasoning?. Y Wang, J Duan, D Fox, S Srinivasa, 10.18653/v1/2023.findings-emnlp.652Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023b</p>
<p>Judge finds out why brief cited nonexistent cases-ChatGPT did the research. M Weiler, P Forré, E Verlinde, M Welling, arXiv:2106.06020Coordinate independent convolutional networks-isometry and gauge equivariant convolutions on riemannian manifolds. 2021. 2023arXiv preprint</p>
<p>R S Westfall, The Construction of Modern Science: Mechanisms and Mechanics. Cambridge University Press1977</p>
<p>Synergizing artificial intelligence and operations research: Perspectives from informs fellows on the next frontier. H Wiberg, T Dai, H Lam, R Kulkarni, INFORMS Journal on Data Science. 2025</p>
<p>Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. Z Wu, L Qiu, A Ross, E Akyürek, B Chen, B Wang, N Kim, J Andreas, Y Kim, arXiv:2307.024772023arXiv preprint</p>
<p>Nature language model: Deciphering the language of nature for scientific discovery. Y Xia, P Jin, S Xie, L He, C Cao, R Luo, G Liu, Y Wang, Z Liu, Y.-J Chen, Z Guo, Y Bai, P Deng, Y Min, Z Lu, H Hao, H Yang, J Li, C Liu, J Zhang, J Zhu, R Bi, K Wu, W Zhang, K Gao, Q Pei, Q Wang, X Liu, Y Li, H Zhu, Y Lu, M Ma, Z Wang, T Xie, K Maziarz, M Segler, Z Yang, Z Chen, Y Shi, S Zheng, L Wu, C Hu, P Dai, T.-Y Liu, H Liu, T Qin, arXiv:2502.075272025arXiv preprint</p>
<p>Embedding symbolic knowledge into deep networks. Y Xie, Z Xu, M S Kankanhalli, K S Meel, H Soh, Advances in neural information processing systems. 201932</p>
<p>A semantic loss function for deep learning with symbolic knowledge. J Xu, Z Zhang, T Friedman, Y Liang, G Broeck, International conference on machine learning. PMLR2018</p>
<p>Reinforcement symbolic regression machine. Y Xu, Y Liu, H Sun, The Twelfth International Conference on Learning Representations. 2024</p>
<p>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. Y Yamada, R T Lange, C Lu, S Hu, C Lu, J Foerster, J Clune, D Ha, arXiv:2504.080662025arXiv preprint</p>
<p>Differentiable learning of logical rules for knowledge base reasoning. F Yang, Z Yang, W W Cohen, 201730Advances in neural information processing systems</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Z Yang, X Du, J Li, J Zheng, S Poria, E Cambria, ACL 2024 findings. 2024</p>
<p>Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses. Z Yang, W Liu, B Gao, T Xie, Y Li, W Ouyang, S Poria, E Cambria, D Zhou, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Exploring the role of large language models in the scientific method: from hypothesis to discovery. Y Zhang, S A Khan, A Mahmud, H Yang, A Lavin, M Levin, J Frey, J Dunnmon, J Evans, A Bundy, Artificial Intelligence. 11142025</p>
<p>D M Ziegler, N Stiennon, J Wu, T B Brown, A Radford, D Amodei, P Christiano, G Irving, arXiv:1909.08593Fine-tuning language models from human preferences. 2019arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
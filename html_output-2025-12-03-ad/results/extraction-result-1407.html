<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1407 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1407</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1407</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-228f59ad153863bc7dc63057da031af31a03d4fa</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/228f59ad153863bc7dc63057da031af31a03d4fa" target="_blank">Distal Explanations for Model-free Explainable Reinforcement Learning</a></p>
                <p><strong>Paper TL;DR:</strong> A distal explanation model for model-free reinforcement learning agents that can generate explanations for `why' and `why not' questions is introduced and improved outcomes are shown over three scenarios compared with two baseline explanation models.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1407.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1407.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Decision-tree surrogate (DP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Depth-limited Decision-tree Surrogate Policy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interpretable surrogate policy learned from replayed state-action pairs as a decision tree whose number of leaves is limited to the number of domain actions; used to extract decision nodes as explanations and to filter causal chains for minimally complete explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Depth-limited Decision-tree Surrogate</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A supervised decision-tree classifier/regressor trained on the agent's replay dataset (state vectors x -> action labels y). Tree growth is constrained (max leaves = number of actions) so each leaf corresponds to a single action; explanations are produced by traversing the path to an action leaf and mapping decision nodes to state feature thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>symbolic interpretable surrogate model (decision tree)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Model-free reinforcement learning tasks (OpenAI benchmarks: CartPole, MountainCar, Taxi, LunarLander, BipedalWalker; StarCraft II adversarial task)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Task-prediction accuracy (percentage of next-action predictions matching the agent) computed over 100 episodes after training</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported DP task-prediction accuracies across 6 domains: Cartpole-PG 96.83%, MountainCar-DQN 88.66%, Taxi-SARSA 82.44%, LunarLander-DDQN 72.82%, BipedalWalker-PPO 67.99%, StarCraft-A3C 97.36% (see Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>High: decision nodes correspond directly to feature thresholds and can be read as human-understandable rules; depth-limiting further improves interpretability by producing a small set of leaves (one per action).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Direct inspection of decision nodes/paths (mapping nodes to state features and thresholds), limiting tree depth/leaves for human-readability.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not explicitly reported for training time/compute; decision-tree training is conceptually lightweight; surrogate trees are trained on replay batches during agent training. (Paper used a cluster with 2x V100 GPUs for overall experiments but per-model costs not provided.)</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Computational cost not quantified, but depth-limited DP is argued to be more interpretable and adequate in accuracy compared to unconstrained trees; DP substantially outperforms action-influence structural-equation baselines in task-prediction accuracy in all benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>As fidelity_performance above; using DP as surrogate yielded higher task-prediction accuracy than structural-equation baselines in every evaluated domain.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>The DP surrogate provides reasons 'why' by exposing decision nodes that align with causal reward nodes, improving the faithfulness of explanations to the agent's policy and supporting downstream measures (task-prediction and human explanation satisfaction).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Design choice to limit leaves greatly improves interpretability with minimal or no loss in task-prediction accuracy; unconstrained trees (DP_n) offer little extra predictive benefit in many domains while being less interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Max leaves set to number of actions; trained on replay dataset drawn during RL training; traverse root-to-leaf to extract decision nodes; used in combination with action-influence causal chains for filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperforms action-influence models with structural equations (trained with LR/DT/MLP) in task-prediction accuracy across six benchmarks; compared with unconstrained decision tree (DP_n), depth-limited DP has similar or better interpretability and comparable predictive performance.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends a depth-limited tree where leaves equal domain action count (DP) as an optimal balance between fidelity and interpretability for explainability purposes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Distal Explanations for Model-free Explainable Reinforcement Learning', 'publication_date_yy_mm': '2020-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1407.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1407.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Unconstrained Decision-tree (DP_n)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unconstrained Decision-tree Surrogate Policy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decision-tree surrogate trained without explicit depth/leaf limits until leaves are pure; used as an alternative surrogate policy to evaluate whether unconstrained trees improve task-prediction accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Unconstrained Decision-tree (DP_n)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A decision tree trained on replayed state-action pairs with no constraint on depth; leaves are pure and can create many leaves/subtrees mapping to actions, producing larger decision rulesets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>symbolic interpretable surrogate model (decision tree, unconstrained)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Same RL benchmark suite as DP (CartPole, MountainCar, Taxi, LunarLander, BipedalWalker, StarCraft II)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Task-prediction accuracy (%) over 100 episodes used in the evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported DP_n accuracies: Cartpole-PG 97.10%, MountainCar-DQN 86.75%, Taxi-SARSA 86.19%, LunarLander-DDQN 72.91%, BipedalWalker-PPO 69.28%, StarCraft-A3C 86.04% (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Lower than depth-limited DP due to larger, more complex trees (many leaves); can overwhelm explainees with numerous decision nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Same as DP (inspection of paths), but practical interpretability reduced by tree size; no additional interpretability methods reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not explicitly reported; likely higher memory and slower inference/inspection cost than depth-limited DP due to larger tree size; training cost modest but greater than constrained tree.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Gives minor or inconsistent gains in fidelity compared to DP in some domains and worse in others (e.g., StarCraft), so increased complexity does not reliably improve performance and harms interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Similar to DP in several domains; sometimes slightly higher (Cartpole) or lower (StarCraft) — overall not a clear win over depth-limited DP.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Because interpretability degrades while not consistently improving predictive fidelity, DP_n is less well-suited for explainability-focused applications despite being a more expressive surrogate.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Higher representational capacity vs interpretability: unconstrained trees can model more fine-grained distinctions but provide diminishing returns for task-prediction and reduce human interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>No limit on leaves; trained until leaves pure; used for ablation comparing unconstrained expressivity with depth-limited DP.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to depth-limited DP, DP_n is more complex and not consistently more accurate; compared to structural-equation baselines DP_n often outperforms them but at cost of interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests depth-limited DP is preferable for explanation purposes; DP_n not recommended as 'optimal' due to interpretability costs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Distal Explanations for Model-free Explainable Reinforcement Learning', 'publication_date_yy_mm': '2020-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1407.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1407.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Action Influence Model (AIM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Action Influence Models (SCM augmented with actions / action influence graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A causal representation that extends structural causal models by associating edges/functions with agent actions, producing action-conditioned structural equations and action-influence graphs used to extract causal chains and reward nodes for explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Action Influence Model (action-influence graph / AIM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An extension of Structural Causal Models (SCMs) where each endogenous variable has multiple structural functions F_{X,A} — one per influencing action A — and edges in the graph are annotated with actions; the actual instantiation sets state variables to current values and counterfactual instantiations are obtained by setting values corresponding to counterfactual actions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>symbolic causal world model / augmented SCM</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Explainable reinforcement learning domains (StarCraft II adversarial scenario used as illustrative example and used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Fidelity evaluated indirectly via task-prediction accuracy when paired with different structural-equation learners (LR/DT/MLP); fidelity of causal explanations judged by task-prediction and human study outcome measures (task prediction and explanation satisfaction)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Structural-equation versions of AIM (see LR/DT/MLP below) achieved task-prediction accuracies (examples): Cartpole LR 83.8% / DT 81.6% / MLP 86.0%; MountainCar LR 69.7% / DT 57.8% / MLP 69.6%; StarCraft LR 94.7% / DT 91.8% / MLP 91.4% (Table 3). Overall, AIM-based explanations produced lower task-prediction accuracy than the DP surrogate in each benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>High conceptual interpretability: causal graph structure and causal chains (reward nodes, intermediate nodes) are explicit and align with human causal reasoning; interpretability depends on choice of structural-equation learners (linear/decision-tree models preserve interpretability better than black-box regressors).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization and traversal of action-influence graphs to extract causal chains, reward nodes, and minimal explanations; mapping of decision nodes to causal nodes when combined with surrogate policies.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not explicitly quantified; cost depends on method used to learn structural equations (LR cheap, DT moderate, MLP more expensive). Overall overhead moderate for constructing/using causal graphs during explanation generation.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>When structural equations are estimated with simple models (LR/DT) efficiency is good, but predictive fidelity remains below DP surrogate; MLP can increase representational power at the cost of interpretability and likely higher compute.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>As fidelity_performance above: AIM with structural-equation learners produced lower task-prediction accuracy across benchmarks compared to the decision-tree surrogate approach.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>AIM provides explicit causal structure and supports counterfactual explanations and minimally-complete causal explanations; however, in this paper AIM alone (with learned structural equations) was less faithful to the agent's policy (lower task-prediction) than the surrogate DP approach, limiting its utility for accurate task prediction despite strong causal interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>AIM offers strong causal interpretability but may sacrifice policy-faithfulness when structural equations are approximated (esp. with simple learners); richer function approximators (MLP) can improve fidelity but reduce interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Represent each variable with multiple structural functions indexed by actions; define reward variables as sink nodes; use actual and counterfactual instantiations for explanations; combine AIM causal chains with surrogate policy decision nodes to filter causally irrelevant features.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to DP surrogate, AIM (with learned structural equations) produced lower task-prediction accuracy in all domains evaluated; AIM's advantage is causal semantics and support for counterfactual reasoning, while DP surrogate better matches the learned policy for prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests combining AIM (for goals/reward nodes and causal chains) with a surrogate decision-tree policy yields the best balance: AIM contributes causal goals and chains while DP provides faithful policy reasons; this combination is the distal explanation model proposed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Distal Explanations for Model-free Explainable Reinforcement Learning', 'publication_date_yy_mm': '2020-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1407.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1407.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Structural Equations (SE) variants</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structural-equation learners for action-influence models (LR / DT / MLP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Function approximators used to learn the structural equations F_{X,A} of action-influence models; the paper evaluates linear regression (LR), decision trees (DT), and multi-layer perceptrons (MLP) as variants and reports their task-prediction fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Structural-equation learners (LR / DT / MLP) for AIM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Different supervised learners fit to predict state-variable outcomes under actions: linear regression (LR) for linear structural equations, decision trees (DT) as interpretable non-linear fits, and MLPs as more flexible non-linear approximators; these provide the instantiated causal relationships used by AIM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>learned parametric structural-equation models (linear / tree / neural)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Same RL benchmark suite used for evaluating task-prediction accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Task-prediction accuracy (%) when causal structural-equations are used to predict next actions via the causal chain instantiation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Per Table 3, example accuracies: LR: Cartpole 83.8%, MountainCar 69.7%, Taxi 68.2%, LunarLander 68.4%, BipedalWalker 56.9%, StarCraft 94.7%; DT: Cartpole 81.6%, MountainCar 57.8%, Taxi 74.2%, LunarLander 63.7%, BipedalWalker 56.4%, StarCraft 91.8%; MLP: Cartpole 86.0%, MountainCar 69.6%, Taxi 67.9%, LunarLander 72.1%, BipedalWalker 56.7%, StarCraft 91.4%.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>LR and DT variants are more interpretable (linear weights, tree rules); MLP variant is a black-box neural approximator and reduces interpretability of the causal functions.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Linear coefficient inspection (LR), tree path inspection (DT); no additional interpretability methods for MLP reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not explicitly reported; LR is computationally cheap, DT moderate, MLP most expensive among the three; all trained on replay data during RL training.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>LR and DT are computationally efficient but sometimes less accurate; MLP can improve fidelity in some domains but at higher compute and reduced interpretability; overall these SE variants were less accurate than DP surrogate for task-prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Performance varies by learner and domain (see fidelity_performance list); none of the SE variants matched DP surrogate performance across all domains.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>SE variants enable explicit causal explanations (goals and causal chains) but their imperfect approximation of the agent's policy leads to lower task-prediction. Choosing more expressive SE (MLP) can increase fidelity at cost of interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Simplicity vs fidelity: LR/DT keep interpretability at possible cost in predictive accuracy; MLP increases fidelity in some domains but sacrifices interpretability; in all cases DP surrogate delivered higher task-prediction fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Evaluate multiple learners to fit structural-equation functions; report task-prediction comparisons to surrogate DP to assess which approach better matches agent behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>SE+AIM (with LR/DT/MLP) consistently underperformed relative to the decision-tree surrogate (DP) in task-prediction accuracy, though SE provides richer causal semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper does not prescribe a single best SE learner; suggests combining AIM causal structure with a faithful surrogate policy (decision tree) yields the best practical trade-off for explainability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Distal Explanations for Model-free Explainable Reinforcement Learning', 'publication_date_yy_mm': '2020-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1407.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1407.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Distal action predictor (RNN)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Many-to-one Recurrent Neural Network Distal Action Predictor</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequence model (many-to-one RNN) trained on state-action traces from replay buffer to predict the 'distal' (future dependent/enabled) action and its expected cumulative reward, used to produce opportunity-chain (distal) explanations in model-free RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Many-to-one RNN distal action predictor</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A recurrent neural network (implemented with one fully connected hidden layer of 10 units in the paper) that takes a sequence of prior state-action pairs (a state-action trace) as input and predicts the next action in the causal chain (distal action) and expected cumulative reward; trained on replay dataset with batch size 100.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>neural sequence predictor (RNN)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Model-free reinforcement learning tasks where distal/enabling actions exist (StarCraft II adversarial example and other RL benchmarks for training data)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Not explicitly reported as a standalone metric in the paper; intended fidelity is correctness of distal-action prediction relative to ground-truth 'next action in a causal chain' derived from action-influence graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not reported numerically in the paper; the distal-action predictor is described and used in explanations but no standalone accuracy numbers for RNN distal predictions are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box neural sequence model; interpretation of internal states not reported. Outputs (predicted distal action and reward) are interpretable as labels but internal reasoning is not.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None reported for internal RNN interpretability; RNN outputs are used as explicit labels (predicted distal action) integrated into textual explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Model instantiated with a single fully connected hidden layer of 10 units and trained with batch size 100 on replay data — very small in parameter count; exact training time or resources not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>No quantitative comparison provided; authors note other sequence predictors could be used, implying RNN chosen for simplicity and sufficiency for predicting distal actions in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not directly quantified; qualitative results show distal explanations (which include the RNN prediction) improved human outcomes and explanation satisfaction in user studies, indicating practical utility.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Enables approximating enabling/distal actions in model-free settings where preconditions are not explicitly available; adding predicted distal actions to causal explanations makes explanations more human-aligned (opportunity chains) and improved human study outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Provides task-relevant distal predictions in absence of explicit environment model, but as an approximation it may be incorrect without guarantees; lack of reported fidelity metrics limits evaluation of tradeoff between prediction accuracy and explanation utility.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Many-to-one RNN; fully connected hidden layer of 10 units; batch size 100; training sequences chosen to end at actions that are last in a causal chain; output includes action and expected cumulative reward.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Authors note alternative sequence predictors could be used; no empirical comparison to other sequence model types is provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>No exhaustive configuration search reported; the paper reports a small RNN architecture (10 hidden units) sufficed for their explanation pipeline and suggests flexibility to replace with other sequence models if desired.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Distal Explanations for Model-free Explainable Reinforcement Learning', 'publication_date_yy_mm': '2020-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1407.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1407.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structural Causal Models (SCMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A formal causal modeling framework (endogenous/exogenous variables + structural equations) used as the theoretical foundation for action-influence models and for providing counterfactual and causal explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Structural Causal Model (SCM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A set of endogenous and exogenous variables together with structural equations F_X that define each endogenous variable as a function of other variables, enabling definitions of contexts, actual and counterfactual instantiations, and formal notions of actual cause and explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>symbolic causal world model (formal causal model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General causal explanation and counterfactual reasoning; used here as foundational theory for explainable RL and action-influence models</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>SCMs are not evaluated by 'fidelity' in the usual predictive sense in this paper; they provide formal semantics for counterfactuals and causal chains used in explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not applicable / not reported (SCMs are theoretical constructs and not empirically fitted here).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>High: SCMs provide explicit causal structure and formal definitions for causality and counterfactuals, aligning with human causal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Graphical causal models, structural equations, and counterfactual instantiation; SCM formalism itself is the interpretability mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not applicable as presented; computational cost depends on how structural functions are instantiated/learned in practice (see SE variants).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Not directly compared; SCMs are theoretical foundation rather than a concrete trained model in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not applicable; SCMs underpin the explanation machinery and counterfactual semantics rather than perform a task.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>SCMs provide the semantics for generating counterfactual and causal explanations (including opportunity chains) which are used throughout the explanation models developed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>SCMs provide clear causal semantics but require instantiated structural equations to be useful in practice; quality of learned structural equations determines empirical utility.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use of endogenous/exogenous variables and structural equations; contexts and actual/counterfactual instantiations; used as basis for action-influence models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>SCMs are the standard formalism for causal reasoning; action-influence models extend SCMs by associating actions to structural functions to make them suitable for RL explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not applicable; SCM formalism stands as the recommended causal foundation; the practical choice is how to instantiate structural equations (LR/DT/MLP) and whether to combine SCMs with surrogate policy models for better fidelity and interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Distal Explanations for Model-free Explainable Reinforcement Learning', 'publication_date_yy_mm': '2020-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Distal Explanations for Model-free Explainable Reinforcement Learning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1407",
    "paper_id": "paper-228f59ad153863bc7dc63057da031af31a03d4fa",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "Decision-tree surrogate (DP)",
            "name_full": "Depth-limited Decision-tree Surrogate Policy",
            "brief_description": "An interpretable surrogate policy learned from replayed state-action pairs as a decision tree whose number of leaves is limited to the number of domain actions; used to extract decision nodes as explanations and to filter causal chains for minimally complete explanations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Depth-limited Decision-tree Surrogate",
            "model_description": "A supervised decision-tree classifier/regressor trained on the agent's replay dataset (state vectors x -&gt; action labels y). Tree growth is constrained (max leaves = number of actions) so each leaf corresponds to a single action; explanations are produced by traversing the path to an action leaf and mapping decision nodes to state feature thresholds.",
            "model_type": "symbolic interpretable surrogate model (decision tree)",
            "task_domain": "Model-free reinforcement learning tasks (OpenAI benchmarks: CartPole, MountainCar, Taxi, LunarLander, BipedalWalker; StarCraft II adversarial task)",
            "fidelity_metric": "Task-prediction accuracy (percentage of next-action predictions matching the agent) computed over 100 episodes after training",
            "fidelity_performance": "Reported DP task-prediction accuracies across 6 domains: Cartpole-PG 96.83%, MountainCar-DQN 88.66%, Taxi-SARSA 82.44%, LunarLander-DDQN 72.82%, BipedalWalker-PPO 67.99%, StarCraft-A3C 97.36% (see Table 3).",
            "interpretability_assessment": "High: decision nodes correspond directly to feature thresholds and can be read as human-understandable rules; depth-limiting further improves interpretability by producing a small set of leaves (one per action).",
            "interpretability_method": "Direct inspection of decision nodes/paths (mapping nodes to state features and thresholds), limiting tree depth/leaves for human-readability.",
            "computational_cost": "Not explicitly reported for training time/compute; decision-tree training is conceptually lightweight; surrogate trees are trained on replay batches during agent training. (Paper used a cluster with 2x V100 GPUs for overall experiments but per-model costs not provided.)",
            "efficiency_comparison": "Computational cost not quantified, but depth-limited DP is argued to be more interpretable and adequate in accuracy compared to unconstrained trees; DP substantially outperforms action-influence structural-equation baselines in task-prediction accuracy in all benchmarks.",
            "task_performance": "As fidelity_performance above; using DP as surrogate yielded higher task-prediction accuracy than structural-equation baselines in every evaluated domain.",
            "task_utility_analysis": "The DP surrogate provides reasons 'why' by exposing decision nodes that align with causal reward nodes, improving the faithfulness of explanations to the agent's policy and supporting downstream measures (task-prediction and human explanation satisfaction).",
            "tradeoffs_observed": "Design choice to limit leaves greatly improves interpretability with minimal or no loss in task-prediction accuracy; unconstrained trees (DP_n) offer little extra predictive benefit in many domains while being less interpretable.",
            "design_choices": "Max leaves set to number of actions; trained on replay dataset drawn during RL training; traverse root-to-leaf to extract decision nodes; used in combination with action-influence causal chains for filtering.",
            "comparison_to_alternatives": "Outperforms action-influence models with structural equations (trained with LR/DT/MLP) in task-prediction accuracy across six benchmarks; compared with unconstrained decision tree (DP_n), depth-limited DP has similar or better interpretability and comparable predictive performance.",
            "optimal_configuration": "Paper recommends a depth-limited tree where leaves equal domain action count (DP) as an optimal balance between fidelity and interpretability for explainability purposes.",
            "uuid": "e1407.0",
            "source_info": {
                "paper_title": "Distal Explanations for Model-free Explainable Reinforcement Learning",
                "publication_date_yy_mm": "2020-01"
            }
        },
        {
            "name_short": "Unconstrained Decision-tree (DP_n)",
            "name_full": "Unconstrained Decision-tree Surrogate Policy",
            "brief_description": "A decision-tree surrogate trained without explicit depth/leaf limits until leaves are pure; used as an alternative surrogate policy to evaluate whether unconstrained trees improve task-prediction accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Unconstrained Decision-tree (DP_n)",
            "model_description": "A decision tree trained on replayed state-action pairs with no constraint on depth; leaves are pure and can create many leaves/subtrees mapping to actions, producing larger decision rulesets.",
            "model_type": "symbolic interpretable surrogate model (decision tree, unconstrained)",
            "task_domain": "Same RL benchmark suite as DP (CartPole, MountainCar, Taxi, LunarLander, BipedalWalker, StarCraft II)",
            "fidelity_metric": "Task-prediction accuracy (%) over 100 episodes used in the evaluation",
            "fidelity_performance": "Reported DP_n accuracies: Cartpole-PG 97.10%, MountainCar-DQN 86.75%, Taxi-SARSA 86.19%, LunarLander-DDQN 72.91%, BipedalWalker-PPO 69.28%, StarCraft-A3C 86.04% (Table 3).",
            "interpretability_assessment": "Lower than depth-limited DP due to larger, more complex trees (many leaves); can overwhelm explainees with numerous decision nodes.",
            "interpretability_method": "Same as DP (inspection of paths), but practical interpretability reduced by tree size; no additional interpretability methods reported.",
            "computational_cost": "Not explicitly reported; likely higher memory and slower inference/inspection cost than depth-limited DP due to larger tree size; training cost modest but greater than constrained tree.",
            "efficiency_comparison": "Gives minor or inconsistent gains in fidelity compared to DP in some domains and worse in others (e.g., StarCraft), so increased complexity does not reliably improve performance and harms interpretability.",
            "task_performance": "Similar to DP in several domains; sometimes slightly higher (Cartpole) or lower (StarCraft) — overall not a clear win over depth-limited DP.",
            "task_utility_analysis": "Because interpretability degrades while not consistently improving predictive fidelity, DP_n is less well-suited for explainability-focused applications despite being a more expressive surrogate.",
            "tradeoffs_observed": "Higher representational capacity vs interpretability: unconstrained trees can model more fine-grained distinctions but provide diminishing returns for task-prediction and reduce human interpretability.",
            "design_choices": "No limit on leaves; trained until leaves pure; used for ablation comparing unconstrained expressivity with depth-limited DP.",
            "comparison_to_alternatives": "Compared to depth-limited DP, DP_n is more complex and not consistently more accurate; compared to structural-equation baselines DP_n often outperforms them but at cost of interpretability.",
            "optimal_configuration": "Paper suggests depth-limited DP is preferable for explanation purposes; DP_n not recommended as 'optimal' due to interpretability costs.",
            "uuid": "e1407.1",
            "source_info": {
                "paper_title": "Distal Explanations for Model-free Explainable Reinforcement Learning",
                "publication_date_yy_mm": "2020-01"
            }
        },
        {
            "name_short": "Action Influence Model (AIM)",
            "name_full": "Action Influence Models (SCM augmented with actions / action influence graphs)",
            "brief_description": "A causal representation that extends structural causal models by associating edges/functions with agent actions, producing action-conditioned structural equations and action-influence graphs used to extract causal chains and reward nodes for explanations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Action Influence Model (action-influence graph / AIM)",
            "model_description": "An extension of Structural Causal Models (SCMs) where each endogenous variable has multiple structural functions F_{X,A} — one per influencing action A — and edges in the graph are annotated with actions; the actual instantiation sets state variables to current values and counterfactual instantiations are obtained by setting values corresponding to counterfactual actions.",
            "model_type": "symbolic causal world model / augmented SCM",
            "task_domain": "Explainable reinforcement learning domains (StarCraft II adversarial scenario used as illustrative example and used in experiments)",
            "fidelity_metric": "Fidelity evaluated indirectly via task-prediction accuracy when paired with different structural-equation learners (LR/DT/MLP); fidelity of causal explanations judged by task-prediction and human study outcome measures (task prediction and explanation satisfaction)",
            "fidelity_performance": "Structural-equation versions of AIM (see LR/DT/MLP below) achieved task-prediction accuracies (examples): Cartpole LR 83.8% / DT 81.6% / MLP 86.0%; MountainCar LR 69.7% / DT 57.8% / MLP 69.6%; StarCraft LR 94.7% / DT 91.8% / MLP 91.4% (Table 3). Overall, AIM-based explanations produced lower task-prediction accuracy than the DP surrogate in each benchmark.",
            "interpretability_assessment": "High conceptual interpretability: causal graph structure and causal chains (reward nodes, intermediate nodes) are explicit and align with human causal reasoning; interpretability depends on choice of structural-equation learners (linear/decision-tree models preserve interpretability better than black-box regressors).",
            "interpretability_method": "Visualization and traversal of action-influence graphs to extract causal chains, reward nodes, and minimal explanations; mapping of decision nodes to causal nodes when combined with surrogate policies.",
            "computational_cost": "Not explicitly quantified; cost depends on method used to learn structural equations (LR cheap, DT moderate, MLP more expensive). Overall overhead moderate for constructing/using causal graphs during explanation generation.",
            "efficiency_comparison": "When structural equations are estimated with simple models (LR/DT) efficiency is good, but predictive fidelity remains below DP surrogate; MLP can increase representational power at the cost of interpretability and likely higher compute.",
            "task_performance": "As fidelity_performance above: AIM with structural-equation learners produced lower task-prediction accuracy across benchmarks compared to the decision-tree surrogate approach.",
            "task_utility_analysis": "AIM provides explicit causal structure and supports counterfactual explanations and minimally-complete causal explanations; however, in this paper AIM alone (with learned structural equations) was less faithful to the agent's policy (lower task-prediction) than the surrogate DP approach, limiting its utility for accurate task prediction despite strong causal interpretability.",
            "tradeoffs_observed": "AIM offers strong causal interpretability but may sacrifice policy-faithfulness when structural equations are approximated (esp. with simple learners); richer function approximators (MLP) can improve fidelity but reduce interpretability.",
            "design_choices": "Represent each variable with multiple structural functions indexed by actions; define reward variables as sink nodes; use actual and counterfactual instantiations for explanations; combine AIM causal chains with surrogate policy decision nodes to filter causally irrelevant features.",
            "comparison_to_alternatives": "Compared to DP surrogate, AIM (with learned structural equations) produced lower task-prediction accuracy in all domains evaluated; AIM's advantage is causal semantics and support for counterfactual reasoning, while DP surrogate better matches the learned policy for prediction.",
            "optimal_configuration": "Paper suggests combining AIM (for goals/reward nodes and causal chains) with a surrogate decision-tree policy yields the best balance: AIM contributes causal goals and chains while DP provides faithful policy reasons; this combination is the distal explanation model proposed.",
            "uuid": "e1407.2",
            "source_info": {
                "paper_title": "Distal Explanations for Model-free Explainable Reinforcement Learning",
                "publication_date_yy_mm": "2020-01"
            }
        },
        {
            "name_short": "Structural Equations (SE) variants",
            "name_full": "Structural-equation learners for action-influence models (LR / DT / MLP)",
            "brief_description": "Function approximators used to learn the structural equations F_{X,A} of action-influence models; the paper evaluates linear regression (LR), decision trees (DT), and multi-layer perceptrons (MLP) as variants and reports their task-prediction fidelity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Structural-equation learners (LR / DT / MLP) for AIM",
            "model_description": "Different supervised learners fit to predict state-variable outcomes under actions: linear regression (LR) for linear structural equations, decision trees (DT) as interpretable non-linear fits, and MLPs as more flexible non-linear approximators; these provide the instantiated causal relationships used by AIM.",
            "model_type": "learned parametric structural-equation models (linear / tree / neural)",
            "task_domain": "Same RL benchmark suite used for evaluating task-prediction accuracy",
            "fidelity_metric": "Task-prediction accuracy (%) when causal structural-equations are used to predict next actions via the causal chain instantiation",
            "fidelity_performance": "Per Table 3, example accuracies: LR: Cartpole 83.8%, MountainCar 69.7%, Taxi 68.2%, LunarLander 68.4%, BipedalWalker 56.9%, StarCraft 94.7%; DT: Cartpole 81.6%, MountainCar 57.8%, Taxi 74.2%, LunarLander 63.7%, BipedalWalker 56.4%, StarCraft 91.8%; MLP: Cartpole 86.0%, MountainCar 69.6%, Taxi 67.9%, LunarLander 72.1%, BipedalWalker 56.7%, StarCraft 91.4%.",
            "interpretability_assessment": "LR and DT variants are more interpretable (linear weights, tree rules); MLP variant is a black-box neural approximator and reduces interpretability of the causal functions.",
            "interpretability_method": "Linear coefficient inspection (LR), tree path inspection (DT); no additional interpretability methods for MLP reported.",
            "computational_cost": "Not explicitly reported; LR is computationally cheap, DT moderate, MLP most expensive among the three; all trained on replay data during RL training.",
            "efficiency_comparison": "LR and DT are computationally efficient but sometimes less accurate; MLP can improve fidelity in some domains but at higher compute and reduced interpretability; overall these SE variants were less accurate than DP surrogate for task-prediction.",
            "task_performance": "Performance varies by learner and domain (see fidelity_performance list); none of the SE variants matched DP surrogate performance across all domains.",
            "task_utility_analysis": "SE variants enable explicit causal explanations (goals and causal chains) but their imperfect approximation of the agent's policy leads to lower task-prediction. Choosing more expressive SE (MLP) can increase fidelity at cost of interpretability.",
            "tradeoffs_observed": "Simplicity vs fidelity: LR/DT keep interpretability at possible cost in predictive accuracy; MLP increases fidelity in some domains but sacrifices interpretability; in all cases DP surrogate delivered higher task-prediction fidelity.",
            "design_choices": "Evaluate multiple learners to fit structural-equation functions; report task-prediction comparisons to surrogate DP to assess which approach better matches agent behavior.",
            "comparison_to_alternatives": "SE+AIM (with LR/DT/MLP) consistently underperformed relative to the decision-tree surrogate (DP) in task-prediction accuracy, though SE provides richer causal semantics.",
            "optimal_configuration": "Paper does not prescribe a single best SE learner; suggests combining AIM causal structure with a faithful surrogate policy (decision tree) yields the best practical trade-off for explainability.",
            "uuid": "e1407.3",
            "source_info": {
                "paper_title": "Distal Explanations for Model-free Explainable Reinforcement Learning",
                "publication_date_yy_mm": "2020-01"
            }
        },
        {
            "name_short": "Distal action predictor (RNN)",
            "name_full": "Many-to-one Recurrent Neural Network Distal Action Predictor",
            "brief_description": "A sequence model (many-to-one RNN) trained on state-action traces from replay buffer to predict the 'distal' (future dependent/enabled) action and its expected cumulative reward, used to produce opportunity-chain (distal) explanations in model-free RL.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Many-to-one RNN distal action predictor",
            "model_description": "A recurrent neural network (implemented with one fully connected hidden layer of 10 units in the paper) that takes a sequence of prior state-action pairs (a state-action trace) as input and predicts the next action in the causal chain (distal action) and expected cumulative reward; trained on replay dataset with batch size 100.",
            "model_type": "neural sequence predictor (RNN)",
            "task_domain": "Model-free reinforcement learning tasks where distal/enabling actions exist (StarCraft II adversarial example and other RL benchmarks for training data)",
            "fidelity_metric": "Not explicitly reported as a standalone metric in the paper; intended fidelity is correctness of distal-action prediction relative to ground-truth 'next action in a causal chain' derived from action-influence graphs.",
            "fidelity_performance": "Not reported numerically in the paper; the distal-action predictor is described and used in explanations but no standalone accuracy numbers for RNN distal predictions are provided.",
            "interpretability_assessment": "Black-box neural sequence model; interpretation of internal states not reported. Outputs (predicted distal action and reward) are interpretable as labels but internal reasoning is not.",
            "interpretability_method": "None reported for internal RNN interpretability; RNN outputs are used as explicit labels (predicted distal action) integrated into textual explanations.",
            "computational_cost": "Model instantiated with a single fully connected hidden layer of 10 units and trained with batch size 100 on replay data — very small in parameter count; exact training time or resources not reported.",
            "efficiency_comparison": "No quantitative comparison provided; authors note other sequence predictors could be used, implying RNN chosen for simplicity and sufficiency for predicting distal actions in their experiments.",
            "task_performance": "Not directly quantified; qualitative results show distal explanations (which include the RNN prediction) improved human outcomes and explanation satisfaction in user studies, indicating practical utility.",
            "task_utility_analysis": "Enables approximating enabling/distal actions in model-free settings where preconditions are not explicitly available; adding predicted distal actions to causal explanations makes explanations more human-aligned (opportunity chains) and improved human study outcomes.",
            "tradeoffs_observed": "Provides task-relevant distal predictions in absence of explicit environment model, but as an approximation it may be incorrect without guarantees; lack of reported fidelity metrics limits evaluation of tradeoff between prediction accuracy and explanation utility.",
            "design_choices": "Many-to-one RNN; fully connected hidden layer of 10 units; batch size 100; training sequences chosen to end at actions that are last in a causal chain; output includes action and expected cumulative reward.",
            "comparison_to_alternatives": "Authors note alternative sequence predictors could be used; no empirical comparison to other sequence model types is provided in the paper.",
            "optimal_configuration": "No exhaustive configuration search reported; the paper reports a small RNN architecture (10 hidden units) sufficed for their explanation pipeline and suggests flexibility to replace with other sequence models if desired.",
            "uuid": "e1407.4",
            "source_info": {
                "paper_title": "Distal Explanations for Model-free Explainable Reinforcement Learning",
                "publication_date_yy_mm": "2020-01"
            }
        },
        {
            "name_short": "SCM",
            "name_full": "Structural Causal Models (SCMs)",
            "brief_description": "A formal causal modeling framework (endogenous/exogenous variables + structural equations) used as the theoretical foundation for action-influence models and for providing counterfactual and causal explanations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Structural Causal Model (SCM)",
            "model_description": "A set of endogenous and exogenous variables together with structural equations F_X that define each endogenous variable as a function of other variables, enabling definitions of contexts, actual and counterfactual instantiations, and formal notions of actual cause and explanation.",
            "model_type": "symbolic causal world model (formal causal model)",
            "task_domain": "General causal explanation and counterfactual reasoning; used here as foundational theory for explainable RL and action-influence models",
            "fidelity_metric": "SCMs are not evaluated by 'fidelity' in the usual predictive sense in this paper; they provide formal semantics for counterfactuals and causal chains used in explanations.",
            "fidelity_performance": "Not applicable / not reported (SCMs are theoretical constructs and not empirically fitted here).",
            "interpretability_assessment": "High: SCMs provide explicit causal structure and formal definitions for causality and counterfactuals, aligning with human causal reasoning.",
            "interpretability_method": "Graphical causal models, structural equations, and counterfactual instantiation; SCM formalism itself is the interpretability mechanism.",
            "computational_cost": "Not applicable as presented; computational cost depends on how structural functions are instantiated/learned in practice (see SE variants).",
            "efficiency_comparison": "Not directly compared; SCMs are theoretical foundation rather than a concrete trained model in experiments.",
            "task_performance": "Not applicable; SCMs underpin the explanation machinery and counterfactual semantics rather than perform a task.",
            "task_utility_analysis": "SCMs provide the semantics for generating counterfactual and causal explanations (including opportunity chains) which are used throughout the explanation models developed in the paper.",
            "tradeoffs_observed": "SCMs provide clear causal semantics but require instantiated structural equations to be useful in practice; quality of learned structural equations determines empirical utility.",
            "design_choices": "Use of endogenous/exogenous variables and structural equations; contexts and actual/counterfactual instantiations; used as basis for action-influence models.",
            "comparison_to_alternatives": "SCMs are the standard formalism for causal reasoning; action-influence models extend SCMs by associating actions to structural functions to make them suitable for RL explanations.",
            "optimal_configuration": "Not applicable; SCM formalism stands as the recommended causal foundation; the practical choice is how to instantiate structural equations (LR/DT/MLP) and whether to combine SCMs with surrogate policy models for better fidelity and interpretability.",
            "uuid": "e1407.5",
            "source_info": {
                "paper_title": "Distal Explanations for Model-free Explainable Reinforcement Learning",
                "publication_date_yy_mm": "2020-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Distal Explanations for Model-free Explainable Reinforcement Learning",
            "rating": 2
        }
    ],
    "cost": 0.019957,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Distal Explanations for Model-free Explainable Reinforcement Learning</h1>
<h4>Abstract</h4>
<p>In this paper we introduce and evaluate a distal explanation model for model-free reinforcement learning agents that can generate explanations for 'why' and 'why not' questions. Our starting point is the observation that causal models can generate opportunity chains that take the form of 'A enables B and B causes C'. Using insights from an analysis of 240 explanations generated in a human-agent experiment, we define a distal explanation model that can analyse counterfactuals and opportunity chains using decision trees and causal models. A recurrent neural network is employed to learn opportunity chains, and decision trees are used to improve the accuracy of task prediction and the generated counterfactuals. We computationally evaluate the model in 6 reinforcement learning benchmarks using different reinforcement learning algorithms. From a study with 90 human participants, we show that our distal explanation model results in improved outcomes over three scenarios compared with two baseline explanation models.</p>
<p>Keywords: Explainable AI; Explainable Reinforcement Learning; Human-Agent Interaction; Human-Agent Collaboration</p>
<h2>1. Introduction</h2>
<p>Understanding how artificially intelligent systems behave and make decisions has long since been a topic of interest, and in recent years has resurfaced as 'Explainable AI' (XAI). The ability to provide explanations of the behaviour of these systems is particularly important in scenarios where humans need to collaborate with intelligent agents. Often, the success of these collaborative tasks depends on how well the human understands both the long-term goals and immediate actions of the agent.</p>
<p>Explanation models that emulate human models of explanations have the potential to provide intuitive and natural explanations, allowing the human a deeper understanding of the agent (De Graaf and Malle, 2017; Abdul et al., 2018; Miller, 2018b; Wang et al., 2019). There exists a large body of literature in cognitive psychology that studies the nature of explanations. One prevalent theory is that explanations are innately causal (Halpern and Pearl, 2005). Causal explanations resonate with humans as we make use of causal models of the world to encode cause-effect relationships in our mind (Sloman, 2005), and leverage these models to explain why events happen. Causal models also enable the generation of counterfactual explanations - explanations about events that did not happen but could have under different circumstances (Halpern and Pearl, 2005). So causal explanations have the potential to provide 'better' explanations to humans.</p>
<p>Recent work in the XAI research community has demonstrated the effectiveness of causality and causal explanations for interpretability and explainability (Byrne, 2019; Klein, 2018; Gunning and Aha, 2019; Schwab and Karlen, 2019; Madumal et al., 2020). In the con-</p>
<p>text of model-free reinforcement learning (RL) agents, causal models have been encoded using action influence graphs to generate explanations using causal chains (Madumal et al., 2020) and have been shown to support subjectively 'better' explanations and yield improved performance in task prediction (Hoffman et al., 2018) as compared with state-action based explanations (Khan et al., 2009). While action influence models provide a skeleton to generate causal explanations for RL agents, finer details of the composition of causal explanations can be absent. We argue that, through investigating interactions of RL agents and humans, some shortcomings of action influence models can potentially be alleviated.</p>
<p>To ground the effect that explanation models have on human explanation, we conduct human-agent experiments on how humans formulate explanations of agent behaviour. Participants of this study received explanations from three different models: visual agent behaviour explanations; state-action based explanations (Khan et al., 2009); and causal explanations (Madumal et al., 2020). Then participants were asked to formulate their own explanations of the agents' behaviour as a textual input. The study was carried out with 30 participants and we obtained 240 explanations in total. We used thematic analysis (Braun and Clarke, 2006) to identify recurring concepts present in the explanations.</p>
<p>Results of our analysis show that while causality was indeed present, these self-provided explanations predominantly referred to a future action that was dependent on the current action. Participants' tendency to include a future action in their explanations indicates an understanding of the causal chain of actions and events. This phenomenon is well explored in cognitive psychology and is defined as opportunity chains (Hilton et al., 2005). We use insights gained from the human-agent study to inform our design of an explanation model that can explain opportunity chains and the future action termed the distal action.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An opportunity chain (Hilton et al., 2005), where event $A$ enables $B$ and $B$ causes $C$.</p>
<p>Hilton et al. (Hilton et al., 2005; McClure et al., 2007; Hilton et al., 2010) note that humans make use of opportunity chains to describe events through causal explanation. An opportunity chain takes the form of $A$ enables $B$ and $B$ causes $C$ (depicted in Figure 1), in which we call $B$ the 'distal' event or action. For example, an accident can be caused by slipping on ice which was enabled by water from a storm the day before. Opportunity chains are causal chains that can be extracted from action influence models. Thus action influence models can be used as a platform to augment causal explanations with opportunity chains.</p>
<p>To that end, we propose a distal explanation model that can generate opportunity chains as explanations for model-free RL agents. We provide definitions for distal explanations and learn the opportunity chains of extracted causal chains using a recurrent neural network (Schuster and Paliwal, 1997). A distal explanations by itself would not make a complete explanation. For this reason, we use action influence models (Madumal et al., 2020) to get</p>
<p>the agent's 'goals'. We further improve upon action influence models by using decision trees to represent the agent's policy.</p>
<p>We computationally evaluate the accuracy of task prediction (Hoffman et al., 2018, p.12) and counterfactuals in 6 RL benchmark domains using 6 different RL algorithms, and show that our distal explanation model is robust and accurate across different environments and algorithms. Then we conduct human experiments using RL agents trained to solve 3 different scenarios, where agents solve 1) an adversarial task; 2) a search and rescue task; and 3) a human-AI collaborative build task. The human study was run with $\mathbf{9 0}$ participants, where we evaluate task prediction (Hoffman et al., 2018) and explanation satisfaction. Results indicate that our model performs better than the two tested baselines.</p>
<p>Our main contribution in this paper is twofold: 1) we introduce a distal action explanation model that is grounded on human data; 2) we extend action influence models by using decision trees to represent the agent's policy and formalise explanation generation from decision nodes and causal chains. As secondary contributions, we also provide the coded corpus of human-agent experiment with $\mathbf{2 4 0}$ explanations and two custom maps that are suited for explainability in the StarCraft II environment.</p>
<h1>2. Related Work</h1>
<p>In this section we discuss the body of literature that explores explainability in reinforcement learning agents. We also note work that have influenced explanation and interpretability of reinforcement learning, some of which are central to our own method. Further, we briefly overview human-centred literature on explanation which has greatly influenced this work.</p>
<h3>2.1 Explanations of MDP based Agents</h3>
<p>Literature that sought to generate explanations for MDP based agents fall into the scope of preceding work on explainable RL. Often, these earlier work provided local explanations in that the explanation is for a question about an action of the agent.</p>
<p>The concept of 'relevant variables' in a factored state of an MDP was exploited by Elizalde et al. (2007) to generate explanations. Explanations were primarily targeted at human trainees of a system and explanations were built-in and were presented when an the operator (trainee) selected an incorrect action. An explanation constitutes a relevant variable that is selected by an expert for each action. Elizalde et al. later extended this work to generate explanations automatically based on the utility that a state variable had on the policy selecting the action (Elizalde et al., 2008; Elizalde and Sucar, 2009). Khan et al. (2009) was influenced from the relevant variable explanations and proposed minimally sufficient explanations for MDPs. Here, the long term effects of an optimal action is considered when generating the explanation. Three domain independent templates were used as the basis of explanations. We later use one of these templates as a benchmark method in the evaluation section. Relevant variable explanations present a straightforward method of generating explanations from an MDP, though their inability provide contrastive explanations of counterfactuals remains a weakness. Khan et al. (2009) attempted to remedy this by generating contrastive explanations through value-function comparisons. The effect MDP based agents' explanations have on 'trust' was examined by Wang et al. (2016). Experiments were carried out to measure trust in human-robot teams influenced by Partially Observable MDP based</p>
<p>explanations. As the measurement of trust was self-report, it is unclear whether the trust gain was from actually understanding the system.</p>
<h1>2.2 Policy Explanations</h1>
<p>Policy explanations make use of the agent's policy to extract explanations. Explanations can be at the local level or the global level. Global level explanations generally provide an explanation for the whole policy. Some studies suggest that humans are more receptive to global explanations of agents in certain situations (van der Waa et al., 2018). We discuss literature on both global and local explanation methods in this section.</p>
<p>Struckmeier et al. (2019) introduced a model-agnostic explanation generation method using agent policies. In cases where the underlying model (i.e. the policy function) is blackbox, Struckmeier et al. sample the policy of the agent to extract relevant state dimensions. Understanding of the agents' policies were measured in a human-experiment and the perceived understanding of the human participant was used as a proxy to show the transparency of the agent.</p>
<p>Policy explanations of an agent generally aims to provide a 'global' interpretation of the agent's behaviour. Hayes and Shah (2017) sought to improve the transparency by providing policy level explanations for agent based robot controllers. These behavioural explanations of the agent are considered as 'summaries' of the agent's policy. Discreet, continuous and multi-agent domains were used to evaluate the generated policy descriptions against expert descriptions and were shown to improve the transparency of the robot. Amir and Amir (2018) also aims to summarise the agent's behaviour and introduced the HIGHLIGHTS algorithm. Important states are extracted from the agent's execution trace based on the Q-values. Human-subject experiments showed that participants preferred HIGHLIGHTS summary explanations compared to full policy explanations though in some situations participants' assessments did not always correlate with their confidence. Policy summarisation was also explored in the context of inverse reinforcement learning to investigate if these explanations are viable if there is a discrepancy between the agent's model and the human's mental model.</p>
<h3>2.3 Reinforcement Learning Agent Explanations</h3>
<p>Here we discuss work in recent years that specifically use characteristics of reinforcement learning (e.g. rewards) to explain behaviour. These characteristics can be used to create an approximate model of the agents policy or model and then generate explanations through using the approximate model.</p>
<p>Tabrez et al. (2019) proposed a framework (RARE) that repair the agent's understanding of the domain reward function through explanation. RARE is especially useful in humanagent collaborative scenarios when the human's reward function of the collaborative task is erroneous. Explanations are given to the collaborators to update their own reward function. Human experiments were conducted to demonstrate the effectiveness of the RARE framework in collaborative tasks. Explanation in the context of interactive reinforcement learning (IRL) has been studied (Fukuchi et al., 2017). This approach uses the instructions given in the IRL process to the agent as representations to generate explanations about the future behaviour of the agent. Evaluated through a human study, this method affirms</p>
<p>that when explanations are given in a familiar medium to the human (e.g. using instruction representations) can yield a deeper understanding of the agent. van der Waa et al. (2018) developed a method that can translate an MDP of a RL agent to an interpretable MDP. This translation model can then be used to generate a contrastive policy that can be queried using contrastive questions. A pilot study was carried out to evaluate the method, where the reported findings show that participants preferred the interpretable policy level explanations. Though these explanations were contrastive they were not based on an underlying causal model. Reward decomposition was used by Juozapaitis et al. (2019) to generate minimally sufficient explanations, where reward differences were used to provide explanations which answer what action does have an 'advantage' over another. Juozapaitis et al. utilise the nature of the reward structure often present in domains to explain action preferences of the agent.</p>
<h1>2.4 Decision Tree Policy Explanations</h1>
<p>Central to our own work, we discuss how interpretability and explainability was achieved through representing agents' policies as decision trees or graphs.</p>
<p>From early work that represented the agent policy as a decision tree using the ' $G$ ' algorithm (Chapman and Kaelbling, 1991), past literature has explored how decision trees can be used to represent and abstract policies of MDPs. Roth et al. (2019) proposed a Qimprovment algorithm that builds an abstract decision tree policy for factored MDP based RL agents. Although decision tree policies are claimed to be more interpretable to humans than blackbox policies, the extent to which this is true is unclear as this work lacks human experiments. Abstract policy graphs have also been used as the basis to generate policy level explanations (Topin and Veloso, 2019). A feature importance measure was used to abstract multiple states into an abstract state which is then used to build the policy graph. The interpetability of the graph was evaluated computationally that shows a linear growth of the explanation size against an exponential growth of state-space. Although this implicitly demonstrates the interpretabilty of the approach, human experiments are needed to understand the effectiveness of the method.</p>
<p>Though above methods address interpretability to an extent, to the best of our knowledge previous literature has not studied how decision nodes from a decision tree can be incorporated with causal chains to provide explanations that are human-centred.</p>
<h3>2.5 Human-Centred Explanation</h3>
<p>Some researchers have recently emphasised how humans models of explanations can benefit XAI systems (Miller, 2018b) and how humans expect familiar models of explanations from XAI systems (De Graaf and Malle, 2017). Though some recent progress has been made (Madumal et al., 2020), human-centred computational models is still in its infancy.</p>
<p>Hilton et al. (Hilton et al., 2005; McClure et al., 2007; Hilton et al., 2010) has explored how causal chains of events inform and influence the explanations of humans. Opportunity chains can inform the explainee about long term dependencies that events have on each other, where certain events enable others. Human experiments have also been carried out that investigate he effects of opportunity chains on human-to-human explanation (Hilton et al., 2005). However, this work has not yet been extended to the case of model-free MDPs.</p>
<p>Our proposed distal explanation model take insights from social psychology literature to combine opportunity chains with causal explanations. To the best of our knowledge this is the first of such model in the context of explainable reinforcement learning agents.</p>
<h1>3. Human-Agent Study: Insights from Human Explanations</h1>
<p>In this section, we discuss insights we can gain from human models of explanation in literature. We then ground these models in data by conducting a human-agent experiment.</p>
<h3>3.1 Human Models of Causal Explanation</h3>
<p>Causality is a recurring concept in explanation models of social psychology and cognitive science literature (Hilton, 1990; Hornsby, 1993; Lombrozo and Vasilyeva, 2017). Using causal models as the basis for explanation seems natural and intuitive to humans (Sloman, 2005), since we build causal models to represent the world and to reason about it. Thus, it is plausible that, when used in intelligent agents, causal models have the ability to provide 'good' explanations to humans.</p>
<p>Importantly, causal models consist of causal chains. A causal chain is a path that connects a set of events, where a path from event $A$ to event $B$ indicates that $A$ has to occur before $B$ (Miller, 2018b) (we use event and action interchangeably in the paper). Hilton et al. ( 2005) define five types of causal chains that lead to five different types of explanations. Hilton et al. categorise these as, temporal, coincidental, unfolding, opportunity chains and pre-emptive. Through human experiments, Nagel and Stephan (2016) demonstrated that distal causes forms significant portion of an explainee's understanding of a terminal cause. Böhm and Pfister (2015) also affirms that, humans give both proximal and distal causes as explanations. Its important to note that, while in cognitive psychology literature a distal cause is a remote cause of an event in the past (essentially looking 'backward' from an event), in our agent simulations we use the distal terminology to denote an 'action' that is remote in the future (according to the agent's viewpoint this is looking 'forward' from a present event/action). Hilton et al. (2010) also explored how humans select different causal chains to provide explanations through human experiments. We conduct a similar study to gain insights from human models of explanation in a human-agent setting, and report results below.</p>
<h3>3.2 Study Objectives</h3>
<p>We seek to investigate how humans provide explanations of intelligent agents' behaviour and what concepts are present in such explanations. In contrast to similar studies done in social psychology (Hilton et al., 2010), our experiments present explanations of the agent's behaviour first to the participant and then gives the freedom to form their own explanations of the agent. The main objective of the study is to discover the frequency of different concepts in these human generated explanations given the agent behaviour explanations using different explanation methods.</p>
<p>Table 1: Codes (of the concepts) and descriptions of human generated explanations of agent behaviour. Examples are given from different participants.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Code</th>
<th style="text-align: center;">Description</th>
<th style="text-align: center;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Action</td>
<td style="text-align: center;">An action of the agent</td>
<td style="text-align: center;">P10: "It will keep attacking while it has the advantage"</td>
</tr>
<tr>
<td style="text-align: center;">Feature</td>
<td style="text-align: center;">A feature of the agent</td>
<td style="text-align: center;">P4: "The optimal number of supply depots is 2. and they should build those before a barracks"</td>
</tr>
<tr>
<td style="text-align: center;">Temporal</td>
<td style="text-align: center;">Refer some temporal quality</td>
<td style="text-align: center;">P12: "I think the artificial player will want to train marines right away so that it has an army quickly and be able to attack the enemy."</td>
</tr>
<tr>
<td style="text-align: center;">Objective</td>
<td style="text-align: center;">Refer to a short term objective</td>
<td style="text-align: center;">P12: "I think the artificial player will want to train marines right away so that it has an army quickly and be able to attack the enemy."</td>
</tr>
<tr>
<td style="text-align: center;">Causality</td>
<td style="text-align: center;">Implies a causal relationship</td>
<td style="text-align: center;">P3: "you need an army to attack[action] and by training marines you can do that"</td>
</tr>
<tr>
<td style="text-align: center;">Quantitative</td>
<td style="text-align: center;">Refers to a numerical value of a feature</td>
<td style="text-align: center;">P4: "The optimal number of supply depots is 2. and they should build those before a barracks"</td>
</tr>
<tr>
<td style="text-align: center;">Qualitative</td>
<td style="text-align: center;">A qualitative reference to a feature</td>
<td style="text-align: center;">P4: "As long as they have enough healthy marines. they should keep attacking"</td>
</tr>
<tr>
<td style="text-align: center;">Uncertainty</td>
<td style="text-align: center;">Mentions uncertainty</td>
<td style="text-align: center;">P5: "The army is in good health. it will most likely continue to attack."</td>
</tr>
<tr>
<td style="text-align: center;">Contrastive</td>
<td style="text-align: center;">Contrasting a feature with another</td>
<td style="text-align: center;">P15 "There are 2 supply depots but only 1 <br> barrack."</td>
</tr>
<tr>
<td style="text-align: center;">Goal</td>
<td style="text-align: center;">Refer to the goal(s) of the agent</td>
<td style="text-align: center;">P10: "The point of the game is to kill the enemy and destroy their base. so (incorrectly) the AI thinks the next step is to attack."</td>
</tr>
</tbody>
</table>
<h1>3.3 Experiment Design</h1>
<p>We conducted a human-agent study with 30 participants. In the first phase, participants were shown reinforcement learning agents playing the game StarCraft II. The agent behaviour (policy) was explained by providing 'local' explanations of agents' actions using one of 3 different explanations models: 1) No explanations, just visual description of the agent's behaviour; 2) State-action based explanations (Khan et al., 2009); and 3) Causal explanations (Madumal et al., 2020). Participants were divided evenly for each of these explanation models. Experiment was run on a web based interactive interface in through the Amazon Mechanical Turk (Buhrmester et al., 2011).</p>
<p>In the second phase, participants were shown new agent behaviour and were asked to 'predict' the agent's next action. Participants are expected to predict the next action based on the learned model of the agent in the first phase through explanations. This prediction task is not important to the objectives of this study, but is used as a way to get the participants to reasong about behaviour. In the same page, participants were then asked formulate</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Codes and their frequencies of 240 human explanations of reinforcement learning agents (that were using 3 different explanation models)
their own explanations about the agent. Participants were given a text-box to input the formulated explanations with no restrictions to word limit. This process is repeated for 8 rounds.</p>
<p>To filter out devious participants, we used the following approaches. Explanations containing less than three words or gibberish text were omitted. We also considered the time it took to input the explanation as a threshold. We omitted six participants according to the above criteria. In total we obtained a total of 240 explanations.</p>
<h1>3.4 Method</h1>
<p>We use thematic analysis (Braun and Clarke, 2006) to code the data and to identify concepts. By using thematic analysis, meaningful insights can be gained on how explanations of agents behaviour relates to existing literature on human explanations. As the first step in the thematic analysis, each explanation will be be divided into small chunks to identify categories and then these will be divided further into codes. Intuitively, a 'code' represents an atomic concept that exist in the explanation corpus. For an example, when a reference to an 'action' of the agent is present in the explanation, the sub-string of that reference can be coded (tagged) as an Action. This process is done manually until all the data chunks and explanations are coded. To ensure correctness, further passes through the explanation corpus is done as an attempt to identify new concepts that might have been missed in the first pass. Coded concepts and their descriptions are given in Table 1, along with example explanations extracted from participants.</p>
<h1>3.5 Results</h1>
<p>Figure 2 shows the frequencies of 9 codes across the 3 explanation models of the RL agents. Participants referred to 'actions' and 'features' of the agent the most, and often included the 'objective' or the 'goal' of the agent, which is present in action influence models (Madumal et al., 2020). Most importantly, the third most frequent code is 'temporal', in which participants refer to future actions the agent will take (i.e. distal actions). For example, consider an explanation from the data corpus, "The AI will want to have barracks so that it can then train soldiers to engage in attacks. It will want to progress". Here, the participant's explanation contains the distal action 'train soldiers' which is enabled by 'have barracks'. 'Causality' is also present in the explanations, interestingly even in 'No explanation' and State-action based explanation models. This suggests that humans frequently associate causal relationships when generating explanations. Our human-agent experimental data reaffirm the presence of opportunity chains in causal chains (Hilton et al., 2005), and show that these are frequently used to express how future actions are dependent on current actions of agents.</p>
<h3>3.6 Discussion</h3>
<p>Table 2: Presence of the concepts that were derived from codes, in different explainable reinforcement learning methods.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">XRL Method</th>
<th style="text-align: center;">Action</th>
<th style="text-align: center;">Feature</th>
<th style="text-align: center;">Temporal</th>
<th style="text-align: center;">Causal</th>
<th style="text-align: center;">Contrast</th>
<th style="text-align: center;">Objec</th>
<th style="text-align: center;">Goal</th>
<th style="text-align: center;">Quan</th>
<th style="text-align: center;">Qual</th>
<th style="text-align: center;">Uncer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Elizalde et al. (2007)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Khan et al. (2009)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Wang et al. (2016)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">van der Waa et al. (2018)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Struckmeier et al. (2019)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Hayes and Shah (2017)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Amir and Amir (2018)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Tabrez et al. (2019)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Fukuchi et al. (2017)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Juozapaitis et al. (2019)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Madumal et al. (2020)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Proposed method</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>The concepts that were derived from the codes are present in previous explainable reinforcement learning methods to varying degrees. Table 2 shows how these concepts are distributed. As most of these methods were not developed in a ground-up manner, some important concepts present in human explanations were not implemented in their explanation generation. When developing novel explanation models, insights gained from human-agent studies can help ground the model in the characteristics of human explanation. Human grounded explainable models can be more effective and accepted when deployed (Miller, 2018b; Langley et al., 2017). To this end, we conducted human-agent experiments to discover how a human would explain the reasoning and behaviour of an agent, when the agent has given prior explanations of it's own actions. When these human explanations were abstracted into 'codes', notable concepts like 'causality' and 'temporality' emerged. Previous work done in social psychology support our findings and coincide well with notions like opportunity chains (Hilton et al., 2005).</p>
<p>Though previous studies have explored the structure of causal chains in human explanations (Hilton et al., 2010), these are largely done in the absence of an intelligent agent. Further, in (Hilton et al., 2010), an explanation structure is investigated for events that have already occurred. In our study, as human explanations are for the behaviour of the agent, they can refer to how the past and present actions of the agent can influence the future. Ultimately, we use the resultant concepts of causality and distal opportunity chains to propose the distal explanation model for reinforcement learning agents.</p>
<h1>4. Preliminaries</h1>
<p>In this section, we present the necessary background that is required to follow the remainder of the paper.</p>
<h3>4.1 Markov Decision Processes</h3>
<p>We concern ourselves with providing an explanations for Markov Decision Process (MDP) based model-free RL agents. An MDP is a tuple $(\mathcal{S}, \mathcal{A}, \mathcal{T}, \mathcal{R}, \gamma)$, where $\mathcal{S}$ and $\mathcal{A}$ give state and action spaces respectively (here we assume the state and action space is finite and state features are described by a set of variables $\phi) ; \mathcal{T}=\left{P_{s a}\right}$ gives a set of state transition functions where $P_{s a}$ denotes state transition distribution of taking action $a$ in state $s ; \mathcal{R}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ is a reward function and $\gamma=[0,1)$ gives a discount factor. The objective of a reinforcement learning agent is to find a policy $\pi$ that maps states to actions maximizing the expected discounted sum of rewards. In model-free reinforcement learning, $\mathcal{T}$ and $\mathcal{R}$ is not known and the agent does not explicitly learn them.</p>
<h3>4.2 Structural Causal Models</h3>
<p>Structural causal models (SCMs) (Halpern and Pearl, 2005) provide a formalism for representing variables and causal relationships between those variables. SCMs represent the world using random variables, divided into exogenous (external) and endogenous (internal), some of which might have causal relationships which each other. These relationships can be described with a set of structural equations.</p>
<p>Definition 4.1. A signature $\mathcal{S}$ is a tuple $(\mathcal{U}, \mathcal{V}, \mathcal{R})$, where $\mathcal{U}$ is the set of exogenous variables, $\mathcal{V}$ the set of endogenous variables, and $\mathcal{R}$ is a function that denotes the range of values for every variable $\mathcal{Y} \in \mathcal{U} \cup \mathcal{V}$.
Definition 4.2. A structural causal model is a tuple $M=(\mathcal{S}, \mathcal{F})$, where $\mathcal{S}$ is as in Definition 4.1 and $\mathcal{F}$ denotes a set of structural equations, one for each $X \in \mathcal{V}$, such that $F_{X}$ : $\left(\times_{U \in \mathcal{U}} \mathcal{R}(U)\right) \times\left(\times_{Y \in \mathcal{V}-{X}} \mathcal{R}(Y)\right) \rightarrow \mathcal{R}(X)$ give the value of $X$ based on other variables in $\mathcal{U} \cup \mathcal{V}$. That is, the equation $F_{X}$ defines the value of $X$ based on some other variables in the model.</p>
<p>A context $\vec{u}$ is a vector of unique values of each exogenous variable $u \in \mathcal{U}$. A situation is defined as a model/context pair $(M, \vec{u})$. Given a situation $(M, \vec{u})$ an instantiation of $M$ given $\vec{u}$ is defined by assigning all endogenous variables the values corresponding to those defined by their structural equations.</p>
<p>An actual cause of an event $\varphi$ is a vector of endogenous variables and their values such that there is some counterfactual context in which the variables in the cause are different and the event $\varphi$ does not occur. An explanation is those causes that an explainee does not already know. Following example gives perspective to the notions discussed above.
Example 4.1. Consider the coffee task (Boutilier et al., 1995) where a robot has to deliver coffee to a user. The state consists of six binary variables, robot location $(L)$, robot is wet $(W)$, robot has umbrella ( $U m b$ ), raining ( $R n$ ), robot has coffee $(C)$ and user has coffee (Usr). Actions of the robot are go, buy coffee, get umbrella and deliver coffee. Then we can identify the set of endogenous variables $\mathcal{U}$ as $L, W, U m b, C$ and $U s r$ because the values of these variables can be influenced by the actions of the robot. In contrast, variable the variable $R n$ (raining) is an exogenous $(\mathcal{V})$ variable, because it is not defined by a function. A signature for this is generated by combining $\mathcal{U}, \mathcal{V}$ and the value range the variables can take (in this case either 0 or 1 ). Having the signature at hand, we can formulate a structural causal model $M$ by identifying the set of functions $\mathcal{F}$ that describe causal relationships of state variables. Assuming there is only one such function, we can define it $F_{U s r}=C+L$. This implies that the variable 'user has coffee' is causally influenced by variables 'robot has coffee' and 'robot location'. Model $M$ can be instantiated by getting the current values of the state variables and applying them to the set of $\mathcal{F}$. The actual cause of the event $U s r$ being true is the vector $(C=1, L=1)$ as both of these variables needs to be true for the user to have the coffee.</p>
<p>For a more complete review of SCM's we direct the reader to (Halpern and Pearl, 2005).</p>
<h1>4.3 Action Influence Models</h1>
<p>Action influence models (Madumal et al., 2020) provide explanations of the agent's behaviour based on the knowledge of how actions influence the environment. Informally, action influence models are an extension of SCMs that are augmented with agent actions. These models capture the causal relationships that exist in agent's knowledge about the world (i.e. state variables). Action influence models are formally defined for RL agents as follows,
Definition 4.3. Formally, a signature $S_{a}$ for an action influence model is a tuple $(\mathcal{U}, \mathcal{V}, \mathcal{R}, \mathcal{A})$, in which $\mathcal{U}, \mathcal{V}$, and $\mathcal{R}$ are as in SCMs from Defintion 4.1, and $\mathcal{A}$ is the set of actions from an MDP.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Action influence graph of a StarCraft II agent (Madumal et al., 2020)</p>
<p>Definition 4.4. An action influence model is a tuple $\left(S_{a}, \mathcal{F}\right)$, where $S_{a}$ is as above, and $\mathcal{F}$ is the set of structural equations, in which we have multiple for each $X \in \mathcal{V}$ - one for each unique action set that influences $X$. A function $F_{X, A}$, for $A \in \mathcal{A}$, defines the causal effect on $X$ from applying action $A$. The set of reward variables $R \subseteq \mathcal{V}$ are defined by the set of nodes with an out-degree of 0 ; that is, the set of sink nodes.</p>
<p>Definition 4.5. The actual instantiation Madumal et al. (2020) of an action influence graph is defined as $M_{\vec{\mathcal{V}} \leftarrow \vec{S}}$, in which $\vec{S}$ is the vector of state variable values from an MDP and $\mathcal{V}$ as in Definition 4.3. A counterfactual instantiation for a counterfactual action $B$ is a model $M_{\vec{Z} \leftarrow \vec{S}<em Z="Z">{Z}}$, where $\vec{Z}$ gives the instantiation of a counterfactual state $\overrightarrow{S</em>$.}</p>
<p>In an actual instantiation, we set the values of all state variables in the model, effectively making the exogenous variables irrelevant. Similarly, a counterfactual instantiation assign values to the model $M$ that could have realised under the action $B$.</p>
<p>Figure 3 shows the graphical representation of Definition 4.4 as an action influence graph of the StarCraft II agent described in the previous section, with exogenous variables hidden. These action influence models are SCMs except that each edge is associated with an action. In the action influence model, each state variable has a set of structural equations: one for each unique incoming action. As an example, from Figure 3, variable $\overrightarrow{A_{n}}$ is causally influenced by $\hat{S}$ and $\hat{B}$ only when action $A_{m}$ is executed, thus the structural equation $\mathcal{F}<em n="n">{A</em>(S, B)$ captures that relationship.}, A_{m}</p>
<h1>4.4 Explanations</h1>
<p>An explanation is generally defined as a pair that contains; 1) an explanandum, the event to be explained and 2) an explanan, the subset of causes that explain that event (Miller, 2018b). In its simplest form, the explanation for the question 'Why $P$ ?' would be in the form of 'Because $Q$ '. In the above example, $P$ is the explanandum and $Q$ is the explanan. As Lim et al. (2009) notes, why and why not questions are the most demanded explanatory questions. In the context of RL agents, we are interested in answering 'Why $A$ ?' and 'Why not $A$ ?' questions. Here, $A$ is an action of the agent and the explanation will be local.</p>
<p>Action influence models can be used to generate minimally complete explanations. An explanation that constitutes all the causes as an explanan risk overwhelming the explainee, thus it is important to balance the completeness and the minimality of the explanations (Miller, 2018b).</p>
<p>Definition 4.6. A minimally complete explanation for an action $a$ under the actual instantiation $M_{\vec{\mathcal{V}} \leftarrow \vec{S}}$ is a tuple $(\vec{R}=\vec{r}, \vec{H}=\vec{h}, \vec{I}=\vec{i})$, in which $\vec{R}$ is the vector of reward variables reached by following the causal chain of the graph to sink nodes; $\vec{H}$ the vector of variables of the head node of action $a, \vec{I}$ is the vector of variables that are immediate predecessors of any variable in $\vec{R}$ within the causal chain, with $\vec{r}, \vec{h}, \vec{i}$ giving the values of these variables under $M_{\vec{\mathcal{V}} \leftarrow \vec{S}}$ from Definition 4.5 .</p>
<p>McClure and Hilton (1997) argue that 'goals' should be referred to in some form when explaining actions. In reinforcement learning, rewards of the agent can be thought of as a proxy for the goals. Though in most cases the 'rewards' ( $\vec{R}$ from Definition 4.6) on itself would not form a complete explanation, because they are not attached to variables. Immediate predecessor nodes $(\vec{I})$ of the reward nodes refer to the state variables that 'trigger' rewards.. Though this combination now can explain the long term motivation of the agent, the head node $(\vec{H})$ attached to the action is used to explain the immediate (short-term) cause. From Figure 3, the explanation for Why action $A_{s}$ would constitute, $D_{u}$ and $D_{b}$ in as reward variables $\vec{R}, A_{n}$ in $\vec{I}$ and $S$ in $\vec{H}$. Madumal et al. (2020) present a method for generating such explanations, and evaluate this on a large-scale user study.</p>
<h1>5. Distal Explanation Model</h1>
<p>From the insights gained from human explanations discussed in Section 3 we propose a distal explanation model that can generate explanations for opportunity chains. In the following sections we use the adversarial scenario (discussed at length in Section 6.1) of the StarCraft II environment as a running example to aid the definitions.</p>
<h3>5.1 Overview</h3>
<p>Figure 4 shows an overview of the distal explanation model. The model consists of four distinct components. First, state-action pairs are extracted as a replay dataset from the episodes during reinforcement learning. Dataset generation happens at the agent training time. This dataset is used to train the decision-tree policy (indicated as the blue subcomponent in Figure 4). The decision-tree policy is used as a surrogate policy for the agent, where it is used to extract reasons (in the form of decision nodes) for a given action (we discuss this process at length in Section 5.2). The dataset is also used to train the distal action predictor (shown as the green sub-component), which predicts dependent actions. Because we want to predict distal actions (contained in a opportunity chain) using a sequence of prior actions from the agent action trace, a many-to-one recurrent neural network (Schuster and Paliwal, 1997) is used as the predictor, though other sequence predictors can also be used. An action influence graph is used to extract causal chains (shown in red) that is used in conjunction with the decision tree policy to produce the final explanation. The explanation</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: An overview of the Distal explanation model
is given as a three-tuple: reward nodes of the causal chain; matched decision nodes; and the predicted distal action.</p>
<p>Before formalising the distal explanation model we first discuss how explanations can be generated using decision tree policies.</p>
<h1>5.2 Causal Explanations from Decision Trees</h1>
<p>Although causal explanations from action influence models have been shown to perform better than state-action based (Khan et al., 2009) explanation models, the use of structural equations (Madumal et al., 2020) models the environment rather than the policy of the agent. Thus, the explanations from these model why an action would be a good idea, rather than why the agent chose it. In this work, we instead propose to extract reasons for action selection from a surrogate policy. We learn an interpretable surrogate policy in the form of a decision tree using batched replay data. If the agent's underlying policy is also a decision tree, this step can be omitted.</p>
<p>Training The Surrogate Policy: The distal explanation model we introduce uses decision nodes of a decision tree that represent a surrogate policy to generate explanations with the aid of causal chains from an action influence model. Let $\widehat{\mathbb{T}}$ be a decision tree model. In each episode at the training of the RL agent, we perform experience replay (Lin, 1992) by saving $e_{t}=\left(s_{t}, a_{t}\right)$ at each time step $t$ in a data set $D_{t}=\left{e_{1}, \ldots, e_{t}\right}$. Drawing uniformly from $D$ as mini-batches, we train $\widehat{\mathbb{T}}$ using input $x=\vec{s}$ and output $y=\vec{a}$. Clearly, explanations generated from an unconstrained decision tree can overwhelm the explainee, as these produce a large number of decision nodes for a question. Thus we limit the growth of $\widehat{\mathbb{T}}$ by setting the max number of leaves to the number of actions in the domain (i.e. the leaves of the trained $\widehat{\mathbb{T}}$ will be the set of actions of the agent). We later show that this hardly affects the task prediction accuracy compared to a depth unconstrained decision tree for our experiments. To get the decision nodes of $\widehat{\mathbb{T}}$ in state $S_{t}$, we simply traverse the tree from the root node until we reach a leaf node and get the nodes of the path. The decision tree of the StarCraft II adversarial task is given in Figure 5 a), with the decision nodes $A_{n}$ and $B$</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Generating explanations by mapping (a) decision nodes to (b) causal chains.
for the action $A_{s}$. Each decision node maps to a feature variable of the agent's state. Figure 5 shows how the decision nodes are mapped to the action influence graph, in the StarCraft II adversarial scenario.</p>
<p>Generating Explanations Using the Surrogate Policy: In the context of an RL agent, we introduce a new definition of minimally complete explanations using decision nodes for 'why' questions below.</p>
<p>A primitive explanation can be generated by using the decision-tree policy alone, by extracting the decision nodes of an action. E.g. for the question Why $A_{s}$, we can obtain the decision nodes simply by traversing to the leaf node $A_{s}$ from the root node ( $A_{n}$ and $B$ are the decision nodes in this case, as highlighted in Figure 5 a)). However, an explanation like this can contain variables that are not causally relevant to the action performed. This primitive explanation can be enhanced by taking the causal chain for the action being explained from an action influence model and filtering out causally irrelevant variables. We define this as a minimally complete explanation below.</p>
<p>Definition 5.1. Given the set of decision nodes $\overrightarrow{A_{d}}=\overrightarrow{x_{d}}$ for the action $a$ from a decision tree $\widehat{\mathbb{T}}$, we define a minimally complete explanation for a why question as a pair $(\vec{R}=\vec{r}, \vec{N}=\vec{n})$, in which $\vec{R}$ is the vector of reward variables reached by following the causal chain of the graph to sink nodes; $\vec{N}$ is such that $\vec{N}$ is the maximal set of variables in which $\vec{N}=\left(\overrightarrow{A_{a}}=\right.$ $\left.\overrightarrow{x_{a}}\right) \cap\left(\overrightarrow{A_{d}}=\overrightarrow{x_{d}}\right)$, where $\overrightarrow{A_{a}}$ is the set of intermediate nodes of the causal chain of action $a$, with $\vec{r}, \overrightarrow{x_{a}}$ and $\overrightarrow{x_{d}}$ giving the values under the actual instantiation $M_{\vec{v} \leftarrow \vec{S}}$ from Definition 4.5 .</p>
<p>Above definition only select the decision nodes (from the total set of decision nodes given from the decision-tree policy) that exist as intermediate nodes of the causal chain of the given action.</p>
<p>In the StarCraft II scenario, for the question 'Why action $A_{s}$ ?', we can generate the minimally complete explanation by first finding the decision nodes for action $A_{s}$, shown as medium grey nodes in Figure 5(a). Then finding the causal chain of action $A_{s}$ (given by the bold path in Figure 5). And finally getting the common set of nodes from the causal chain and the decision nodes ( $B$ in Figure 5) and appending the reward nodes ( $D_{u}$ and $D_{b}$ ). Example 5.1 below compare and contrast an explanation with and without the use of action influence models.</p>
<h1>Algorithm 1 Generating Counterfactuals</h1>
<p>Input: causal model $\mathcal{M}$, current state $S_{t}$, trained decision tree $\widehat{\mathbb{T}}$, actual action $a, \Delta$
Output: contrastive explanation $\mathrm{t} \vec{X}<em d="d">{c}$
1: $\vec{X}</em>$
2: $\vec{X}} \leftarrow \widehat{\mathbb{T}} \cdot$ traversetree $(a)$; vector of decision nodes of $a$ from $\widehat{\mathbb{T}<em d="d">{c} \leftarrow[]$; vector of counterfactual decision nodes.
3: for every $D \in \vec{X}</em>$ do
$4: \quad x_{d} \leftarrow D \cdot$ decisionNodeValue $($; decision boundary value of $D$
5: $\quad x_{m} \leftarrow D \cdot$ moveBoundary $\left(x_{d}\right)$; boundary value changed by a $\Delta$.
6: $\quad S_{t} m \leftarrow S_{t} \cup x_{m}$; modify the corresponding state feature variables with the new $x_{m}$.
7: $\quad \vec{X}<em c="c">{c} \leftarrow \vec{X}</em> m\right)$; get the counterfactual decision nodes by getting the counterfactual action and then traversing the tree.
8: end for
9: return $\vec{X}_{c}$} \cup \widehat{\mathbb{T}} \cdot \operatorname{predict}\left(S_{t</p>
<p>Example 5.1. Question: Why $A_{s}$ ?
Just decision-tree policy: Because Ally unit number $\left(A_{n}\right)$ is 4 and Barracks number $(B)$ is 1 .
With action influence models: Because ally unit number $\left(A_{n}\right)$ is 4 and the goal is to have more Destroyed Units $\left(D_{u}\right)$ and Destroyed buildings $\left(D_{b}\right)$.</p>
<h3>5.3 Contrastive Explanations from Counterfactuals</h3>
<p>Counterfactuals explain events that did not happen-but could have under different circumstances. Counterfactuals are used to describe events from a 'possible world' and to contrast them with what happened in actuality. Embedding these counterfactuals in explanations can make the explanation more meaningful (Byrne, 2019). Naturally, an explanation given to a 'why not' question should compare the counterfactuals with the actual facts to form a contrastive explanation (Miller, 2018b,a). For this reason, we concern ourselves with generating contrastive explanations from decision nodes and causal models.</p>
<p>We generate the counterfactual decision nodes using Algorithm 1, in which we find the decision nodes of the counterfactual action $b$ by changing the decision boundary of the actual action $b$ in the decision tree. We can now define minimally complete contrastive explanations for 'why not' questions using these counterfactual decision nodes.</p>
<p>Definition 5.2. Given the set of decision nodes $\vec{X}<em d="d">{d}=\overrightarrow{x</em>}}$ for the action $a$ from a decision tree $\widehat{\mathbb{T}}$, a minimally complete contrastive explanation for a why not question is a pair $\left(\vec{R}=\vec{r}, X_{\text {con }}^{\vec{\ }}=x_{\text {con }}^{\vec{\ }}\right)$, in which $\vec{R}$ is same as in Definition 5.1; $X_{\text {con }}^{\vec{\ }}$ is such that $X_{\text {con }}^{\vec{\ }}$ is the maximal set of variables in which $X_{\text {con }}^{\vec{\ }}=\left(\vec{X<em b="b">{b}=\overrightarrow{x</em>}}\right) \cap\left(\vec{X<em c="c">{c}=\overrightarrow{x</em>}}\right)$, where $\vec{X<em c="c">{b}$ gives the set of intermediate nodes of the causal chain of the counterfactual action $b$, and $\vec{X}</em>$ from Definition 4.5.}$ is generated using the Algorithm 1. Values $\vec{r}, \overrightarrow{x_{c}}$ are contrasted using the actual instantiation $M_{\vec{Y} \leftarrow \vec{S}}$ and counterfactual instantiation $M_{\vec{Z} \leftarrow S_{Z}^{\prime}</p>
<p>Instead of just having the intermediate nodes of the causal chain of the actual action (as in Definition 5.1), we now get the set of intermediate nodes for for the counterfactual action</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Visual example of the explanation for Why not $A_{b}$. Actual action and the actual causal chain is shown in blue, counterfactual chain and nodes are shown in red and the contrast node is shown in green.
from its causal chain. Then the intermediate nodes of the counterfactual chain is compared with the set of nodes we get from the Algorithm 1, to get the common set of nodes, of which the variable values will finally be contrasted.</p>
<p>As before, we explain Definition 5.2 using the adversarial StarCraft II task. Consider the question 'Why not action $A_{b}$ ', when the actual action is $A_{s}$, for which the explanation is generated as follows. We first get the decision nodes $A_{n}$ and $B$ having $&lt;=5$ and $&gt;2$ as the decision boundaries respectively. Then each decision boundary value starting with the node closest to the leaf node, is moved by a small $\Delta$ amount 0.01 and applied as the new feature value in the current state of the agent ( $B$ feature value will change to 1.99). We use this new state to predict the counterfactual action as $A_{b}$ from the decision tree, and to get the counterfactual decision nodes (which remains the same). Next, we get the intersection of nodes in the causal chain of the counterfactual action $A_{b}\left(B \rightarrow A_{n} \rightarrow\left[D_{u}, D_{b}\right]\right)$ with $\overrightarrow{X_{c}}$, which gives $B$ as $\overrightarrow{X_{\text {con }}}$ with the actual value 3 and counterfactual value 1.99. Finally, these values are contrasted and appended with the reward nodes of the causal chain of $A_{b}$ to generate the explanation. A graphical interpretation of this explanation is shown in Figure 6 .</p>
<h1>5.4 Learning Opportunity Chains</h1>
<p>Explaining the behaviour of the agent using only the policy (or a surrogate policy) alone, even if the explanation is causal, has shortcomings as this does not consider that some actions might be chosen because they enable other actions. In this section we discuss how information on enabling actions can be used to form a more complete explanation.</p>
<p>In the context of reinforcement learning, we define a 'distal action' as the action that depends the most on the execution of the current action of the agent. The agent might not be able to execute the distal action unless some other action was executed first (i.e. some actions 'enable' the execution of other actions). For example, in the StarCraft II domain, the action 'train marines' cannot be executed until 'build barracks' action is executed. While it is possible to extract distal actions from environment dynamics and pre-conditions in a</p>
<p>model-based system, for model-free RL agents, this remains a challenge. However, for the purpose of explanation, it is possible to provide an approximation and predict the distal action.</p>
<p>We use a many-to-one recurrent neural network (RNN) (Schuster and Paliwal, 1997) as our prediction model $\widehat{\mathbb{L}}$ to approximate the distal action given a sequence of previous states and actions of the agent. We implement $\widehat{\mathbb{L}}$ with a fully connected hidden layer of 10 units, and a batch size of 100 . For training data, we use the batch replay dataset $D_{t}$ discussed in Section 4.2. We define a sequence as a state-action trace that ends in an action that is one of the last actions in a causal chain (e.g. in Figure 2, the last action of all causal chains in the 'attack' action). The output of the model $\widehat{\mathbb{L}}$ will be the distal action and its expected cumulative reward. Formally, this action prediction model can be written as: $\hat{y}<em N_1="N+1">{t</em>}}=f\left(x_{t_{1}}, x_{t_{2}}, \ldots, x_{t_{N}} ; t_{1}, t_{2}, \ldots, t_{N}\right)$, where $x_{t_{N}}$ is the state-action pair (including state features) of the last action of a causal chain, $\hat{y<em N_1="N+1">{t</em>$ in hand, we now define minimally complete distal explanations for 'why' and 'why not' questions that incorporate causal nature to the explanations.}}$ gives the distal action and the reward. Here, we use the immediate next action that lies in a particular causal chain as the ground truth. Note that even though we used an RNN to implement the prediction model, it is entirely possible to use other models to approximate the distal action. With the distal action prediction model $\widehat{\mathbb{L}</p>
<p>Definition 5.3. Given a minimally complete contrastive explanation, current action $a$ and a prediction model $\widehat{\mathbb{L}}$, a minimally complete distal explanation is a tuple $\left(\vec{R}=\vec{r}, \overrightarrow{X_{\text {con }}}=x_{\text {con }}^{\vec{\sigma}}, a_{d}\right)$, in which $\vec{R}$ and $\overrightarrow{X_{\text {con }}}$ do not change from Definition 5.2; and $a_{d}$ gives the distal action predicted through $\widehat{\mathbb{L}}$ such that $a_{d} \in A \cap A_{c}$, where $A$ is the action set of the agent and $A_{c}$ gives the action set of the causal chain of current action $a$.</p>
<p>Informally, this simply prepends the predicted distal action to a minimally complete contrastive explanation generated through Definition 5.2 if the distal action exists in the causal chain of the current action. Consider the example 'Why not action build_barracks $\left(A_{b}\right)$, when the actual action is train_marine $\left(A_{m}\right)$. This would yield the counterfactual decision node $A_{n}$ (ally unit number) with the actual value 10 and the counterfactual value 5. When the predicted distal action is attack $\left(A_{a}\right)$, we can generate the below explanation text using a simple natural language template. The causal explanation is generated with Definition 4.6 while the distal explanation is generated through Definition 5.3.</p>
<p>Causal Explanation: Because it is more desirable to do the action train marine $\left(A_{m}\right)$ to have more ally units $\left(A_{n}\right)$ as the goal is to have more Destroyed Units $\left(D_{u}\right)$ and Destroyed buildings $\left(D_{b}\right)$.
Distal Explanation: Because ally unit number $\left(A_{n}\right)$ is less than the optimal number 18, it is more desirable do the action train marine $\left(A_{m}\right)$ to enable the action attack $\left(A_{a}\right)$ as the goal is to have more Destroyed Units $\left(D_{u}\right)$ and Destroyed buildings $\left(D_{b}\right)$.</p>
<p>Note that the Definition 5.3 can also be used in conjunction with the Definition 5.1 to generate distal explanations for 'why' questions.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Env - RL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SE - Accuracy (\%)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">DP - Accuracy (\%)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Size</td>
<td style="text-align: center;">LR</td>
<td style="text-align: center;">DT</td>
<td style="text-align: center;">MLP</td>
<td style="text-align: center;">$D P$</td>
<td style="text-align: center;">$D P_{n}$</td>
</tr>
<tr>
<td style="text-align: left;">Cartpole-PG</td>
<td style="text-align: center;">$4 / 2$</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">96.83</td>
<td style="text-align: center;">97.10</td>
</tr>
<tr>
<td style="text-align: left;">MountainCar-DQN</td>
<td style="text-align: center;">$3 / 3$</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">88.66</td>
<td style="text-align: center;">86.75</td>
</tr>
<tr>
<td style="text-align: left;">Taxi-SARSA</td>
<td style="text-align: center;">$4 / 6$</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">67.9</td>
<td style="text-align: center;">82.44</td>
<td style="text-align: center;">86.19</td>
</tr>
<tr>
<td style="text-align: left;">LunarLander-DDQN</td>
<td style="text-align: center;">$8 / 4$</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">63.7</td>
<td style="text-align: center;">72.1</td>
<td style="text-align: center;">72.82</td>
<td style="text-align: center;">72.91</td>
</tr>
<tr>
<td style="text-align: left;">BipedalWalker-PPO</td>
<td style="text-align: center;">$14 / 4$</td>
<td style="text-align: center;">56.9</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">67.99</td>
<td style="text-align: center;">69.28</td>
</tr>
<tr>
<td style="text-align: left;">StarCraft-A3C</td>
<td style="text-align: center;">$9 / 4$</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">91.8</td>
<td style="text-align: center;">91.4</td>
<td style="text-align: center;">97.36</td>
<td style="text-align: center;">86.04</td>
</tr>
</tbody>
</table>
<p>Table 3: Distal explanation model evaluation in 6 benchmark reinforcement learning domains that use different RL algorithms, measuring mean task prediction accuracy in 100 episodes after training. SE-structural equations (trained with LR-linear regression, DT-decision trees, MLP-multi layer perceptrons), $D P$-decision policy tree and $D P_{n}$-unconstrained decision policy tree.</p>
<h1>5.5 Computational Evaluation</h1>
<p>We use five OpenAI benchmarks (Brockman et al., 2016) and the adversarial StarCraft II scenario (discussed in Section 5.1) to evaluate the task prediction (Hoffman et al., 2018) accuracy of our distal explanation model and compare against action influence models (Madumal et al., 2020) as a baseline. Task prediction can be used to predict what the agent will do in the next instance, and measures how faithful the surrogate policy is against the underlying policy.</p>
<p>We choose the benchmarks to have a mix of complexity levels and causal graph sizes (given by the number of actions and state variables). We train the RL agents using different types of model-free RL algorithms (see Table 3), using a high performance computer cluster node with 2 Nvidia V100 GPUs, 56GB of memory and 20 core CPU with 2.2 GHz speed. All agents were trained until the reward threshold (to consider as 'solved') of the environment specification is reached.</p>
<p>We evaluate two versions of the distal explanation model, where one is a based on a depth limited decision tree with the number of actions ( $D P$ in table 3), other trained until all leaves are pure nodes $\left(D P_{n}\right)$. Results summarised in Table 3 show our model outperforms task prediction of action influence models (with their structural equations trained by either linear regression (LR), decision trees (DT) or multi layer perceptrons (MLP)) in every benchmark, some by a substantial margin.</p>
<p>The benefit gained through unconstrained decision trees $\left(D P_{n}\right)$ does not translate well into an increase in task prediction accuracy. We conclude that for the purpose of using distal models for explanation, a depth limited tree $(D P)$ provide an adequate level of accuracy. Moreover, as a depth limited tree is likely to be more interpretable to a human, it is more suited for explainability and explanation.</p>
<h2>6. Evaluation: Human Study</h2>
<p>We consider human subject experiments to be an integral part of XAI model evaluation and as such conduct a human study with $\mathbf{9 0}$ participants. We consider two hypotheses for</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: StarCraft II Collaborative task scenario: The agent is controlling the leftmost section and the participant controls the right section (divided by the fissure)
our empirical evaluation; 1) Distal explanation models leads to a improved understanding of the agent; and 2) Distal explanation models provide subjectively 'better' explanations. Our experiment involves RL agents that complete objectives in three distinct scenarios, which are based on the StarCraft II (Vinyals et al., 2017) learning environment. We first discuss these scenarios below.</p>
<h1>6.1 Scenarios</h1>
<p>In addition to the default scenario of the StarCraft II, we developed two additional scenarios as custom maps using the StarCraft II platform as a framework, that are better suited for explainability. Custom maps were made to add a more strategic nature to scenarios and in some cases to elicit cooperation from the interacting human. Note that these scenarios are completely different from the StarCraft II game. We only use StarCraft II assets as a simulation framework, similar to how e.g. a grid-world framework can be used to make many different scenarios. We also release these maps with state and action specifications as test-beds for explainability research.</p>
<p>Adversarial In this scenario, the agent's objective is to build its base by gathering resources and destroy the enemy's base. The agent can build offensive units (marines) to attack the enemy's base and to defend its own base. This is the default objective in a normal StarCraft II game, but here we only use 4 actions for the purpose of the experiment rewards are given for the number of enemies and buildings destroyed (shown in Figure 5 b) as an action influence graph). During the experiment, the trained RL agent will provide</p>
<p>explanations to the participant and the strength of the explanations are evaluated through task prediction.</p>
<p>Rescue This scenario is a custom map, where the agent's objective is to find a missing unit and bring it back to the base using an aerial vehicle. The agent also has to avoid or destroy enemy units during the rescue and aid the aerial vehicle using an armed unit. The agent has access to 5 actions, the reward is given for the number of missing units saved. The evaluation is done through task prediction as before.</p>
<p>Collaborative Task The collaborative task is fundamentally different from the previous scenarios, in that the participant has to help the agent to complete the objective. We made this task as a custom map (depicted in Figure 7) where the map is partitioned as the agent and human 'area'. The agent can perform 5 actions in this task, while the human can choose 4 actions to execute. The objective of the task is to build a series of structures that finally leads to the creation of an 'elite' unit, which the human has to transport to a base. The success of the task depends on the participant choosing to execute the action that best support the agent.</p>
<h1>6.2 Experiment Design and Methodology</h1>
<p>To investigate the two main hypotheses, we use a mixed design (Keren, 2014) (within subject and between subject) for our experiment. Every participant will be evaluated on the 3 independent variables which are 1) 'no explanations', where only a visual description of the agent behaviour is provided; 2) causal explanations generated with action influence models (Madumal et al., 2020) and 3) our distal explanation model. At a glance, the experiment has 3 phases where participants receive explanations from RL agents, subjectively evaluate the explanation and are then evaluated through task prediction (Hoffman et al., 2018) to gauge their understanding of the agent.</p>
<p>Task prediction is an effective measure that can peek into the mental model of an explainee to evaluate how successful the given explanation was in transferring the knowledge from the explainer (Hoffman et al., 2018; Miller, 2018b). In task prediction, the participant is asked the question 'What will the agent do next?'. We use task prediction to evaluate the hypothesis 1) for the Adversarial and Rescue scenarios, and invert the question as to ask 'What would you do next?' in the Collaborative task. We investigate hypothesis 1) by employing the 5-point Likert explanation satisfaction scale of Hoffman et al. (2018, p.39). Explanation satisfaction is evaluated after each explanation and also at the end of the experiment which compares explanations of causal and distal models.
Experiment Design: We use Amazon Mechanical Turk (Mturk)—a crowd sourcing platform well known for obtaining human-subject data (Buhrmester et al., 2011)-to conduct the experiments. A web-based interactive interface is used as the medium of interaction.</p>
<p>We first display the ethics approval obtained through a university, and after the participants' consent gather demographic information. We then show video clips of the agents solving the 3 StarCraft II scenarios that capture the behaviour of the agents. Each scenario has 4 distinct behaviours of the respective agent (around 10 seconds per clip). Every participant sees all three scenarios, and all three explanation types, but between participants, the combination of scenario and explanation type are mixed. For example, a participant may</p>            </div>
        </div>

    </div>
</body>
</html>
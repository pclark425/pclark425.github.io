<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2461 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2461</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2461</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-65.html">extraction-schema-65</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-272423732</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.02977v1.pdf" target="_blank">Large Language Model-Based Agents for Software Engineering: A Survey</a></p>
                <p><strong>Paper Abstract:</strong> The recent advance in Large Language Models (LLMs) has shaped a new paradigm of AI agents, i.e., LLM-based agents. Compared to standalone LLMs, LLM-based agents substantially extend the versatility and expertise of LLMs by enhancing LLMs with the capabilities of perceiving and utilizing external resources and tools. To date, LLM-based agents have been applied and shown remarkable effectiveness in Software Engineering (SE). The synergy between multiple agents and human interaction brings further promise in tackling complex real-world SE problems. In this work, we present a comprehensive and systematic survey on LLM-based agents for SE. We collect 106 papers and categorize them from two perspectives, i.e., the SE and agent perspectives. In addition, we discuss open challenges and future directions in this critical domain. The repository of this survey is at https://github.com/FudanSELab/Agent4SE-Paper-List.</p>
                <p><strong>Cost:</strong> 0.039</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2461.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2461.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Elicitron</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Elicitron</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent framework for requirements elicitation that initializes multiple agents with different personas to simulate stakeholder interactions, document action/observation/challenge traces, and extract potential requirements via iterative agent interviews and filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Elicitron: An LLM agent-based simulation framework for design requirements elicitation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Elicitron</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Multi-agent framework for the requirements elicitation phase. It spawns multiple persona agents representing diverse user viewpoints which interact with a simulated product; each agent records steps (actions, observations, challenges) into a shared record; the system aggregates interview transcripts and filters candidate requirements according to criteria to produce elicited requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>multiple (variable personas)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>stakeholder-persona agents (simulate users with different viewpoints); collector/recorder functionality (document actions/observations/challenges); filter/selector (applies criteria to candidate requirements).</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>requirements elicitation (requirements discovery and initial documentation)</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>shared workspace / simulated interview loop where persona agents interact with the target product and with each other; aggregation step merges outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>natural language dialogues among persona agents and structured records written to a shared workspace (action/observation/challenge entries)</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>iterative agent interviews with internal filtering: agents document challenges and observations which are used to refine subsequent simulations and filter candidate requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>multi-round interactive (agents simulate interactions iteratively until elicitation converges)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>requirements engineering within software engineering (elicitation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>reported qualitative effectiveness (uncovers and categorizes hidden needs) and cost reductions vs conventional user studies (no detailed numeric metrics provided in survey text).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>compared qualitatively to conventional methodologies such as user studies; reported reductions in cost and ability to discover hidden needs (no exact numeric baseline figures in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>diverse-perspective coverage (multiple personas improves completeness of elicited requirements) and reduced cost compared with traditional user studies.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>potential for noisy/redundant outputs from many persona agents and need to define filtering criteria to remove spurious/low-quality requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>not specified; the system relies on designing diverse persona roles and criteria for filtering to balance coverage and noise.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model-Based Agents for Software Engineering: A Survey', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2461.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2461.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SpecGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SpecGen</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage system to generate formal requirement specifications (JML) for programs using an LLM-driven conversation stage followed by mutation-based generation and external verification with OpenJML, integrating verifier feedback into iterative prompt refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Specgen: Automated generation of formal program specifications via large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SpecGen</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Two-stage LLM-based pipeline: (1) conversation-driven specification generation where an agent drafts JML specification and invokes an external verifier (OpenJML) to validate; (2) mutation-based specification generation where failing specs are mutated and re-verified to produce diverse and correct specifications. Verifier feedback is integrated into prompts iteratively.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>single LLM-driven agent with iterative verifier interaction (effectively single-agent with tool loop)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>specification generator (LLM), external tool verifier (OpenJML) used as a feedback oracle</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>specification generation and validation (requirements/specification phase)</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>iterative generator–tool loop: LLM generates spec, verifier returns validation results, LLM refines spec based on feedback</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>structured prompts and verifier messages (tool feedback integrated into natural-language prompts); specifications expressed in JML (semi-structured formal language)</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>tool feedback from OpenJML (validation errors) used to iteratively adjust prompts and mutate failing specs to produce more diverse/correct specifications</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>on each generation/verification iteration (generate -> verify -> refine loop)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>software specification / requirements engineering (formal specification generation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>experimental claim: SpecGen outperforms state-of-the-art approaches in the surveyed evaluation (no numerical details in survey extract).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>compared against state-of-the-art specification generation approaches and shown to perform better (exact metrics not provided in the survey text).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>integrating external verifier feedback reduces invalid or uncheckable specifications and improves quality/diversity via mutation stage.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>dependence on verifier coverage and ability to represent verification failures usefully inside prompts; mutation management to avoid combinatorial explosion.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>two-stage workflow (conversation-driven + mutation-based) with verifier-in-the-loop recommended by authors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model-Based Agents for Software Engineering: A Survey', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2461.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2461.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MARE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MARE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent collaboration framework that covers multiple requirements engineering phases (elicitation, modeling, verification, specification) via specialized agents communicating through a shared workspace using predefined actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MARE: multi-agents collaboration framework for requirements engineering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MARE</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Multi-agent RE framework where stakeholder agents express needs, collector agent drafts requirements, modeler agent identifies entities/relationships and builds requirement models, checker agent assesses draft quality against criteria, and documenter agent writes final specifications or reports errors. Agents use a shared workspace to exchange intermediate results.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>multiple (role-based: stakeholder agents + collector + modeler + checker + documenter)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>stakeholder agents (express needs), collector agent (organizes inputs), modeler agent (entity/relationship extraction and requirement modeling), checker agent (quality assessment), documenter agent (writes specifications/reports).</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>elicitation, modeling, verification, specification (end-to-end RE pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>shared workspace (blackboard) where agents write/read structured messages and artifacts; sequential phase handovers (collector -> modeler -> checker -> documenter)</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>structured messages/artifacts in a shared workspace (semi-structured documents, JSON-like artifacts), plus natural-language prompts within agent steps</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>checker agent provides quality assessments and error reports back to modeler/collector; iterative handoffs allow refinement before final documentation</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>phase transitions plus on-demand within phases (sequential pipeline with iterative refinements per phase)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>requirements engineering for software engineering</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>survey reports that MARE covers multiple RE phases and facilitates role simulation; no numeric performance metrics provided in survey extract.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>qualitative comparison to single-agent approaches and manual processes; no numeric baselines in survey extract.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>role specialization supports structured progression from elicitation to specification and enables verification/quality checks before documentation.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>designing effective shared workspace artifacts and avoiding information distortion in multi-turn communications.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>predefined role pipeline and shared workspace with structured message formats suggested to ensure traceability and exchange of intermediate artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model-Based Agents for Software Engineering: A Survey', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2461.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2461.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeAgent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent code review system that simulates a waterfall-like team with six distinct agent characters (user, CEO, CPO, CTO, coder, reviewer) across four pipeline stages (information sync, review, alignment, documentation) to perform code review tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Codeagent: Collaborative agents for software engineering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CodeAgent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Simulates a multi-role code review team across four stages: basic information synchronization (CEO/CPO/coder analyze input), code review (coder & reviewer collaborate to produce analysis), code alignment (coder/reviewer iterate edits), and documentation (CEO/CPO/coder produce final artifacts). Designed to automate code review tasks including consistency, vulnerability, format, and revision.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>6</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>CEO/CPO (high-level coordination), coder (implementation & edits), reviewer (code review/analysis), user (role to provide external perspective)</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>code review and post-review code alignment/documentation (quality assurance phase of SE)</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>ordered multi-stage pipeline (waterfall-like within the review workflow) with role-specific responsibilities and handoffs between stages</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>natural language dialogues among role agents and shared artifacts/reports passed to next stages (structured analysis reports produced)</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>reviewer provides analysis reports to coder; coder revises code and re-enters review; higher-level agents check final documentation.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>structured stage-based communication (after each stage and during review iterations)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>code review (software quality assurance)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>survey reports CodeAgent demonstrates effectiveness and efficiency across consistency analysis, vulnerability analysis, format analysis, and code revision tasks; no precise numeric scores provided in survey extract.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>compared qualitatively to manual code review processes; no numeric baselines in survey extract.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>simulating multi-role real-world teams enables division of responsibility and more comprehensive reviews (vulnerabilities, style, correctness).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>overhead of managing many role-specific agents; possible redundant or conflicting review suggestions requiring alignment phase.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>waterfall-like staged review pipeline with explicit role definitions recommended for structured code-review automation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model-Based Agents for Software Engineering: A Survey', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2461.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2461.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AgentFL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AgentFL</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent system for project-level fault localization that scales LLM-based localization by assigning four specialized agents (test code reviewer, source code reviewer, software architect, software test engineer), each with dedicated tools and expertise, to decompose the localization task into fault comprehension, codebase navigation, and fault confirmation phases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AgentFL: Scaling LLM-based fault localization to project-level context</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AgentFL</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Multi-agent fault localization framework. The four agents (test code reviewer, source code reviewer, software architect, software test engineer) collaborate to comprehend failures, navigate large code repositories using specialized tools, and confirm fault locations. Each agent is customized with tools and expertise for its subtask.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>test code reviewer (analyzes failing tests), source code reviewer (inspects source code), software architect (high-level program structure reasoning), software test engineer (test execution & replication)</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>fault comprehension, repository navigation, and fault confirmation (fault localization phase in debugging)</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>sequential decomposition into three phases with role-based responsibilities and exchanges of intermediate analysis artifacts</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>natural language summaries and structured intermediate outputs; agents share summarized findings (artifacts) for subsequent agents to consume</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>observations and analysis results from reviewers are fed to subsequent agents; tool outputs (e.g., call traces) inform next steps</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>phase-based, iterative across localization steps (multi-turn until fault confirmed)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>fault localization in software debugging (project-level codebases)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>survey does not list numeric performance for AgentFL in text excerpt; emphasis is on scaling to project-level contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>positioned as a scaling improvement over single-agent LLM localization approaches; no numeric baseline reported in survey extract.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>specialization permits handling large project context and complex failure traces; improves navigation and comprehension vs single-agent approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>requires careful tool integration per agent and consistent artifact formats for communication; context length constraints remain an issue.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>partitioning of responsibilities into reviewers and architect/test-engineer roles with tailored toolsets recommended for project-level localization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model-Based Agents for Software Engineering: A Survey', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2461.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2461.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RCAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RCAgent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent system for root cause analysis in industrial cloud settings that uses a controller agent to orchestrate expert agents (e.g., log and code analysis), a key-value store for memory/observations, tool invocation for data retrieval, and a self-consistency mechanism for robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>RCAgent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Controller-expert architecture: a controller agent manages the thought-action-observation loop, invokes expert agents for specialized analysis tasks (code analysis, log analysis), and uses a key-value store to remember observations. Expert outputs are summarized and fed back as observations to the controller for decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>controller + multiple expert agents (variable)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>controller agent (orchestration and overall cycle oversight), expert agents (log analysis, code analysis, retrieval), memory module (key-value store) acts as shared state</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>diagnosis and root-cause analysis (incident investigation) — analogous to execution/analysis phases</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>centralized controller that invokes expert agents as needed (star-like / controller-centered orchestration)</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>structured memory via key-value store for observations and summaries; natural language prompts/instructions to expert agents; summarized outputs returned as observations</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>expert summaries returned to controller as observations; controller updates memory and decision-making; self-consistency checks across agent outputs improve robustness</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>on-demand (controller invokes experts when needed during the thought-action-observation cycle)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>root-cause analysis in cloud/industrial systems (software/system reliability)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>survey text does not provide numeric performance metrics for RCAgent in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>not reported in survey excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>modular expert invocation reduces burden on any single agent and enables specialized analysis; key-value memory helps manage large observations and context length constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>requires reliable memory management and consistent summary formats; controller must effectively decide when and which experts to invoke.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>centralized controller + compact key-value memory with specialized expert agents recommended for large observation spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model-Based Agents for Software Engineering: A Survey', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2461.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2461.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPTLENS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPTLENS</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adversarial-synergic multi-agent framework for smart contract vulnerability detection using multiple auditor agents that propose vulnerabilities and a critic agent that scores candidates, achieving marked improvements in identification rate on a set of real CVEs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPTLENS</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Two-stage adversarial-synergic design: several auditor agents independently generate potential vulnerabilities and detailed reasoning; a critic agent scrutinizes and scores the candidates according to specific criteria; top candidates are selected for further analysis. All agents use GPT-4 as backend in reported work.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>multiple auditors + 1 critic (variable number of auditor agents)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>auditor agents (generate candidate vulnerabilities and reasoning exhaustively), critic agent (scrutinize, score, and rank candidates against criteria)</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>vulnerability generation and vetting (static security analysis / vulnerability detection)</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>parallel generation by auditors followed by centralized critique and scoring by critic (parallel-then-centralized pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>natural-language generation from auditors; critic receives auditor outputs and returns scores/decisions (structured scoring criteria internally used)</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>critic agent scores auditor outputs and rejects/accepts candidates; critic feedback drives selection for final vulnerability list</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>auditors generate once or in rounds; critic evaluates after generation rounds (iteration possible but not specified in survey text)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>smart contract vulnerability detection (security)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>evaluated on 13 real-world smart contract CVEs; reported improvement up to 76.9% in identification rate (survey text)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>compared to single-agent or baseline auditors; shows up to 76.9% improvement on the CVE set (survey text provides this numeric improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>parallel auditor diversity increases coverage of potential vulnerabilities; critic reduces false positives and focuses on higher-quality candidates—substantial identification improvements reported.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>auditor redundancy and potential for many low-quality candidates; requires robust critic scoring criteria to avoid noise.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>using multiple diverse auditor agents with a strong critic for scoring recommended to balance coverage and precision.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model-Based Agents for Software Engineering: A Survey', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2461.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2461.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FixAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FixAgent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent unified debugging system that integrates specialized agents (localizer, repairer, crafter, revisitor) to perform end-to-end fault localization, repair, and analysis; reported very high repair success on QuixBugs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>FixAgent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Unified debugging multi-agent system leveraging agent specialization and synergy: LLM localizer (identifies suspicious elements), LLM repairer (produces patches), LLM crafter (constructs contextual edits), and LLM revisitor (tracks variables/context), integrating variable tracking and program context comprehension in an iterative loop between localization and repair.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>4 (localizer, repairer, crafter, revisitor)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>localizer (fault localization), repairer (patch generation), crafter (crafts patch edits and context), revisitor (re-evaluates and tracks context/variables post-repair)</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>fault localization, program repair, bug analysis (unified debugging end-to-end)</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>specialized-agent pipeline with iterative loops between localization and repair phases (hierarchical/sequential with feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>natural language explanations/requests plus structured tracking of key variables (artifacts passed between agents as contextual prompts and variable-tracking records)</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>repair results feed back into localizer for re-localization; revisitor monitors variable behavior and directs further repair iterations (iterative refinement loop)</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>iterative per-repair cycle (after each repair attempt results are fed back for re-localization/refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>debugging (unified fault localization + program repair) for small-benchmark programs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>claimed to fix 79 out of 80 bugs in the QuixBugs benchmark (survey text)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>compared to previous approaches on QuixBugs; high success rate (79/80) indicates strong performance versus prior systems (exact baselines not enumerated in survey excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>specialized roles and iterative feedback between localization and repair enable near-complete fixes on QuixBugs; variable tracking and context comprehension increases repair correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>requires careful orchestration of multiple agents and high-quality context passing; may face scaling challenges for large projects.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>specialized localizer + repairer + revisitor roles with iterative feedback loops recommended for high-recall unified debugging.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model-Based Agents for Software Engineering: A Survey', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2461.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2461.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoSD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoSD</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent system that simulates scientific debugging: includes an LLM-based hypothesis generator, execution-based validator, LLM-based conclusion maker, and LLM-based fixer in iterative debugging cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoSD</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Multi-agent design emulating scientific debugging: hypothesis generator proposes causes, execution-based validator (tool-in-the-loop) tests hypotheses, conclusion maker determines hypothesis status (rejected/accepted), and fixer generates patches. Iterative loop continues until bug is resolved or rebooted.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>4 (hypothesis generator, validator, conclusion maker, fixer)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>hypothesis generator (proposes explanations), execution validator (uses debugger/tools to validate), conclusion maker (decides hypothesis validity), fixer (generates candidate patches and explanations)</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>debugging (hypothesis generation, validation, repair) — scientific-debugging workflow</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>circular/iterative loop where generator -> validator -> conclusion maker -> fixer form a closed cycle (circular structure with feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>natural language for hypotheses and conclusions; tool outputs (execution traces, validation results) are incorporated as structured observations into subsequent prompts</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>execution-based validator provides runtime feedback to the conclusion maker which affects subsequent hypotheses and fixes; iterative refinement based on validation outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>per-iteration (each hypothesis run triggers validation and potential new hypothesis/fix generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>program repair / scientific debugging (software debugging tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 9 in survey lists AutoSD's results: 189/835 (Defects4J) and 187/200 (HumanEval) — these numbers reported in the survey table for AutoSD.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>compared to other repair systems in Table 9 (exact baselines vary across datasets); AutoSD shows reported fix counts on Defects4J and HumanEval benchmarks as above.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>structured scientific-debugging loop leverages execution feedback and explicit hypothesis testing to improve repair reliability; reported competitive fix counts on benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>high cost due to repeated executions and validator invocations; dependency on reproducible test harnesses and adequate validation coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>iterative hypothesis-validation-fix cycles with explicit execution-based validators recommended for higher-fidelity repair.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model-Based Agents for Software Engineering: A Survey', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2461.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2461.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ACFIX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ACFIX</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A role-specialized multi-agent system for fixing role-based access control (RBAC) vulnerabilities in smart contracts, including identification, patch generation, and multi-modal validation (static checks + multi-agent debate).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ACFIX: guiding LLMs with mined common RBAC practices for context-aware repair of access control vulnerabilities in smart contracts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ACFIX</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Specialized pipeline for access control vulnerability repair: RBAC mechanism identifier finds roles; role-permission pair identifier extracts relevant pairs; patch generator produces fixes; validator checks patches via static grammar rules and via model-feedback multi-agent debate. Iterative refinement integrates validator feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>multiple specialized agents (identifier(s), generator, validator, debaters) — counts variable by role</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>RBAC identifier (detects access-control constructs), role-permission identifier, patch generator (produces candidate fixes), validator (static tool checks + multi-agent debate for semantic correctness)</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>vulnerability localization, patch generation, patch validation (security repair)</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>role-specialized sequential pipeline with validator-in-the-loop and multi-agent debate for semantic verification (sequential plus circular refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>natural language for agent outputs plus structured patch artifacts; validator uses tool feedback and the debate is natural-language based with scoring/consensus.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>static-check feedback and multi-agent debate provide corrective signals to generator for patch refinement; iterative until validator accepts patch</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>on-demand per patch generation/validation iteration</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>smart contract security (access control vulnerability repair)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 9 reports ACFIX fixed 112/118 on a self-curated smart-contract dataset (survey text).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>compared to baseline repair methods on the same self-curated dataset; ACFIX demonstrates high fix rate (112/118) though specific baselines are not enumerated in the survey excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>specialization and combined static+model-feedback debate yields strong patch validity and high fix rate for the targeted RBAC vulnerability class.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>reliance on mined RBAC patterns and the quality of debate scoring; potential for overfitting to RBAC categories in dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>using both static checks and multi-agent debate as validator components recommended to maximize patch correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model-Based Agents for Software Engineering: A Survey', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2461.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2461.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DroidAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DroidAgent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent framework for mobile GUI testing composed of planner, actor, observer, and reflector agents supported by memory modules, enabling long-term planning and coordinated interaction with external tools and virtual devices.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DroidAgent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Four-agent mobile-app testing architecture: planner (sets test tasks/schedules), actor (executes actions on GUI), observer (collects runtime/UI observations), reflector (summarizes/refines testing strategy). Memory modules store historic actions/states to support long-term planning and re-use.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>planner (task planning), actor (executes GUI interactions), observer (perceives GUI states and execution outcomes), reflector (reflects on results and refines future actions)</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>system testing (GUI testing) — planning, execution, observation, reflection</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>iterative loop with nested inner loops (actor-observer inner loop) supervised by planner-reflector outer loop (circular/iterative structure)</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>natural language prompts for high-level intentions; memory modules store embeddings/summaries; agents subscribe to needed memory artifacts (publish-subscribe style)</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>observer returns execution observations to planner and reflector; reflector synthesizes lessons and updates planner strategies for subsequent iterations</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>continuous/iterative during test generation and execution (actor/observer exchange every action; planner/reflector update between rounds)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>mobile application GUI testing</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>survey describes architecture and capabilities (long-term memory improves reasoning) but does not report numeric performance metrics in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>described as addressing autonomy and coherence gaps in prior LLM-driven GUI testing; no numeric baselines in excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>division into observation and reflection roles increases long-term planning and test coherence; memory modules improve reuse of testing knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>perception noise from GUI images, need for robust visual recognition, and complexity of memory management for long test sessions.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>planner+actor+observer+reflector with dedicated memory modules and nested loop coordination recommended for robust GUI testing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model-Based Agents for Software Engineering: A Survey', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2461.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e2461.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KernelGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KernelGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based agent for kernel fuzzing that generates driver syscall specifications by iteratively completing specification components, invoking a code extractor and external fuzzing tools (Syzkaller) and using feedback to correct syscall specs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>KernelGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Analysis agent identifies drivers and incrementally constructs syscall specifications; a code extractor (LLVM toolchain) parses kernel code to provide source context; the agent invokes Syzkaller (syz-extract) and receives feedback messages to validate and iteratively correct generated syscall specifications.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>2 logical roles (analysis agent + code extractor/toolchain) — implemented as agent + external tools</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>analysis agent (generates syscall specifications), code extractor (parses repository to provide code-level context), fuzzing tool (Syzkaller) provides feedback</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>system testing (kernel fuzzing): specification generation, validation via fuzzing, iterative correction</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>agent-tool loop where the analysis agent uses code extractor inputs and receives feedback from fuzzing tool; iterative refinement</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>structured specification artifacts (syscall specs) and tool error/feedback messages incorporated into prompts</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Syzkaller execution/feedback used to validate and correct syscall specs iteratively</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>generate-spec -> extract-code -> fuzz -> receive feedback -> refine (per-specification iterative loop)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>operating system kernel fuzzing / system testing</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>survey text does not provide numeric performance metrics for KernelGPT in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>not provided in survey excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>combining LLM reasoning with static code extraction and a fuzzing tool allows practical syscall specification generation and validation for kernel testing.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>complexity of kernel codebases and need for precise context for valid syscall specification; dependency on toolchain parsing quality.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>tight integration with code-extraction toolchain and fuzzing feedback loop recommended for correct syscall specification generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model-Based Agents for Software Engineering: A Survey', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2461.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e2461.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fuzz4All</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fuzz4All</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A universal LLM-based fuzzer composed of two agents — a distillation LLM for user input distillation and an LLM for input generation — designed to perform general and targeted fuzzing across multiple programming languages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fuzz4All: Universal fuzzing with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Fuzz4All</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Two-agent system: (1) distillation LLM condenses user goals and produces distilled prompts/exemplars; (2) generation LLM creates fuzzing inputs (witness programs/test cases) guided by distilled prompts. The generation LLM references previously generated samples and adapts strategy to increase input diversity and coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>2</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>distillation agent (user intent distillation and prompt crafting), generation agent (fuzz input/program generation and adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>fuzz testing (input generation and mutation for software testing/security)</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>two-agent pipeline where distillation output seeds generation; generation consults previous samples and can update distilled strategy</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>natural language / structured distilled prompt templates passed from distillation agent to generation agent; generation returns samples/results</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>generation LLM reviews previously generated samples and adapts generation strategy (implicit self-feedback loop), plus possible external tester feedback not detailed in survey text</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>iterative: distill -> generate -> review -> adapt across fuzzing loop</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>universal fuzzing / software testing across languages</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>survey mentions design and two-agent split for cost-effectiveness but no explicit numerical evaluation results in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>positioned as first universal LLM-based fuzzer; no numeric baselines in excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>separation of distillation and generation roles improves cost-effectiveness and allows focused strategies for diverse fuzzing tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>requires careful distillation to avoid loss of critical constraints; adaptation depends on adequate sample diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>two-agent configuration (distillation + generation) recommended for broad fuzzing applicability and cost management.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model-Based Agents for Software Engineering: A Survey', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2461.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e2461.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAGIS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MAGIS</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent framework for GitHub issue resolution that uses a manager agent to decompose issues into file-level tasks, assigns development teams, and includes a quality-assurance step (code review) for generated patches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MAGIS: llm-based multi-agent framework for github issue resolution</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MAGIS</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Manager-agent-centric multi-agent system for end-to-end issue resolution: manager breaks an issue into file-level tasks and spawns development teams; specialized agents perform navigation-based retrieval, patch generation, and QA review; final patches pass through a QA agent for verification.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (manager + spawned dev-team agents + QA agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>manager (task decomposition & assignment), developer agents (patch generation per file/task), repository custodian/retrieval agents (context collection), QA/quality-assurance agent (code review and verification)</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>issue preprocessing, localization, task decomposition, patch generation, patch verification (end-to-end maintenance pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>hierarchical manager-driven decomposition and spawn pattern (manager spawns teams and delegates tasks — hierarchical/tree-like structure)</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>natural language task descriptions and structured artifacts (file-level task lists, patch candidates) passed between manager and agents; shared repository context used for retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>QA agent reviews generated patches and returns critiques for developers to refine; ranking or acceptance decisions guide final selection</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>manager-centric on-demand communications during decomposition and iterative refinement loops between developer agents and QA agent</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>end-to-end software maintenance (issue resolution on GitHub repositories)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>survey notes MAGIS uses retrieval-based localization and includes code review verification, but specific numeric performance metrics not presented in excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>not specified in survey excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>file-level task decomposition and team spawning enables parallelism and modular handling of complex issues; QA agent reduces incorrect patches.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>task delegation granularity and synchronization overhead; ensuring sufficient cross-file context for correct edits.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>manager-driven hierarchical decomposition with per-file developer teams and QA review recommended for repository-scale issue resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model-Based Agents for Software Engineering: A Survey', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2461.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e2461.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MASAI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MASAI</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent system for end-to-end software maintenance that includes preprocessing, a test-template generator for issue reproduction, navigation-based localization, ranker agent for patch ranking, and both static/dynamic checking for verification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MASAI</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Maintenance-focused multi-agent system: preprocessing phases build test templates; an edit localizer navigates the repo to find related snippets; patch generation agents produce candidate fixes; ranker agent ranks candidates; static and dynamic checks filter/validate patches. Designed to improve reproduction and verification success.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>multiple (manager, test-template generator, edit localizer, patch generators, ranker, verifier) — variable</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>test-template generator (prepares reproduction test templates), edit localizer (navigation/localization), patch generator(s), ranker agent (ranks candidate patches), verifier(s) (static/dynamic checking)</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>preprocessing, issue reproduction, localization, patch generation, patch verification, patch ranking (end-to-end maintenance)</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>pipeline with parallel branches (generation + reproduction) that converge at ranking; manager selects workflows</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>natural language instructions and structured test templates and patch artifacts; retrieval APIs provide cross-file contexts</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>verification results (static/dynamic) feed back to ranker and patch generators for refinement; ranked outputs guide selection</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>iterative, especially in reproduction and verification loops; ranker invoked after candidate generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>end-to-end software maintenance and issue resolution</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>survey references MASAI's architecture; no numeric performance values are provided in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>not detailed in the survey excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>two-stage reproduction with test templates improves issue reproduction success; ranking reduces human inspection burden.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>reproducibility of test environments and generation of valid reproduction tests; ranking accuracy depends on quality of verification tests.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>two-stage issue reproduction (test-template + reproduction) and an explicit ranker agent recommended to improve maintenance throughput.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model-Based Agents for Software Engineering: A Survey', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2461.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e2461.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agentless</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agentless</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A minimalistic, less-agentic approach that structures a repository as a hierarchical tree and uses simpler workflows (non-multi-agent) which, per the survey, can outperform more complex autonomous multi-agent systems on some maintenance benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Agentless: Demystifying LLM-based software engineering agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Agentless</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Simpler workflow approach (not heavy multi-agent): the repository is represented as a tree-like structure for navigation; an LLM-driven process uses this structured view to localize issues and generate patches with verification loops. Emphasizes simpler, SE-domain-informed pipelines over many-agent autonomy.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>single or minimal (agentless implies single LLM-driven pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>repository-structuring module (creates hierarchical tree), single agent (performs localization, generation, verification using structured views and prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>repository understanding, issue localization, patch generation, verification (maintenance pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>not multi-agent — single LLM process using repository-structured context; effectively centralized/simplified pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>structured repository tree representation plus natural language prompts to LLM</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>tool feedback (static/dynamic checks) and iterative LLM refinement loops; majority voting in patch ranking when multiple candidates produced</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>on-demand per phase; iterative refinement as needed</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>end-to-end software maintenance (issue resolution)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>survey notes Agentless (study [211]) and OpenAI [215] found that simplistic workflows based on traditional pipelines can outperform more complex autonomous agents in maintenance benchmarks (no specific numeric values provided in the excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>compared qualitatively to complex multi-agent autonomous systems; reported to outperform them on some benchmarks per referenced studies [211] and OpenAI [215].</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>simplicity reduces coordination overhead and may improve robustness and replicability; using SE domain knowledge can beat heavy multi-agent approaches on some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>may lack flexibility and parallelism benefits from multi-agent specialization; single-agent context-length constraints still apply.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>leveraging SE domain pipelines (traditional fault localization + repair loop) with structured repository views appears optimal for many maintenance tasks, per survey discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model-Based Agents for Software Engineering: A Survey', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2461.16">
                <h3 class="extraction-instance">Extracted Data Instance 16 (e2461.16)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetaGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MetaGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent collaborative framework that simulates software development teams using predefined roles, a shared message pool (structured communication), and standardized SOPs to produce artifacts across the development pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Metagpt: Meta programming for a multi-agent collaborative framework</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MetaGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Simulates real-world development teams via role agents (e.g., product manager, architect, engineer). Uses a shared message pool (structured messages/artifacts) to store artifacts like requirement docs; adopts SOPs to standardize intermediate outputs and agent responsibilities to facilitate collaboration across waterfall-like development phases.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>multiple pre-defined role agents (variable by project: managers, designers, developers, QA, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>product manager (requirements), architect (design), engineers/developers (implementation), QA (testing), documentation agents (artifact production)</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>end-to-end software development (requirements, design, implementation, testing, documentation)</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>ordered pipeline with shared message pool (blackboard) and SOP-defined responsibilities (sequential/structured coordination)</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>structured message pool with artifacts (semi-structured JSON-like messages/documents) and natural language dialogs for negotiation/clarification</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>shared artifacts and role-based reviews; agents subscribe to messages relevant to their roles and provide feedback via updated artifacts stored in the shared pool</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>phase transitions and event-driven (agents subscribe/receive messages when relevant artifacts updated)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>end-to-end software development processes</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>survey describes MetaGPT architecture and use of structured communication; no numeric performance metrics provided in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>positioned as structured alternative to purely natural-language agent dialogues; comparisons not numerically described in survey excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>structured messages and SOPs reduce multi-turn distortion and improve traceability; shared pool supports role-specific retrieval and subscription.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>designing schema for shared artifacts and preventing drift/distortion over many turns; balancing structured artifacts with flexible expression.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>use structured shared-message pool (semi-structured artifacts) plus predefined roles/SOPs to improve collaboration fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model-Based Agents for Software Engineering: A Survey', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2461.17">
                <h3 class="extraction-instance">Extracted Data Instance 17 (e2461.17)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AgentVerse</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AgentVerse</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dynamic multi-agent facilitation framework that supports multi-agent collaboration, emergent behavior exploration, and expert recruitment to form agent groups for tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AgentVerse</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Platform/framework to facilitate multi-agent collaboration by recruiting agents with different expertise (expert recruitment stage), enabling dynamic formation of teams, and supporting study/exploration of emergent behaviors in agent interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>dynamic (team composition determined by recruitment stage)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>expert agents of various specializations recruited dynamically (e.g., planners, implementers, testers depending on task)</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>team formation and collaborative execution across SE tasks (planning, implementation, QA depending on recruited roles)</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>dynamic team formation via recruitment stage, then ordered/sequential task execution depending on assembled team (manager + worker structure possible)</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>natural language dialogues; recruitment specifications define roles and capabilities (structured descriptors for agents)</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>observer/planner agents evaluate team performance and may adjust recruitment or tasks; self-consistency or evaluation modules provide feedback</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>on-demand (recruit at start, then iterative communication during task execution)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>general multi-agent collaboration (applied to software engineering in cited works)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>survey describes framework capabilities; no numeric performance metrics provided in text excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>designed to enable exploration rather than provide a single optimized pipeline; comparisons not specified in excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>flexible team composition allows tailored allocation of expertise and study of emergent collaborative patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>dynamic recruitment requires reliable capability descriptions and matching; potential instability from emergent behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>use recruitment stage to form teams matching task granularity; include observer/planner to monitor and adapt teams.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model-Based Agents for Software Engineering: A Survey', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2461.18">
                <h3 class="extraction-instance">Extracted Data Instance 18 (e2461.18)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoAgents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoAgents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework for automatic agent generation where a planner and an agent observer collaborate to dynamically assemble agent teams and assign roles, enabling flexible role creation for task-specific teams.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autoagents: A framework for automatic agent generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoAgents</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Two meta agents — a planner and an observer — conduct a drafting stage to determine what roles are needed; the planner assigns or generates specific LLM agent roles and the observer assesses and reviews role relevance. Roles are represented in structured JSON including available tools and prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>meta-level 2 (planner & agent observer) plus dynamically spawned worker agents (variable)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>planner (decides roles/workflows), agent observer (assesses role quality), spawned worker agents (task-specific roles generated dynamically with structured descriptors)</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>team assembly and planning phases prior to execution; supports downstream execution by generated agents</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>meta-level dynamic role creation and assignment (planner/observer assemble team) leading to ordered execution pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>structured JSON descriptors for roles (name, description, tools, prompts) plus natural-language interactions between meta agents and spawned agents</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>observer reviews and critiques role assignments; planner revises team composition iteratively</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>dynamic during drafting/assembly phase; runtime communications follow assigned workflows</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>general multi-agent task orchestration (applied in SE contexts in cited works)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>survey describes design and role-creation capabilities; no quantitative performance metrics provided in excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>not provided in excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>dynamic role creation improves flexibility and cost-efficiency by assembling only needed roles for tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>requires robust role-description templates and reliable assessment by observer to avoid unsuitable role generation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>use planner + observer meta-agents with structured role descriptors (JSON) to generate concise, fit-for-purpose agent teams.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model-Based Agents for Software Engineering: A Survey', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2461.19">
                <h3 class="extraction-instance">Extracted Data Instance 19 (e2461.19)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>INTERVENOR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>INTERVENOR</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent teaching/learning framework where a teacher coder observes execution feedback and provides error explanations and bug-fixing plans to a student coder in iterative repair loops.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>INTERVENOR</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Multi-agent interactive repair framework: a teacher coder agent watches program execution results (tests/errors), explains errors, and prescribes bug-fixing plans; a student coder agent uses this guidance to regenerate or refine code in iterative loops.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>2 (teacher coder, student coder)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>teacher coder (observes execution, explains errors, proposes fixes), student coder (applies guidance to regenerate/refine code)</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>code repair and refinement (program repair/debugging)</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>dual-role generation-validation loop (teacher audits execution, student revises), circular iterative structure</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>natural language explanations and suggested fix plans exchanged between teacher and student agents; execution outputs (error messages) used as structured feedback</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>teacher uses execution feedback to craft explanations and plans; student uses that to produce new code; loop repeats until success</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>after each execution/validation step (per-iteration teacher feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>program repair and debugging</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>survey cites INTERVENOR as example of multi-agent interactive repair systems but no numeric metrics provided in excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>not provided in excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>teacher-student dynamic helps produce targeted fixes and clearer guidance than single-agent self-refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>quality of teacher explanations depends on accurate interpretation of execution traces; communication overhead doubles model invocations.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>teacher observing execution and producing explicit fix plans for student yields effective iterative repair loops.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model-Based Agents for Software Engineering: A Survey', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Elicitron: An LLM agent-based simulation framework for design requirements elicitation <em>(Rating: 2)</em></li>
                <li>Specgen: Automated generation of formal program specifications via large language models <em>(Rating: 2)</em></li>
                <li>MARE: multi-agents collaboration framework for requirements engineering <em>(Rating: 2)</em></li>
                <li>Agentfl: Scaling LLM-based fault localization to project-level context <em>(Rating: 2)</em></li>
                <li>ACFIX: guiding LLMs with mined common RBAC practices for context-aware repair of access control vulnerabilities in smart contracts <em>(Rating: 2)</em></li>
                <li>Metagpt: Meta programming for a multi-agent collaborative framework <em>(Rating: 2)</em></li>
                <li>Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents <em>(Rating: 2)</em></li>
                <li>Agentless: Demystifying LLM-based software engineering agents <em>(Rating: 2)</em></li>
                <li>Fuzz4All: Universal fuzzing with large language models <em>(Rating: 2)</em></li>
                <li>Autoagents: A framework for automatic agent generation <em>(Rating: 2)</em></li>
                <li>KernelGPT: Enhanced kernel fuzzing via large language models <em>(Rating: 1)</em></li>
                <li>DroidAgent / Autonomous large language model agents enabling intent-driven mobile gui testing <em>(Rating: 1)</em></li>
                <li>MAGIS: llm-based multi-agent framework for github issue resolution <em>(Rating: 2)</em></li>
                <li>MASAI: Modular architecture for software-engineering AI agents <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2461",
    "paper_id": "paper-272423732",
    "extraction_schema_id": "extraction-schema-65",
    "extracted_data": [
        {
            "name_short": "Elicitron",
            "name_full": "Elicitron",
            "brief_description": "A multi-agent framework for requirements elicitation that initializes multiple agents with different personas to simulate stakeholder interactions, document action/observation/challenge traces, and extract potential requirements via iterative agent interviews and filtering.",
            "citation_title": "Elicitron: An LLM agent-based simulation framework for design requirements elicitation",
            "mention_or_use": "use",
            "system_name": "Elicitron",
            "system_description": "Multi-agent framework for the requirements elicitation phase. It spawns multiple persona agents representing diverse user viewpoints which interact with a simulated product; each agent records steps (actions, observations, challenges) into a shared record; the system aggregates interview transcripts and filters candidate requirements according to criteria to produce elicited requirements.",
            "number_of_agents": "multiple (variable personas)",
            "agent_specializations": "stakeholder-persona agents (simulate users with different viewpoints); collector/recorder functionality (document actions/observations/challenges); filter/selector (applies criteria to candidate requirements).",
            "research_phases_covered": "requirements elicitation (requirements discovery and initial documentation)",
            "coordination_mechanism": "shared workspace / simulated interview loop where persona agents interact with the target product and with each other; aggregation step merges outputs.",
            "communication_protocol": "natural language dialogues among persona agents and structured records written to a shared workspace (action/observation/challenge entries)",
            "feedback_mechanism": "iterative agent interviews with internal filtering: agents document challenges and observations which are used to refine subsequent simulations and filter candidate requirements.",
            "communication_frequency": "multi-round interactive (agents simulate interactions iteratively until elicitation converges)",
            "task_domain": "requirements engineering within software engineering (elicitation)",
            "performance_metrics": "reported qualitative effectiveness (uncovers and categorizes hidden needs) and cost reductions vs conventional user studies (no detailed numeric metrics provided in survey text).",
            "baseline_comparison": "compared qualitatively to conventional methodologies such as user studies; reported reductions in cost and ability to discover hidden needs (no exact numeric baseline figures in survey).",
            "coordination_benefits": "diverse-perspective coverage (multiple personas improves completeness of elicited requirements) and reduced cost compared with traditional user studies.",
            "coordination_challenges": "potential for noisy/redundant outputs from many persona agents and need to define filtering criteria to remove spurious/low-quality requirements.",
            "ablation_studies": null,
            "optimal_configurations": "not specified; the system relies on designing diverse persona roles and criteria for filtering to balance coverage and noise.",
            "uuid": "e2461.0",
            "source_info": {
                "paper_title": "Large Language Model-Based Agents for Software Engineering: A Survey",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "SpecGen",
            "name_full": "SpecGen",
            "brief_description": "A two-stage system to generate formal requirement specifications (JML) for programs using an LLM-driven conversation stage followed by mutation-based generation and external verification with OpenJML, integrating verifier feedback into iterative prompt refinement.",
            "citation_title": "Specgen: Automated generation of formal program specifications via large language models",
            "mention_or_use": "use",
            "system_name": "SpecGen",
            "system_description": "Two-stage LLM-based pipeline: (1) conversation-driven specification generation where an agent drafts JML specification and invokes an external verifier (OpenJML) to validate; (2) mutation-based specification generation where failing specs are mutated and re-verified to produce diverse and correct specifications. Verifier feedback is integrated into prompts iteratively.",
            "number_of_agents": "single LLM-driven agent with iterative verifier interaction (effectively single-agent with tool loop)",
            "agent_specializations": "specification generator (LLM), external tool verifier (OpenJML) used as a feedback oracle",
            "research_phases_covered": "specification generation and validation (requirements/specification phase)",
            "coordination_mechanism": "iterative generator–tool loop: LLM generates spec, verifier returns validation results, LLM refines spec based on feedback",
            "communication_protocol": "structured prompts and verifier messages (tool feedback integrated into natural-language prompts); specifications expressed in JML (semi-structured formal language)",
            "feedback_mechanism": "tool feedback from OpenJML (validation errors) used to iteratively adjust prompts and mutate failing specs to produce more diverse/correct specifications",
            "communication_frequency": "on each generation/verification iteration (generate -&gt; verify -&gt; refine loop)",
            "task_domain": "software specification / requirements engineering (formal specification generation)",
            "performance_metrics": "experimental claim: SpecGen outperforms state-of-the-art approaches in the surveyed evaluation (no numerical details in survey extract).",
            "baseline_comparison": "compared against state-of-the-art specification generation approaches and shown to perform better (exact metrics not provided in the survey text).",
            "coordination_benefits": "integrating external verifier feedback reduces invalid or uncheckable specifications and improves quality/diversity via mutation stage.",
            "coordination_challenges": "dependence on verifier coverage and ability to represent verification failures usefully inside prompts; mutation management to avoid combinatorial explosion.",
            "ablation_studies": null,
            "optimal_configurations": "two-stage workflow (conversation-driven + mutation-based) with verifier-in-the-loop recommended by authors.",
            "uuid": "e2461.1",
            "source_info": {
                "paper_title": "Large Language Model-Based Agents for Software Engineering: A Survey",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "MARE",
            "name_full": "MARE",
            "brief_description": "A multi-agent collaboration framework that covers multiple requirements engineering phases (elicitation, modeling, verification, specification) via specialized agents communicating through a shared workspace using predefined actions.",
            "citation_title": "MARE: multi-agents collaboration framework for requirements engineering",
            "mention_or_use": "use",
            "system_name": "MARE",
            "system_description": "Multi-agent RE framework where stakeholder agents express needs, collector agent drafts requirements, modeler agent identifies entities/relationships and builds requirement models, checker agent assesses draft quality against criteria, and documenter agent writes final specifications or reports errors. Agents use a shared workspace to exchange intermediate results.",
            "number_of_agents": "multiple (role-based: stakeholder agents + collector + modeler + checker + documenter)",
            "agent_specializations": "stakeholder agents (express needs), collector agent (organizes inputs), modeler agent (entity/relationship extraction and requirement modeling), checker agent (quality assessment), documenter agent (writes specifications/reports).",
            "research_phases_covered": "elicitation, modeling, verification, specification (end-to-end RE pipeline)",
            "coordination_mechanism": "shared workspace (blackboard) where agents write/read structured messages and artifacts; sequential phase handovers (collector -&gt; modeler -&gt; checker -&gt; documenter)",
            "communication_protocol": "structured messages/artifacts in a shared workspace (semi-structured documents, JSON-like artifacts), plus natural-language prompts within agent steps",
            "feedback_mechanism": "checker agent provides quality assessments and error reports back to modeler/collector; iterative handoffs allow refinement before final documentation",
            "communication_frequency": "phase transitions plus on-demand within phases (sequential pipeline with iterative refinements per phase)",
            "task_domain": "requirements engineering for software engineering",
            "performance_metrics": "survey reports that MARE covers multiple RE phases and facilitates role simulation; no numeric performance metrics provided in survey extract.",
            "baseline_comparison": "qualitative comparison to single-agent approaches and manual processes; no numeric baselines in survey extract.",
            "coordination_benefits": "role specialization supports structured progression from elicitation to specification and enables verification/quality checks before documentation.",
            "coordination_challenges": "designing effective shared workspace artifacts and avoiding information distortion in multi-turn communications.",
            "ablation_studies": null,
            "optimal_configurations": "predefined role pipeline and shared workspace with structured message formats suggested to ensure traceability and exchange of intermediate artifacts.",
            "uuid": "e2461.2",
            "source_info": {
                "paper_title": "Large Language Model-Based Agents for Software Engineering: A Survey",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "CodeAgent",
            "name_full": "CodeAgent",
            "brief_description": "A multi-agent code review system that simulates a waterfall-like team with six distinct agent characters (user, CEO, CPO, CTO, coder, reviewer) across four pipeline stages (information sync, review, alignment, documentation) to perform code review tasks.",
            "citation_title": "Codeagent: Collaborative agents for software engineering",
            "mention_or_use": "use",
            "system_name": "CodeAgent",
            "system_description": "Simulates a multi-role code review team across four stages: basic information synchronization (CEO/CPO/coder analyze input), code review (coder & reviewer collaborate to produce analysis), code alignment (coder/reviewer iterate edits), and documentation (CEO/CPO/coder produce final artifacts). Designed to automate code review tasks including consistency, vulnerability, format, and revision.",
            "number_of_agents": "6",
            "agent_specializations": "CEO/CPO (high-level coordination), coder (implementation & edits), reviewer (code review/analysis), user (role to provide external perspective)",
            "research_phases_covered": "code review and post-review code alignment/documentation (quality assurance phase of SE)",
            "coordination_mechanism": "ordered multi-stage pipeline (waterfall-like within the review workflow) with role-specific responsibilities and handoffs between stages",
            "communication_protocol": "natural language dialogues among role agents and shared artifacts/reports passed to next stages (structured analysis reports produced)",
            "feedback_mechanism": "reviewer provides analysis reports to coder; coder revises code and re-enters review; higher-level agents check final documentation.",
            "communication_frequency": "structured stage-based communication (after each stage and during review iterations)",
            "task_domain": "code review (software quality assurance)",
            "performance_metrics": "survey reports CodeAgent demonstrates effectiveness and efficiency across consistency analysis, vulnerability analysis, format analysis, and code revision tasks; no precise numeric scores provided in survey extract.",
            "baseline_comparison": "compared qualitatively to manual code review processes; no numeric baselines in survey extract.",
            "coordination_benefits": "simulating multi-role real-world teams enables division of responsibility and more comprehensive reviews (vulnerabilities, style, correctness).",
            "coordination_challenges": "overhead of managing many role-specific agents; possible redundant or conflicting review suggestions requiring alignment phase.",
            "ablation_studies": null,
            "optimal_configurations": "waterfall-like staged review pipeline with explicit role definitions recommended for structured code-review automation.",
            "uuid": "e2461.3",
            "source_info": {
                "paper_title": "Large Language Model-Based Agents for Software Engineering: A Survey",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "AgentFL",
            "name_full": "AgentFL",
            "brief_description": "A multi-agent system for project-level fault localization that scales LLM-based localization by assigning four specialized agents (test code reviewer, source code reviewer, software architect, software test engineer), each with dedicated tools and expertise, to decompose the localization task into fault comprehension, codebase navigation, and fault confirmation phases.",
            "citation_title": "AgentFL: Scaling LLM-based fault localization to project-level context",
            "mention_or_use": "use",
            "system_name": "AgentFL",
            "system_description": "Multi-agent fault localization framework. The four agents (test code reviewer, source code reviewer, software architect, software test engineer) collaborate to comprehend failures, navigate large code repositories using specialized tools, and confirm fault locations. Each agent is customized with tools and expertise for its subtask.",
            "number_of_agents": "4",
            "agent_specializations": "test code reviewer (analyzes failing tests), source code reviewer (inspects source code), software architect (high-level program structure reasoning), software test engineer (test execution & replication)",
            "research_phases_covered": "fault comprehension, repository navigation, and fault confirmation (fault localization phase in debugging)",
            "coordination_mechanism": "sequential decomposition into three phases with role-based responsibilities and exchanges of intermediate analysis artifacts",
            "communication_protocol": "natural language summaries and structured intermediate outputs; agents share summarized findings (artifacts) for subsequent agents to consume",
            "feedback_mechanism": "observations and analysis results from reviewers are fed to subsequent agents; tool outputs (e.g., call traces) inform next steps",
            "communication_frequency": "phase-based, iterative across localization steps (multi-turn until fault confirmed)",
            "task_domain": "fault localization in software debugging (project-level codebases)",
            "performance_metrics": "survey does not list numeric performance for AgentFL in text excerpt; emphasis is on scaling to project-level contexts.",
            "baseline_comparison": "positioned as a scaling improvement over single-agent LLM localization approaches; no numeric baseline reported in survey extract.",
            "coordination_benefits": "specialization permits handling large project context and complex failure traces; improves navigation and comprehension vs single-agent approaches.",
            "coordination_challenges": "requires careful tool integration per agent and consistent artifact formats for communication; context length constraints remain an issue.",
            "ablation_studies": null,
            "optimal_configurations": "partitioning of responsibilities into reviewers and architect/test-engineer roles with tailored toolsets recommended for project-level localization.",
            "uuid": "e2461.4",
            "source_info": {
                "paper_title": "Large Language Model-Based Agents for Software Engineering: A Survey",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "RCAgent",
            "name_full": "RCAgent",
            "brief_description": "A multi-agent system for root cause analysis in industrial cloud settings that uses a controller agent to orchestrate expert agents (e.g., log and code analysis), a key-value store for memory/observations, tool invocation for data retrieval, and a self-consistency mechanism for robustness.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "RCAgent",
            "system_description": "Controller-expert architecture: a controller agent manages the thought-action-observation loop, invokes expert agents for specialized analysis tasks (code analysis, log analysis), and uses a key-value store to remember observations. Expert outputs are summarized and fed back as observations to the controller for decision-making.",
            "number_of_agents": "controller + multiple expert agents (variable)",
            "agent_specializations": "controller agent (orchestration and overall cycle oversight), expert agents (log analysis, code analysis, retrieval), memory module (key-value store) acts as shared state",
            "research_phases_covered": "diagnosis and root-cause analysis (incident investigation) — analogous to execution/analysis phases",
            "coordination_mechanism": "centralized controller that invokes expert agents as needed (star-like / controller-centered orchestration)",
            "communication_protocol": "structured memory via key-value store for observations and summaries; natural language prompts/instructions to expert agents; summarized outputs returned as observations",
            "feedback_mechanism": "expert summaries returned to controller as observations; controller updates memory and decision-making; self-consistency checks across agent outputs improve robustness",
            "communication_frequency": "on-demand (controller invokes experts when needed during the thought-action-observation cycle)",
            "task_domain": "root-cause analysis in cloud/industrial systems (software/system reliability)",
            "performance_metrics": "survey text does not provide numeric performance metrics for RCAgent in the excerpt.",
            "baseline_comparison": "not reported in survey excerpt.",
            "coordination_benefits": "modular expert invocation reduces burden on any single agent and enables specialized analysis; key-value memory helps manage large observations and context length constraints.",
            "coordination_challenges": "requires reliable memory management and consistent summary formats; controller must effectively decide when and which experts to invoke.",
            "ablation_studies": null,
            "optimal_configurations": "centralized controller + compact key-value memory with specialized expert agents recommended for large observation spaces.",
            "uuid": "e2461.5",
            "source_info": {
                "paper_title": "Large Language Model-Based Agents for Software Engineering: A Survey",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "GPTLENS",
            "name_full": "GPTLENS",
            "brief_description": "An adversarial-synergic multi-agent framework for smart contract vulnerability detection using multiple auditor agents that propose vulnerabilities and a critic agent that scores candidates, achieving marked improvements in identification rate on a set of real CVEs.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPTLENS",
            "system_description": "Two-stage adversarial-synergic design: several auditor agents independently generate potential vulnerabilities and detailed reasoning; a critic agent scrutinizes and scores the candidates according to specific criteria; top candidates are selected for further analysis. All agents use GPT-4 as backend in reported work.",
            "number_of_agents": "multiple auditors + 1 critic (variable number of auditor agents)",
            "agent_specializations": "auditor agents (generate candidate vulnerabilities and reasoning exhaustively), critic agent (scrutinize, score, and rank candidates against criteria)",
            "research_phases_covered": "vulnerability generation and vetting (static security analysis / vulnerability detection)",
            "coordination_mechanism": "parallel generation by auditors followed by centralized critique and scoring by critic (parallel-then-centralized pipeline)",
            "communication_protocol": "natural-language generation from auditors; critic receives auditor outputs and returns scores/decisions (structured scoring criteria internally used)",
            "feedback_mechanism": "critic agent scores auditor outputs and rejects/accepts candidates; critic feedback drives selection for final vulnerability list",
            "communication_frequency": "auditors generate once or in rounds; critic evaluates after generation rounds (iteration possible but not specified in survey text)",
            "task_domain": "smart contract vulnerability detection (security)",
            "performance_metrics": "evaluated on 13 real-world smart contract CVEs; reported improvement up to 76.9% in identification rate (survey text)",
            "baseline_comparison": "compared to single-agent or baseline auditors; shows up to 76.9% improvement on the CVE set (survey text provides this numeric improvement).",
            "coordination_benefits": "parallel auditor diversity increases coverage of potential vulnerabilities; critic reduces false positives and focuses on higher-quality candidates—substantial identification improvements reported.",
            "coordination_challenges": "auditor redundancy and potential for many low-quality candidates; requires robust critic scoring criteria to avoid noise.",
            "ablation_studies": null,
            "optimal_configurations": "using multiple diverse auditor agents with a strong critic for scoring recommended to balance coverage and precision.",
            "uuid": "e2461.6",
            "source_info": {
                "paper_title": "Large Language Model-Based Agents for Software Engineering: A Survey",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "FixAgent",
            "name_full": "FixAgent",
            "brief_description": "A multi-agent unified debugging system that integrates specialized agents (localizer, repairer, crafter, revisitor) to perform end-to-end fault localization, repair, and analysis; reported very high repair success on QuixBugs.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "FixAgent",
            "system_description": "Unified debugging multi-agent system leveraging agent specialization and synergy: LLM localizer (identifies suspicious elements), LLM repairer (produces patches), LLM crafter (constructs contextual edits), and LLM revisitor (tracks variables/context), integrating variable tracking and program context comprehension in an iterative loop between localization and repair.",
            "number_of_agents": "4 (localizer, repairer, crafter, revisitor)",
            "agent_specializations": "localizer (fault localization), repairer (patch generation), crafter (crafts patch edits and context), revisitor (re-evaluates and tracks context/variables post-repair)",
            "research_phases_covered": "fault localization, program repair, bug analysis (unified debugging end-to-end)",
            "coordination_mechanism": "specialized-agent pipeline with iterative loops between localization and repair phases (hierarchical/sequential with feedback)",
            "communication_protocol": "natural language explanations/requests plus structured tracking of key variables (artifacts passed between agents as contextual prompts and variable-tracking records)",
            "feedback_mechanism": "repair results feed back into localizer for re-localization; revisitor monitors variable behavior and directs further repair iterations (iterative refinement loop)",
            "communication_frequency": "iterative per-repair cycle (after each repair attempt results are fed back for re-localization/refinement)",
            "task_domain": "debugging (unified fault localization + program repair) for small-benchmark programs",
            "performance_metrics": "claimed to fix 79 out of 80 bugs in the QuixBugs benchmark (survey text)",
            "baseline_comparison": "compared to previous approaches on QuixBugs; high success rate (79/80) indicates strong performance versus prior systems (exact baselines not enumerated in survey excerpt).",
            "coordination_benefits": "specialized roles and iterative feedback between localization and repair enable near-complete fixes on QuixBugs; variable tracking and context comprehension increases repair correctness.",
            "coordination_challenges": "requires careful orchestration of multiple agents and high-quality context passing; may face scaling challenges for large projects.",
            "ablation_studies": null,
            "optimal_configurations": "specialized localizer + repairer + revisitor roles with iterative feedback loops recommended for high-recall unified debugging.",
            "uuid": "e2461.7",
            "source_info": {
                "paper_title": "Large Language Model-Based Agents for Software Engineering: A Survey",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "AutoSD",
            "name_full": "AutoSD",
            "brief_description": "A multi-agent system that simulates scientific debugging: includes an LLM-based hypothesis generator, execution-based validator, LLM-based conclusion maker, and LLM-based fixer in iterative debugging cycles.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "AutoSD",
            "system_description": "Multi-agent design emulating scientific debugging: hypothesis generator proposes causes, execution-based validator (tool-in-the-loop) tests hypotheses, conclusion maker determines hypothesis status (rejected/accepted), and fixer generates patches. Iterative loop continues until bug is resolved or rebooted.",
            "number_of_agents": "4 (hypothesis generator, validator, conclusion maker, fixer)",
            "agent_specializations": "hypothesis generator (proposes explanations), execution validator (uses debugger/tools to validate), conclusion maker (decides hypothesis validity), fixer (generates candidate patches and explanations)",
            "research_phases_covered": "debugging (hypothesis generation, validation, repair) — scientific-debugging workflow",
            "coordination_mechanism": "circular/iterative loop where generator -&gt; validator -&gt; conclusion maker -&gt; fixer form a closed cycle (circular structure with feedback)",
            "communication_protocol": "natural language for hypotheses and conclusions; tool outputs (execution traces, validation results) are incorporated as structured observations into subsequent prompts",
            "feedback_mechanism": "execution-based validator provides runtime feedback to the conclusion maker which affects subsequent hypotheses and fixes; iterative refinement based on validation outcomes",
            "communication_frequency": "per-iteration (each hypothesis run triggers validation and potential new hypothesis/fix generation)",
            "task_domain": "program repair / scientific debugging (software debugging tasks)",
            "performance_metrics": "Table 9 in survey lists AutoSD's results: 189/835 (Defects4J) and 187/200 (HumanEval) — these numbers reported in the survey table for AutoSD.",
            "baseline_comparison": "compared to other repair systems in Table 9 (exact baselines vary across datasets); AutoSD shows reported fix counts on Defects4J and HumanEval benchmarks as above.",
            "coordination_benefits": "structured scientific-debugging loop leverages execution feedback and explicit hypothesis testing to improve repair reliability; reported competitive fix counts on benchmarks.",
            "coordination_challenges": "high cost due to repeated executions and validator invocations; dependency on reproducible test harnesses and adequate validation coverage.",
            "ablation_studies": null,
            "optimal_configurations": "iterative hypothesis-validation-fix cycles with explicit execution-based validators recommended for higher-fidelity repair.",
            "uuid": "e2461.8",
            "source_info": {
                "paper_title": "Large Language Model-Based Agents for Software Engineering: A Survey",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "ACFIX",
            "name_full": "ACFIX",
            "brief_description": "A role-specialized multi-agent system for fixing role-based access control (RBAC) vulnerabilities in smart contracts, including identification, patch generation, and multi-modal validation (static checks + multi-agent debate).",
            "citation_title": "ACFIX: guiding LLMs with mined common RBAC practices for context-aware repair of access control vulnerabilities in smart contracts",
            "mention_or_use": "use",
            "system_name": "ACFIX",
            "system_description": "Specialized pipeline for access control vulnerability repair: RBAC mechanism identifier finds roles; role-permission pair identifier extracts relevant pairs; patch generator produces fixes; validator checks patches via static grammar rules and via model-feedback multi-agent debate. Iterative refinement integrates validator feedback.",
            "number_of_agents": "multiple specialized agents (identifier(s), generator, validator, debaters) — counts variable by role",
            "agent_specializations": "RBAC identifier (detects access-control constructs), role-permission identifier, patch generator (produces candidate fixes), validator (static tool checks + multi-agent debate for semantic correctness)",
            "research_phases_covered": "vulnerability localization, patch generation, patch validation (security repair)",
            "coordination_mechanism": "role-specialized sequential pipeline with validator-in-the-loop and multi-agent debate for semantic verification (sequential plus circular refinement)",
            "communication_protocol": "natural language for agent outputs plus structured patch artifacts; validator uses tool feedback and the debate is natural-language based with scoring/consensus.",
            "feedback_mechanism": "static-check feedback and multi-agent debate provide corrective signals to generator for patch refinement; iterative until validator accepts patch",
            "communication_frequency": "on-demand per patch generation/validation iteration",
            "task_domain": "smart contract security (access control vulnerability repair)",
            "performance_metrics": "Table 9 reports ACFIX fixed 112/118 on a self-curated smart-contract dataset (survey text).",
            "baseline_comparison": "compared to baseline repair methods on the same self-curated dataset; ACFIX demonstrates high fix rate (112/118) though specific baselines are not enumerated in the survey excerpt.",
            "coordination_benefits": "specialization and combined static+model-feedback debate yields strong patch validity and high fix rate for the targeted RBAC vulnerability class.",
            "coordination_challenges": "reliance on mined RBAC patterns and the quality of debate scoring; potential for overfitting to RBAC categories in dataset.",
            "ablation_studies": null,
            "optimal_configurations": "using both static checks and multi-agent debate as validator components recommended to maximize patch correctness.",
            "uuid": "e2461.9",
            "source_info": {
                "paper_title": "Large Language Model-Based Agents for Software Engineering: A Survey",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "DroidAgent",
            "name_full": "DroidAgent",
            "brief_description": "A multi-agent framework for mobile GUI testing composed of planner, actor, observer, and reflector agents supported by memory modules, enabling long-term planning and coordinated interaction with external tools and virtual devices.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "DroidAgent",
            "system_description": "Four-agent mobile-app testing architecture: planner (sets test tasks/schedules), actor (executes actions on GUI), observer (collects runtime/UI observations), reflector (summarizes/refines testing strategy). Memory modules store historic actions/states to support long-term planning and re-use.",
            "number_of_agents": "4",
            "agent_specializations": "planner (task planning), actor (executes GUI interactions), observer (perceives GUI states and execution outcomes), reflector (reflects on results and refines future actions)",
            "research_phases_covered": "system testing (GUI testing) — planning, execution, observation, reflection",
            "coordination_mechanism": "iterative loop with nested inner loops (actor-observer inner loop) supervised by planner-reflector outer loop (circular/iterative structure)",
            "communication_protocol": "natural language prompts for high-level intentions; memory modules store embeddings/summaries; agents subscribe to needed memory artifacts (publish-subscribe style)",
            "feedback_mechanism": "observer returns execution observations to planner and reflector; reflector synthesizes lessons and updates planner strategies for subsequent iterations",
            "communication_frequency": "continuous/iterative during test generation and execution (actor/observer exchange every action; planner/reflector update between rounds)",
            "task_domain": "mobile application GUI testing",
            "performance_metrics": "survey describes architecture and capabilities (long-term memory improves reasoning) but does not report numeric performance metrics in the excerpt.",
            "baseline_comparison": "described as addressing autonomy and coherence gaps in prior LLM-driven GUI testing; no numeric baselines in excerpt.",
            "coordination_benefits": "division into observation and reflection roles increases long-term planning and test coherence; memory modules improve reuse of testing knowledge.",
            "coordination_challenges": "perception noise from GUI images, need for robust visual recognition, and complexity of memory management for long test sessions.",
            "ablation_studies": null,
            "optimal_configurations": "planner+actor+observer+reflector with dedicated memory modules and nested loop coordination recommended for robust GUI testing.",
            "uuid": "e2461.10",
            "source_info": {
                "paper_title": "Large Language Model-Based Agents for Software Engineering: A Survey",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "KernelGPT",
            "name_full": "KernelGPT",
            "brief_description": "An LLM-based agent for kernel fuzzing that generates driver syscall specifications by iteratively completing specification components, invoking a code extractor and external fuzzing tools (Syzkaller) and using feedback to correct syscall specs.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "KernelGPT",
            "system_description": "Analysis agent identifies drivers and incrementally constructs syscall specifications; a code extractor (LLVM toolchain) parses kernel code to provide source context; the agent invokes Syzkaller (syz-extract) and receives feedback messages to validate and iteratively correct generated syscall specifications.",
            "number_of_agents": "2 logical roles (analysis agent + code extractor/toolchain) — implemented as agent + external tools",
            "agent_specializations": "analysis agent (generates syscall specifications), code extractor (parses repository to provide code-level context), fuzzing tool (Syzkaller) provides feedback",
            "research_phases_covered": "system testing (kernel fuzzing): specification generation, validation via fuzzing, iterative correction",
            "coordination_mechanism": "agent-tool loop where the analysis agent uses code extractor inputs and receives feedback from fuzzing tool; iterative refinement",
            "communication_protocol": "structured specification artifacts (syscall specs) and tool error/feedback messages incorporated into prompts",
            "feedback_mechanism": "Syzkaller execution/feedback used to validate and correct syscall specs iteratively",
            "communication_frequency": "generate-spec -&gt; extract-code -&gt; fuzz -&gt; receive feedback -&gt; refine (per-specification iterative loop)",
            "task_domain": "operating system kernel fuzzing / system testing",
            "performance_metrics": "survey text does not provide numeric performance metrics for KernelGPT in the excerpt.",
            "baseline_comparison": "not provided in survey excerpt.",
            "coordination_benefits": "combining LLM reasoning with static code extraction and a fuzzing tool allows practical syscall specification generation and validation for kernel testing.",
            "coordination_challenges": "complexity of kernel codebases and need for precise context for valid syscall specification; dependency on toolchain parsing quality.",
            "ablation_studies": null,
            "optimal_configurations": "tight integration with code-extraction toolchain and fuzzing feedback loop recommended for correct syscall specification generation.",
            "uuid": "e2461.11",
            "source_info": {
                "paper_title": "Large Language Model-Based Agents for Software Engineering: A Survey",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Fuzz4All",
            "name_full": "Fuzz4All",
            "brief_description": "A universal LLM-based fuzzer composed of two agents — a distillation LLM for user input distillation and an LLM for input generation — designed to perform general and targeted fuzzing across multiple programming languages.",
            "citation_title": "Fuzz4All: Universal fuzzing with large language models",
            "mention_or_use": "use",
            "system_name": "Fuzz4All",
            "system_description": "Two-agent system: (1) distillation LLM condenses user goals and produces distilled prompts/exemplars; (2) generation LLM creates fuzzing inputs (witness programs/test cases) guided by distilled prompts. The generation LLM references previously generated samples and adapts strategy to increase input diversity and coverage.",
            "number_of_agents": "2",
            "agent_specializations": "distillation agent (user intent distillation and prompt crafting), generation agent (fuzz input/program generation and adaptation)",
            "research_phases_covered": "fuzz testing (input generation and mutation for software testing/security)",
            "coordination_mechanism": "two-agent pipeline where distillation output seeds generation; generation consults previous samples and can update distilled strategy",
            "communication_protocol": "natural language / structured distilled prompt templates passed from distillation agent to generation agent; generation returns samples/results",
            "feedback_mechanism": "generation LLM reviews previously generated samples and adapts generation strategy (implicit self-feedback loop), plus possible external tester feedback not detailed in survey text",
            "communication_frequency": "iterative: distill -&gt; generate -&gt; review -&gt; adapt across fuzzing loop",
            "task_domain": "universal fuzzing / software testing across languages",
            "performance_metrics": "survey mentions design and two-agent split for cost-effectiveness but no explicit numerical evaluation results in the excerpt.",
            "baseline_comparison": "positioned as first universal LLM-based fuzzer; no numeric baselines in excerpt.",
            "coordination_benefits": "separation of distillation and generation roles improves cost-effectiveness and allows focused strategies for diverse fuzzing tasks.",
            "coordination_challenges": "requires careful distillation to avoid loss of critical constraints; adaptation depends on adequate sample diversity.",
            "ablation_studies": null,
            "optimal_configurations": "two-agent configuration (distillation + generation) recommended for broad fuzzing applicability and cost management.",
            "uuid": "e2461.12",
            "source_info": {
                "paper_title": "Large Language Model-Based Agents for Software Engineering: A Survey",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "MAGIS",
            "name_full": "MAGIS",
            "brief_description": "A multi-agent framework for GitHub issue resolution that uses a manager agent to decompose issues into file-level tasks, assigns development teams, and includes a quality-assurance step (code review) for generated patches.",
            "citation_title": "MAGIS: llm-based multi-agent framework for github issue resolution",
            "mention_or_use": "use",
            "system_name": "MAGIS",
            "system_description": "Manager-agent-centric multi-agent system for end-to-end issue resolution: manager breaks an issue into file-level tasks and spawns development teams; specialized agents perform navigation-based retrieval, patch generation, and QA review; final patches pass through a QA agent for verification.",
            "number_of_agents": "variable (manager + spawned dev-team agents + QA agent)",
            "agent_specializations": "manager (task decomposition & assignment), developer agents (patch generation per file/task), repository custodian/retrieval agents (context collection), QA/quality-assurance agent (code review and verification)",
            "research_phases_covered": "issue preprocessing, localization, task decomposition, patch generation, patch verification (end-to-end maintenance pipeline)",
            "coordination_mechanism": "hierarchical manager-driven decomposition and spawn pattern (manager spawns teams and delegates tasks — hierarchical/tree-like structure)",
            "communication_protocol": "natural language task descriptions and structured artifacts (file-level task lists, patch candidates) passed between manager and agents; shared repository context used for retrieval",
            "feedback_mechanism": "QA agent reviews generated patches and returns critiques for developers to refine; ranking or acceptance decisions guide final selection",
            "communication_frequency": "manager-centric on-demand communications during decomposition and iterative refinement loops between developer agents and QA agent",
            "task_domain": "end-to-end software maintenance (issue resolution on GitHub repositories)",
            "performance_metrics": "survey notes MAGIS uses retrieval-based localization and includes code review verification, but specific numeric performance metrics not presented in excerpt.",
            "baseline_comparison": "not specified in survey excerpt.",
            "coordination_benefits": "file-level task decomposition and team spawning enables parallelism and modular handling of complex issues; QA agent reduces incorrect patches.",
            "coordination_challenges": "task delegation granularity and synchronization overhead; ensuring sufficient cross-file context for correct edits.",
            "ablation_studies": null,
            "optimal_configurations": "manager-driven hierarchical decomposition with per-file developer teams and QA review recommended for repository-scale issue resolution.",
            "uuid": "e2461.13",
            "source_info": {
                "paper_title": "Large Language Model-Based Agents for Software Engineering: A Survey",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "MASAI",
            "name_full": "MASAI",
            "brief_description": "A multi-agent system for end-to-end software maintenance that includes preprocessing, a test-template generator for issue reproduction, navigation-based localization, ranker agent for patch ranking, and both static/dynamic checking for verification.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "MASAI",
            "system_description": "Maintenance-focused multi-agent system: preprocessing phases build test templates; an edit localizer navigates the repo to find related snippets; patch generation agents produce candidate fixes; ranker agent ranks candidates; static and dynamic checks filter/validate patches. Designed to improve reproduction and verification success.",
            "number_of_agents": "multiple (manager, test-template generator, edit localizer, patch generators, ranker, verifier) — variable",
            "agent_specializations": "test-template generator (prepares reproduction test templates), edit localizer (navigation/localization), patch generator(s), ranker agent (ranks candidate patches), verifier(s) (static/dynamic checking)",
            "research_phases_covered": "preprocessing, issue reproduction, localization, patch generation, patch verification, patch ranking (end-to-end maintenance)",
            "coordination_mechanism": "pipeline with parallel branches (generation + reproduction) that converge at ranking; manager selects workflows",
            "communication_protocol": "natural language instructions and structured test templates and patch artifacts; retrieval APIs provide cross-file contexts",
            "feedback_mechanism": "verification results (static/dynamic) feed back to ranker and patch generators for refinement; ranked outputs guide selection",
            "communication_frequency": "iterative, especially in reproduction and verification loops; ranker invoked after candidate generation",
            "task_domain": "end-to-end software maintenance and issue resolution",
            "performance_metrics": "survey references MASAI's architecture; no numeric performance values are provided in the excerpt.",
            "baseline_comparison": "not detailed in the survey excerpt.",
            "coordination_benefits": "two-stage reproduction with test templates improves issue reproduction success; ranking reduces human inspection burden.",
            "coordination_challenges": "reproducibility of test environments and generation of valid reproduction tests; ranking accuracy depends on quality of verification tests.",
            "ablation_studies": null,
            "optimal_configurations": "two-stage issue reproduction (test-template + reproduction) and an explicit ranker agent recommended to improve maintenance throughput.",
            "uuid": "e2461.14",
            "source_info": {
                "paper_title": "Large Language Model-Based Agents for Software Engineering: A Survey",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Agentless",
            "name_full": "Agentless",
            "brief_description": "A minimalistic, less-agentic approach that structures a repository as a hierarchical tree and uses simpler workflows (non-multi-agent) which, per the survey, can outperform more complex autonomous multi-agent systems on some maintenance benchmarks.",
            "citation_title": "Agentless: Demystifying LLM-based software engineering agents",
            "mention_or_use": "use",
            "system_name": "Agentless",
            "system_description": "Simpler workflow approach (not heavy multi-agent): the repository is represented as a tree-like structure for navigation; an LLM-driven process uses this structured view to localize issues and generate patches with verification loops. Emphasizes simpler, SE-domain-informed pipelines over many-agent autonomy.",
            "number_of_agents": "single or minimal (agentless implies single LLM-driven pipeline)",
            "agent_specializations": "repository-structuring module (creates hierarchical tree), single agent (performs localization, generation, verification using structured views and prompts)",
            "research_phases_covered": "repository understanding, issue localization, patch generation, verification (maintenance pipeline)",
            "coordination_mechanism": "not multi-agent — single LLM process using repository-structured context; effectively centralized/simplified pipeline",
            "communication_protocol": "structured repository tree representation plus natural language prompts to LLM",
            "feedback_mechanism": "tool feedback (static/dynamic checks) and iterative LLM refinement loops; majority voting in patch ranking when multiple candidates produced",
            "communication_frequency": "on-demand per phase; iterative refinement as needed",
            "task_domain": "end-to-end software maintenance (issue resolution)",
            "performance_metrics": "survey notes Agentless (study [211]) and OpenAI [215] found that simplistic workflows based on traditional pipelines can outperform more complex autonomous agents in maintenance benchmarks (no specific numeric values provided in the excerpt).",
            "baseline_comparison": "compared qualitatively to complex multi-agent autonomous systems; reported to outperform them on some benchmarks per referenced studies [211] and OpenAI [215].",
            "coordination_benefits": "simplicity reduces coordination overhead and may improve robustness and replicability; using SE domain knowledge can beat heavy multi-agent approaches on some tasks.",
            "coordination_challenges": "may lack flexibility and parallelism benefits from multi-agent specialization; single-agent context-length constraints still apply.",
            "ablation_studies": null,
            "optimal_configurations": "leveraging SE domain pipelines (traditional fault localization + repair loop) with structured repository views appears optimal for many maintenance tasks, per survey discussion.",
            "uuid": "e2461.15",
            "source_info": {
                "paper_title": "Large Language Model-Based Agents for Software Engineering: A Survey",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "MetaGPT",
            "name_full": "MetaGPT",
            "brief_description": "A multi-agent collaborative framework that simulates software development teams using predefined roles, a shared message pool (structured communication), and standardized SOPs to produce artifacts across the development pipeline.",
            "citation_title": "Metagpt: Meta programming for a multi-agent collaborative framework",
            "mention_or_use": "use",
            "system_name": "MetaGPT",
            "system_description": "Simulates real-world development teams via role agents (e.g., product manager, architect, engineer). Uses a shared message pool (structured messages/artifacts) to store artifacts like requirement docs; adopts SOPs to standardize intermediate outputs and agent responsibilities to facilitate collaboration across waterfall-like development phases.",
            "number_of_agents": "multiple pre-defined role agents (variable by project: managers, designers, developers, QA, etc.)",
            "agent_specializations": "product manager (requirements), architect (design), engineers/developers (implementation), QA (testing), documentation agents (artifact production)",
            "research_phases_covered": "end-to-end software development (requirements, design, implementation, testing, documentation)",
            "coordination_mechanism": "ordered pipeline with shared message pool (blackboard) and SOP-defined responsibilities (sequential/structured coordination)",
            "communication_protocol": "structured message pool with artifacts (semi-structured JSON-like messages/documents) and natural language dialogs for negotiation/clarification",
            "feedback_mechanism": "shared artifacts and role-based reviews; agents subscribe to messages relevant to their roles and provide feedback via updated artifacts stored in the shared pool",
            "communication_frequency": "phase transitions and event-driven (agents subscribe/receive messages when relevant artifacts updated)",
            "task_domain": "end-to-end software development processes",
            "performance_metrics": "survey describes MetaGPT architecture and use of structured communication; no numeric performance metrics provided in the excerpt.",
            "baseline_comparison": "positioned as structured alternative to purely natural-language agent dialogues; comparisons not numerically described in survey excerpt.",
            "coordination_benefits": "structured messages and SOPs reduce multi-turn distortion and improve traceability; shared pool supports role-specific retrieval and subscription.",
            "coordination_challenges": "designing schema for shared artifacts and preventing drift/distortion over many turns; balancing structured artifacts with flexible expression.",
            "ablation_studies": null,
            "optimal_configurations": "use structured shared-message pool (semi-structured artifacts) plus predefined roles/SOPs to improve collaboration fidelity.",
            "uuid": "e2461.16",
            "source_info": {
                "paper_title": "Large Language Model-Based Agents for Software Engineering: A Survey",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "AgentVerse",
            "name_full": "AgentVerse",
            "brief_description": "A dynamic multi-agent facilitation framework that supports multi-agent collaboration, emergent behavior exploration, and expert recruitment to form agent groups for tasks.",
            "citation_title": "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents",
            "mention_or_use": "use",
            "system_name": "AgentVerse",
            "system_description": "Platform/framework to facilitate multi-agent collaboration by recruiting agents with different expertise (expert recruitment stage), enabling dynamic formation of teams, and supporting study/exploration of emergent behaviors in agent interactions.",
            "number_of_agents": "dynamic (team composition determined by recruitment stage)",
            "agent_specializations": "expert agents of various specializations recruited dynamically (e.g., planners, implementers, testers depending on task)",
            "research_phases_covered": "team formation and collaborative execution across SE tasks (planning, implementation, QA depending on recruited roles)",
            "coordination_mechanism": "dynamic team formation via recruitment stage, then ordered/sequential task execution depending on assembled team (manager + worker structure possible)",
            "communication_protocol": "natural language dialogues; recruitment specifications define roles and capabilities (structured descriptors for agents)",
            "feedback_mechanism": "observer/planner agents evaluate team performance and may adjust recruitment or tasks; self-consistency or evaluation modules provide feedback",
            "communication_frequency": "on-demand (recruit at start, then iterative communication during task execution)",
            "task_domain": "general multi-agent collaboration (applied to software engineering in cited works)",
            "performance_metrics": "survey describes framework capabilities; no numeric performance metrics provided in text excerpt.",
            "baseline_comparison": "designed to enable exploration rather than provide a single optimized pipeline; comparisons not specified in excerpt.",
            "coordination_benefits": "flexible team composition allows tailored allocation of expertise and study of emergent collaborative patterns.",
            "coordination_challenges": "dynamic recruitment requires reliable capability descriptions and matching; potential instability from emergent behavior.",
            "ablation_studies": null,
            "optimal_configurations": "use recruitment stage to form teams matching task granularity; include observer/planner to monitor and adapt teams.",
            "uuid": "e2461.17",
            "source_info": {
                "paper_title": "Large Language Model-Based Agents for Software Engineering: A Survey",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "AutoAgents",
            "name_full": "AutoAgents",
            "brief_description": "A framework for automatic agent generation where a planner and an agent observer collaborate to dynamically assemble agent teams and assign roles, enabling flexible role creation for task-specific teams.",
            "citation_title": "Autoagents: A framework for automatic agent generation",
            "mention_or_use": "use",
            "system_name": "AutoAgents",
            "system_description": "Two meta agents — a planner and an observer — conduct a drafting stage to determine what roles are needed; the planner assigns or generates specific LLM agent roles and the observer assesses and reviews role relevance. Roles are represented in structured JSON including available tools and prompts.",
            "number_of_agents": "meta-level 2 (planner & agent observer) plus dynamically spawned worker agents (variable)",
            "agent_specializations": "planner (decides roles/workflows), agent observer (assesses role quality), spawned worker agents (task-specific roles generated dynamically with structured descriptors)",
            "research_phases_covered": "team assembly and planning phases prior to execution; supports downstream execution by generated agents",
            "coordination_mechanism": "meta-level dynamic role creation and assignment (planner/observer assemble team) leading to ordered execution pipeline",
            "communication_protocol": "structured JSON descriptors for roles (name, description, tools, prompts) plus natural-language interactions between meta agents and spawned agents",
            "feedback_mechanism": "observer reviews and critiques role assignments; planner revises team composition iteratively",
            "communication_frequency": "dynamic during drafting/assembly phase; runtime communications follow assigned workflows",
            "task_domain": "general multi-agent task orchestration (applied in SE contexts in cited works)",
            "performance_metrics": "survey describes design and role-creation capabilities; no quantitative performance metrics provided in excerpt.",
            "baseline_comparison": "not provided in excerpt.",
            "coordination_benefits": "dynamic role creation improves flexibility and cost-efficiency by assembling only needed roles for tasks.",
            "coordination_challenges": "requires robust role-description templates and reliable assessment by observer to avoid unsuitable role generation.",
            "ablation_studies": null,
            "optimal_configurations": "use planner + observer meta-agents with structured role descriptors (JSON) to generate concise, fit-for-purpose agent teams.",
            "uuid": "e2461.18",
            "source_info": {
                "paper_title": "Large Language Model-Based Agents for Software Engineering: A Survey",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "INTERVENOR",
            "name_full": "INTERVENOR",
            "brief_description": "A multi-agent teaching/learning framework where a teacher coder observes execution feedback and provides error explanations and bug-fixing plans to a student coder in iterative repair loops.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "INTERVENOR",
            "system_description": "Multi-agent interactive repair framework: a teacher coder agent watches program execution results (tests/errors), explains errors, and prescribes bug-fixing plans; a student coder agent uses this guidance to regenerate or refine code in iterative loops.",
            "number_of_agents": "2 (teacher coder, student coder)",
            "agent_specializations": "teacher coder (observes execution, explains errors, proposes fixes), student coder (applies guidance to regenerate/refine code)",
            "research_phases_covered": "code repair and refinement (program repair/debugging)",
            "coordination_mechanism": "dual-role generation-validation loop (teacher audits execution, student revises), circular iterative structure",
            "communication_protocol": "natural language explanations and suggested fix plans exchanged between teacher and student agents; execution outputs (error messages) used as structured feedback",
            "feedback_mechanism": "teacher uses execution feedback to craft explanations and plans; student uses that to produce new code; loop repeats until success",
            "communication_frequency": "after each execution/validation step (per-iteration teacher feedback)",
            "task_domain": "program repair and debugging",
            "performance_metrics": "survey cites INTERVENOR as example of multi-agent interactive repair systems but no numeric metrics provided in excerpt.",
            "baseline_comparison": "not provided in excerpt.",
            "coordination_benefits": "teacher-student dynamic helps produce targeted fixes and clearer guidance than single-agent self-refinement.",
            "coordination_challenges": "quality of teacher explanations depends on accurate interpretation of execution traces; communication overhead doubles model invocations.",
            "ablation_studies": null,
            "optimal_configurations": "teacher observing execution and producing explicit fix plans for student yields effective iterative repair loops.",
            "uuid": "e2461.19",
            "source_info": {
                "paper_title": "Large Language Model-Based Agents for Software Engineering: A Survey",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Elicitron: An LLM agent-based simulation framework for design requirements elicitation",
            "rating": 2,
            "sanitized_title": "elicitron_an_llm_agentbased_simulation_framework_for_design_requirements_elicitation"
        },
        {
            "paper_title": "Specgen: Automated generation of formal program specifications via large language models",
            "rating": 2,
            "sanitized_title": "specgen_automated_generation_of_formal_program_specifications_via_large_language_models"
        },
        {
            "paper_title": "MARE: multi-agents collaboration framework for requirements engineering",
            "rating": 2,
            "sanitized_title": "mare_multiagents_collaboration_framework_for_requirements_engineering"
        },
        {
            "paper_title": "Agentfl: Scaling LLM-based fault localization to project-level context",
            "rating": 2,
            "sanitized_title": "agentfl_scaling_llmbased_fault_localization_to_projectlevel_context"
        },
        {
            "paper_title": "ACFIX: guiding LLMs with mined common RBAC practices for context-aware repair of access control vulnerabilities in smart contracts",
            "rating": 2,
            "sanitized_title": "acfix_guiding_llms_with_mined_common_rbac_practices_for_contextaware_repair_of_access_control_vulnerabilities_in_smart_contracts"
        },
        {
            "paper_title": "Metagpt: Meta programming for a multi-agent collaborative framework",
            "rating": 2,
            "sanitized_title": "metagpt_meta_programming_for_a_multiagent_collaborative_framework"
        },
        {
            "paper_title": "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents",
            "rating": 2,
            "sanitized_title": "agentverse_facilitating_multiagent_collaboration_and_exploring_emergent_behaviors_in_agents"
        },
        {
            "paper_title": "Agentless: Demystifying LLM-based software engineering agents",
            "rating": 2,
            "sanitized_title": "agentless_demystifying_llmbased_software_engineering_agents"
        },
        {
            "paper_title": "Fuzz4All: Universal fuzzing with large language models",
            "rating": 2,
            "sanitized_title": "fuzz4all_universal_fuzzing_with_large_language_models"
        },
        {
            "paper_title": "Autoagents: A framework for automatic agent generation",
            "rating": 2,
            "sanitized_title": "autoagents_a_framework_for_automatic_agent_generation"
        },
        {
            "paper_title": "KernelGPT: Enhanced kernel fuzzing via large language models",
            "rating": 1,
            "sanitized_title": "kernelgpt_enhanced_kernel_fuzzing_via_large_language_models"
        },
        {
            "paper_title": "DroidAgent / Autonomous large language model agents enabling intent-driven mobile gui testing",
            "rating": 1,
            "sanitized_title": "droidagent_autonomous_large_language_model_agents_enabling_intentdriven_mobile_gui_testing"
        },
        {
            "paper_title": "MAGIS: llm-based multi-agent framework for github issue resolution",
            "rating": 2,
            "sanitized_title": "magis_llmbased_multiagent_framework_for_github_issue_resolution"
        },
        {
            "paper_title": "MASAI: Modular architecture for software-engineering AI agents",
            "rating": 2,
            "sanitized_title": "masai_modular_architecture_for_softwareengineering_ai_agents"
        }
    ],
    "cost": 0.039085499999999995,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Model-Based Agents for Software Engineering: A Survey
SEPTEMBER 2024</p>
<p>Junwei Liu 
Kaixin Wang 
Yixuan Chen 
Xin Peng 
Zhenpeng Chen 
Lingming Zhang 
Yiling Lou 
Large Language Model-Based Agents for Software Engineering: A Survey
SEPTEMBER 20249F82B4E6ECE7874264AE78D6510F2DD0arXiv:2409.02977v1[cs.SE]Large Language ModelAI AgentSoftware Engineering Memory § 5.1.2 Perception § 5.1.3 Action § 5.1.4 Multi-agent System § 5.2 Agent Roles § 5.2.1 Collaboration Mechanism § 5.2.2
The recent advance in Large Language Models (LLMs) has shaped a new paradigm of AI agents, i.e., LLM-based agents.Compared to standalone LLMs, LLM-based agents substantially extend the versatility and expertise of LLMs by enhancing LLMs with the capabilities of perceiving and utilizing external resources and tools.To date, LLM-based agents have been applied and shown remarkable effectiveness in Software Engineering (SE).The synergy between multiple agents and human interaction brings further promise in tackling complex real-world SE problems.In this work, we present a comprehensive and systematic survey on LLM-based agents for SE.We collect 106 papers and categorize them from two perspectives, i.e., the SE and agent perspectives.In addition, we discuss open challenges and future directions in this critical domain.The repository of this survey is at https://github.com/FudanSELab/Agent4SE-Paper-List.</p>
<p>INTRODUCTION</p>
<p>Large Language Models (LLMs) [1] have achieved remarkable progress and demonstrated potential of human-like intelligence.In recent years, LLMs have been widely applied in Software Engineering (SE).As shown by recent surveys [2], [3], LLMs have been adopted and shown promising performance in various software development and maintenance tasks, such as program generation [4]- [8], software testing [9]- [11] and debugging [12]- [17], and program improvement [18]- [20].</p>
<p>AI Agents are artificial entities that can autonomously perceive and act on surrounding environments so as to achieve specific goals [21].The concept of AI agents has been evolving for a long time (e.g., early agents are constructed on symbolic logic or reinforcement learning [22]- [25]).Recently, the remarkable progress in LLMs has further shaped a new paradigm of AI agents, i.e., LLM-based agents, which leverage LLMs as the central agent controller.Different from standalone LLMs, LLM-based agents extend the versatility and expertise of LLMs by equipping LLMs with the capabilities of perceiving and utilizing external resources and tools, which can tackle more complex real-world goals via collaboration between multiple agents or involvement of human interaction.</p>
<p>In this work, we present a comprehensive and systematic survey on LLM-based agents for SE.We collect 106 papers and categorize them from two perspectives, i.e., both the SE and agent perspectives.Additionally, we discuss the open challenges and future directions in this domain.</p>
<p>• J. Liu, K. Wang, Y. Chen, X. Peng, and Y. Lou are with the Department of Computer Science, Fudan University, China.E-mails: {jwliu24, kxwang23, yixuanchen23}@m.fudan.edu.cn,{pengxin, yilin-glou}@fudan.edu.cn• Z. Chen is with the School of Computer Science and Engineering, Nanyang Technological University, Singapore.E-mail: zhenpeng.chen@ntu.edu.sg• L. Zhang is with the Department of Computer Science, University of Illinois Urbana-Champaign, USA.E-mail: lingming@illinois.edu</p>
<p>From the SE perspective, we analyze how LLM-based agents are applied across different software development and improvement activities, including individual tasks (e.g., requirements engineering, code generation, static code checking, testing, and debugging) as well as the endto-end procedure of software development and improvement.From this perspective, we provide a comprehensive overview of how SE tasks are tackled by LLM-based agents.</p>
<p>From the agent perspective, we focus on the design of components in LLM-based agents for SE.Specifically, we analyze key components, including planning, memory, perception, and action, in these agents.Beyond basic agent construction, we also analyze multi-agent systems, including their agent roles, collaboration mechanisms, and humanagent collaboration.From this perspective, we summarize the characteristics of different components of LLM-based agents when applied to the SE domain.</p>
<p>In summary, this survey makes the following contributions:</p>
<p>• It provides the first comprehensive survey of 106 papers that apply LLM-based agents to SE. • It analyzes how existing LLM-based agents are designed and applied for software development and maintenance from both the SE and agent perspectives.</p>
<p>• It discusses research opportunities and future directions in this critical domain.Survey Structure.Figure 1 summarizes the structure of this survey.Section 2 introduces background knowledge, while Section 3 presents the methodology.Section 4 and Section 5 present the relevant work from the SE perspective and the agent perspective, respectively.Finally, Section 6 discusses the potential research opportunities.</p>
<p>BACKGROUND AND PRELIMINARY</p>
<p>In this section, we first introduce the background about the basic and advanced LLM-based agents, and then we discuss the related surveys.</p>
<p>Basic Framework of LLM-based Agents</p>
<p>LLM-based agents are typically composed of four key components: planning, memory, perception, and action [21].The planning and memory serve as the key components of the LLM-controlled brain, which interacts with the environment through the perception and action components to achieve specific goals.Figure 2 illustrates the basic framework of LLM-based agents.</p>
<p>Planning.The planning component decomposes complex tasks into multiple sub-tasks and schedules the subtasks to achieve final goals.In particular, agents can (i) generate a plan without adjustment by different reasoning strategies, or (ii) adjust a generated plan with the external feedback (e.g., environmental feedback or human feedback).</p>
<p>Memory.The memory component records the historical thoughts, actions, and environmental observations generated during the agent execution [21], [26], [27].Based on the accumulated memory, agents can revisit and utilize the previous records and experience, so as to tackle the complex tasks more effectively.The memory management (i.e., how to represent the memory) and utilization (i.e., how to read/write or retrieve the memory) are essential, which directly impact the efficiency and effectiveness of the agent system.</p>
<p>Perception.The perception component receives the information from the environment, which can facilitate better planning.In particular, agents can perceive multi-modal inputs, e.g., textual inputs, visual inputs, and auditory inputs.</p>
<p>Action.Based on the planning and decisions made by the brain, the action component conducts concrete actions to interact with and impact the environment.One essential mechanism in action is to control and utilize external tools, which can extend the inherent capabilities of LLMs by accessing more external resources and extending the action space beyond textual-alone interaction.</p>
<p>Fig. 2: Basic Framework of LLM-based Agents</p>
<p>Advanced LLM-based Agent Systems</p>
<p>Multi-agent Systems.While a single-agent system can be specialized to solve one certain task, enabling the collaboration between multiple agents (i.e., multi-agent systems) can further solve more complex tasks associated with diverse knowledge domains.In particular, in a multi-agent system, each agent is assigned a distinct role and relevant expertise, making it specifically responsible for different tasks; in addition, the agents can communicate with each other and share the progress/information as the task proceeds.Typically, agents can work collaboratively (i.e., by working on different sub-tasks to achieve a final goal) or competitively (i.e., by working on the same task while debating adversarially).</p>
<p>Human-Agent Coordination.Agent systems can further incorporate the instructions from humans and then proceed with tasks under human guidance.This human-agent coordination paradigm facilitates better alignment with human preference and uses human expertise.In particular, during human-agent interaction, humans can not only provide agents with task requirements and feedback on the current task status, but also cooperate with agents to achieve goals together.</p>
<p>Related Surveys</p>
<p>LLM-based agents in general domains have been widely discussed and surveyed [21], [26], [28]- [32].Different from these surveys, this survey focuses on the design and application of LLM-based agents specifically for the software engineering domain.In software engineering domain, there have been several surveys or literature reviews on the general application of LLMs in software engineering [2], [3], [10], [32], [33].Different from these surveys, this survey specifically focuses on the agent perspective and is more comprehensive on the application of LLM-based agents for software engineering.In addition, He et al. [34] present a vision paper on the potential applications and emerging challenges of multi-agent systems for software engineering.Different from the vision paper, this work focuses on conducting a comprehensive survey of existing agent systems (including both single agent and multi-agent systems).In summary, to the best of our knowledge, this is the first survey specifically focusing on the literature on LLM-based agents for software engineering.</p>
<p>SURVEY METHODOLOGY</p>
<p>This section defines the scope of the survey and describes our approach to collecting and analyzing papers within the scope.</p>
<p>Survey Scope</p>
<p>We focus on the papers that apply LLM-based agents to tackle SE tasks.In the following, we specify the terms.</p>
<p>• SE tasks.Following previous surveys on the application of LLMs in SE [2], [3], we focus on all SE tasks along the software life cycle, including requirements engineering, software design, code generation, software quality assurance (i.e., static checking and testing), and software improvement.</p>
<p>• LLM-based agents.A standalone LLM can work as a naive "agent" since it can take textual inputs and produce textual outputs, leaving it no clear boundary between LLMs and LLM-based agents.However, this could result in an overly broad scope and significant overlap with existing surveys on LLM applications in SE [2], [3].Based on the widely-adopted consensus about AI agents, the key characteristic of agents is their ability to autonomously and iteratively perceive feedback from, and act upon, a dynamic environment [21].To ensure a more focused discussion from the perspective of agents, this survey focuses on LLM-based agents that not only incorporate LLMs as the core of their "brains", but also have the capacity to iteratively interact with the environment, taking feedback and acting in real time.</p>
<p>More specifically, we apply the following inclusion and exclusion criteria for paper collection.</p>
<p>• Inclusion criteria.A paper will be included in our survey if it meets any of the following criteria: (i) The paper proposes a technique, framework, or tool for addressing specific SE tasks using LLM-based agents; (ii) The paper presents a general technique, framework, or tool applicable across various domains, provided that its evaluation includes at least one SE task; (iii) The paper presents an empirical study evaluating LLM-based agents on specific SE tasks.• Exclusion criteria.A paper will be excluded from our survey if it meets any of the following criteria: (i) The paper does not involve any SE tasks; (ii) The paper only discusses LLM-based agents in the context of discussion or future work, without integrating them into the main approach; (iii) The paper only uses a standalone LLM for processing textual inputs and generating textual outputs, without any iterative interaction with the environment.</p>
<p>Paper Collection</p>
<p>Our paper collection process includes two steps: keyword searching and snowballing.</p>
<p>Keyword Searching</p>
<p>We follow established practices in SE surveys [35]- [39] by using the DBLP database [40] for paper collection.Recent research [39] has demonstrated that papers gathered from other prominent publication databases are typically a subset of those available on DBLP, which encompasses over 7 million publications from more than 6,500 academic conferences and 1,850 journals in computer science [41].DBLP also covers arXiv [42], a widely adopted open-access repository.</p>
<p>We employ an iterative trial-and-error approach, which is widely adopted in SE surveys [35], [43], to determine search keywords.Initially, all authors, with relevant research experience/publication in LLM and SE, convene to suggest papers relevant to our scope, yielding an initial set of relevant papers.Subsequently, the first two authors review the titles, abstracts, and introductions of these papers to identify additional keywords.We then conduct brainstorming sessions to expand and refine our search strings, incorporating related terms, synonyms, and variations.This process enables iterative enhancement of our search keyword list.The final keywords include ("agent" OR "llm" OR "language model") AND ("api" OR "bug" OR "code" OR "coding" OR "debug" OR "defect" OR "deploy" OR "evolution" OR "fault" OR "fix" OR "maintenance" OR "program" OR "refactor" OR "repair" OR "requirement" OR "software" OR "test" OR "verification" OR "vulnerab").</p>
<p>Based on the keywords, we conduct 57 searches on DBLP on July 1st, 2024, and obtain 10,362 hits.Table 1 presents the statistics of papers collected through keyword searching.The first two authors manually review each paper to filter out those not within the scope of this survey.As a result, we identify 67 relevant papers through this process.</p>
<p>Snowballing</p>
<p>To enhance the comprehensiveness of our survey, we adopt snowballing approaches to identify papers that are transitively relevant and expand our paper collection [35].Specifically, between July 1 and July 10, 2024, we conduct both backward and forward snowballing.Backward snowballing involves examining references in each collected paper to identify relevant ones within our scope, while forward snowballing uses Google Scholar to find relevant papers citing the collected ones.This iterative process continues until no new relevant papers are found.In this process, we retrieve an additional 39 papers.</p>
<p>Statistics of Collected Papers</p>
<p>As shown in Table 1, we have collected a total of 106 papers for this survey.Figure 3 presents the cumulative number of papers published over time, up to July 10, 2024.We observe that there is a continuous increase of research interest in this field, highlighting the necessity and relevance of this survey.Additionally, Figure 4 shows the distribution of publication venues for the papers, covering diverse research communities such as software engineering, artificial intelligence, and human-computer interaction.In particular, the majority of the papers are from arXiv and have not yet undergone peer review.This is expected, as this field is emerging and still undergoing rapid development.</p>
<p>ANALYSIS FROM SE PERSPECTIVES</p>
<p>In this section, we organize the collected papers from the perspective of different SE tasks.Figure 5 presents the SE It is worth noting that, LLM-based agents can be designed not only to tackle individual SE tasks but also to support end-to-end software development or maintenance processes involving multiple SE activities.From the collected papers, we observe LLM-based agents designed for (i) end-to-end software development and (ii) end-toend software maintenance.Specifically, agents for end-to-end software development can generate a complete program based on requirements by performing multiple SE tasks, such as requirements engineering, design, code generation, and code quality assurance (e.g., verification, static checking, and testing); agents for end-to-end software maintenance can generate patches for user-reported issues by supporting multiple SE maintenance activities, such as debugging (e.g., fault localization and repair) and feature maintenance.As shown in previous papers [2], [3], standalone LLMs are primarily specialized in tackling single SE tasks and are generally inadequate for complex end-to-end software development and maintenance processes.In contrast, LLMbased agents, through their components (i.e., planning, memory, perception, and action), coordination among multiple agents, and human interaction, provide the autonomy Distribution of LLM-based agents in different SE activities.In Figure 5, the numbers in brackets indicate the count of collected papers in each category.Notably, if LLM-based agents are designed for end-to-end software development or maintenance, they are only reported at the end-to-end level rather than at the level of individual tasks.Overall, we observe that the majority of LLM-based agents focus on individual-level SE tasks, especially for code generation and code quality assurance (e.g., static checking and testing); in addition, a portion of agents are designed for end-toend software development or maintenance tasks, indicating the promise of LLM-based agents in tackling more complex real-world SE tasks.</p>
<p>Requirements Engineering</p>
<p>Requirements Engineering (RE) is a crucial phase for initializing the software development procedure.Generally, it can cover the following phases [44]- [46].</p>
<p>• Elicitation: New requirements are elicited and collected.</p>
<p>• Modeling: Abstract yet interpretable models are used to describe requirements, e.g., Unified Modeling Language (UML) [47] and Entity-Relationship-Attribute (ERA) model [48].In real-world software development, RE can take lots of manual efforts due to the strong demand for massive interactions with various stakeholders.Although researchers have leveraged deep learning models (including standalone LLMs) to boost requirements engineering activities, most of them still remain on individual tasks during RE, such as classification [49], specification [50], information retrieval [51], evaluation [52], and enhancement [53] of existing requirements.Recently, multi-agent systems are designed to automate individual phases or multiple phases.Table 2 summarizes existing LLM-based agents specifically designed for RE, and Figure 6 illustrates their common pipeline.</p>
<p>Fig. 6: Pipeline of LLM-based Agents for Requirements Engineering</p>
<p>A. LLM-based Agents for Individual RE Phases.Elicitation [54] is a multi-agent framework for the requirement elicitation phase, which aims at mining requirements as completely as possible.Elicitation first initializes multiple agents with different personas within the designed context to cover various user viewpoints, and then makes them simulate interactions with the target product while documenting records (i.e., action, observation, and challenge) in each step.The potential requirements are eventually identified through the agent interviews and filtered on the provided criteria.Experimental results indicate that Elicitation can uncover and categorize hidden needs while reducing costs compared with conventional methodologies such as user studies.</p>
<p>SpecGen [55] is a system designed for generating requirement specifications of a given program.It contains two stages: conversation-driven specification generation and mutation-based specification generation.In the first stage, an agent generates Java Modeling Language (JML) requirement specifications [58], which are further validated by the external OpenJML verifier [59].The feedback is integrated into the prompt and kicks off iterative generation.In the second stage, specifications that fail the validation are mutated and verified to generate more diverse specifications.Experimental results demonstrate that SpecGen outperforms state-of-the-art approaches.
✓ ✓ SpecGen [55] × ✓ Arora et al. [56] ✓ ✓ ✓ ✓ ✓ MARE [57] ✓ ✓ ✓ ✓ ✓
propose a multi-agent system to cover four phases of RE: elicitation, specification, analysis (synonymous with negotiation), and validation.For each phase, they explore the roles that can be simulated by LLM-based agents and systemically analyze the strengths and weaknesses.In the elicitation phase, agents play the role of stakeholders or requirement engineers and collect preliminary requirements, which are then formatted into structured documentation by another agent in the specification phase.Then, the analysis phase involves multiple stakeholder agents, aiming to evaluate, prioritize, and refine requirements.The validation phase marks the final validation by the stakeholder agents, and then the requirement documentation is decided and ready for subsequent design/implementation.MARE [57] is another multi-agent framework that covers multiple RE phases, including elicitation, modeling, verification, and specification.In the elicitation phase, a set of stakeholder agents express their needs, which would then be organized into a draft by the collector agent.Subsequently, the modeler agent identifies entities and relationships in the draft and constructs a requirement model; In the verification phase, the checker agent assesses the quality of the current requirements draft on its criteria and hands it over to the documenter agent, which will write the requirement specifications or report errors.All of these agents are equipped with predefined actions and can communicate within a shared workspace, enabling the seamless exchange of intermediate information.</p>
<p>Code Generation</p>
<p>Code generation has been extensively explored with the development of AI technology [33].Due to being pre-trained on massive textual data (especially large code corpus), LLMs demonstrate promising effectiveness in generating code for given code contexts or natural language descriptions.Nevertheless, the code generated by LLMs can sometimes be unsatisfactory due to issues such as the notorious hallucination [60].Therefore, beyond simply leveraging standalone LLMs for code generation, researchers also build LLMbased agents that can enhance the capabilities of LLMs via planning and iterative refinement.Figure 7 illustrates how existing LLMs extend standalone LLMs in code generation.</p>
<p>Code Generation with Planning</p>
<p>LLM-based agents employ advanced planning methods to extend the code generation capabilities of LLMs.Chainof-thought (CoT) [99] is the most popular strategy, which decomposes the code generation task into sub-tasks and achieves higher generation correctness [61], [70], [76], [84], [86], [91]- [93], [95].For example, CodeCoT [84] leverages CoT to break down the given requirements into steps that are described in natural language and then convert them to code.Some works employ dynamic planning strategies, where observations of the current action determine the next step [62], [85], [87], [88].For example, CodePlan [88] employs an adaptive planning algorithm that dynamically detects the affected code snippets in the repository and adapts the plan.However, there is still a gap between the narrative-based steps and the final generated code.Therefore, some works propose different representations to describe the planning steps, including pseudocode [95], intermediate code [61], or code skeleton [93], [97].For example, Agent-Coder [95] prompts the agent to generate pseudocode after problem understanding and algorithm selection phases, which serves as a draft for the final code.In addition, some existing works explore multi-path planning strategies.For example, LATS [76] simulates all possible generation paths as a tree and optimizes the plan with the Monte Carlo Tree Search algorithm.In MapCoder [98], the planning agent generates multiple plans along with confidence scores for sorting.The highest-scoring plan is used to generate the target code.If the code is erroneous, the plan with the next highest confidence is selected to continue the iterative generation process.</p>
<p>Code Generation with Iterative Refinement</p>
<p>One essential capability of agents is to act on the feedback from the environment.In the code generation scenario, some agents dynamically refine the previously-generated code based on the feedback via multiple iterations.We organize the relevant research based on the feedback sources, including model feedback, tool feedback, human feedback, and hybrid feedback.✓ Reflexion [62], Self-Repair [63], AutoGen [64],</p>
<p>INTERVENOR [65], TGen [66], AutoCoder [67] ✓ ✓ ✓ ✓ CAMEL [68], Li et al. [69], DyLAN [70] ✓ ✓ SELF-DEBUGGING [71], SEIDR [72], µF iX [73],</p>
<p>AlphaCodium [74], LDB [75], LATS [76], RRR [77] × ✓ ✓ ✓</p>
<p>ToolCoder [78], SELFEVOLVE [79], KPC [80],</p>
<p>LEMUR [81], CODEAGENT [82], LLM4TDD [83],</p>
<p>CodeCoT [84], CodeAct [85], CoCoST [86],</p>
<p>InterCode [87], CodePlan [88], TOOLGEN [89] × ✓
Self-Refine [90] × ✓ Flows [91] ✓ ✓ ✓ ✓ ✓ MINT [92] × ✓ ✓ ✓ CodeChain [93] × ✓ ✓ ClarifyGPT [94]
× ✓ ✓ AgentCoder [95], Gentopia [96],</p>
<p>SoA [97], MapCoder [98] ✓ ✓</p>
<p>A.1: Peer-reflection.Peer-reflection refers to information exchange and interaction between multiple models.The most common paradigm is collaboration in multi-agent systems through role specialization and structured communication (e.g., code review) [62], [64], [68].This approach underscores the specialized responsibilities of each role and how they exchange information based on these responsibilities.Besides, when generating initial code, some works produce multiple results at once [69].Thus, a selection mechanism is employed to retain the most suitable result.Li et al. [69] use Bilingual Evaluation Understudy (BLEU) to calculate and aggregate the similarity scores of each initial code with the rest, retaining the result with the highest score.Moreover, there is also a modality that involves treating each agent role equally in expressing their opinions or engaging in debates to solve problems.DyLAN [70] allows multiple agents to engage in dynamic interactions over multiple rounds, organized into a multi-layer feed-forward network.The network employs an additional LLM ranker to analyze the responses of agents from the previous layer and selects the bestperforming agent to continue in subsequent interactions.</p>
<p>A.2: Self-reflection.Apart from the interaction between models, there are works that conduct self-refinement of a single model [71], [73], [90].This means that the current modification is based on the previous output of the model, iteratively optimizing through this approach.Le et al. [93] guide LLMs to generate modularized code, leveraging cluster representatives from previously generated sub-modules in each iteration.SELF-DEBUGGING [71] draws inspiration from the rubber duck debugging method used by programmers.During the explanation phase, the model provides a line-by-line explanation of the generated initial code.As Wang et al. [92] mention in their work, all models benefit from natural language feedback, with absolute performance gains by 2-17% each additional turn of natural language feedback.</p>
<p>B: Tool Feedback.The code generated by models can be of limited quality with numerous uncertainties.One solution to address this challenge is to equip LLM-based agents with tools that can collect informative feedback and assist the agents to generate and refine code.</p>
<p>B.1: Dynamic Execution Tools.One common group is to invoke the compiler, interpreter, and execution engine to directly compile or execute the code.This approach leverages the outputs and run-time behaviors, such as test results or compilation errors, as feedback for code improvement [79], [81]- [87], [92]- [98].</p>
<p>B.2: Static Checking Tools.Agents can get more restricted knowledge on code constraints by applying code analysis tools.For example, some agents apply static analysis tools to obtain syntactically-valid program symbols/tokens [89] or dependencies between code during code generation [82], [88].Including the analyzed information into the prompt can guide LLMs towards generating valid code.</p>
<p>B.3: Retrieval Tools.Agents can get the access to rich external resources by applying retrieval or searching tools.For example, some agents retrieve local knowledge repositories [86] such as private API documentations [78], [82] to facilitate better code generation; in addition, some apply online search engines [78], [82], [86], [96] or web crawling [80] to collect information such as content from relevant websites (e.g., StackOverflow and datagy.io)[78], [82], [86], [96] and official online documentations [80], [86].Including the retrieved resources into the prompt can provide additional knowledge for language models.Notably, ToolCoder [78] integrates the agent with online search and local documentation search tools that provide helpful information for both public and private APIs, alleviating the hallucination of LLMs.</p>
<p>C: Human Feedback.Another approach involves incorporating human feedback into the process, as humans play a critical role in clarifying ambiguous requirements.For instance, in software development, humans can check whether the generated code aligns with their initial intent.</p>
<p>Any discrepancies are often attributed to vagueness or incompleteness in the requirements, prompting a revision of the requirement documents [91], [92].To further minimize human involvement, some methods enable the agent to handle the task of observing execution results.For example, ClarifyGPT [94] automatically identifies potential ambiguities in the manually-given requirements and proactively poses relevant questions for humans; then the responses from humans are further used to refine the requirements.</p>
<p>D: Hybrid Feedback.Agents can also incorporate multiple types of feedback and progressively enhance each other as hybrid feedback.For example, a common approach is to combine tool feedback and model feedback.Specifically, an LLM receives error messages returned after executing a program or test case, and utilizes its contextual understanding to provide corresponding feedback output (e.g., explanations, suggestions, instructions, etc.) [62]- [67], [71]- [77], [91], [97].For example, in the multi-agent system INTERVENOR [65], a teacher coder is designated to observe the program execution results and provide error explanations and bug-fixing plans for the student coder to understand and regenerate the code.Furthermore, to accurately pinpoint issues, some works provide more finegrained environment feedback for the subsequent processing of the model.LDB [75] constructs a control flow graph, which divides the program into multiple blocks.It uses breakpoints to obtain the runtime values of variables, and the model compares these values against the requirements, assessing each block for anomalies.</p>
<p>Static Code Checking</p>
<p>Static code checking refers to examining the quality of code without executing the code.In particular, static code checking has been essential in the modern continuous integration pipeline, as it is efficient to identify diverse categories of code quality issues (e.g., different bugs, vulnerabilities, or code smells) before extensively executing the tests.In practice, it is common to adopt static analysis techniques to automatically detect bugs/vulnerabilities (i.e., static bug detection) or involve peer reviews to check the quality of code (i.e., code review).</p>
<p>Static Bug Detection</p>
<p>Preliminary studies [2], [3] show that LLMs can help identify potential quality issues in the given code under inspection.For example, fine-tuning LLMs on existing buggy/correct code or simply prompting LLMs has demonstrated promising effectiveness in identifying the bugs, vulnerabilities, or code smells in the given code snippets [15], [100].However, given the diversity and complexity of the root causes of different code issues as well as the long code contexts under inspection, standalone LLMs exhibit limited accuracy and recall in the real-world static code checking scenario [101].Recently, researchers have built LLM-based agents to enhance the capabilities of standalone LLMs in bug or vulnerability detection.Table 4 summarizes these agents and Figure 8 illustrates their common pipeline.</p>
<p>A. Co-inspection with Multi-agent.One effective vulnerability detection strategy focuses on the perspective of  [111] propose an approach for vulnerability detection through mutual discussion and consensus among different LLMs representing the testers and developers respectively.It mimics the real-world code review process that involves the collaboration of various roles within a team, enhancing the detection effectiveness through the interaction and reflection between LLMs.Moreover, GPTLENS [104] is an adversarial yet synergic framework for detecting vulnerabilities in smart contracts.It is designed as a two-stage method involving several auditor agents and a critic agent, all adopting GPT-4 as the backend.The auditor agents randomly generate potential vulnerabilities and corresponding reasoning as thoroughly as possible, while the critic scrutinizes and scores the candidates based on specific criteria.Their evaluation on 13 real-world smart contract CVEs indicates the effectiveness by an improvement of up to 76.9% in the identification rate.Fan et al. [105] conceive the Intelligent Code Analysis Agent (ICAA) concept for static code analysis.ICAA is an integration of AI models (e.g., the LLMs), engineering process designs, and traditional non-AI components (e.g., static analysis tools).As a dynamic decision-making system, ICAA can be composed of multiple sub-agents to enhance its functionality.Two examples of ICAA implementations include bug detection and code-intention consistency checking, both of which rely on the collaboration of various subagents, such as the ReAct-based Analysis Agent and the Report Agent.</p>
<p>B. Additional Knowledge from Tool Execution.Other approaches concentrate on strengthening the capabilities of LLMs via tool invocation.ART [102] is a general framework that boosts LLMs for unseen tasks with multi-step planning and effective tool utilization.ART includes a tool library (e.g., search tools), which can facilitate task decomposition and the appropriate invocation of tools.In addition, ART accommodates the integration of human inputs, allowing for seamless updates to its libraries.ART is evaluated on the multiple downstream tasks such as bug detection, and outperforms both few-shot prompting and the automated generation of CoT reasoning.Sun et al. [110] propose LLM4Vuln to enhance the vulnerability reasoning capabilities of LLMs by decoupling and augmenting them.The agent improves in several aspects.It retrieves external knowledge from both the raw text of vulnerability reports and summarized key sentences, which include information about the functionality of the vulnerable code and its root cause.This information is stored in a vector database.Additionally, the agent invokes tools to actively seek further context about the C. Combined with Traditional Static Bug Detection.Some researchers have combined LLM-based agents with traditional static checking techniques to improve their static bug detection capability.For example, LLIFT [115] is an LLM-assisted Use-Before-Initialization (UBI) bug detection tool.Based on undecided bugs reported by the powerful static analysis tool UBITect, LLIFT further leverages the capability of LLMs in code comprehension and summarization to identify UBI bugs in the Linux kernel.However, LLMs have the inherent limitation in accepting and understanding long input context (e.g., numerous functions in the Linux kernel) as well as the hallucination and stochasticity issues.LLIFT addresses these issues by augmenting the basic LLM with some agentic techniques.For example, instead of flooding the LLMs with all possible related code, it only responds to the LLM's demand for specific function definitions through static analysis, achieving a balance between context length and information completeness.Besides, it employs in-context learning, task decomposition, self-validation, and majority voting strategies to further alleviate hallucination and stochasticity and guarantee detection accuracy.This framework identifies 13 UBI bugs from 1,000 potential UBI bug instances reported by UBI-Tect [116], with a precision rate of 50%.E&amp;V [107] is an agent designed for conducting static analysis of code in the Linux kernel.The high-level workflow of E&amp;V is a loop of employing an LLM-based agent for static analysis through pseudo-code execution, verifying the output of pseudocode, and providing feedback for re-analysis.To avoid fact hallucination caused by missing necessary code snippets (e.g., caller and callee functions in inter-procedural analysis), a source code retrieval component is introduced to fetch needed functions or structures through traditional static analysis tools (e.g., Clang [108]).E&amp;V has been evaluated against 170 Linux Kernel bugs and correctly pinpointed the blamed function in 81.2% of the cases.IRIS [113] is an agent augmented with CodeQL (a static analysis tool) [114] for vulnerability detection.IRIS first utilizes CodeQL to extract candidate APIs in the given repository.Then, it labels these APIs as potential sources or sinks of the given vulnerability via querying the LLM-based agent, which will be further handed over to CodeQL for detecting vulnerable paths.The final verdict is achieved by prompting the LLM agent to analyze the vulnerable paths and surrounding code of the source and sink.</p>
<p>Code Review</p>
<p>Developers review each other's code changes to ensure and improve the code quality before merging the changes into the branch.To mitigate the manual efforts in code review, researchers leverage learning approaches to automate the code review procedure.In particular, code review is formulated as a binary classification problem (i.e., code quality classification [117]) or a sequence-to-sequence generation problem (i.e., review comment generation [118]), which are tackled by fine-tuning or prompting deep learning models (including LLMs).Different from these works, LLM-based agents mimic the real-world peer review procedure by including multiple agents as different code reviewers.Table 5 summarizes existing agents for code review.</p>
<p>CodeAgent [119] is a multi-agent system that simulates a waterfall-like pipeline with four stages (i.e., basic information synchronization, code review, code alignment, and document) and set up a code review team with six agents of different characters (i.e., user, CEO, CPO, CTO, coder, and reviewer).In the basic information synchronization phase, CEO, CPO, and coder agents analyze the input modality and programming language.After that, the coder and reviewer agents collaborate to conduct code review and produce the analysis report.In the code alignment phase, the coder and reviewer agents continue to revise the code based on the analysis reports.Finally, in the document phase, the CEO, Different from CodeAgent which constructs a team that can conduct various code review tasks, Rasheed et al. [120] design an approach with each agent specialized for a single code review task individually.Notably, it proposes four agents including the code review agent, bug report agent, code smell agent, and code optimization agent.Each agent is trained on relevant Github data and evaluated on 10 AI-based projects.The results demonstrate the potential of applying multi-agent systems in the code review task.</p>
<p>ICAA [105] designs a multi-agent system to identify code-intention inconsistencies.It first uses the Context &amp; Prompt Incubation Agent to collect necessary information from the code repository through a thinking-decision-action loop.The Consistency Checking Agent will then analyze collected information and identify inconsistencies, which will be handed over to the Report Agent to form a final report.</p>
<p>CORE [121] designs a system with two agents along with traditional static analysis tools to fix code quality issues automatically.Specifically, the Proposer agent takes the static analysis report, the suspicious file, and the issue documentation from language-specific static analysis tools (e.g., CodeQL [114]) and the tool provider (e.g., the QA team, and proposes candidate revisions for each suspicious file.After that, static analysis tools will prune revisions that still have issues, while the rest will be scored and re-ranked based on their likelihood of acceptance by the Ranker agent.</p>
<p>Testing</p>
<p>Software testing is essential for software quality assurance.LLMs have demonstrated promising proficiency in test generation, including generating test code, test inputs, and test oracles.However, generating high-quality tests in practice can be challenging, as the generated tests should not only be syntactically and semantically correct (i.e., both the inputs and oracles should satisfy the specification of the software under test) but also be sufficient (i.e., the tests should cover as many states of the software under test as possible).As shown by previous work [122], the tests generated by standalone LLMs still exhibit correctness issues (i.e., compilation errors, run-time errors, and oracle issues) and unsatisfactory coverage.Therefore, researchers build LLM-based agents to extend the capabilities of standalone LLMs in test genera-tion.We organize these works based on the test levels (i.e., unit testing and system testing).Unit testing checks the isolated and small unit (e.g., method or class) in the software under test, which helps quickly identify and localize the bugs, especially for complicated software systems.Yuan et al. [122] perform a study showing the potentials of LLMs (e.g., ChatGPT) in generating unit tests with decent readability and usability.However, the unit tests generated by standalone LLMs still exhibit compilation/execution errors and limited coverage.Therefore, recent works have built LLM-based agents that primarily extend standalone LLMs by iteratively refining the generated unit tests towards better correctness, coverage, and fault detection capabilities.Table 6 summarizes the existing LLMbased agents for unit test generation, and Figure 9 illustrates their common pipeline.</p>
<p>A. Iterative Refinement to Fix Compilation/Execution Errors.The test cases directly generated by LLMs can exhibit compilation or execution errors.Therefore, inspired by program repair [128], LLM-based agents further eliminate such errors by iteratively collecting the error messages and fixing the buggy test code [122]- [124].For example, TestPilot [123] generates tests by constructing detailed prompts, including the function signature, implementation, documentation, and usage examples; it reflects on the feedback of failing tests and error messages to refine prompts and generate corrective tests iteratively.ChatTester [122] leverages LLMs to understand the intention of focal methods, and then generates a corresponding unit test; in addition, the iterative  [126] is an LLM-powered test generation system designed to achieve high coverage rates.It dissects the task by segmenting the source code and employs the SlipCover tool [129] to conduct a detailed coverage analysis.Using an iterative approach, CoverUp refines its prompts to focus on areas of the code that lack coverage, thereby enhancing the overall quality and comprehensiveness of the generated test suite.Yang et al. [125] propose TELPA to enhance the coverage of hard-to-reach branches in software testing.Its methodology involves a two-pronged program analysis: backward and forward method invocation analysis, for a better understanding of the methods with uncovered branches.TELPA also employs counter-example sampling to guide LLMs toward generating novel tests that diverge from ineffective ones.The feedback-based process refines these tests iteratively through a CoT strategy, further improving coverage.</p>
<p>C. Iterative Refinement to Increase Fault Detection Capabilities.MuTAP [127] is a single LLM-based agent system that aims at generating unit tests of better bug detection capabilities with the feedback of mutation testing.It employs prompt augmentation with surviving mutants and refining steps to correct syntax and intended behavior.During each iteration, the LLM first generates initial test cases and selfrefines their syntax errors and wrong behaviors, with the help of the Python parsing tool executing the tests.Then the tests run against the mutated programs, while the surviving mutants serve as feedback to direct the LLM in improving the test cases.</p>
<p>System Testing</p>
<p>System testing is a comprehensive process that assesses an integrated software system/component to guarantee that it fulfills its specification and operates as intended across diverse settings.For example, fuzzing testing and GUI (Graphical User Interface) testing are common testing paradigms at the system level.Leveraging LLMs for system testing can be challenging, as generating valid and effective system-level test cases should satisfy the constraints that are contained implicitly and explicitly in the specifications or domain knowledge of the software system under test.LLMbased agents are designed to better incorporate the domain knowledge of the software system under test compared to generating system-level tests via standalone LLMs.We then organize these works according to the software systems under test.Table 7 summarizes the existing agents for different software systems.</p>
<p>A. OS Kernel.KernelGPT [130] is an LLM-based agent for kernel fuzzing.The analysis agent serves as the brain to automatically generate driver syscall specifications.It identifies drivers first and iteratively completes each specification component, during which it can determine whether additional information is needed based on its previous memory.A code extractor (implemented using LLVM toolchain [132]) is invoked to parse the kernel codebase for the provision of source code information.KernelGPT finally invokes the Syzkaller tool [131] (e.g., syz-extract) and receives its feedback messages to validate and correct the generated syscall specifications iteratively.</p>
<p>B. Compiler.WhiteFox [133] encompasses two LLMbased agents working together, an analysis agent and a generation agent.The former examines the low-level optimization source code and produces requirements on the high-level test programs that can trigger the optimizations, while the latter crafts test programs based on summarized requirements.The generation agent further incorporates tests that have successfully triggered optimizations as feedback during the iterative process, thereby producing more satisfactory tests.LLM4CBI [134] is a single agent that aims at isolating compiler bugs by generating test cases of better fault detection capabilities.The agent utilizes tools to collect static information about the program (e.g., srcSlice [136] for data flow) to construct precise prompts to guide the LLM for program mutation.The memorized component records meaningful prompts and selects better ones to instruct LLMs to generate variants.The generated programs undergo validation by static analysis (e.g., the Frama-C tool [138]), and the feedback helps LLMs to avoid the same mistakes.The final test cases are used to identify suspicious files with spectrum-based fault localization techniques.</p>
<p>C. Mobile Applications.LLM-based agents are proposed to automate the testing process of mobile applications, including GUI testing, bug replay, and user acceptance testing.</p>
<p>C.1: GUI Testing.Some agents are developed to execute GUI testing for mobile applications.GUI testing is a commonly used software testing method aimed at verifying whether the user interface meets service specifications and user requirements.Previous LLM-based GUI testing approaches lack adequate autonomy, long-term planning, and coherence [154], [155].The emergence of LLM-based agents enables GUI testing to focus more on higher-level test objectives [139], [144], [145], such as clear task objectives, without  [139] propose a framework called GPTDroid, where the LLM iterates the entire process by perceiving GUI page information, generating test scripts in the form of Q&amp;A, executing these scripts through tools, and receiving feedback from the application.GPTDroid keeps a long-term memory to retain testing knowledge, which would help to improve the reasoning process.The DroidAgent [144] framework employs multiple LLM-based agents coordinating through different memory modules and can set its own tasks according to the functionalities of the apps under test.It is composed of four LLM-based agents: planner, actor, observer, and reflector, each with specific roles and supported by memory modules that enable long-term planning and interaction with external tools.AXNav [145] is another multi-agent system designed for replaying accessibility tests on mobile apps.It includes the planner agent, the action agent, and the evaluation agent, which together form the LLM-based UI navigation system.These agents translate test instructions into executable steps, conduct tests on a cloud-based iOS device, and summarize the test results in a chaptered video annotated with potential issues in the application, respectively.</p>
<p>C.2: Bug Replay.For automating Android bug replay, Feng et al. [15] introduce AdbGPT.Equipped with the knowledge of Step-to-Reproduce (S2R) entity specifications (i.e., predefined actions and action primitives), AdbGPT analyzes bug reports to translate identified entities into a sequence of actions for bug reproduction using the CoT strategy.It then perceives GUI states dynamically and maps the S2R entities to actual GUI events to replicate the reported bug.</p>
<p>C.3: User Acceptance Testing.To increase the automation of the user acceptance testing process, Wang et al. [148] propose XUAT-Copilot.The system is primarily comprised of three LLM-based agents responsible for action planning, state checking, and parameter selection, as well as two additional modules for state awareness and case rewriting.These agents interact with the testing equipment collaboratively, making human-like decisions and generating action commands.</p>
<p>D. Web Applications.RESTful APIs are popular among web applications as they provide a standardized, stateless, and easily integrable means of communication that enhances scalability and performance through a resourceoriented approach.RESTSpecIT [149] leverages LLMs to automatically infer RESTful API specifications and conduct black-box testing.Given an API name, RESTSpecIT generates and mutates HTTP requests through a reflection loop.By sending these requests to the API endpoint, it analyzes the HTTP responses for inference and testing.The LLM uses valid requests as feedback to refine the mutations in each iteration.Requests are validated based on the status code and message of the returned response.</p>
<p>E. Universal Software Categories.Some agent systems are not designed with a task-specific workflow, enabling them to be universally applicable across various target software systems.Xia et al. [150] present Fuzz4All, the first universal LLM-based fuzzer for general and targeted fuzzing across multiple programming languages.For a higher costeffectiveness ratio, Fuzz4All consists of two agents, (i) the distillation LLM for user input distillation and initial</p>
<p>Debugging</p>
<p>Software debugging typically includes two phases: fault localization [157] and program repair [158].In particular, fault localization techniques aim at identifying buggy elements (e.g., buggy statements or methods) of the program based on the buggy symptoms (e.g., test failure information); then, based on the buggy elements identified in the fault localization phase, program repair techniques generate patches to fix the buggy code.In addition, recent works also propose unified debugging to bridge fault localization and program repair in a bidirectional way [159].We then organize the works in LLM-based agents for debugging into three parts, i.e., fault localization, program repair, and unified debugging.Learning-based fault localization has been widely studied before the era of LLMs, which typically trains deep learning models to predict the probability of each code element being buggy or not [164].However, precisely identifying the buggy element in the software is challenging, given the large scale of the software systems as well as the massive and diverse error messages, which are often beyond the capabilities of standalone learning models including LLMs.Therefore, recent works build LLM-based agents, which incorporate multi-agents and tool usage to help LLMs tackle these challenges.Table 8 summarizes the existing LLMbased agents for fault localization, and Figure 10 illustrates their common pipeline.</p>
<p>A. Multi-agent Synergy.AgentFL [160] is a multi-agent system for project-level fault localization.The main insight of AgentFL is to scale up LLM-based fault localization to project-level code context via the synergy of multiple agents.The system consists of four distinct LLM-driven agents: test code reviewer, source code reviewer, software architect, and software test engineer.Each agent is customized with specialized tools and a unique set of expertise.With the four agents, AgentFL streamlines the project-level fault localization process by breaking it down into three phases: fault comprehension, codebase navigation, and fault confirmation.</p>
<p>RCAgent [162] is a multi-agent system for root cause analysis in industrial cloud settings.RCAgent includes two components: the controller agent and expert agents.The controller agent oversees the comprehensive thoughtaction-observation cycle, while the expert agents act for specialized tasks and can be utilized by the controller agent.A key-value store is employed for the controller agent to memorize the observation information (e.g., logs and table entries) to help the decision-making, as well as handling the context length constraint.The expert agents perform code analysis and log analysis tasks respectively, and their summarized results are fed back to the controller agent as observation.The agents can invoke tools for information collection (e.g., log data and repositories retrieval) or memory retrieval, through the function calling.Besides, a selfconsistency mechanism is built to enhance the performance.</p>
<p>B. Tool Invocation.AUTOFL [163] is a single-agent system, which enhances standalone LLMs with tool invocation (i.e., four specialized function calls) to better explore the repository.It first performs root cause explanation, invoking tools to oversee the source code repository for pertinent information, requiring only a single failing test and its failure stack.During this stage, it autonomously decides whether to continue function calling or to terminate with the production of root cause explanation.Subsequently, a post-processing step is used to correlate the outputs with exact code elements, aiming at bug localization.In addition, AgentFL [160] and RCAgent [162] also incorporate tool invocation (e.g., static analysis, dynamic instrument, and code base navigation) into their framework.</p>
<p>Program Repair</p>
<p>Fine-tuning and fixed prompting have been the most widely adopted paradigms for program repair techniques based on standalone LLMs.In particular, program repair is formulated as a translation problem [165] (i.e., translating the buggy code to correct code) or a generation problem [12] (e.g., infilling the correct code in the buggy code context).However, patches generated by LLMs in a single iteration are not always correct; they may fail to pass all the tests or may be overfitting to the test cases.Therefore, existing LLM-based agents all follow an iterative paradigm to refine patch generation based on the tool or model feedback in each iteration.Table 9 summarizes the existing LLM-based agents for program repair, and Figure 11 illustrates their common pipeline.ChatRepair [128], [166] is the first automated approach to iteratively refine patch or program generation based on environmental feedback.In each iteration, ChatRepair leverages tools to compile and execute the generated patches, and generates new patches based on the compilation/execution feedback and also earlier patch attempts in the same session.</p>
<p>CigaR [169] is a similar agent system for function-level program repair.CigaR leverages feedback to refine its outputs iteratively and decides to reboot the repair process when needed, ensuring the production of both plausible and diverse patches.</p>
<p>RepairAgent [171] adopts a more agentic design and improves the iterative refinement procedure by making the hard-coded feedback mechanism more flexible.In particular, RepairAgent allows the LLM itself to decide when or which tool to invoke.In addition to the basic tools (e.g., compilation tools and test execution tools) used in previous work, RepairAgent further includes tools for code reading, codebase searching, and hypotheses stating/discarding.</p>
<p>AutoSD [172] is a multi-agent system that iteratively fixes the buggy program via simulating the scientific debugging [179].AutoSD includes four components: LLM-based hypothesis generator, execution-based validator, LLM-based conclusion maker, and LLM-based fixer.In each iteration, the generator first generates a hypothesis about the bug, then invokes the debugger tool for hypothesis validation; the conclusion maker further identifies whether the hypothesis is rejected or not; and the fixer finally returns potential patches with explanations.</p>
<p>ACFIX [174] is a multi-agent system for fixing the access control vulnerabilities in smart contracts.By specializing LLMs with different roles, ACFIX includes a Rolebased Access Control (RBAC) mechanism identifier, a rolepermission pair identifier, a patch generator, and a validator.In particular, the validator checks the validity of the generated patches with both tool feedback (static grammar rule checking) and model feedback (multi-agent debate process).The feedback is further provided to iteratively refine the patch.</p>
<p>FlakyDoctor [175] is an agent to repair flaky tests.It takes into consideration test execution results and the location of test failures.Following this, the system generates targeted repairs and tests them for validation.This process is iterative, with the aim of continually refining the repairs until the issue of test flakiness is resolved.[190] ✓ -Pre-defined Ordered Natural Language AISD [191] ✓ Waterfall Pre-defined Ordered Natural Language LLM4PLC [192] × -Pre-defined Ordered Natural Language CodePori [193] ✓ -Pre-defined Ordered Natural Language LCG Waterfall [194] ✓ Waterfall Pre-defined Ordered Natural Language LCG Scrum [194] × Agile Pre-defined Debate Natural Language CodeS [195] ✓ -Pre-defined Ordered Natural Language Qian et al. [196] ✓ -Pre-defined Ordered Natural Language CTC [197] ✓ Waterfall Pre-defined Dual-role Natural Language AgileCoder [198] ✓ Agile Pre-defined Dual-role Natural Language comprehend, utilize, and unify the outputs of both fault localization and program repair.FixAgent [159], a multi-agent system for unified debugging, enables end-to-end fault localization, bug repair, and bug analysis.Based on manual debugging (e.g., rubber duck debugging), FixAgent uses agent specialization and synergy (i.e., LLM localizer, LLM repairer, LLM crafter, and LLM revisitor) to incorporate key variable tracking and program context comprehension.FixAgent can fix 79 out of 80 bugs in the QuixBugs [168] benchmark.</p>
<p>LDB [75] is an agent for end-to-end fault localization and program repair.LDB divides the buggy program into basic blocks according to a control-flow graph, and leverages LLMs to detect the incorrect blocks with run-time execution values and then to provide refinement suggestions.</p>
<p>End-to-end Software Development</p>
<p>Given the high autonomy and the flexibility from multiagent synergy, LLM-based agent systems can further tackle the end-to-end procedure of software development (e.g., developing a Snake Game application from scratch) beyond an individual phase of software development.In particular, alike the real-wold software development team, these agent systems can cover the entire software development life cycle (i.e., requirements engineering, architecture design, code generation, and software quality assurance) by incorporating the synergy between multiple agents that are specialized with different roles and relevant expertise.Table 10 summarizes the existing LLM-based agents for end-to-end software development.</p>
<p>Software Development Process Model</p>
<p>Real-world software teams often follow classic software process models (e.g., waterfall [199], incremental model [200], unified process model [201], and agile development [202]) to facilitate a more standardized software development life cycle.Existing LLM-based agents for end-to-end software development also design their workflows according to common software process models, e.g., waterfall process model and agile development.Figure 12 illustrates how existing LLM-based agents go through different process models.</p>
<p>A. Waterfall Process Model.The majority of existing LLM-based agent systems (e.g., AISD [191], LCG [194], Chat-Dev [186], CTC [197], and Self-Collaboration [4]) follow the classic waterfall process model for software development.</p>
<p>The traditional waterfall process model [199] is a linear and sequential software development workflow that divides the project into distinct phases, i.e., requirements engineering, design, code implementation, testing, deployment, and maintenance.Once a phase is finished, the project moves forward to the next phase without iteration.Based on this process, some end-to-end software development agents [4], [187], [191], [194] further extend the traditional waterfall process by including iterations in specific phases to ensure the high quality of the generated content.For example, the results of the testing phase might be fed back to the developer agent to revise the generated code; MetaGPT [187] further integrates the waterfall model with human-like Standardized Operating Procedures (SOPs), which assign responsibilities to each role and standardize the intermediate outputs, promoting collaboration among different team members.</p>
<p>B. Agile Development.Some works explore the potential of LLM-based agents with the agile development, including Test-Driven-Development (TDD) [194] and Scrum [194], [198].TDD prioritizes writing tests before the actual coding and fosters a cycle of writing test suites, implementing the code to pass the test suites, and concluding with a reflective phase to refinement.Scrum is an agile software development process model that breaks down software development into several sprints, achieving complex software systems through iterative updates.Experiments on function-level code generation benchmarks show that the Scrum model can achieve the best and most stable A. Role Categories.The roles in existing agents are primarily designed by simulating real-world software development teams or specialized by the workflow.</p>
<p>A.1: Simulating Real-world Software Teams.Most end-toend frameworks simulate the real-world software development teams and assign basic roles including managers (e.g., project managers or product managers), requirement analyzers, designers, developers, and quality assurance experts (e.g., software testers or code reviewers) to cover the entire pipeline of software development [4], [187], [191], [193], [194], [198].In addition to these common roles, there are some special roles that can be assigned to tackle fine-grained tasks.For example, a Scrum master role is also included into the requirements analysis and planning tasks for the agents with the Scrum workflow [194], [198]; and CEO/CTO roles are also included in some agents to complete the design task [186], [197]; in addition, there are special supervisor roles in some agents to ensure the smooth progress of the collaboration (e.g., providing coordination or critiques), such as the oracle roles in previous work [185] and the action observer in AutoAgents [189].The detailed categories of roles in existing agents are discussed in Section 5.2.1</p>
<p>A.2: Specialized by Workflow.Instead of simulating the real-world development teams, some agents break down roles according to the agent framework workflow.For example, CodeS [195] decomposes the complex code generation task into the implementation of repository, file, and method layers, and sets up the roles of RepoSketcher, FileSketcher, and SketchFiller.Co-Learning [190] and its subsequent work [196] abstract the code generation process into instruction-response pairs, thus only setting up the roles of instructor and assistant.</p>
<p>B. Role Creation.The roles in multi-agent systems are either created in a pre-defined way or in a dynamic way.</p>
<p>B.1: Pre-defined.The majority of specialized roles are predefined by the agent framework [4], [183], [186], [187], [190], [191], [193]- [198].In other words, the roles are fixed by the agent for each task.</p>
<p>B.2: Dynamic Creation.In addition to the pre-defined roles, some agents assign roles in a dynamic way, which can equip multi-agent systems with more flexibility.For example, AutoAgents [189] designs a drafting stage that aims at determining the roles of the multi-agent group via the communication between two meta agents: the planner and the agent observer; in addition, AgentVerse [188] sets up a group of different roles through an expert recruitment stage.Talebirad et al. [185] propose a novel framework and enable an agent to spawn additional agents to the system.Such dynamic strategies aim at creating roles in a more diverse and flexible way.</p>
<p>Collaboration Mechanism in Multi-agent</p>
<p>Within the multi-agent systems for end-to-end software development, it is essential to schedule how each agent is coordinating with each other.We then discuss the collaboration mode and the communication protocol adopted in existing agents.</p>
<p>A. Collaboration Mode.In particular, there are mainly two collaboration modes in multi-agent systems for end-toend software development, i.e., the ordered mode and the unordered mode.</p>
<p>A.1: Ordered Mode.It is a sequential mode wherein each agent makes decisions independently and uses optional feedback mechanisms to improve quality of their generated content.Ordered mode is the most prevalent collaboration mode adopted in existing agent systems [4], [186]- [193], [195]- [198].Previous research [188] suggests that this approach is more suitable for software development, as it focuses on producing only the final refined decision.</p>
<p>A.2: Unordered Mode.It primarily introduces an unordered debate mechanism, where multiple agents present their opinions separately and equally, with the ultimate decision reached via summarization.For example, LCG [194] involves the sprint meeting in the Scrum process model, wherein participating agents propose their opinions and share them with all other agents through a buffer.The Scrum master ultimately summarizes and extracts user stories.Besides, some works adopt a multi-role mechanism  [186], [190], [196], [197] CAASD [191] 72 Software Description (50 words) Multiple Files Python [186], [187], [191] SoftwareDev [187] 70 Software Description (30 words) Multiple Files Python [186]- [188] SketchEval [195] 19 README (421 words) Structured Multiple Files Python [186], [195] ProjectDev [198] 14 Software Description (262 words) Multiple Files Python [186], [187], [198] HumanEval [170] HumanEval-ET [203] 164 Function Description (68 words) Single Function Python [4], [187]- [189], [193], [194], [198] MBPP [204] MBPP-ET [203] 974 Function Description (15 words) Single Function Python [4], [187], [193], [194], [198]  #Errors [4], [186], [187] [188]- [190] [193], [194], [196] [197], [198] Similarity SketchBLEU [195] Cosine Distance [186], [190], [195] [196], [197] Costs Running Time Token Usage Expenses #Sprints [187], [198] Manual Efforts Human Revision Costs [187] Generated Code Scale Line of Code Code Files Completeness [186], [187], [190] [196], [197] to ensure the generation quality [186], [190], [196]- [198].Instead of passing intermediate results from one agent to another, these works assign two agents to work together to complete a sub-task in a communication way.For example, in ChatDev [186] and CTC [197], agents with different roles will collaborate to complete specific tasks (e.g., the system design is achieved through communication between the CEO and CTO).</p>
<p>B. Communication Protocol.</p>
<p>Within the multi-agent systems, agents communicate with other agents to exchange information.In particular, there are two communication protocols, i.e., the pure natural language and the structured communication.</p>
<p>B.1: Natural Language.The most common communication protocol is direct dialogue [4], [186], [188], [189], [191], [193], [194], which leverages natural language to exchange information.This approach allows for flexible expression of intent and is close to human communication.</p>
<p>B.2: Structured.Some agents (e.g., MetaGPT [187]) structure communication by having agents exchange documents and diagrams instead of relying solely on dialogue, as pure natural language may be insufficient for solving complex tasks due to distortion in multi-turn communication [187].</p>
<p>Agent Evaluation</p>
<p>Given the complexity of end-to-end software development, researchers further build diverse benchmarks and metrics for a comprehensive evaluation.</p>
<p>A. Benchmarks.Table 11 summarizes the benchmarks used for evaluating existing LLM-based agents for end-toend software development.In particular, we can observe that there are still a large number (e.g., 7) of papers using the classic code generation benchmarks (e.g., HumanEval [170] or MBPP [204]) for evaluating end-to-end software development.Although these traditional code benchmarks can represent end-to-end software development to some extent, they still involve simplified, small-scale development tasks (i.e., input of short function descriptions and output of a single function, as shown in Table 11).In addition, there are five more complicated benchmarks that aim at simulating the end-to-end software development, i.e., SRDD [186], [190], [196], [197], CAASD [191], SoftwareDev [187], SketchEval [195], and ProjectDev [198].The tasks in these benchmarks include more complicated and longer requirement description (e.g., the average length of software description in Pro-jectDev is 262 words), and their expected outputs are supposed to contain multiple files.In particular, the benchmark SketchEval is built upon the real-world GitHub repositories, and its input descriptions are extracted from the README file of the software while its output expects multiple files that are organized in a repository structure.</p>
<p>B. Metrics.Table 12 summarizes the metrics used for evaluating existing LLM-based agents for end-to-end software development.In fact, given the difficulty of generating complicated program, it can be possible that the generated program cannot perfectly pass the tests.Therefore, in addition to the common metrics (e.g., Pass Rate or Pass@K) that execute the generated program for validation, there are multiple dimensions for assessing how existing agents perform in end-to-end to software development.In particular, there are (i) the similarity metrics between the generated program and the ground truth (e.g., SketchBLEU [195] measures the structure similarity), (ii) the costs of executing or generating the program, (iii) the manual efforts to further refine the generated program, and (iv) the scale of the generated</p>
<p>End-to-end Software Maintenance</p>
<p>Software systems undergo maintenance as requirements continuously change (i.e., adding, deleting, or modifying features) or unexpected software behaviors arise.In practice, users report unsatisfactory behaviors that they encounter; developers then diagnose the reported issues and modify the software to fix them.Such an end-to-end software maintenance process can be time-consuming and labor-intensive in practice, as it involves multiple phases including understanding user-reported issues, localizing code for maintenance, and precisely editing code to address issues.Recently, there has been an increasing number of multi-agent systems aiming at automatically solving issues of real-world software projects.Table 13 summarizes the characteristics of these agents.</p>
<p>Common Pipeline</p>
<p>Figure 13 illustrates the pipeline of existing LLM-based agent systems for end-to-end software maintenance.Basically all of the existing agents follow a common pipeline of three phases, i.e., issue localization, patch generation, and patch verification, where different agents incorporate different strategies to tackle each phase.In addition, some agents further include additional phases, i.e., preprocessing, issue reproduction, issue localization, task decomposition, patch generation, patch verification, and patch ranking.</p>
<p>A. Preprocessing.To better understand the whole repository, some agents first perform preprocessing to prepare pre-knowledge before the entire procedure.The agent system RepoUnderstander [209] constructs a knowledge graph of the entire code repository to facilitate the subsequent process of issue localization.Meanwhile, Agentless [211], which is simplistic and less agentic, simply turns the whole project into a tree-like structure that demonstrates all directories and files of the repository in a hierarchical format, which facilitates the issue localization phase.In CODER [208], a manager agent first chooses a plan from several workflows pre-defined by human experts.In MASAI [210], the test template generator is used to analyze the testing setup of the repository and generate a test template with the running command, which further serves as an example for the following issue reproduction phase.</p>
<p>B. Issue Reproduction.A test script that triggers the unexpected behaviors users encounter is essential for issue resolution.It not only helps with issue localization but also serves as the verification criterion for patch correctness.However, in practice, users do not always provide such reproduction tests when they report issues; and such reproduction tests are often added by developers after they fix the buggy software.Therefore, some agents design the issue reproduction phase that aims at generating the test script that can trigger the unexpected behaviors encountered by users.For example, SWE-agent [207] and CodeR [208] directly leverage LLMs to generate reproduction tests based on issue descriptions when there is no existing reproduction script in issue descriptions.However, generating reproduction tests can be challenging, as the tests must be executable and ideally should fail on the buggy software version while passing on the fixed version.Therefore, to increase the success rate of issue reproduction, the multi-agent system MASAI [210] includes a two-stage approach for issue reproduction, which first investigates the test framework and existing tests for generating a sample test template (generated in the preprocessing phase) and then uses the template as a demonstration to create the reproduction script.</p>
<p>C. Issue Localization.Issue localization is one of the most important phases where the agents are supposed to precisely identify the code elements (e.g., classes, methods, or code blocks) that are related to issues and should be edited.We then summarize the common localization strategies used in existing LLM-based agents.</p>
<p>C.1: Retrieval-based Localization.All the existing agents use the retrieval-based strategy as the basis for issue localization, which identifies the relevant code elements based on their similarity with issue descriptions.For example, in MAGIS [205], all code files are compared to issue descriptions via BM25 [212], and the Top-K relevant code files are selected as the potential issue locations.However, only using retrieval-based strategy can be insufficient and often at coarse granularities (e.g., files).Therefore, some agents further extend the basic retrieval with other strategies, e.g., navigation-based localization, spectrum-based localization, and simulation-based localization.</p>
<p>C.2: Navigation-based Localization.This strategy provides agents with a set of code search actions that can navigate through the entire code repository to check all directories and files.For example, the issue localization phase of SWEagent [207] uses several pre-defined search-based interfaces to locate the target directories or files and then to view the code snippets in the target file through scrolling interfaces.Similarly, MASAI [210] assigns an edit localizer that can navigate the repository to find the related code snippets.AUTOCODEROVER [206] designs a stratified context retrieval process, which allows the LLM itself to decide whether to further refine the location based on the current context, thus forming an iterative navigation process.Agentless [211] provides the hierarchical structure of the target repository and instructs the LLM to gradually localize files, classes, functions, and concrete edit locations.</p>
<p>C.3 Spectrum-based Localization.Some agents integrate the traditional fault localization approaches, especially spectrum-based fault localization techniques [157], which calculate the suspiciousness score of code elements based on their coverage of failed tests and passing tests.For example, AUTOCODEROVER [206] explores spectrum-based techniques for issue localization by using the ground-truth reproduction tests provided in SWE-bench Lite, which improves issue resolution rate from 17.00% to 20.33%.While AUTOCODEROVER explores the spectrum-based fault localization in an ideal case (as the ground-truth reproduction tests are not always available in practice), CODER [208] investigates the improvement of the spectrum-based fault localization in a more practical setting by using tests generated in the issue reproduction phase.In particular, CODER calculates the suspiciousness scores based on the coverage of the reproduction tests, and then combines them with the basic retrieval-based strategy (i.e., BM25 similarity between code elements and issue descriptions) via weighted computation.</p>
<p>C.4 Simulation.Simulation is a special technique adopted by RepoUnderstander [209] for issue localization.It applies the classic Monte Carlo Tree Search algorithm.By recursively incorporating nodes of the high BM25 score with the issue, it evaluates and ranks the most relevant paths in the repository knowledge graph.The collected code is then summarized for issue localization.</p>
<p>D. Task Decomposition.Before generating patches, some agents decompose the task into more fine-grained sub-tasks.For instance, in MAGIS [205], its manager agent breaks down the issue into file-level tasks and delegates them to a newly-formed development team; similarly, in RepoUnderstander [209], its summary agent summarizes the collected code and issue description, and then outlines the fine-grained steps for issue resolution.</p>
<p>E. Patch Generation.In this phase, the agents generate patches for the localized suspicious code elements.The input context of this phase typically includes the issue/task description and the suspicious code elements for modification [205], [207], [210], [211].In addition, some agents (e.g., AUTOCODEROVER [206], CodeR [208], and RepoUnderstander [209]) further refine the input contexts by including relevant cross-file code contexts that are collected by retrieval APIs.</p>
<p>F. Patch Verification.Agents further verify the correctness of the generated patches, which is challenging as the reproduction tests are not always available in practice.Therefore, agents incorporate different verification strategies.</p>
<p>F.1: Code Review.Some agents (e.g., MAGIS [205]) design a quality assurance agent to review the quality of generated patches.</p>
<p>F.2: Static Checking.Some agents (e.g., AU-TOCODEROVER [206], RepoUnderstander [209], MASAI [210], Agentless [211], and SWE-agent [207]) use static checking approaches to assess the syntactic correctness, indentation, and compatibility of the generated patch with the repository environment.</p>
<p>F.3: Dynamic Checking.Since the static checking cannot find the semantic violation of the patches, some agents (e.g., CodeR [208] and MASAI [210]) further perform dynamic checking by executing the reproduction test on the patch.The patch that passes the reproduction test can be considered as effectively resolving the issue.In particular, existing reproduction tests are reused (if available); otherwise, reproduction tests generated during the issue reproduction phase are used.Agentless [211] also implements a dynamic checking approach by conducting regression testing to filter out incorrect candidate patches.</p>
<p>G. Patch Ranking.Since the patch verification phase can sometimes be insufficient for filtering out all the incorrect patches, some agents further include a patch ranking phase to identify the patch with the highest probability of being correct.For example, in MASAI [210], a ranker agent is responsible for ranking all potential patches based on the issue description and reproduction tests; In Agentless [211], all patches are normalized and re-ranked based on the number of occurrences with the majority voting strategy.</p>
<p>Benchmarks</p>
<p>To evaluate how LLM-based agents tackle end-to-end software maintenance, researchers build benchmarks from realworld Github issues, including SWE-bench [213], SWEbench Lite [214], SWE-bench Lite-S [211], and SWE-bench Verified [215].Table 14 summarizes the evolution timeline of existing benchmarks for end-to-end software maintenance.SWE-bench [213] is the first benchmark for end-to-end software maintenance, which consists of 2,294 real-world GitHub issues across 12 popular Python repositories.Each task in SWE-bench includes an original text from a GitHub issue (i.e., the issue description or problem statement), the entire code repository, the execution environment (i.e., Docker environment), and validation tests (i.e., tests that are hidden from the evaluated agents).However, the full SWE-bench benchmark can take too much evaluation costs and it contains particularly difficult or problematic tasks [211], which can underestimate the evaluation of LLM-based agents.Therefore, researchers have dedicated lots of manual efforts to identifying highquality tasks with reasonable difficulty, self-contained information, informative issue descriptions, and sufficient evaluation tests.For example, SWE-bench Lite [214] is a subset of SWE-bench that manually removes tasks requiring complicated edits (e.g., editing more than one file) or the tasks including images or hyperlinks; SWE-bench Lite-S [211] removes tasks that contain exact patches, misleading solutions, or insufficient information in the issue descriptions; similarly, SWE-bench Verified [215] removes cases with unspecified descriptions or insufficient tests.</p>
<p>ANALYSIS FROM AGENT PERSPECTIVE</p>
<p>This section organizes the collected papers from the perspective of agents.Specifically, Section 5.1 summarizes the components of existing LLM-based agents for SE; Section 5.2 focuses on existing multi-agent systems for SE by summarizing their roles and collaboration mechanisms; and Section 5.3 summarizes how humans coordinate with agents for SE.</p>
<p>Agent Framework</p>
<p>Based on the common framework of LLM-based agents [21], [26], [28], this section summarizes the common paradigms of the planning, perception, memory, and action components in existing LLM-based agents for SE.</p>
<p>Planning</p>
<p>In SE, intricate tasks such as development and maintenance activities necessitate the orchestrated efforts of various agents through multiple iterative cycles.Therefore, planning is an essential component for agent systems by meticulously delineating task sequences and strategically scheduling agents to ensure the seamless progression of the SE process.Figure 15 presents the taxonomy of the planning components in existing LLM-based agents for SE.</p>
<p>A. Single Planner vs. Multiple Planners.In LLMbased agent systems, planning is typically handled by a specialized agent [4], [61], [91], [98], [105], [144], [145], [183], [187], [191], [193] or as a core responsibility of an individual agent [76], [82], [85], [86], [148], [162], [192].Some works use the function-calling interface [216] provided by  or GPT-4 [218], handing over the planning task to high-performance models [82].However, given the pivotal role that planning plays in influencing subsequent action steps, some works incorporate a collaborative approach among several agents to further enhance the accuracy and practicality of the plans formulated [91], [186], [189], [195], [197], [198], [205].</p>
<p>B. Single-turn Planning vs. Multi-turn Planning.</p>
<p>The fundamental planning strategy is to craft a holistic plan from the very beginning meticulously and then proceed to implement it in successive rounds [4], [61], [82], [86], [91], [98], [105], [183], [186], [187], [191]- [193], [195], [197], [198], [205].Further, many SE agents have adopted a ReActlike [219] architecture, which implements a multi-turn planning mechanism wherein the next-round actions will not be determined until receiving the environmental feedback from the previous round.This form allows for dynamic revision and expansion of the plan, enabling it to adapt to more flexible task scenarios, such as issue resolution [76], [210], iterative code generation [82], [85], mobile app testing [144], [145], [148], among others [162].</p>
<p>C. Single-path Planning vs. Multi-path Planning.</p>
<p>Most LLM-based agents use single-path planning strategies, i.e., they plan and execute tasks in a linear manner [4], [61], [86], [105], [151], [186], [187], [191]- [195], [198], [205], [220].However, agents inherit the randomness from the backbone LLMs, leading to fluctuations in task decomposition.Some approaches improve upon single-path planning by using feedback from each round to dynamically plan the next round of actions [82], [85], [88], [97], [144], [145], [148], [162], [210].Although these dynamic strategies are still singlepath, they offer considerable flexibility due to their ability to be adjusted based on progress and execution outcomes.Another approach to address this issue is to design a multipath planning strategy, which instructs the agents to generate or simulate multiple plans, and select [76], switch [98], or aggregate [197] the optimal paths for execution.</p>
<p>D. Plan Representation.</p>
<p>The plan can be exhibited in different forms, including natural language descriptions, semi-structured representations, or graphs.</p>
<p>• Natural Language.Most agents describe the plan in natural language, especially as a list of procedural steps [4], [86], [91], [98], [105], [183] or features to be implemented [187], [191], [194], [198].</p>
<p>• Semi-structured.The agent system AXNav [145] represents the action list in JSON format; and some code-generating agent systems directly output the code skeleton [61], [97], [192], [195] or present the plan as executable code [205], which can be seen as a special plan form in SE tasks.• Graph.Some agents model the plan as a graph to facilitate the expansion and traceability of execution paths [76], [88], [151].</p>
<p>Memory</p>
<p>The memory component is a pivotal mechanism responsible for storing the trajectories of historical thoughts, actions, and environmental observations, enabling agents to sustain coherent reasoning and address intricate tasks.In SE, complex development and maintenance tasks generally necessitate agents conducting iterative revisions, wherein historical intermediate information, e.g., generated code and testing reports, significantly impacts integrity and continuity.We then detail the implementation of memory mechanisms in SE from four perspectives: memory duration, ownership, format, and operation.Figure 16 presents the taxonomy of the memory components in existing LLM-based agents for SE.</p>
<p>A. Memory Duration.Inspired by human memory systems, agent memory can be classified into short-term memory and long-term memory based on the memory duration.</p>
<p>A.1: Short-term Memory.Short-term memory, also known as working memory [221], is integrated into agents to enhance their ability to sustain trajectories of the current ongoing task and is frequently used when multi-turn interactions are involved.In SE, there are some predominant patterns of short-term memory.</p>
<p>• Dialog Records.This pattern is generally used to memorize the pure dialog history among agents and is typically in the form of history summary [148], [189] and multiturn instruction-response pairs [186], [197], [198].It is straightforward to implement and can offer a thorough and detailed historical record of the task-solving process.However, the weakness is that the dialog history can be lengthy and contain irrelevant and redundant information.</p>
<p>• Action-Observation-Critique Records.While dialog history concentrates on the thoughts and responses among agents, some works highlight the interaction between agents and the environment by memorizing the actionobservation sequences.Moreover, the critique information is also retained in case certain reflection mechanisms are introduced [144].This pattern has been adopted in SE tasks that necessitate iterative feedback from the environment, e.g., mobile app testing [139], [144], [148], wherein operations on widgets in each turn should be memorized to facilitate the next-turn decision-making, and iterative code generation [62], [88], [187], [189], wherein the previous editing, execution, or debugging history serves as important information for code revision.• Intermediate Outputs.Some agents store the outputs of previous turns in short-term memory to avoid overrunning the limited space as well as being overly influenced by irrelevant or inaccurate chat history.For example, in E&amp;V [107], only intermediate analysis results are summarized to avoid inconsistency with the previously generated outputs.In SoA [97], to implement a self-organized framework, each agent is equipped with memory that stores the self-generated code and unit tests.These intermediate results allow delayed test execution and code modification for agents in different layers, facilitating hierarchical collaborative code generation.</p>
<p>A.2: Long-term Memory.Long-term memory, on the other hand, is used to memorize valuable experiences of historical tasks, which can be recalled by agents when solving unseen tasks.Due to extensive trajectories, long-term memory commonly uses distilling techniques or only stores the pivotal information.</p>
<p>• Distilled Trajectory.The entire task execution trajectory may involve extensive context, and given the limited memory space, it can be challenging to store it all completely.As a result, distilling techniques have been</p>
<p>Memory</p>
<p>Memory Duration</p>
<p>Short-term Memory</p>
<p>Dialog Records ChatDev [186], AGILE-CODER [198], CTC [197], etc.</p>
<p>Action-Observation-Critique Records</p>
<p>DROIDAGENT [144], GPTDroid [139], MetaGPT [187], AutoAgents [189], etc.</p>
<p>Intermediate output E&amp;V [107], SoA [97] Long-term Memory Distilled Trajectory DROIDAGENT [144], Qian et al. [190], [196], MetaGPT [187] Selective Storage ChatDev [186], AGILE-CODER [198], CTC [197], etc.</p>
<p>Memory Ownership</p>
<p>Specific Memory</p>
<p>XUAT-Copilort [148], SoA [97], GPTDroid [139], Reflexion [62], etc.</p>
<p>Shared Memory</p>
<p>MetaGPT [187], Self-Collaboration [4], MARE [57], AGILECODER [198], etc.</p>
<p>Memory Format Natural Languages</p>
<p>ChatDev [186], CTC [197], XUAT-Copilot [148], MAGIS [205],AG-ILECODER [198], etc.</p>
<p>Programming Languages CodePlan [88], SoA [97] Structured Messages MetaGPT [187], MARE [57], E&amp;V [107] Key-value Pairs Qian et al. [190], [196], RCAgent [162] Embeddings DROIDAGENT [144] Trees LATS [76], Olausson et al. [63] Memory Operations</p>
<p>Memory Writing</p>
<p>Preprocessing XUAT-Copilot [148], DROIDA-GENT [144], MAGIS [205], Qian et al. [190], [196], etc.</p>
<p>Eliminiation Reflexion [62], Qian et al. [190], [196] Memory Reading</p>
<p>Filtering Criteria</p>
<p>Recency ChatDev [186], AGILE-CODER [198], CTC [197], etc.</p>
<p>Relevance</p>
<p>MetaGPT [187], AGILE-CODER [198], CodePlan [88]</p>
<p>Similarity</p>
<p>Qian et al. [190], [196], DROIDAGENT [144] Reading Manners</p>
<p>Reflection LCG [194], AutoAgents [189] Retrieval DROIDAGENT [144], Qian et al. [190], [196] Subscription MetaGPT [187], AGILECODER [198] Fig. 16: Taxonomy of Memory Design in LLM-based Agents for SE proposed, e.g., trajectory summarization [144], [187] and shortcut extraction [190], [196].These distilled records retain the task execution process in a more concise manner, thereby alleviating the burden on limited memory and prompt windows.• Selective Storage.Another manner to save long-term memory space is to store vital data of each task, e.g., the final results [186], [189], [197], [198], reflections [62], [189], and action-observations [76], [144], [162].Compared to complete historical trajectories, these data highlight the pivotal trace information, which can still retain the effects and feedback of previous tasks.</p>
<p>B. Memory Ownership.</p>
<p>In agent systems, the memory module can be designed to serve specific agents or to serve all agents.Based on its ownership, we categorize memory into Specific Memory and Shared Memory.</p>
<p>B.1: Specific Memory.Specific memory is an agent mechanism designed specifically for a limited group of agents.This type of memory has strict pre-defined usage regulations, only storing and serving specific agents in the workflow [62], [76], [88], [97], [98], [107], [139], [144], [148], [162], [186], [187], [189], [190], [196]- [198], [205].For example, in SoA [97], each agent is equipped with individual memory for storing its own generated code fragments and unit tests, which will be used to evaluate the correctness of the final code and provide feedback to the agent for modification.</p>
<p>B.2: Shared Memory.Shared memory, on the other hand, serves all agents by maintaining the record of their outputs and offering essential historical data.In most cases, shared memory serves as a dynamic information exchange hub in the intricate SE environment, which is akin to the traditional blackboard system [222].Generally, information stored in the shared memory is the intermediate results of previous phases, hence the agents from subsequent phases can obtain necessary information in a more convenient manner [4], [57], [160], [187], [198].Representative work like MetaGPT [187] introduces a shared message pool, which saves artifacts from different agent roles, e.g., the product requirement documents from the product manager.Another typical application of shared memory is to store comments in a decentralized debate scenario.Specifically, LCG scrum [194] simulates the Sprint Meeting by providing a shared buffer, storing the problem and the discussion comment of all participated agents from which the product manager could extract a list of user stories.</p>
<p>C. Memory Format.In this section, we elaborate on the format of data stored in the memory.In SE tasks, the most commonly used storage formats include natural languages, program languages, structured messages, key-value pairs, embeddings, and trees.</p>
<p>C.1: Natural Languages.LLM-based agents solve tasks specified in natural language, which is thus the most fundamental and prevalent data format in memory [4], [62], [139], [148], [186], [189], [194], [197], [198], [205].The advantage of raw natural language is that it allows for a more flexible storage of trajectories, thereby enhancing the universality.Moreover, raw natural language can better preserve the integrity of the original dialogue, which minimizes the loss and distortion of essential information.</p>
<p>C.2: Programming Languages.Some agents directly store the generated code for subsequent utilization [88], [97].For example, SoA [97] is a hierarchical code generation framework with each agent focusing on single function implementation.It stores the generated function code and unit tests in the memory for testing, modification, and aggregation.</p>
<p>C.3: Structured Messages.In this format, memory is organized as a list of messages with multiple attributes.The strength of this format is that it allows data to be stored in a structured manner, making it more convenient for indexing and processing.Moreover, it can store vital metadata of the message, e.g., message source and destination, task type, etc., so it is easier for agents to trace and subscribe to required messages, making it commonly used in shared memory.Representative works like MetaGPT [187] and MARE [57], both wrap the artifacts of each agent as informative messages, involving the original content, instruction, task name, sender, and receiver, etc.In E&amp;V [107], intermediate results of previous turns will be summarized and stored in JSON format.</p>
<p>C.4: Key-value Pairs.In this format, information received from the agents is stored in an external memory, with a key extracted for the agents to query required history memories [162], [190], [196].More specifically, in Co-Learning [190], shortcuts are extracted from the trajectories to construct two key-value databases: the solution-toinstruction database for the instructor, and the instructionto-solution database for the assistant.RCAgent [162] stores the whole observation body in a key-value store, remaining a snapshot key for agents for query details.</p>
<p>C.5: Embeddings.In this format, the memory is embedded into a vector, which can help retrieve the most relevant task experiences.Representative work like DROIDAGENT [144] embeds the textual history into vectors and stores them in an external embedding database.Compared to text similarity retrieval, it can further provide semantic similarity retrieval.</p>
<p>C.6: Trees.Some approaches construct a tree or graph for memorizing, especially in scenarios requiring flexible extension or path tracing.For example, in LATS [76], the task-solving process is modeled into a tree with each node representing a state with the instruction, the action, and the observation, and then an extended Monte Carlo Tree Search algorithm can be integrated.Similarly, Olausson et al. [63] propose a repair tree that stores multiple generationfeedback-repair paths.D. Memory Operations.We categorize operations on memory into two main sections: memory writing and memory reading.</p>
<p>D.1: Memory Writing.The purpose of memory writing is to store essential information in the memory.In SE, the most commonly considered problems include how to process the received information (memory preprocessing) and how to avoid memory overflow (memory overflow).</p>
<p>• Memory Preprocessing.Information stored in memory is usually the raw task execution trajectories [186], [197], [198].However, considering that the raw task trajectories might be lengthy, distilling approaches have been proposed to retain a more informative summary in the memory [62], [107], [144], [148], [187], [190], [196], [205].</p>
<p>For instance, in XUAT-Copilot [148], dialog and action history are stored in working memory as summarized texts.Moreover, Co-Learning [190] proposes a novel distilling approach by first constructing a task execution graph and then extracting shortcuts linking non-adjacent solution nodes, which can serve as solution refinement paths for future tasks.• Memory Elimination.The limited memory storage and prompt window size result in finite memory records.When overflow occurs, some records must be forgotten.For example, in Reflexion [62], the past experiences are stored in a sliding window with a maximum number of 3 to avoid exceeding the prompt window.Additionally, low-quality and rarely-used data also consume memory storage space.Previous research [190] sets a threshold to filter out experiences with limited information.Further, an elimination mechanism based on the usage frequency is introduced to exclude rarely-used experiences [196].</p>
<p>D.2: Memory Reading.Memory reading aims at obtaining the required task history and experiences from the memory module.We elaborate on the memory reading process from two perspectives: the filtering criteria required historical records and reading manners to obtain these records.</p>
<p>Filtering Criteria.The factors influencing whether a historical record should be integrated into the current task can be chiefly categorized into three parts: recency [88], [139], [144], [148], [186], [187], [197], [198], [205], relevance [88], [187], [198] and similarity [144], [190], [196].Agents integrate the most relevant and similar task experiences to provide the best reference for the current task.Additionally, the preference and weight of these factors vary across different works.In DROIDAGENT [144], the planner agent considers the 20 most recent task summaries and the 5 most similar task knowledge items.In MAGIS [205], the agent uses the most recent summary of a code file to identify differences.In MetaGPT [187] and AGILECODER [198], agents retrieve only relevant messages from shared memory based on their roles.</p>
<p>Reading Manners.In SE, instead of directly providing the raw memory to the agent [4], [139], [160], [186], [187], [197], [198], [205], researchers prefer using three manners for obtaining relevant memory: reflection, retrieval, and subscription.</p>
<p>• Reflection.Reflection refers to extracting pivotal experiences from the extensive trajectory memory [189], [194].For example, in AutoAgents [189], the dynamic memory mechanism is designed to instruct an agent to extract in-sights from long-term memory that will serve the current action.In LCG scrum [194], the product manager summarizes the comments collected from all agents and extracts a list of user stories to implement.• Retrieval.In this manner, the memory is retrieved based on its text or semantic similarity with the current tasks [144], [190], [196].For example, in Co-Learning [190], the reasoning module uses the prompt as a query to retrieve similar shortcuts from the constructed experience pool, which will serve as examples to facilitate future reasoning.</p>
<p>In DROIDAGENT [144], the past tasks and widgets with similar GUI state embeddings are retrieved by comparing the cosine similarity.• Subscription.The subscription mechanism is chiefly used in shared memory.It permits agents to directly obtain required information according to their roles, without additional interaction costs with other agents, thus improving efficiency.Representative works include MetaGPT [187] and AGILECODER [198], both adopting this kind of publish-subscribe mechanism.</p>
<p>Perception</p>
<p>Existing LLM-based agents for SE primarily adopt two perception paradigms: textual input perception and visual input perception.</p>
<p>A. Textual Input.Text can flexibly express the intent, information, and knowledge.In SE, the majority of historical data, e.g., documentation, code, and issues, is stored in the textual form.This alignment with the strengths of LLMs in processing natural language makes textual input the predominant form of perception for agents for SE.Textual input in existing agents can be further categorized into natural language input (i.e., instructions and auxiliary information collected from the environment) and programming language input (i.e., the code context).For example, in NL2Code tasks [186], [187], user requirements and function descriptions are provided as the instruction to agents.But in some code-related tasks, e.g., software testing [126], [127] and debugging [159], [160], [171], [174], the target code can also be provided for analysis.Specifically, in repository-level tasks such as issue-resolution [206]- [208], repository-level fault localization [160], and code edits [88], only a portion of code snippets are provided due to context length limitations, with further inspections achieved through navigation in the code repository.</p>
<p>B. Visual Input.Images represent a two-dimensional medium for storing information.In traditional SE scenarios, there is also a portion of data presented in image form, e.g., UML diagrams [223] and UI pages.A small number of existing agents use this visual information to enhance their understanding of target tasks.For example, in MetaGPT [187], the product manager creates a competitive quadrant chart for the architect, who then provides system architecture and sequence flow diagrams to the engineer agents.</p>
<p>Visual input is more widely used in mobile app testing tasks, since the clickable widgets can be located easily in screenshots.Previous work [144] has also discovered that the widgets in some apps might be presented in raw pictures without any textual information [144].Therefore, in XUAT-Copilot [148] and AXNav [145], the screenshot of the current page is provided to the agent to visualize the accessible widgets.Just as humans use their eyes to interpret images, these agents integrate external visual models to process and understand image data.XUAT-Copilot [148] uses the SegLink++ model [224] for detecting bounding boxes and a ConvNeXts model [225] for text recognition.</p>
<p>Action</p>
<p>The action component of existing LLM-based agents for SE primarily involves using external tools to extend their capabilities beyond the interactive dialogue typical of standalone LLMs. Figure 17 summarizes the tools used in these agent systems.</p>
<p>A. Searching Tools.In SE, agents frequently use searching tools to retrieve relevant information (e.g., documentation or code snippets) that can be helpful for the task completion.</p>
<p>A.1: Web Searching.Online search engine tools use community and tutorial websites to offer programmers accurate and practical suggestions based on shared experiences and Q&amp;A.When faced with gaps in specific domain knowledge, programmers distill their needs into a query and use existing search engines (e.g., Google, Bing, WikiSearch) to find the necessary information, an approach that can also be employed by agents [78], [96], [105], [153], [187], [193].For example, some agents [78], [81], [82], [188] use Duck-Duckgo [228] to search the relative content such as APIs.Paranjape1 et al. [102] employ SerpAPI [229] and extract answer box snippets when they are available or combine the Top-2 search result snippets together.He et al. [230] query Google and then extract pertinent information to construct prompts for LLMs.Depending on the task at hand, the search space can be restricted to specific websites (e.g., as StackOverflow) or certain websites can be blocked to prevent data leakage.</p>
<p>A.2: Knowledge Base Searching.Besides using a web searching tool to externally collect the information from the wide, it is also common for existing agents to retrieve relevant knowledge from the self-established knowledge base (e.g., memory pool or code repository).In particular, similarity and string matching are two common retrieval strategies.Similarity-based retrieval approaches include sparse wordbag and dense text embedding.Both approaches vectorize codes or documents, calculating similarity between the query and the segment in knowledge base to obtain relevant information.The sparse word-bag approach (e.g., BM25 [78], [82]) vectorizes text while partially preserving its semantics.Dense text embedding model such as dual-encoder encodes the text into embedding vectors and calculates their cosine similarity [64], [77], [105], [110], [144], [151], [162], [190].String matching approaches directly split the given code element name and match it within the knowledge base [107], [171], which is used for locating files within a repository by the file name.</p>
<p>B. File Operation.As SE activities frequently access massive files especially for the code repository and documentation, it is common for agent systems [113], [153], [171], [172], [207], [210] to use file operations including shell commands (e.g., Linux shell) or the code utils (e.g., Python os package) for file browsing, file adding, file deleting, and file editing.For example, for file browsing, agents open files
Action Searching Tools Web Searching
MetaGPT [187], CodePori [193], ICAA [105], AgentVerse [188], etc.</p>
<p>Knowledge Base Searching</p>
<p>CodeAgent [82], ToolCoder [78], Co-Learning [190], EV [107], etc. File Operation SWE-agent [207], RepairAgent [171], MASAI [210], etc.</p>
<p>GUI Operation</p>
<p>GPTDroid [139], XUAT-Copilot [148], AXNav [145], AdbGPT [15] Static Program Analysis</p>
<p>Static Information Collection</p>
<p>Abstract Syntax Tree AutoCodeRover [206], TOOLGEN [89],</p>
<p>AdbGPT [15], TELPA [125], etc.</p>
<p>Control Flow Graph LDB [75], LLM4CBI [134] Call Graph TELPA [125], AutoSpec [226] Dynamic Analysis Method Call Trace AgentFL [160] Runtime Values AUTOSD [75], LDB [75] Coverage CoverUp [126], TELPA [125], LLM4CBI [134] Testing Tools Test Validation AUTOSD [172], LCG [194], AISD [191], ClarifyGPT [94], etc.</p>
<p>Test Generation</p>
<p>TELPA [125] Mutation Testing MuTAP [127] Fault Localization Tools RepairAgent [171], Au-toCodeRover [206] Version Control Tools RepoAgent [227] Fig. 17: Taxonomy of Action Components in LLM-based Agents for SE based on their paths, scroll through the contents, and jump to specific lines.</p>
<p>C. GUI Operation.For SE activities related to software with GUI, it is necessary to enable various GUI interaction operations for agent systems [15], [139], [145], [148], including clicking, text input, scrolling, swiping, returning, and termination.In particular, for UI element identification, they use visual and text recognition models (e.g., SegLink++ [231], Screen Recognition [232], and Con-vNeXts [233]), dump (e.g., Android UIAutomator [147]), or parse the UI view hierarchy [15], [139]; then they simulate the testing environment using virtual Android devices (e.g., Genymotion [146], VirtualBox [140], and pyvbox [141]) and autonomously execute or reply actions through tools such as Android Debug Bridge [143] to mimic user interactions.These actions enable agent systems to test in various GUI environments.</p>
<p>D. Static Program Analysis.</p>
<p>Static program analysis tools are widely used in agent systems for SE tasks, as they can provide more rigorous code features (e.g., data-flow and control-flow) for LLMs.Existing agents primarily use static program analysis tools for two purposes: collecting static program information and checking code quality.</p>
<p>D.1: Static Information Collection.Agents invoke static analysis tools to parse the program and to collect additional program information; and the collected information can further help agents better understand the program and thus tackle the relevant tasks.Existing agents primarily collect the following static information.</p>
<p>• Abstract Syntax Tree (AST).AST is a common representation to describe the syntactic structure of the source code and is widely used by agents.In particular, the collected ASTs help agents extract syntactic elements (e.g., class names, method names, and variable names) [15], [77], [82], [88], [88], [89], [107], [125], [126], [198], [206], [210], [226] and identify dependency among these code elements [88], [125], [198], [226], [227].Tree-sitter [161] and ANTLR [234] are AST parsing tools that are widely used in existing agent systems [82], [88], [160], [171], [174], [198], [210].• Control Flow Graph (CFG).LDB [75] uses CFG to divide a program into multiple blocks, making it easier to track intermediate variables with the help of the debug-ger.LLM4CBI [134] calculates the cyclomatic complexity based on a CFG that represents failed tests and accurately identifies high-complexity blocks of the code.These complicated code blocks would be regarded as the targets for program mutation.• Call Graph (CG).TELPA [125] constructs a method CG, extracting all call sequences that reach uncovered target methods.Based on these sequences, new test cases are generated to ensure comprehensive coverage of previously untested methods.AutoSpec [226] treats loops as nodes as well, constructing an extended call graph; and it then traverses the CG from the bottom up to generate specifications.• Data Flow Graph (DFG).IRIS [113] constructs a data flow graph to assist taint analysis, which helps detect security vulnerabilities.LLM4CBI [134] performs data flow analysis to output a list of the most complex variables defined and used in the failed test, which guides test generation.• Code Dependency Graph (CDG).Agents such as Agile-Coder [198] and CodePlan [88] build a Code Dependency Graph for the entire codebase.The graph represents complex relationships between code blocks (e.g., call relationships, inheritance, and import dependencies) and enables the accurate extraction of task-relevant context information (e.g., error trace-back path for repair) within constraints of a limited prompt length.In addition, by dynamically maintaining the CDG, agents can perform incremental analysis in a more efficient way.• Code Completion Tokens.In code generation tasks, it is common for agents [77], [88], [89], [227] to use language servers (e.g., Jedi [235] and EclipseJDTLS [236]) to collect candidate tokens at the certain position.In particular, candidate tokens returned by language servers often pass the syntactic violation (e.g., only defined variable names are returned), which can effectively alleviate the hallucinations of standalone LLMs.</p>
<p>D.2: Code Quality Checking.Static analysis tools are also widely used by agent systems to check code quality, e.g., syntactic correctness checking, code format checking, code complexity checking, vulnerability detection, and specifications checking.The checked results can then provide feedback or additional hints for agents to further improve code quality.In particular, existing agents [77], [84], [197] use compilers or interpreter (e.g., GCC or Python) for syntactic correctness checking; existing agents [82], [192] use Black [237] and nuXmv [238] for code format checking; the agent in [134] uses OClint [135] and srcSlice [136] for code complexity checking; existing agents [134], [174] use static analysis tools such as Frama-C [138] and Slither [239] to detect vulnerabilities; the agent in [226] uses static tools (e.g., Frama-C) to verify the satisfiability and sufficiency of generated specifications.</p>
<p>E. Dynamic Analysis.In addition to static analysis, existing agents also use dynamic analysis tools to collect dynamic runtime information (i.e., method call trace, runtime values, and coverage) that can further provide runtime behaviors for agents.</p>
<p>• Method Call Trace.AgentFL [160] uses the java.lang.instrumentpackage [240] to record all method call traces during the execution of failed tests, which can facilitate more accurate fault localization.</p>
<p>• Runtime Values.Some agents [75], [172] mimic manual debugging to set breakpoints, so as to capture runtime values for variables.The runtime values can be integrated into the prompt along with requirements to aid in defect localization.• Coverage.Coverage serves as important feedback for whether each code element is executed by tests or not.For example, some agents [125], [126], [134] leverage tools such as SlipCover [129], Pynguin [241], and Gcov [137] to collect the coverage information.F. Testing Tools.Test cases validate whether the software behaviors violate the specifications, and it is common for agents in SE to invoke testing tools, including tools for test validation, test generation, and mutation testing.</p>
<p>F.1: Test Validation.Validating the software with test execution frameworks (e.g., PyTest, unittest, or JUnit) can reveal the runtime errors and test failures, which are widely used in existing agent systems [62]- [67], [71]- [74], [79], [81]- [85], [87], [91], [92], [94], [95], [97], [98], [102], [122], [123], [125], [128], [133], [153], [159], [171], [172], [175], [187], [188], [191], [194], [198], [206], [210], [230].The revealed execution violations can further serve as feedback for agents to improve programs; otherwise, the absence of execution violation can serve as a signal for the correctness of programs (e.g., a plausible patch is found for program repair agents).</p>
<p>F.2: Test Generation Tools.Although an LLM itself has promising capabilities of directly generating test code, traditional test generation tools provide complementary benefits as they are good at generating high-coverage tests in a costefficient way.For example, some agents [125] use automated test case generation tools (e.g., Pynguin [241]) to generate an initial set of unit test cases.</p>
<p>F.3.Mutation Testing.Some agents [127] use mutation testing tools (e.g., MutPy [242]) to evaluate the sufficiency of test cases, as killing mutants (i.e., exhibiting different behaviors on the mutated program than the original program) indicates the fault detection capabilities of tests.The mutation testing results can further serve as the feedback for agents to iteratively enhance the tests.</p>
<p>G. Fault Localization Tools.Agent systems [171], [206] can invoke traditional fault localization techniques, especially spectrum-based fault localization tools (e.g., GZoltar [243]) to localize suspicious code elements.For example, RepairAgent [171] invokes GZoltar to get the suspiciousness score of each code element (i.e., the probability of being fault).</p>
<p>H. Version Control Tools.Version control systems manage the changes of various files in a repository such as changes in code, configuration files, or documentation throughout the software development process.Some agents [227] involving managing an entire repository often leverage version control tools.For example, RepoAgent [227] uses Git [244] to track changes in code files and promptly synchronize updates to project documentation.</p>
<p>Multi-agent System</p>
<p>Based on our statistics, 52.8% of existing agents for SE are multi-agent systems.These systems benefit from the division of specialized roles and coordination among agents, Agent Roles Manager Roles Task Decomposition Manager Bot [193], Planner [144], [145], Manager [205], Scrum Master [194], [198], Controller [162],</p>
<p>Planning Agent [98], Instructive Agent [196], Product Manager [187], Planning LLM [183] Decision Making CEO &amp; CTO [119], Instructor [190], AI User [68] Team Organization</p>
<p>Mother Agent [97], Planner &amp; Agent Observer [189] Requirement Analyzing Roles Product Manager [185], [187], [191], [198], Analyst [4], Task Interpretation and Planing LLM [105],</p>
<p>Requirement Engineer [194], Task Specifier Agent [68], User [54] Architect [187], [194], Software Architect [185], [191], CEO &amp; CTO [186] UI/UX Designer User Experience Designer [185], User Interface Designer [185],</p>
<p>UI/UX Designer [188] Developer Roles Dev-bot [193], Writer [64], Engineer [187], Programmer [67], [95], [186], [188], [191], Software Developer [185], SketchFiller [195], Coder Agent [66], Child Agent [97], Coding Agent [98],</p>
<p>AI assistant [68], Developer [194], [198], [205], Code Learner [65],</p>
<p>Code Model [63], Coder [4] Software Quality Assurance Roles</p>
<p>Code Reviewer</p>
<p>Reviewer [119], [186], [197], QA Engineer [205], Verification Bot [193],</p>
<p>Senior Developer [198] [160], Safeguard [64], Auditor &amp; Critic [104],</p>
<p>Consistency Checking Agent [105] Tester Crafter [159], Tester [194], [198], Test Designer [95], QA Engineer [187], Software Tester [185], Generation LLM [133], Action Agent &amp; Evaluation Agent [145], Actor &amp; Observer [144], Operation Agent [148] Debugging Roles Test Failure Analysis Remediation Agent [66], Feedback Model [63], Questioner [67], Reflector [144], Revisitor [159] Test Failure Reproduction Test Template Generator &amp; Issue Reproducer [210] Repair</p>
<p>Localizer &amp; Repairer [159], Generator [174], Fixer [210],</p>
<p>Debugging Agent [98] Assistant Roles</p>
<p>Repository Custodian [205], RepoSketcher [195], Edit Localizer [210], Retrieval Agent [98], Report Agent [105] Fig. 18: Taxonomy of Agent Roles in LLM-based Multi-agent Systems for SE which effectively addresses the complexity of SE tasks, particularly for end-to-end activities spanning multiple phases.This section provides an overview of existing multi-agent systems for SE, with a focus on their agent roles and coordination mechanisms.</p>
<p>Agent Roles</p>
<p>In a multi-agent system, each agent is typically assigned a specialized role designed to address specific tasks.This role assignment forms the foundation of the system, directly influencing task decomposition and agent coordination.Specifically, role assignment mainly delineates duties, available actions, attributes, and constraints of roles.It makes agents as experts of corresponding tasks.Figure 18 summarizes common agent roles in existing multi-agent systems for SE.</p>
<p>A: Manager Roles.Managerial roles, such as CEO, CTO, commander, and controller, serve as the leaders of a multiagent team.These roles are responsible for making decisions, planning, task decomposition and assignment, and overseeing team coordination.</p>
<p>A.1: Roles for Task Decomposition.To enhance the overall system performance, managers break down a project into manageable sub-tasks and draw up a guiding plan for developers or testers to execute [98], [144], [145], [162], [193], [196], [198], [205].They analyze problem statements, facilitate discussions on issues among various agent roles [194], review design documents submitted by designers [187], and incorporate related information gathered by assistants.Subsequently, they produce a specific task list or implementation blueprint, which may be presented in either natural language or as a structured workflow [183].</p>
<p>A.2: Roles for Decision Making.This role is designed for orchestrating collaboration within a team and providing further guidance for task execution.For example, CEO and CTO in CodeAgent [119] communicate with staff and make high-level decisions.Similarly, the instructor in the agent [190] and the AI user within the CAMEL system [68] issue directions to working agents.</p>
<p>A.3: Roles for Team Organization.This role is primarily designed for flexibly deciding the constitution of the agent team, i.e., what roles are included in the team.The main benefits of including such roles are to flexibly optimize costs and better meet project demands.For instance, SoA [97] sets the mother agent, which generates new mother or child agents and designates concrete tasks (e.g., unimplemented functions) to them.AutoAgents [189] includes a planner agent and an observer agent, which collaborate to assemble a team for particular tasks.The planner agent is responsible for assigning existing LLM agent roles or generating new ones, while the observer agent assesses and reviews the relevant roles.These roles are represented in a structured JSON format, encapsulating details such as name, description, available tools, suggestions, and prompts to guide agent behaviors.</p>
<p>B. Requirement Analyzing Roles.These roles are primarily responsible for analyzing software requirements, such as translating vague and preliminary user concepts into a coherent and structured format.Existing agents [4], [105], [185], [187], [194], [198] include such roles (e.g., product manager [191] or task specifier [68]) to identify key requirement elements and intended objectives for a precise and organized requirement document, which may range from an elaborated task or function description [68] to a formal software requirement specification [187].</p>
<p>In addition, some agents further design more finegrained roles for requirements analysis.For example, Elicitron [54] incorporates a set of User agents to identify diverse user requirements by mimicking user perspectives and conducting interviews for exploring potential user needs; MARE [57] uses a requirements engineering team (i.e., stakeholder, collector, modeler, checker, and documenter) to produce requirements specifications.The requirements engineering process is segmented into four sub-tasks corresponding to specific roles, seamlessly transitioning rough user ideas to precise requirement specifications.</p>
<p>C. Designer Roles.Designer roles take input information on requirements (such as detailed task descriptions and use cases) and shape the software architecture and system integration.</p>
<p>C.1: Software Architecture Designer.This role is responsible for conceptualizing and defining the high-level structure of software, e.g., the software architect role in agents [185]- [187], [191], [194].They create a design document that serves as a blueprint for the subsequent stages of development; and the design document can be presented in various forms, including natural language descriptions, structured formats (e.g., JSON for listing project architecture files), and graphical representations (e.g., class diagrams and sequence flowcharts [187]).</p>
<p>C.2: UI/UX Designer.This role primarily focuses on crafting the visual and interactive aspects of the software interface, such as user experience (UX) designer or user interface (UI) designer roles in agents [185], [188].</p>
<p>D: Developer Roles.Developers take a vital role in software development and maintenance activities, which is one of the most common roles (e.g., also called as programmer or coder) in existing agents [64], [66], [68], [97], [98], [185]- [188], [193], [195] for tasks involving code generation.In accordance with software design schemes, task plans provided by other agents, or user requirements, the developer roles generate or finalize code at various levels (i.e., from function to file and even project levels).In addition, the developer roles also engage in the code refinement process, which refines their previously generated code [4], [63], [65], [67], [95], [194], [205].Furthermore, the developer roles can be set to meet more customized standards, such as elucidating their work through supplementary docstrings or adhering to particular coding criteria [191], [198].</p>
<p>E: Software Quality Assurance Roles.Agent systems include roles dedicated to software quality assurance, similar to real-world QA teams.These roles typically encompass code reviewers, testers, and debuggers, each focused on checking and improving software quality.</p>
<p>E.1: Code Reviewer.This role is responsible for identifying potential software quality issues by statically inspecting the software without execution.For example, some agents [119], [186], [188], [193], [197], [205] include such roles to review generated code or patches; AgileCoder [198] and CAMEL [68] include the roles such as senior developer or critic agent to offer suggestions for enhancement; the agent in [120] sets up code review agent, bug report agent, code smell agent, and code optimization agent to access code quality from different aspects; AGENTFL [160] sets test code reviewer and source code reviewer to summarize code behaviour to help fault location; in addition, some agents [64], [104], [105] include such roles (e.g., the auditor agent and the critic agent in GPTLENS [104] and the consistency checking agent in [105]) to detect the vulnerability or implementation issues.</p>
<p>E.2: Tester.The tester roles are widely incorporated in multi-agent systems [95], [133], [144], [145], [148], [159], [185], [187], [194], [198] for software quality assurance, which are mainly responsible for writing new tests or generating testing action sequences.For example, the tester agent in multi-agent systems [95], [159], [194] generates test cases based on relevant code skeleton or patches, requirement documents, existing tests, or rationale for the test; the tester agent in multi-agent systems [144], [148] generates operational actions for GUI tests or systematic functional tests based on the specified requirements.</p>
<p>E.3: Debugging Roles.In multi-agent systems, debugging roles are responsible for diagnosing test failures or unexpected software behaviors.</p>
<p>• Test Failure Analysis.Some multi-agent systems include debugging roles in analyzing test reports.For example, the remediation agent in TGen [66] and the feedback module in Self-repair [63] are similarly designed to analyze the test failure reports and relevant faulty code to provide explanations and suggestions; the questioner agent in AutoCoder [67] describes execution errors to help modify the generated code; the reflector agent in DROIDAGENT [144] reflects and summaries on the test results; and some agents [65], [159] further include debugging roles (e.g., the revisitor) to provide explanations for test failures.• Test Failure Reproduction.MASAI [210] uses the test template generator to produce test templates based on the repository information, which further helps the issue reproducer reproduce behaviors described in the issue reports.• Repair.Some multi-agent systems include such roles in fault localization and program repair.For example, agents [98], [159], [174], [210] include roles such as repairer and fixer to generate patches for bugs; F: Assistant Roles.Assistant roles primarily provide assistance for other agents.For example, the repository custodian in MAGIS [205], the RepoSketcher in CodeS [195], and the edit localizer in MASAI [210] are designed to enhance the comprehension of the target repository architecture for the team; in addition, MapCoder [98] uses the retrieval agent to facilitate memory recall; ICAA [105] introduces the report agent to convert natural language responses into formatted bug reports.</p>
<p>Collaboration Mechanism</p>
<p>The collaboration mechanism is essential for multi-agent systems, which can significantly impact the effectiveness and costs of the entire system.In particular, the collaborative mechanisms of existing multi-agent systems for SE tasks can be categorized into four types: layered structure, circular structure, tree-like structure, and star-like structure.Figure 19 illustrates each structure.It is a hierarchical structure, where tasks are decomposed into several sub-stages and each is assigned to a specific agent or a group of agents selected from the agent pool.Agents between different stages collaborate in a sequential manner, i.e., they receive intermediate results from agents in the previous stage as input and produce their processed data to agents in the next stage.For example, the workflow within agents of [105], [133], [160], [183], [191], [206] is a simple chain, where each agent focuses on its own sub-task and only interacts with adjacent agents.In addition, agents can also refer to the message produced by the previous non-adjacent agents [98], [187], [195].In the sequential workflow, each sub-task can also be handled by a group of agents [57], [91].In [119], [186], [197], each subtask is solved by the conversation between two agent roles.LCG [194] and AgileCoder [198] incorporate even more agents in a single stage.In addition to interactive collaboration, another scenario involves agents within the same layer working in parallel to offer their solutions.These solutions are then combined and passed down to the next layer.For example, GPTLENS [104] employs several auditors to present possible vulnerable functions individually in the generation stage.The work [69] incorporates the majority voting mechanism.DyLAN [70] formulates the LLM-agent collaboration structure into a multi-layered feed-forward network.</p>
<p>B. Circular Structure.This structure typically manifests as multi-turn dialogues or integrates the feedback mechanism within the overall collaborative processes among agents.The feedback loop facilitates the refinement of the agents' outputs through iterative cycles.The circular structure may consist of dual roles in a dialogue or extend to multiple roles, which results in a more complex iterative circle.</p>
<p>B.1: Dual Roles.On the one hand, some agents [63], [66], [67], [95], [174] implement a generation-validation style loop between two agents.In this setup, one agent is tasked with the primary function, such as generating code snippets or patches, while the other agent provides validation feedback, including static analysis results, test outcomes, and improvement suggestions.In the INTERVENOR framework [65], the code learner initiates the process by generating the initial code and subsequently engages in iterative repairs guided by the suggestions from the code teacher.On the other hand, some agents [68], [111], [190] adopt a dual-agent dialogue cooperative model to achieve objectives.The work [111] facilitates agreement through a discussion between the tester and the developer; and the agents [68], [190] progress through tasks in a step-wise manner, utilizing an instructor-assistant conversational approach.</p>
<p>B.2: Multiple Roles.When more agents are included, the collaborative loop will become more flexible.For example, some works [62], [145], [193] incorporate multiple agents in a larger feedback loop, further dismantling the tasks.DroidAgent [144] embeds an inner loop between the Actor and Observer in the overall loop between the Planner and the Reflector.</p>
<p>C. Tree-like Structure.Different from the layered structure, agents in the same layer of the tree-like structure do not cooperate with each other for the same sub-task but focus on their own work.For example, in SoA [97], the mother agent can dynamically spawn new mother or child agents for code generation, thereby forming a tree-like collaboration structure.In MASAI [210], the tests for reproducing issues and the possible patches are generated parallelly and finally aggregated to the ranker to select the best one.</p>
<p>D. Star-like Structure.This structure is a centralized structure, where a central agent serves as the pivot to interact with other agents.For example, the controller agent in the RCAgent [162] framework can invoke other expert agents as a kind of tool when necessary.The commander in AutoGen [64] coordinates with the writer and the safeguard separately, to craft code and ensure safety.XUAT-Copilot [148] adopts the operation agent as the core, to receive the judgment from the inspection agent and invoke the parameter selection agent to help the action planning.</p>
<p>Human-Agent Collaboration</p>
<p>While most agents aim to achieve maximum automation, where users only need to propose a request and wait for the agents to complete the task, previous studies [183], [191] show that LLM-based agents often encounter bottlenecks during the software development process.Therefore, some agents incorporate the human-agent cooperation paradigm to further align and enhance the agent performance with human preference and expertise.As summarized in Figure 20, existing agents primarily include the human participation in four phases: planning, requirements, development, and evaluation.</p>
<p>Fig. 20: Human-Agent Collaboration in SE</p>
<p>A. Planning Phase.Some agents include human intervention into the planning stage of the agent workflow.For example, the low-code LLM platform [183] offers users a selection of predefined actions to modify auto-generated workflows.The generated workflows can be checked and revised by users before execution.However, revising the system design requires a certain level of expertise, so it is optional in some agents such as AISD [191] and LLM4PLC [192].</p>
<p>B. Requirements Phase.The initial requirements (i.e., the task description) provided by users can be ambiguous, which can lead to a gap between the final outputs of the agent system and the user intention.Therefore, it is common for agent systems to further include manual refinement for the requirements.For example, ClarifyGPT [94] and CodeAct [85] employ a dialogue-based interaction with humans; similarly, Sapper IDE [184] and MARE [57] use human feedback to refine the requirements.AISD [191] enables users to assess and refine the generated use cases.In addition, HARA [245] produces a concise and readable summary table of the generated requirements for further manual expert review.</p>
<p>C. Development Phase.Human involvement can be included in the software development phase to guide agents to strategize solutions and overcome potential failures.Flows [91] leverages human-crafted solutions to produce better code for competitive programming challenges.Both AutoGen [64] and LLM4PLC [192] allow users to furnish feedback when necessary, directing the workflow as needed.</p>
<p>In CodeS [195], the repository is constructed from a tripartite sketch, assigning users the flexibility to edit each individual layer.</p>
<p>D. Evaluation Phase.Human participation also serves as a post-evaluation mechanism for the outcomes produced by the agent system, which can further ensure the outputs are aligned with user intention.For example, AISD [191] and Prompt Sapper [184] both include human intervention at the acceptance testing phase.Users can conduct manual testing of the final system and the test reports help with the necessary refinements.Similarly, in ART [102], users can enhance agent performance on particular tasks by offering feedback through the modification of the task and tool libraries.</p>
<p>RESEARCH OPPORTUNITIES</p>
<p>This section discusses promising research directions and open problems in LLM-based agents for SE.</p>
<p>Evaluation of Agents for SE.Given the emergence of LLM-based agents for SE, it is crucial to develop comprehensive and rigorous evaluation frameworks, including (i) designing more diverse metrics and (ii) constructing higherquality, more realistic benchmarks.</p>
<p>Metrics.Current evaluations of SE agents primarily focus on their ability to solve specific tasks, such as measuring the success rate of agents on benchmarks such as SWEbench.However, these evaluations often concentrate on the final success rate without delving into the intermediate states during the agent's workflow.This lack of fine-grained metrics makes it difficult to assess why agents fail in certain tasks or to what extent they fall short.Given the complexity of SE tasks, failures are common, and without deeper analysis, improving agent performance becomes challenging.Therefore, the design of fine-grained metrics is necessary, allowing researchers to move beyond "black-box" evaluations and gain insights into the agent's decision-making process and failure points.</p>
<p>Additionally, existing metrics heavily emphasize effectiveness, leaving trustworthy requirements such as robustness, security, and fairness underexplored.Given the flexibility and autonomy of LLM-based agents, they may exhibit unstable behavior, which can limit their practical application in real-world SE environments.Evaluating these attributes is essential for building trust in these systems.</p>
<p>Another critical consideration is the cost associated with these agents, particularly as they often involve lengthy workflows, frequent LLM invocations, and the management of large datasets.According to our analysis, only 44.3% of the papers we surveyed have explicitly considered the efficiency of agents in SE tasks, incorporating quantitative analyses of time, token consumption, monetary cost, and feedback loops (e.g., tool invocation frequency or inter-agent discussion frequency).These efficiency and computational costs are particularly important when applying agents to large-scale code repositories, complex documentation, or intricate workflows.</p>
<p>Benchmarks.LLM-based agents significantly extend the capabilities of standalone LLMs, showing great promise in tackling more complex, end-to-end SE tasks.However, existing benchmarks used for evaluating these agents often suffer from quality issues.For instance, prior research [211], [215] has identified that the SWE-bench benchmark includes tasks with vague or incomplete issue descriptions, reducing their relevance and applicability.</p>
<p>Moreover, many tasks in current benchmarks are far simpler than real-world SE challenges.As outlined in Table 11, the software generated by LLM-based agents for endto-end development tends to be relatively small in scale (e.g., consisting of a single function or a few files), which is not representative of the complexity of real-world software projects.In addition, previous work [215] reveals that the majority of tasks (77.8%) in the SWE-bench benchmark can be completed within an hour by an experienced software engineer, further highlighting the gap between benchmark tasks and real-world SE challenges.</p>
<p>To address these shortcomings, future research can focus on creating more realistic, high-quality benchmarks that better reflect the complexity and demands of real-world SE.These improved benchmarks will enable more accurate and meaningful evaluations of LLM-based agents' capabilities and potential.</p>
<p>Human-Agent Collaboration.Software development is inherently a creative process, transforming human requirements into executable software.As such, aligning agent systems with human preferences and intentions is a critical goal.While some existing agents incorporate human participation at various stages of the workflow (as discussed in Section 5.3), there has been limited exploration of how to more thoroughly integrate human involvement throughout the entire software development life cycle.Additionally, the interaction mechanisms between agents and humans remain underexplored.</p>
<p>Currently, agents mainly involve humans in tasks such as requirements clarification, planning adjustments, coding assistance, or evaluation.However, extending human participation to other phases, such as architecture design, test generation, code review, and the end-to-end software maintenance process, remains largely unexplored.A deeper integration of human input across these phases could significantly enhance both the quality and adaptability of the agent's output.</p>
<p>Moreover, designing effective interaction mechanisms is essential for human-agent collaboration.This includes creating user-friendly interfaces for (i) displaying relevant information, such as intermediate outputs from agents, and (ii) collecting user feedback in a streamlined way.Given the complexity of information produced during the agent's workflow, designing such interfaces presents challenges.For instance, when agents are tasked with generating or maintaining a software repository, simply presenting all code files in a flat format would be resource-intensive and inefficient.Therefore, more sophisticated methods of organizing and representing complex data are required to facilitate effective human-agent interaction.</p>
<p>Perception Modality.Most agents applied to SE tasks primarily rely on textual or visual perception.This is largely because software development and maintenance activities are heavily associated with processing large volumes of code, documentation, and images.However, there is still significant potential to explore and incorporate more diverse perception modalities into these agents.</p>
<p>For example, in the context of programming assistance, most LLM-powered coding agents predominantly use textual input, such as chat interfaces or integrated development environment (IDE) code contexts.Alternative input formats, such as voice commands or user gestures, remain underutilized.Expanding the range of perception modalities could significantly enhance the flexibility and accessibility of coding assistants, allowing users to interact with agents in ways that better suit their individual workflows and preferences.</p>
<p>Furthermore, exploring diverse perception modalities may shape the future of software development and maintenance, offering new opportunities to streamline interactions and improve the efficiency of agent-driven processes.</p>
<p>Applying Agents for More SE Tasks.While existing agents have been deployed across various software SE tasks, several critical phases remain underexplored.As highlighted by our analysis in Section 4, there is a lack of LLM-based agents specifically designed for tasks such as design, verification, and feature maintenance during software development and maintenance.</p>
<p>Developing agent systems tailored to these phases presents unique challenges.Tasks like design and verification require advanced reasoning and comprehension capabilities from the LLM-based agents, extending beyond basic code generation.These tasks demand a deeper understanding of architecture, system logic, and the ability to make informed decisions-skills that traditional LLM-controlled agents may not yet fully possess.</p>
<p>Training Software-oriented LLMs for SE Agents.LLMs are the central component controlling the "brain" of agent systems.Most existing agents for SE rely on LLMs trained on general-purpose data (e.g., ChatGPT [246]) or code-specific data (e.g., Deepseek-Coder [247] and Star-Coder [248]).While massive code from GitHub has been leveraged to train LLMs for code, addressing complex SE tasks requires more specialized data.The reason is that software is not just about code.For example, valuable data from the whole software development life cycle, such as design, architecture, developer discussions/communications, historical code changes, and even dynamic runtime information, remain largely untapped.Incorporating such data into training could lead to the development of more powerful LLMs for software (not just for code), better suited for the unique demands of SE.These enhanced models could form the foundation for more advanced and capable agent systems designed to tackle a wider range of SE tasks.SE Expertise in Building Agents.Incorporating wellestablished SE expertise into the design of agent systems is crucial.For instance, widely adopted SE techniques can be integrated as tools or sub-components of agent systems.As discussed in Section 5.1.4,some existing agents already leverage SE toolkits and techniques, but many other SE tools and techniques-such as advanced debugging and testing methods-remain underutilized.Further efforts are needed to comprehensively integrate these tools and techniques into agent systems to enhance their functionality.</p>
<p>In addition, SE domain knowledge can guide the workflow of agent systems.As noted in Section 4.6, some agents for end-to-end software development follow traditional software process models, such as the waterfall or agile models.However, many other software process models remain unexplored.Rather than granting agents full autonomy, existing software development and maintenance methodologies can be used to partially control their workflows.For example, as revealed by the recent Agentless study [211] and also further confirmed by OpenAI [215], LLMs using a simplistic workflow based on traditional fault localization and program repair pipelines can even outperform other more complex, fully autonomous agents.This suggests that leveraging domain expertise from SE can potentially help improve the effectiveness, robustness, efficiency, interpretability, and replicability of agentic solutions.</p>
<p>Fig. 1 :
1
Fig. 1: Structure of This Survey</p>
<p>Fig. 3 :Fig. 4 :
34
Fig. 3: Cumulative Number of Papers Over Time</p>
<p>Fig. 5 :
5
Fig. 5: Agent Distribution along Software Development and Maintenance Tasks</p>
<p>• Negotiation: Negotiation plays a crucial role in facilitating communication of different stakeholders and ensuring consistency, especially in conflicting requirements.• Specification: Requirements are determined and documented in a formal format.• Verification: Requirements and models are validated to ensure they fully and unambiguously reflect the intent of stakeholders.• Evolution: Requirements evolution refers to the ongoing process of refining and adapting requirements in response to changing needs and conditions.</p>
<p>Fig. 7 :
7
Fig. 7: Pipeline of LLM-based Agents for Code Generation</p>
<p>Fig. 8 :
8
Fig. 8: Pipeline of LLM-based Agents for Static Bug Detection</p>
<p>Fig. 9 :
9
Fig. 9: Pipeline of LLM-based Agents for Unit Testing</p>
<p>Fig. 10 :
10
Fig. 10: Pipeline of LLM-based Agents for Fault Localization</p>
<p>Fig. 11 :
11
Fig. 11: Pipeline of LLM-based Agents for Program Repair</p>
<ol>
<li>5 . 3
53
Unified Debugging Instead of tackling fault localization or program repair as isolated phases, unified debugging techniques treat them as a unified procedure, which leverages the outputs of each phase to refine each other.In particular, traditional unified debugging techniques [180]-[182] primarily pre-define heuristic rules to refine fault localization based on the patch validation results during program repair.Recently, LLMbased agents have enhanced traditional unified debugging techniques with more flexibility by leveraging LLMs to</li>
</ol>
<p>Fig. 12 :
12
Fig. 12: Process Models Adopted by LLM-based Agents for End-to-end Software Development</p>
<p>Fig. 13 :
13
Fig. 13: Pipeline of LLM-based Agents for End-to-end Software Maintenance</p>
<p>Fig. 14 :
14
Fig. 14: Benchmark Evolution in Software Maintenance</p>
<p>Fig. 15 :
15
Fig. 15: Taxonomy of Planning Strategies in LLM-based Agents for Software Engineering</p>
<p>Fig. 19 :
19
Fig. 19: Multi-agent System Collaboration Mechanisms</p>
<p>TABLE 1 :
1
Statistics of Paper Collection Keyword Hits agent | llm | language model + api 83 agent | llm | language model + bug 98 agent | llm | language model + code 915 agent | llm | language model + coding 70 agent | llm | language model + debug 95 agent | llm | language model + defect 22 agent | llm | language model + deploy 295 agent | llm | language model + evolution 1,349 agent | llm | language model + fault 685 agent | llm | language model + fix 318 agent | llm | language model + maintenance 64 agent | llm | language model + program 1,969 agent | llm | language model + refactor 15 agent | llm | language model + repair 137 agent | llm | language model + requirement 451 agent | llm | language model + software 2,151 agent | llm | language model + test 976 agent | llm | language model + verification 525 agent | llm | language model + vulnerab 144
After manual inspection67After snowballing106</p>
<p>TABLE 2 :
2
Existing LLM-based Agents for Requirements Engineering
AgentsMulti-</p>
<p>Agent Covered RE Phases Elicitation Modeling Negotiation Specification Verification Evolution Elicitron</p>
<p>[54]</p>
<p>Table 3
3
summarizes existing LLM-based agents for code generation with the iterative refinement.A: Model Feedback.Model feedback can be classified into peer-reflection and self-reflection.</p>
<p>TABLE 3 :
3
Existing LLM-based Agents for Code Generation
AgentsMulti-AgentIterative Refinement Model Feedback Tool Feedback Human Feedback Hybrid FeedbackParsel [61]</p>
<p>TABLE 4 :
4
Existing LLM-based Agents for Static Bug Detection
AgentsMulti-AgentTool Utilization Tool Category Specific ToolsDatasetTarget ProgramBug CategoryART [102]× Custom Toolkit ✓ --Self-curatedSmart ContractSmart Contract VulnerabilityContext Splitting ToolICAA [105]✓Custom ToolkitCode Retrieval Tool Document Retrieval ToolNFBugs [106] Self-curatedPython Program Java ProgramNon-functional Bugs API MisusageWeb Search ToolE&amp;V [107]×Static AnalysisClang [108]Sampled syzbot [109]Linux KernelKernel Address Sanitizer BugsLLM4Vuln [110]×Custom ToolkitDatabase Retrieval Tool Context Collection ToolSelf-curatedSmart ContractSmart Contract VulnerabilityLibrary/API Function CallMao et al. [111]✓--SySeVR [112]C/C++ ProgramArithmetic Expression Array UsagePointer UsagePath-TraversalIRIS [113]×Static AnalysisCodeQL [114]CWE-Bench-Java [113]Java ProgramOS Command Injection Cross-Site ScriptingCode InjectionLLIFT [115]×Static AnalysisUBITect [116]Rnd-300 [115]Linux Kernel C ProgramUBI Bugstarget code (e.g., function or variable definitions) throughfunction calls.</p>
<p>TABLE 5 :
5
Existing LLM-based Agents for Code Review CTO, and coder agents cooperate to document the holistic code review process.Experimental results demonstrate the effectiveness and efficiency of CodeAgent in various code review tasks, including consistency analysis, vulnerability analysis, format analysis, and code revision.
AgentsMulti-Agent RolesReview Target Consistency Vulnerability Code Smell Code OptimizationCodeAgent [119]User, CEO, CPO, CTO, Coder, Reviewer✓✓✓✓
Rasheed et al. [120] Code Review, Bug Report, Code Smell, Code Optimization Agent ✓ ✓ ✓ ICAA [105] Context &amp; Prompt Incubation Agent, Consistency Checking Agent, Report Agent ✓ CORE [121] Proposer LLM, Ranker LLM ✓</p>
<p>TABLE 6 :
6
Existing LLM-based Agents for Unit Testing
AgentsMulti-AgentFeedback GoalFeedback SourceTarget LanguageChatTester [122]×Reduce compilation/execution errorsError messagesJava, PythonTestPilot [123]×Reduce compilation/execution errorsError messagesJavaScriptChatUniTest [124]×Reduce compilation/execution errorsError messagesJavaTELPA [125]×Increase coverageProgram analysis resultsPythonCoverUp [126]×Increase coverageExecution results &amp; CoveragePythonMuTAP [127]×Enhance fault detectionSurviving mutantsPythontest refiner of ChatTester performs a more fine-grained re-finement than TestPilot, which analyzes error messages andleverages static analysis tools to localize the buggy code forthe next iteration refinement. Similarly, ChatUniTest [124]adopts a generation-validation-repair mechanism to refinethe tests.B. Iterative Refinement to Increase Coverage.CoverUp</p>
<p>TABLE 7 :
7
Existing LLM-based Agents for System Testing
Software SystemAgentsMulti-AgentTool CategoryToolSpecific ToolsOutputOS KernelKernelGPT [130]×Static Analysissyz-extract [131] LLVM Toolchain [132]Syzkaller SpecificationsCompilerWhiteFox [133]✓--OClint [135]Test CasesLLM4CBI [134]×Static AnalysissrcSlice [136] Gcov [137]Mutated ProgramsFrama-C [138]VirtualBox [140]GPTDroid [139]×Execution Environmentpyvbox [141] Android UIAutomator [142]Test ScriptsMobile AppDroidAgent [144]✓Custom ToolkitAndroid Debug Bridge [143] Navigation Action ToolkitTest ScriptsAXNav [145]✓Custom ToolkitNavigation Action ToolkitBug Replay VideoGenymotion [146]AdbGPT [15]×Execution EnvironmentAndroid UIAutomator2 [147]Bug Replay StepsAndroid Debug Bridge [143]XUAT-Copilot [148]✓--Test ScriptsWeb AppRESTSpecIT [149]×--HTTP RequestsFuzz4All [150]✓--Test CasesUniversalPentestGPT [151] Fang et al. [153]✓ ×Testing Tool Custom ToolkitMetasploit [152] Web Browsering Tool File Creation and Editing ToolTest Operations Exploit ActionsExecution EnvironmentTerminal Code Interpreter
relying on specific GUI states.Liu et al.</p>
<p>TABLE 8 :
8
Existing LLM-based Agents for Fault Localization
AgentsMulti-AgentTool CategoryTools Specific ToolsInput ContextFL GranularityTarget LanguageAgentFL [160]✓Static AnalysisTree-sitter [161]Project LevelMethodJavaCode Analysis ToolRCAgent [162]✓Custom ToolkitLog Analysis Tool Memory Retrieval ToolProject LevelComponentJava, PythonInformation Collection ToolsAUTOFL [163]×Custom ToolkitRepository Retrieval ToolsProject LevelMethodJavaprompt generation, and (ii) the generation LLM for fuzzinginput generation. They are powered by LLMs with differentcapabilities. In the fuzzing loop, the generation LLM refersto the previously generated samples and dynamically ad-justs its strategy, thereby producing diverse fuzzing inputs.Deng et al. [151] design a modular framework, PentestGPT,to conduct Penetration Testing. The system includes infer-ence, generation, and parsing modules. With the planningstrategy of Pentesting Task Tree (which is based on the cy-
[153]curity attack tree[156]) and CoT methods, PentestGPT solves the problems of context loss and inaccurate instruction generation that may be encountered during automated penetration testing.Fang et al.[153]develop a benchmark consisting of 15 one-day vulnerabilities to assess the efficacy of their agent framework in exploiting such weaknesses, utilizing various LLM backbones.Their agents are imbued with an understanding of the Common Vulnerabilities and Exposures (CVE) descriptions and are capable of harnessing a suite of tools to facilitate the exploitation process.These tools include web browsing capabilities for navigation, web search functionalities for traversing web pages, as well as terminal and code interpreter access for the generation and execution of scripts.</p>
<p>TABLE 9 :
9
Existing LLM-based Agents for Program Repair
AgentsMulti-AgentFeedback SourceTarget SoftwareBenchmarkFix RateChatRepair [166]×Execution/CompilationJavaSampled Defects4J [167] &amp; QuixBugs [168]162/337 (Defects4J) 80/80 (QuixBugs)CigaR [169]×Execution/CompilationJavaSampled Defects4J &amp; HumanEval-Java [170]69/267 (Defects4J) 102/162 (HumanEval)RepairAgent [171]×Execution/CompilationJavaDefects4J164/835AutoSD [172]✓ExecutionJava/PythonDefects4J &amp; BugsInPy [173] &amp; Almost-Right HumanEval189/835 (Defects4J) 187/200 (HumanEval)ACFIX [174]✓Static Checking Model DebateSmart ContractSelf-curated Dataset112/118Conversational APR [128]×Execution/CompilationJava/PythonSampled QuixBugs59/60FlakyDoctor [175]✓Execution Static CheckingJavaSampled IDoFT [176] &amp; DexFix dataset [177]&amp; Sampled ODRepair dataset [178]245/419 (Implementation-Dependent Flakiness) 185/247 (Order-Dependent Flakiness)</p>
<p>TABLE 10 :
10
Existing LLM-based Agents for End-to-end Software Development
AgentsMulti-AgentProcess ModelRoles CreationCollaboration ModeCommunication ProtocalSelf-Collaboration [4]✓WaterfallPre-definedOrderedNatural LanguageLow-code LLM [183]✓-Pre-definedOrderedNatural LanguagePrompt Sapper [184]×-Pre-definedOrderedNatural LanguageTalebirad et al. [185]✓-DynamicOrderedNatural LanguageChatDev [186]✓WaterfallPre-definedDual-roleNatural LanguageMetaGPT [187]✓WaterfallPre-definedOrderedStructuredAgentVerse [188]✓-DynamicOrderedNatural LanguageAutoAgents [189]✓-DynamicOrderedNatural LanguageQian et al.</p>
<p>TABLE 11 :
11
Benchmarks for End-to-end Software Development
Benchmarks#TasksInput ScaleOutput ScaleLanguageEvaluated AgentsSRDD [186]1,200Software Description (55 words)Multiple FilesPython</p>
<p>TABLE 12 :
12
Metrics Used in Evaluating Agents for End-toend Software Development
CategoryMetricsUsed AgentsPass RateExecution ValidationPassK Executability</p>
<p>TABLE 13 :
13
Existing LLM-based Agents for End-to-end Software Maintenance
AgentsMulti-AgentPreprocessingIssue Reprod.Issue LocalizationPhasesTask Decomp.Patch GenerationPatch VerificationRankingMAGIS [205]✓××Retrieval-based×w/ local contextCode Review×AUTOCODEROVER [206]✓××Navigation/Spectrum-based×w/ cross-file contextStatic Check×SWE-agent [207]××✓Navigation-based×w/ local contextStatic Check×CodeR [208]✓Plan Selection✓Spectrum-based×w/ cross-file contextDynamic Check×RepoUnderstander [209]✓Knowledge Graph Const.×Simulation✓w/ cross-file contextStatic Check×MASAI [210]✓Test Template Generation✓Navigation-based✓w/ local contextStatic/Dynamic Check✓Agentless [211]×Repository Tree Const.×Navigation-based×w/ local contextStatic/Dynamic Check✓
CONCLUSIONIn this paper, we have presented a comprehensive and systematic survey of 106 papers on LLM-based agents for SE.We analyzed the current research from both the SE and agent perspectives.From the SE perspective, we analyzed how LLM-based agents are applied across different software development and maintenance activities.From the agent perspective, we focus on the design of components in LLM-based agents for SE.In addition, we discussed open challenges and future directions in this critical domain.Code Generation Tool Code Execution ToolBigBench[103]Python Program Code Errors GPTLENS[104]Data Flow Graph IRIS[113], LLM4CBI[134]Code Dependency Graph AgileCoder[198], CodePlan[88]Code Completion Tokens TOOLGEN[89], CodePlan[88],Re-poAgent[227], RRR[77]Code Quality Checking RRR[77], CTC[197], Code-CoT[84], ACFIX[174], etc.
. Metagpt, MapCoder [98], etc. Multi-planner ChatDev [186], MAGIS [205</p>
<p>Chatdev, 186], MAGIS [205], etc. Multi-turn CodeAct [85], CodeAgent [82], XUAT-Copilot [148DroidAgent. 148RCAgent [162], etc. Number of Paths Single-path CodePori [193], AISD [191</p>
<p>. Metagpt, AXNav [145], etc. Multi-path MapCoder [98], LATS [76], CTC [197</p>
<p>Self-Collaboration. Plan Representation Natural Language CoCoST [86], MapCoder [98. Semi-structured AXNav [145], CodeS [195], SoA [97], Parsel [61], MAGIS [205</p>
<p>. Graph Codeplan, 88Pentest-GPT [151], LATS [76] REFERENCES</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Yifan Dong, Chen Du, Yushuo Yang, Zhipeng Chen, Jinhao Chen, Ruiyang Jiang, Yifan Ren, Xinyu Li, Zikang Tang, Peiyu Liu, Jian-Yun Liu, Ji-Rong Nie, Wen, CoRR, abs/2303.18223A survey of large language models. 2023</p>
<p>Large language models for software engineering: A systematic literature review. Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John C Grundy, Haoyu Wang, CoRR, abs/2308.106202023</p>
<p>Large language models for software engineering: Survey and open problems. Angela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho Sengupta, Shin Yoo, Jie M Zhang, IEEE/ACM International Conference on Software Engineering: Future of Software Engineering, ICSE-FoSE 2023. Melbourne, AustraliaIEEEMay 14-20, 2023. 2023</p>
<p>Self-collaboration code generation via chatgpt. Yihong Dong, Xue Jiang, Zhi Jin, Ge Li, CoRR, abs/2304.075902023</p>
<p>Evaluating the code quality of ai-assisted code generation tools: An empirical study on github copilot, amazon codewhisperer, and chatgpt. Isik Burak Yetistiren, Miray Özsoy, Eray T Üz Ayerdem, Ün, CoRR, abs/2304.107782023</p>
<p>Towards enhancing in-context learning for code generation. Jia Li, Yunfei Zhao, Yongmin Li, Ge Li, Zhi Jin, CoRR, abs/2303.177802023</p>
<p>STALL+: boosting llm-based repository-level code completion with static analysis. Junwei Liu, Yixuan Chen, Mingwei Liu, Xin Peng, Yiling Lou, CoRR, abs/2406.100182024</p>
<p>Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, Lingming Zhang, Advances in Neural Information Processing Systems. 202436</p>
<p>Large language models are zero-shot fuzzers: Fuzzing deep-learning libraries via large language models. Yinlin Deng, Chunqiu Steven Xia, Haoran Peng, Chenyuan Yang, Lingming Zhang, 2023</p>
<p>Software testing with large language models: Survey, landscape, and vision. Junjie Wang, Yuchao Huang, Chunyang Chen, Zhe Liu, Song Wang, Qing Wang, IEEE Trans. Software Eng. 5042024</p>
<p>Codamosa: Escaping coverage plateaus in test generation with pre-trained large language models. Caroline Lemieux, Jeevana Priya Inala, K Shuvendu, Siddhartha Lahiri, Sen, 45th IEEE/ACM International Conference on Software Engineering, ICSE 2023. Melbourne, AustraliaIEEEMay 14-20, 2023. 2023</p>
<p>Less training, more repairing please: revisiting automated program repair via zeroshot learning. Chunqiu Steven, Xia , Lingming Zhang, Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering2022</p>
<p>A preliminary evaluation of llm-based fault localization. Sungmin Kang, Gabin An, Shin Yoo, CoRR, abs/2308.054872023</p>
<p>Repair is nearly generation: Multilingual program repair with llms. Harshit Joshi, José Pablo, Cambronero Sánchez, Sumit Gulwani, Gust Vu Le, Ivan Verbruggen, Radicek, Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence. Brian Williams, Yiling Chen, Jennifer Neville, Washington, DC, USAAAAI PressFebruary 7-14, 2023. 20232023</p>
<p>Prompting is all you need: Automated android bug replay with large language models. Sidong Feng, Chunyang Chen, Proceedings of the 46th IEEE/ACM International Conference on Software Engineering, ICSE 2024. the 46th IEEE/ACM International Conference on Software Engineering, ICSE 2024Lisbon, PortugalACMApril 14-20, 2024. 202467</p>
<p>Automated program repair in the era of large pre-trained language models. Chunqiu Steven Xia, Yuxiang Wei, Lingming Zhang, 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE2023</p>
<p>Impact of code language models on automated program repair. Nan Jiang, Kevin Liu, Thibaud Lutellier, Lin Tan, 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE2023</p>
<p>Learning performance-improving code edits. Alexander Shypula, Aman Madaan, Yimeng Zeng, Uri Alon, Jacob R Gardner, Yiming Yang, Milad Hashemi, Graham Neubig, Osbert Parthasarathy Ranganathan, Amir Bastani, Yazdanbakhsh, The Twelfth International Conference on Learning Representations, ICLR 2024. Vienna, AustriaMay 7-11, 2024. 2024OpenReview.net</p>
<p>Ai-assisted coding: Experiments with GPT-4. Thomas Russell A Poldrack, Gasper Lu, Begus, CoRR, abs/2304.131872023</p>
<p>Large language models for compiler optimization. Chris Cummins, Volker Seeker, Dejan Grubisic, Mostafa Elhoushi, Youwei Liang, Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Kim M Hazelwood, Gabriel Synnaeve, Hugh Leather, CoRR, abs/2309.070622023</p>
<p>Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, CoRR, abs/2309.07864Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. The rise and potential of large language model based agents: A survey. 2023</p>
<p>Reinforcement learning agents. Artificial intelligence review. Chcr Ribeiro, 200217</p>
<p>Reinforcement learning: A survey. Leslie Pack, Kaelbling Michael L Littman, Andrew W Moore, Journal of artificial intelligence research. 41996</p>
<p>Steps toward artificial intelligence. Marvin Minsky, Proceedings of the IRE. the IRE196149</p>
<p>A social reinforcement learning agent. Charles Isbell, Christian R Shelton, Michael Kearns, Satinder Singh, Peter Stone, Proceedings of the fifth international conference on Autonomous agents. the fifth international conference on Autonomous agents2001</p>
<p>A survey on large language model based autonomous agents. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, Jirong Wen, Frontiers Comput. Sci. 1861863452024</p>
<p>A survey on the memory mechanism of large language model based agents. Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, Ji-Rong Wen, CoRR, abs/2404.135012024</p>
<p>Large language model based multi-agents: A survey of progress and challenges. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, CoRR, abs/2402.016802024</p>
<p>Exploring large language model based intelligent agents: Definitions, methods, and prospects. Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong, Wenhao Li, Zihao Wang, Zekai Wang, Feng Yin, Junhua Zhao, Xiuqiang He, CoRR, abs/2401.034282024</p>
<p>Augmented language models: a survey. Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru, Roberta Raileanu, Timo Baptiste Rozière, Jane Schick, Asli Dwivedi-Yu, Edouard Celikyilmaz, Yann Grave, Thomas Lecun, Scialom, Trans. Mach. Learn. Res. 20232023</p>
<p>Agent design pattern catalogue: A collection of architectural patterns for foundation model based agents. Yue Liu, Kit Sin, Qinghua Lo, Liming Lu, Dehai Zhu, Xiwei Zhao, Stefan Xu, Jon Harrer, Whittle, CoRR, abs/2405.104672024</p>
<p>Exploring autonomous agents through the lens of large language models: A review. Saikat Barua, CoRR, abs/2404.044422024</p>
<p>A survey on large language models for code generation. Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, Sunghun Kim, CoRR, abs/2406.005152024</p>
<p>Llm-based multiagent systems for software engineering: Vision and the road ahead. Junda He, Christoph Treude, David Lo, arXiv:2404.048342024arXiv preprint</p>
<p>Fairness testing: A comprehensive survey and analysis of trends. Zhenpeng Chen, Jie M Zhang, Max Hort, Mark Harman, Federica Sarro, ACM Trans. Softw. Eng. Methodol. 335592024</p>
<p>A survey of compiler testing. Junjie Chen, Jibesh Patra, Michael Pradel, Yingfei Xiong, Hongyu Zhang, Dan Hao, Lu Zhang, ACM Computing Surveys. 5312020</p>
<p>Finding trends in software research. George Mathew, Amritanshu Agrawal, Tim Menzies, IEEE Trans. Software Eng. 4942023</p>
<p>Empirical research in software engineering -A literature survey. Li Zhang, Jia-Hao Tian, Jing Jiang, Yi-Jun Liu, Meng-Yuan Pu, Tao Yue, J. Comput. Sci. Technol. 3352018</p>
<p>Machine learning testing: Survey, landscapes and horizons. M Jie, Mark Zhang, Lei Harman, Yang Ma, Liu, IEEE Transactions on Software Engineering. 4822022</p>
<p>DBLP. 2024</p>
<p>Opinion mining for software development: A systematic literature review. Bin Lin, Nathan Cassee, Alexander Serebrenik, Gabriele Bavota, Nicole Novielli, Michele Lanza, ACM Trans. Softw. Eng. Methodol. 313412022</p>
<p>Requirements engineering: An overview. Klaus Pohl, 1996Citeseer</p>
<p>Requirements engineering: a roadmap. Bashar Nuseibeh, Steve Easterbrook, Proceedings of the Conference on the Future of Software Engineering. the Conference on the Future of Software Engineering2000</p>
<p>Requirements engineering: a survey. Vivek Shukla, Dhirendra Pandey, Raj Shree, Communications on Applied Electronics. 352015</p>
<p>The unified modeling language. Grady Booch, Ivar Jacobson, James Rumbaugh, Unix Review. 141351996</p>
<p>Entityrelationship-attribute designs and sketches. Theory and Applications of Categories. Michael Johnson, Robert Rosebrugh, Wood, 200210</p>
<p>PRCBERT: prompt learning for requirement classification using bert-based pretrained language models. Xianchang Luo, Yinxing Xue, Zhenchang Xing, Jiamou Sun, 37th IEEE/ACM International Conference on Automated Software Engineering, ASE 2022. Rochester, MI, USAACMOctober 10-14, 2022. 202275</p>
<p>Using llms in software requirements specifications: An empirical evaluation. Madhava Krishna, Bhagesh Gaur, Arsh Verma, Pankaj Jalote, arXiv:2404.178422024arXiv preprint</p>
<p>Empirical evaluation of chatgpt on requirements information retrieval under zero-shot setting. Jianzhang Zhang, Yiyang Chen, Chuang Liu, Nan Niu, Yinglin Wang, 2023 International Conference on Intelligent Computing and Next Generation Networks (ICNGN). IEEE2023</p>
<p>Chatgpt as a tool for user story quality evaluation: Trustworthy out of the box?. Krishna Ronanki, Beatriz Cabrero-Daniel, Christian Berger, International Conference on Agile Software Development. Springer2022</p>
<p>Improving requirements completeness: Automated assistance through large language models. Dipeeka Luitel, Shabnam Hassani, Mehrdad Sabetzadeh, Requirements Engineering. 2912024</p>
<p>Elicitron: An LLM agent-based simulation framework for design requirements elicitation. Mohammadmehdi Ataei, Hyunmin Cheong, Daniele Grandi, Ye Wang, Nigel Morris, Alexander Tessier, CoRR, abs/2404.160452024</p>
<p>Specgen: Automated generation of formal program specifications via large language models. Lezhi Ma, Shangqing Liu, Yi Li, Xiaofei Xie, Lei Bu, arXiv:2401.088072024arXiv preprint</p>
<p>Advancing requirements engineering through generative ai: Assessing the role of llms. Chetan Arora, John Grundy, Mohamed Abdelrazek, Generative AI for Effective Software Development. Springer2024</p>
<p>MARE: multi-agents collaboration framework for requirements engineering. Dongming Jin, Zhi Jin, Xiaohong Chen, Chunhui Wang, CoRR, abs/2405.032562024</p>
<p>An overview of jml tools and applications. Lilian Burdy, Yoonsik Cheon, Michael D David R Cok, Joseph R Ernst, Kiniry, Erik Gary T Leavens, K Rustan M Leino, Poll, International journal on software tools for technology transfer. 72005</p>
<p>Openjml: Jml for java 7 by extending openjdk. David R Cok, NASA Formal Methods: Third International Symposium, NFM 2011. Pasadena, CA, USASpringerApril 18-20, 2011. 20113</p>
<p>Siren's song in the AI ocean: A survey on hallucination in large language models. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, Shuming Shi, CoRR, abs/2309.012192023</p>
<p>Parsel: Algorithmic reasoning with language models by composing decompositions. Eric Zelikman, Qian Huang, Gabriel Poesia, Noah D Goodman, Nick Haber, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems. Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, Sergey Levine, NeurIPS; New Orleans, LA, USA2023. 2023. December 10 -16, 2023, 2023</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, 2023</p>
<p>Jianfeng Gao, and Armando Solar-Lezama. Is self-repair a silver bullet for code generation?. Theo X Olausson, Jeevana Priya Inala, Chenglong Wang, 2024</p>
<p>Autogen: Enabling next-gen llm applications via multiagent conversation. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang, 2023</p>
<p>Intervenor: Prompting the coding ability of large language models with the interactive chain of repair. Hanbin Wang, Zhenghao Liu, Shuo Wang, Ganqu Cui, Ning Ding, Zhiyuan Liu, Ge Yu, 2024</p>
<p>Test-driven development for code generation. Noble Saji, Mathews , Meiyappan Nagappan, CoRR, abs/2402.135212024</p>
<p>Autocoder: Enhancing code large language model with aiev-instruct. Bin Lei, Yuchen Li, Qiuwu Chen, CoRR, abs/2405.149062024</p>
<p>Camel: Communicative agents for "mind" exploration of large language model society. Guohao Li, Hasan Abed, Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem, 2023</p>
<p>More agents is all you need. Junyou Li, Qin Zhang, Yangbin Yu, Qiang Fu, Deheng Ye, 2024</p>
<p>Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization. Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, Diyi Yang, 2023</p>
<p>Teaching large language models to self-debug. Xinyun Chen, Maxwell Lin, Nathanael Schärli, Denny Zhou, 2023</p>
<p>Fully autonomous programming with large language models. Vadim Liventsev, Anastasiia Grishina, Aki Härmä, Leon Moonen, Proceedings of the Genetic and Evolutionary Computation Conference. the Genetic and Evolutionary Computation Conference2023</p>
<p>Test-case-driven programming understanding in large language models for better code generation. Junjie Zhao Tian, Xiangyu Chen, Zhang, 2024</p>
<p>Code generation with alphacodium: From prompt engineering to flow engineering. Tal Ridnik, Dedy Kredo, Itamar Friedman, 2024</p>
<p>LDB: A large language model debugger via verifying runtime execution stepby-step. Lily Zhong, Zilong Wang, Jingbo Shang, CoRR, abs/2402.169062024</p>
<p>Language agent tree search unifies reasoning acting and planning in language models. Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, Yu-Xiong Wang, CoRR, abs/2310.044062023</p>
<p>Class-level code generation from natural language using iterative, tool-enhanced reasoning over repository. Ajinkya Deshpande, Anmol Agarwal, Shashank Shet, Arun Iyer, Aditya Kanade, Ramakrishna Bairi, Suresh Parthasarathy, 2024</p>
<p>Toolcoder: Teach code generation models to use api search tools. Kechi Zhang, Huangzhao Zhang, Ge Li, Jia Li, Zhuo Li, Zhi Jin, 2023</p>
<p>Selfevolve: A code evolution framework via large language models. Shuyang Jiang, Yuhao Wang, Yu Wang, 2023</p>
<p>From misuse to mastery: Enhancing code generation with knowledge-driven ai chaining. Xiaoxue Ren, Xinyuan Ye, Dehai Zhao, Zhenchang Xing, Xiaohu Yang, 2023</p>
<p>Yiheng Xu, Hongjin Su, Chen Xing, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, Zhoujun Cheng, Siheng Zhao, Lingpeng Kong, Bailin Wang, Caiming Xiong, Tao Yu, Lemur: Harmonizing natural language and code for language agents. 2023</p>
<p>Codeagent: Enhancing code generation with tool-integrated agent systems for real-world repo-level coding challenges. Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, Zhi Jin, 2024</p>
<p>Llm4tdd: Best practices for test driven development using large language models. Sanyogita Piya, Allison Sullivan, 2023</p>
<p>Codecot: Tackling code syntax errors in cot reasoning for code generation. Dong Huang, Qingwen Bu, Yuhao Qing, Heming Cui, 2024</p>
<p>Executable code actions elicit better llm agents. Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, Heng Ji, 2024</p>
<p>CONLINE: complex code generation and refinement with online searching and correctness testing. Xinyi He, Jiaru Zou, Yun Lin, Mengyu Zhou, Shi Han, Zejian Yuan, Dongmei Zhang, CoRR, abs/2403.135832024</p>
<p>Intercode: Standardizing and benchmarking interactive coding with execution feedback. John Yang, Akshara Prabhakar, Karthik Narasimhan, Shunyu Yao, 2023</p>
<p>Codeplan: Repository-level coding using llms and planning. Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, D C Vageesh, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B Ashok, Shashank Shet, 2023</p>
<p>Teaching code llms to use autocompletion tools in repository-level code generation. Chong Wang, Jian Zhang, Yebo Feng, Tianlin Li, Weisong Sun, Yang Liu, Xin Peng, 2024</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems. Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, Sergey Levine, NeurIPS; New Orleans, LA, USA2023. 2023. December 10 -16, 2023, 2023</p>
<p>Martin Josifoski, Lars Henning Klein, Maxime Peyrard, Yifei Li, Saibo Geng, Julian Paul Schnitzler, Yuxing Yao, Jiheng Wei, Debjit Paul, Robert West, CoRR, abs/2308.01285Flows: Building blocks of reasoning and collaborating AI. 2023</p>
<p>Mint: Evaluating llms in multiturn interaction with tools and language feedback. Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, Heng Ji, 2024</p>
<p>Codechain: Towards modular code generation through chain of self-revisions with representative sub-modules. Hung Le, Hailin Chen, Amrita Saha, Akash Gokul, Doyen Sahoo, Shafiq Joty, 2024</p>
<p>Clarifygpt: Empowering llm-based code generation with intention clarification. Fangwen Mu, Lin Shi, Song Wang, Zhuohao Yu, Binquan Zhang, Chenxue Wang, Shichao Liu, Qing Wang, 2023</p>
<p>Agentcoder: Multi-agent-based code generation with iterative testing and optimisation. Dong Huang, Jie M Zhang, Michael Luck, Qingwen Bu, Yuhao Qing, Heming Cui, 2024</p>
<p>Gentopia: A collaborative platform for tool-augmented llms. Binfeng Xu, Xukun Liu, Hua Shen, Zeyu Han, Yuhan Li, Murong Yue, Zhiyuan Peng, Yuchen Liu, Ziyu Yao, Dongkuan Xu, 2023</p>
<p>Self-organized agents: A LLM multi-agent framework toward ultra large-scale code generation and optimization. Yoichi Ishibashi, Yoshimasa Nishimura, CoRR, abs/2404.021832024</p>
<p>Mapcoder: Multi-agent code generation for competitive problem solving. Ashraful Md, Mohammed Eunus Islam, Md Rizwan Ali, Parvez, CoRR, abs/2405.114032024</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. S Sanmi Koyejo, A Mohamed, Danielle Agarwal, K Belgrave, A Cho, Oh, NeurIPS; New Orleans, LA, USA2022. 2022. November 28 -December 9, 2022, 2022</p>
<p>Evaluating instruction-tuned large language models on code comprehension and generation. Zhiqiang Yuan, Junwei Liu, Qiancheng Zi, Mingwei Liu, Xin Peng, Yiling Lou, arXiv:2308.012402023arXiv preprint</p>
<p>Xueying Du, Geng Zheng, Kaixin Wang, Jiayi Feng, Wentai Deng, Mingwei Liu, Bihuan Chen, Xin Peng, Tao Ma, Yiling Lou, arXiv:2406.11147Vul-rag: Enhancing llm-based vulnerability detection via knowledge-level rag. 2024arXiv preprint</p>
<p>ART: automatic multi-step reasoning and tool-use for large language models. Bhargavi Paranjape, Scott M Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, Marco T Úlio Ribeiro, CoRR, abs/2303.090142023</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.046152022arXiv preprint</p>
<p>Large language model-powered smart contract vulnerability detection: New perspectives. Sihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Furkan Tekin, Ling Liu, 5th IEEE International Conference on Trust, Privacy and Security in Intelligent Systems and Applications, TPS-ISA 2023. Atlanta, GA, USAIEEENovember 1-4, 2023. 2023</p>
<p>Static code analysis in the AI era: An in-depth exploration of the concept, function, and potential of intelligent code analysis agents. Gang Fan, Xiaoheng Xie, Xunjin Zheng, Yinan Liang, Peng Di, CoRR, abs/2310.088372023</p>
<p>A dataset of non-functional bugs. Aida Radu, Sarah Nadi, Proceedings of the 16th International Conference on Mining Software Repositories, MSR 2019. D Margaret-Anne, Bram Storey, Sonia Adams, Haiduc, the 16th International Conference on Mining Software Repositories, MSR 2019Montreal, Canada26-27 May 2019. 2019</p>
<p>E&amp;v: Prompting large language models to perform static analysis by pseudo-code execution and verification. Yu Hao, Weiteng Chen, Ziqiao Zhou, Weidong Cui, CoRR, abs/2312.084772023</p>
<p>. Clang, 2024</p>
<p>Llm4vuln: A unified evaluation framework for decoupling and enhancing llms' vulnerability reasoning. Yuqiang Sun, Daoyuan Wu, Yue Xue, Han Liu, Wei Ma, Lyuye Zhang, Miaolei Shi, Yang Liu, CoRR, abs/2401.161852024</p>
<p>Multi-role consensus through llms discussions for vulnerability detection. Zhenyu Mao, Jialong Li, Munan Li, Kenji Tei, CoRR, abs/2403.142742024</p>
<p>Sysevr: A framework for using deep learning to detect software vulnerabilities. Zhen Li, Deqing Zou, Shouhuai Xu, Hai Jin, Yawei Zhu, Zhaoxuan Chen, IEEE Trans. Dependable Secur. Comput. 1942022</p>
<p>Llm-assisted static analysis for detecting security vulnerabilities. Ziyang Li, Saikat Dutta, Mayur Naik, 2024</p>
<p>Ql: Object-oriented queries on relational data. Pavel Avgustinov, Oege De Moor, Michael Peyton Jones, Max Schäfer, 30th European Conference on Object-Oriented Programming. 2016. 2016Schloss-Dagstuhl-Leibniz Zentrum f ür Informatik</p>
<p>Enhancing static analysis for practical bug detection: An llm-integrated approach. Haonan Li, Yu Hao, Yizhuo Zhai, Zhiyun Qian, Proc. ACM Program. Lang. 8OOPSLA12024</p>
<p>Ubitect: a precise and scalable method to detect usebefore-initialization bugs in linux kernel. Yizhuo Zhai, Yu Hao, Hang Zhang, Daimeng Wang, Chengyu Song, Zhiyun Qian, Mohsen Lesani, Paul Srikanth V Krishnamurthy, Yu, Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering2020</p>
<p>Comparative assessment of software quality classification techniques: An empirical case study. M Taghi, Naeem Khoshgoftaar, Seliya, Empirical Software Engineering. 92004</p>
<p>Auger: automatically generating review comments with pre-training models. Lingwei Li, Li Yang, Huaxi Jiang, Jun Yan, Tiejian Luo, Zihan Hua, Geng Liang, Chun Zuo, Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering2022</p>
<p>Codeagent: Collaborative agents for software engineering. Daniel Tang, Zhenghan Chen, Kisub Kim, Yewei Song, Haoye Tian, Saad Ezzini, Yongfeng Huang, Jacques Klein, Tegawendé F Bissyandé, CoRR, abs/2402.021722024</p>
<p>Ai-powered code review with llms: Early results. Zeeshan Rasheed, Malik Abdul Sami, Muhammad Waseem, Kai-Kristian Kemell, Xiaofeng Wang, Anh Nguyen, Kari Systä, Pekka Abrahamsson, CoRR, abs/2404.184962024</p>
<p>Nalin Wadhwa, Jui Pradhan, Atharv Sonwane, Surya Prakash Sahu, Nagarajan Natarajan, Aditya Kanade, Suresh Parthasarathy, Sriram K Rajamani, abs/2309.12938Frustrated with code quality issues? llms can help! CoRR. 2023</p>
<p>No more manual tests? evaluating and improving chatgpt for unit test generation. Zhiqiang Yuan, Yiling Lou, Mingwei Liu, Shiji Ding, Kaixin Wang, Yixuan Chen, Xin Peng, 2024</p>
<p>An empirical evaluation of using large language models for automated unit test generation. Max Schäfer, Sarah Nadi, Aryaz Eghbali, Frank Tip, 2023</p>
<p>Chatunitest: a chatgpt-based automated unit test generation tool. Zhuokui Xie, Yinghao Chen, Chen Zhi, Shuiguang Deng, Jianwei Yin, CoRR, abs/2305.047642023</p>
<p>Enhancing llm-based test generation for hard-to-cover branches via program analysis. Chen Yang, Junjie Chen, Bin Lin, Jianyi Zhou, Ziqi Wang, 2024</p>
<p>Coverup: Coverageguided llm-based test generation. Juan Altmayer, Pizzorno , E Berger, ArXiv, abs/2403.162182024</p>
<p>Effective test generation using pre-trained large language models and mutation testing. Arghavan Moradi Dakhel, Amin Nikanjam, Vahid Majdinasab, Foutse Khomh, Michel C Desmarais, 2023</p>
<p>Conversational automated program repair. Chunqiu Steven, Xia , Lingming Zhang, arXiv:2301.132462023arXiv preprint</p>
<p>Slipcover: Near zero-overhead code coverage for python. Juan Altmayer, Pizzorno , Emery D Berger, Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA '23. the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA '23ACMJuly 2023</p>
<p>Kernelgpt: Enhanced kernel fuzzing via large language models. Chenyuan Yang, Zijie Zhao, Lingming Zhang, 2023</p>
<p>Syzkaller. </p>
<p>. The LLVM Compiler Infrastructure. </p>
<p>White-box compiler fuzzing empowered by large language models. Chenyuan Yang, Yinlin Deng, Runyu Lu, Jiayi Yao, Jiawei Liu, Reyhaneh Jabbarvand, Lingming Zhang, 2023</p>
<p>Isolating compiler bugs by generating effective witness programs with large language models. Haoxin Tu, Zhide Zhou, He Jiang, Imam Nur, Bani Yusuf, Yuxian Li, Lingxiao Jiang, IEEE Transactions on Software Engineering. 2024</p>
<p>OCLint. </p>
<p>srcslice: A tool for efficient static forward slicing. Tessandra Christian D Newman, Sage, Hakam W Michael L Collard, Jonathan I Alomari, Maletic, Proceedings of the 38th International Conference on Software Engineering Companion. the 38th International Conference on Software Engineering Companion2016</p>
<p>. Gcov, 2023</p>
<p>. -C Frama, </p>
<p>Make llm a testing expert: Bringing human-like interaction to mobile gui testing via functionality-aware decisions. Zhe Liu, Chunyang Chen, Junjie Wang, Mengzhuo Chen, Boyu Wu, Xing Che, Dandan Wang, Qing Wang, Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. the IEEE/ACM 46th International Conference on Software Engineering2023</p>
<p>Python wrapper of Android uiautomator test tool. 2021</p>
<p>Android Debug, Bridge , Android Developers. 2023</p>
<p>Autonomous large language model agents enabling intent-driven mobile gui testing. Juyeon Yoon, Robert Feldt, Shin Yoo, ArXiv, abs/2311.086492023</p>
<p>Axnav: Replaying accessibility tests from natural language. Maryam Taeb, Amanda Swearngin, Eldon Schoop, Ruijia Cheng, Yue Jiang, Jeffrey Nichols, Proceedings of the CHI Conference on Human Factors in Computing Systems, CHI '24. the CHI Conference on Human Factors in Computing Systems, CHI '24ACMMay 2024</p>
<p>Genymotion -Android Emulator for app testing. 2023</p>
<p>Android Uiautomator2 Python Wrapper. 2023</p>
<p>Zhitao Wang, Wei Wang, Zirao Li, Long Wang, Can Yi, Xinjie Xu, Luyang Cao, Hanjing Su, Shouzhi Chen, Jun Zhou, Xuatcopilot: Multi-agent collaborative system for automated user acceptance testing with large language model. 2024</p>
<p>You can rest now: Automated specification inference and black-box testing of restful apis with large language models. Alix Decrop, Gilles Perrouin, Mike Papadakis, Xavier Devroey, Pierre-Yves Schobbens, 2024</p>
<p>Fuzz4all: Universal fuzzing with large language models. Chunqiu Steven Xia, Matteo Paltenghi, Jia Le Tian, Michael Pradel, Lingming Zhang, 2024</p>
<p>Pentestgpt: An llm-empowered automatic penetration testing tool. Gelei Deng, Yi Liu, Víctor Mayoral-Vilches, Peng Liu, Yuekang Li, Yuan Xu, Tianwei Zhang, Yang Liu, Martin Pinzger, Stefan Rass, 2024</p>
<p>Llm agents can autonomously exploit one-day vulnerabilities. Richard Fang, Rohan Bindu, Akul Gupta, Daniel Kang, 2024</p>
<p>Fill in the blank: Context-aware automated text input generation for mobile GUI testing. Zhe Liu, Chunyang Chen, Junjie Wang, Xing Che, Yuekai Huang, Jun Hu, Qing Wang, 45th IEEE/ACM International Conference on Software Engineering, ICSE 2023. Melbourne, AustraliaIEEEMay 14-20, 2023. 2023</p>
<p>Droidbot-gpt: Gpt-powered UI automation for android. Hongming Hao Wen, Jiaxuan Wang, Yuanchun Liu, Li, CoRR, abs/2304.070612023</p>
<p>Foundations of attack trees. Sjouke Mauw, Martijn Oostdijk, Information Security and Cryptology-ICISC 2005: 8th International Conference. Revised Selected Papers. Seoul, KoreaSpringerDecember 1-2, 2005. 20068</p>
<p>A survey on software fault localization. Eric Wong, Ruizhi Gao, Yihao Li, Rui Abreu, Franz Wotawa, IEEE Transactions on Software Engineering. 4282016</p>
<p>Automatic software repair: A survey. Luca Gazzola, Daniela Micucci, Leonardo Mariani, Proceedings of the 40th International Conference on Software Engineering. the 40th International Conference on Software Engineering2018</p>
<p>A unified debugging approach via llm-based multi-agent synergy. Cheryl Lee, Chunqiu Steven Xia, Jen-Tse Huang, Zhouruixin Zhu, Lingming Zhang, Michael R Lyu, CoRR, abs/2404.171532024</p>
<p>Agentfl: Scaling llm-based fault localization to project-level context. Yihao Qin, Shangwen Wang, Yiling Lou, Jinhao Dong, Kaixin Wang, Xiaoling Li, Xiaoguang Mao, CoRR, abs/2403.163622024</p>
<p>Rcagent: Cloud root cause analysis by autonomous agents with tool-augmented large language models. Zefan Wang, Zichuan Liu, Yingying Zhang, Aoxiao Zhong, Lunting Fan, Lingfei Wu, Qingsong Wen, CoRR, abs/2310.163402023</p>
<p>A preliminary evaluation of llm-based fault localization. Sungmin Kang, Gabin An, Shin Yoo, CoRR, abs/2308.054872023</p>
<p>Deepfl: Integrating multiple fault diagnosis dimensions for deep fault localization. Xia Li, Wei Li, Yuqun Zhang, Lingming Zhang, Proceedings of the 28th ACM SIGSOFT international symposium on software testing and analysis. the 28th ACM SIGSOFT international symposium on software testing and analysis2019</p>
<p>Sequencer: Sequence-to-sequence learning for end-to-end program repair. Zimin Chen, Steve Kommrusch, Michele Tufano, Louis-Noël Pouchet, Denys Poshyvanyk, Martin Monperrus, IEEE Transactions on Software Engineering. 4792019</p>
<p>Automated program repair via conversation: Fixing 162 out of 337 bugs for $0.42 each using chatgpt. Chunqiu Steven, Xia , Lingming Zhang, ISSTA. 2024</p>
<p>Defects4j: A database of existing faults to enable controlled testing studies for java programs. René Just, Darioush Jalali, Michael D Ernst, Proceedings of the 2014 international symposium on software testing and analysis. the 2014 international symposium on software testing and analysis2014</p>
<p>Quixbugs: a multi-lingual program repair benchmark set based on the quixey challenge. Derrick Lin, James Koppel, Angela Chen, Armando Solar-Lezama, Proceedings Companion of the 2017 ACM SIGPLAN International Conference on Systems, Programming, Languages, and Applications: Software for Humanity. Gail C Murphy, Companion of the 2017 ACM SIGPLAN International Conference on Systems, Programming, Languages, and Applications: Software for HumanityVancouver, BC, Canada, OcACM2017. tober 23 -27, 2017. 2017</p>
<p>Cigar: Cost-efficient program repair with llms. Dávid Hidvégi, Khashayar Etemadi, Sofia Bobadilla, Martin Monperrus, CoRR, abs/2402.065982024</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet ; Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Josh Achiam, Vedant Misra, Felipe Petroski Such. Jan Leike,. 2021Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code</p>
<p>Repairagent: An autonomous, llm-based agent for program repair. Islem Bouzenia, T Premkumar, Michael Devanbu, Pradel, CoRR, abs/2403.171342024</p>
<p>Explainable automated debugging via large language model-driven scientific debugging. Sungmin Kang, Bei Chen, Shin Yoo, Jian-Guang Lou, CoRR, abs/2304.021952023</p>
<p>Bugsinpy: a database of existing bugs in python programs to enable controlled testing and debugging studies. Ratnadira Widyasari, Qin Sheng, Camellia Sim, Haodi Lok, Jack Qi, Qijin Phan, Constance Tay, Fiona Tan, Jodie Ethelda Wee, Yuheng Tan, Yieh, Proceedings of the 28th ACM joint meeting on european software engineering conference and symposium on the foundations of software engineering. the 28th ACM joint meeting on european software engineering conference and symposium on the foundations of software engineering2020</p>
<p>ACFIX: guiding llms with mined common RBAC practices for context-aware repair of access control vulnerabilities in smart contracts. Lyuye Zhang, Kaixuan Li, Kairan Sun, Daoyuan Wu, Ye Liu, Haoye Tian, Yang Liu, CoRR, abs/2403.068382024</p>
<p>Flakiness repair in the era of large language models. Yang Chen, Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings. the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings2024</p>
<p>International Dataset of Flaky tests. </p>
<p>TestingResearchIllinois/idoft. </p>
<p>Domain-specific fixes for flaky tests with wrong assumptions on underdetermined specifications. Peilun Zhang, Yanjie Jiang, Anjiang Wei, Victoria Stodden, Darko Marinov, August Shi, 43rd IEEE/ACM International Conference on Software Engineering, ICSE 2021. Madrid, SpainIEEE22-30 May 2021. 2021</p>
<p>Repairing order-dependent flaky tests via test generation. Chengpeng Li, Chenguang Zhu, Wenxi Wang, August Shi, 44th IEEE/ACM 44th International Conference on Software Engineering, ICSE 2022. Pittsburgh, PA, USAACMMay 25-27, 2022. 2022</p>
<p>Why Programs Fail -A Guide to Systematic Debugging. Andreas Zeller, 2009Academic Press2nd Edition</p>
<p>Can automated program repair refine fault localization? a unified debugging approach. Yiling Lou, Ali Ghanbari, Xia Li, Lingming Zhang, Haotian Zhang, Dan Hao, Lu Zhang, ISSTA '20: 29th ACM SIGSOFT International Symposium on Software Testing and Analysis. Sarfraz Khurshid, Corina S Pasareanu, Virtual Event, USAACMJuly 18-22, 2020. 2020</p>
<p>Evaluating and improving unified debugging. Samuel Benton, Xia Li, Yiling Lou, Lingming Zhang, IEEE Trans. Software Eng. 48112022</p>
<p>On the effectiveness of unified debugging: An extensive study on 16 program repair systems. Samuel Benton, Xia Li, Yiling Lou, Lingming Zhang, 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020. Melbourne, AustraliaIEEESeptember 21-25, 2020. 2020</p>
<p>Low-code LLM: visual programming over llms. Yuzhe Cai, Shaoguang Mao, Wenshan Wu, Zehua Wang, Yaobo Liang, Tao Ge, Chenfei Wu, Wang You, Ting Song, Yan Xia, Jonathan Tien, Nan Duan, CoRR, abs/2304.081032023</p>
<p>Prompt sapper: Llm-empowered software engineering infrastructure for ai-native services. Zhenchang Xing, Qing Huang, Yu Cheng, Liming Zhu, Qinghua Lu, Xiwei Xu, CoRR, abs/2306.022302023</p>
<p>Multi-agent collaboration: Harnessing the power of intelligent LLM agents. Yashar Talebirad, Amirhossein Nadiri, CoRR, abs/2306.033142023</p>
<p>Communicative agents for software development. Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, Maosong Sun, CoRR, abs/2307.079242023</p>
<p>Metagpt: Meta programming for a multi-agent collaborative framework. Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka, Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, Schmidhuber, 2023</p>
<p>Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, Zhiyuan Liu, Maosong Sun, Jie Zhou, CoRR, abs/2308.108482023</p>
<p>Autoagents: A framework for automatic agent generation. Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, B , F Karlsson, Jie Fu, Yemin Shi, CoRR, abs/2309.172882023</p>
<p>Experiential co-learning of software-developing agents. Chen Qian, Yufan Dang, Jiahao Li, Wei Liu, Weize Chen, Cheng Yang, Zhiyuan Liu, Maosong Sun, CoRR, abs/2312.170252023</p>
<p>Experimenting a new programming practice with llms. Simiao Zhang, Jiaping Wang, Guoliang Dong, Jun Sun, Yueling Zhang, Geguang Pu, CoRR, abs/2401.010622024</p>
<p>LLM4PLC: harnessing large language models for verifiable programming of plcs in industrial control systems. Mohamad Fakih, Rahul Dharmaji, Yasamin Moghaddas, Gustavo Quiros, Oluwatosin Ogundare, Mohammad Abdullah, Al Faruque, Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice, ICSE-SEIP 2024. the 46th International Conference on Software Engineering: Software Engineering in Practice, ICSE-SEIP 2024Lisbon, PortugalACMApril 14-20, 2024. 2024</p>
<p>Codepori: Large scale model for autonomous software development by using multi-agents. Zeeshan Rasheed, Muhammad Waseem, Mika Saari, Kari Systä, Pekka Abrahamsson, CoRR, abs/2402.014112024</p>
<p>When llm-based code generation meets the software development process. Feng Lin, Dong Jae Kim, Tse-Hsun Chen, CoRR, abs/2403.158522024</p>
<p>Codes: Natural language to code repository via multi-layer sketch. Daoguang Zan, Ailun Yu, Wei Liu, Dong Chen, Bo Shen, Wei Li, Yafen Yao, Yongshun Gong, Xiaolin Chen, Bei Guan, Zhiguang Yang, Yongji Wang, Qianxiang Wang, Lizhen Cui, CoRR, abs/2403.164432024</p>
<p>Iterative experience refinement of softwaredeveloping agents. Chen Qian, Jiahao Li, Yufan Dang, Wei Liu, Yifei Wang, Zihao Xie, Weize Chen, Cheng Yang, Yingli Zhang, Zhiyuan Liu, Maosong Sun, CoRR, abs/2405.042192024</p>
<p>Multi-agent software development through cross-team collaboration. Zhuoyun Du, Chen Qian, Wei Liu, Zihao Xie, Yifei Wang, Yufan Dang, Weize Chen, Cheng Yang, 2024</p>
<p>Minh Huynh Nguyen, Thang Phan Chau, Phong X Nguyen, Nghi, Bui, arXiv:2406.11912Agilecoder: Dynamic collaborative agents for software development based on agile methodology. 2024arXiv preprint</p>
<p>Managing the development of large software systems: Concepts and techniques. W W Royce, Proceedings, 9th International Conference on Software Engineering. William E Riddle, Robert M Balzer, Kouichi Kishida, 9th International Conference on Software EngineeringMonterey, California, USAACM PressMarch 30 -April 2, 1987. 1987</p>
<p>A survey on incremental software development life cycle model. Ha Thakur, Maurya, Int. J. Eng. Technol. Comput. Res. 322016</p>
<p>The unified process. Ivar Jacobson, Grady Booch, James Rumbaugh, Ieee Software. 163961999</p>
<p>Agile software process model. Mikio Aoyama, Proceedings Twenty-First Annual International Computer Software and Applications Conference (COMPSAC'97). Twenty-First Annual International Computer Software and Applications Conference (COMPSAC'97)IEEE1997</p>
<p>Codescore: Evaluating code generation by learning code execution. Yihong Dong, Jiazheng Ding, Xue Jiang, Ge Li, Zhuo Li, Zhi Jin, 2023</p>
<p>Program synthesis with large language models. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, Charles Sutton, 2021</p>
<p>MAGIS: llm-based multi-agent framework for github issue resolution. Wei Tao, Yucheng Zhou, Wenqiang Zhang, Yu Cheng, CoRR, abs/2403.179272024</p>
<p>Autocoderover: Autonomous program improvement. Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, Abhik Roychoudhury, CoRR, abs/2404.054272024</p>
<p>John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, Ofir Press, arXiv:2405.15793Swe-agent: Agent-computer interfaces enable automated software engineering. 2024arXiv preprint</p>
<p>Dong Chen, Shaoxin Lin, Muhan Zeng, Daoguang Zan, Jian-Gang Wang, Anton Cheshkov, Jun Sun, Hao Yu, Guoliang Dong, Artem Aliev, arXiv:2406.01304Issue resolving with multi-agent and task graphs. 2024arXiv preprint</p>
<p>Yingwei Ma, Qingping Yang, Rongyu Cao, Binhua Li, Fei Huang, Yongbin Li, arXiv:2406.01422How to understand whole software repository?. 2024arXiv preprint</p>
<p>Daman Arora, Atharv Sonwane, Nalin Wadhwa, Abhav Mehrotra, Saiteja Utpala, Ramakrishna Bairi, Aditya Kanade, and Nagarajan Natarajan. Masai: Modular architecture for softwareengineering ai agents. 2024</p>
<p>Agentless: Demystifying llm-based software engineering agents. Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, Lingming Zhang, CoRR, abs/2407.014892024</p>
<p>Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. Steve Stephen E Robertson, Walker, SIGIR'94: Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval. Springer1994organised by Dublin City University</p>
<p>Swe-bench: Can language models resolve real-world github issues?. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, Karthik Narasimhan, CoRR, abs/2310.067702023</p>
<p>. Swe-Bench Lite, 2024</p>
<p>Introducing SWE-bench Verified. 2024</p>
<p>Function Calling and other API updates. 2023</p>
<p>GPT-4. 2023</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, R Karthik, Yuan Narasimhan, Cao, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, RwandaMay 1-5, 2023. OpenReview.net, 2023</p>
<p>Low-code llm: Visual programming over llms. Yuzhe Cai, Shaoguang Mao, Wenshan Wu, Zehua Wang, Yaobo Liang, Tao Ge, Chenfei Wu, Wang You, Ting Song, Yan Xia, arXiv:2304.081032023arXiv preprint</p>
<p>Transformer feed-forward layers are key-value memories. Mor Geva, Roei Schuster, Jonathan Berant, Omer Levy, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana. Marie-Francine Moens, Xuanjing Huang, Lucia Specia, Scott , Wen-Tau Yih, the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta CanaDominican Republic7-11 November, 2021. 2021Association for Computational Linguistics</p>
<p>Blackboard systems. Iain D Craig, Artificial Intelligence Review. 221988</p>
<p>Uml diagrams in software engineering research: a systematic literature review. Hatice Koc, ¸ , Ali Mert Erdo Gan, Yousef Barjakly, Serhat Peker, Proceedings. nullMDPI20217413</p>
<p>Seglink++: Detecting dense and arbitrary-shaped scene text by instance-aware component grouping. Pattern recognition. Jun Tang, Zhibo Yang, Yongpan Wang, Qi Zheng, Yongchao Xu, Xiang Bai, 201996106954</p>
<p>A convnet for the 2020s. Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Enchanting program specification synthesis by large language models using static analysis and program verification. Cheng Wen, Jialun Cao, Jie Su, Zhiwu Xu, Shengchao Qin, Mengda He, Haokun Li, Shing-Chi Cheung, Cong Tian, 2024</p>
<p>Repoagent: An llm-powered open-source framework for repository-level code documentation generation. Qinyu Luo, Yining Ye, Shihao Liang, Zhong Zhang, Yujia Qin, Yaxi Lu, Yesai Wu, Xin Cong, Yankai Lin, Yingli Zhang, Xiaoyin Che, Zhiyuan Liu, Maosong Sun, 2024</p>
<p>Cocost: Automatic complex code generation with online searching and correctness testing. Xinyi He, Jiaru Zou, Yun Lin, Mengyu Zhou, Shi Han, Zejian Yuan, Dongmei Zhang, 2024</p>
<p>Seglink++: Detecting dense and arbitrary-shaped scene text by instance-aware component grouping. Pattern Recognition. Jun Tang, Zhibo Yang, Yongpan Wang, Qi Zheng, Yongchao Xu, Xiang Bai, 201996106954</p>
<p>Screen recognition: Creating accessibility metadata for mobile applications from pixels. Xiaoyi Zhang, Lilian De Greef, Amanda Swearngin, Samuel White, Kyle Murray, Lisa Yu, Qi Shan, Jeffrey Nichols, Jason Wu, Chris Fleizach, Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. the 2021 CHI Conference on Human Factors in Computing Systems2021</p>
<p>A convnet for the 2020s. Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Antlr: A predicated-ll(k) parser generator. Software: Practice and Experience. T J Parr, R W Quong, 199525</p>
<p>EclipseJDTLS. </p>
<p>. Black, </p>
<p>The nuxmv symbolic model checker. Roberto Cavada, Alessandro Cimatti, Michele Dorigatti, Alberto Griggio, Alessandro Mariotti, Andrea Micheli, Sergio Mover, Marco Roveri, Stefano Tonetta, Computer Aided Verification. Armin Biere, Roderick Bloem, ChamSpringer International Publishing2014</p>
<p>Slither: a static analysis framework for smart contracts. Josselin Feist, Gustavo Grieco, Alex Groce, IEEE/ACM 2nd International Workshop on Emerging Trends in Software Engineering for Blockchain. 2019. 2019IEEE</p>
<p>GPT-4. 2024</p>
<p>Pynguin: automated unit test generation for python. Stephan Lukasczyk, Gordon Fraser, Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings, ICSE '22. the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings, ICSE '22ACMMay 2022</p>
<p>Mutpy: a mutation testing tool for python 3.x source code. Konrad Hałas, 2019</p>
<p>Gzoltar: an eclipse plug-in for testing and debugging. José Campos, André Riboira, Alexandre Perez, Rui Abreu, Proceedings of the 27th IEEE/ACM international conference on automated software engineering. the 27th IEEE/ACM international conference on automated software engineering2012</p>
<p>Git. </p>
<p>Engineering safety requirements for autonomous driving with large language models. Ali Nouri, Beatriz Cabrero Daniel, Håkan Fredrik T Örner, Christian Sivencrona, Berger, CoRR, abs/2403.162892024</p>
<p>. OpenAI: Introducing ChatGPT. 2022</p>
<p>Deepseek-coder: When the large language model meets programming -the rise of code intelligence. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu, Y K Li, Fuli Luo, Yingfei Xiong, Wenfeng Liang, CoRR, abs/2401.141962024</p>
<p>Starcoder 2 and the stack v2: The next generation. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, arXiv:2402.191732024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
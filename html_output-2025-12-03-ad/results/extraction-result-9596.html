<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9596 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9596</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9596</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-166.html">extraction-schema-166</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-389e6eca925821e4833beee8ddb4459a26241db3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/389e6eca925821e4833beee8ddb4459a26241db3" target="_blank">Are large language models superhuman chemists?</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> An automated framework for evaluating the chemical knowledge and reasoning abilities of state-of-the-art LLMs against the expertise of chemists, which reveals LLMs' impressive chemical capabilities while emphasizing the need for further research to improve their safety and usefulness.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have gained widespread interest due to their ability to process human language and perform tasks on which they have not been explicitly trained. However, we possess only a limited systematic understanding of the chemical capabilities of LLMs, which would be required to improve models and mitigate potential harm. Here, we introduce"ChemBench,"an automated framework for evaluating the chemical knowledge and reasoning abilities of state-of-the-art LLMs against the expertise of chemists. We curated more than 2,700 question-answer pairs, evaluated leading open- and closed-source LLMs, and found that the best models outperformed the best human chemists in our study on average. However, the models struggle with some basic tasks and provide overconfident predictions. These findings reveal LLMs' impressive chemical capabilities while emphasizing the need for further research to improve their safety and usefulness. They also suggest adapting chemistry education and show the value of benchmarking frameworks for evaluating LLMs in specific domains.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9596.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9596.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemBench benchmarking framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curated, annotated benchmark corpus and evaluation framework introduced in this paper to measure LLM chemical knowledge, reasoning, and calculation across diverse chemistry topics using text-completion outputs and special semantic annotations for molecules and equations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChemBench (framework, not an LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not an LLM — a dataset and evaluation framework comprising annotated question-answer pairs, prompt/parse presets, and tooling to evaluate text-completion systems and tool-augmented agents.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (broad subfields: general, organic, inorganic, analytical, toxicology, materials, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>N/A (framework designed to evaluate LLMs on question answering, reasoning, and extraction tasks; intended to support evaluation of extraction of equations/structured entities via annotations)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Curated corpus of 2,788 Q/A pairs (1,039 manual, 1,749 semi-automatic) annotated by topic, required skills (knowledge, reasoning, calculation, intuition), difficulty, and semantic tags (e.g., [START_SMILES]...[END_SMILES], LaTeX/ETEX handling). Evaluations operate on text completions; parsing pipeline uses regex, word-to-number conversion, and fallback to an LLM for parsing when needed.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Corpus: 2,788 question-answer pairs assembled from exams, textbooks, semi-automatic generation from cheminformatics sources, web resources, and curated datasets; includes a 236-question ChemBench-Mini subset for cheaper routine tests. Questions are annotated to allow special handling of molecules, units, and equations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Strict binary correctness on questions (correct/incorrect), aggregated by topic and skill; human baseline collected via custom web app; models evaluated using greedy decoding; additional analyses include confidence calibration and topic-wise/per-molecule descriptor analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Framework was used to evaluate many leading LLMs and tool-augmented systems; it revealed strengths and limits of LLMs across chemistry tasks, but the paper does not report any instance of using ChemBench to distill new quantitative laws from literature — rather it evaluates QA and extraction/ reasoning capabilities. The framework explicitly supports special encoding of equations and chemical entities to enable future extraction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Semantic annotation feature allowing special handling of SMILES and equation fragments (e.g., [START_SMILES]...[END_SMILES], ETEX handling) to enable more reliable parsing of model outputs; ChemBench-Mini as a representative subset.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>ChemBench itself is an evaluation framework rather than a law-discovery model; it evaluates text completions and so inherits limitations of models under test (hallucination, safety refusals, inability to access specialized structured databases). The paper notes that current LLMs still struggle with certain reasoning tasks (e.g., NMR peak counts) and knowledge-intensive queries whose facts are not readily present in primary literature but instead require specialized databases.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Human baseline of 19 experts on a subset; leading LLMs outperformed average humans on many question types, but the framework found systematic weakness for knowledge-intensive tasks and poor calibration of confidence estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are large language models superhuman chemists?', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9596.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9596.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaperQA2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaperQA2 (agentic literature-search system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agentic system that augments an LLM with literature search capabilities to answer questions by retrieving and using information from papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaperQA2 (agentic, literature-search agent)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described as an agentic system that can search the literature to obtain answers; used in this study (accessed via FutureHouse API) as a tool-augmented system wrapping an LLM; internal architecture and parameterization not specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (used here to answer chemistry Q/A from literature)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Intended for extraction/use of factual information from papers (could support distillation of relationships if applied), but no concrete law-distillation experiments reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Tool-augmented LLM agent performing retrieval over research papers (literature search) to ground answers; effectively a retrieval-augmented/agentic approach operating on text completions returned to the ChemBench evaluator.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in detail in this paper — described generally as 'the literature' accessible to PaperQA2 (operated in August 2024 via FutureHouse API). The paper notes PaperQA2 only has access to papers as external knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>PaperQA2 was evaluated on ChemBench (including ChemBench-Mini) using the same correctness criterion as other models; performance compared to other LLMs and human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>PaperQA2 was included among evaluated systems; the authors report that retrieval-augmented generation via PaperQA2 did not overcome limitations on knowledge-intensive questions, because some required facts were not easily accessible in papers but resided in specialized databases used by humans (e.g., PubChem, Gestis). No examples of successful distillation of quantitative laws from literature are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Used as an example of a literature-search augmented agentic system that can be evaluated by ChemBench; highlighted as insufficient to supply specialized database facts for some safety/knowledge queries.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Limited by scope of accessible literature; failed to address knowledge gaps when required facts are in specialized databases rather than in papers; evaluation operated on final text completions so internal token-level probabilities of the agent are not observable; safety filters and provider-specific limitations may also reduce accessible content.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared directly to other LLMs and humans on ChemBench; authors conclude RAG/PaperQA2 did not meaningfully resolve certain knowledge deficiencies compared to humans using specialized databases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are large language models superhuman chemists?', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9596.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9596.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Galactica model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scientific LLM that uses special encoding procedures for molecules and equations to better handle scientific text and symbolic content.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described in the paper as a model that supports special encoding for molecular strings and equations (e.g., special handling of SMILES and LaTeX/ETEX), though the paper does not report running Galactica-specific experiments nor provide model size/architecture details.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific text broadly, including chemistry (molecules, equations, reactions).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Designed to better handle equations and symbolic scientific notation (enables extraction/representation of mathematical relationships), but no direct law-discovery experiments are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Model-level special tokenization/encoding for structured scientific modalities, allowing models to treat molecules and equations differently from ordinary text; the paper's ChemBench dataset includes annotations to be compatible with such models.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in this paper; Galactica is cited as an example of a model that handles modalities specially — ChemBench stores semantically annotated questions so models like Galactica can be evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not evaluated in a law-distillation context in this work; mentioned to motivate ChemBench's semantic tagging to allow evaluation of models that expect special encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>No law-distillation results reported for Galactica in this work; Galactica is offered as an example motivating the need for semantic annotations (molecules/equations) in benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Reference to special wrappers and ETEX handling to enable compatibility with models that expect structured scientific tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Existing benchmarks typically do not account for special treatment of equations/molecules; without semantic annotations, models like Galactica cannot be fairly or fully evaluated. The paper stresses the need for annotated inputs to evaluate such models.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not applicable in the paper for law-distillation; Galactica is a motivating example rather than an evaluated baseline here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are large language models superhuman chemists?', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9596.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9596.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of methods that augment LLMs with a retrieval component that fetches documents (e.g., papers, web pages) for grounding model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A general approach (not a single model) combining retrieval of external text and generative LLM completions; referenced as a commonly used technique (e.g., PaperQA2 is an agentic RAG-like system).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General; in this paper applied conceptually to chemistry information extraction from literature.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Potentially useful for extracting quantitative relationships from text (empirical equations, numerical facts) by grounding generation on retrieved documents; no concrete law-distillation experiments reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Augment an LLM with a retrieval step over relevant documents (papers). The paper evaluated RAG-like agent (PaperQA2) on ChemBench, comparing its answers to other models and human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Depends on system; in the PaperQA2 example, the retrieval corpus is the available literature accessible to the agent (not otherwise specified).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>In this work RAG systems were evaluated by their final text outputs on ChemBench QA tasks (binary correctness).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The authors report that RAG (as exemplified by PaperQA2) did not solve the shortcomings for knowledge-intensive chemistry questions — many required facts were in specialized databases rather than in papers, limiting the benefit of literature-only retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>PaperQA2 used as an instance of a RAG/agentic literature-search system; authors explicitly state RAG did not overcome failures tied to missing specialized database knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>RAG effectiveness is constrained by the retrieval corpus: if required quantitative facts are not present in papers but exist in specialized databases, RAG over literature fails to supply correct answers; parsing/formatting and provider safety refusals further limit usable output; final text-only evaluation hides internal retrieval signals.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared implicitly to human experts who used specialized databases (PubChem, Gestis) — humans outperformed PaperQA2 on knowledge-intensive tasks where databases were needed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are large language models superhuman chemists?', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9596.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9596.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs-general</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models (general: GPT-4, Claude, Llama variants, o1, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>General-purpose large language models discussed throughout the paper as tools that can process scientific text, perform reasoning tasks, and be augmented with tools for literature search and calculation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs (GPT-4, Claude-3.5/4, Llama-3.1-405B-Instruct, o1, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The paper evaluates a range of proprietary and open-source LLMs (some named: GPT-4, Claude-3.5, Llama-3.1-405B-Instruct, o1) using greedy decoding; specific architectures and parameter counts are only given implicitly in model names (e.g., '405B' in Llama-3.1-405B-Instruct). Many models include safety filters and API access only to text completions.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (primary focus here), but LLMs are general and have been applied to other scientific domains as noted in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Potential to extract or distill empirical relationships and equations from text (discussed as a motivation), but this paper does not present direct experiments where LLMs were used to discover new quantitative laws from large corpora of scholarly papers.</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Approaches discussed include prompting, instruction-tuning, tool-augmentation (e.g., RAG, literature search), special token encodings for molecules/equations, and verbalized confidence elicitation. The paper used controlled prompts and parsing to obtain answers, and sometimes asked models to provide confidence on an ordinal scale.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Models were evaluated on the ChemBench question set; their training corpora are not specified in this paper. For tool-augmented variants, external literature or other tools may have been used (e.g., PaperQA2).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Models were evaluated on ChemBench (binary correct/incorrect per question) and compared to human experts; additional analyses include topic breakdowns, dependence on molecular complexity descriptors, and confidence calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Leading LLMs achieved strong performance on many chemistry tasks (the top model 'o1' outperformed the best human in the study on average), but models struggled with knowledge-intensive questions, tasks requiring deep structural chemical reasoning (e.g., NMR peak counting), and provided poorly calibrated or misleading verbalized confidence estimates. No instances of automated distillation of new quantitative scientific laws from scholarly paper corpora are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Models gave correct answers on many textbook-style and knowledge-lite tasks, but failed on tasks needing specialist databases or structure-based reasoning. The paper shows explicit examples where models refused or were blocked on safety-related questions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Key challenges: hallucination and overconfidence; missing specialized structured data in papers (so literature-only approaches can fail); inability to reliably reason about chemical structure/topology from SMILES for some analytical tasks; provider safety filters that prevent retrieval/answering on certain topics; inconsistent confidence calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against a human baseline of chemistry experts; while some LLMs outperformed average humans across the benchmark, humans using specialized databases outperformed LLMs on knowledge-intensive queries. RAG over literature did not close that gap per the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are large language models superhuman chemists?', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>GPT-4 Technical Report <em>(Rating: 2)</em></li>
                <li>Autonomous chemical research with large language models <em>(Rating: 2)</em></li>
                <li>Augmenting large language models with chemistry tools <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9596",
    "paper_id": "paper-389e6eca925821e4833beee8ddb4459a26241db3",
    "extraction_schema_id": "extraction-schema-166",
    "extracted_data": [
        {
            "name_short": "ChemBench",
            "name_full": "ChemBench benchmarking framework",
            "brief_description": "A curated, annotated benchmark corpus and evaluation framework introduced in this paper to measure LLM chemical knowledge, reasoning, and calculation across diverse chemistry topics using text-completion outputs and special semantic annotations for molecules and equations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChemBench (framework, not an LLM)",
            "model_description": "Not an LLM — a dataset and evaluation framework comprising annotated question-answer pairs, prompt/parse presets, and tooling to evaluate text-completion systems and tool-augmented agents.",
            "scientific_domain": "Chemistry (broad subfields: general, organic, inorganic, analytical, toxicology, materials, etc.)",
            "law_type": "N/A (framework designed to evaluate LLMs on question answering, reasoning, and extraction tasks; intended to support evaluation of extraction of equations/structured entities via annotations)",
            "method_description": "Curated corpus of 2,788 Q/A pairs (1,039 manual, 1,749 semi-automatic) annotated by topic, required skills (knowledge, reasoning, calculation, intuition), difficulty, and semantic tags (e.g., [START_SMILES]...[END_SMILES], LaTeX/ETEX handling). Evaluations operate on text completions; parsing pipeline uses regex, word-to-number conversion, and fallback to an LLM for parsing when needed.",
            "input_corpus_description": "Corpus: 2,788 question-answer pairs assembled from exams, textbooks, semi-automatic generation from cheminformatics sources, web resources, and curated datasets; includes a 236-question ChemBench-Mini subset for cheaper routine tests. Questions are annotated to allow special handling of molecules, units, and equations.",
            "evaluation_method": "Strict binary correctness on questions (correct/incorrect), aggregated by topic and skill; human baseline collected via custom web app; models evaluated using greedy decoding; additional analyses include confidence calibration and topic-wise/per-molecule descriptor analyses.",
            "results_summary": "Framework was used to evaluate many leading LLMs and tool-augmented systems; it revealed strengths and limits of LLMs across chemistry tasks, but the paper does not report any instance of using ChemBench to distill new quantitative laws from literature — rather it evaluates QA and extraction/ reasoning capabilities. The framework explicitly supports special encoding of equations and chemical entities to enable future extraction tasks.",
            "notable_examples": "Semantic annotation feature allowing special handling of SMILES and equation fragments (e.g., [START_SMILES]...[END_SMILES], ETEX handling) to enable more reliable parsing of model outputs; ChemBench-Mini as a representative subset.",
            "limitations_challenges": "ChemBench itself is an evaluation framework rather than a law-discovery model; it evaluates text completions and so inherits limitations of models under test (hallucination, safety refusals, inability to access specialized structured databases). The paper notes that current LLMs still struggle with certain reasoning tasks (e.g., NMR peak counts) and knowledge-intensive queries whose facts are not readily present in primary literature but instead require specialized databases.",
            "baseline_comparison": "Human baseline of 19 experts on a subset; leading LLMs outperformed average humans on many question types, but the framework found systematic weakness for knowledge-intensive tasks and poor calibration of confidence estimates.",
            "uuid": "e9596.0",
            "source_info": {
                "paper_title": "Are large language models superhuman chemists?",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "PaperQA2",
            "name_full": "PaperQA2 (agentic literature-search system)",
            "brief_description": "An agentic system that augments an LLM with literature search capabilities to answer questions by retrieving and using information from papers.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaperQA2 (agentic, literature-search agent)",
            "model_description": "Described as an agentic system that can search the literature to obtain answers; used in this study (accessed via FutureHouse API) as a tool-augmented system wrapping an LLM; internal architecture and parameterization not specified in the paper.",
            "scientific_domain": "Chemistry (used here to answer chemistry Q/A from literature)",
            "law_type": "Intended for extraction/use of factual information from papers (could support distillation of relationships if applied), but no concrete law-distillation experiments reported in this paper.",
            "method_description": "Tool-augmented LLM agent performing retrieval over research papers (literature search) to ground answers; effectively a retrieval-augmented/agentic approach operating on text completions returned to the ChemBench evaluator.",
            "input_corpus_description": "Not specified in detail in this paper — described generally as 'the literature' accessible to PaperQA2 (operated in August 2024 via FutureHouse API). The paper notes PaperQA2 only has access to papers as external knowledge.",
            "evaluation_method": "PaperQA2 was evaluated on ChemBench (including ChemBench-Mini) using the same correctness criterion as other models; performance compared to other LLMs and human experts.",
            "results_summary": "PaperQA2 was included among evaluated systems; the authors report that retrieval-augmented generation via PaperQA2 did not overcome limitations on knowledge-intensive questions, because some required facts were not easily accessible in papers but resided in specialized databases used by humans (e.g., PubChem, Gestis). No examples of successful distillation of quantitative laws from literature are reported.",
            "notable_examples": "Used as an example of a literature-search augmented agentic system that can be evaluated by ChemBench; highlighted as insufficient to supply specialized database facts for some safety/knowledge queries.",
            "limitations_challenges": "Limited by scope of accessible literature; failed to address knowledge gaps when required facts are in specialized databases rather than in papers; evaluation operated on final text completions so internal token-level probabilities of the agent are not observable; safety filters and provider-specific limitations may also reduce accessible content.",
            "baseline_comparison": "Compared directly to other LLMs and humans on ChemBench; authors conclude RAG/PaperQA2 did not meaningfully resolve certain knowledge deficiencies compared to humans using specialized databases.",
            "uuid": "e9596.1",
            "source_info": {
                "paper_title": "Are large language models superhuman chemists?",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Galactica",
            "name_full": "Galactica model",
            "brief_description": "A scientific LLM that uses special encoding procedures for molecules and equations to better handle scientific text and symbolic content.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Galactica",
            "model_description": "Described in the paper as a model that supports special encoding for molecular strings and equations (e.g., special handling of SMILES and LaTeX/ETEX), though the paper does not report running Galactica-specific experiments nor provide model size/architecture details.",
            "scientific_domain": "Scientific text broadly, including chemistry (molecules, equations, reactions).",
            "law_type": "Designed to better handle equations and symbolic scientific notation (enables extraction/representation of mathematical relationships), but no direct law-discovery experiments are reported here.",
            "method_description": "Model-level special tokenization/encoding for structured scientific modalities, allowing models to treat molecules and equations differently from ordinary text; the paper's ChemBench dataset includes annotations to be compatible with such models.",
            "input_corpus_description": "Not specified in this paper; Galactica is cited as an example of a model that handles modalities specially — ChemBench stores semantically annotated questions so models like Galactica can be evaluated.",
            "evaluation_method": "Not evaluated in a law-distillation context in this work; mentioned to motivate ChemBench's semantic tagging to allow evaluation of models that expect special encodings.",
            "results_summary": "No law-distillation results reported for Galactica in this work; Galactica is offered as an example motivating the need for semantic annotations (molecules/equations) in benchmarks.",
            "notable_examples": "Reference to special wrappers and ETEX handling to enable compatibility with models that expect structured scientific tokens.",
            "limitations_challenges": "Existing benchmarks typically do not account for special treatment of equations/molecules; without semantic annotations, models like Galactica cannot be fairly or fully evaluated. The paper stresses the need for annotated inputs to evaluate such models.",
            "baseline_comparison": "Not applicable in the paper for law-distillation; Galactica is a motivating example rather than an evaluated baseline here.",
            "uuid": "e9596.2",
            "source_info": {
                "paper_title": "Are large language models superhuman chemists?",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation (RAG)",
            "brief_description": "A class of methods that augment LLMs with a retrieval component that fetches documents (e.g., papers, web pages) for grounding model outputs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Retrieval-Augmented Generation (RAG)",
            "model_description": "A general approach (not a single model) combining retrieval of external text and generative LLM completions; referenced as a commonly used technique (e.g., PaperQA2 is an agentic RAG-like system).",
            "scientific_domain": "General; in this paper applied conceptually to chemistry information extraction from literature.",
            "law_type": "Potentially useful for extracting quantitative relationships from text (empirical equations, numerical facts) by grounding generation on retrieved documents; no concrete law-distillation experiments reported in this paper.",
            "method_description": "Augment an LLM with a retrieval step over relevant documents (papers). The paper evaluated RAG-like agent (PaperQA2) on ChemBench, comparing its answers to other models and human experts.",
            "input_corpus_description": "Depends on system; in the PaperQA2 example, the retrieval corpus is the available literature accessible to the agent (not otherwise specified).",
            "evaluation_method": "In this work RAG systems were evaluated by their final text outputs on ChemBench QA tasks (binary correctness).",
            "results_summary": "The authors report that RAG (as exemplified by PaperQA2) did not solve the shortcomings for knowledge-intensive chemistry questions — many required facts were in specialized databases rather than in papers, limiting the benefit of literature-only retrieval.",
            "notable_examples": "PaperQA2 used as an instance of a RAG/agentic literature-search system; authors explicitly state RAG did not overcome failures tied to missing specialized database knowledge.",
            "limitations_challenges": "RAG effectiveness is constrained by the retrieval corpus: if required quantitative facts are not present in papers but exist in specialized databases, RAG over literature fails to supply correct answers; parsing/formatting and provider safety refusals further limit usable output; final text-only evaluation hides internal retrieval signals.",
            "baseline_comparison": "Compared implicitly to human experts who used specialized databases (PubChem, Gestis) — humans outperformed PaperQA2 on knowledge-intensive tasks where databases were needed.",
            "uuid": "e9596.3",
            "source_info": {
                "paper_title": "Are large language models superhuman chemists?",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LLMs-general",
            "name_full": "Large language models (general: GPT-4, Claude, Llama variants, o1, etc.)",
            "brief_description": "General-purpose large language models discussed throughout the paper as tools that can process scientific text, perform reasoning tasks, and be augmented with tools for literature search and calculation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Various LLMs (GPT-4, Claude-3.5/4, Llama-3.1-405B-Instruct, o1, etc.)",
            "model_description": "The paper evaluates a range of proprietary and open-source LLMs (some named: GPT-4, Claude-3.5, Llama-3.1-405B-Instruct, o1) using greedy decoding; specific architectures and parameter counts are only given implicitly in model names (e.g., '405B' in Llama-3.1-405B-Instruct). Many models include safety filters and API access only to text completions.",
            "scientific_domain": "Chemistry (primary focus here), but LLMs are general and have been applied to other scientific domains as noted in related work.",
            "law_type": "Potential to extract or distill empirical relationships and equations from text (discussed as a motivation), but this paper does not present direct experiments where LLMs were used to discover new quantitative laws from large corpora of scholarly papers.",
            "method_description": "Approaches discussed include prompting, instruction-tuning, tool-augmentation (e.g., RAG, literature search), special token encodings for molecules/equations, and verbalized confidence elicitation. The paper used controlled prompts and parsing to obtain answers, and sometimes asked models to provide confidence on an ordinal scale.",
            "input_corpus_description": "Models were evaluated on the ChemBench question set; their training corpora are not specified in this paper. For tool-augmented variants, external literature or other tools may have been used (e.g., PaperQA2).",
            "evaluation_method": "Models were evaluated on ChemBench (binary correct/incorrect per question) and compared to human experts; additional analyses include topic breakdowns, dependence on molecular complexity descriptors, and confidence calibration.",
            "results_summary": "Leading LLMs achieved strong performance on many chemistry tasks (the top model 'o1' outperformed the best human in the study on average), but models struggled with knowledge-intensive questions, tasks requiring deep structural chemical reasoning (e.g., NMR peak counting), and provided poorly calibrated or misleading verbalized confidence estimates. No instances of automated distillation of new quantitative scientific laws from scholarly paper corpora are reported.",
            "notable_examples": "Models gave correct answers on many textbook-style and knowledge-lite tasks, but failed on tasks needing specialist databases or structure-based reasoning. The paper shows explicit examples where models refused or were blocked on safety-related questions.",
            "limitations_challenges": "Key challenges: hallucination and overconfidence; missing specialized structured data in papers (so literature-only approaches can fail); inability to reliably reason about chemical structure/topology from SMILES for some analytical tasks; provider safety filters that prevent retrieval/answering on certain topics; inconsistent confidence calibration.",
            "baseline_comparison": "Compared against a human baseline of chemistry experts; while some LLMs outperformed average humans across the benchmark, humans using specialized databases outperformed LLMs on knowledge-intensive queries. RAG over literature did not close that gap per the authors.",
            "uuid": "e9596.4",
            "source_info": {
                "paper_title": "Are large language models superhuman chemists?",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "GPT-4 Technical Report",
            "rating": 2,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "Autonomous chemical research with large language models",
            "rating": 2,
            "sanitized_title": "autonomous_chemical_research_with_large_language_models"
        },
        {
            "paper_title": "Augmenting large language models with chemistry tools",
            "rating": 2,
            "sanitized_title": "augmenting_large_language_models_with_chemistry_tools"
        }
    ],
    "cost": 0.01468425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Are large language models superhuman chemists?</h1>
<p>Adrian Mirza ${ }^{1,2, <em>}$, Nawaf Alampara ${ }^{1, </em>}$, Sreekanth Kunchapu ${ }^{1, *}$, Martin̄o Rios-Garcia ${ }^{1,3}$, , Benedict Emoekabu, Aswanth Krishnan ${ }^{4}$, Tanya Gupta ${ }^{5,6}$, Mara Schilling-Wilhelmi ${ }^{1}$, Macjonathan Okereke ${ }^{1}$, Anagha Aneesh ${ }^{1}$, Mehrdad Asgari ${ }^{7}$, Juliane Eberhardt ${ }^{8}$, Amir Mohammad Elahi ${ }^{9}$, Hani M. Elbeheiry ${ }^{10}$, María Victoria Gil ${ }^{3}$, Christina Glaubitz (1), Maximilian Greiner ${ }^{1}$, Caroline T. Holick ${ }^{1,14}$, Tim Hoffmann ${ }^{1,14}$, Abdelrahman Ibrahim ${ }^{1}$, Lea C. Klepsch ${ }^{1,14}$, Yannik Köster ${ }^{1}$, Fabian Alexander Kreth ${ }^{11,12}$, Jakob Meyer ${ }^{1}$, Santiago Miret ${ }^{13}$, Jan Matthias Peschel ${ }^{1}$, Michael Ringleb ${ }^{1,14}$, Nicole Roesner ${ }^{1,14}$, Johanna Schreiber ${ }^{1,14}$, Ulrich S. Schubert ${ }^{1,2,10,14}$, Leanne M. Stafast ${ }^{1,14}$, Dinga Wonanke ${ }^{15}$, Michael Pieler ${ }^{16,17}$, Philippe Schwaller ${ }^{5,6}$, and Kevin Maik Jablonka ${ }^{1,2,11,14, \boxtimes}$</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>November 4, 2024</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have gained widespread interest due to their ability to process human language and perform tasks on which they have not been explicitly trained.</p>
<p>However, we possess only a limited systematic understanding of the chemical capabilities of LLMs, which would be required to improve models and mitigate potential harm. Here, we introduce "ChemBench," an automated framework for evaluating the chemical knowledge and reasoning abilities of state-of-the-art LLMs against the expertise of chemists.</p>
<p>We curated more than 2,700 question-answer pairs, evaluated leading openand closed-source LLMs, and found that the best models outperformed the best human chemists in our study on average. However, the models struggle with some basic tasks and provide overconfident predictions.</p>
<p>These findings reveal LLMs' impressive chemical capabilities while emphasizing the need for further research to improve their safety and usefulness. They also suggest adapting chemistry education and show the value of benchmarking frameworks for evaluating LLMs in specific domains.</p>
<h1>1 Introduction</h1>
<p>Large language models (LLMs) are machine learning (ML) models trained on massive amounts of text to complete sentences. Aggressive scaling of these models has led to a rapid increase in their capabilities, ${ }^{1,2}$ with the leading models now being able to pass the United States Medical Licensing Examination ${ }^{3}$ or other professional licensing exams. They also have been shown to design and autonomously perform chemical reactions when augmented with external tools such as web search and synthesis planners. ${ }^{4-7}$ While some see "sparks of artificial general intelligence (AGI)" in them, ${ }^{8}$ others consider them as "stochastic parrots"-i.e., systems that only regurgitate what they have been trained on ${ }^{9}$ and that show inherent limitations due to the way they are trained. ${ }^{10}$ Nevertheless, the promise of these models is that they have shown the ability to solve a wide variety of tasks they have not been explicitly trained on. ${ }^{11-13}$</p>
<p>Chemists and materials scientists have quickly caught on to the mounting attention given to LLMs, with some voices even suggesting that "the future of chemistry is language." ${ }^{14}$ This statement is motivated by a growing number of reports that use LLMs to predict properties of molecules or materials, ${ }^{2,15-19}$ optimize reactions, ${ }^{20,21}$ generate materials, ${ }^{22-25}$ extract information, ${ }^{26-33}$ or to even prototype systems that can autonomously perform experiments in the physical world based on commands provided in natural language. ${ }^{5-7}$</p>
<p>In addition, since a lot-if not most-of the information about chemistry is currently stored and communicated in text, there is a strong reason to believe that there is still a lot of untapped potential in LLMs for chemistry and materials science. ${ }^{34}$ For instance, most insights in chemical research do not directly originate from data stored in databases but rather from the scientists interpreting the data. Many of these insights are in the form of text in scientific publications. Thus, operating on such texts might be our best way of unlocking these insights and learning from them. This might ultimately lead to general copilot systems for chemists that can provide answers to questions or even suggest new experiments based on vastly more information than a human could ever read.</p>
<p>However, the rapid increase in capabilities of chemical ML models led (even before the recent interest in LLMs) to concerns about the potential for dual use of these technologies, e.g., for the design of chemical weapons. ${ }^{35-40}$ To some extent, this is not surprising as any technology that, for instance, is used to design non-toxic molecules can also be used inversely to predict toxic ones (even though the synthesis would still require access to controlled physical resources and facilities). Still, it is essential to realize that the user base of LLMs is broader than that of chemistry and materials science experts who can critically reflect on every output these models produce. For</p>
<p>example, many students frequently consult these tools-perhaps even to prepare chemical experiments. ${ }^{41}$ This also applies to users from the general public, who might consider using LLMs to answer questions about the safety of chemicals. Thus, for some users, misleading information-especially about safety-related aspects-might lead to harmful outcomes. However, even for experts, chemical knowledge and reasoning capabilities are essential as they will determine the capabilities and limitations of their models in their work, e.g., in copilot systems for chemists. Unfortunately, apart from exploratory reports such as by prompting leading models with various scientific questions, ${ }^{13}$ there is little systematic evidence on how LLMs perform compared to expert (human) chemists.</p>
<p>Thus, to better understand what LLMs can do for the chemical sciences and where they might be improved with further developments, evaluation frameworks are needed to allow us to measure progress and mitigate potential harms systematically. For the development of LLMs, evaluation is currently primarily performed via standardized benchmark suites such as BigBench ${ }^{42}$ or the LM Eval Harness. ${ }^{43}$ Among 204 tasks (such as linguistic puzzles), the former contains only two tasks classified as "chemistry related", whereas the latter contains no specific chemistry tasks. Due to the lack of widely accepted standard benchmarks, the developers of chemical language models ${ }^{16,44-47}$ frequently utilize language-interfaced ${ }^{48}$ tabular datasets such as the ones reported in MoleculeNet, ${ }^{49}$ Therapeutic Data Commons ${ }^{50}$ or MatBench. ${ }^{51}$ In these cases, the models are evaluated on predicting very specific properties of molecules (e.g., solubility, toxicity, melting temperature or reactivity) or on predicting the outcome of specific chemical reactions. This, however, only gives a very limited view of the general chemical capabilities of the models.</p>
<p>While some benchmarks based on university entrance exams ${ }^{52,53}$ or automatic text mining ${ }^{54-56}$ have been proposed, none of them have been widely accepted. This is likely because they cannot automatically be used with black box (or tool-augmented) systems, do not cover a wide range of topics and skills, or are not carefully validated by experts. On top of that, the existing benchmarks are not designed to be used with models that support special treatment of molecules or equations and do not provide insights on how the models compare relative to experts ${ }^{49}$.</p>
<p>In this work, we report a novel benchmarking framework (Figure 1), which we call ChemBench, and use it to reveal limitations of current frontier models for use in the chemical sciences. Our benchmark consists of 2788 question-answer pairs compiled from diverse sources (1039 manually generated, and 1749 semi-automatically generated). Our corpus measures reasoning, knowledge and intuition across a large fraction of the topics taught in undergraduate and graduate chemistry curricula. It can be used to evaluate any system that can return text (i.e., including tool-augmented systems).</p>
<p>To contextualize the scores, we also surveyed 19 experts in chemistry on a subset of the benchmark corpus to be able to compare the performance of current frontier</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of the ChemBench framework. The figure shows the different components of the ChemBench framework. The framework's foundation is the benchmark corpus comprising thousands of questions and answers that we manually or semi-automatically compiled from various sources (see Section 4.1). Questions are classified based on topics, required skills (reasoning, calculation, knowledge, intuition), and difficulty levels. We then used this corpus to evaluate the performance of various models and tool-augmented systems using a custom framework. To provide a baseline, we built a web application that we used to survey experts in chemistry. The results of the evaluations are then compiled in publicly accessible leaderboards (Appendix A.15), which we propose as a foundation for evaluating future models.
models with (human) chemists of different specializations. In parts of the survey, the volunteers were also allowed to use tools such as web search to create a realistic setting.</p>
<h1>2 Results and Discussion</h1>
<h3>2.1 Benchmark corpus</h3>
<p>To compile our benchmark corpus, we utilized a broad list of sources (see Section 4.1), ranging from completely novel, manually crafted questions over university exams to semi-automatically generated questions based on curated subsets of data in chemical databases. For quality assurance, all questions have been reviewed by at least two scientists in addition to the original curator and automated checks. Importantly, our large pool of questions encompasses a wide range of topics and question types (Figure 2). The topics range from general chemistry to more specialized fields such as inorganic, analytical or technical chemistry. We also classify the questions based on what skills are required to answer them. Here, we distinguish between questions that require knowledge, reasoning, calculation, intuition, or a combination of these.</p>
<p>Moreover, the annotator also classifies the questions by difficulty to allow for a more nuanced evaluation of the models' capabilities.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Distribution of topics and required skills. This circular plot illustrates the distribution of questions across various chemistry topics, along with the primary skills required to address them. The topics were manually classified, showing a varied representation across different aspects of chemistry. Each topic is associated with a combination of three key skills: Calculation, Reasoning, and Knowledge, as indicated by the colored bars. ChemBench samples diverse topics and diverse skills, setting a high bar for LLMs to demonstrate human-competitive performance across a wide range of chemistry tasks.</p>
<p>While many existing benchmarks are designed around multiple-choice question (MCQ), this does not reflect the reality of chemistry education and research. For this reason, ChemBench samples both MCQ and open-ended questions ( 2544 MCQ questions and 244 open-ended questions). In addition, ChemBench samples different skills on various difficulty levels: From basic knowledge questions (as knowledge underpins reasoning processes ${ }^{57,58}$ ) to complex reasoning tasks (such as finding out which ions are in a sample given a description of observations). We also include questions about chemical intuition, as showing human-aligned preferences is relevant for applications such as hypothesis generation or optimization tasks. ${ }^{59}$</p>
<p>ChemBench-Mini It is important to note that a smaller subset of the corpus might be more practical for routine evaluations. ${ }^{60}$ For instance, Liang et al. [61] report costs of more than $\$ 10,000$ for application programming interface (API) calls for a single evaluation on the widely used Holistic Evaluation of Language Models (HELM) benchmark. To address this, we also provide a subset (ChemBench-Mini, 236 questions) of the corpus that was curated to be a diverse and representative subset of the full corpus. While it is impossible to comprehensively represent the full corpus in a subset, we aimed to include a maximally diverse set of questions and a more balanced distribution of topics and skills (see Section 4.4 for details on the curation process). Our human volunteers answered all the questions in this subset.</p>
<h1>2.2 Model evaluation</h1>
<p>Benchmark suite design Because the text used in scientific settings differs from typical natural language, many models have been developed that deal with such text in a particular way. For instance, the Galactica model ${ }^{62}$ uses special encoding procedures for molecules and equations. Current benchmarking suites, however, do not account for such special treatment of scientific information. To address this, ChemBench encodes the semantic meaning of various parts (e.g., chemicals, units, equations) of the question or answer. For instance, molecules represented in simplified molecular input line-entry system (SMILES) are enclosed in [START_SMILES] [ \END_SMILES] tags. This allows the model to treat the SMILES string differently from other text. ChemBench can seamlessly handle such special treatment in an easily extensible way because the questions are stored in an annotated format.</p>
<p>Since many widely utilized LLM systems only provide access to text completions (and not the raw model outputs), ChemBench is designed to operate on text completions. This is also important given the growing number of tool-augmented systems that are deemed essential for building chemical copilot systems. Such systems can augment the capabilities of LLMs through the use of external tools such as search APIs or code executors. ${ }^{63-65}$ In those cases, the LLM that returns the probabilities for various tokens, i.e., text fragments, is only a part of the whole system, and it is not clear how to interpret the probabilities in the context of the whole system. The text completions, however, are the system's final outputs, which would also be used in a real-world application. Hence, we use them for our evaluations. ${ }^{66}$</p>
<p>Overall system performance To understand the current capabilities of LLMs in the chemical sciences, we evaluated a wide range of leading models ${ }^{67}$ on the ChemBench corpus, including systems augmented with external tools. An overview of the results of this evaluation is shown in Figure 3 (all results can be found in Table A3 and Table A4). In this figure, we show the percentage of questions that the models answered correctly. Moreover, we show the worst, best, and average performance of</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Performance of models and humans on ChemBench-Mini. The figure shows the percentage of questions that the models answered correctly. We use horizontal bars to indicate the performance of various models and highlight statistics of the human performance. The evaluation we use here is very strict as it only considers a question answered correctly or incorrectly, partially correct answers are also considered incorrect. Figure A3 provides an overview of the performance of various models on the entire corpus. PaperQA2 ${ }^{33}$ is an agentic system that can also search the literature to obtain an answer. We find that the best models outperform all humans in our study when averaged over all questions (even though humans had access to tools such as web search and ChemDraw for a subset of the questions).
the experts in our study, which we obtained via a custom web application (chembench. org) that we used to survey the experts. Remarkably, the figure shows that the leading LLM, o1, outperforms the best human in our study in this overall metric by almost a factor of two. Many other models also outperform the average human performance. Interestingly, Llama-3.1-405B-Instruct shows performance that is close to the leading proprietary models, indicating that new open-source models can be competitive with the best proprietary models also in chemical settings.</p>
<p>Notably, we find that models are still limited in their ability to answer knowledgeintensive questions (Table A4); that is, they did not memorize the relevant facts.</p>
<p>Our results indicate that this is not a limitation that could be overcome by simple application of retrieval augmented generation (RAG) systems such as PaperQA2. This is likely because the required knowledge cannot easily be accessed via papers (which is the only external knowledge PaperQA2 has access to) but rather by lookup in specialized databases (e.g., PubChem, Gestis), which also the humans in our study used to answer such questions (Figure A17). This indicates that there is still room for improving chemical LLMs by training them on more specialized data sources or integrating them with specialized databases.</p>
<p>In addition, our analysis shows that the performance of models is correlated with their size (see Figure A11). This is in line with observations in other domains but also indicates that chemical LLMs could, to some extent, be further improved by scaling them up.</p>
<p>Performance per topic To obtain a more detailed understanding of the performance of the models, we also analyzed the performance of the models in different subfields of the chemical sciences. For this analysis, we defined a set of topics (see Section 4.5) and classified all questions in the ChemBench corpus into these topics. We then computed the percentage of questions the models or experts answered correctly for each topic and show them in Figure 4. In this spider chart, the worst score for every dimension is zero (no question answered correctly), and the best score is one (all questions answered correctly). Thus, a larger colored area indicates a better performance.</p>
<p>One can observe that this performance varies across models and topics. While general and technical receive relatively high scores for many models, this is not the case for topics such as toxicity and safety or analytical chemistry.</p>
<p>In the subfield of analytical chemistry, the prediction of the number of signals observable in a nuclear magnetic resonance (NMR) spectrum proved difficult even for the best models (e.g., 22 percent correct answers for o1). Importantly, while the human experts are given a drawing of the compounds, the models are only shown the SMILES string of a compound and have to use this to reason about the symmetry of the compound (i.e., to identify the number of diasterotopically distinct protons, which requires reasoning about the topology and structure of a molecule).</p>
<p>These findings also shine an interesting light on the value of textbook-inspired questions. A subset of the questions in ChemBench are based on textbooks targeted at undergraduate students. On those questions, the models tend to perform better than on some of our semi-automatically constructed tasks (see Figure A5). For instance, while the overall performance in the chemical safety topic is low, the models would pass the certification exam according to the German Chemical Prohibition Ordinance based on a subset of questions we sampled from the corresponding question bank (e.g., $71 \%$ correct answers for GPT-4, $61 \%$ for Claude-3.5 (Sonnet), and 3\% for the human experts). While those findings are impacted by the subset of questions we</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Performance of the models and humans on the different topics on ChemBench-Mini. The radar plot shows the performance of the models and humans on the different topics of ChemBench-Mini. The performance is measured as the fraction of questions that were answered correctly by the models. The best score for every dimension is one (all questions answered correctly), and the worst is zero (no question answered correctly). A larger colored area indicates a better performance. This figure shows the performance on ChemBench-Mini. The performance of models on the entire corpus is shown in Figure A3.
sampled, the results still highlight that good performance on such question bank or textbook questions does not necessarily translate to good performance on other questions that require more reasoning or are further away from the training corpus. ${ }^{10}$ The findings also underline that such exams might have been a good surrogate for the general performance of skills for humans, but their applicability in the face of systems that can consume vast amounts of data is up for debate.</p>
<p>We also gain insight into the models' struggles with chemical reasoning tasks by examining their performance as a function of molecular descriptors. If the model would answer questions after reasoning about the structures, one would expect the performance to depend on the complexity of the molecules. However, we find that the models' performance does not correlate with complexity indicators (see Appendix A.5). This indicates that the models may not be able to reason about the structures of the molecules (in the way one might expect) but instead rely on the proximity of the molecules to the training data. ${ }^{10}$</p>
<p>It is important to note that the model performance for some topics, however, is slightly underestimated in the current evaluation. This is because models provided via APIs typically have safety mechanisms that prevent them from providing answers that the provider deems unsafe. For instance, models might refuse to provide answers about cyanides. Statistics of the frequency of such refusals are shown in Table A7. To overcome this, direct access to the model weights would be required, and we strive to collaborate with the developers of frontier models to overcome this limitation in the future. This is facilitated by the tooling ChemBench provides, thanks to which contributors can automatically add new models in an open science fashion.</p>
<p>Judging chemical preference One interesting finding of recent research is that foundation models can judge interestingness or human preferences in some domains. ${ }^{59,68}$ If models could do so for chemical compounds, this would open opportunities for novel optimization approaches. Such open-ended tasks, however, depend on an external observer defining what interestingness is. ${ }^{69}$ Here, we posed models the same question Choung et al. [70] asked chemists at a drug company: "Which of the two compounds do you prefer?" (in the context of an early virtual screening campaign setting, see Table A2 for an example). Despite chemists demonstrating a reasonable level of interrater agreement, our models largely fail to align with expert chemists' preferences. Their performance is often indistinguishable from random guessing, even though these same models excel in other tasks in ChemBench (Table A4). This indicates that using preference tuning for chemical settings is a promising approach to explore in future research.</p>
<p>Confidence estimates One might wonder whether the models can estimate if they can answer a question correctly. If they could do so, incorrect answers would be less problematic.</p>
<p>To investigate this, we prompted ${ }^{66}$ some of the top-performing models to estimate, on an ordinal scale, their confidence in their ability to answer the question correctly (see Appendix A. 12 for details on the methodology and comparison to logit-based approaches).</p>
<p>In Figure 5, we show that for some models, there is no significant correlation between the estimated difficulty and whether the models answered the question correctly or not. For applications in which humans might rely on the models to provide answers with trustworthy uncertainty estimates, this is a concerning observation highlighting the need for critical reasoning in the interpretation of the model's outputs. ${ }^{34,71}$ For example, for the questions about the safety profile of compounds, GPT-4 reported a confidence of 1.0 (on a scale of 1-5) for the 1 questions it answered correctly and 4.0 for the 6 questions it answered incorrectly. While, on average, the verbalized confidence estimates from Claude 3.5 seem better calibrated (Figure 5), they</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Reliability and distribution of confidence estimates. For this analysis, we used verbalized confidence estimates from the model. We prompted the models to return a confidence score on an ordinal scale to obtain those estimates. The line plot shows the average fraction of correctly answered questions for each confidence level. The bar plot shows the distribution of confidence estimates. A confidence estimate would be well-calibrated if the average fraction of correctly answered questions increases with the confidence level. The dashed black line indicates this ideal behavior, which would be monotonically increasing correctness with higher levels of confidence. We find that most models are not well-calibrated and provide misleading confidence estimates.
are still misleading in some cases. For example, for the questions about the globally harmonized system of classification and labelling of chemicals (GHS) pictograms
(7) Claude 3.5 returns an average score of 2.0 for correct answers and 1.83 for incorrect answers.</p>
<h1>3 Conclusions</h1>
<p>On the one hand, our findings underline the impressive capabilities of LLMs in the chemical sciences: Leading models outperform domain experts in specific chemistry questions on many topics. On the other hand, there are still striking limitations. For very relevant topics, the answers that models provide are wrong. On top of that,</p>
<p>many models are not able to reliably estimate their own limitations. Yet, the success of the models in our evaluations perhaps also reveals more about the limitations of the questions we use to evaluate models-and chemists-than about the models themselves. For instance, while models perform well on many textbook questions, they struggle with questions requiring more reasoning about chemical structures (e.g., number of isomers or NMR peaks). Given that the models outperformed the average human in our study, we need to rethink how we teach and examine chemistry. Critical reasoning is increasingly essential, and rote solving of problems or memorization of facts is a domain in which LLMs will continue to outperform humans (when trained on the right training corpus).</p>
<p>Our findings also highlight the nuanced trade-off between breadth and depth of evaluation frameworks. The analysis of model performance on different topics shows that models' performance varies widely across the subfields they are tested on. However, even within a topic, the performance of models can vary widely depending on the type of question and the reasoning required to answer it.</p>
<p>The current evaluation frameworks for chemical LLMs are primarily designed to measure the performance of the models on specific property prediction tasks. They cannot be used to evaluate reasoning or systems built for scientific applications. Thus, we had little understanding of the capabilities of LLMs in the chemical sciences. Our work shows that carefully curated benchmarks can provide a more nuanced understanding of the capabilities of LLMs in the chemical sciences. Importantly, our findings also illustrate that more focus is required in developing better human-model interaction frameworks, given that models cannot estimate their limitations.</p>
<p>While our findings indicate many areas for further improvement of LLM-based systems, such as for agents (more discussion in Appendix A.11), it is also important to realize that clearly defined metrics have been the key to the progress of many fields of ML, such as computer vision. Although current systems might be far from reasoning like a chemist, our ChemBench framework will be a stepping stone for developing systems that might come closer to this goal.</p>
<h1>4 Methods</h1>
<h3>4.1 Curation workflow</h3>
<p>For our dataset, we curated questions from existing exams or exercise sheets but also programmatically created new questions (Table 1). Questions were added via Pull Requests on our GitHub repository and only merged into the corpus after passing manual review (Figure 6) as well as automated checks (e.g., for compliance with a standardized schema).</p>
<p>To ensure that the questions do not enter a training dataset, we use the same canary string as the BigBench project. This requires that LLM developers filter their training dataset for this canary string. ${ }^{4,42}$</p>
<p>Table 1: Overview of sources of the curated questions. The table provides an overview of the types of sources the questions have been curated from. Detailed sources are available in the source data on GitHub. Questions without a source have been curated completely from scratch. Questions based on lecture notes or URLs have been curated based on content presented in those resources. All questions have been rephrased, annotated, and reviewed before being added to the corpus.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Source</th>
<th style="text-align: right;">Count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Semiautomatically generated</td>
<td style="text-align: right;">1749</td>
</tr>
<tr>
<td style="text-align: left;">URL</td>
<td style="text-align: right;">375</td>
</tr>
<tr>
<td style="text-align: left;">Textbook</td>
<td style="text-align: right;">206</td>
</tr>
<tr>
<td style="text-align: left;">Exam</td>
<td style="text-align: right;">149</td>
</tr>
<tr>
<td style="text-align: left;">IChO</td>
<td style="text-align: right;">149</td>
</tr>
<tr>
<td style="text-align: left;">No source</td>
<td style="text-align: right;">139</td>
</tr>
<tr>
<td style="text-align: left;">Lectures</td>
<td style="text-align: right;">21</td>
</tr>
</tbody>
</table>
<p>Manually curated questions Manually curated questions were sourced from various sources, including university exams, exercises, and question banks.</p>
<p>Semi-programmatically generated questions In addition to the manually curated questions, we also generated questions programmatically. An overview of the sources of the semi-programmatically generated questions is provided in Table 2.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Overview of the workflow for the assembly of the ChemBench corpus. To assemble the ChemBench corpus, we first collected questions from various sources. Some tasks were manually curated, others semi-programmatically. We added semantic annotations for all questions to make them compatible with systems that use special processing for modalities that are not conventional natural text. We reviewed the questions using manual and automatic methods before adding them to the corpus.</p>
<p>Table 2: Sources of semi-programatically generated questions. The table shows the sources and a brief description as well as the number of the semi-programatically generated questions.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">source</th>
<th style="text-align: left;">description</th>
<th style="text-align: left;">question <br> count</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Number of isomers $\quad$| MAYGEN $^{72}$ was used to compute the num- | 24 |
| :-- | :-- |
| ber of isomers for a set of SMILES ex- |  |
| tracted from the ZINC dataset ${ }^{73}$ |  |</p>
<p>Total electron count of $\quad$| Electron counts based on the data from | 25 |
| :-- | :-- |
| molecules | https://www.cheminfo.org/ |</p>
<p>Oxidation states $\quad$| Oxidation states questions based on | 10 |
| :-- | :-- |
| the data from https://www.cheminfo. <br> org/ |  |</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Chemical reactivity</th>
<th style="text-align: center;">Questions are framed based on the infor mation from the Cameo Chemicals website</th>
<th style="text-align: center;">0</th>
<th style="text-align: center;">276</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Number of NMR sig nals</td>
<td style="text-align: center;">Molecules are sampled from the ZINC database ${ }^{73}$, OpenChemLib ${ }^{74}$ is used to compute the number of diasterotopically distinct hydrogen atoms</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">Point group of molecules</td>
<td style="text-align: center;">Our ChemCaption tool is used to assign the point group using spglib, ${ }^{75}$ and then each case was manually checked to select well-defined cases</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">IUPAC-SMILES pairs</td>
<td style="text-align: center;">Sampled from the PubChem ${ }^{76}$ database</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$10+$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">PubChem ${ }^{76}$ safety data</td>
<td style="text-align: center;">Daily allowable intakes according to the World Health Organization</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Definitions of hazard statements GHS classification of chemicals mined through the API</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: center;">Safety</td>
<td style="text-align: center;">Materials' compatibility <br> Chemical compatibility</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">296</td>
</tr>
</tbody>
</table>
<p>Chemical preference data These questions assess the ability to establish a "preference", such as favoring a specific molecule. Chemical preference is of major importance in drug discovery projects, where the optimization process to reach the desired molecular properties is a process that takes several years within a chemist's career. Our data corpus is adapted from the published dataset by Choung et al. [70], which consists of more than 5000 question-answer pairs about chemical intuition. To build the dataset, they presented 35 medicinal chemists with two different molecules, asking them what molecule they would like to continue with when imaging an early virtual screening campaign setting. The question was designed so the scientists do not spend much time answering it, relying only on their feelings or "chemical preference".</p>
<p>To understand whether the capabilities of the leading models align with the preferences of professional chemists, we randomly selected 1000 data points from the original dataset to create a meaningful evaluation set, where molecules are represented as SMILES. To ablate the effect of different molecular representations, we only considered questions for which we could obtain International Union of Pure and Applied Chemistry (IUPAC) names for both molecules present.</p>
<h1>4.2 Model evaluation workflow</h1>
<p>A graphical overview of the pipeline is shown in Figure A12.
Prompting We employ distinct prompt templates tailored for completion and instruction-tuned models to maintain consistency with the training. As explained below, we impose constraints on the models within these templates to receive responses in a specific format so that robust, fair, and consistent parsing can be performed. Certain models are trained with special annotations and $\mathrm{ET}<em _mathrm_E="\mathrm{E">{\mathrm{E}} \mathrm{X}$ syntax for scientific notations, chemical reactions, or symbols embedded within the text. For example, all the SMILES representations are encapsulated within [START_SMILES] [ \END_SMILES] in Galactica ${ }^{62}$. Our prompting strategy consistently adheres to these details in a model-specific manner by post-processing $\mathrm{ET}</em>$ syntax, chemical symbols, chemical equations, and physical units (by either adding or removing wrappers). This step can be easily customized in our codebase, and we provide presets for the models we evaluated.}} \mathrm{X</p>
<p>Parsing Our parsing workflow is multistep and primarily based on regular expressions. In the case of instruction-tuned models, we first identify the [ANSWER] [ \ANSWER] environment we prompt the model to report the answer in. In the case of completion models, this step is skipped. From there, we attempt to extract the relevant enumeration letters (for multiple-choice questions) or numbers. In the case of numbers, our regular expression was engineered to deal with various forms of scientific notation. As initial tests indicated that models sometimes return integers in the form of words, e.g., "one" instead of " 1 ", we also implemented a word-to-number conversion using regular expressions. If these hard-coded parsing steps fail, we use a LLM, e.g., Claude-3.5 (Sonnet), to parse the completion (Appendix A. 8 provides more details on this step).</p>
<p>Models For all models, we performed inference using greedy decoding (i.e., temperature 0 ). We used the API endpoints provided by the model developers and those provided by Groq. PaperQA2 was used (in August 2024) via an API provided by FutureHouse.</p>
<h3>4.3 Confidence estimate</h3>
<p>To estimate the models' confidence, we prompted them with the question (and answer options for MCQ) and the task to rate their confidence to produce the correct answer on a scale from 1 to 5 . We decided to use verbalized confidence estimates ${ }^{66}$ since we found those closer to current practical use cases than other prompting strategies, which might be more suitable when implemented in systems. In addition, this approach captures semantic uncertainty, which is not the same as the probability</p>
<p>of a token being given a sequence of tokens (i.e., the uncertainty one obtains from logit-based approaches). On top of that, many proprietary models do not provide access to the logits, making this approach more general. In Appendix A.12, we provide more details and comparisons with a logit-based approach.</p>
<h1>4.4 Human baseline</h1>
<p>Question selection Several design choices were made when selecting ChemBenchMini. Firstly, from the full dataset, we kept all the questions labeled as advanced. In this way, we can obtain a deeper insight into the capabilities of LLMs on advanced tasks when compared to actual chemists. Secondly, we sample a maximum of three questions across all possible combinations of categories (i.e., knowledge or reasoning) and topics (e.g., organic chemistry, physical chemistry). Thirdly, we do not include any intuition questions in this subset because the intended use of ChemBench-Mini is to provide a fast and fair evaluation of LLMs independent of any human baseline.
(7) In total, 236 questions have been sampled for ChemBench-Mini. Then, this set is divided into two subsets based on the aforementioned combinations. One of the question subsets allows tool use, and the other does not.</p>
<p>Study design Human volunteers were asked the questions in a custom-built web interface (see Appendix A.10), which rendered chemicals and equations. Questions were shown in random order, and volunteers were not allowed to skip questions. For a subset of the questions, the volunteers were allowed to use external tools (excluding other LLM or asking other people) to answer the questions. Prior to answering questions, volunteers were asked to provide information about their education and experience in chemistry. The study was conducted in English.</p>
<p>Human volunteers Users were open to reporting about their experience in chemistry. Overall, 16 did so. Out of those, 2 are beyond a first postdoc, 13 have a master's degree (and are currently enrolled in Ph.D. studies), and 1 has a bachelor's degree. For the analysis, we excluded volunteers with less than two years of experience in chemistry after their first university-level course in chemistry.</p>
<p>Comparison with models For the analysis, we treated each human as a model. We computed the topic aggregated averages per human for analyses grouped by topic and then averaged over all humans. The performance metrics reported for models in the main text are computed on the same questions the humans answered. Metrics for the entire corpus are reported in the appendix (Appendix A.4).</p>
<h1>4.5 Data annotation</h1>
<p>In the curation of our dataset, we manually assigned difficulty levels and required skills to each question. We used the following guidelines for these annotations: calculation is required if answering a question would require the use of a calculator, knowledge is required if answering a question requires non-trivial knowledge of facts (e.g., the $\mathrm{H} / \mathrm{P}$ statements of chemicals). Reasoning is required if answering a question requires multiple reasoning steps. Basic questions only require those skills up to the high school level. Advanced questions would require an expert multiple minutes up to hours to answer.</p>
<h2>Data and code availability</h2>
<p>The code and data for ChemBench are available at https://github.com/lamalaborg/chem-bench and archived on Zenodo under https://zenodo.org/records/14010212. The code for the app for our human baseline study is available at https://github. com/lamalab-org/chem-bench-app. To ensure reproducibility, this manuscript was generated using the show your work framework. ${ }^{77}$ The code to rebuild the paper (including code for all figures and numbers next to which there is a GitHub icon) can be found at https://github.com/lamalab-org/chembench-paper. To facilitate reproduction, some intermediate analysis results are cached at http://dx. doi.org/10.5072/zenodo. 34706.</p>
<h2>Acknowledgements</h2>
<p>This work was supported by the Carl Zeiss Foundation, and a "Talent Fund" of the "Life" profile line of the Friedrich Schiller University Jena.</p>
<p>In addition, M.S-W.'s work was supported by Intel and Merck via the AWASES programme.</p>
<p>Parts of A.M.'s work was supported as part of the "SOL-AI" project funded by the Helmholtz Foundation model initative.
K.M.J. is part of the NFDI consortium FAIRmat funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - project 460197019.
K.M.J. thanks FutureHouse (a non-profit research organization supported by the generosity of Eric and Wendy Schmidt) for supporting PaperQA2 runs via access to the API. We also thank Stability.AI for the access to its HPC cluster.
M.R.G. and M.V.G. acknowledge financial support from the Spanish Agencia Estatal de Investigación (AEI) through grants TED2021-131693B-I00 and CNS2022135474, funded by MICIU/AEI/10.13039/501100011033 and by the European Union NextGenerationEU/PRTR. M.V.G. acknowledges support from the Spanish National</p>
<p>Research Council (CSIC) through Programme for internationalization i-LINK 2023 (Project ILINK23047).
A.A. gratefully acknowledges financial support for this research by the Fulbright U.S. Student Program, which is sponsored by the U.S. Department of State and German-American Fulbright Commission. Its contents are solely the responsibility of the author and do not necessarily represent the official views of the Fulbright Program, the Government of the United States, or the German-American Fulbright Commission.
M.A. expresses gratitude to the European Research Council (ERC) for evaluating the project with the reference number 101106377 titled "CLARIFIER" and accepting it for funding under the HORIZON TMA MSCA Postdoctoral Fellowships - European Fellowships. Furthermore, M.A. acknowledges the funding provided by UK Research and Innovation (UKRI) under the UK government's Horizon Europe funding guarantee (Grant Reference: EP/Y023447/1; Organization Reference: 101106377).
M.R. and U.S.S. thank the "Deutsche Forschungsgemeinschaft" for funding under the regime of the priority programme SPP 2363 "Utilization and Development of Machine Learning for Molecular Applications - Molecular Machine Learning" (SCHU 1229/63-1; project number 497115849).</p>
<p>In addition, we thank the OpenBioML.org community and their ChemNLP project team for valuable discussions. Moreover, we thank Pepe Márquez for discussions and support and Julian Kimmig for feedback on the web app. In addition, we acknowledge support from Sandeep Kumar with an initial prototype of the web app. We thank Bastian Rieck for developing the $\mathrm{ET}_{\mathrm{E}} \mathrm{X}$-credit package (https://github. com/Pseudomanifold/latex-credits) and thank Berend Smit for feedback on an early version of the manuscript.</p>
<h1>Statement of ethical compliance</h1>
<p>The authors confirm to have complied with all relevant ethical regulations, according to the Ethics Commission of the Friedrich Schiller University Jena (which decided that study is ethically safe). Informed consent was obtained from all volunteers.</p>
<h2>Conflicts of interest</h2>
<p>K.M.J. was a paid consultant for OpenAI (as part of the red teaming network). M.P. is an employee of Stability.AI, and A.M. and N.A. were paid contractors of Stability.AI.</p>
<h1>Author contributions</h1>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<h2>References</h2>
<ol>
<li>Brown, T. B. et al. Language Models are Few-Shot Learners. arXiv preprint arXiv:2005.14165 (2020).</li>
<li>Zhong, Z., Zhou, K. \&amp; Mottin, D. Benchmarking Large Language Models for Molecule Prediction Tasks. arXiv preprint arXiv:2403.05075 (2024).</li>
<li>Kung, T. H. et al. Performance of ChatGPT on USMLE: potential for AI-assisted medical education using large language models. PLoS digit. health 2, e0000198 (2023).</li>
<li>OpenAI et al. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774 (2024).</li>
<li>Boiko, D. A., MacKnight, R., Kline, B. \&amp; Gomes, G. Autonomous chemical research with large language models. Nature 624, 570-578 (Dec. 20, 2023).</li>
<li>M. Bran, A. et al. Augmenting large language models with chemistry tools. Nat. Mach. Intell. 6, 525-535 (2024).</li>
<li>Darvish, K. et al. ORGANA: A Robotic Assistant for Automated Chemistry Experimentation and Characterization. arXiv preprint arXiv:2401.06949 (2024).</li>
<li>Bubeck, S. et al. Sparks of Artificial General Intelligence: Early experiments with GPT-4. arXiv preprint arXiv:2303.12712 (2023).</li>
<li>Bender, E. M., Gebru, T., McMillan-Major, A. \&amp; Shmitchell, S. On the dangers of stochastic parrots: Can language models be too big? in Proceedings of the 2021 ACM conference on fairness, accountability, and transparency (2021), 610-623.</li>
</ol>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Laboratory of Organic and Macromolecular Chemistry (IOMC), Friedrich Schiller University Jena, Humboldtstrasse 10, 07743 Jena, Germany
${ }^{2}$ Helmholtz Institute for Polymers in Energy Applications Jena (HIPOLE Jena), Lessingstrasse 12-14, 07743 Jena, Germany
${ }^{3}$ Institute of Carbon Science and Technology (INCAR), CSIC, Francisco Pintado Fe 26, 33011 Oviedo, Spain ${ }^{4}$ QpiVolta Technologies Pvt Ltd
${ }^{5}$ Laboratory of Artificial Chemical Intelligence (LIAC), Institut des Sciences et Ingénierie Chimiques, Ecole Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland
${ }^{6}$ National Centre of Competence in Research (NCCR) Catalysis, Ecole Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland
${ }^{7}$ Department of Chemical Engineering \&amp; Biotechnology, University of Cambridge, Philippa Fawcett Drive, Cambridge CB3 0AS, United Kingdom
${ }^{8}$ Macromolecular Chemistry, University of Bayreuth, 95447 Bayreuth, Germany
${ }^{9}$ Laboratory of Molecular Simulation (LSMO), Institut des Sciences et Ingénierie Chimiques, Ecole Polytechnique Fédérale de Lausanne (EPFL), Sion, Switzerland
${ }^{10}$ Institute for Inorganic and Analytical Chemistry (IAAC), Friedrich Schiller University Jena, Humboldtstrasse 8, 07743 Jena, Germany
${ }^{11}$ Center for Energy and Environmental Chemistry Jena (CEEC Jena), Friedrich Schiller University Jena, Philosophenweg 7a, 07743 Jena, Germany
${ }^{12}$ Institute for Technical Chemistry and Environmental Chemistry (ITUC), Friedrich Schiller University Jena, Philosophenweg 7a, 07743 Jena, Germany
${ }^{13}$ Intel Labs
${ }^{14}$ Jena Center for Soft Matter (JCSM), Friedrich Schiller University Jena, Philosophenweg 7, 07743 Jena, Germany
${ }^{15}$ Theoretical Chemistry, Technische Universität Dresden, Dresden 01062, Germany
${ }^{16}$ OpenBioML.org
${ }^{17}$ Stability.AI
${ }^{18}$ mail@kjablonka.com
*These authors contributed equally.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
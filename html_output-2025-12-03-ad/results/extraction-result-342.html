<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-342 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-342</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-342</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-251979509</p>
                <p><strong>Paper Title:</strong> <a href="https://ojs.aaai.org/index.php/AAAI/article/download/26549/26321" target="_blank">On Grounded Planning for Embodied Tasks with Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Language models (LMs) have demonstrated their capability in possessing commonsense knowledge of the physical world, a crucial aspect of performing tasks in everyday life. However, it remains unclear **whether LMs have the capacity to generate grounded, executable plans for embodied tasks.** This is a challenging task as LMs lack the ability to perceive the environment through vision and feedback from the physical environment. In this paper, we address this important research question and present the first investigation into the topic. Our novel problem formulation, named **G-PlanET**, inputs a high-level goal and a data table about objects in a specific environment, and then outputs a step-by-step actionable plan for a robotic agent to follow. To facilitate the study, we establish an **evaluation protocol** and design a dedicated metric to assess the quality of the plans. Our experiments demonstrate that the use of tables for encoding the environment and an iterative decoding strategy can significantly enhance the LMs' ability in grounded planning. Our analysis also reveals interesting and non-trivial findings.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e342.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e342.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>G-PlanET</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grounded Planning for Embodied Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A task formulation that asks encoder-decoder language models to generate step-by-step, executable action plans for an embodied agent given a high-level goal plus a structured object table describing a specific environment (positions, rotations, receptacles, types, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>encoder-decoder LMs (BART-family, T5-style)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained encoder-decoder language models (BART-base, BART-large) fine-tuned in seq2seq fashion to map [Goal + flattened object table] -> natural-language step sequence; optionally used with iterative decoding where previously generated steps are appended to the input to generate the next step.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>G-PlanET (grounded planning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given (1) a high-level natural-language goal (e.g., 'move a teapot from the stove to a shelf') and (2) a structured object table extracted from a realistic AI2-THOR environment (object id, type, 3D coordinates, rotation, parent receptacle, properties), generate a sequence of actionable natural-language steps S1, S2, ... that an embodied agent can follow to accomplish the goal in that specific environment. The environment is provided as a flattened tabular textual input; the agent itself is not given visual/sensory streams during planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step planning / household tasks / instruction following / object manipulation + navigation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretraining on text corpora (implicit commonsense) + fine-tuning on ALFRED-derived seq2seq data + explicit environment encoding via flattened object tables (derived from AI2-THOR)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>supervised fine-tuning (seq2seq) and iterative decoding during generation; also few-shot prompting experiments (GPT-J 6B) reported</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>environment encoded as a flattened textual table (rows with id, type, 3D coordinates, rotation, receptacle parent, properties); plans represented as natural-language action-step sequences; models retain implicit commonsense knowledge in weights, and TAPEX-style table pretraining used to better leverage tabular inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>CIDEr, SPICE, and KeyActionScore (KAS) — step-wise evaluation with possible temporal re-weighting</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>See model-specific entries below; metrics reported per-split (unseen/seen room layouts). G-PlanET evaluation protocol uses step-wise KAS as primary measure for action correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>When given object tables, models grounded references to object locations and receptacles (e.g., correctly identifying 'laptop on coffee table' vs incorrect 'bed'), produced more location-aware first steps, and used object type information to infer affordances (e.g., 'microwave' implies heating). Iterative decoding improved coherence across multiple steps and better matched the correct number of steps for longer tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Without environment tables, models produced non-grounded plans (e.g., selecting wrong object locations). Non-iterative single-pass decoding often underestimated the number of steps, duplicated or copied last steps to fill missing steps, and accumulated errors across steps; long-horizon tasks saw large performance degradation. Models still struggle with precise directional distinctions (left/right) and early-step importance.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Vanilla (no table) BART baselines vs table-augmented and TAPEX variants; see model entries for numerical comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Primary ablations: (1) remove table (vanilla) — large drop in KAS/SPICE/CIDEr; (2) add table — consistent gain; (3) add iterative decoding — further large gains, especially for KAS and on longer tasks; (4) replace BART table handling with TAPEX pretraining — large gains vs plain BART w/table. Detailed numbers in model rows.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Providing a textual, structured representation of the environment (flattened object table) enables LMs to ground procedural plans into spatial and object-relational details; iterative stepwise decoding encourages temporally dynamic attention to different objects and substantially improves step coherence and step-count prediction; pretraining on tabular tasks (TAPEX) helps the model better exploit object tables; models operate without direct sensory input and therefore rely on symbolic-like tabular environment encodings plus commonsense in weights.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Grounded Planning for Embodied Tasks with Language Models', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e342.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e342.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BART-large (vanilla)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BART-large used as vanilla seq2seq baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Encoder-decoder LM fine-tuned to generate entire step-by-step plans from only the high-level goal, without any environment table supplied.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard pretrained BART-large (encoder-decoder Transformer) fine-tuned on ALFRED-derived plan generation data; input contains only the goal text, output is full plan in single-pass decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>G-PlanET (vanilla/no-environment baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate full plan from goal only (no table), i.e., non-grounded planning producing sequences of steps the agent could plausibly take in some environment but not specifically grounded to the provided environment.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step planning / instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + implicit object-relational (commonsense only)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretraining on text corpora (implicit commonsense) + fine-tuning on plan sequences (ALFRED-derived) without environment encoding</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>supervised fine-tuning (seq2seq), single-pass full-plan decoding</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>procedures and commonsense affordances are implicit in model weights; no explicit environment encoding provided at inference time</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>CIDEr, SPICE, KAS (step-wise averaged)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Unseen-room split: CIDEr 1.4632, SPICE 0.3168, KAS 0.4069; Seen-room split: CIDEr 1.4414, SPICE 0.3161, KAS 0.3900 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Able to produce plausible high-level procedural steps relying on commonsense affordances (e.g., general strategies to complete household tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Plans are not grounded to the actual environment—models often pick wrong object locations or wrong receptacles (example: 'close the laptop and pick it up from the bed' when laptop was on a table). Underestimates number of steps and suffers from accumulated coherence errors in long tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>This is the primary baseline; compared against BART w/table, TAPEX, iterative decoding methods which outperform it substantially (see numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Removal of environment table (this baseline) yields notably lower KAS and SPICE vs table-augmented methods; single-pass decoding misses step-count and sequential coherence that iterative decoding restores.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LMs trained only on textual goals can produce plausible procedural plans but cannot ground those plans into specific spatial/object relations without explicit environment encoding; thus, environment encoding is critical for grounded embodied planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Grounded Planning for Embodied Tasks with Language Models', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e342.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e342.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BART w/table + iterative</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BART (base/large) fine-tuned with flattened object table and iterative decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Encoder-decoder LMs that receive flattened object-table tokens appended to the goal and use iterative step-by-step decoding (re-encoding input with previous predicted steps) to generate grounded plans with improved step coherence and grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART-base / BART-large (table + iterative decoding)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BART models fine-tuned to take as input: [Goal] Env: [row1] [SEP] [row2] ... where each row contains object id, type, coordinates, rotation, parent receptacles, etc.; iterative decoding appends S1..St to input to generate St+1, encouraging temporally dynamic attention.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>G-PlanET (table-augmented, iterative)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same as G-PlanET but model has explicit textual access to environment (object table) and generates plans step-by-step using iterative decoding until an END token.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step planning / navigation + manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretraining on text corpora + fine-tuning on ALFRED-derived data with explicit environment encoding; iterative decoding uses model's internal reasoning across time steps (autoregressive feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>supervised fine-tuning and iterative decoding (re-encode after each generated step); during training ground-truth previous steps are used; during inference previous model predictions are used.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>environment as flattened textual table (explicit symbolic object representation); plans as natural-language step sequences; LM weights capture commonsense affordances; iterative decoding implements a temporal attention / stateful planning mechanism in text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>CIDEr, SPICE, KAS (step-wise); reported on unseen and seen room splits</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Examples from Table 1 (Unseen Room Layouts): BART-base w/table + iterative decoding: CIDEr 2.9147, SPICE 0.5107, KAS 0.6334. BART-large w/table + iterative decoding: CIDEr 2.8580, SPICE 0.5194, KAS 0.6518. (Seen rooms similar magnitudes.) These are substantially higher than vanilla baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>More accurate grounding of object locations and receptacles (e.g., identifies 'laptop on coffee table'), better prediction of correct number of steps, improved handling of long-horizon tasks, and higher step-wise action correctness (KAS). Iterative decoding reduces error accumulation by allowing the model to condition on previous generated steps.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Still degrades on very long tasks; spatial precision and early-step correctness remain challenging in some cases; errors in step ordering or inexact spatial phrasing can still cause incorrect execution if followed literally.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to BART-large (vanilla): KAS improved from 0.4069 -> 0.6518 on unseen rooms. Compared to BART-large w/table single-pass: 0.4411 -> 0.6518 (iterative adds big improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Ablating the table (i.e., vanilla) strongly reduces KAS/SPICE/CIDEr. Replacing single-pass with iterative decoding substantially increases KAS (example: BART-large w/table single-pass KAS 0.4411 -> iterative 0.6518). Iterative decoding also improves step-count prediction accuracy (less underestimation).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Flattened object tables + iterative stepwise decoding is a practically effective combination for grounding LM-generated plans: explicit tabular environment encodings provide spatial and object-relational facts (positions, receptacles, rotations) and iterative decoding enforces temporal coherence and better state tracking across steps, significantly improving key-action correctness (KAS).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Grounded Planning for Embodied Tasks with Language Models', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e342.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e342.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TAPEX (table-pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TAPEX (pretrained table encoder applied to BART)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model pre-trained on table-related tasks (SQL execution pretraining) that better leverages tabular inputs; when used in place of plain BART for environment-encoded inputs, it yields large gains in grounded planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TAPEX-adapted encoder (used with BART-large)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>TAPEX is a pretrained model optimized for tabular reasoning (pretraining task: SQL execution); in this paper TAPEX-style pretraining is applied to BART-large to improve use of flattened object tables.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>G-PlanET (table-augmented with TAPEX pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same as G-PlanET; model is specifically pre-trained for tabular inputs to better parse and reason over object tables (types, coordinates, receptacles) when producing grounded plans.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step planning / navigation + manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational (table-focused)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>table-pretraining (SQL execution tasks) + fine-tuning on ALFRED-derived grounded-plan data</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>fine-tuning with TAPEX-pretrained weights for tabular reasoning; single-pass and iterative decoding evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit tabular encoding (flattened rows) used as input and processed by a model with tabular pretraining, producing natural-language action sequences; spatial/object relations are represented as table fields</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>CIDEr, SPICE, KAS</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Unseen-room split: BART-large (TAPEX) single-pass: CIDEr 2.8824, SPICE 0.5054, KAS 0.6373. BART-large (TAPEX) + iterative decoding: CIDEr 2.8440, SPICE 0.5210, KAS 0.6313. These are large improvements over vanilla BART without table.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>TAPEX-pretrained model makes fuller use of tabular environment inputs and shows strong gains in step-action correctness and semantic alignment of steps to object relations; better at mapping object fields to grounded actions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Even with TAPEX, some iterative variants showed similar or slightly lower KAS vs BART-iterative in certain splits; long-horizon planning and precise early-step importance remain challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms BART-large vanilla and BART-large w/table single-pass in many metrics (e.g., TAPEX single-pass KAS 0.6373 vs BART-large vanilla 0.4069).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Replacing plain BART table encoder with TAPEX-style pretraining yields substantial gains; combining TAPEX with iterative decoding yields comparable but not strictly superior results to BART iterative in all reported splits (mixed gains).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretraining on table-specific tasks (TAPEX) makes models substantially better at using flattened object tables to ground plans, confirming that specialized pretraining enables LMs to extract spatial and object-relational information from textual tables more effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Grounded Planning for Embodied Tasks with Language Models', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e342.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e342.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-J-6B (few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-J 6B (in-context few-shot LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large causal transformer model evaluated in few-shot in-context learning mode to test whether large LMs can perform grounded planning with only a few examples and textual environment data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J-6B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 6B-parameter causal language model (GPT-J) used in in-context few-shot experiments to probe whether large models can learn G-PlanET-like mapping from goal+table to plan with few demonstrations rather than fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>G-PlanET (few-shot in-context)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provide a handful of example (goal + flattened object table -> plan) demonstrations in-context and prompt GPT-J to generate a plan for a new goal/environment pair without parameter updates.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step planning / instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational (in-context)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pretraining on large text corpora (implicit commonsense) + in-context examples at inference time</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>in-context few-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>uses flattened textual table and examples in prompt; knowledge remains implicit in model activations and in-context demonstration patterns</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>CIDEr, SPICE, KAS</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Reported in Table 1 (Unseen): CIDEr 1.1968, SPICE 0.2655, KAS 0.3622; (Seen): CIDEr 1.1047, SPICE 0.2509, KAS 0.3370. These are better than some vanilla baselines but worse than fine-tuned table-augmented models.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Some use of in-context examples allows GPT-J to produce reasonably plausible action sequences and exploit some object-relational cues from the textual table when examples are well chosen.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Few-shot performance lags fine-tuned models; in-context learning alone insufficient to robustly ground plans to precise spatial coordinates, receptacle relations, or long-horizon coherence; sensitive to prompt formatting and example selection.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms BART-base vanilla in some metrics? (See Table 1 for direct numbers) but underperforms fine-tuned BART w/table + iterative and TAPEX variants.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Paper reports GPT-J as a probing experiment; no detailed ablations reported for prompt variations in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large causal LMs with few-shot prompting can partially leverage textual object tables and in-context examples for grounded planning, but fine-tuning and specialized table-pretraining plus iterative decoding produce substantially better, more reliable grounded plans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Grounded Planning for Embodied Tasks with Language Models', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ALFRED <em>(Rating: 2)</em></li>
                <li>AI2-THOR: An Interactive 3D Environment for Visual AI <em>(Rating: 2)</em></li>
                <li>TAPEX <em>(Rating: 2)</em></li>
                <li>Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents <em>(Rating: 2)</em></li>
                <li>SayCan <em>(Rating: 2)</em></li>
                <li>ALFWorld <em>(Rating: 1)</em></li>
                <li>SciWorld <em>(Rating: 1)</em></li>
                <li>Language Models are Few-Shot Learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-342",
    "paper_id": "paper-251979509",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "G-PlanET",
            "name_full": "Grounded Planning for Embodied Tasks",
            "brief_description": "A task formulation that asks encoder-decoder language models to generate step-by-step, executable action plans for an embodied agent given a high-level goal plus a structured object table describing a specific environment (positions, rotations, receptacles, types, etc.).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "encoder-decoder LMs (BART-family, T5-style)",
            "model_size": null,
            "model_description": "Pretrained encoder-decoder language models (BART-base, BART-large) fine-tuned in seq2seq fashion to map [Goal + flattened object table] -&gt; natural-language step sequence; optionally used with iterative decoding where previously generated steps are appended to the input to generate the next step.",
            "task_name": "G-PlanET (grounded planning)",
            "task_description": "Given (1) a high-level natural-language goal (e.g., 'move a teapot from the stove to a shelf') and (2) a structured object table extracted from a realistic AI2-THOR environment (object id, type, 3D coordinates, rotation, parent receptacle, properties), generate a sequence of actionable natural-language steps S1, S2, ... that an embodied agent can follow to accomplish the goal in that specific environment. The environment is provided as a flattened tabular textual input; the agent itself is not given visual/sensory streams during planning.",
            "task_type": "multi-step planning / household tasks / instruction following / object manipulation + navigation",
            "knowledge_type": "spatial+procedural+object-relational",
            "knowledge_source": "pretraining on text corpora (implicit commonsense) + fine-tuning on ALFRED-derived seq2seq data + explicit environment encoding via flattened object tables (derived from AI2-THOR)",
            "has_direct_sensory_input": false,
            "elicitation_method": "supervised fine-tuning (seq2seq) and iterative decoding during generation; also few-shot prompting experiments (GPT-J 6B) reported",
            "knowledge_representation": "environment encoded as a flattened textual table (rows with id, type, 3D coordinates, rotation, receptacle parent, properties); plans represented as natural-language action-step sequences; models retain implicit commonsense knowledge in weights, and TAPEX-style table pretraining used to better leverage tabular inputs.",
            "performance_metric": "CIDEr, SPICE, and KeyActionScore (KAS) — step-wise evaluation with possible temporal re-weighting",
            "performance_result": "See model-specific entries below; metrics reported per-split (unseen/seen room layouts). G-PlanET evaluation protocol uses step-wise KAS as primary measure for action correctness.",
            "success_patterns": "When given object tables, models grounded references to object locations and receptacles (e.g., correctly identifying 'laptop on coffee table' vs incorrect 'bed'), produced more location-aware first steps, and used object type information to infer affordances (e.g., 'microwave' implies heating). Iterative decoding improved coherence across multiple steps and better matched the correct number of steps for longer tasks.",
            "failure_patterns": "Without environment tables, models produced non-grounded plans (e.g., selecting wrong object locations). Non-iterative single-pass decoding often underestimated the number of steps, duplicated or copied last steps to fill missing steps, and accumulated errors across steps; long-horizon tasks saw large performance degradation. Models still struggle with precise directional distinctions (left/right) and early-step importance.",
            "baseline_comparison": "Vanilla (no table) BART baselines vs table-augmented and TAPEX variants; see model entries for numerical comparisons.",
            "ablation_results": "Primary ablations: (1) remove table (vanilla) — large drop in KAS/SPICE/CIDEr; (2) add table — consistent gain; (3) add iterative decoding — further large gains, especially for KAS and on longer tasks; (4) replace BART table handling with TAPEX pretraining — large gains vs plain BART w/table. Detailed numbers in model rows.",
            "key_findings": "Providing a textual, structured representation of the environment (flattened object table) enables LMs to ground procedural plans into spatial and object-relational details; iterative stepwise decoding encourages temporally dynamic attention to different objects and substantially improves step coherence and step-count prediction; pretraining on tabular tasks (TAPEX) helps the model better exploit object tables; models operate without direct sensory input and therefore rely on symbolic-like tabular environment encodings plus commonsense in weights.",
            "uuid": "e342.0",
            "source_info": {
                "paper_title": "On Grounded Planning for Embodied Tasks with Language Models",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "BART-large (vanilla)",
            "name_full": "BART-large used as vanilla seq2seq baseline",
            "brief_description": "Encoder-decoder LM fine-tuned to generate entire step-by-step plans from only the high-level goal, without any environment table supplied.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "BART-large",
            "model_size": null,
            "model_description": "Standard pretrained BART-large (encoder-decoder Transformer) fine-tuned on ALFRED-derived plan generation data; input contains only the goal text, output is full plan in single-pass decoding.",
            "task_name": "G-PlanET (vanilla/no-environment baseline)",
            "task_description": "Generate full plan from goal only (no table), i.e., non-grounded planning producing sequences of steps the agent could plausibly take in some environment but not specifically grounded to the provided environment.",
            "task_type": "multi-step planning / instruction following",
            "knowledge_type": "procedural + implicit object-relational (commonsense only)",
            "knowledge_source": "pretraining on text corpora (implicit commonsense) + fine-tuning on plan sequences (ALFRED-derived) without environment encoding",
            "has_direct_sensory_input": false,
            "elicitation_method": "supervised fine-tuning (seq2seq), single-pass full-plan decoding",
            "knowledge_representation": "procedures and commonsense affordances are implicit in model weights; no explicit environment encoding provided at inference time",
            "performance_metric": "CIDEr, SPICE, KAS (step-wise averaged)",
            "performance_result": "Unseen-room split: CIDEr 1.4632, SPICE 0.3168, KAS 0.4069; Seen-room split: CIDEr 1.4414, SPICE 0.3161, KAS 0.3900 (Table 1).",
            "success_patterns": "Able to produce plausible high-level procedural steps relying on commonsense affordances (e.g., general strategies to complete household tasks).",
            "failure_patterns": "Plans are not grounded to the actual environment—models often pick wrong object locations or wrong receptacles (example: 'close the laptop and pick it up from the bed' when laptop was on a table). Underestimates number of steps and suffers from accumulated coherence errors in long tasks.",
            "baseline_comparison": "This is the primary baseline; compared against BART w/table, TAPEX, iterative decoding methods which outperform it substantially (see numbers).",
            "ablation_results": "Removal of environment table (this baseline) yields notably lower KAS and SPICE vs table-augmented methods; single-pass decoding misses step-count and sequential coherence that iterative decoding restores.",
            "key_findings": "LMs trained only on textual goals can produce plausible procedural plans but cannot ground those plans into specific spatial/object relations without explicit environment encoding; thus, environment encoding is critical for grounded embodied planning.",
            "uuid": "e342.1",
            "source_info": {
                "paper_title": "On Grounded Planning for Embodied Tasks with Language Models",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "BART w/table + iterative",
            "name_full": "BART (base/large) fine-tuned with flattened object table and iterative decoding",
            "brief_description": "Encoder-decoder LMs that receive flattened object-table tokens appended to the goal and use iterative step-by-step decoding (re-encoding input with previous predicted steps) to generate grounded plans with improved step coherence and grounding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BART-base / BART-large (table + iterative decoding)",
            "model_size": null,
            "model_description": "BART models fine-tuned to take as input: [Goal] Env: [row1] [SEP] [row2] ... where each row contains object id, type, coordinates, rotation, parent receptacles, etc.; iterative decoding appends S1..St to input to generate St+1, encouraging temporally dynamic attention.",
            "task_name": "G-PlanET (table-augmented, iterative)",
            "task_description": "Same as G-PlanET but model has explicit textual access to environment (object table) and generates plans step-by-step using iterative decoding until an END token.",
            "task_type": "multi-step planning / navigation + manipulation",
            "knowledge_type": "spatial+procedural+object-relational",
            "knowledge_source": "pretraining on text corpora + fine-tuning on ALFRED-derived data with explicit environment encoding; iterative decoding uses model's internal reasoning across time steps (autoregressive feedback)",
            "has_direct_sensory_input": false,
            "elicitation_method": "supervised fine-tuning and iterative decoding (re-encode after each generated step); during training ground-truth previous steps are used; during inference previous model predictions are used.",
            "knowledge_representation": "environment as flattened textual table (explicit symbolic object representation); plans as natural-language step sequences; LM weights capture commonsense affordances; iterative decoding implements a temporal attention / stateful planning mechanism in text.",
            "performance_metric": "CIDEr, SPICE, KAS (step-wise); reported on unseen and seen room splits",
            "performance_result": "Examples from Table 1 (Unseen Room Layouts): BART-base w/table + iterative decoding: CIDEr 2.9147, SPICE 0.5107, KAS 0.6334. BART-large w/table + iterative decoding: CIDEr 2.8580, SPICE 0.5194, KAS 0.6518. (Seen rooms similar magnitudes.) These are substantially higher than vanilla baselines.",
            "success_patterns": "More accurate grounding of object locations and receptacles (e.g., identifies 'laptop on coffee table'), better prediction of correct number of steps, improved handling of long-horizon tasks, and higher step-wise action correctness (KAS). Iterative decoding reduces error accumulation by allowing the model to condition on previous generated steps.",
            "failure_patterns": "Still degrades on very long tasks; spatial precision and early-step correctness remain challenging in some cases; errors in step ordering or inexact spatial phrasing can still cause incorrect execution if followed literally.",
            "baseline_comparison": "Compared to BART-large (vanilla): KAS improved from 0.4069 -&gt; 0.6518 on unseen rooms. Compared to BART-large w/table single-pass: 0.4411 -&gt; 0.6518 (iterative adds big improvement).",
            "ablation_results": "Ablating the table (i.e., vanilla) strongly reduces KAS/SPICE/CIDEr. Replacing single-pass with iterative decoding substantially increases KAS (example: BART-large w/table single-pass KAS 0.4411 -&gt; iterative 0.6518). Iterative decoding also improves step-count prediction accuracy (less underestimation).",
            "key_findings": "Flattened object tables + iterative stepwise decoding is a practically effective combination for grounding LM-generated plans: explicit tabular environment encodings provide spatial and object-relational facts (positions, receptacles, rotations) and iterative decoding enforces temporal coherence and better state tracking across steps, significantly improving key-action correctness (KAS).",
            "uuid": "e342.2",
            "source_info": {
                "paper_title": "On Grounded Planning for Embodied Tasks with Language Models",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "TAPEX (table-pretraining)",
            "name_full": "TAPEX (pretrained table encoder applied to BART)",
            "brief_description": "A model pre-trained on table-related tasks (SQL execution pretraining) that better leverages tabular inputs; when used in place of plain BART for environment-encoded inputs, it yields large gains in grounded planning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "TAPEX-adapted encoder (used with BART-large)",
            "model_size": null,
            "model_description": "TAPEX is a pretrained model optimized for tabular reasoning (pretraining task: SQL execution); in this paper TAPEX-style pretraining is applied to BART-large to improve use of flattened object tables.",
            "task_name": "G-PlanET (table-augmented with TAPEX pretraining)",
            "task_description": "Same as G-PlanET; model is specifically pre-trained for tabular inputs to better parse and reason over object tables (types, coordinates, receptacles) when producing grounded plans.",
            "task_type": "multi-step planning / navigation + manipulation",
            "knowledge_type": "spatial+procedural+object-relational (table-focused)",
            "knowledge_source": "table-pretraining (SQL execution tasks) + fine-tuning on ALFRED-derived grounded-plan data",
            "has_direct_sensory_input": false,
            "elicitation_method": "fine-tuning with TAPEX-pretrained weights for tabular reasoning; single-pass and iterative decoding evaluated",
            "knowledge_representation": "explicit tabular encoding (flattened rows) used as input and processed by a model with tabular pretraining, producing natural-language action sequences; spatial/object relations are represented as table fields",
            "performance_metric": "CIDEr, SPICE, KAS",
            "performance_result": "Unseen-room split: BART-large (TAPEX) single-pass: CIDEr 2.8824, SPICE 0.5054, KAS 0.6373. BART-large (TAPEX) + iterative decoding: CIDEr 2.8440, SPICE 0.5210, KAS 0.6313. These are large improvements over vanilla BART without table.",
            "success_patterns": "TAPEX-pretrained model makes fuller use of tabular environment inputs and shows strong gains in step-action correctness and semantic alignment of steps to object relations; better at mapping object fields to grounded actions.",
            "failure_patterns": "Even with TAPEX, some iterative variants showed similar or slightly lower KAS vs BART-iterative in certain splits; long-horizon planning and precise early-step importance remain challenging.",
            "baseline_comparison": "Outperforms BART-large vanilla and BART-large w/table single-pass in many metrics (e.g., TAPEX single-pass KAS 0.6373 vs BART-large vanilla 0.4069).",
            "ablation_results": "Replacing plain BART table encoder with TAPEX-style pretraining yields substantial gains; combining TAPEX with iterative decoding yields comparable but not strictly superior results to BART iterative in all reported splits (mixed gains).",
            "key_findings": "Pretraining on table-specific tasks (TAPEX) makes models substantially better at using flattened object tables to ground plans, confirming that specialized pretraining enables LMs to extract spatial and object-relational information from textual tables more effectively.",
            "uuid": "e342.3",
            "source_info": {
                "paper_title": "On Grounded Planning for Embodied Tasks with Language Models",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "GPT-J-6B (few-shot)",
            "name_full": "GPT-J 6B (in-context few-shot LLM)",
            "brief_description": "A large causal transformer model evaluated in few-shot in-context learning mode to test whether large LMs can perform grounded planning with only a few examples and textual environment data.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-J-6B",
            "model_size": "6B",
            "model_description": "A 6B-parameter causal language model (GPT-J) used in in-context few-shot experiments to probe whether large models can learn G-PlanET-like mapping from goal+table to plan with few demonstrations rather than fine-tuning.",
            "task_name": "G-PlanET (few-shot in-context)",
            "task_description": "Provide a handful of example (goal + flattened object table -&gt; plan) demonstrations in-context and prompt GPT-J to generate a plan for a new goal/environment pair without parameter updates.",
            "task_type": "multi-step planning / instruction following",
            "knowledge_type": "spatial+procedural+object-relational (in-context)",
            "knowledge_source": "pretraining on large text corpora (implicit commonsense) + in-context examples at inference time",
            "has_direct_sensory_input": false,
            "elicitation_method": "in-context few-shot prompting",
            "knowledge_representation": "uses flattened textual table and examples in prompt; knowledge remains implicit in model activations and in-context demonstration patterns",
            "performance_metric": "CIDEr, SPICE, KAS",
            "performance_result": "Reported in Table 1 (Unseen): CIDEr 1.1968, SPICE 0.2655, KAS 0.3622; (Seen): CIDEr 1.1047, SPICE 0.2509, KAS 0.3370. These are better than some vanilla baselines but worse than fine-tuned table-augmented models.",
            "success_patterns": "Some use of in-context examples allows GPT-J to produce reasonably plausible action sequences and exploit some object-relational cues from the textual table when examples are well chosen.",
            "failure_patterns": "Few-shot performance lags fine-tuned models; in-context learning alone insufficient to robustly ground plans to precise spatial coordinates, receptacle relations, or long-horizon coherence; sensitive to prompt formatting and example selection.",
            "baseline_comparison": "Outperforms BART-base vanilla in some metrics? (See Table 1 for direct numbers) but underperforms fine-tuned BART w/table + iterative and TAPEX variants.",
            "ablation_results": "Paper reports GPT-J as a probing experiment; no detailed ablations reported for prompt variations in main text.",
            "key_findings": "Large causal LMs with few-shot prompting can partially leverage textual object tables and in-context examples for grounded planning, but fine-tuning and specialized table-pretraining plus iterative decoding produce substantially better, more reliable grounded plans.",
            "uuid": "e342.4",
            "source_info": {
                "paper_title": "On Grounded Planning for Embodied Tasks with Language Models",
                "publication_date_yy_mm": "2022-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ALFRED",
            "rating": 2
        },
        {
            "paper_title": "AI2-THOR: An Interactive 3D Environment for Visual AI",
            "rating": 2,
            "sanitized_title": "ai2thor_an_interactive_3d_environment_for_visual_ai"
        },
        {
            "paper_title": "TAPEX",
            "rating": 2
        },
        {
            "paper_title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents",
            "rating": 2,
            "sanitized_title": "language_models_as_zeroshot_planners_extracting_actionable_knowledge_for_embodied_agents"
        },
        {
            "paper_title": "SayCan",
            "rating": 2
        },
        {
            "paper_title": "ALFWorld",
            "rating": 1
        },
        {
            "paper_title": "SciWorld",
            "rating": 1
        },
        {
            "paper_title": "Language Models are Few-Shot Learners",
            "rating": 1,
            "sanitized_title": "language_models_are_fewshot_learners"
        }
    ],
    "cost": 0.015164999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>On Grounded Planning for Embodied Tasks with Language Models</p>
<p>Bill Yuchen Lin 
University of Southern
California</p>
<p>Chengsong Huang 
Fudan University</p>
<p>Qian Liu liuqian@sea.com 
Sea AI Lab</p>
<p>Wenda Gu wendagu@usc.edu 
University of Southern
California</p>
<p>Sam Sommerer sommerer@usc.edu 
University of Southern
California</p>
<p>Xiang Ren xiangren@usc.edu 
University of Southern
California</p>
<p>On Grounded Planning for Embodied Tasks with Language Models
738C41124BC4C50B4B70D17BFFF62272
Language models (LMs) have demonstrated their capability in possessing commonsense knowledge of the physical world, a crucial aspect of performing tasks in everyday life.However, it remains unclear whether they have the capacity to generate grounded, executable plans for embodied tasks.This is a challenging task as LMs lack the ability to perceive the environment through vision and feedback from the physical environment.In this paper, we address this important research question and present the first investigation into the topic.Our novel problem formulation, named G-PlanET, inputs a highlevel goal and a data table about objects in a specific environment, and then outputs a step-by-step actionable plan for a robotic agent to follow.To facilitate the study, we establish an evaluation protocol and design a dedicated metric, KAS, to assess the quality of the plans.Our experiments demonstrate that the use of tables for encoding the environment and an iterative decoding strategy can significantly enhance the LMs' ability in grounded planning.Our analysis also reveals interesting and non-trivial findings.</p>
<p>Introduction</p>
<p>Pre-trained language models (LMs) demonstrate exceptional proficiency in a wide range of natural language processing (NLP) tasks such as question answering, machine translation, and summarization.They indeed capture some commonsense knowledge about our physical world such as "birds can fly".However, the question of whether LMs can exhibit reasoning abilities within a grounded, realistic setting remains an open issue.This is because LMs lack the sensory experiences and physical interactions with the environment that enable human beings to grasp the nuances of real-life situations and plan for completing tasks.</p>
<p>Embodied robotics learning is a growing field that seeks to create artificial intelligence agents capable of navigating and performing tasks within real-world environments, typically simulated through physical engines such as AI2THOR (Kolve et al. 2017).The ALFRED benchmark (Shridhar et al. 2020) represents one of the pioneering</p>
<p>Grounded planning</p>
<p>Ø Turn right and walk to the stove.</p>
<p>Ø Pick up the tea pot on the left side of the stove.</p>
<p>Ø Turn left and walk to the shelves on the right.</p>
<p>Ø Place the tea pot on the middle shelf to the left of the glass container.</p>
<p>LMs</p>
<p>Task: Move a teapot from the stove to a shelf.</p>
<p>Figure 1: The task of grounded planning for embodied tasks (G-PlanET).The input to the LMs is a goal with a specific environment, and the output is a step-by-step plan that can guide a robot to complete the task.</p>
<p>datasets that bridges the gap between NLP and robotics, providing a platform for investigating language-directed agents.The objective of these studies is to design and test agents that can translate language instructions into sequences of low-level actions that enable the agent to manipulate objects within an environment and achieve a desired outcome (e.g., cleaning an object and placing it elsewhere).</p>
<p>However, the primary emphasis of the ALFRED benchmark and related datasets is on the comprehension of preestablished plans, rather than the ability to reason and independently plan within a realistic environment.Prior research focuses on the capacity of agents to comprehend and execute step-by-step plans, but not on their capacity for decomposing tasks and generating such plans, which represents a more advanced skill.Additionally, the role of LMs has received limited examination in the context of these benchmarks, where they are mainly used as encoders for embedding token sequences, rather than for planning or reasoning.</p>
<p>Prior studies have explored the planning capability of LMs, with Huang et al. (2022) demonstrating that GPT-3 and similar models are capable of generating general plans for executing everyday tasks.However, these plans lack grounding in a realistic environment, as LMs are not environment-specific.As a result, these plans are not necessarily executable by agents.For instance, in the context of an ALFRED task to "move a teapot from the stove to a shelf," embodied agents require knowledge of the location of the teapot and the path to reach it.Humans, on the other hand, can readily observe the location of the teapot on the stove and their current position in the kitchen, allowing them to formulate a grounded plan that starts with "turn right and walk to the stove."This highlights the need for generating detailed, step-by-step action sequences for robotic agents to use in their execution processes.</p>
<p>Can LMs also learn grounded planning ability?How should we evaluate and improve LMs for grounded planning?In this paper, we address the question of whether LMs can also learn grounded planning abilities.To this end, we propose a study on the ability of language models for grounded planning for embodied tasks (G-PlanET).Our approach involves providing LMs with two inputs: a highlevel task description and a realistic environment in the form of an object table.The output is a plan consisting of executable, step-by-step actions.We formulate G-PlanET as a language generation task and focus on encoder-decoder language models such as BART (Lewis et al. 2020).</p>
<p>In order to establish a dataset and evaluation protocol for G-PlanET, we leveraged the ALFRED data by developing a suite of data conversion programs.They extract the object information from the environment and format it into data tables, thereby enabling models to access observations from realistic scenarios.Additionally, we formulated a new evaluation metric, referred to as KAS, that is more appropriate for the task than existing ones for text generation.As regards the methodology of G-PlanET, we suggest flattening an object table into a sequence of tokens and appending it to the task description as input to the model.The base LMs are then fine-tuned with these seq2seq data to learn to generate plans.Furthermore, we propose a simple yet effective decoding strategy that iteratively generates subsequent steps by incorporating the previous generation into the input.Our empirical results and analysis indicate that incorporating object tables into inputs and the proposed iterative decoding strategies are both crucial for enhancing the performance of language models in G-PlanET.</p>
<p>To summarize, our main contributions are:</p>
<p>• The task of G-PlanET: To the best of our knowledge, this is one of the first studies to investigate the ability of LMs for embodied planning in realistic environments.G-PlanET is crucial for advancing the grounded generalization of large LMs and bridging the gap between NLP and embodied intelligence.(Sec.2)</p>
<p>• A comprehensive evaluation protocol: We put significant effort to convert the ALFRED and AI2THOR data into data tables to support the evaluation of G-PlanET.We also created a new evaluation metric, KAS, to effectively assess the plans generated by the LMs.• Improving LMs for G-PlanET: We present two simple but effective components for enhancing the grounded planning ability of LMs -flattening object tables and an iterative decoding strategy.Our experiments show that these components lead to notable performance gains.(Sec.3) Also, through extensive experimentation and indepth analysis, we have gained a deeper understanding of the behavior of LMs for G-PlanET and present a series of non-trivial findings in our study.</p>
<p>Problem Formulation</p>
<p>Here we present the background knowledge, the problem formulation and the data sources for G-PlanET.Language instructions.Language instructions play an important role in the ALFRED benchmark.The embodied tasks are annotated with a high-level goal and a low-level plan (i.e., a sequence of executable actions for robots) in natural language, which are both inputs to the agents.The agents need to understand such language instructions and parse them into action templates.Note that the agents do not need to plan for the task, as they already have the step-bystep instructions to follow.</p>
<p>Background Knowledge</p>
<p>Task planning.Prior works show that large pre-trained language models (LMs) such as GPT-3 (Brown et al. 2020) can generate general procedures for completing a task.However, such plans are not aligned with the particular environment in which we are interested.This is because these methods never encode the environment as part of the inputs to LMs for grounding the plans to the given environment.Therefore, such non-grounded plans are hardly useful in guiding agents to work in real-world situations.</p>
<p>G-PlanET with LMs</p>
<p>As discussed in Sec.2.1, the ALFRED benchmark does not explicitly test the planning ability, while prior works on planning with LMs have not considered grounding to a specific environment.In this work, we focus on evaluating and improving the ability to generate grounded plans for embodied tasks with LMs, which we dub as G-PlanET.It has been an underexplored open problem for both the robotics and NLP communities.</p>
<p>Task formulation.The task we aim to study in this paper is essentially a language generation problem.Specifically, the input is two-fold: 1) a high-level goal G and 2)
G +𝑆 ! + … + 𝑆 " + E 𝑆 "#!</p>
<p>Object table for the env. (E)</p>
<p>Realistic env.</p>
<p>G + E  !,  % , … ,  &amp; Task (G): Move a teapot from the stove to a shelf.</p>
<p>Figure 2: The overall workflow of the proposed methods.First, we extract the object table from the realistic environment.Then we flatten the table into a sequence of tokens E (Sec.3.2).We provide two learning methods for generating plans: 1) generate the whole plan S 1 , S 2 , ⋯, S T and 2) iteratively decode the S t+1 (Sec.3.3).</p>
<p>a specific environment E that the agents need to ground to.The expected output is a sequence of actionable plans S = {S 1 , S 2 , ⋯} to solve the given goal in the specific environment step-by-step.The goal G and the plan S are in the form of natural language, while the environment E can be viewed as a data table consisting of the object information in a room.Figure 2 shows an illustrative example and we will discuss more details in Section 3.2.</p>
<p>Data for G-PlanET</p>
<p>To build a large-scale dataset for studying the G-PlanET task, we re-use the goals and the plans of ALFRED and extract object information from AI2THOR for the aligned environment.The ALFRED dataset uses the AI2THOR engine to provide an interactive environment for agents with an egocentric vision to perform actions.However, the dataset does not contain explicit data about objects in the environment (e.g., the coordination, rotation, and spatial relationship with each other).</p>
<p>We develop a suite of conversion programs for using AI2THOR to re-purpose the ALFRED benchmark for evaluating the methods shown in Section 3. We managed to get a structured data table to describe the environment of each task in the ALFRED dataset.We explore the AI2THOR engine and write conversion programs such that we can get full observations of all objects: properties (movable, openable, etc.), positions (3D coordinates &amp; rotation), sizes, and spatial relationships (e.g., object A is on the top of object B).We believe our variant of the ALFRED data will be a great resource for the community to study G-PlanET and future directions in grounded reasoning.</p>
<p>Methods</p>
<p>Herein, we introduce the methods that we adopt or propose to address the G-PlanET problem.First of all, we present the base language models that are encoder-decoder architectures.Then, we show in detail how we encode the environment data and integrate them with the seq2seq learning frameworks.Finally, we propose an interactive decoding strategy that significantly improves performance.</p>
<p>Base Language Models</p>
<p>Pretrained encoder-decoder language models, such as BART (Lewis et al. 2020) and T5 (Raffel et al. 2019), have achieved promising performance in many well-known language generation tasks such as summarization and question answering.They also show great potential for general commonsense reasoning tasks such as CommonsenseQA (Talmor et al. 2019), suggesting that these large LMs have common sense to some extent.As the G-PlanET can be also viewed as a text generation problem, we use these LMs as the backbone for developing further planning methods, hoping that their common sense can be grounded in real-world situations for embodied tasks.</p>
<p>Vanilla baseline methods.</p>
<p>As shown in many papers, BART and T5, when sizes are similar, show comparable performance in many generation tasks.Thus, we use BARTbase and BART-large as two selected LMs for evaluation.The simplest and most straightforward baseline method of using such LMs to solve G-PlanET is to ignore the environment and only use the goal as the sole input.Then, we fine-tune the base LMs with the training data and expect they can directly output the whole plan as a single sequence of tokens (including special separator tokens).This simple method does not allow the LMs to perceive the environment, although training from the large-scale data can still teach the LMs some general strategies for planning.Therefore, we see this as an important baseline method to analyze.</p>
<p>Encoding Realistic Environments</p>
<p>To enable the LMs to perceive an environment, we need to encode the object tables described in Sec.2.2.Following prior works in table-based NLP tasks (Chen et al. 2020;Liu et al. 2022b), we flatten a table into token sequences row by row, thus creating a linearized version of an object table.Then, we append the flattened table after the goal to form a complete input sequence.Thus, the input side of the encoder-decoder finally has the environment information for generating a grounded plan.</p>
<p>Considering the max sequence limit, we only choose to encode objects by their type, position, rotation, and the receptacle parent.The object type does not only tell what an object is but also implies commonsense affordance (e.g., a microwave can heat up something, a knife can slice something) which is very important for planning.The position information is essential for agents to navigate and find objects, thus playing an important part in planning.The rotation is also useful for some objects that can only be used with a certain orientation (e.g., a refrigerator can only be opened when the agent is in front of it).The receptacle of an object and itself has a close spatial connection (e.g., a pen is on a desk; an apple is in a fridge).Every object has a unique identifier such that objects of the same type can be referred to precisely when they are receptacles of others.In addition, the agent is represented as a special object.</p>
<p>Iterative Decoding Strategy</p>
<p>Adding the flattened table of object information to the input sequences indeed improves the LMs in terms of their perception of the realistic environments, which forms the foundation of grounded planning.However, the thinking process is still limited by the conventional seq2seq learning framework, which assumes LMs should output a complete plan by a single pass of decoding.We argue that a thoughtful planning process should carefully handle the coherence of each step, otherwise errors accumulate and cause a failed plan.</p>
<p>Therefore, we propose a simple yet effective decoding strategy that learns to iteratively generate a plan step by step.Specifically, we append previously generated steps until the current step t to the input sequence (i.e., Input = [G + S 1 + ⋯ + S t (+E)]) for generating the next step (i.e., Output = S t+1 ).This iterative decoding process will end until the LM generates the special token END.In the training stage, we use the ground-truth references for S ≤t ; in the inference stage, we do not have such references, so we use the model predictions as S ≤t .</p>
<p>Notably, in contrast to the conventional seq2seq learning process, the iterative decoding strategy needs to run the encoder-decoder model N + 1 times to generate a plan with N steps.The additional computation cost for re-encoding is worthy.Imagine when we humans are planning a task in a room.It is natural for us to come up with the plans step by step, and it is very likely that the most useful information to generate different steps is about different objects.Therefore, a temporally dynamic attention mechanism is favorable in planning with LMs.Our iterative decoding strategy encourages the encoder-decoder architectures to learn such ability.</p>
<p>Other Methods</p>
<p>Pretrained table encoders.Since we use environmental information in a tabular format and BART has not been pretrained in the tabular form of input, BART may not be able to use this part of information well.Therefore, we employ TAPEX (Liu et al. 2022b), the state-of-the-art pre-trained language model on tabular data.Using SQL execution as the only pre-training task, TAPEX achieves better tabular reasoning capability than BART, and thus we expect TAPEX can make full use of the environmental information represented by the table in our task.</p>
<p>In-context few-shot learning with GPT-J.Finally, to explore whether large-scale language models can master the task with few-shot examples, we also experimented with few-shot performance on a larger language model GPT-J 6B.</p>
<p>Evaluation</p>
<p>How do we evaluate a method for G-PlanET?Due to the novelty of the problem setup, it is challenging to evaluate and analyze the methods.In this section, we present a general evaluation protocol and a complementary metric to measure the quality of generated plans.We report the main experimental results with the proposed evaluation protocol.We leave the analysis in Sec. 5.</p>
<p>Metrics</p>
<p>Step-wise evaluation.Conventional evaluation metrics such as BLEU (Papineni et al. 2002) and ROUGE (Lin 2004) measure the similarity between generated text and truth references as a whole, which is suitable for translation and summarization.However, the output text of planning tasks such as our G-PlanET is highly structured.A plan naturally can be split into a sequence of step-by-step actions.Using the conventional way to evaluate plans inevitably breaks such internal structures and will lead to inaccurate measurement.For example, if the first step of the generated plan is the same as the last step of the reference plan, the conventional evaluation will still assign a high score to such a generated plan, even though it is not useful at all.Therefore, we argue that it is much more reasonable to evaluate the similarity of a pair of plans step by step.Specifically, we first align the generations and the truths and compute the scores of every step2 by multiple metrics.Then, we aggregate the final score by taking the average of all steps.We also consider other temporal weighting aggregation for more analysis in Sec. 5.</p>
<p>Measuring grounded plans.It is a unique challenge for evaluating G-PlanET to consider the grounding nature of plans.Metrics, such as BLEU, METEOR, and ROUGE, do not give a suitable penalty when a plan is similar to the reference in terms of word usage, yet leading to totally different states in an interactive environment for embodied tasks.For example, it is only a one-word difference between "turn to the left" vs "turn to the right", but the agents that faithfully follow these instructions can arrive at very different places.</p>
<p>The LM-based metrics, e.g., BERTScore (Zhang et al. 2020), are not suitable either because the neural embeddings of "left" and "right" are also very similar.Plus, the grounded plans for G-PlanET are object-centric in a context and very similar to the captions of a sequence of events by visual perception, for which these metrics are not specifically designed.Considering these limitations, we use two typical metrics that are widely used for captions and devise a new metric for complementary measurement.</p>
<p>The first two metrics are CIDEr (Vedantam, Zitnick, and Parikh 2015) and SPICE (Anderson et al. 2016), which are both widely used for tasks where the outputs are highly contextualized and describe natural scenarios in everyday life, e.g., VaTex (Wang et al. 2019) and CommonGen (Lin et al. 2020).In particular, SPICE parses both the generation and references to scene graphs, a graph-based semantic representation.Then, it calculates the edge-based F1 score to measure the similarity between each step.Note that SPICE computation has a special focus on the propositions.This is particularly favorable for evaluating G-PlanET since there are many actions in the grounded plans, where propositions can be seen as atomic units for evaluation.</p>
<p>KeyActionScore (KAS).</p>
<p>Inspired by SPICE, a step in a plan can be deconstructed into several propositions that are represented as edges.However, not all propositions in SPICE are necessarily important in evaluating plans for G-PlanET.Not to mention that SPICE relies on an external parser that is expensive to run yet sometimes contains noisy outputs.Also, most of the truth plans in the ALFRED annotations are overly specific, and it is not necessary for a plan to cover all details.Therefore, we devise a metric that focuses on the key actions of the generated plans and checks if they are part of references, named Key Action Score (KAS).</p>
<p>Specifically, we extract a set of key action phrases from each step in the generated plan Ŝi and the truth reference S i respectively.We denote this two sets as Ŝi = {â 1 , â2 , ⋯}  and S i = {a 1 , a 2 , ⋯}.Then, we check how many action phrases in Ŝi are covered by the truth set S i , the precision then becomes the KAS score for the i-th step in the plan.To increase the matching quality, we curate a set of rules and a dictionary to map the actions that share the same behaviors.For example, "turn to the left" and "turn left" are counted as a single match; "go straight" and "walk straight" can be matched too.In addition, we break the compound nouns such that we allow partial scores to match for a smoother scoring (e.g., "xxx on the table" vs "xxx on the coffee table").Simply put, the KAS metric looks at the key actions extracted from the plans and checks if these important elements can be (fuzzy) matched to count as a valid step.</p>
<p>Experimental Setup</p>
<p>Data statistics.Table 2 shows some statistics of our dataset that we described in Sec.2.2.We follow the data split in ALFRED to split the train, valid, and test dataset.The data split is based on whether the room layout has been seen in the training tasks.It is usually easier for robotic agents to map instructions to low-level actions in seen rooms than in unseen rooms.However, for the planning ability that we want to study with G-PlanET in this paper, the two splits do not differ very much.We keep using this split to make the results consistent and convenient for people who want to connect our results with the ALFRED results.</p>
<p>Implementation</p>
<p>details.In single-pass decoding, we format the output sequences as follows:
"Step 1:[S 1 ] | Step 2: [S 2 ] | ⋯ | END". When appending the flattened table of objects, we format input with "[G] Env: [row 1] [SEP] [row 2] ⋯",
where the [row i] is a sequence of the i-th object including its id, type, coordinates, rotation, parent receptacles, etc. Due to the page limit, we leave the details of the data, methods, and hyper-parameters in the Appendix that are linked to our project website.</p>
<p>Main Results</p>
<p>We report the main results in Table 1, and leave the deeper analysis in the next section.To sum up, we find that encoding the object table as part of the inputs will significantly improve the performance, and pre-training on other tablerelated tasks can benefit G-PlanET a lot.The iterative decoding strategy is also an important component that can further improve the results to some extent.</p>
<p>Case Study of Table Effect</p>
<p>Although we have added environment information E to the input, it is still a problem whether the model effectively uses this information.To verify this, we present a case study here.In a number of instances, we have demonstrated that the introduction of environmental information can be helpful.Here is one example:</p>
<p>• truth: Close the laptop that is on the table.</p>
<p>• vanilla: Close the laptop and pick it up from the bed • w/ table: Pick up the laptop on the coffee table.As shown in the example, the model successfully identified the location of a laptop with the help of the object table.</p>
<p>Effect of model sizes.Table 1 shows that small models can perform as well or even better than large models in some cases.This is mainly due to the following reasons.1) The sentences in plans are relatively simpler than other NLG tasks, with a smaller vocabulary and shorter length.This leaves the power of large models in terms of generation unexpressed, 2) G-PlanET is a task to examine the ability to plan rather than write.Whether this ability changes with Step Range model size remains to be explored.3) For scenarios with the table, the form of the task is not the same as the traditional generation task, so the training phase will have a greater impact.Models with fewer parameters are more sufficiently tuned with limited data.
Count Vanilla Table TAPEX Table Iter TAPEX Iter</p>
<p>Analysis</p>
<p>In this section, we deeply analyze the performance of the methods in Table 1 from multiple aspects and provide nontrivial findings that can help future research.For a fair comparison, all analytical experiments were performed in the BART-large model on the unseen split of the test data.</p>
<p>Temporal Re-weighting of Scores</p>
<p>When we computed the overall score of a plan with a metric, we use the average score to aggregate the score for each step.However, in a realistic environment, there are causality constraints for an agent to complete the steps -i.e., some tasks can only be done when their prerequisite steps are finished.For example, only when the agent arrives at the microwave can it heat the bread in its hands.Therefore, the earlier steps in a plan should be of higher importance, while our previous evaluation is based on a uniform distribution of the weights across steps.To this end, we adopt geometric distribution to re-weight the step-wise importance for weighted aggregation.The geometric distribution can be used to model the number of failures before the first success in repeated mutually independent Bernoulli trials, each with a probability of success p.
f (x) = p(1 − p) x (0 &lt; p &lt; 1)
This suits our setup well because when the first step is incorrect, the whole task can hardly be completed and executed in a generated plan for ALFRED.The range of p in the original setting of the geometric distribution is restricted to between 0 and 1.When p = 0, each step has the same weight (uniform importance), which is exactly what we have done in Tab. 1.When p = 1, the first step is the only thing we look at for evaluation, meaning that the other steps will be given zero weights for aggregation.</p>
<p>Figure 3 shows the results on unseen subset which is more realistic.The performance of the iterative and non-iterative approaches is very close in the case of the first step.This is mainly because iterative methods are similar to non-iterative methods when generating the first step, and differ only after   the second step.At the same time, it can be seen that there is an overall downward trend in performance as the focus moves to the early step.The main reason is that the later the subtask is, the closer it is to the high-level instruction.For example, if the task goal is to place the sponge in the sink, the final step must be to place the sponge in the sink.This feature makes the last step of subtask generation very simple, resulting in high performance.We also see that the performance of the non-iterative method rises and then falls in KAS, and the change in a downward trend in SPICE.The main reason is an error in the number of steps in the noniterative method, which will be explained next.</p>
<p>Error Analysis on the Lengths of Plans</p>
<p>We found a huge gap in the prediction of the number of task steps between iterative and non-iterative methods, which may be an important reason for the final performance difference.As shown in Figure 4, iterative methods have a higher probability of predicting the number of steps for the correct task, while non-iterative methods do underestimate the number of steps.In our evaluation framework, the missing follow-up steps of non-iterative methods are often generated by copying.This might be reason for the poor performance of non-iterative methods and the performance of noniterative methods increases first in the reweight step process.</p>
<p>Impact of Task Length on Performance</p>
<p>Although all the tasks in the dataset are part of daily life tasks, they differ in difficulty.A simple metric to evaluate the difficulty of a task is the number of steps they require.Figure 5 illustrates the decrease in the quality of the generated steps as the number of task steps increases.The figure also reflects the relatively small difference in the performance of the different methods on shorter tasks.And the performance of all methods degrades rapidly on the longest tasks.The iterative approach has more significant performance benefits on longer tasks.This may be because this approach makes better use of the state changes due to intermediate steps and fixes some previous errors.</p>
<p>Related Work</p>
<p>Grounded commonsense reasoning.ALFWorld (Shridhar et al. 2021) also uses LM to generate the next step in a text game which is based on ALFRED.SciWorld (Wang et al. 2022) designed a text game to find whether the LMs have learned to reason about commonsense.SayCan (Ahn et al. 2022) also uses LM to find the potential next step in the real world.Both these three works only expect to learn the next step in a text game.Our methods share similar motivation with decision transformer (Chen et al. 2021) and Behavior Cloning (Farag and Saleh 2018), but we work on very different applications.</p>
<p>Table-based NLP.Our work is closely related to two lines of tabular data usage in NLP: the approach to modeling tabular representations and the application of a table as an intermediate representation.For the first line of work, there is rich literature focusing on modeling tabular representations, including TabNet (Arik and Pfister 2019), TAPAS (Herzig et al. 2020), TaBERT (Yin et al. 2020) and TAPEX (Liu et al. 2022b).We have explored the impact of state-of-theart table representation models (e.g., TAPEX) on our task in experiments.As for the second line of work, previous work has explored to use of tables in several downstream tasks, including visual question answering (Yi et al. 2018), code modeling (Pashakhanloo et al. 2022), andnumerical reasoning (Pi et al. 2022;Yoran, Talmor, and Berant 2022).Different from them, our work is the first to explore the use of tabular representations in embodied tasks.</p>
<p>ALFRED Agents.Some previous research has been published on embodied tasks in realistic environments since the appearance of ALFRED.E.T. (Pashevich, Schmid, and Sun 2021) first encoded the history with a transformer to solve compositional tasks and proved that pretraining and joint training with synthetic instructions can improve performance.FILM (Min et al. 2021) proposed an explicit spatial memory and a semantic search policy to provide a more effective representation for state tracking and guidance.LEBP (Liu et al. 2022a), the currently published SOTA method, generated a sequence of sub-steps by understanding the language instruction and used the predefined actual actions template to complete the sub-steps.We also try to use these methods to evaluate our generated low-level instructions.However, due to the limited importance of the low-level instructions, there is no gap with conspicuousness between our generated instructions and the ones in ALFRED.</p>
<p>Conclusion</p>
<p>In this work, we present the first investigation into grounded planning for embodied tasks using language models.The G-PlanET problem is of utmost significance for advancing the embodied intelligence of LMs and constitutes a critical step towards artificial general intelligence.To evaluate the performance of encoder-decoder LMs in solving G-PlanET, we developed a benchmark as well as a specialized evaluation metric named KAS to assess the quality of generated plans.Furthermore, we propose two methods for improving LMs' ability in G-PlanET -flattening object tables and an iterative decoding strategy.Our experiments and analyses demonstrate their effectiveness and yield non-trivial findings.This study is expected to encourage further research into G-PlanET and pave the way for integrating LMs and embodied tasks in realistic environments.</p>
<p>Figure 3 :
3
Figure 3: The step-wise reweighting results of KAS (Left) and SPICE (Right).The x-axis indicates the parameter p in the geometric distribution and also the importance of the preceding step, and the y-axis indicates the weighted result of each step.A larger coefficient means that the previous step is more important.</p>
<p>Figure 4 :
4
Figure 4: The result of error statistics for # of step.</p>
<p>Figure 5 :
5
Figure 5: The result of KAS of tasks with a different number of steps.Due to the large variance caused by the small number of samples of certain lengths, we use the statistics by dividing the intervals.</p>
<p>Table 1 :
1
Experimental results for the G-PlanET by different base LMs.The methods are grouped by model types and whether encoding the environment; by decoding strategies.
Data Split →Unseen Room LayoutsSeen Room LayoutsMethods ↓ Metrics → CIDEr SPICEKASCIDEr SPICEKASBART-base (vanilla)0.9417 0.1378 0.2455 0.8231 0.1277 0.2197BART-large (vanilla)1.4632 0.3168 0.4069 1.4414 0.3161 0.3900GPT-J-6B1.1968 0.2655 0.3622 1.1047 0.2509 0.3370BART-base w/table1.6706 0.3692 0.4584 1.6230 0.3595 0.4339BART-large w/table1.6630 0.3491 0.4411 1.5865 0.3393 0.4204BART-large (TAPEX)2.8824 0.5054 0.6373 2.7432 0.4944 0.6045BART-base w/table + iterative decoding2.9147 0.5107 0.6334 2.8582 0.5118 0.6124BART-large w/table + iterative decoding2.8580 0.5194 0.6518 2.8799 0.5096 0.6326BART-large (TAPEX) + iterative decoding 2.8440 0.5210 0.6313 2.6959 0.5036 0.6074</p>
<p>Table 2 :
2
The avg. |G| means the average length of goal and the avg.|S i | means the average length of each step.The avg.</p>
<h1>O is the average number of objects in each room and the avg.# T is the average number of steps.</h1>
<p>Table Iter TAPEX Iter
IterIter
The ALFRED authors ensure that the references consist of atomic action steps and all references share the same length. Therefore, we consider the length of truth plans as the standard: when the generated plan has more steps than the truth plans, we cut off them; when the generation has fewer steps than the references, we duplicate the last step to make them even for step-wise evaluation.</p>
<p>Grounding Language in Robotic Affordances. M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, K Gopalakrishnan, K Hausman, A Herzog, D Ho, J Hsu, J Ibarz, B Ichter, A Irpan, E Jang, R J Ruano, K Jeffrey, S Jesmonth, N J Joshi, R C Julian, D Kalashnikov, Y Kuang, K.-H Lee, S Levine, Y Lu, L Luu, C Parada, P Pastor, J Quiambao, K Rao, J Rettinghouse, D M Reyes, P Sermanet, N Sievers, C Tan, A Toshev, V Vanhoucke, F Xia, T Xiao, P Xu, S Xu, M Yan, Conference on Robot Learning. Not As I Say2022Do As I Can</p>
<p>SPICE: Semantic Propositional Image Caption Evaluation. P Anderson, B Fernando, M Johnson, S Gould, Proc. of ECCV. of ECCV2016</p>
<p>TabNet: Attentive Interpretable Tabular Learning. S Ö Arik, T Pfister, ArXiv, abs/1908.074422019</p>
<p>Language Models are Few-Shot Learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M Balcan, H Lin, NeurIPS2020. 2020. 2020. December 6-12, 2020</p>
<p>Decision Transformer: Reinforcement Learning via Sequence Modeling. L Chen, K Lu, A Rajeswaran, K Lee, A Grover, M Laskin, P Abbeel, A Srinivas, I Mordatch, 2021In NeurIPS</p>
<p>Behavior Cloning for Autonomous Driving using Convolutional Neural Networks. W Chen, H Wang, J Chen, Y Zhang, H Wang, S Li, X Zhou, W Y Wang, International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT). W A Farag, Z Saleh, Addis Ababa, Ethiopia2020. April 26-30, 2020. 2018. 201820208th International Conference on Learning Representations</p>
<p>TaPas: Weakly Supervised Table Parsing via Pre-training. J Herzig, P K Nowak, T Müller, F Piccinno, J Eisenschlos, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline: Association for Computational Linguistics2020</p>
<p>Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents. W Huang, P Abbeel, D Pathak, I Mordatch, E Kolve, R Mottaghi, W Han, E Vanderbilt, L Weihs, A Herrasti, M Deitke, K Ehsani, D Gordon, Y Zhu, A Kembhavi, A K Gupta, A Farhadi, ArXiv, abs/1712.05474AI2-THOR: An Interactive 3D Environment for Visual AI. 2022. 2017ICML</p>
<p>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. M Lewis, Y Liu, N Goyal, M Ghazvininejad, A Mohamed, O Levy, V Stoyanov, L Zettlemoyer, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2020</p>
<p>CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning. B Y Lin, W Zhou, M Shen, P Zhou, C Bhagavatula, Y Choi, X Ren, Findings of the Association for Computational Linguistics: EMNLP 2020, 1823-1840. Online: Association for Computational Linguistics. 2020</p>
<p>ROUGE: A Package for Automatic Evaluation of Summaries. C.-Y Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>LEBP -Language Expectation &amp; Binding Policy: A Two-Stream Framework for Embodied Vision-and-Language Interaction Task Learning Agents. H Liu, Y Liu, H He, H Yang, ArXiv, abs/2203.046372022a</p>
<p>Bleu: a Method for Automatic Evaluation of Machine Translation. Q Liu, B Chen, J Guo, M Ziyadi, Z Lin, W Chen, J.-G Lou, S Y Min, D S Chaplot, P Ravikumar, Y Bisk, R Salakhutdinov, K Papineni, S Roukos, T Ward, W.-J Zhu, ArXiv, abs/2110.07342Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPennsylvania, USAAssociation for Computational Linguistics2022b. 2021. 2002FILM: Following Instructions in Language with Modular Methods. Philadelphia</p>
<p>CodeTrek: Flexible Modeling of Code using an Extensible Relational Representation. P Pashakhanloo, A Naik, Y Wang, H Dai, P Maniatis, M Naik, A Pashevich, C Schmid, C Sun, Q Liu, B Chen, M Ziyadi, Z Lin, Y Gao, Q Fu, J.-G Lou, W Chen, Conference on Empirical Methods in Natural Language Processing. Pi, X2022. 2021. 2022Proc. of ICLR</p>
<p>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. C Raffel, N M Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, M Shridhar, J Thomason, D Gordon, Y Bisk, W Han, R Mottaghi, L Zettlemoyer, D Fox, X Yuan, M Côté, Y Bisk, A Trischler, M J Hausknecht, ArXiv, abs/1910.106839th International Conference on Learning Representations, ICLR 2021, Virtual Event. Seattle, WA, USA; Austria2019. 2020. June 13-19, 2020. 2021. May 3-7, 20212020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020. OpenReview.net</p>
<p>Com-monsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge. A Talmor, J Herzig, N Lourie, J Berant, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinnesotaAssociation for Computational Linguistics20191Minneapolis</p>
<p>CIDEr: Consensus-based image description evaluation. R Vedantam, C L Zitnick, D Parikh, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015. Boston, MA, USAIEEE Computer Society2015. June 7-12, 2015</p>
<p>ScienceWorld: Is your Agent Smarter than a 5th Grader?. R Wang, P A Jansen, M.-A Côté, P Ammanabrolu, Conference on Empirical Methods in Natural Language Processing. 2022</p>
<p>VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research. X Wang, J Wu, J Chen, L Li, Y Wang, W Y Wang, 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019. Seoul, Korea (South)IEEE2019. October 27 -November 2, 2019</p>
<p>Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding. K Yi, J Wu, C Gan, A Torralba, P Kohli, J Tenenbaum, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems. S Bengio, H M Wallach, H Larochelle, K Grauman, N Cesa-Bianchi, R Garnett, NeurIPS; Montréal, Canada2018. 2018. 2018. December 3-8, 2018</p>
<p>TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data. P Yin, G Neubig, W.-T Yih, S Riedel, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline: Association for Computational Linguistics2020</p>
<p>Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills. O Yoran, A Talmor, J Berant, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>BERTScore: Evaluating Text Generation with BERT. T Zhang, V Kishore, F Wu, K Q Weinberger, Y Artzi, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020. April 26-30, 20202020OpenReview.net</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1978 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1978</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1978</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-40.html">extraction-schema-40</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <p><strong>Paper ID:</strong> paper-282058156</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2510.10912v2.pdf" target="_blank">More than A Point: Capturing Uncertainty with Adaptive Affordance Heatmaps for Spatial Grounding in Robotic Tasks</a></p>
                <p><strong>Paper Abstract:</strong> Many language-guided robotic systems rely on collapsing spatial reasoning into discrete points, making them brittle to perceptual noise and semantic ambiguity. To address this challenge, we propose RoboMAP, a framework that represents spatial targets as continuous, adaptive affordance heatmaps. This dense representation captures the uncertainty in spatial grounding and provides richer information for downstream policies, thereby significantly enhancing task success and interpretability. RoboMAP surpasses the previous state-of-the-art on a majority of grounding benchmarks with up to a 50x speed improvement, and achieves an 82\% success rate in real-world manipulation. Across extensive simulated and physical experiments, it demonstrates robust performance and shows strong zero-shot generalization to navigation. More details and videos can be found at https://robo-map.github.io.</p>
                <p><strong>Cost:</strong> 0.029</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1978.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1978.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoboMAP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoboMAP (More than A Point: Adaptive Affordance Heatmaps)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language spatial-grounding framework that predicts dense, adaptive affordance heatmaps as a probabilistic intermediate representation for language-conditioned robotic tasks, trained with procedurally synthesized dense supervision from sparse annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoboMAP</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses a PaliGemma multimodal backbone to produce language-conditioned visual tokens reshaped into a low-resolution grid (F_low). A lightweight Adaptive Heatmap Decoder (AHD) with two branches (Coarse Affordance Predictor and Adaptive Kernel Generator) performs content-aware upsampling to output a high-resolution, continuous 2D affordance heatmap M mapping f_theta(I,x) -> M. Trained with BCE against procedurally synthesized ground-truth heatmaps from keypoints, boxes, and trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>PaliGemma multimodal VLM (backbone producing language-conditioned visual tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Language and image jointly encoded by PaliGemma producing language-conditioned visual tokens aligned to image grid; decoder produces pixelwise affordance probabilities via content-aware upsampling (per-pixel adaptive kernels) — i.e., dense pixel-level grounding from multimodal features.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>pixel-level (dense affordance heatmap)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>2D adaptive affordance heatmap over image grid (continuous probability distribution); used with depth/post-processing to compute 6-DoF grasps and 3D place coordinates.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>object manipulation (single-arm, dual-arm), industrial sorting, mobile navigation, pick-and-place</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Where2place, RoboRefIt, RefSpatial, VABench, SimplerEnv (WidowX evaluations), real-world dual-arm and industrial single-arm experiments, zero-shot navigation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>simulated environments (SimplerEnv), indoor/outdoor navigation, and multiple real-world robot setups (tabletop dual-arm, single industrial arm)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>localization accuracy / success rate / inference time</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Where2place: 73.0% accuracy; RoboRefIt: 88.73% accuracy; RefSpatial: 36.50% accuracy; VABench-Point: 70.0% accuracy (Table I/II); Zero-shot SimplerEnv manipulation: 60.5% success; Real-world manipulation average success rate: 82%; Inference time: 0.04 s (25 Hz).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Ablations replacing AHD with standard upsampling show degraded grounding: Bilinear upsampling yields Where2place 64.0% (vs AHD 73.0%), Deconvolution 64.0%, PixelShuffle 59.0% (Table II).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>AHD improves Where2place by +9.0% absolute over bilinear, and improves across benchmarks up to +9% (Table II). Overall RoboMAP outperforms best prior discrete/point-based methods on 3 of 4 benchmarks and improves embodied task success (e.g., zero-shot manipulation +4.3% vs Embodied-R1 in SimplerEnv).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Paper identifies an information bottleneck from collapsing spatial targets into deterministic points/bounding boxes: discarding spatial extent and uncertainty leads to brittleness under perceptual noise and semantic ambiguity; also notes downstream post-processing (selecting only the peak pixel) as a separate bottleneck that causes failures on surfaces with height variation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Enumerates grounding failure modes: (1) point-based methods either localize to wrong surrounding objects (clear localization errors) or place scattered/insufficient points failing to represent a coherent valid region; (2) prior heatmap baseline (BridgeVLA-pretrain) fails to ground object-free regions; (3) RoboMAP's downstream heuristic of selecting single peak causes execution failures on surfaces with height variation even when heatmap is correct. Paper gives empirical evidence via qualitative visualizations and task-specific success rates (e.g., Table IV real-world place task variations).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Handles domain shift by training on a hybrid dataset mix (Robodata, COCO, OXE-MAP, RoboRefIt — total 132K samples) and synthesizing trajectory supervision to bridge disparate data sources (OXE->OXE-MAP). Demonstrates strong zero-shot transfer to real robots and navigation without task-specific fine-tuning (e.g., 60.5% zero-shot SimplerEnv success, 82% real-world manipulation), but no explicit domain-adaptation layers.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Paper reports strong zero-shot generalization qualitatively and via overall success rates in real-world and novel navigation tasks, but does not provide per-class novel-vs-seen object breakdown.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Vision encoder and token embeddings remained frozen during fine-tuning; the paper does not report a comparison between frozen and fully fine-tuned encoder settings.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Presents ablation over training data composition: full hybrid mix (Robodata+COCO+OXE-MAP+RoboRefIt) yields best performance; OXE-MAP-only and other single-source mixes perform substantially worse (Table III). No explicit study of pretraining scale (web-scale) effects.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Early joint multimodal fusion via PaliGemma producing language-conditioned visual tokens aligned to image grid; decoder then conditions on these tokens (implicit cross-modal fusion inside backbone).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Fine-tuned on 132K training samples (Robodata 62K, COCO 22K, OXE-MAP 18K, RoboRefIt 30K) for 2 epochs; training performed with frozen backbone and light decoder (1.6M params). No direct sample-efficiency comparison vs non-grounding methods provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Dense, adaptive heatmaps that model spatial uncertainty produce more robust and interpretable grounding than point/bbox/trajectory collapse; a content-aware upsampling decoder (AHD) is critical for high-fidelity adaptive heatmaps, hybrid/synthesized supervision (converting sparse labels to dense heatmaps) enables scalable training, and using the full heatmap (rather than a single peak) is crucial for better downstream execution though current pipelines underutilize this signal.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1978.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1978.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AHD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive Heatmap Decoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Lightweight decoder that generates high-resolution adaptive affordance heatmaps from low-resolution language-conditioned features via content-aware upsampling with per-pixel learned kernels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Adaptive Heatmap Decoder (AHD)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-branch decoder: (1) Coarse Affordance Predictor (CAP) produces a low-resolution affordance estimate M_low; (2) Adaptive Kernel Generator (AKG) predicts a unique k×k normalized upsampling kernel W(i,j) for each output pixel. Final pixel values are computed by applying W(i,j) to the corresponding neighborhood of M_low (content-aware, softmax-normalized kernels).</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Per-pixel adaptive upsampling of a language-conditioned coarse affordance map to produce dense pixelwise grounding reflecting instruction uncertainty (sharp kernels for precise targets, broad kernels for ambiguous regions).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>pixel-level</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>2D adaptive affordance heatmap</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>used as intermediate grounding representation for manipulation and navigation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Where2place, RoboRefIt, RefSpatial, VABench (benchmarks used in ablations)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>benchmarks and robot datasets (simulated and real images)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>benchmark localization accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table II: AHD yields Where2place 73.00%, RoboRefIt 88.73%, RefSpatial 36.50%, VABench 70.00% — outperforming Bilinear/Deconvolution/PixelShuffle variants.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Replacing AHD with Bilinear: Where2place 64.0% (−9.0%); PixelShuffle: 59.0% (−14.0%); Deconvolution: 64.0% (−9.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>AHD improves grounding accuracy across benchmarks up to +9% absolute compared to standard decoders (Table II).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>No additional failure-mode breakdown beyond showing other decoders produce lower-quality heatmaps and lower downstream accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Consumes language-conditioned feature grid from backbone (implicit fusion already performed in backbone); decoder performs content-aware spatial upsampling.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Decoder has ~1.6M parameters and was trained with frozen backbone; contributes little compute overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Content-aware per-pixel adaptive upsampling is essential to produce heatmaps that adapt shape to instruction ambiguity, improving grounding accuracy versus content-agnostic upsampling.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1978.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1978.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AKG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive Kernel Generator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A branch of AHD that predicts a unique normalized k×k upsampling kernel for each output pixel, enabling content-aware upsampling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Adaptive Kernel Generator (AKG)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>AKG processes the low-resolution feature grid and outputs per-target-pixel k×k kernels W(i,j) that are softmax-normalized; these kernels determine how to aggregate local coarse affordance values to form the high-resolution heatmap.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Creates per-pixel, content-conditioned upsampling kernels that shape the final affordance distribution, allowing grounding to express uncertainty and non-circular regions.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>pixel-level</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>contributes to 2D affordance heatmap shape via per-pixel kernel predictions</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>manipulation, navigation (as part of RoboMAP)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Where2place, RoboRefIt, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>images (simulated and real)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>benchmark accuracy improvements when used in AHD (see AHD entry / Table II)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Contributes to AHD's performance: final AHD results (Where2place 73.0%, RoboRefIt 88.73%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Ablation of adaptive upsampling (i.e., using content-agnostic upsampling) leads to substantial performance drops (see Table II).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Enables shape-adaptive heatmaps which yield up to +9% absolute improvements on benchmarks relative to non-adaptive upsampling.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Works on language-conditioned visual feature grid; kernel predictions depend on multimodal context.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Learning per-pixel upsampling kernels is an effective mechanism to allow heatmap shape to reflect instruction-specific ambiguity and scene content.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1978.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1978.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CAP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Coarse Affordance Predictor</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A branch of AHD that predicts a low-resolution affordance map (M_low) which provides the content that AKG upsamples into the final heatmap.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Coarse Affordance Predictor (CAP)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Processes the low-resolution language-conditioned features to produce a preliminary low-resolution affordance estimate M_low used by the per-pixel adaptive kernels to assemble the final high-resolution heatmap.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Predicts coarse pixelwise affordance probabilities conditioned on multimodal features; grounding refined via AKG upsampling.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>low-resolution pixel-level</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>coarse 2D affordance map</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>manipulation, navigation (as part of RoboMAP)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>benchmarks used in paper</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>images</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>contributes to final accuracy (see AHD)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Essential component of AHD pipeline; removal or replacement degrades final heatmap quality shown in ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Operates on language-conditioned visual tokens from PaliGemma.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>A coarse-to-fine pipeline (CAP + AKG) provides a flexible and efficient route to produce high-resolution, instruction-adaptive heatmaps.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1978.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1978.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OXE-MAP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OXE-MAP dataset (procedurally synthesized from OXE trajectories)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset created by converting trajectory-based supervision (from OXE) into dense image-instruction-heatmap triplets using GPT-4o to identify objects and Grounding DINO + SAM to extract goal masks, followed by distance-transform Gaussian heatmap conversion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OXE-MAP (dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Procedural pipeline: use language model (GPT-4o) to identify manipulated object, track with Grounding DINO and SAM to get goal mask in final trajectory frame, reproject mask to initial frame, convert binary mask to heatmap via distance transform and Gaussian. Used as 18K samples in RoboMAP training.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Provides dense supervision (heatmaps) that enable vision-language models to learn spatial grounding from trajectory-style datasets that lack direct dense labels.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>pixel-level heatmap supervision</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>2D goal mask -> distance-transformed Gaussian heatmap</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>manipulation (trajectory datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>derived from OXE trajectories (Open X-Embodiment style datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>robotic manipulation dataset frames</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>contribution to final model accuracy when included in training mix (Table III)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>OXE-MAP only model: Where2place 48.0%, RoboRefIt 56.8%, RefSpatial 20.2%, VABench 65.5% (Table III); full data mix performs better.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Including OXE-MAP in the hybrid training mix improves generalization; full mix outperforms single-source mixes (Table III).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>OXE-MAP pipeline bridges domain gap between trajectory-form supervision and VLM training by converting trajectories into image-level heatmaps.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Shows that mixing diverse data sources helps generalization; OXE-MAP alone provides decent results but full mix is best (Table III).</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>OXE-MAP contributes ~18K samples to the total 132K training samples used for fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Procedurally converting sparse/trajectory supervision into dense heatmaps is an effective way to scale training data for vision-language spatial grounding.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1978.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1978.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Aggregated Point Supervision</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Aggregated Point Supervision (Gaussian aggregation of keypoints)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A heatmap synthesis method that converts one-or-more annotated keypoints into a dense ground-truth heatmap by aggregating Gaussian peaks (using max over points to preserve sharp multimodal peaks).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Aggregated Point Supervision (procedural label)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>When provided with discrete points {p*_i}, generates M*(u)=max_i exp(-||u - p*_i||^2/(2σ^2)). Single-point uses small σ for sharp peak; multi-point uses max to avoid blurring peaks.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Converts sparse point annotations into dense probabilistic supervision for pixelwise grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>pixel-level heatmap supervision</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Gaussian peaks centered at annotated keypoints; multi-modal peaks preserved by max aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>used for spatial grounding tasks during training</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>training supervision for Where2place, RoboRefIt, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>image frames</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Enables learning from point-labeled datasets by converting points to dense supervision; part of hybrid dataset that yields best performance.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Designed to mitigate point-based bottleneck by turning point labels into dense targets for learning distributional affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Using max-aggregated Gaussians preserves distinct multiple modes in supervision and prevents blurring of close targets, supporting learning of multi-modal affordance distributions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1978.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1978.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Shape-Aware BBox Supervision</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Shape-Aware Bounding Box Supervision (elliptical Gaussian)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Converts axis-aligned bounding box annotations into elliptical Gaussian heatmaps whose standard deviations scale with box width/height to preserve object aspect ratio in the supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Shape-Aware Bounding Box Supervision (procedural label)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Given bounding box B*, synthesize M*(u)=exp(-((u_x - c_x)^2/(2σ_x^2) + (u_y - c_y)^2/(2σ_y^2))) with σ_x,σ_y proportional to box width and height to produce elongated/compressed heatmaps matching object shape.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Provides shape-aware dense supervision so the model learns geometric extent from box labels rather than a generic circular Gaussian.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>pixel-level heatmap supervision</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>elliptical Gaussian aligned to bounding-box center and aspect ratio</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>training supervision for spatial grounding</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Where2place, RoboRefIt, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>images</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Preserves geometric cues from bounding boxes improving training signal compared to naive circular Gaussians.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Shape-aware heatmap synthesis provides richer geometric supervision from box labels, aiding learning of spatial extent.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1978.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1978.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mask-based Trajectory Supervision</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mask-based Trajectory Supervision (trajectory -> mask -> heatmap)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pipeline that converts action trajectories into dense heatmap supervision by extracting the manipulated-object goal mask from final trajectory frames and reprojecting it to produce a distance-transform Gaussian heatmap.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mask-based Trajectory Supervision</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses GPT-4o to identify manipulated object from instruction, Grounding DINO + SAM for object tracking and goal mask extraction, reprojects mask to initial frame, converts mask to heatmap via distance transform and Gaussian (Eq.4), producing OXE-MAP samples.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Enables learning from trajectory datasets by converting implicit final goals into explicit dense spatial supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>pixel-level</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>goal mask -> distance-transformed Gaussian heatmap</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>manipulation (trajectory datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>OXE-derived training samples</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>robotic trajectory frames</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Allows inclusion of large-scale trajectory datasets for VLM grounding which otherwise lack dense labels; contributes to improved generalization in hybrid training mix.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Bridges dataset domain gap by procedural label synthesis rather than explicit domain-adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Converting trajectories to dense heatmaps enables VLM training from large robotics datasets that previously did not align with 2D VLM supervision formats.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1978.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1978.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaliGemma</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaliGemma (backbone VLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The multimodal vision-language backbone used by RoboMAP that produces language-conditioned visual tokens aligned to the image grid used as input to the Adaptive Heatmap Decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Paligemma: A versatile 3b vlm for transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaliGemma</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A multimodal backbone that jointly encodes image and language into tokens; selected because it produces language-conditioned visual tokens that remain consistent with the input image grid after multimodal fusion, enabling pixelwise decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>PaliGemma multimodal VLM (3B variant referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Performs multimodal fusion to yield language-conditioned visual tokens which are then reshaped into a spatial grid for pixelwise decoding (implicit cross-modal fusion inside the backbone).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>language-conditioned spatial visual tokens (grid-aligned)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>implicit grid-aligned visual tokens; used to produce dense heatmaps</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>backbone used for grounding in manipulation and navigation experiments</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Used for experiments across Where2place, RoboRefIt, RefSpatial, VABench, SimplerEnv, real-world tasks</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>images (simulated and real)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>PaliGemma encoder and token embeddings were kept frozen during RoboMAP fine-tuning; no frozen-vs-finetuned comparison reported.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>joint multimodal fusion inside PaliGemma to produce language-conditioned visual tokens</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>A multimodal backbone that provides grid-aligned language-conditioned visual tokens enables effective pixel-level decoding for dense affordance prediction.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1978.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1978.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoboPoint</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoboPoint (points-based method)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Representative points-based spatial grounding baseline that predicts sparse 2D coordinates as the intermediate representation for downstream robotic control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoboPoint</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Point-based VLM method that outputs deterministic 2D coordinates as grounding targets; typically predicts one or multiple keypoints to indicate target locations.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Direct coordinate prediction (sparse point outputs) from visual-language features.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>point-level (sparse keypoints)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>2D coordinate(s) (points)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>manipulation, grounding benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Where2place, RoboRefIt, RefSpatial, VABench (as comparative baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>images</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy / success rate / inference time</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table I: Where2place 46.80%, RoboRefIt 49.80%, RefSpatial 16.07%, VABench-Point 19.09%, Inference time 3.10s (as reported).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Paper uses RoboPoint as an example of point-based methods that collapse spatial extent into single points, creating brittleness and loss of uncertainty modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Point-based failure modes include incorrect localization to surrounding objects and lack of coherent region representation (scattered or overconfident predictions).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Sparse point outputs cause an information bottleneck for spatial relations and object-free region grounding, making policies brittle under ambiguity.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1978.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e1978.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Embodied-R1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embodied-R1 (reinforced embodied reasoning method)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A strong prior VLM-based method that uses both supervised fine-tuning and reinforced fine-tuning (RFT) plus an explicit reasoning stage to produce more accurate point-based grounding, at cost of higher inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Embodied-R1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Combines a VLM with an explicit reasoning stage and reinforcement fine-tuning (RFT) to produce grounding outputs (points) guiding downstream policies; requires iterative/compute-heavy steps during inference.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Iterative reasoning and reinforcement fine-tuning to refine point-based grounding predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>point-level (with reasoning stage)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>2D point(s)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>manipulation benchmarks, zero-shot SimplerEnv</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Benchmarks in Table I/V (Where2place etc.), SimplerEnv zero-shot tasks</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>images, simulated environments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy / success rate / inference time</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table I: Where2place 69.50%, RoboRefIt 85.58%, RefSpatial 38.46%, VABench-Point 66.00%, Inference time 2.18s. Zero-shot SimplerEnv success 56.2% (Table V).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Used to illustrate trade-off: higher accuracy often requires heavier reasoning/RFT and longer inference times compared to RoboMAP's efficient heatmap-based approach.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Demonstrates that achieving high grounding accuracy with point-based methods can require reinforcement fine-tuning and explicit reasoning stages, increasing inference latency.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1978.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e1978.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BridgeVLA-pretrain</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BridgeVLA-pretrain (prior heatmap-based VLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior heatmap-based vision-language model compared in the paper; reported to perform poorly on object-free region grounding tasks despite being a heatmap approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BridgeVLA-pretrain</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior heatmap-based VLM used as baseline; reportedly uses non-adaptive heatmaps and does not successfully model task ambiguity in object-free regions.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Heatmap prediction (non-adaptive / fixed shape according to paper's critique)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>pixel-level heatmap (non-adaptive)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>2D heatmap but non-adaptive</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>spatial grounding benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Where2place, RoboRefIt, RefSpatial, VABench (as compared)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>images</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy / inference time</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table I: Where2place 15.00%, RoboRefIt 26.25%, RefSpatial 11.50%, VABench-Point 10.50%, Inference time 0.06s.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Paper notes BridgeVLA-pretrain fails to ground object-free regions, indicating limitations of non-adaptive heatmaps and dataset mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Complete inability to ground object-free instructions in the qualitative examples.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Non-adaptive heatmaps without adaptive upsampling or diverse dense supervision perform poorly on ambiguous/object-free grounding tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1978.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e1978.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FSD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FSD (prior specialized robotic grounding VLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior robotic grounding model evaluated as a baseline; uses specialized mechanisms for spatial affordance prediction but is slower at inference and generally underperforms RoboMAP on key metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FSD</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Specialized VLM for robotic spatial grounding (details not expanded in this paper); used as a baseline for comparison on benchmarks and real-world tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Prior specialized grounding approach (likely point/trace or trajectory-based), not adaptive heatmaps.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>point/trajectory-level (paper groups it with specialized robotic grounding VLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>not explicitly adaptive heatmap</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>manipulation benchmarks and real-world tasks</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Where2place, RoboRefIt, RefSpatial, VABench, real-world manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>images, robotic setups</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy / success rate / inference time</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table I: Where2place 45.81%, RoboRefIt 56.73%, RefSpatial 14.90%, VABench-Point 61.82%, Inference time 15.68s. Table V zero-shot SimplerEnv overall ~40.6% success.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Demonstrates that specialized methods can be effective but may incur substantial inference latency compared to RoboMAP's efficient heatmap approach.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1978.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e1978.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoboRefer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoboRefer (spatial referring VLM baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior model focused on spatial referring and disambiguation in clutter, used as a strong baseline on RoboRefIt-type tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoboRefer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A VLM specialized for spatial referring in cluttered scenes, producing discrete grounding outputs (details not expanded here).</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Object-centric referring/prediction (point or object selection)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>object/point-level</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>object-centric predictions</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>object disambiguation / manipulation benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>RoboRefIt and other grounding benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>images</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy / inference time</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table I: Where2place 66.00%, RoboRefIt 72.80%, RefSpatial 33.77%, VABench-Point 24.67%, Inference time 1.54s.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Used as an example of object-centric methods that can disambiguate objects but still suffer from limited modeling of object-free spatial extents.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Object-centric referring models can be strong on disambiguation tasks but may not model continuous affordance regions required for object-free instructions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1978.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e1978.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoboBrain2.0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoboBrain2.0 (unified brain model for robotics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior unified model for robotic manipulation used as a baseline; paper reports two variants (3B and 7B) with different training/inference regimes and speeds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robobrain 2.0 technical report.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoboBrain2.0</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Unified brain model approach for robotic manipulation incorporating hierarchical/iterative reasoning; reported in two variants with differing fine-tuning and inference strategies (one faster, one uses SFT+RFT).</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Hierarchical/iterative reasoning producing grounding outputs (likely point/object predictions with explicit reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>object/point-level with reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>2D points/object-centric outputs</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>manipulation, grounding benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Where2place, RoboRefIt, RefSpatial, VABench, real-world tasks (Table I/IV)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>images, robot setups</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy / success rate / inference time</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table I: RoboBrain2.0 (3B) Where2place 54.86%, RoboRefIt 54.42%, RefSpatial 35.81%, VABench-Point 8.08%, Inference time 0.67s. RoboBrain2.0 (7B, SFT+RFT) Where2place 63.59%, RoboRefIt 47.55%, RefSpatial 32.50%, VABench-Point 12.70%, Inference time 10.41s. Real-world Table IV: RoboBrain2.0-3B success 66% (varied per-subtask), RoboBrain2.0-7B success 64%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Demonstrates trade-offs between accuracy and inference latency when using larger models and additional reasoning/fine-tuning stages compared to RoboMAP's efficient heatmap approach.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Robopoint: A vision-language model for spatial affordance prediction for robotics. <em>(Rating: 2)</em></li>
                <li>Roborefer: Towards spatial referring with reasoning in vision-language models for robotics. <em>(Rating: 2)</em></li>
                <li>Robobrain 2.0 technical report. <em>(Rating: 2)</em></li>
                <li>Bridgevla: Improving vision-language-action models by bridging vision-language alignment. <em>(Rating: 2)</em></li>
                <li>CARAFE: Content-Aware ReAssembly of FEatures. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1978",
    "paper_id": "paper-282058156",
    "extraction_schema_id": "extraction-schema-40",
    "extracted_data": [
        {
            "name_short": "RoboMAP",
            "name_full": "RoboMAP (More than A Point: Adaptive Affordance Heatmaps)",
            "brief_description": "A vision-language spatial-grounding framework that predicts dense, adaptive affordance heatmaps as a probabilistic intermediate representation for language-conditioned robotic tasks, trained with procedurally synthesized dense supervision from sparse annotations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RoboMAP",
            "model_description": "Uses a PaliGemma multimodal backbone to produce language-conditioned visual tokens reshaped into a low-resolution grid (F_low). A lightweight Adaptive Heatmap Decoder (AHD) with two branches (Coarse Affordance Predictor and Adaptive Kernel Generator) performs content-aware upsampling to output a high-resolution, continuous 2D affordance heatmap M mapping f_theta(I,x) -&gt; M. Trained with BCE against procedurally synthesized ground-truth heatmaps from keypoints, boxes, and trajectories.",
            "visual_encoder_type": "PaliGemma multimodal VLM (backbone producing language-conditioned visual tokens)",
            "visual_encoder_pretraining": "",
            "grounding_mechanism": "Language and image jointly encoded by PaliGemma producing language-conditioned visual tokens aligned to image grid; decoder produces pixelwise affordance probabilities via content-aware upsampling (per-pixel adaptive kernels) — i.e., dense pixel-level grounding from multimodal features.",
            "representation_level": "pixel-level (dense affordance heatmap)",
            "spatial_representation": "2D adaptive affordance heatmap over image grid (continuous probability distribution); used with depth/post-processing to compute 6-DoF grasps and 3D place coordinates.",
            "embodied_task_type": "object manipulation (single-arm, dual-arm), industrial sorting, mobile navigation, pick-and-place",
            "embodied_task_name": "Where2place, RoboRefIt, RefSpatial, VABench, SimplerEnv (WidowX evaluations), real-world dual-arm and industrial single-arm experiments, zero-shot navigation tasks",
            "visual_domain": "simulated environments (SimplerEnv), indoor/outdoor navigation, and multiple real-world robot setups (tabletop dual-arm, single industrial arm)",
            "performance_metric": "localization accuracy / success rate / inference time",
            "performance_value": "Where2place: 73.0% accuracy; RoboRefIt: 88.73% accuracy; RefSpatial: 36.50% accuracy; VABench-Point: 70.0% accuracy (Table I/II); Zero-shot SimplerEnv manipulation: 60.5% success; Real-world manipulation average success rate: 82%; Inference time: 0.04 s (25 Hz).",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Ablations replacing AHD with standard upsampling show degraded grounding: Bilinear upsampling yields Where2place 64.0% (vs AHD 73.0%), Deconvolution 64.0%, PixelShuffle 59.0% (Table II).",
            "grounding_improvement": "AHD improves Where2place by +9.0% absolute over bilinear, and improves across benchmarks up to +9% (Table II). Overall RoboMAP outperforms best prior discrete/point-based methods on 3 of 4 benchmarks and improves embodied task success (e.g., zero-shot manipulation +4.3% vs Embodied-R1 in SimplerEnv).",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Paper identifies an information bottleneck from collapsing spatial targets into deterministic points/bounding boxes: discarding spatial extent and uncertainty leads to brittleness under perceptual noise and semantic ambiguity; also notes downstream post-processing (selecting only the peak pixel) as a separate bottleneck that causes failures on surfaces with height variation.",
            "failure_mode_analysis": "Enumerates grounding failure modes: (1) point-based methods either localize to wrong surrounding objects (clear localization errors) or place scattered/insufficient points failing to represent a coherent valid region; (2) prior heatmap baseline (BridgeVLA-pretrain) fails to ground object-free regions; (3) RoboMAP's downstream heuristic of selecting single peak causes execution failures on surfaces with height variation even when heatmap is correct. Paper gives empirical evidence via qualitative visualizations and task-specific success rates (e.g., Table IV real-world place task variations).",
            "domain_shift_handling": "Handles domain shift by training on a hybrid dataset mix (Robodata, COCO, OXE-MAP, RoboRefIt — total 132K samples) and synthesizing trajectory supervision to bridge disparate data sources (OXE-&gt;OXE-MAP). Demonstrates strong zero-shot transfer to real robots and navigation without task-specific fine-tuning (e.g., 60.5% zero-shot SimplerEnv success, 82% real-world manipulation), but no explicit domain-adaptation layers.",
            "novel_object_performance": "Paper reports strong zero-shot generalization qualitatively and via overall success rates in real-world and novel navigation tasks, but does not provide per-class novel-vs-seen object breakdown.",
            "frozen_vs_finetuned": "Vision encoder and token embeddings remained frozen during fine-tuning; the paper does not report a comparison between frozen and fully fine-tuned encoder settings.",
            "pretraining_scale_effect": "Presents ablation over training data composition: full hybrid mix (Robodata+COCO+OXE-MAP+RoboRefIt) yields best performance; OXE-MAP-only and other single-source mixes perform substantially worse (Table III). No explicit study of pretraining scale (web-scale) effects.",
            "fusion_mechanism": "Early joint multimodal fusion via PaliGemma producing language-conditioned visual tokens aligned to image grid; decoder then conditions on these tokens (implicit cross-modal fusion inside backbone).",
            "sample_efficiency": "Fine-tuned on 132K training samples (Robodata 62K, COCO 22K, OXE-MAP 18K, RoboRefIt 30K) for 2 epochs; training performed with frozen backbone and light decoder (1.6M params). No direct sample-efficiency comparison vs non-grounding methods provided.",
            "key_findings_grounding": "Dense, adaptive heatmaps that model spatial uncertainty produce more robust and interpretable grounding than point/bbox/trajectory collapse; a content-aware upsampling decoder (AHD) is critical for high-fidelity adaptive heatmaps, hybrid/synthesized supervision (converting sparse labels to dense heatmaps) enables scalable training, and using the full heatmap (rather than a single peak) is crucial for better downstream execution though current pipelines underutilize this signal.",
            "uuid": "e1978.0"
        },
        {
            "name_short": "AHD",
            "name_full": "Adaptive Heatmap Decoder",
            "brief_description": "Lightweight decoder that generates high-resolution adaptive affordance heatmaps from low-resolution language-conditioned features via content-aware upsampling with per-pixel learned kernels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Adaptive Heatmap Decoder (AHD)",
            "model_description": "Two-branch decoder: (1) Coarse Affordance Predictor (CAP) produces a low-resolution affordance estimate M_low; (2) Adaptive Kernel Generator (AKG) predicts a unique k×k normalized upsampling kernel W(i,j) for each output pixel. Final pixel values are computed by applying W(i,j) to the corresponding neighborhood of M_low (content-aware, softmax-normalized kernels).",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Per-pixel adaptive upsampling of a language-conditioned coarse affordance map to produce dense pixelwise grounding reflecting instruction uncertainty (sharp kernels for precise targets, broad kernels for ambiguous regions).",
            "representation_level": "pixel-level",
            "spatial_representation": "2D adaptive affordance heatmap",
            "embodied_task_type": "used as intermediate grounding representation for manipulation and navigation tasks",
            "embodied_task_name": "Where2place, RoboRefIt, RefSpatial, VABench (benchmarks used in ablations)",
            "visual_domain": "benchmarks and robot datasets (simulated and real images)",
            "performance_metric": "benchmark localization accuracy",
            "performance_value": "Table II: AHD yields Where2place 73.00%, RoboRefIt 88.73%, RefSpatial 36.50%, VABench 70.00% — outperforming Bilinear/Deconvolution/PixelShuffle variants.",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Replacing AHD with Bilinear: Where2place 64.0% (−9.0%); PixelShuffle: 59.0% (−14.0%); Deconvolution: 64.0% (−9.0%).",
            "grounding_improvement": "AHD improves grounding accuracy across benchmarks up to +9% absolute compared to standard decoders (Table II).",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": false,
            "perception_bottleneck_details": null,
            "failure_mode_analysis": "No additional failure-mode breakdown beyond showing other decoders produce lower-quality heatmaps and lower downstream accuracy.",
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Consumes language-conditioned feature grid from backbone (implicit fusion already performed in backbone); decoder performs content-aware spatial upsampling.",
            "sample_efficiency": "Decoder has ~1.6M parameters and was trained with frozen backbone; contributes little compute overhead.",
            "key_findings_grounding": "Content-aware per-pixel adaptive upsampling is essential to produce heatmaps that adapt shape to instruction ambiguity, improving grounding accuracy versus content-agnostic upsampling.",
            "uuid": "e1978.1"
        },
        {
            "name_short": "AKG",
            "name_full": "Adaptive Kernel Generator",
            "brief_description": "A branch of AHD that predicts a unique normalized k×k upsampling kernel for each output pixel, enabling content-aware upsampling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Adaptive Kernel Generator (AKG)",
            "model_description": "AKG processes the low-resolution feature grid and outputs per-target-pixel k×k kernels W(i,j) that are softmax-normalized; these kernels determine how to aggregate local coarse affordance values to form the high-resolution heatmap.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Creates per-pixel, content-conditioned upsampling kernels that shape the final affordance distribution, allowing grounding to express uncertainty and non-circular regions.",
            "representation_level": "pixel-level",
            "spatial_representation": "contributes to 2D affordance heatmap shape via per-pixel kernel predictions",
            "embodied_task_type": "manipulation, navigation (as part of RoboMAP)",
            "embodied_task_name": "Where2place, RoboRefIt, etc.",
            "visual_domain": "images (simulated and real)",
            "performance_metric": "benchmark accuracy improvements when used in AHD (see AHD entry / Table II)",
            "performance_value": "Contributes to AHD's performance: final AHD results (Where2place 73.0%, RoboRefIt 88.73%).",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Ablation of adaptive upsampling (i.e., using content-agnostic upsampling) leads to substantial performance drops (see Table II).",
            "grounding_improvement": "Enables shape-adaptive heatmaps which yield up to +9% absolute improvements on benchmarks relative to non-adaptive upsampling.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": false,
            "perception_bottleneck_details": null,
            "failure_mode_analysis": null,
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Works on language-conditioned visual feature grid; kernel predictions depend on multimodal context.",
            "sample_efficiency": null,
            "key_findings_grounding": "Learning per-pixel upsampling kernels is an effective mechanism to allow heatmap shape to reflect instruction-specific ambiguity and scene content.",
            "uuid": "e1978.2"
        },
        {
            "name_short": "CAP",
            "name_full": "Coarse Affordance Predictor",
            "brief_description": "A branch of AHD that predicts a low-resolution affordance map (M_low) which provides the content that AKG upsamples into the final heatmap.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Coarse Affordance Predictor (CAP)",
            "model_description": "Processes the low-resolution language-conditioned features to produce a preliminary low-resolution affordance estimate M_low used by the per-pixel adaptive kernels to assemble the final high-resolution heatmap.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Predicts coarse pixelwise affordance probabilities conditioned on multimodal features; grounding refined via AKG upsampling.",
            "representation_level": "low-resolution pixel-level",
            "spatial_representation": "coarse 2D affordance map",
            "embodied_task_type": "manipulation, navigation (as part of RoboMAP)",
            "embodied_task_name": "benchmarks used in paper",
            "visual_domain": "images",
            "performance_metric": "contributes to final accuracy (see AHD)",
            "performance_value": null,
            "has_grounding_ablation": true,
            "performance_without_grounding": null,
            "grounding_improvement": "Essential component of AHD pipeline; removal or replacement degrades final heatmap quality shown in ablations.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": false,
            "perception_bottleneck_details": null,
            "failure_mode_analysis": null,
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Operates on language-conditioned visual tokens from PaliGemma.",
            "sample_efficiency": null,
            "key_findings_grounding": "A coarse-to-fine pipeline (CAP + AKG) provides a flexible and efficient route to produce high-resolution, instruction-adaptive heatmaps.",
            "uuid": "e1978.3"
        },
        {
            "name_short": "OXE-MAP",
            "name_full": "OXE-MAP dataset (procedurally synthesized from OXE trajectories)",
            "brief_description": "A dataset created by converting trajectory-based supervision (from OXE) into dense image-instruction-heatmap triplets using GPT-4o to identify objects and Grounding DINO + SAM to extract goal masks, followed by distance-transform Gaussian heatmap conversion.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OXE-MAP (dataset)",
            "model_description": "Procedural pipeline: use language model (GPT-4o) to identify manipulated object, track with Grounding DINO and SAM to get goal mask in final trajectory frame, reproject mask to initial frame, convert binary mask to heatmap via distance transform and Gaussian. Used as 18K samples in RoboMAP training.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Provides dense supervision (heatmaps) that enable vision-language models to learn spatial grounding from trajectory-style datasets that lack direct dense labels.",
            "representation_level": "pixel-level heatmap supervision",
            "spatial_representation": "2D goal mask -&gt; distance-transformed Gaussian heatmap",
            "embodied_task_type": "manipulation (trajectory datasets)",
            "embodied_task_name": "derived from OXE trajectories (Open X-Embodiment style datasets)",
            "visual_domain": "robotic manipulation dataset frames",
            "performance_metric": "contribution to final model accuracy when included in training mix (Table III)",
            "performance_value": "OXE-MAP only model: Where2place 48.0%, RoboRefIt 56.8%, RefSpatial 20.2%, VABench 65.5% (Table III); full data mix performs better.",
            "has_grounding_ablation": true,
            "performance_without_grounding": null,
            "grounding_improvement": "Including OXE-MAP in the hybrid training mix improves generalization; full mix outperforms single-source mixes (Table III).",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": false,
            "perception_bottleneck_details": null,
            "failure_mode_analysis": null,
            "domain_shift_handling": "OXE-MAP pipeline bridges domain gap between trajectory-form supervision and VLM training by converting trajectories into image-level heatmaps.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": "Shows that mixing diverse data sources helps generalization; OXE-MAP alone provides decent results but full mix is best (Table III).",
            "fusion_mechanism": null,
            "sample_efficiency": "OXE-MAP contributes ~18K samples to the total 132K training samples used for fine-tuning.",
            "key_findings_grounding": "Procedurally converting sparse/trajectory supervision into dense heatmaps is an effective way to scale training data for vision-language spatial grounding.",
            "uuid": "e1978.4"
        },
        {
            "name_short": "Aggregated Point Supervision",
            "name_full": "Aggregated Point Supervision (Gaussian aggregation of keypoints)",
            "brief_description": "A heatmap synthesis method that converts one-or-more annotated keypoints into a dense ground-truth heatmap by aggregating Gaussian peaks (using max over points to preserve sharp multimodal peaks).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Aggregated Point Supervision (procedural label)",
            "model_description": "When provided with discrete points {p*_i}, generates M*(u)=max_i exp(-||u - p*_i||^2/(2σ^2)). Single-point uses small σ for sharp peak; multi-point uses max to avoid blurring peaks.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Converts sparse point annotations into dense probabilistic supervision for pixelwise grounding.",
            "representation_level": "pixel-level heatmap supervision",
            "spatial_representation": "Gaussian peaks centered at annotated keypoints; multi-modal peaks preserved by max aggregation",
            "embodied_task_type": "used for spatial grounding tasks during training",
            "embodied_task_name": "training supervision for Where2place, RoboRefIt, etc.",
            "visual_domain": "image frames",
            "performance_metric": null,
            "performance_value": null,
            "has_grounding_ablation": true,
            "performance_without_grounding": null,
            "grounding_improvement": "Enables learning from point-labeled datasets by converting points to dense supervision; part of hybrid dataset that yields best performance.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": false,
            "perception_bottleneck_details": "Designed to mitigate point-based bottleneck by turning point labels into dense targets for learning distributional affordances.",
            "failure_mode_analysis": null,
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": null,
            "sample_efficiency": null,
            "key_findings_grounding": "Using max-aggregated Gaussians preserves distinct multiple modes in supervision and prevents blurring of close targets, supporting learning of multi-modal affordance distributions.",
            "uuid": "e1978.5"
        },
        {
            "name_short": "Shape-Aware BBox Supervision",
            "name_full": "Shape-Aware Bounding Box Supervision (elliptical Gaussian)",
            "brief_description": "Converts axis-aligned bounding box annotations into elliptical Gaussian heatmaps whose standard deviations scale with box width/height to preserve object aspect ratio in the supervision.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Shape-Aware Bounding Box Supervision (procedural label)",
            "model_description": "Given bounding box B*, synthesize M*(u)=exp(-((u_x - c_x)^2/(2σ_x^2) + (u_y - c_y)^2/(2σ_y^2))) with σ_x,σ_y proportional to box width and height to produce elongated/compressed heatmaps matching object shape.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Provides shape-aware dense supervision so the model learns geometric extent from box labels rather than a generic circular Gaussian.",
            "representation_level": "pixel-level heatmap supervision",
            "spatial_representation": "elliptical Gaussian aligned to bounding-box center and aspect ratio",
            "embodied_task_type": "training supervision for spatial grounding",
            "embodied_task_name": "Where2place, RoboRefIt, etc.",
            "visual_domain": "images",
            "performance_metric": null,
            "performance_value": null,
            "has_grounding_ablation": true,
            "performance_without_grounding": null,
            "grounding_improvement": "Preserves geometric cues from bounding boxes improving training signal compared to naive circular Gaussians.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": false,
            "perception_bottleneck_details": null,
            "failure_mode_analysis": null,
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": null,
            "sample_efficiency": null,
            "key_findings_grounding": "Shape-aware heatmap synthesis provides richer geometric supervision from box labels, aiding learning of spatial extent.",
            "uuid": "e1978.6"
        },
        {
            "name_short": "Mask-based Trajectory Supervision",
            "name_full": "Mask-based Trajectory Supervision (trajectory -&gt; mask -&gt; heatmap)",
            "brief_description": "Pipeline that converts action trajectories into dense heatmap supervision by extracting the manipulated-object goal mask from final trajectory frames and reprojecting it to produce a distance-transform Gaussian heatmap.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mask-based Trajectory Supervision",
            "model_description": "Uses GPT-4o to identify manipulated object from instruction, Grounding DINO + SAM for object tracking and goal mask extraction, reprojects mask to initial frame, converts mask to heatmap via distance transform and Gaussian (Eq.4), producing OXE-MAP samples.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Enables learning from trajectory datasets by converting implicit final goals into explicit dense spatial supervision.",
            "representation_level": "pixel-level",
            "spatial_representation": "goal mask -&gt; distance-transformed Gaussian heatmap",
            "embodied_task_type": "manipulation (trajectory datasets)",
            "embodied_task_name": "OXE-derived training samples",
            "visual_domain": "robotic trajectory frames",
            "performance_metric": null,
            "performance_value": null,
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": "Allows inclusion of large-scale trajectory datasets for VLM grounding which otherwise lack dense labels; contributes to improved generalization in hybrid training mix.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": false,
            "perception_bottleneck_details": null,
            "failure_mode_analysis": null,
            "domain_shift_handling": "Bridges dataset domain gap by procedural label synthesis rather than explicit domain-adaptation.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": null,
            "sample_efficiency": null,
            "key_findings_grounding": "Converting trajectories to dense heatmaps enables VLM training from large robotics datasets that previously did not align with 2D VLM supervision formats.",
            "uuid": "e1978.7"
        },
        {
            "name_short": "PaliGemma",
            "name_full": "PaliGemma (backbone VLM)",
            "brief_description": "The multimodal vision-language backbone used by RoboMAP that produces language-conditioned visual tokens aligned to the image grid used as input to the Adaptive Heatmap Decoder.",
            "citation_title": "Paligemma: A versatile 3b vlm for transfer.",
            "mention_or_use": "use",
            "model_name": "PaliGemma",
            "model_description": "A multimodal backbone that jointly encodes image and language into tokens; selected because it produces language-conditioned visual tokens that remain consistent with the input image grid after multimodal fusion, enabling pixelwise decoding.",
            "visual_encoder_type": "PaliGemma multimodal VLM (3B variant referenced)",
            "visual_encoder_pretraining": "",
            "grounding_mechanism": "Performs multimodal fusion to yield language-conditioned visual tokens which are then reshaped into a spatial grid for pixelwise decoding (implicit cross-modal fusion inside the backbone).",
            "representation_level": "language-conditioned spatial visual tokens (grid-aligned)",
            "spatial_representation": "implicit grid-aligned visual tokens; used to produce dense heatmaps",
            "embodied_task_type": "backbone used for grounding in manipulation and navigation experiments",
            "embodied_task_name": "Used for experiments across Where2place, RoboRefIt, RefSpatial, VABench, SimplerEnv, real-world tasks",
            "visual_domain": "images (simulated and real)",
            "performance_metric": null,
            "performance_value": null,
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": false,
            "perception_bottleneck_details": null,
            "failure_mode_analysis": null,
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": "PaliGemma encoder and token embeddings were kept frozen during RoboMAP fine-tuning; no frozen-vs-finetuned comparison reported.",
            "pretraining_scale_effect": null,
            "fusion_mechanism": "joint multimodal fusion inside PaliGemma to produce language-conditioned visual tokens",
            "sample_efficiency": null,
            "key_findings_grounding": "A multimodal backbone that provides grid-aligned language-conditioned visual tokens enables effective pixel-level decoding for dense affordance prediction.",
            "uuid": "e1978.8"
        },
        {
            "name_short": "RoboPoint",
            "name_full": "RoboPoint (points-based method)",
            "brief_description": "Representative points-based spatial grounding baseline that predicts sparse 2D coordinates as the intermediate representation for downstream robotic control.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RoboPoint",
            "model_description": "Point-based VLM method that outputs deterministic 2D coordinates as grounding targets; typically predicts one or multiple keypoints to indicate target locations.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Direct coordinate prediction (sparse point outputs) from visual-language features.",
            "representation_level": "point-level (sparse keypoints)",
            "spatial_representation": "2D coordinate(s) (points)",
            "embodied_task_type": "manipulation, grounding benchmarks",
            "embodied_task_name": "Where2place, RoboRefIt, RefSpatial, VABench (as comparative baselines)",
            "visual_domain": "images",
            "performance_metric": "accuracy / success rate / inference time",
            "performance_value": "Table I: Where2place 46.80%, RoboRefIt 49.80%, RefSpatial 16.07%, VABench-Point 19.09%, Inference time 3.10s (as reported).",
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Paper uses RoboPoint as an example of point-based methods that collapse spatial extent into single points, creating brittleness and loss of uncertainty modeling.",
            "failure_mode_analysis": "Point-based failure modes include incorrect localization to surrounding objects and lack of coherent region representation (scattered or overconfident predictions).",
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": null,
            "sample_efficiency": null,
            "key_findings_grounding": "Sparse point outputs cause an information bottleneck for spatial relations and object-free region grounding, making policies brittle under ambiguity.",
            "uuid": "e1978.9"
        },
        {
            "name_short": "Embodied-R1",
            "name_full": "Embodied-R1 (reinforced embodied reasoning method)",
            "brief_description": "A strong prior VLM-based method that uses both supervised fine-tuning and reinforced fine-tuning (RFT) plus an explicit reasoning stage to produce more accurate point-based grounding, at cost of higher inference time.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Embodied-R1",
            "model_description": "Combines a VLM with an explicit reasoning stage and reinforcement fine-tuning (RFT) to produce grounding outputs (points) guiding downstream policies; requires iterative/compute-heavy steps during inference.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Iterative reasoning and reinforcement fine-tuning to refine point-based grounding predictions.",
            "representation_level": "point-level (with reasoning stage)",
            "spatial_representation": "2D point(s)",
            "embodied_task_type": "manipulation benchmarks, zero-shot SimplerEnv",
            "embodied_task_name": "Benchmarks in Table I/V (Where2place etc.), SimplerEnv zero-shot tasks",
            "visual_domain": "images, simulated environments",
            "performance_metric": "accuracy / success rate / inference time",
            "performance_value": "Table I: Where2place 69.50%, RoboRefIt 85.58%, RefSpatial 38.46%, VABench-Point 66.00%, Inference time 2.18s. Zero-shot SimplerEnv success 56.2% (Table V).",
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": false,
            "perception_bottleneck_details": "Used to illustrate trade-off: higher accuracy often requires heavier reasoning/RFT and longer inference times compared to RoboMAP's efficient heatmap-based approach.",
            "failure_mode_analysis": null,
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": null,
            "sample_efficiency": null,
            "key_findings_grounding": "Demonstrates that achieving high grounding accuracy with point-based methods can require reinforcement fine-tuning and explicit reasoning stages, increasing inference latency.",
            "uuid": "e1978.10"
        },
        {
            "name_short": "BridgeVLA-pretrain",
            "name_full": "BridgeVLA-pretrain (prior heatmap-based VLM)",
            "brief_description": "A prior heatmap-based vision-language model compared in the paper; reported to perform poorly on object-free region grounding tasks despite being a heatmap approach.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "BridgeVLA-pretrain",
            "model_description": "Prior heatmap-based VLM used as baseline; reportedly uses non-adaptive heatmaps and does not successfully model task ambiguity in object-free regions.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Heatmap prediction (non-adaptive / fixed shape according to paper's critique)",
            "representation_level": "pixel-level heatmap (non-adaptive)",
            "spatial_representation": "2D heatmap but non-adaptive",
            "embodied_task_type": "spatial grounding benchmarks",
            "embodied_task_name": "Where2place, RoboRefIt, RefSpatial, VABench (as compared)",
            "visual_domain": "images",
            "performance_metric": "accuracy / inference time",
            "performance_value": "Table I: Where2place 15.00%, RoboRefIt 26.25%, RefSpatial 11.50%, VABench-Point 10.50%, Inference time 0.06s.",
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Paper notes BridgeVLA-pretrain fails to ground object-free regions, indicating limitations of non-adaptive heatmaps and dataset mismatch.",
            "failure_mode_analysis": "Complete inability to ground object-free instructions in the qualitative examples.",
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": null,
            "sample_efficiency": null,
            "key_findings_grounding": "Non-adaptive heatmaps without adaptive upsampling or diverse dense supervision perform poorly on ambiguous/object-free grounding tasks.",
            "uuid": "e1978.11"
        },
        {
            "name_short": "FSD",
            "name_full": "FSD (prior specialized robotic grounding VLM)",
            "brief_description": "A prior robotic grounding model evaluated as a baseline; uses specialized mechanisms for spatial affordance prediction but is slower at inference and generally underperforms RoboMAP on key metrics.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "FSD",
            "model_description": "Specialized VLM for robotic spatial grounding (details not expanded in this paper); used as a baseline for comparison on benchmarks and real-world tasks.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Prior specialized grounding approach (likely point/trace or trajectory-based), not adaptive heatmaps.",
            "representation_level": "point/trajectory-level (paper groups it with specialized robotic grounding VLMs)",
            "spatial_representation": "not explicitly adaptive heatmap",
            "embodied_task_type": "manipulation benchmarks and real-world tasks",
            "embodied_task_name": "Where2place, RoboRefIt, RefSpatial, VABench, real-world manipulation",
            "visual_domain": "images, robotic setups",
            "performance_metric": "accuracy / success rate / inference time",
            "performance_value": "Table I: Where2place 45.81%, RoboRefIt 56.73%, RefSpatial 14.90%, VABench-Point 61.82%, Inference time 15.68s. Table V zero-shot SimplerEnv overall ~40.6% success.",
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": false,
            "perception_bottleneck_details": null,
            "failure_mode_analysis": null,
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": null,
            "sample_efficiency": null,
            "key_findings_grounding": "Demonstrates that specialized methods can be effective but may incur substantial inference latency compared to RoboMAP's efficient heatmap approach.",
            "uuid": "e1978.12"
        },
        {
            "name_short": "RoboRefer",
            "name_full": "RoboRefer (spatial referring VLM baseline)",
            "brief_description": "A prior model focused on spatial referring and disambiguation in clutter, used as a strong baseline on RoboRefIt-type tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RoboRefer",
            "model_description": "A VLM specialized for spatial referring in cluttered scenes, producing discrete grounding outputs (details not expanded here).",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Object-centric referring/prediction (point or object selection)",
            "representation_level": "object/point-level",
            "spatial_representation": "object-centric predictions",
            "embodied_task_type": "object disambiguation / manipulation benchmarks",
            "embodied_task_name": "RoboRefIt and other grounding benchmarks",
            "visual_domain": "images",
            "performance_metric": "accuracy / inference time",
            "performance_value": "Table I: Where2place 66.00%, RoboRefIt 72.80%, RefSpatial 33.77%, VABench-Point 24.67%, Inference time 1.54s.",
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Used as an example of object-centric methods that can disambiguate objects but still suffer from limited modeling of object-free spatial extents.",
            "failure_mode_analysis": null,
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": null,
            "sample_efficiency": null,
            "key_findings_grounding": "Object-centric referring models can be strong on disambiguation tasks but may not model continuous affordance regions required for object-free instructions.",
            "uuid": "e1978.13"
        },
        {
            "name_short": "RoboBrain2.0",
            "name_full": "RoboBrain2.0 (unified brain model for robotics)",
            "brief_description": "A prior unified model for robotic manipulation used as a baseline; paper reports two variants (3B and 7B) with different training/inference regimes and speeds.",
            "citation_title": "Robobrain 2.0 technical report.",
            "mention_or_use": "use",
            "model_name": "RoboBrain2.0",
            "model_description": "Unified brain model approach for robotic manipulation incorporating hierarchical/iterative reasoning; reported in two variants with differing fine-tuning and inference strategies (one faster, one uses SFT+RFT).",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Hierarchical/iterative reasoning producing grounding outputs (likely point/object predictions with explicit reasoning).",
            "representation_level": "object/point-level with reasoning",
            "spatial_representation": "2D points/object-centric outputs",
            "embodied_task_type": "manipulation, grounding benchmarks",
            "embodied_task_name": "Where2place, RoboRefIt, RefSpatial, VABench, real-world tasks (Table I/IV)",
            "visual_domain": "images, robot setups",
            "performance_metric": "accuracy / success rate / inference time",
            "performance_value": "Table I: RoboBrain2.0 (3B) Where2place 54.86%, RoboRefIt 54.42%, RefSpatial 35.81%, VABench-Point 8.08%, Inference time 0.67s. RoboBrain2.0 (7B, SFT+RFT) Where2place 63.59%, RoboRefIt 47.55%, RefSpatial 32.50%, VABench-Point 12.70%, Inference time 10.41s. Real-world Table IV: RoboBrain2.0-3B success 66% (varied per-subtask), RoboBrain2.0-7B success 64%.",
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": false,
            "perception_bottleneck_details": null,
            "failure_mode_analysis": null,
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": null,
            "sample_efficiency": null,
            "key_findings_grounding": "Demonstrates trade-offs between accuracy and inference latency when using larger models and additional reasoning/fine-tuning stages compared to RoboMAP's efficient heatmap approach.",
            "uuid": "e1978.14"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Robopoint: A vision-language model for spatial affordance prediction for robotics.",
            "rating": 2
        },
        {
            "paper_title": "Roborefer: Towards spatial referring with reasoning in vision-language models for robotics.",
            "rating": 2
        },
        {
            "paper_title": "Robobrain 2.0 technical report.",
            "rating": 2
        },
        {
            "paper_title": "Bridgevla: Improving vision-language-action models by bridging vision-language alignment.",
            "rating": 2
        },
        {
            "paper_title": "CARAFE: Content-Aware ReAssembly of FEatures.",
            "rating": 1
        }
    ],
    "cost": 0.028711,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>More than A Point: Capturing Uncertainty with Adaptive Affordance Heatmaps for Spatial Grounding in Robotic Tasks
15 Oct 2025</p>
<p>Xinyu Shao 
Yanzhe Tang 
Pengwei Xie 
Kaiwen Zhou 
Yuzheng Zhuang 
Xingyue Quan 
Jianye Hao 
Long Zeng 
Xiu Li 
Bboxes Points 
Trajectory 
Adaptive Affordance 
Heatmap Gemma 
Siglip Tokenizer 
More than A Point: Capturing Uncertainty with Adaptive Affordance Heatmaps for Spatial Grounding in Robotic Tasks
15 Oct 20253ACE3E439B32DA82B06BC012244882E4arXiv:2510.10912v2[cs.RO]
Many language-guided robotic systems rely on collapsing spatial reasoning into discrete points, making them brittle to perceptual noise and semantic ambiguity.To address this challenge, we propose RoboMAP, a framework that represents spatial targets as continuous, adaptive affordance heatmaps.This dense representation captures the uncertainty in spatial grounding and provides richer information for downstream policies, thereby significantly enhancing task success and interpretability.RoboMAP surpasses the previous state-of-theart on a majority of grounding benchmarks with up to a 50x speed improvement, and achieves an 82% success rate in realworld manipulation.Across extensive simulated and physical experiments, it demonstrates robust performance and shows strong zero-shot generalization to navigation.More details and videos can be found at robo-map.github.io.</p>
<p>I. INTRODUCTION</p>
<p>Vision-Language Models have recently shown impressive capabilities in perception and reasoning, naturally motivating their use in language-guided robotics [1], [2], [3], [4], [5].A prominent paradigm is the hierarchical approach, where a VLM serves as a high-level planner [6], [7], [8], [9].This process, known as spatial grounding, translates instructions and observations into a mid-level representation to guide a low-level controller.An effective intermediate representation is key to bridging perception and action, enabling precise and versatile robotic behaviors.</p>
<p>However, the choice of this intermediate representation is critical and remains a key limitation.Most existing methods distill complex language and visual information into deterministic keypoints [6], bounding boxes or visual traces [10].This creates a severe information bottleneck.By discarding all spatial uncertainty, it makes the system brittle.</p>
<p>This challenge is most pronounced when the robot's goal is defined by a spatial relationship (e.g., near the chair) rather than by targeting a specific, named object (e.g., the red chair).Collapsing such an inherently ambiguous region into a single keypoint fails to capture its spatial extent.This makes the resulting action fragile and hard to interpret, especially in cluttered or dynamic environments.</p>
<p>To address this brittleness, we argue that robust languageguided robotics requires more than a point.We introduce RoboMAP (More than A Point), a framework that generates our core intermediate representation: the adaptive affordance heatmap.This heatmap models the entire workspace as a continuous probability distribution, quantifying the *Equal contribution. 1Shenzhen International Graduate School, Tsinghua University, China. 2 Noah's Ark Lab, Huawei, China.</p>
<p>Vision Language Model</p>
<p>Fail to capture the complex free space Find the free space near the teal bowl</p>
<p>Points based method</p>
<p>BBoxes based method</p>
<p>Fig. 1: Given an ambiguous instruction like Find the free space near the teal bowl, prior works (a) that rely on discrete points or bounding boxes fail to capture the complex, non-rectangular nature of the goal region.In contrast, our RoboMAP (b) generates a dense affordance heatmap that accurately represents the entire continuous distribution of suitable locations, successfully grounding the complex spatial concept.suitability of each region for a given language-conditioned task.Figure 1 provides a clear example of its effectiveness: while discrete representations like points or bounding boxes (a) are brittle, our dense heatmap (b) successfully captures the inherent spatial uncertainty in complex instructions.</p>
<p>To generate these dense heatmaps in a scalable and dataefficient manner, we develop a pre-training pipeline that converts diverse, sparse annotations (keypoints, bounding boxes, and trajectories) into dense supervisory signals.The resulting adaptive affordance heatmap endows the system with two critical advantages: robustness, by providing multiple highprobability locations for the robot to attempt, and safety, as the robot can refuse to act if the peak of the generated heatmap falls below a predefined confidence threshold.</p>
<p>Our contributions are three-fold:</p>
<p>• Adaptive Heatmaps for Robust Grounding: We introduce a framework that leverages an adaptive affordance heatmap for robust spatial grounding in ambiguous, object-free regions where traditional points-based or bounding boxes-based representations fall short.• Unifying Sparse Labels into Heatmaps: A pre-training pipeline that unifies diverse and sparse annotations (e.g., keypoints, boxes, trajectories) by synthesizing them into a consistent adaptive affordance heatmap.This data-efficient approach enables cross-domain learning.• Zero-Shot Generalization: We validate RoboMAP's effectiveness through extensive experiments in both simulation and the real world.We deploy the system on diverse platforms, demonstrating successful zero-shot transfer to a range of complex tasks, such as pick-andplace, industrial sorting, and navigation.</p>
<p>II. RELATED WORK</p>
<p>A. VLM-driven Language-Guided Robotics</p>
<p>Recent progress in language-guided robotics is largely driven by the powerful reasoning capabilities of Vision-Language Models (VLMs).Two dominant paradigms have emerged: end-to-end Vision-Language-Action (VLA) models and hierarchical approaches that use VLMs for high-level planning.End-to-end models, like RT-2 [5], OpenVLA [11] and π 0 [12], directly map language commands to low-level executable actions.Hierarchical approaches [13], [14], [6], [8], which we detail in Sec.II-B, use VLMs to generate an intermediate representation to guide a low-level controller.</p>
<p>Despite their architectural differences, both paradigms share a common limitation in grounding instructions that refer to object-free regions, e.g., place the block to the left of the bowl.End-to-end VLA models struggle to directly map abstract spatial relations to precise action vectors, a process that is often brittle and data-intensive.Hierarchical approaches are constrained by their object-centric intermediate representations, which ground an object like the bowl but discard crucial information about the surrounding free space.This reveals a shared need for a representation capable of explicitly modeling continuous space and spatial relations, which motivates our work.</p>
<p>B. Intermediate representation-based Spatial Grounding</p>
<p>In the hierarchical paradigm, an intermediate spatial representation bridges high-level planning and low-level control.The design of this representation is critical, as it directly dictates the system's capabilities and limitations when grounding complex spatial instructions.</p>
<p>Point-based and BBox-based methods represent the target as discrete 2D coordinates.While some works predict multiple keypoints [6], and others enhance the reasoning process [15], [8], they ultimately output a few sparse points or a bounding box.This discrete representation is fundamentally limited as it collapses a potentially large, valid region into an overconfident prediction.By discarding information about the region's shape and the distribution of affordances, this approach makes the system brittle.</p>
<p>Trajectory-based methods offer more detailed guidance by predicting an explicit path for the robot to follow, such as a visual trace or a post-contact trajectory [10], [16].While this is useful for complex actions like pushing, the approach is still limited to a single, deterministic path.Critically, this static representation cannot adapt to the task's ambiguity, failing to broaden the possibilities for vague instructions or narrow them for precise ones [3].</p>
<p>Heatmap-based methods, while used for tasks like grasp planning [17] or localizing interaction points [18], have two core limitations.They are typically non-adaptive, acting as fixed-shape pointers unable to model task ambiguity.Furthermore, they are not used as a unified representation for learning from diverse 2D internet-scale data, often requiring specialized 3D simulations.</p>
<p>Our framework, RoboMAP, overcomes these limitations with its adaptive affordance heatmaps.Unlike prior representations that are discrete, deterministic, or non-adaptive, ours is a truly probabilistic and dynamic distribution.It adapts its shape to match the task's ambiguity and serves as a unified format for learning from varied 2D data sources.This enables RoboMAP to provide a richer and more flexible signal for the robot, leading to more robust and interpretable execution.</p>
<p>III. PROBLEM FORMULATION</p>
<p>Given an RGB image I ∈ R H×W ×3 and a language instruction x, we aim to learn a unified representation for spatial grounding across diverse robotic tasks.This representation must handle both precise targets (e.g., pick up the screw) and high-uncertainty ambiguous regions (e.g., near the window or any of the empty drawers).</p>
<p>We formulate this as predicting a single, dense adaptive affordance heatmap, M ∈ [0, 1] H×W .Formally, we train a Vision-Language Model f θ that learns the mapping:
f θ (I, x) → M .
The predicted heatmap M is a continuous probability distribution that dynamically adapts its shape to reflect the instruction's uncertainty, forming sharp peaks for precise targets and broad distributions for vague or disjoint regions.</p>
<p>To train this model, a key component of our approach is to generate a ground-truth heatmap M * as the supervision signal for each training sample.Since dense heatmap annotations are not readily available, we synthesize these labels from various common, sparse annotation types, including Keypoints (P * ), Bounding boxes (B * ), and Trajectories (T * ), as detailed in Section IV-C.</p>
<p>IV. METHODOLOGY: THE ROBOMAP FRAMEWORK</p>
<p>RoboMAP is a framework for spatial grounding in robotic tasks.It combines a vision-language backbone with our proposed Adaptive Heatmap Decoder (AHD) to produce adaptive affordance heatmaps.To leverage heterogeneous supervision, we introduce a procedural heatmap synthesis strategy and a heatmap-based training objective.The overall pipeline is shown in Fig. 2.</p>
<p>A. Overall Architecture</p>
<p>RoboMAP builds upon a vision-language backbone to extract semantically grounded, spatially aligned features for robotic tasks.We adopt PaliGemma [19] for its ability to produce language-conditioned visual tokens that remain consistent with the input image grid after multi-modal fusion.Given an RGB image I and a natural language instruction x, the backbone jointly encodes both modalities and outputs a sequence of multi-modal tokens.A subset of these tokens, corresponding to visual content, is reshaped into a lowresolution feature map F low (e.g., 16×16).This map captures the spatial structure and serves as the input to our Adaptive Heatmap Decoder, which generates the final high-resolution affordance heatmap M .</p>
<p>Cross-Embodiment Real-World Evaluation Simulation</p>
<p>B. Adaptive Heatmap Decoder (AHD)</p>
<p>Generating high-resolution heatmaps for the variable shapes of robotic affordances is a key challenge, as common upsampling methods are fundamentally content-agnostic.For instance, Bilinear interpolation ignores feature semantics, Deconvolution [20] applies a single, computationally expensive global kernel, and PixelShuffle [21] rearranges pixels according to a fixed, content-agnostic rule.To overcome these limitations, our AHD adopts a content-aware upsampling mechanism inspired by CARAFE [22].As illustrated in Figure 3, our AHD implements this by decomposing the task into two parts: an Adaptive Kernel Generator (AKG) that learns how to upsample, and a Coarse Affordance Predictor (CAP) that provides the content of what to upsample.</p>
<p>The two branches take the coarse feature map F low as input.The CAP first derives a preliminary low-resolution affordance estimate M low .In parallel, the AKG outputs a unique k×k kernel W (i, j) for each target pixel (i, j), which is normalized by a Softmax function.The final heatmap value M (i, j) is then computed by applying the predicted kernel to its corresponding neighborhood in the coarse map:
M (i, j) = p,q∈N k W (i, j) p,q • M low (i ′ + p, j ′ + q),(1)
where (i ′ , j ′ ) are the source coordinates in the low-resolution map corresponding to the target pixel (i, j), and N k defines the k×k neighborhood, with p and q being the integer offsets.</p>
<p>This learnable convex upsampling allows the AHD to adapt its behavior to the instruction's ambiguity.For precise targets, the predicted kernels are sharp and focused; for vague regions, they become broader and smoother.This results in high-fidelity, shape-adaptive heatmaps that accurately capture spatial uncertainty and improve downstream task success rates across diverse scenarios.</p>
<p>C. Procedural Ground-Truth Heatmap Synthesis</p>
<p>A key component of RoboMAP is the procedural generation of high-quality supervision signals from diverse raw annotations (e.g., keypoints, bounding boxes, trajectories), as illustrated in Fig 2 .For each training sample, we generate a ground-truth adaptive affordance heatmap M * that encodes the target's spatial properties.</p>
<p>1) Aggregated Point Supervision: When supervision is provided as one or more discrete points {p * i } N i=1 , we generate the heatmap by aggregating Gaussian distributions centered at these points:
M * (u) = max i exp − ∥u − p * i ∥ 2 2σ 2 , (2)
where u is a pixel coordinate.For a single-point (N = 1), we use a small, fixed variance σ to create a sharp peak, promoting precise localization.For multiple points, a max operation, instead of summation, is used to prevent blurring, thus preserving clear peaks that help the model distinguish between close targets.</p>
<p>2) Shape-Aware Bounding Box Supervision: For supervision provided as an axis-aligned bounding box B * , we generate an elliptical Gaussian heatmap that matches the box's aspect ratio.This is achieved by decoupling the Gaussian into two components along the x and y axes:
M * (u) = exp − (u x − c x ) 2 2σ 2 x + (u y − c y ) 2 2σ 2 y ,(3)
where u = (u x , u y ) is a pixel coordinate, c = (c x , c y ) is the center of the bounding box.The standard deviations σ x and σ y are set to be proportional to the box's width and height, respectively.This creates a heatmap that is elongated or compressed according to the object's shape, providing richer geometric cues than a simple circular Gaussian.</p>
<p>3) Mask-based Trajectory Supervision: To leverage largescale robotics datasets like OXE [23], which provide supervision as action trajectories incompatible with standard VLM training, we introduce a pipeline to synthesize ground-truth heatmaps.This process converts the implicit spatial goals within trajectories into a dense supervision signal, directly enabling VLMs to learn the robust and generalizable spatial reasoning required for robotic tasks.</p>
<p>Our pipeline first uses GPT-4o [24] to identify the manipulated object from the instruction.We then track this object using Grounding DINO [25] and SAM [26] to extract a goal mask from the trajectory's final frame, which is then reprojected to the initial frame.This binary mask is converted into a heatmap using a distance transform with a Gaussian kernel (Eq.4).This entire process results in our new dataset of (image, instruction, heatmap) triplets, which we name OXE-MAP, providing rich supervision for our model.
M * (u) = exp − min m∈Mgoal ∥u − m∥ 2 2σ 2 . (4)</p>
<p>D. Training Objective</p>
<p>We formulate the training as a dense prediction task.The model is optimized by minimizing the pixel-wise Binary Cross-Entropy (BCE) loss between the predicted heatmap M and the procedurally generated ground-truth heatmap M * .This objective effectively encourages the network to produce sharp, high-confidence peaks for precise targets while permitting broader distributions for ambiguous goals, all without requiring additional hyperparameters.</p>
<p>V. IMPLEMENTATION DETAILS</p>
<p>RoboMAP uses PaliGemma [19] as the backbone, with our lightweight Adaptive Heatmap Decoder appended for adaptive heatmap prediction.This decoder, consisting of two branches (a Coarse Affordance Predictor and an Adaptive Kernel Generator), contains only 1.6M parameters, a negligible amount compared to the backbone.</p>
<p>All input images are resized to 224 × 224 pixels.During training, we apply only photometric data augmentations (e.g., brightness, contrast, and color jitter) without geometric transformations such as rotation or flipping.Geometric augmentations are avoided because they would distort the spatial correspondence between the image and the ground-truth heatmap, potentially introducing ambiguity and hindering the model's ability to learn precise language-conditioned spatial grounding.The training set is a combination of Robodata (62K) [6], COCO (22K) [27], OXE-MAP (18K) [23] and RoboRefIt (30K) [28], for a total of 132K samples.</p>
<p>We finetune the model for 2 epochs.The vision encoder and token embeddings remain frozen.We use AdamW optimizer [29] with β 1 = 0.9, β 2 = 0.999, and weight decay of 0.1.The learning rate warms up to a peak of 3 × 10 −5 over the first 400 steps, followed by cosine decay.Training is performed with a global batch size of 384 on four 48G GPUs used for all experiments, using BF16 mixed precision [30].The full process takes approximately 20 hours.All experiments are implemented in PyTorch [31].</p>
<p>VI. EXPERIMENTS</p>
<p>In this section, we conduct a series of experiments to rigorously evaluate the proposed RoboMAP framework.Our evaluation aims to answer three primary questions:</p>
<p>(Q1) How does RoboMAP perform against state-of-the-art and baseline methods in spatial grounding tasks?</p>
<p>(Q2) What is the contribution of each core component?(Q3) How well does RoboMAP generalize to novel robotic tasks and platforms, enabling effective zero-shot generalization in both simulation and the real world?</p>
<p>To answer these questions, we structure our experiments as follows: Section VI-A addresses Q1, providing a rigorous quantitative and qualitative evaluation against state-of-theart baselines on four grounding benchmarks.Section VI-B then addresses Q2, conducting a thorough ablation study to pinpoint the contribution of each key design choice.Finally, Section VI-C tackles Q3, demonstrating RoboMAP's powerful generalization by successfully testing its zero-shot transfer to both simulated and diverse real-world platforms.</p>
<p>A. Benchmark Evaluation on Spatial Grounding (Q1) a) Benchmarks: We evaluate RoboMAP on four benchmarks, each targeting a key aspect of spatial grounding: Ro-boRefIt for disambiguating objects in clutter; Where2place for grounding instructions in object-free regions; RefSpatial for multi-step localization and placement; and VABench for translating instructions into actionable regions.These benchmarks collectively serve to evaluate the model's core capability in spatial grounding, primarily measured through localization accuracy.</p>
<p>b) Baselines: We compare RoboMAP against two main categories of baselines to ensure a comprehensive evaluation.The first category includes leading general-purpose VLMs, such as GPT-4o [24], Gemini-2.5-pro[32], Qwen2.5VL[33], and GLM-4.5v[34].To test their intrinsic grounding capabilities, these models are evaluated in a strict zero-shot setting, prompted to directly output target coordinates based on the visual and language input.The second category comprises state-of-the-art models specifically designed for robotic spatial grounding.This includes top-performing points-based methods like RoboRefer [28] 1 , RoboBrain2.0 [7], Robo-Point [6], FSD [10], and Embodied-R1 [35].This comparison directly contrasts RoboMAP's dense, uncertaintyaware heatmap approach against these established discrete prediction methods.Additionally, we include a quantitative and qualitative comparison with the prior heatmap-based VLM, BridgeVLA-pretrain [18].c) Quantitative Evaluation: We evaluate RoboMAP against a comprehensive suite of VLMs in Table I.While RoboMAP's core advantage is its rich spatial heatmaps, current benchmarks are limited to point-based evaluation, requiring us to simplify our output for a fair comparison.Specifically, we extract only the single highest-confidence point from our heatmaps, meaning the following results are achieved with our model's capabilities partially constrained.</p>
<p>Even under this constrained evaluation, RoboMAP demonstrates an outstanding balance of performance and efficiency, as shown in the table.It establishes new stateof-the-art results on three of the four key benchmarks.</p>
<p>The source of this strong performance lies in our core design.RoboMAP's ability to capture spatial uncertainty makes it exceptionally effective on benchmarks requiring reasoning about ambiguous regions.Its significant lead on RoboRefIt further shows its strength in disambiguating objects in clutter.RoboMAP remains highly competitive on the multi-step RefSpatial benchmark, despite not being explicitly designed for sequential reasoning.Its architecture is optimized to predict an instruction's final outcome at once.</p>
<p>Perhaps most notably, RoboMAP achieves its top-ranked performance with a significantly simpler and more efficient methodology.Unlike the next-best competitor, Embodied-R1, which requires both Reinforced Fine-tuning (RFT) and an explicit reasoning stage, our model relies solely on a standard Supervised Fine-tuning (SFT) stage and a Direct inference pass.Critically, this results in an inference time of just 0.04 seconds, making it over 50 times faster than Embodied-R1.This combination of state-of-the-art accuracy, architectural simplicity, and exceptional efficiency validates RoboMAP as a powerful and practical foundation for spatial grounding in robotic tasks.d) Qualitative Analysis: We present representative visualizations comparing RoboMAP with baseline models in Fig 4 .These examples illustrate how RoboMAP captures broader, uncertain regions.This enables more robust and interpretable spatial grounding.In contrast, baselines relying on discrete points often fail to account for spatial uncertainty, especially in object-free or cluttered areas.</p>
<p>The specific advantages of our model are evident.When tasked to identify object-free spaces, such as between the mouse and the green cup (top row), the baseline models exhibit significant limitations, each in distinct ways.The
Original Image BrigeVLA FSD Ground Truth RoboMAP RoboBrain2.0
The free space between the mouse and the green cup.</p>
<p>The free space towards which the handle of the dark blue mug.</p>
<p>Embodied R1</p>
<p>Fig. 4: Qualitative comparison on challenging spatial grounding instructions.The visualizations highlight RoboMAP's ability to generate coherent heatmaps for ambiguous regions.This contrasts with the scattered points or diffuse heatmaps produced by baseline methods in the same scenarios.point-based models exhibit two primary error patterns: some make clear localization errors by incorrectly targeting the surrounding objects, while others place scattered points in the correct region but fail to represent it as a coherent region.The other heatmap-based baseline, BridgeVLA-pretrain, also demonstrates a complete inability to ground instructions in object-free areas.In contrast, RoboMAP successfully generates a well-defined intermediate representation, forming a heatmap that is both focused on the target area and expressive of its spatial extent and uncertainty.</p>
<p>B. Ablation Studies (Q2)</p>
<p>We conduct extensive ablation studies to validate two core components of our design: (i) the effectiveness of our proposed heatmap decoder, and (ii) the impact of the hybrid data composition strategy.</p>
<p>a) Analysis of Decoder Architecture: As shown in Table II, our Adaptive Heatmap Decoder (AHD) establishes a new state-of-the-art on all benchmarks with gains of up to 9%.This superior performance comes from its ability to generate adaptive heatmaps that intelligently adjust to scene content, overcoming a key limitation of static, contentagnostic decoders.</p>
<p>b) Analysis of Training Data Composition: The results in Table III validate our hybrid data strategy, as the full data mix achieves the top performance across all benchmarks.This demonstrates that unifying diverse datasets is highly effective for balancing general grounding with roboticsspecific understanding, creating a model that avoids the specialization trade-offs inherent in single-source training.</p>
<p>C. Zero-Shot Generalization and Deployment (Q3)</p>
<p>To answer Q3, we evaluate RoboMAP's ability to generate effective spatial representations for diverse robotic embodiments in a zero-shot setting, without any task-specific fine-tuning.We test its capabilities on both manipulation scenarios and a completely new task domain: navigation.</p>
<p>1) Embodied Performance on Manipulation Tasks: a) Performance in Simulation: We first evaluate our method in SimplerEnv [36], a scalable simulator for manipulation policies.In these tests, RoboMAP generates affordance heatmaps in a zero-shot manner to guide a motion planner.As shown in Table V, our method achieves a 60.5% success rate, establishing a new state-of-the-art among VLM-based methods.It outperforms prior leading works such as Embodied-R1 (56.2%) and SoFar (53.8%).This demonstrates RoboMAP's strong foundational ability to ground instructions in complex, interactive scenarios.</p>
<p>b) Performance in the Real World: c) Performance in the Real World: To validate its utility, we tested RoboMAP in two real-world settings.First, on a custom dual-arm robot, we evaluated 5 tabletop manipulation tasks, as detailed in Table IV.RoboMAP converts language commands into 2D grasp and place heatmaps, which guide a downstream pipeline using depth data and modules like SAM [26] and GraspNet-1billion [39] to compute final 6-DoF grasp and 3D place coordinates for execution by a motion planner.Figure 5 shows several example executions.Single-arm: Industrial sorting.</p>
<p>Dual-arm: Put the fork beside the plate.</p>
<p>Dual-arm: Place the corn into the empty area in the basket.RoboMAP achieves an 82% average success rate, significantly outperforming all baselines.This success stems from our dense heatmap representation.Unlike the sparse points from baselines, our heatmaps enable more precise object segmentation with SAM.The lower performance in Place [A] relative to [B] is not a model grounding failure but a limitation of our post-processing, which selects only the peak-probability pixel.This heuristic fails on surfaces with height variations, even when the underlying heatmap correctly identifies the entire valid placement region.</p>
<p>Beyond accuracy, RoboMAP's inference speed of just 0.04 seconds makes it highly practical for real-time applications.This is orders of magnitude faster than baselines like FSD (15.68s) and RoboBrain2.0-7B(8.07s), which incorporate lengthy iterative reasoning processes.</p>
<p>We plan to develop more sophisticated downstream policies to better leverage both the rich information in these heatmaps and high inference speed of our model.</p>
<p>In addition to the dual-arm setup, we further deployed RoboMAP on an industrial single-arm robot for highprecision sorting, such as locating a screw that is isolated from the main cluster.The successful zero-shot experiments demonstrate that RoboMAP can produce a generalizable and embodiment-agnostic spatial representation.Videos of these robotic experiments will be released publicly.</p>
<p>2) Embodied Performance on Mobile Navigation: For navigation with the mobile robot, Figure 6 showcases RoboMAP's superior performance over the SOTA pointsbased Robobrain2.0-7B and BridgeVLA baselines across simulated, indoor, and outdoor environments.Our model accurately grounds complex instructions, such as interpreting qualifiers like in the distance (Simulated), finding the free space between two sweepers (Indoor), and following the curve (Outdoor).In each environment, RoboMAP provides a precise navigational heatmap where the baselines are either inaccurate or fail completely.</p>
<p>VII. CONCLUSION, LIMITATION AND FUTURE WORK</p>
<p>We presented RoboMAP, a framework that grounds spatial language into adaptive affordance heatmaps.By representing targets as distributions instead of points, our method captures spatial uncertainty to achieve state-of-the-art performance on key robotics benchmarks.Our method achieves an 82% success rate in real-world manipulation, successfully generalizes to navigation, and operates at 25 Hz (0.04s per inference).</p>
<p>The primary limitation of our work lies not in the heatmap generation, but in how this rich information is underutilized by downstream policies.Our current heuristic of selecting only the single, peak-confidence pixel is overly simplistic, causing failures on surfaces with varying heights even when the heatmap correctly identifies the entire valid region.This directly motivates the need to design more sophisticated downstream policies.</p>
<p>Looking ahead, such policies could unlock two key research directions.First, they would enable smarter motion planning by leveraging the entire distribution to find optimal targets or recover from errors by targeting secondary peaks.Second, they would allow for uncertainty-aware behaviors, using the heatmap's confidence to trigger actions like proactively asking for clarification on ambiguous commands.</p>
<p>Fig. 1 .
1
Fig. 1.From Brittle Points to Adaptive Affordance Heatmaps</p>
<p>Fig. 2 :
2
Fig. 2: Overall Architecture of the RoboMAP Framework.</p>
<p>Fig. 3 :
3
Fig.3: The architecture of Adaptive Heatmap Decoder (AHD).A PaliGemma Backbone processes the image and instruction into visual tokens.These are reshaped into a language-conditioned feature grid F low , which is then fed into our AHD.The AHD uses two branches (AKG and CAP) to compute the final adaptive affordance heatmap.</p>
<p>Fig. 5 :Fig. 6 :
56
Fig. 5: RoboMAP's zero-shot generalization across diverse environments (simulation, real-world) and robotic embodiments (simulated arm, single industrial arm, dual-arm humanoid).</p>
<p>TABLE I :
I
Performance and efficiency comparison on robotic grounding benchmarks.
ModelSizeFine-tuning MethodWhere2placeAccuracy (%) ↑ RoboRefIt RefSpatialVABench-PointInference Time (s) ↓General-Purpose VLMsGPT-4o [24]--23.0415.289.1216.59-Gemini-2.5-pro [32]--52.9549.5029.2421.71-Qwen2.5VL [33]72B-37.1578.5020.8523.30-GLM-4.5v [34]106B-29.8974.5011.1929.23-Specialized Robotic Grounding VLMsRoboPoint [6]13BSFT46.8049.8016.0719.093.10RoboRefer [28]2BSFT66.0072.8033.7724.671.54RoboBrain2.0 [7]3B-54.8654.4235.818.080.67RoboBrain2.0 [7]7BSFT + RFT63.5947.5532.5012.7010.41FSD [10]13BSFT45.8156.7314.9061.8215.68Embodied-R1 [35]3BSFT + RFT69.5085.5838.4666.002.18BridgeVLA-pretrain [18]3BSFT15.0026.2511.5010.500.06RoboMAP3BSFT73.0088.7336.5070.000.04</p>
<p>TABLE II :
II
Ablation study on the heatmap decoder architecture.
Decoder Architecture Where2place RoboRefIt RefSpatial VABenchDeconvolution [20]64.00%81.00%23.00%69.00%PixelShuffle [21]59.00%80.00%29.00%68.00%Bilinear64.00%80.70%26.39%68.00%Ours (AHD)73.00%88.73%36.50%70.00%</p>
<p>TABLE III :
III
Ablation study on the composition of training data.
Training DataWhere2place RoboRefIt RefSpatial VABenchw/ RoboRefIt only5.00%79.50%15.20%5.50%w/ COCO only15.20%67.80%12.00%6.50%w/ OXE-MAP only48.00%56.80%20.20%65.50%w/ RoboData only40.00%54.50%13.40%11.00%Ours (Full Data Mix)73.00%88.73%36.50%70.00%</p>
<p>TABLE IV :
IV
Evaluation of Real-World Robotic Manipulation Tasks.
MethodPlace [A]Move [A]Successes (out of 10 trials) Move [Specified A]Place [A]Place [A] into theSuccess RateInference Speed (s)on [B]beside [B]to [B]relative to [B]empty area in [B]FSD2/103/104/104/101/1028%15.68RoboBrain2.0-3B5/107/107/107/107/1066%0.45RoboBrain2.0-7B8/107/106/105/106/1064%8.07RoboMAP9/109/108/106/109/1082%0.04
Simulation: Put spoon on table cloth.</p>
<p>TABLE V :
V
Zero-Shot Evaluation in SimplerEnv on WidowX Robot.
ModelSpoon→Towel Carrot→Plate Green→Yellow Eggplant→Basket Success RateVLA-based MethodsOpenVLA [11]0.0%0.0%0.0%4.1%1.0%π0 [12]29.1%0.0%16.6%62.5%27.1%π0 FAST [12]29.1%21.9%10.8%66.6%32.1%VLM-based MethodsMOKA [37]45.8%41.6%33.3%12.5%33.3%SoFar [38]55.5%56.9%62.5%40.2%53.8%RoboPoint [6]16.7%20.8%8.3%25.0%17.7%FSD [10]41.6%50.0%33.3%37.5%40.6%Embodied-R1 [35]65.2%68.0%36.1%58.3%56.2%RoboMAP66.0%54.0%52.0%70.0%60.5%
We evaluate the publicly available SFT-trained version of RoboRefer, as its RFT-trained counterpart has not been open-sourced.</p>
<p>Language-conditioned learning for robotic manipulation: A survey. H Zhou, X Yao, Y Meng, S Sun, Z Bing, K Huang, A Knoll, arXiv:2312.108072023arXiv preprint</p>
<p>Robot learning in the era of foundation models: A survey. X Xiao, J Liu, Z Wang, Y Zhou, Y Qi, S Jiang, B He, Q Cheng, Neurocomputing. 1299632025</p>
<p>Generative models in decision making: A survey. Y Li, X Shao, J Zhang, H Wang, L M Brunswic, K Zhou, J Dong, K Guo, X Li, Z Chen, arXiv:2502.171002025arXiv preprint</p>
<p>Roboground: Robotic manipulation with grounded vision-language priors. H Huang, X Chen, Y Chen, H Li, X Han, Z Wang, T Wang, J Pang, Z Zhao, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference202522550</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. B Zitkovich, T Yu, S Xu, P Xu, T Xiao, F Xia, J Wu, P Wohlhart, S Welker, A Wahid, Conference on Robot Learning. PMLR2023</p>
<p>Robopoint: A vision-language model for spatial affordance prediction for robotics. W Yuan, J Duan, V Blukis, W Pumacay, R Krishna, A Murali, A Mousavian, D Fox, arXiv:2406.107212024arXiv preprint</p>
<p>B R Team, M Cao, H Tan, Y Ji, M Lin, Z Li, Z Cao, P Wang, E Zhou, Y Han, arXiv:2507.02029Robobrain 2.0 technical report. 2025arXiv preprint</p>
<p>Robospatial: Teaching spatial understanding to 2d and 3d visionlanguage models for robotics. C H Song, V Blukis, J Tremblay, S Tyree, Y Su, S Birchfield, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference2025780</p>
<p>Bridging perception and action: Spatially-grounded mid-level representations for robot generalization. J Yang, C K Fu, D Shah, D Sadigh, F Xia, T Zhang, arXiv:2506.061962025arXiv preprint</p>
<p>From seeing to doing: Bridging reasoning and decision for robotic manipulation. Y Yuan, H Cui, Y Chen, Z Dong, F Ni, L Kou, J Liu, P Li, Y Zheng, J Hao, arXiv:2505.085482025arXiv preprint</p>
<p>Openvla: An open-source vision-language-action model. M J Kim, K Pertsch, S Karamcheti, T Xiao, A Balakrishna, S Nair, R Rafailov, E Foster, G Lam, P Sanketi, arXiv:2406.092462024arXiv preprint</p>
<p>K Black, N Brown, D Driess, A Esmail, M Equi, C Finn, N Fusai, L Groom, K Hausman, B Ichter, S Jakubczak, T Jones, L Ke, S Levine, A Li-Bell, M Mothukuri, S Nair, K Pertsch, L X Shi, J Tanner, Q Vuong, A Walling, H Wang, U Zhilinsky, arXiv:2410.24164A vision-language-action flow model for general robot control. 2024arXiv preprint</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. M J Ahn, A Brohan, Robotics: Science and Systems (RSS). 2022</p>
<p>Robobrain: A unified brain model for robotic manipulation from abstract to concrete. Y Ji, H Tan, J Shi, X Hao, Y Zhang, H Zhang, P Wang, M Zhao, Y Mu, P An, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference2025</p>
<p>Spatialcot: Advancing spatial reasoning through coordinate alignment and chain-of-thought for embodied task planning. Y Liu, D Chi, S Wu, Z Zhang, Y Hu, L Zhang, Y Zhang, S Wu, T Cao, G Huang, arXiv:2501.100742025arXiv preprint</p>
<p>A0: An affordance-aware hierarchical model for general robotic manipulation. P Li, A Singh, Y Zhu, C Finn, D Xu, arXiv:2504.126362025arXiv preprint</p>
<p>Efficient heatmapguided 6-dof grasp detection in cluttered scenes. S Chen, W Tang, P Xie, W Yang, G Wang, IEEE Robotics and Automation Letters. 2023</p>
<p>Bridgevla: Improving vision-language-action models by bridging vision-language alignment. D Xu, Y Ma, E Xie, W Wang, arXiv:2403.024152024arXiv preprint</p>
<p>L Beyer, A Steiner, A S Pinto, A Kolesnikov, X Wang, D Salz, M Neumann, I Alabdulmohsin, M Tschannen, E Bugliarello, arXiv:2407.07726Paligemma: A versatile 3b vlm for transfer. 2024arXiv preprint</p>
<p>Visualizing and understanding convolutional networks. M D Zeiler, R Fergus, European conference on computer vision. Springer2014</p>
<p>Real-time single image and video superresolution using an efficient sub-pixel convolutional neural network. W Shi, J Caballero, F Huszár, J Totz, A P Aitken, R Bishop, D Rueckert, Z Wang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>CARAFE: Content-Aware ReAssembly of FEatures. J Wang, K Chen, R Xu, Z Liu, C C Loy, D Lin, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)2019</p>
<p>Open x-embodiment: Robotic learning datasets and rt-x models: Open xembodiment collaboration 0. A O'neill, A Rehman, A Maddukuri, A Gupta, A Padalkar, A Lee, A Pooley, A Gupta, A Mandlekar, A Jain, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>A Hurst, A Lerer, A P Goucher, A Perelman, A Ramesh, A Clark, A Ostrow, A Welihinda, A Hayes, A Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>Grounding dino: Marrying dino with grounded pre-training for open-set object detection. S Liu, Z Zeng, T Ren, F Li, H Zhang, J Yang, Q Jiang, C Li, J Yang, H Su, European conference on computer vision. Springer2024</p>
<p>Segment anything. A Kirillov, E Mintun, N Ravi, H Mao, C Rolland, L Gustafson, T Xiao, S Whitehead, A C Berg, W.-Y Lo, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2023</p>
<p>Microsoft coco: Common objects in context. T.-Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Dollár, C L Zitnick, European conference on computer vision. Springer2014</p>
<p>Roborefer: Towards spatial referring with reasoning in vision-language models for robotics. E Zhou, J An, C Chi, Y Han, S Rong, C Zhang, P Wang, Z Wang, T Huang, L Sheng, arXiv:2506.043082025arXiv preprint</p>
<p>Decoupled weight decay regularization. I Loshchilov, F Hutter, arXiv:1711.051012017arXiv preprint</p>
<p>A study of bfloat16 for deep learning training. D Kalamkar, D Mudigere, N Mellempudi, D Das, K Banerjee, S Avancha, D T Vooturi, N Jammalamadaka, J Huang, H Yuen, arXiv:1905.123222019arXiv preprint</p>
<p>Pytorch: An imperative style, high-performance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, Advances in neural information processing systems. 201932</p>
<p>Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. G Comanici, E Bieber, M Schaekermann, I Pasupat, N Sachdeva, I Dhillon, M Blistein, O Ram, D Zhang, E Rosen, arXiv:2507.062612025arXiv preprint</p>
<p>S Bai, K Chen, X Liu, J Wang, W Ge, S Song, K Dang, P Wang, S Wang, J Tang, H Zhong, Y Zhu, M Yang, Z Li, J Wan, P Wang, W Ding, Z Fu, Y Xu, J Ye, X Zhang, T Xie, Z Cheng, H Zhang, Z Yang, H Xu, J Lin, arXiv:2502.13923Qwen2.5-vl technical report. 2025arXiv preprint</p>
<p>GLM: general language model pretraining with autoregressive blank infilling. Z Du, Y Qian, X Liu, M Ding, J Qiu, Z Yang, J Tang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL). the 60th Annual Meeting of the Association for Computational Linguistics (ACL)2022</p>
<p>Embodied-r1: Reinforced embodied reasoning for general robotic manipulation. Y Yuan, H Cui, Y Huang, Y Chen, F Ni, Z Dong, P Li, Y Zheng, J Hao, arXiv:2508.139982025arXiv preprint</p>
<p>Evaluating real-world robot manipulation policies in simulation. X Li, K Hsu, J Gu, K Pertsch, O Mees, H R Walke, C Fu, I Lunawat, I Sieh, S Kirmani, S Levine, J Wu, C Finn, H Su, Q Vuong, T Xiao, 8th Annual Conference on Robot Learning. 2024</p>
<p>Moka: Open-world robotic manipulation through mark-based visual prompting. K Fang, RSSF Liu, RSSP Abbeel, RSSS Levine, RSSRobotics: Science and Systems. 2024</p>
<p>Sofar: Language-grounded orientation bridges spatial reasoning and object manipulation. Z Qi, W Zhang, Y Ding, R Dong, X Yu, J Li, L Xu, B Li, X He, G Fan, arXiv:2502.131432025arXiv preprint</p>
<p>Graspnet-1billion: A largescale benchmark for general object grasping. H.-S Fang, C Wang, M Gou, C Lu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202011453</p>            </div>
        </div>

    </div>
</body>
</html>
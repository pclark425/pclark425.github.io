<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5313 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5313</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5313</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-112.html">extraction-schema-112</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <p><strong>Paper ID:</strong> paper-268297295</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.05152v1.pdf" target="_blank">Towards a Psychology of Machines: Large Language Models Predict Human Memory</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) are demonstrating remarkable capabilities across various tasks despite lacking a foundation in human cognition. This raises the question: can these models, beyond simply mimicking human language patterns, offer insights into the mechanisms underlying human cognition? This study explores the ability of ChatGPT to predict human performance in a language-based memory task. Building upon theories of text comprehension, we hypothesize that recognizing ambiguous sentences (e.g., "Because Bill drinks wine is never kept in the house") is facilitated by preceding them with contextually relevant information. Participants, both human and ChatGPT, were presented with pairs of sentences. The second sentence was always a garden-path sentence designed to be inherently ambiguous, while the first sentence either provided a fitting (e.g., "Bill has chronic alcoholism") or an unfitting context (e.g., "Bill likes to play golf"). We measured both human’s and ChatGPT's ratings of sentence relatedness, ChatGPT’s memorability ratings for the garden-path sentences, and humans' spontaneous memory for the garden-path sentences. The results revealed a striking alignment between ChatGPT's assessments and human performance. Sentences deemed more related and assessed as being more memorable by ChatGPT were indeed better remembered by humans, even though ChatGPT's internal mechanisms likely differ significantly from human cognition. This finding, which was confirmed with a robustness check employing synonyms, underscores the potential of generative AI models to predict human performance accurately. We discuss the broader implications of these findings for leveraging LLMs in the development of psychological theories and for gaining a deeper understanding of human cognition.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5313.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5313.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4 (ChatGPT / GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transformer-based large language model accessed via OpenAI's ChatGPT API (June 2023). Evaluated zero-shot on sentence-pair relatedness and memorability prompts to predict human recognition memory for garden-path sentences preceded by fitting or unfitting contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (accessed via ChatGPT / OpenAI API, June 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based autoregressive large language model trained on very large, heterogeneous human-created text corpora to produce humanlike language; accessed zero-shot via OpenAI API with temperature set to 1 for response variability.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Garden-path sentence relatedness rating and surprise recognition memory test (contextual memory task)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Memory / language comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Stimuli: garden-path sentences (Sentence 2) each paired with either a fitting or an unfitting preceding context sentence (Sentence 1). Procedure for LLM: zero-shot prompts asking (a) relatedness of the two sentences on a 1–10 scale and (b) memorability/recognizability of the garden-path sentence on a 1–10 scale; 100 independent LLM responses collected per prompt per sentence (45 sentences). Procedure for humans: participants (N=85 after exclusions) read 22 sentence pairs (half fitting, half unfitting), rated relatedness 1–10, then completed a surprise old/new recognition test of 44 garden-path sentences (22 targets, 22 distractors); memory analyzed with d' (signal detection theory) and item-level proportion-correct.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>LLM responses (GPT-4) produced higher relatedness and memorability ratings in the fitting than in the unfitting condition (relatedness: χ2(1)=44843.00, p < .001; memorability: χ2(1)=2660.00, p < .001). Robustness check (alternative wording) reported mean relatedness in fitting M = 6.67 (SD = 2.25) vs unfitting M = 1.12 (SD = 0.59); recognizability (memorability proxy) fitting M = 2.67 (SD = 1.34) vs unfitting M = 1.00 (SD = 0.02). A mixed-effects model showed a significant interaction of context and relatedness predicting LLM memorability ratings (χ2(1)=86.37, p < .001), with higher relatedness predicting higher memorability particularly in the unfitting condition.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human participants (N = 85; 57 female, 27 male, 1 no response; mean age M = 45.34, SD = 13.95) showed higher relatedness ratings in the fitting than unfitting condition (χ2(1)=4087.60, p < .001). Recognition memory (d') was significantly higher in the fitting than unfitting context: t(84) = 5.75, p < .001, Cohen's d = 0.62. Response bias (c) differed by condition (fitting M = -0.08, SD = 0.22; unfitting M = 0.13, SD = 0.29; t(84) = -5.78, p < .001). At the item level, a significant interaction of context and relatedness predicted human proportion-correct (χ2(1)=9.39, p = .002), with higher relatedness increasing memory selectively in the unfitting condition.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Direct comparisons: LLM relatedness ratings closely mirrored human relatedness ratings (both showed higher values for fitting vs. unfitting contexts), and GPT-4's memorability ratings predicted human memory performance at the item level (significant LLM context×relatedness interaction and correspondence with human interaction pattern). The paper reports statistical evidence that GPT-4's ratings align with human judgments and predict human recognition variability, rather than a claim that GPT-4 'outperforms' humans on the task. Reported statistics show both LLM and human data yield the same qualitative pattern (context main effect and context×relatedness interaction), with significance reported for both LLM and human models.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Key caveats reported: (1) GPT-4 lacks a human-like memory system — it only provided ratings (not actual episodic encoding/retrieval); (2) stimuli may appear in LLM training data (performativity), potentially inflating model alignment with human judgments; (3) temperature was set to 1 to increase variability and 100 responses were sampled per prompt, which is a methodological choice affecting response distribution; (4) model size/parameter count not reported in paper; (5) LLM memorability ratings were numeric judgments, not behavioral memory measures — interpretation as predictive proxies requires caution; (6) potential fluency effects and dataset overlap noted by authors as alternative explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>additional_notes</strong></td>
                            <td>LLM data consisted of 100 independent responses per prompt per sentence (45 sentences → 4500 responses per condition; total 9000). Zero-shot prompting was used with explicit phrasing for relatedness and memorability; robustness checks used synonyms ('linked' and 'recognizability') which produced similar results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards a Psychology of Machines: Large Language Models Predict Human Memory', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies <em>(Rating: 2)</em></li>
                <li>Using cognitive psychology to understand GPT-3 <em>(Rating: 2)</em></li>
                <li>Overlap in meaning is a stronger predictor of semantic activation in GPT-3 than in humans <em>(Rating: 2)</em></li>
                <li>Mind meets machine: Unravelling GPT-4's cognitive psychology <em>(Rating: 2)</em></li>
                <li>BERT Shows Garden Path Effects <em>(Rating: 1)</em></li>
                <li>Emergent analogical reasoning in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5313",
    "paper_id": "paper-268297295",
    "extraction_schema_id": "extraction-schema-112",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4 (ChatGPT / GPT-4)",
            "brief_description": "Transformer-based large language model accessed via OpenAI's ChatGPT API (June 2023). Evaluated zero-shot on sentence-pair relatedness and memorability prompts to predict human recognition memory for garden-path sentences preceded by fitting or unfitting contexts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (accessed via ChatGPT / OpenAI API, June 2023)",
            "model_description": "Transformer-based autoregressive large language model trained on very large, heterogeneous human-created text corpora to produce humanlike language; accessed zero-shot via OpenAI API with temperature set to 1 for response variability.",
            "model_size": null,
            "cognitive_test_name": "Garden-path sentence relatedness rating and surprise recognition memory test (contextual memory task)",
            "cognitive_test_type": "Memory / language comprehension",
            "cognitive_test_description": "Stimuli: garden-path sentences (Sentence 2) each paired with either a fitting or an unfitting preceding context sentence (Sentence 1). Procedure for LLM: zero-shot prompts asking (a) relatedness of the two sentences on a 1–10 scale and (b) memorability/recognizability of the garden-path sentence on a 1–10 scale; 100 independent LLM responses collected per prompt per sentence (45 sentences). Procedure for humans: participants (N=85 after exclusions) read 22 sentence pairs (half fitting, half unfitting), rated relatedness 1–10, then completed a surprise old/new recognition test of 44 garden-path sentences (22 targets, 22 distractors); memory analyzed with d' (signal detection theory) and item-level proportion-correct.",
            "llm_performance": "LLM responses (GPT-4) produced higher relatedness and memorability ratings in the fitting than in the unfitting condition (relatedness: χ2(1)=44843.00, p &lt; .001; memorability: χ2(1)=2660.00, p &lt; .001). Robustness check (alternative wording) reported mean relatedness in fitting M = 6.67 (SD = 2.25) vs unfitting M = 1.12 (SD = 0.59); recognizability (memorability proxy) fitting M = 2.67 (SD = 1.34) vs unfitting M = 1.00 (SD = 0.02). A mixed-effects model showed a significant interaction of context and relatedness predicting LLM memorability ratings (χ2(1)=86.37, p &lt; .001), with higher relatedness predicting higher memorability particularly in the unfitting condition.",
            "human_baseline_performance": "Human participants (N = 85; 57 female, 27 male, 1 no response; mean age M = 45.34, SD = 13.95) showed higher relatedness ratings in the fitting than unfitting condition (χ2(1)=4087.60, p &lt; .001). Recognition memory (d') was significantly higher in the fitting than unfitting context: t(84) = 5.75, p &lt; .001, Cohen's d = 0.62. Response bias (c) differed by condition (fitting M = -0.08, SD = 0.22; unfitting M = 0.13, SD = 0.29; t(84) = -5.78, p &lt; .001). At the item level, a significant interaction of context and relatedness predicted human proportion-correct (χ2(1)=9.39, p = .002), with higher relatedness increasing memory selectively in the unfitting condition.",
            "performance_comparison": "Direct comparisons: LLM relatedness ratings closely mirrored human relatedness ratings (both showed higher values for fitting vs. unfitting contexts), and GPT-4's memorability ratings predicted human memory performance at the item level (significant LLM context×relatedness interaction and correspondence with human interaction pattern). The paper reports statistical evidence that GPT-4's ratings align with human judgments and predict human recognition variability, rather than a claim that GPT-4 'outperforms' humans on the task. Reported statistics show both LLM and human data yield the same qualitative pattern (context main effect and context×relatedness interaction), with significance reported for both LLM and human models.",
            "notable_differences_or_limitations": "Key caveats reported: (1) GPT-4 lacks a human-like memory system — it only provided ratings (not actual episodic encoding/retrieval); (2) stimuli may appear in LLM training data (performativity), potentially inflating model alignment with human judgments; (3) temperature was set to 1 to increase variability and 100 responses were sampled per prompt, which is a methodological choice affecting response distribution; (4) model size/parameter count not reported in paper; (5) LLM memorability ratings were numeric judgments, not behavioral memory measures — interpretation as predictive proxies requires caution; (6) potential fluency effects and dataset overlap noted by authors as alternative explanations.",
            "additional_notes": "LLM data consisted of 100 independent responses per prompt per sentence (45 sentences → 4500 responses per condition; total 9000). Zero-shot prompting was used with explicit phrasing for relatedness and memorability; robustness checks used synonyms ('linked' and 'recognizability') which produced similar results.",
            "uuid": "e5313.0",
            "source_info": {
                "paper_title": "Towards a Psychology of Machines: Large Language Models Predict Human Memory",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies",
            "rating": 2,
            "sanitized_title": "using_large_language_models_to_simulate_multiple_humans_and_replicate_human_subject_studies"
        },
        {
            "paper_title": "Using cognitive psychology to understand GPT-3",
            "rating": 2,
            "sanitized_title": "using_cognitive_psychology_to_understand_gpt3"
        },
        {
            "paper_title": "Overlap in meaning is a stronger predictor of semantic activation in GPT-3 than in humans",
            "rating": 2,
            "sanitized_title": "overlap_in_meaning_is_a_stronger_predictor_of_semantic_activation_in_gpt3_than_in_humans"
        },
        {
            "paper_title": "Mind meets machine: Unravelling GPT-4's cognitive psychology",
            "rating": 2,
            "sanitized_title": "mind_meets_machine_unravelling_gpt4s_cognitive_psychology"
        },
        {
            "paper_title": "BERT Shows Garden Path Effects",
            "rating": 1,
            "sanitized_title": "bert_shows_garden_path_effects"
        },
        {
            "paper_title": "Emergent analogical reasoning in large language models",
            "rating": 1,
            "sanitized_title": "emergent_analogical_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.00815,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards a Psychology of Machines: Large Language Models Predict Human Memory</p>
<p>Markus Huff markus.huff@uni-tuebingen.de 
Leibniz-Institut für Wissensmedien
TübingenGermany</p>
<p>Eberhard Karls Universität Tübingen
Germany</p>
<p>Elanur Ulakçı 
Leibniz-Institut für Wissensmedien
TübingenGermany</p>
<p>Eberhard Karls Universität Tübingen
Germany</p>
<p>Leibniz-Institut für Wissensmedien
Schleichstr. 672072TübingenGermany</p>
<p>Towards a Psychology of Machines: Large Language Models Predict Human Memory
6D723895F9297356CFDD1A1009BBC470generative artificial intelligencegarden-path sentencesmachine psychologymemorycontext
Large language models (LLMs), such as ChatGPT, have shown remarkable abilities in natural language processing, opening new avenues in psychological research.This study explores whether LLMs can predict human memory performance in tasks involving garden-path sentences and contextual information.In the first part, we used ChatGPT to rate the relatedness and memorability of garden-path sentences preceded by either fitting or unfitting contexts.In the second part, human participants read the same sentences, rated their relatedness, and completed a surprise memory test.The results demonstrated that ChatGPT's relatedness ratings closely matched those of the human participants, and its memorability ratings effectively predicted human memory performance.Both LLM and human data revealed that higher relatedness in the unfitting context condition was associated with better memory performance, aligning with probabilistic frameworks of context-dependent learning.These findings suggest that LLMs, despite lacking human-like memory mechanisms, can model aspects of human cognition and serve as valuable tools in psychological research.We propose the field of machine psychology to explore this interplay between human cognition and artificial intelligence, offering a bidirectional approach where LLMs can both benefit from and contribute to our understanding of human cognitive processes.</p>
<p>Towards a Psychology of Machines: Large Language Models Predict Human Memory</p>
<p>Transformer-based large language models (LLMs) have revolutionized the area of natural language processing with their exceptional performance, offering profound implications for the science of psychology, given the integral role of language in all subfields (Demszky et al., 2023).Despite their inherent lack of subjective experience possessed by humans, such as thinking or feeling, LLMs are arguably more than mere stochastic replicators of their input statistics.They are designed to produce human-like realistic linguistic outputs, and they are trained on extensive datasets crafted by humans themselves.</p>
<p>These attributes have markedly elevated the prominence of LLMs in a field we introduce as machine psychology.Although-from a linguistic perspective-LLMs are no models of cognitive processes ("Language Models and Linguistic Theories beyond Words," 2023), studies in this field explore the capabilities of LLMs and the degree to which they align with human behavior and cognitive processes (Aher et al., 2023;Binz &amp; Schulz, 2023b;Buschoff et al., 2023;Dhingra et al., 2023;Dillion et al., 2023;Gilardi et al., 2023;Horton, 2023;Michelmann et al., 2023;Ritter et al., 2017;Tak &amp; Gratch, 2023).Taken together, these studies emphasize a remarkable congruity between human cognition and LLMs, indicating the potential of LLMs to play a meaningful role in psychological research.In this paper, we go beyond the current understanding of LLMs, particularly ChatGPT, by investigating its potential for predicting human memory performance.</p>
<p>Machine Psychology: Exploring the Intersection of Human and Artificial Cognition</p>
<p>Following their launch, LLMs have exhibited exceptional capabilities in natural language processing (NLP) and production (Manning et al., 2020).Ranging from the simplest interactions as chatbots to their implementation in various areas such as education (Kasneci et al., 2023), healthcare (Thirunavukarasu et al., 2023), and information search (Stadler et al., 2024), these models have woven themselves into human experiences with their remarkable advantages and the convenience they provide.Beyond their expertise in NLP and practical applications, LLMs reveal significant alignment with human cognitive functions and behaviors in psychological tests where they are evaluated as subjects, despite not being fundamentally developed for this purpose.To effectively utilize LLMs, and to better understand the increasing harmony between them and humans, it is crucial to examine the strengths and limitations of these models.Nonetheless, the lack of transparency in how exactly LLMs function poses a challenge (Schwartz, 2022;Zubiaga, 2024).To address this challenge and to open up these "black boxes", psychology offers a strong framework with its multidisciplinary perspective.In this paper, we propose machine psychology, an area dedicated to explore the interplay between human cognition and behavior, and the capabilities of LLMs.In the field of machine psychology, LLMs are perceived as a distinct yet analogous "second mind" that is yet to be uncovered, focusing on their resemblance to human cognitive functions while acknowledging their artificial nature.</p>
<p>This emerging field adopts a bidirectional approach, integrating the unique capabilities of LLMs and the rich and profound knowledge of psychology to deepen the understanding of both artificial and human cognition.On one hand, psychological knowledge and methodologies can be applied to understand LLMs through examining them as participants in assessments and experiments.This approach enables the systematic exploration of their internal processes, behavioral tendencies, and cognitive patterns, as well as the extent to which these models mirror human cognition (Binz &amp; Schulz, 2023b;Mei et al., 2024;Webb et al., 2023).On the other hand, LLMs possess significant potential to advance the study of human cognition.Their distinct information processing mechanisms open new avenues for research and provide the opportunity to use LLMs as proxies/simulations for investigating human cognitive processes, thereby broadening our understanding of the human mind (Binz &amp; Schulz, 2023a;Horton, 2023).Drawing from the concept of Symbiotic Autonomy in human-robot interaction studies, which focuses on the reciprocal cooperation between humans and robots (Vanzo et al., 2020), we propose a similar bidirectional approach of machine psychology, where humans and LLMs mutually contribute to advance the understanding of the complexities of both human and artificial cognition.Our focus in this paper is on the latter direction, utilizing LLMs as a proxy to investigate human cognitive processes.For the first time, we apply LLMs to predict human memory performance.We achieve this by conducting a language-based memory task that incorporates garden-path sentences and contextual information.In the following sections, we examine the implications of LLMs' ability to predict the memory performance of humans, despite their absence of a human-like memory mechanism.</p>
<p>Context-Driven Memory: A Framework for Evaluating LLMs' Predictions</p>
<p>Our investigation into the potential of LLMs to predict human memory performance is centered on the critical role of context in the formation of memories.Context is important in resolving ambiguity (Szewczyk &amp; Federmeier, 2022), the pervasive and fundamental element of the natural language, which poses a challenge to comprehension (MacGregor et al., 2020).The recognized detrimental impact of ambiguity on language processing is not exclusive to humans, it also constitutes a significant impediment for LLMs (Irwin et al., 2023;Liu et al., 2023).While GPT employs an autoregressive design with the sequential generation process, BERT builds upon this approach by utilizing a bidirectional architecture, considering both preceding and following contexts (Naveed et al., 2023).Despite possessing advanced linguistic processing capabilities, both models encounter difficulties in effectively managing the ambiguity inherent in language, mirroring the challenges present in human cognitive processing during comprehension (Irwin et al., 2023;Liu et al., 2023).This shared challenge of managing ambiguity is particularly evident in the interpretation of complex sentence structures, such as garden-path sentences (Fujita, 2021;Li et al., 2024).Although grammatically correct, these sentences introduce temporary ambiguity which can lead to misinterpretation and hinder comprehension (Ferreira et al., 2001).While garden-path sentences present serious challenges for understanding due to inherent ambiguity, prior context is acknowledged to reduce those complications (Grodner et al., 2005;Kaiser &amp; Trueswell, 2004).The beneficial impact of prior context on comprehending garden-path sentences can be explained through the structure-building framework (Gernsbacher, 1997).</p>
<p>This framework suggests that comprehension involves constructing mental representations by linking incoming information to an established structure.In this case, the relevant context sentence provides a foundation for the mental structure, onto which the garden-path sentence can be integrated.Being part of the same mental representation, the context sentence aids in clarifying ambiguity enabling the cohesive processing of both sentences and improving comprehension (Brich et al., 2024).The help of prior context is not only important for facilitating understanding, but also plays a key role for memory formation.Successful comprehension, achieved by extracting the meaning of sentences (Kaup et al., 2024), thereby envisioning the situations described in the text (Gernsbacher &amp; Robertson, 1995;Schütt et al., 2023) lays the groundwork for creating robust mental representations which are more likely to be retained over time (Kintsch et al., 1990).In the case of not being able to comprehend the sentence, readers make use of the surface form of information (Schnotz &amp; Bannert, 2003), known to be forgotten well before the meaning of the text (verbatim effect; Poppenk et al., 2008).By aiding understanding, prior context enhances the encoding of garden-path sentences into memory, establishing a critical link between comprehension and retention.</p>
<p>Experimental Overview and Hypotheses</p>
<p>Despite lacking a foundation in human cognition, LLMs achieve near-human performance across various tasks (Binz &amp; Schulz, 2023b).This begs the question: can LLMs, if not simply "stochastic parrots" (Digutsch &amp; Kosinski, 2023), reveal insights into the underlying mechanisms of human cognition?Here, we investigate this by harnessing generative AI to predict human memory performance.Specifically, we test if LLMs can predict how well humans remember garden path sentences prefaced by fitting or unfitting contexts.This study probes the potential of LLMs to shed light on human information processing, even while operating on different principles.</p>
<p>Context</p>
<p>Relatedness prompt Memorability prompt</p>
<p>Fitting</p>
<p>Read Sentence 1 and Sentence 2 and answer the following question.How related are the two sentences from 1 (not at all) to 10 (highly)?</p>
<p>Sentence 1: "Bill has chronic alcoholism."Sentence 2: "Because Bill drinks wine is never kept in the house."</p>
<p>Read Sentence 1 and Sentence 2 and answer the following question.How do you rate the memorability of Sentence 2 from 1 (not at all) to 10 (excellent)?</p>
<p>Sentence 1: "Bill has chronic alcoholism."Sentence 2: "Because Bill drinks wine is never kept in the house."</p>
<p>Unfitting</p>
<p>Read Sentence 1 and Sentence 2 and answer the following question.How related are the two sentences from 1 (not at all) to 10 (highly)?</p>
<p>Sentence 1: "Bill likes to play golf."Sentence 2: "Because Bill drinks wine is never kept in the house."</p>
<p>Read Sentence 1 and Sentence 2 and answer the following question.How do you rate the memorability of Sentence 2 from 1 (not at all) to 10 (excellent)?</p>
<p>Sentence 1: "Bill likes to play golf."Sentence 2: "Because Bill drinks wine is never kept in the house."</p>
<p>Note: Sentence 2 always represented the garden path sentence.</p>
<p>In the first part of this study, we submitted the garden-path sentences (Sentence 2 in the prompt) with a preceding sentence (Sentence 1 in the prompt) that matched (fitting) or mismatched (unfitting) the context of the garden-path sentence to ChatGPT (Table 1).We collected 100 responses for each prompt (relatedness of Sentences 1 and 2, and memorability of Sentence 2; including a robustness check with synonyms).In the second part of this study, we used the LLM responses to predict human performance, in which we presented participants with the same material and asked them about the relatedness of the two sentences.After that, we presented participants with a surprise memory test in which we presented only the garden path sentences and measured recognition memory.</p>
<p>We hypothesize that LLMs' and humans' relatedness values are higher in the fitting than the unfitting condition.Further, we hypothesize LLMs' memorability ratings to be higher in the fitting than the unfitting condition.Eventually, we hypothesize that LLM memorability responses would predict participants' memory performance in a surprise memory test.</p>
<p>Method</p>
<p>We report how we determined our sample size, all data exclusions (if any), all data inclusion/exclusion criteria, whether inclusion/exclusion criteria were established prior to data analysis, all manipulations, and all measures in the study.The experiment was approved by the local ethics committee of the Leibniz-Institut für Wissensmedien (LEK 2023/051).</p>
<p>Data Sources</p>
<p>LLM.We used OpenAI's API to access ChatGPT (OpenAI et al., 2023) (model: GPT-4; June 2023) to collect 100 responses (consisting of the relatedness and the memorability values) for each sentence pair.We set the temperature value to 1 to increase the variability in the answers.This resulted in 4500 independent responses in the fitting and 4500 in the unfitting condition and resulted in a total of 9000 independent responses.</p>
<p>Participants.We recruited 100 English-only speaking participants via Prolific.15 participants indicated that their vision was not normal or not corrected-to-normal during the experiment (i.e., they did not wear lenses or glasses).Thus, the resulting sample consisted of 85 participants (57 female, 27 male, 1 w/o response), mean age was M = 45.34 years (SD = 13.95)</p>
<p>Material</p>
<p>Garden-path sentences.We compiled a list of 45 garden-path sentences (e.g., "Because Bill drinks wine is never kept in the house").For each garden-path sentence, we constructed a sentence matching the context of the garden-path sentence (fitting context; e.g., "Bill has chronic alcoholism.") and a sentence not matching its context (unfitting context; e.g., "Bill likes to play golf."; for the complete list, see Supplementary Table 1).For the machine data, we used all 45 garden-path sentences; for the human data, we omitted the sentence with ID 8</p>
<p>for counter-balancing reasons.ID 8 was chosen for omission because its sentence structure closely resembles that of ID 45, making its exclusion less critical compared to omitting other sentences.</p>
<p>Prompts.We submitted zero-shot prompts to ChatGPT regarding relatedness and memorability, which we presented before both sets of sentence pairs, one with a fitting context and the other with an unfitting context sentence preceding the garden-path sentence.</p>
<p>First, we gave the prompt based on the category.Then, we provided the two sentences separately in an order as "Sentence 1" and "Sentence 2" respectively, by "Sentence 1" being the prior context sentence and "Sentence 2" the garden-path sentence (Table 1).By the relatedness prompt, we requested ChatGPT to rate the relatedness of the sentences by giving a value from 1 (not at all) to 10 (highly).Afterward, we requested a value from ChatGPT to indicate the memorability of Sentence 2 (i.e.garden-path sentence) from 1 (not at all) to 10 (excellent).</p>
<p>Human experiment.The experiment was programmed with PsychoPy (Peirce, 2022).All instructions and stimuli appeared in white on a gray background.The stimulus material consisted of sentence pairs, which included a garden-path sentence and its preceding context sentence.Each pair was arranged in a visually specific format, with each sentence starting on a new line, one below the other.</p>
<p>Procedure and Design Human Data</p>
<p>In the learning phase, participants read 22 sentence pairs comprised of a prior context sentence and a garden-path sentence.Half of the sentences shown were in the fitting condition, the other half were in the unfitting condition.Participants read pairs of sentences at their own pace and proceeded to the next pair by pressing the spacebar.The response button (i.e., spacebar) was activated three seconds after stimulus onset to ensure that the sentences were not skipped and were read by the participants.After each sentence pair, participants rated the relatedness of the two sentences by clicking on a value on the 10-point rating scale presented to them (1: "not at all" to 10: "highly").After completing the learning phase of the experiment, participants completed a surprise old/new recognition memory test, including 44 garden-path sentences (22 targets from the learning phase and 22 distractors) without their contexts.Participants indicated whether they remembered the sentence shown on screen from the learning phase by pressing the right arrow key for "yes" and the left arrow key for "no" allowing for the calculation of sensitivity (d') from signal detection theory (Green &amp; Swets, 1966).The study employed a one-factorial design with context (fitting, unfitting) as the within-subjects factor.Four counter-balancing conditions ensured that the garden-path sentences were assigned equally to the conditions (fitting vs. unfitting context, target vs.</p>
<p>distractor) across participants.The experiment lasted approximately 15 minutes.</p>
<p>Results</p>
<p>Machine data (LLM data)</p>
<p>Relatedness as a function of context.We fitted a linear mixed-effect model with context (fitting, unfitting) as fixed effect and sentence ID as random intercept.We submitted the resulting model to a type 2 ANOVA (Fox &amp; Weisberg, 2010).The relatedness of the two sentences is higher in the fitting than the unfitting condition,  2 (1) = 44843.00,p &lt; .001,constituting a successful manipulation check of the context manipulation (Figure 1A).</p>
<p>Memorability as a function of context.Similar to the relatedness analysis, we fitted a linear mixed-effect model with context (fitting, unfitting) as fixed effect and sentence ID as the random intercept.Submitting the resulting model to a type 2 ANOVA showed a significant main effect of context,  2 (1) = 2660.00,p &lt; .001.In the fitting context condition, the memorability of the garden-path sentence was rated higher than in the unfitting context condition (Figure 1B).How do you rate the recognizability of Sentence 2 from 1 (not at all) to 10 (excellent)?In particular, we changed related with linked and memorability with recognizability.</p>
<p>Results resembled the main analysis (see also Supplementary Figure 1).In the fitting condition, the two sentences were judged to be more linked, M = 6.67 (SD = 2.25), than in the unfitting condition, M = 1.12 (SD = 0.59),  2 (1) = 43731.60,p &lt; .001.Further, the recognizability values were higher in the fitting, M = 2.67 (SD = 1.34), than in the unfitting condition, M = 1.00 (SD = 0.02),  2 (1) = 43731.60,p &lt; .001.We thus conclude that the observed effects are stable.</p>
<p>Human data (Human experiment)</p>
<p>Relatedness as a function of context.Analysis was similar to the machine data with the exception that we additionally included participant as random intercept.The relatedness of the two sentences is higher in the fitting than the unfitting condition,  2 (1) = 4087.60,p &lt; .001,again constituting a successful manipulation check and replicating the machine data (Figure 1C).</p>
<p>Recognition memory as a function of context.To assess participants' memory, we calculated d' from signal detection theory (Green &amp; Swets, 1966), which corrects for response bias.We corrected for perfect performance.In particular, in case of no false alarms, we added 0.5 (i.e., a half trial), and in case of all hits, we subtracted 0.5 (i.e., a half trial).A t-test for repeated measures showed a significantly higher recognition performance in the fitting context than in the unfitting context, t(84) = 5.75, p &lt; .001,Cohen's d = 0.62 (Figure 1D).</p>
<p>Response bias (c).</p>
<p>Participants' responses were more liberal in the fitting (M = -0.08,SD = 0.22) than in the unfitting condition (M = 0.13, SD = 0.29), t(84) = -5.78,p &lt; .001,Cohen's d = -0.63.</p>
<p>Our concluding analysis explored a potential mechanism underlying the observed effect, grounded in a probabilistic framework of context-dependent learning and stochastic reasoning.This framework posits that memory performance improves as a function of the probabilistic accumulation and integration of retrieval cues, with context acting as a latent variable that organizes and constrains memory processes (Heald et al., 2023).More precisely, in the fitting condition, where a significant, uniform segment of information spans two sentences, memory performance likely peaks.Thus, relatedness and memory performance should be less related as compared to the unfitting condition, where the relatedness of the two sentences is lower and potentially more heterogeneous.In the latter condition, there is more room for improvement in memory performance.Consequently, in this condition, we expect a significant positive relation, with increases in relatedness of the two sentences leading to noticeable improvements in memory performance.If this is true, we should observe a significant interaction between context (fitting vs. unfitting) and the degree of relatedness, with higher relatedness values, predicting higher memory performance in the unfitting condition but not necessarily in the fitting condition.</p>
<p>To test this framework on the machine data, we fitted a linear mixed effect model with memorability as the dependent variable, context (fitting, unfitting) as a categorical fixed effect, relatedness as a continuous fixed effect, and sentence id as a random intercept (Figure 2A).Submitting this model to a type-2 ANOVA showed a significant interaction of context and relatedness,  2 (1) = 86.37,p &lt; .001.As predicted, in the unfitting condition, higher relatedness values predict higher memory performance.This effect is weaker in the fitting condition.Further, the main effects of condition,  2 (1) = 259.51,p &lt; .001,and relatedness,  2 (1) = 31.23,p &lt; .001,were significant.For the analysis of the human data, we aggregated the proportion correct data1 and the relatedness data at the sentence and context level.We fitted a linear mixed effect model with proportion correct as the dependent variable, context (fitting, unfitting) as a categorical fixed effect, relatedness as a continuous fixed effect, and sentence id as a random intercept (Figure 2B).Submitting this model to a type 2 ANOVA showed a significant interaction of context and relatedness,  2 (1) = 9.39, p = .002.As predicted, in the unfitting condition, higher relatedness values predict higher memory performance, whereas we did not observe such an effect in the fitting condition.Further, the main effects of condition,  2 (1) = 1.13, p = .289,and relatedness,  2 (1) = 1.39, p = .239,were not significant.</p>
<p>Consistent with the proposed stochastic reasoning framework, a significant interaction between context and relatedness emerged for machine and human data.While higher relatedness boosted memory in the unfitting condition, this effect was weaker in the fitting condition.These findings across machine and human data emphasize how the link between relatedness and memory hinges on context and available retrieval cues.</p>
<p>Discussion</p>
<p>We investigated whether generative AI, although lacking a foundation in human cognition, can predict human cognitive performance based on language-based memory tasks.</p>
<p>The results showed that the relatedness ratings of ChatGPT closely corresponded with those of the human participants and that the memorability ratings of ChatGPT indeed predicted the memory performance of humans in the surprise memory test.An analysis to check the robustness of the findings with synonyms (recognizable and linked for memorable and related) confirmed the results.</p>
<p>The findings from this study provide strong evidence that LLMs, such as ChatGPT, have significant potential to be utilized as experimental tools for the investigation of human cognition in the context of machine psychology.ChatGPT's ability in predicting human memory performance even in the absence of a human-like memory system highlights its potential to represent cognitive patterns observed in humans.Particularly, the significant alignment between the relatedness ratings of human subjects and those of ChatGPT underscores the model's competence in processing and analyzing the linguistic information in a manner similar to humans.Moreover, the accuracy of the model in predicting the performance of humans on the memory test also highlights the potential of LLMs as proxies for investigating the human memory processes.The results also indicate that, even though LLMs are built on a fundamentally different architecture, they display outcomes that parallel human cognitive processes, particularly in tasks that involve the understanding of contextual reasoning.Corroborated through analyses using synonyms, the robustness of our findings strengthens the soundness of this approach and demonstrates the great possibility of LLMs functioning as effective tools in psychological research.</p>
<p>The outcomes of this study emphasize the bidirectional benefits of machine psychology: not only LLMs benefit from psychological approaches for deeper evaluation, but researchers also gain a new perspective to examine the complexities of human cognition.We described one potential mechanism based on the interaction of context and relatedness observed in both the machine and human data, which can be effectively explained through a stochastic reasoning framework grounded in the principles of context-dependent learning and probabilistic cue integration (Heald et al., 2023).This framework posits that memory performance is influenced by the probabilistic accumulation of retrieval cues during encoding and retrieval processes, as context serves as a latent variable that organizes and constrains memory representations over time.In the fitting context condition, in which sentences share high semantic relatedness, the abundance of overlapping cues enhances encoding strength and facilitates retrieval, resulting in consistently high memory performance (Polyn et al., 2009).Consequently, variations in relatedness within this condition have a minimal impact because the retrieval cues are already robust.Conversely, in the unfitting context condition, which begins with lower relatedness and fewer available cues, any increase in relatedness significantly boosts the availability of retrieval cues, leading to noticeable improvements in memory performance (Jonker et al., 2013).This aligns with the encoding specificity principle, which posits that memory retrieval is most effective when the context at encoding matches the context at retrieval (Tulving &amp; Thomson, 1973).By modeling how variations in contextual relatedness influence the stochastic processing of retrieval cues, this framework provides a theoretical basis for understanding how both humans and LLMs process linguistic structures (Bhatia &amp; Richie, 2024;Binz &amp; Schulz, 2023b).Heald et al. (2023) further highlight that the probabilistic nature of context inference, reliant on dynamically evolving distributions of context-specific cues, contributes significantly to the robustness and adaptability of memory systems.This broader perspective enriches our understanding of how relatedness and contextual variability interact to influence memory processes in both humans and machine learning models.These results provide a foundation for further study of LLMs as proxies and experimental tools in psychology, offering novel possibilities to discover and represent human cognition and behavior.</p>
<p>Limitations</p>
<p>Despite our thorough approach to conducting this research, there are a few limitations that should be considered.First, there can be a potential interplay between fluency and context, particularly in terms of relatedness (Oppenheimer, 2008).Participants might have processed sentences in the fitting condition easier, increasing the familiarity for those items, therefore making the fluency heuristic a potential contributor to the memory process during the recognition test as the relatedness of sentences increases.</p>
<p>Second, LLMs are trained based on extensive datasets, which most probably also include the garden-path sentences we used in our study, and this could be a factor influencing their behavior (performativity problem) (Horton, 2023).When creating the stimuli, we deliberately chose not to create them entirely independently.Instead, we compiled a diverse set of garden-path sentences with various types of linguistic ambiguities from a wide range of sources within the literature (see Supplementary Table 1).This approach allowed us to expose both humans and GPT to these sentences in a manner that reflects the way LLMs encounter and process information-through diverse and heterogeneous datasets created by a variety of contributors.By gathering input from multiple perspectives, we sought to enhance the representativeness of our stimuli, thereby establishing a realistic and comprehensive foundation for our investigation.</p>
<p>The present findings were derived employing GPT-4 (OpenAI et al., 2023).Although this model was released in 2023, we are confident that, given the robust alignment between human cognitive performance and the model's predictions, the newer models perform at a similar level.</p>
<p>Conclusions</p>
<p>LLMs are at the forefront of research in the area we introduce as machine psychology, owing to their remarkable language processing capabilities.Our research revealed ChatGPT's ability to make accurate predictions for the performance of human memory despite not possessing it.It carries important potential for utilizing LLMs in studying human cognition.</p>
<p>performed the data analyses.All authors drafted the manuscript.All authors approved the final version of the manuscript for submission.</p>
<p>Supporting Information Text</p>
<p>Table S1: List of the used garden-path, fitting context, and unfitting context sentences.The plans of the executives were suggested to the employees for voting.</p>
<p>ID</p>
<p>For tomorrow we plan to go to the beach.</p>
<p>The management plans to cut vacation days are rejected.</p>
<p>Figure 1 :
1
Figure 1: Relatedness (A) and memorability (B) measures of the machine data and</p>
<p>Figure 2 :
2
Figure 2: Memory performance as a function of condition (fitting, unfitting) and</p>
<p>Jacqueline"</p>
<p>Fig. S1: Robustness checks with the following prompts: How closely are the two sentences linked from 1 (not at all) to 10 (highly)?and How do you rate the recognizability of Sentence 2 from 1 (not at all) to 10 (excellent)?</p>
<p>Table 1 .
1
An exemplary representation of the prompts and sentence pairs submitted to ChatGPT.</p>
<p>Note that the nature of this analysis -linking relatedness and memory performance datarequired an analysis on the item level. Thus, we used the proportion correct data and not sensitivity (i.e. d').
AcknowledgmentThis study was part of EU's Erasmus scholarship.Data Availability StatementData and analysis scripts (for the statistical programming language R) have been made publicly available via Zenodo and can be accessed at https://doi.org/10.5281/zenodo.10696562.Author ContributionsMH and EU developed the study concept and design.EU designed the stimuli and programmed the human experiment.MH performed data collection with ChatGPT andID Fitting Context Unfitting ContextGarden-Path Sentence Reference 8 Tom was doing his chores in the kitchen.She was feeling very tired today.While Tom was washing the dishes fell on the floor.DCODRWordsmithing tools. (n.d.).https://wordsmithingtools.com/ list-of-garden-path-sentences 9The men of the village go deer hunting on weekends.The new videos of an UFO are all over social media.When the men hunt the birds typically scatter.His favorite goldfish died.After the Martians invaded the town was evacuated.Ferreira, F., &amp; Henderson, J. M.(1991).Recovery from misanalyses of garden-path sentences.Journal of Memory and Language, 30 (6), 725-745.12The boy was itching all over.He said he hated his new haircut.While the boy scratched the dog yawned loudly.Ferreira, F., &amp; Henderson, J. M.(1991).Recovery from misanalyses of garden-path sentences.Journal of Memory and Language, 30 (6), 725-745.13The concert of the orchestra was great at the beginning.France is the 13th country I have been to.After the musician played the piano was wheeled off of the stage.Goldstein, E. B. (2014). Cognitive psychology:Connecting mind, research and everyday experience.Cengage Learning.14The man was in the forest expecting to hunt a bird.He bought the painting for 45 million Euros.While the man hunted the deer ran into the woods.Christianson, K., Hollingworth, A., Halliwell, J. F.,&amp; Ferreira, F. (2001).Thematic roles assigned along the garden path linger.Cognitive psychology, 42 (4), 368-407.15The lawyer wanted to take a look at the proofs for the homicide.I became a yoga instructor after 7 years of work.The evidence examined by the lawyer turned out to be unreliable.
Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies. G V Aher, R I Arriaga, A T Kalai, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023</p>
<p>Transformer networks of human conceptual knowledge. S Bhatia, R Richie, 10.1037/rev0000319Psychological Review. 13112024</p>
<p>Turning large language models into cognitive models. M Binz, E Schulz, arXiv:2306.039172023a</p>
<p>Using cognitive psychology to understand GPT-3. M Binz, E Schulz, 10.1073/pnas.2218523120Proceedings of the National Academy of Sciences. 1206e22185231202023b</p>
<p>Construction or updating? Event model processes during visual narrative comprehension. I R Brich, F Papenmeier, M Huff, M Merkt, 10.3758/s13423-023-02424-wPsychonomic Bulletin &amp; Review. 3152024</p>
<p>Have we built machines that think like people?. L M S Buschoff, E Akata, M Bethge, E Schulz, arXiv:2311.160932023</p>
<p>Using large language models in psychology. D Demszky, D Yang, D S Yeager, C J Bryan, M Clapper, S Chandhok, J C Eichstaedt, C Hecht, J Jamieson, M Johnson, M Jones, D Krettek-Cobb, L Lai, N Jonesmitchell, D C Ong, C S Dweck, J J Gross, J W Pennebaker, 10.1038/s44159-023-00241-5Nature Reviews Psychology. 2023</p>
<p>Mind meets machine: Unravelling GPT-4's cognitive psychology. S Dhingra, M Singh, S B , V Malviya, N Gill, S S , 10.1016/j.tbench.2023.100139BenchCouncil Transactions on Benchmarks, Standards and Evaluations. 331001392023</p>
<p>Overlap in meaning is a stronger predictor of semantic activation in GPT-3 than in humans. J Digutsch, M Kosinski, 10.1038/s41598-023-32248-6Scientific Reports. 1312023</p>
<p>Can AI language models replace human participants?. D Dillion, N Tandon, Y Gu, K Gray, 10.1016/j.tics.2023.04.008Trends in Cognitive Sciences. 2772023</p>
<p>F Ferreira, K Christianson, A Hollingworth, 10.1023/A:1005290706460Misinterpretations of Garden-Path Sentences: Implications for Models of Sentence Processing and Reanalysis. 200130</p>
<p>An R companion to applied regression. J Fox, S Weisberg, 2010Sage</p>
<p>On the parsing of garden-path sentences. Language. H Fujita, 10.1080/23273798.2021.1922727Cognition and Neuroscience. 36102021</p>
<p>Two decades of structure building. M A Gernsbacher, 10.1080/01638539709544994Discourse Processes. 199723</p>
<p>Reading Skill and Suppression Revisited. M A Gernsbacher, R R W Robertson, 10.1111/j.1467-9280.1995.tb00326.xPsychological Science. 631995</p>
<p>ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks. F Gilardi, M Alizadeh, M Kubli, 10.1073/pnas.2305016120Proceedings of the National Academy of Sciences. 120302023</p>
<p>Signal detection theory and psychophysics. D M Green, J A Swets, 1966Wiley</p>
<p>The influence of contextual contrast on syntactic processing: Evidence for strong-interaction in sentence comprehension. D Grodner, E Gibson, D Watson, 10.1016/j.cognition.2004.01.007Cognition. 9532005</p>
<p>The computational and neural bases of context-dependent learning. J B Heald, D M Wolpert, M Lengyel, 10.1146/annurev-neuro-092322-100402Annual Review of Neuroscience. 4612023</p>
<p>Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus?. J J Horton, 10.48550/arXiv.2301.07543arXiv:2301.075432023a</p>
<p>BERT Shows Garden Path Effects. T Irwin, K Wilson, A Marantz, 10.18653/v1/2023.eacl-main.235Proceedings of the 17th Conference of the European Chapter. A Vlachos, I Augenstein, the 17th Conference of the European ChapterAssociation for Computational Linguistics2023</p>
<p>Putting retrieval-induced forgetting in context: An inhibition-free, context-based account. T R Jonker, P Seli, C M Macleod, 10.1037/a0034246Psychological Review. 12042013</p>
<p>The role of discourse context in the processing of a flexible word-order language. E Kaiser, J Trueswell, 10.1016/j.cognition.2004.01.002Cognition. 9422004</p>
<p>ChatGPT for good? On opportunities and challenges of large language models for education. E Kasneci, K Sessler, S Küchemann, M Bannert, D Dementieva, F Fischer, U Gasser, G Groh, S Günnemann, E Hüllermeier, S Krusche, G Kutyniok, T Michaeli, C Nerdel, J Pfeffer, O Poquet, M Sailer, A Schmidt, T Seidel, G Kasneci, 10.1016/j.lindif.2023.102274Learning and Individual Differences. 1031022742023</p>
<p>Modal and amodal cognition: An overarching principle in various domains of psychology. B Kaup, R Ulrich, K M Bausenhart, D Bryce, M V Butz, D Dignath, C Dudschig, V H Franz, C Friedrich, C Gawrilow, J Heller, M Huff, M Hütter, M Janczyk, H Leuthold, H Mallot, H.-C Nürk, M Ramscar, N Said, H Y Wong, 10.1007/s00426-023-01878-wPsychological Research. 8822024</p>
<p>Language models and linguistic theories beyond words. W Kintsch, D Welsch, F Schmalhofer, S Zimny, 10.1016/0749-596X(90)90069-CJournal of Memory and Language. 2921990. 2023Nature Machine Intelligence. Article 7</p>
<p>Incremental Comprehension of Garden-Path Sentences by Large Language Models: Semantic Interpretation, Syntactic Re-Analysis, and Attention (Version 1). A Li, X Feng, S Narang, A Peng, T Cai, R S Shah, S Varma, 10.48550/ARXIV.2405.160422024</p>
<p>We're Afraid Language Models Aren't Modeling Ambiguity. A Liu, Z Wu, J Michael, A Suhr, P West, A Koller, S Swayamdipta, N Smith, Y Choi, 10.18653/v1/2023.emnlp-main.51Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. H Bouamor, J Pino, &amp; K Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2023</p>
<p>The Neural Time Course of Semantic Ambiguity Resolution in Speech Comprehension. L J Macgregor, J M Rodd, R A Gilbert, O Hauk, E Sohoglu, M H Davis, 10.1162/jocn_a_01493Journal of Cognitive Neuroscience. 3232020</p>
<p>Emergent linguistic structure in artificial neural networks trained by self-supervision. C D Manning, K Clark, J Hewitt, U Khandelwal, O Levy, 10.1073/pnas.1907367117Proceedings of the National Academy of Sciences. 117482020</p>
<p>A Turing test of whether AI chatbots are behaviorally similar to humans. Q Mei, Y Xie, W Yuan, M O Jackson, 10.1073/pnas.2313925121Proceedings of the National Academy of Sciences. 12192024</p>
<p>Evidence That Event Boundaries Are Access Points for Memory Retrieval. S Michelmann, U Hasson, K A Norman, 10.1177/09567976221128206Psychological Science. 095679762211282062023</p>
<p>A Comprehensive Overview of Large Language Models. H Naveed, A U Khan, S Qiu, M Saqib, S Anwar, M Usman, N Akhtar, N Barnes, A Mian, 10.48550/ARXIV.2307.064352023</p>
<p>. Achiam Openai, J Adler, S Agarwal, S Ahmad, L Akkaya, I Aleman, F L Almeida, D Altenschmidt, J Altman, S Anadkat, S Avila, R Babuschkin, I Balaji, S Balcom, V Baltescu, P Bao, H Bavarian, M Belgum, J Zoph, B , 10.48550/arXiv.2303.08774arXiv:2303.087742023GPT-4 Technical Report</p>
<p>The secret life of fluency. D M Oppenheimer, 10.1016/j.tics.2008.02.014Trends in Cognitive Sciences. 1262008</p>
<p>A context maintenance and retrieval model of organizational processes in free recall. S M Polyn, K A Norman, M J Kahana, 10.1037/a0014420Psychological Review. 11612009</p>
<p>Why is the meaning of a sentence better remembered than its form? An fMRI study on the role of novelty-encoding processes. J Poppenk, G Walia, A R Mcintosh, M F Joanisse, D Klein, S Köhler, 10.1002/hipo.20453Hippocampus. 1892008</p>
<p>Cognitive Psychology for Deep Neural Networks: A Shape Bias Case Study. S Ritter, D G T Barrett, A Santoro, M M Botvinick, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning2017</p>
<p>Construction and interference in learning from multiple representation. W Schnotz, M Bannert, S0959-4752(02)00017-8Learning and Instruction. 1322003</p>
<p>Sentence-based mental simulations: Evidence from behavioral experiments using garden-path sentences. E Schütt, C Dudschig, B K Bergen, B Kaup, 10.3758/s13421-022-01367-2Memory &amp; Cognition. 5142023</p>
<p>Should artificial intelligence be interpretable to humans?. M D Schwartz, 10.1038/s42254-022-00538-zNature Reviews Physics. 4122022</p>
<p>Cognitive ease at a cost: LLMs reduce mental effort but compromise depth in student scientific inquiry. M Stadler, M Bannert, M Sailer, 10.1016/j.chb.2024.108386Computers in Human Behavior. 1601083862024</p>
<p>Context-based facilitation of semantic access follows both logarithmic and linear functions of stimulus probability. J M Szewczyk, K D Federmeier, 10.1016/j.jml.2021.104311Journal of Memory and Language. 1231043112022</p>
<p>A N Tak, J Gratch, 10.1109/ACII59096.2023.10388119Is GPT a Computational Model of Emotion? 2023 11th International Conference on Affective Computing and Intelligent Interaction (ACII). 2023</p>
<p>Large language models in medicine. A J Thirunavukarasu, D S J Ting, K Elangovan, L Gutierrez, T F Tan, D S W Ting, 10.1038/s41591-023-02448-8Nature Medicine. 2982023</p>
<p>Encoding specificity and retrieval processes in episodic memory. E Tulving, D M Thomson, 10.1037/h0020071Psychological Review. 8051973</p>
<p>Who is Willing to Help Robots? A User Study on Collaboration Attitude. A Vanzo, F Riccio, M Sharf, V Mirabella, T Catarci, D Nardi, 10.1007/s12369-019-00571-6International Journal of Social Robotics. 1222020</p>
<p>Emergent analogical reasoning in large language models. T Webb, K J Holyoak, H Lu, 10.1038/s41562-023-01659-wNature Human Behaviour. 792023</p>
<p>Natural language processing in the era of large language models. A Zubiaga, 10.3389/frai.2023.1350306Frontiers in Artificial Intelligence. 613503062024</p>            </div>
        </div>

    </div>
</body>
</html>
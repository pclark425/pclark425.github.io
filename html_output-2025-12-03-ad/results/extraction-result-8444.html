<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8444 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8444</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8444</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-150.html">extraction-schema-150</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <p><strong>Paper ID:</strong> paper-90df9c970aa98aa49bcf79e252ab26b1bb6889c5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/90df9c970aa98aa49bcf79e252ab26b1bb6889c5" target="_blank">Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance</a></p>
                <p><strong>Paper Venue:</strong> Conference on Robot Learning</p>
                <p><strong>Paper TL;DR:</strong> It is demonstrated through experiments in realistic household environments that agents trained with the LLM-guidedbootstrapping procedure outperform those trained with naive bootstrapping as well as prior unsupervised skill acquisition methods on zero-shot execution of unseen, long-horizon tasks in new environments.</p>
                <p><strong>Paper Abstract:</strong> We propose BOSS, an approach that automatically learns to solve new long-horizon, complex, and meaningful tasks by growing a learned skill library with minimal supervision. Prior work in reinforcement learning require expert supervision, in the form of demonstrations or rich reward functions, to learn long-horizon tasks. Instead, our approach BOSS (BOotStrapping your own Skills) learns to accomplish new tasks by performing"skill bootstrapping,"where an agent with a set of primitive skills interacts with the environment to practice new skills without receiving reward feedback for tasks outside of the initial skill set. This bootstrapping phase is guided by large language models (LLMs) that inform the agent of meaningful skills to chain together. Through this process, BOSS builds a wide range of complex and useful behaviors from a basic set of primitive skills. We demonstrate through experiments in realistic household environments that agents trained with our LLM-guided bootstrapping procedure outperform those trained with naive bootstrapping as well as prior unsupervised skill acquisition methods on zero-shot execution of unseen, long-horizon tasks in new environments. Website at clvrai.com/boss.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8444.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8444.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BOSS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BOotstrapping your own SkillS (BOSS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that uses a language-conditioned primitive skill policy plus LLM guidance to bootstrap new long-horizon skills by practicing skill chains in the environment and adding LLM-summarized composite skills to a persistent skill library and replay buffer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BOSS (LLM-guided skill-bootstrapping agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Closed-loop language-conditioned policy pre-trained on primitive skill demonstrations (IQL) that performs online bootstrapping: samples initial skills by value, queries an LLM for next-skill proposals conditioned on executed skill chain and skill library, executes sampled skills, relabels and stores trajectories, and fine-tunes on replayed data.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-13B (primary), OPT-1 (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA-13B used for LLM guidance and summarization in the main experiments (13B parameters, open-source). OPT-1 (1B) used as a weaker-LLM ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ALFRED long-horizon household tasks (and real-robot kitchen tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Image-based long-horizon manipulation tasks requiring chaining of primitive skills (2–8 primitive skills per task in ALFRED); in real robot: stylized kitchen tabletop tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>long-horizon control / multi-step instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working (prompt/context) + persistent skill library + replay buffer (episodic/experience memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Prompt concatenation of executed skill chain and entire skill library to the LLM for next-skill proposal (short-term working memory); LLM-generated composite-skill summaries added to an explicit skill library Z (long-term memory); trajectories stored in replay buffer and relabeled (experience memory) used for policy fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Textual executed skill chain (sequence of primitive skill annotations), LLM-generated composite skill summaries (natural language labels), and stored trajectories (s, a, s', r) with both primitive and composite annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Prompt concatenation of current skill chain + skills list for immediate retrieval; LLM generation sampled N times with token likelihoods, then generation strings mapped to nearest existing skill via sentence-embedding nearest-match; value-function-based sampling for initial skill.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Average oracle-normalized return 0.57 ± 0.06 across ALFRED evaluation tasks; oracle-normalized success rate 0.57 ± 0.14 (Table 1). Achieves non-zero success across task lengths and substantially outperforms comparable baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Ablations: BOSS-OPT1 (weaker 1B LLM) and BOSS-Rand (no LLM guidance, next skill sampled uniformly). BOSS (LLaMA-13B) outperforms BOSS-OPT1 (avg return 0.57±0.06 vs 0.49±0.07) and BOSS-Rand (0.45±0.06). Comparisons vs baselines (CIC, SayCan variants, Oracle, No-Bootstrap) show BOSS has highest success on long-horizon tasks; SayCan variants suffer execution failures without environment practice.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM guidance used as prompt-contextual next-skill proposer plus iterative environment practice (closed-loop fine-tuning + replay buffer) enables learning meaningful long-horizon skills; stronger LLMs give better guidance (1B → 13B improves final performance); LLM-guided bootstrapping yields higher real-world and sim success than top-down LLM planning without practice.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires success detection for primitive skills and environment resets (human resets in real-robot experiments); greedy one-step skill proposals may be suboptimal for very long horizons; relies on quality of LLM proposals and verb-aware mapping to skill library.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8444.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8444.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BOSS-Rand</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BOSS (Random next-skill selection ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation of BOSS where no LLM is used: next skills are sampled uniformly (random) from the current skill library during bootstrapping.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BOSS-Rand</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same framework as BOSS but the SAMPLENEXTSKILL step samples the next skill uniformly at random from the skill library rather than using an LLM proposal distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (no LLM guidance)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>No external LLM for next-skill proposals; uses only the pre-trained language-conditioned skill policy and replay buffer.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ALFRED long-horizon household tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same evaluation set as BOSS (image-based, multi-step tasks requiring chaining primitive skills).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>long-horizon control / multi-step instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>replay buffer / skill library still used but no LLM working memory for proposals</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Stores collected trajectories and adds composite skills to library, but next-skill selection uses no LLM/prompt-context memory (random selection).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Stored trajectories and composite skill labels added to library during bootstrapping (same as BOSS), but proposal mechanism does not exploit prompt-context memory.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>None for next-skill proposal (random sampling); replay buffer sampled for training as in BOSS.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Average oracle-normalized return 0.45 ± 0.06 (Table 3) with per-length: Length2 0.32 ± 0.03, Length3 0.29 ± 0.11, Length4 0.61 ± 0.16.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared directly in ablation Table 3 to BOSS and BOSS-OPT1: although BOSS-Rand learns a larger skill library faster (Figure 6), its learned skills are less meaningful and performance is worse than LLM-guided BOSS. This shows LLM guidance (memory in prompt) improves quality of practiced skills.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Random chaining learns many but mostly meaningless/repetitive skills; absence of LLM guidance reduces performance, especially on longer tasks — LLM guidance is important for discovering meaningful skill chains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Learns many spurious/repetitive skills; lower downstream task performance despite faster library growth.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8444.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8444.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BOSS-OPT1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BOSS (OPT-1 ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation of BOSS that replaces the LLaMA-13B guidance LLM with a weaker 1B-parameter OPT model to test sensitivity to LLM quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Opt: Open pre-trained transformer language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BOSS-OPT1</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same BOSS pipeline but uses the 1B-parameter OPT LLM for next-skill proposals and summarization instead of LLaMA-13B.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OPT-1 (1B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OPT family model with 1 billion parameters, used as lower-capacity LLM ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ALFRED long-horizon household tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same ALFRED evaluation of multi-step tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>long-horizon control / multi-step instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working (prompt/context) + persistent skill library + replay buffer</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Same prompt concatenation and skill-library replay mechanics as BOSS but with OPT-1 providing proposals and summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Executed skill chains, LLM-generated composite summaries (from OPT-1), and replay-buffered trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Prompt concatenation and embedding-based mapping from OPT-1 generations to skills (same pipeline as main BOSS).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Average oracle-normalized return 0.49 ± 0.07; per-length: Length2 0.39 ± 0.08, Length3 0.36 ± 0.07, Length4 0.56 ± 0.08 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared to BOSS (LLaMA-13B) and BOSS-Rand: using a weaker LLM degrades performance but still outperforms no-LLM random chaining in some respects. Indicates LLM capability correlates with bootstrapping quality.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM capability matters: stronger LLM (13B) yields better-guided practice and higher task performance than a 1B LLM; however weaker LLM still provides useful guidance vs random.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Weaker LLMs produce lower-quality proposals, reducing bootstrapping effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8444.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8444.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SayCan</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SayCan (LLM-based affordance-guided planner)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A top-down method that uses a pre-trained LLM to decompose a high-level task into step-by-step primitive skills by ranking a given skill library according to language and affordance priors; used as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Do as i can and not as i say: Grounding language in robotic affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SayCan / SayCan+P / SayCan+PF</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>SayCan: LLM ranks skills given a task using affordance grounding; SayCan+P: variant that uses the paper's LLM proposal mechanism (prompting for step-by-step plans) instead of SayCan's original ranking; SayCan+PF: SayCan+P with policy fine-tuning in target environment (single-skill fine-tuning) for same number of steps as BOSS.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-13B (used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same LLaMA-13B used for planning; approach is top-down planning without environment practice in original form.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ALFRED long-horizon household tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same evaluation tasks requiring chaining of primitive skills; SayCan plans are executed by pre-trained primitive skill policies without or with limited fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>long-horizon planning / multi-step instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working (prompt/context) / in-context examples</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>LLM prompt contains in-context examples and the skill library; LLM outputs step-by-step plan (open-loop) which is then executed by low-level skill policies. No iterative environment practice to update the planner.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Skill library and in-context demonstration examples in the prompt; generated step-by-step plan text.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Prompt concatenation (in-context learning) and LLM ranking/proposal; no retrieval-augmented external memory or replay.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>SayCan: average return 0.06 ± 0.00 (from Table 1: Length2 0.06 ± 0.02, Length3 0.14 ± 0.00, Length4 0.10 ± 0.12). SayCan+P: avg return 0.12 ± 0.01. SayCan+PF (fine-tuned single-skill): avg return 0.57 ± 0.05 but reported task success rate 0.00 ± 0.00 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>SayCan+P (using BOSS's proposal prompt) performs better than original SayCan; fine-tuning single skills (SayCan+PF) improves subtask returns but not overall success (0% success) due to execution failures when chaining; analysis indicates ~95% of unsuccessful SayCan(+P/PF) trajectories are caused by policy execution failures rather than planning errors.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Top-down LLM planning (SayCan family) can propose plausible plans but is brittle when low-level policies fail in new environments; without practice/closed-loop finetuning on skill chaining, planners suffer high execution failure rates even if intermediate subtask returns increase.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No mechanism for environment practice or iterative correction; high fraction of failures come from policy execution errors in unseen environments; even fine-tuning single skills does not yield nonzero end-to-end success without chaining practice.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8444.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8444.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProgPrompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ProgPrompt (LLM-to-code planner for robotics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based planner that generates code-like plans for robot tasks which are then translated to primitive skill commands for execution; used as a baseline in real-robot experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ProgPrompt: Generating situated robot task plans using large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ProgPrompt</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Converts natural language tasks into code-like plans (function-like commands) via an LLM, then maps generated code actions into sequences of primitive language instructions which are executed by the pre-trained policy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-13B (used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA-13B used with prompt engineering to produce code-like robot plans.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Real-robot kitchen tabletop manipulation tasks (length 2 and 4)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tabletop manipulation tasks executed on a Kinova Jaco 2 arm; some tasks require chaining multiple primitive skills.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>real-robot multi-step manipulation / multi-step instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working (prompt/context) / in-context examples</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>LLM prompt contains code-like examples converted from natural language demonstrations; generation uses prompt-context information to produce programmatic plans.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>In-context examples turned into code-style plan representations; generated code commands translated to primitive skill language instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Prompt concatenation / in-context learning; no external persistent memory or replay used for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>In real-robot evaluation both ProgPrompt and BOSS performed similarly on length-2 tasks, but only BOSS achieved non-zero success on more difficult length-4 tasks after bootstrapping, indicating that closed-loop bootstrapping (practice + memory in replay/skill library) improves generalization on harder real-world chains.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM-to-code planning can produce plausible plans, but without environment practice / closed-loop adaptation its performance perishes on longer-horizon tasks compared to BOSS.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relies on correct translation from code outputs to executable skill instructions and on pre-trained skills executing reliably in new environment; lacks closed-loop environment practice for chaining robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8444.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8444.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA: Open and efficient foundation language models (13B variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13-billion parameter open-source language model used in this work to provide next-skill proposals and to summarize composite skills for BOSS and baseline planners.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llama: Open and efficient foundation language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLaMA-13B (LLM used for proposal and summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Used to generate distributions over next skills given executed skill chain and skill library (prompted), and to summarize concatenated primitive skill annotations into highlevel composite skill labels to expand the skill library.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>13-billion parameter open-source foundation LLM (Touvron et al.), used with 8-bit inference in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LLM-guided next-skill proposal and summarization inside BOSS; planning baselines (SayCan, ProgPrompt)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate next-step skill proposals conditioned on skill chain + skill list; summarize sequences of primitive skill annotations into compact composite natural language labels.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>planning / summarization (LLM-in-the-loop)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working / in-context (prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Receives prompt with up to: (a) full skill library, (b) executed skill chain (skill history), and (c) in-context examples; outputs next-skill candidates or summary text.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Prompt tokens containing skill library, executed skills, and example demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Prompt concatenation; output candidates sampled N times with averaged token likelihoods used as sampling weights; outputs then mapped via sentence embeddings to library skills.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>As part of BOSS (main LLM), enabled BOSS to reach avg return 0.57 ± 0.06 and success 0.57 ± 0.14 on ALFRED evaluation; replacing it with OPT-1 (weaker LLM) reduced performance (see ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared directly in ablation (BOSS vs BOSS-OPT1): LLaMA-13B guidance outperforms OPT-1 (13B > 1B). Using no LLM (BOSS-Rand) performs worse despite faster library growth.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>High-capacity LLMs provide more accurate next-skill proposals and better composite summaries, improving bootstrapping quality and downstream long-horizon task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>LLM outputs must be mapped to existing skill annotations; naive sentence embeddings emphasized nouns over verbs, requiring a verb-prefixing heuristic to improve mapping. LLM quality limits bootstrapping effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Do as i can and not as i say: Grounding language in robotic affordances. <em>(Rating: 2)</em></li>
                <li>ProgPrompt: Generating situated robot task plans using large language models. <em>(Rating: 2)</em></li>
                <li>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. <em>(Rating: 2)</em></li>
                <li>Inner monologue: Embodied reasoning through planning with language models. <em>(Rating: 2)</em></li>
                <li>Guiding pretraining in reinforcement learning with large language models. <em>(Rating: 1)</em></li>
                <li>Augmenting autotelic agents with large language models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8444",
    "paper_id": "paper-90df9c970aa98aa49bcf79e252ab26b1bb6889c5",
    "extraction_schema_id": "extraction-schema-150",
    "extracted_data": [
        {
            "name_short": "BOSS",
            "name_full": "BOotstrapping your own SkillS (BOSS)",
            "brief_description": "A framework that uses a language-conditioned primitive skill policy plus LLM guidance to bootstrap new long-horizon skills by practicing skill chains in the environment and adding LLM-summarized composite skills to a persistent skill library and replay buffer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "BOSS (LLM-guided skill-bootstrapping agent)",
            "agent_description": "Closed-loop language-conditioned policy pre-trained on primitive skill demonstrations (IQL) that performs online bootstrapping: samples initial skills by value, queries an LLM for next-skill proposals conditioned on executed skill chain and skill library, executes sampled skills, relabels and stores trajectories, and fine-tunes on replayed data.",
            "model_name": "LLaMA-13B (primary), OPT-1 (ablation)",
            "model_description": "LLaMA-13B used for LLM guidance and summarization in the main experiments (13B parameters, open-source). OPT-1 (1B) used as a weaker-LLM ablation.",
            "task_name": "ALFRED long-horizon household tasks (and real-robot kitchen tasks)",
            "task_description": "Image-based long-horizon manipulation tasks requiring chaining of primitive skills (2–8 primitive skills per task in ALFRED); in real robot: stylized kitchen tabletop tasks.",
            "task_type": "long-horizon control / multi-step instruction following",
            "memory_used": true,
            "memory_type": "working (prompt/context) + persistent skill library + replay buffer (episodic/experience memory)",
            "memory_mechanism": "Prompt concatenation of executed skill chain and entire skill library to the LLM for next-skill proposal (short-term working memory); LLM-generated composite-skill summaries added to an explicit skill library Z (long-term memory); trajectories stored in replay buffer and relabeled (experience memory) used for policy fine-tuning.",
            "memory_representation": "Textual executed skill chain (sequence of primitive skill annotations), LLM-generated composite skill summaries (natural language labels), and stored trajectories (s, a, s', r) with both primitive and composite annotations.",
            "memory_retrieval_method": "Prompt concatenation of current skill chain + skills list for immediate retrieval; LLM generation sampled N times with token likelihoods, then generation strings mapped to nearest existing skill via sentence-embedding nearest-match; value-function-based sampling for initial skill.",
            "performance_with_memory": "Average oracle-normalized return 0.57 ± 0.06 across ALFRED evaluation tasks; oracle-normalized success rate 0.57 ± 0.14 (Table 1). Achieves non-zero success across task lengths and substantially outperforms comparable baselines.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Ablations: BOSS-OPT1 (weaker 1B LLM) and BOSS-Rand (no LLM guidance, next skill sampled uniformly). BOSS (LLaMA-13B) outperforms BOSS-OPT1 (avg return 0.57±0.06 vs 0.49±0.07) and BOSS-Rand (0.45±0.06). Comparisons vs baselines (CIC, SayCan variants, Oracle, No-Bootstrap) show BOSS has highest success on long-horizon tasks; SayCan variants suffer execution failures without environment practice.",
            "key_findings": "LLM guidance used as prompt-contextual next-skill proposer plus iterative environment practice (closed-loop fine-tuning + replay buffer) enables learning meaningful long-horizon skills; stronger LLMs give better guidance (1B → 13B improves final performance); LLM-guided bootstrapping yields higher real-world and sim success than top-down LLM planning without practice.",
            "limitations_or_challenges": "Requires success detection for primitive skills and environment resets (human resets in real-robot experiments); greedy one-step skill proposals may be suboptimal for very long horizons; relies on quality of LLM proposals and verb-aware mapping to skill library.",
            "uuid": "e8444.0",
            "source_info": {
                "paper_title": "Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "BOSS-Rand",
            "name_full": "BOSS (Random next-skill selection ablation)",
            "brief_description": "An ablation of BOSS where no LLM is used: next skills are sampled uniformly (random) from the current skill library during bootstrapping.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "BOSS-Rand",
            "agent_description": "Same framework as BOSS but the SAMPLENEXTSKILL step samples the next skill uniformly at random from the skill library rather than using an LLM proposal distribution.",
            "model_name": "N/A (no LLM guidance)",
            "model_description": "No external LLM for next-skill proposals; uses only the pre-trained language-conditioned skill policy and replay buffer.",
            "task_name": "ALFRED long-horizon household tasks",
            "task_description": "Same evaluation set as BOSS (image-based, multi-step tasks requiring chaining primitive skills).",
            "task_type": "long-horizon control / multi-step instruction following",
            "memory_used": false,
            "memory_type": "replay buffer / skill library still used but no LLM working memory for proposals",
            "memory_mechanism": "Stores collected trajectories and adds composite skills to library, but next-skill selection uses no LLM/prompt-context memory (random selection).",
            "memory_representation": "Stored trajectories and composite skill labels added to library during bootstrapping (same as BOSS), but proposal mechanism does not exploit prompt-context memory.",
            "memory_retrieval_method": "None for next-skill proposal (random sampling); replay buffer sampled for training as in BOSS.",
            "performance_with_memory": "Average oracle-normalized return 0.45 ± 0.06 (Table 3) with per-length: Length2 0.32 ± 0.03, Length3 0.29 ± 0.11, Length4 0.61 ± 0.16.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Compared directly in ablation Table 3 to BOSS and BOSS-OPT1: although BOSS-Rand learns a larger skill library faster (Figure 6), its learned skills are less meaningful and performance is worse than LLM-guided BOSS. This shows LLM guidance (memory in prompt) improves quality of practiced skills.",
            "key_findings": "Random chaining learns many but mostly meaningless/repetitive skills; absence of LLM guidance reduces performance, especially on longer tasks — LLM guidance is important for discovering meaningful skill chains.",
            "limitations_or_challenges": "Learns many spurious/repetitive skills; lower downstream task performance despite faster library growth.",
            "uuid": "e8444.1",
            "source_info": {
                "paper_title": "Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "BOSS-OPT1",
            "name_full": "BOSS (OPT-1 ablation)",
            "brief_description": "An ablation of BOSS that replaces the LLaMA-13B guidance LLM with a weaker 1B-parameter OPT model to test sensitivity to LLM quality.",
            "citation_title": "Opt: Open pre-trained transformer language models.",
            "mention_or_use": "use",
            "agent_name": "BOSS-OPT1",
            "agent_description": "Same BOSS pipeline but uses the 1B-parameter OPT LLM for next-skill proposals and summarization instead of LLaMA-13B.",
            "model_name": "OPT-1 (1B)",
            "model_description": "OPT family model with 1 billion parameters, used as lower-capacity LLM ablation.",
            "task_name": "ALFRED long-horizon household tasks",
            "task_description": "Same ALFRED evaluation of multi-step tasks.",
            "task_type": "long-horizon control / multi-step instruction following",
            "memory_used": true,
            "memory_type": "working (prompt/context) + persistent skill library + replay buffer",
            "memory_mechanism": "Same prompt concatenation and skill-library replay mechanics as BOSS but with OPT-1 providing proposals and summaries.",
            "memory_representation": "Executed skill chains, LLM-generated composite summaries (from OPT-1), and replay-buffered trajectories.",
            "memory_retrieval_method": "Prompt concatenation and embedding-based mapping from OPT-1 generations to skills (same pipeline as main BOSS).",
            "performance_with_memory": "Average oracle-normalized return 0.49 ± 0.07; per-length: Length2 0.39 ± 0.08, Length3 0.36 ± 0.07, Length4 0.56 ± 0.08 (Table 3).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Compared to BOSS (LLaMA-13B) and BOSS-Rand: using a weaker LLM degrades performance but still outperforms no-LLM random chaining in some respects. Indicates LLM capability correlates with bootstrapping quality.",
            "key_findings": "LLM capability matters: stronger LLM (13B) yields better-guided practice and higher task performance than a 1B LLM; however weaker LLM still provides useful guidance vs random.",
            "limitations_or_challenges": "Weaker LLMs produce lower-quality proposals, reducing bootstrapping effectiveness.",
            "uuid": "e8444.2",
            "source_info": {
                "paper_title": "Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "SayCan",
            "name_full": "SayCan (LLM-based affordance-guided planner)",
            "brief_description": "A top-down method that uses a pre-trained LLM to decompose a high-level task into step-by-step primitive skills by ranking a given skill library according to language and affordance priors; used as a baseline.",
            "citation_title": "Do as i can and not as i say: Grounding language in robotic affordances.",
            "mention_or_use": "use",
            "agent_name": "SayCan / SayCan+P / SayCan+PF",
            "agent_description": "SayCan: LLM ranks skills given a task using affordance grounding; SayCan+P: variant that uses the paper's LLM proposal mechanism (prompting for step-by-step plans) instead of SayCan's original ranking; SayCan+PF: SayCan+P with policy fine-tuning in target environment (single-skill fine-tuning) for same number of steps as BOSS.",
            "model_name": "LLaMA-13B (used in experiments)",
            "model_description": "Same LLaMA-13B used for planning; approach is top-down planning without environment practice in original form.",
            "task_name": "ALFRED long-horizon household tasks",
            "task_description": "Same evaluation tasks requiring chaining of primitive skills; SayCan plans are executed by pre-trained primitive skill policies without or with limited fine-tuning.",
            "task_type": "long-horizon planning / multi-step instruction following",
            "memory_used": true,
            "memory_type": "working (prompt/context) / in-context examples",
            "memory_mechanism": "LLM prompt contains in-context examples and the skill library; LLM outputs step-by-step plan (open-loop) which is then executed by low-level skill policies. No iterative environment practice to update the planner.",
            "memory_representation": "Skill library and in-context demonstration examples in the prompt; generated step-by-step plan text.",
            "memory_retrieval_method": "Prompt concatenation (in-context learning) and LLM ranking/proposal; no retrieval-augmented external memory or replay.",
            "performance_with_memory": "SayCan: average return 0.06 ± 0.00 (from Table 1: Length2 0.06 ± 0.02, Length3 0.14 ± 0.00, Length4 0.10 ± 0.12). SayCan+P: avg return 0.12 ± 0.01. SayCan+PF (fine-tuned single-skill): avg return 0.57 ± 0.05 but reported task success rate 0.00 ± 0.00 (Table 1).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "SayCan+P (using BOSS's proposal prompt) performs better than original SayCan; fine-tuning single skills (SayCan+PF) improves subtask returns but not overall success (0% success) due to execution failures when chaining; analysis indicates ~95% of unsuccessful SayCan(+P/PF) trajectories are caused by policy execution failures rather than planning errors.",
            "key_findings": "Top-down LLM planning (SayCan family) can propose plausible plans but is brittle when low-level policies fail in new environments; without practice/closed-loop finetuning on skill chaining, planners suffer high execution failure rates even if intermediate subtask returns increase.",
            "limitations_or_challenges": "No mechanism for environment practice or iterative correction; high fraction of failures come from policy execution errors in unseen environments; even fine-tuning single skills does not yield nonzero end-to-end success without chaining practice.",
            "uuid": "e8444.3",
            "source_info": {
                "paper_title": "Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "ProgPrompt",
            "name_full": "ProgPrompt (LLM-to-code planner for robotics)",
            "brief_description": "An LLM-based planner that generates code-like plans for robot tasks which are then translated to primitive skill commands for execution; used as a baseline in real-robot experiments.",
            "citation_title": "ProgPrompt: Generating situated robot task plans using large language models.",
            "mention_or_use": "use",
            "agent_name": "ProgPrompt",
            "agent_description": "Converts natural language tasks into code-like plans (function-like commands) via an LLM, then maps generated code actions into sequences of primitive language instructions which are executed by the pre-trained policy.",
            "model_name": "LLaMA-13B (used in experiments)",
            "model_description": "LLaMA-13B used with prompt engineering to produce code-like robot plans.",
            "task_name": "Real-robot kitchen tabletop manipulation tasks (length 2 and 4)",
            "task_description": "Tabletop manipulation tasks executed on a Kinova Jaco 2 arm; some tasks require chaining multiple primitive skills.",
            "task_type": "real-robot multi-step manipulation / multi-step instruction following",
            "memory_used": true,
            "memory_type": "working (prompt/context) / in-context examples",
            "memory_mechanism": "LLM prompt contains code-like examples converted from natural language demonstrations; generation uses prompt-context information to produce programmatic plans.",
            "memory_representation": "In-context examples turned into code-style plan representations; generated code commands translated to primitive skill language instructions.",
            "memory_retrieval_method": "Prompt concatenation / in-context learning; no external persistent memory or replay used for planning.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "In real-robot evaluation both ProgPrompt and BOSS performed similarly on length-2 tasks, but only BOSS achieved non-zero success on more difficult length-4 tasks after bootstrapping, indicating that closed-loop bootstrapping (practice + memory in replay/skill library) improves generalization on harder real-world chains.",
            "key_findings": "LLM-to-code planning can produce plausible plans, but without environment practice / closed-loop adaptation its performance perishes on longer-horizon tasks compared to BOSS.",
            "limitations_or_challenges": "Relies on correct translation from code outputs to executable skill instructions and on pre-trained skills executing reliably in new environment; lacks closed-loop environment practice for chaining robustness.",
            "uuid": "e8444.4",
            "source_info": {
                "paper_title": "Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLaMA-13B",
            "name_full": "LLaMA: Open and efficient foundation language models (13B variant)",
            "brief_description": "A 13-billion parameter open-source language model used in this work to provide next-skill proposals and to summarize composite skills for BOSS and baseline planners.",
            "citation_title": "Llama: Open and efficient foundation language models",
            "mention_or_use": "use",
            "agent_name": "LLaMA-13B (LLM used for proposal and summarization)",
            "agent_description": "Used to generate distributions over next skills given executed skill chain and skill library (prompted), and to summarize concatenated primitive skill annotations into highlevel composite skill labels to expand the skill library.",
            "model_name": "LLaMA-13B",
            "model_description": "13-billion parameter open-source foundation LLM (Touvron et al.), used with 8-bit inference in experiments.",
            "task_name": "LLM-guided next-skill proposal and summarization inside BOSS; planning baselines (SayCan, ProgPrompt)",
            "task_description": "Generate next-step skill proposals conditioned on skill chain + skill list; summarize sequences of primitive skill annotations into compact composite natural language labels.",
            "task_type": "planning / summarization (LLM-in-the-loop)",
            "memory_used": true,
            "memory_type": "working / in-context (prompt)",
            "memory_mechanism": "Receives prompt with up to: (a) full skill library, (b) executed skill chain (skill history), and (c) in-context examples; outputs next-skill candidates or summary text.",
            "memory_representation": "Prompt tokens containing skill library, executed skills, and example demonstrations.",
            "memory_retrieval_method": "Prompt concatenation; output candidates sampled N times with averaged token likelihoods used as sampling weights; outputs then mapped via sentence embeddings to library skills.",
            "performance_with_memory": "As part of BOSS (main LLM), enabled BOSS to reach avg return 0.57 ± 0.06 and success 0.57 ± 0.14 on ALFRED evaluation; replacing it with OPT-1 (weaker LLM) reduced performance (see ablation).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Compared directly in ablation (BOSS vs BOSS-OPT1): LLaMA-13B guidance outperforms OPT-1 (13B &gt; 1B). Using no LLM (BOSS-Rand) performs worse despite faster library growth.",
            "key_findings": "High-capacity LLMs provide more accurate next-skill proposals and better composite summaries, improving bootstrapping quality and downstream long-horizon task performance.",
            "limitations_or_challenges": "LLM outputs must be mapped to existing skill annotations; naive sentence embeddings emphasized nouns over verbs, requiring a verb-prefixing heuristic to improve mapping. LLM quality limits bootstrapping effectiveness.",
            "uuid": "e8444.5",
            "source_info": {
                "paper_title": "Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Do as i can and not as i say: Grounding language in robotic affordances.",
            "rating": 2
        },
        {
            "paper_title": "ProgPrompt: Generating situated robot task plans using large language models.",
            "rating": 2
        },
        {
            "paper_title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents.",
            "rating": 2
        },
        {
            "paper_title": "Inner monologue: Embodied reasoning through planning with language models.",
            "rating": 2
        },
        {
            "paper_title": "Guiding pretraining in reinforcement learning with large language models.",
            "rating": 1
        },
        {
            "paper_title": "Augmenting autotelic agents with large language models.",
            "rating": 1
        }
    ],
    "cost": 0.019748,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance</h1>
<p>Jesse Zhang ${ }^{1}$, Jiahui Zhang ${ }^{1}$, Karl Pertsch ${ }^{1}$, Ziyi Liu ${ }^{1}$, Xiang Ren ${ }^{1}$, Minsuk Chang ${ }^{2}$, Shao-Hua Sun ${ }^{3}$, Joseph J. Lim ${ }^{4}$<br>${ }^{1}$ University of Southern California, ${ }^{2}$ Google AI, ${ }^{3}$ National Taiwan University, ${ }^{4}$ KAIST<br>jessez@usc.edu</p>
<h4>Abstract</h4>
<p>We propose BOSS, an approach that automatically learns to solve new long-horizon, complex, and meaningful tasks by growing a learned skill library with minimal supervision. Prior work in reinforcement learning requires expert supervision, in the form of demonstrations or rich reward functions, to learn longhorizon tasks. Instead, our approach BOSS (BOotstrapping your own SkillS) learns to accomplish new tasks by performing "skill bootstrapping," where an agent with a set of primitive skills interacts with the environment to practice new skills without receiving reward feedback for tasks outside of the initial skill set. This bootstrapping phase is guided by large language models (LLMs) that inform the agent of meaningful skills to chain together. Through this process, BOSS builds a wide range of complex and useful behaviors from a basic set of primitive skills. We demonstrate through experiments in realistic household environments that agents trained with our LLM-guided bootstrapping procedure outperform those trained with naïve bootstrapping as well as prior unsupervised skill acquisition methods on zero-shot execution of unseen, long-horizon tasks in new environments. View website at clvrai.com/boss.</p>
<h2>1 Introduction</h2>
<p>Robot learning aims to equip robots with the capability of learning and adapting to novel scenarios. Popular learning approaches like reinforcement learning (RL) excel at learning short-horizon tasks such as pick-and-place [1, 2, 3], but they require dense supervision (e.g., demonstrations [4, 5, 6, 7] or frequent reward feedback $[8,9,10]$ ) to acquire long-horizon skills.
In contrast, humans can learn complex tasks with much less supervision-take, for example, the process of learning to play tennis: we may initially practice individual skills like forehand and backhand returns under close supervision of a coach, analogous to RL agents practicing simple pickplace skills using demonstrations or dense rewards. Yet importantly, in between coaching sessions, tennis players return to the tennis court and practice to combine the acquired basic skills into longhorizon gameplay without supervision from the coach. This allows them to develop a rich repertoire of tennis-playing skills independently and perform better during their next match.
Can we enable agents to similarly practice and expand their skills without close human supervision? We introduce BOSS (BOotstrapping your own SkillS), a framework for learning a rich repertoire of long-horizon skills with minimal human supervision (see Figure 1). Starting from a base set of acquired primitive skills, BOSS performs a skill bootstrapping phase in which it progressively grows its skill repertoire by practicing to chain skills into longer-horizon behaviors. BOSS enables us to train generalist agents, starting from a repertoire of only tens of skills, to perform hundreds of long-horizon tasks without additional human supervision.
A crucial question during practice is which skills are meaningful to chain together: randomly chaining tennis moves does not lead to meaningful gameplay; similarly, random chains of pick-place movements do not solve meaningful household tasks. Thus, in BOSS we propose to leverage the rich knowledge captured in large language models (LLMs) to guide skill chaining: given the chain</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: BOSS learns to execute a large set of useful, long-horizon skills with minimal supervision by performing LLM-guided skill bootstrapping. (a): The agent starts with an initial skill library. During bootstrapping, it practices chaining skills into new long-horizon behaviors using guidance from an LLM. The collected experience is used to update the policy. Newly discovered skill chains are summarized with an LLM and added as new skills into the library for further bootstrapping. Thus, the agent's skill repertoire grows over time. (b): After bootstrapping, we condition the policy on novel instructions and show execution in the environment using the bootstrapped skill repertoire.</p>
<p>Of executed skills so far, the LLM predicts a distribution over meaningful next skills to sample. Importantly, in contrast to existing approaches that leverage the knowledge captured in LLMs for long-horizon task planning [11, 12, 13, 14], BOSS can use unsupervised environment interactions to <em>practice</em> how to chain skills into long-horizon task executions; this practice is crucial especially if the target environment differs from the ones used to train the base skill set. This results in a more robust policy that can compensate for accumulating errors from the initial skill repertoire.</p>
<p>We validate the effectiveness of our proposed approach in simulated household environments from the ALFRED benchmark and on a real robot. Experimental results demonstrate that BOSS can practice effectively with LLM guidance, allowing it to solve long-horizon household tasks in novel environments which prior LLM-based planning and unsupervised exploration approaches fail at.</p>
<h2>2 Preliminaries and Related Work</h2>
<h3>Reinforcement Learning</h3>
<p>Reinforcement learning (RL) algorithms aim to learn a policy $\pi(a|s)$ that maximizes the expected discounted return $\mathbb{E}<em t="t">{a\sim\pi,P}\left[\sum</em>(s, a)$, which represent future discounted returns when following the policy at state $s$ or after taking action $a$ from state $s$, respectively [15]. Standard RL algorithms struggle with learning long-horizon tasks and can be prohibitively sample-inefficient.}\gamma^{t}R(s_{t},a_{t},s_{t+1})\right]$ in a Markov Decision Process $\mathcal{M}=(\mathcal{S}, \mathcal{A}, P, \mathcal{R}, \gamma)$, where $\mathcal{S}$ and $\mathcal{A}$ are state and action spaces, $P: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}_{+}$ represents the transition probability distribution, $\mathcal{R}: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}$ denotes the reward function, and $\gamma$ is the discount factor. Temporal-difference algorithms are a class of RL algorithms that also learn critic functions, denoted $V^{\pi}(s)$ or $Q^{\pi</p>
<h3>Skill-based RL</h3>
<p>To solve long-horizon tasks, prior works have focused on pre-training skills, short-horizon behaviors that can be re-combined into long-horizon behaviors [9, 16, 17, 18, 19]. These skills can be represented as learned options [16, 18], sub-goal setting and reaching policies [20, 21], a set of discrete policies [22, 23], or continuous latent spaces that represent behaviors [9, 10, 24, 25, 26]. Yet, most of these approaches need expert supervision (e.g., demonstrations [4, 5, 6, 7, 20, 21, 27], frequent reward feedback [9, 10, 23]). In contrast, BOSS learns to execute long-horizon tasks with minimal human supervision via skill bootstrapping.</p>
<h3>Unsupervised RL</h3>
<p>To learn skills without human supervision, recent works have introduced many unsupervised RL objectives, e.g., based on curiosity [28], contrallability [29, 30], and behavior or state diversification [31, 32, 33, 34, 35]. Because these works learn skills from scratch and explore without supervision, they generally focus on locomotion tasks where most behaviors agents can ex-</p>
<p>plore, such as different running gaits, are already meaningful. Few works demonstrate learning of manipulation tasks, but either require hand-crafted state or action spaces [28] or remain constrained to learning simple, short-horizon skills [36, 37]. BOSS makes two improvements to enable bootstrapping of long-horizon tasks: (1) We start from a base repertoire of language-conditioned skills to enable coherent, long-horizon exploration. (2) We leverage an LLM to guide exploration towards meaningful skill-chains within the exponential number of possible long-horizon behaviors.</p>
<p>Language in RL Prior works have employed language to parameterize rich skill sets to train multitask RL agents [38, 39, 40, 41, 42, 43]. Recent progress in training LLMs has enabled approaches that combine LLMs with pre-trained language-conditioned policies to perform open-loop planning over pre-trained skills [11, 12, 13, 14, 44]. These works do not perform any policy training or finetuning when planning with the LLMs; but instead use the LLMs as top-down planners whose plans are given to fixed low-level skill policies to execute. In contrast, BOSS pratices chaining behaviors in the environment during skill bootstrapping and thus learns a more robust, closed-loop policy. This leads to substantially higher success rate for executing long-horizon tasks.
ELLM [45], LMA3 [46], and IMAGINE [47] are closest to our work. ELLM and LMA3 both use an LLM to generate tasks, with the former requiring a captioning model to reward agents and the latter additionally using the LLM to hindsight label past agent trajectories for task completion; instead, we expand upon a learned skill repertoire, allowing for building skill chains while automatically rewarding the agent based on the completion of skills in the chain. Meanwhile, IMAGINE uses language guidance to generate exploration goals, requiring a "social partner" that modifies the environment according to desired goals. In realistic settings, this social partner requires extensive human effort to design. BOSS instead utilizes LLMs to propose goals in a target environment automatically.</p>
<h1>3 Method</h1>
<p>Our method, BOSS (BOotstrapping your own SkillS), automatically learns to solve new longhorizon, complex tasks by growing a learned skill library with minimal supervision. BOSS consists of two phases: (1) it acquires a base repertoire of skills (Section 3.1) and then (2) it practices chaining these skills into long-horizon behaviors in the skill bootstrapping phase (Section 3.2). BOSS can then zero-shot execute novel natural language instructions describing complex long-horizon tasks.</p>
<h3>3.1 Pre-training a Language-Conditioned Skill Policy</h3>
<p>We assume access to a dataset $\mathcal{D}^{L}=\left{\tau_{z_{1}}, \tau_{z_{2}}, \tau_{z_{3}}, \ldots,\right}$ where $\tau_{z_{i}}$ denotes a trajectory of $\left(s, a, s^{\prime}, r\right)$ tuples and $z_{i}$ is a freeform language description of the trajectory. We also assume access to a sparse reward function for the primitive skills, e.g., an object detector that can detect if an object is placed in the correct location. For example, if $\tau_{z_{i}}$ demonstrates a robot arm picking up a mug, then $z_{i}=$ "pick up the mug." and $r=1$ in the final transition in which the mug is picked up and 0 otherwise. To obtain a language-conditioned primitive skill policy, we train a standard offline RL algorithm on $\mathcal{D}^{L}$. In our experiments, we use Implicit Q-Learning (IQL) [48] as it is performant and amenable to online fine-tuning. We condition the policy and critic networks on the trajectory's natural language annotation $z$, yielding a language-conditioned policy $\pi(a \mid s, z)$ and a critic function $V(s, z)$.</p>
<h3>3.2 Skill Bootstrapping</h3>
<p>After learning the language-conditioned primitive skill policy, we perform skill bootstrapping the agent practices by interacting with the environment, trying new skill chains, then adding them back into its skill repertoire for further bootstrapping. As a result, the agent learns increasingly long-horizon skills without requiring additional supervision beyond the initial set of skills.
Sampling initial skills. At the start of bootstrapping, the skill repertoire $Z=\left{z_{1}, z_{2}, \ldots\right}$ is initialized to the set of pre-trained base skills. Upon initializing the agent in the environment at state $s_{1}$, we must sample an initial skill. Intuitively, the skill we choose should be executable from $s_{1}$ i.e., have a high chance of success. Therefore, in every bootstrapping episode, we sample the initial skill according to probabilities generated from the pre-trained value function, $V\left(s_{1}, z\right)$. We then try to execute the sampled skill until a timeout threshold is reached.
Guiding Skill Chaining via LLMs. If the first skill execution succeeds, the next step is constructing a longer-horizon behavior by chaining together the first skill with a sampled next skill. Naïvely</p>
<p>choosing the next skill by, for example, sampling at random will likely result in a behavior that is not useful for downstream tasks. Even worse, the likelihood of picking a bad skill chain via random sampling increases linearly with the size of the skill repertoire and exponentially with the length of the skill chain. For a modestly sized repertoire with 20 skills and a chain length of 5 there are $20^{5}=3.2 M$ possible skill chains, only few of which are likely meaningful.
Thus, instead of randomly sampling subsequent skills, we propose to use large language models (LLMs) to guide skill selection. Prior work has demonstrated that modern LLMs capture relevant information about meaningful skill chains [11, 12, 14]. Yet, in contrast to prior top-down LLM planning methods, we explore a bottom-up approach to learning long-horizon tasks: by allowing our agent to iteratively sample skill chains and practice their execution in the environment, we train more robust long-horizon task policies that achieve higher empirical success rates, particularly when generalizing to unseen environments (see Section 4).</p>
<h2>LLM Prompt Example</h2>
<p>Predict the next skill from the following list: Pick up the mug; Turn on the lamp; Put the mug in the coffee machine; ...</p>
<p>1: Pick up the mug.
2:
Figure 2: A shortened LLM prompt. See the full prompt in Appendix A.2. pre-trained sentence embedding model [49]. To encourage diversity in the practiced skill chains, we repeat this process $N$ times and sample the true next skill from the distribution of LLM-assigned token likelihoods. Finally, if the sampled skill is successfully executed, we repeat the same process for sampling the following skill. ${ }^{1}$
Learning new skills. Once an episode concludes, either because a skill times out or because a defined maximum skill chain length is reached, we add the collected data back into the replay buffer with a sparse reward of 1 for every completed skill. For example, if an attempted skill chain contains a total of 3 skills, then the maximum return of the entire trajectory is 3 . We then continue policy training via the same offline RL algorithm used to learn the primitive skills-in our case, IQL [48].
Finally, to maximize data efficiency, we relabel the language instructions for the collected episode upon adding it to the replay buffer. Specifically, following prior work [42], we aggregate consecutive skills into composite skill instructions using the same LLM as for skill sampling. We then add the composite skill instruction and associated experience to the replay buffer and also add it to our skill repertoire for continued bootstrapping. We store new trajectories with both their lowest level annotations and the LLM-generated composite instructions so the agent can fine-tune its base skills while learning longer-horizon skill chains online. To ensure the agent does not forget its initial skill repertoire, we sample data from the offline dataset $\mathcal{D}^{L}$ with new data at equal proportions in batch.</p>
<p>In sum, we iterate through these three steps to train a policy during the skill bootstrapping phase: (1) Sampling initial skills using the value function. (2) Sampling next skills by prompting the LLM with skills executed so far. (3) Adding learned skills to the skill library and training on collected agent experience. Algorithm 1 presents a brief overview. The implement</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Algorithm 1 BOSS Pseudocode.
1: Train policy $\pi$ on initial skill repertoire
2: for skill bootstrapping episode do
3: Sample initial skill $z$ and execute
4: while not episode timeout do
5: Sample next skill from LLM and execute
6: Construct composite skill and add to repertoire
7: Update policy $\pi$
<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup> <sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>:    Algorithm 1 BOSS Pseudocode.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Environments. (a) The ALFRED environment is a benchmark for learning agents that can follow natural language instructions to fulfill household tasks. This illustration was drawn from Shridhar et al. [50] with permission. (b) Real-world Jaco arm: Our real-world kitchen manipulation tabletop environment based on RGB image inputs.</p>
<h1>4 Experimental Evaluation</h1>
<p>The goal of our experiments is to test BOSS's ability to acquire long-horizon, complex, and meaningful behaviors. We compare to unsupervised RL and zero-shot planning methods in two challenging, image-based control environments: solving household tasks in the ALFRED simulator [50] and kitchen manipulation tasks with a real-world Jaco robot arm. Concretely, we aim to answer the following questions: (1) Can BOSS learn a rich repertoire of useful skills during skill bootstrapping? (2) How do BOSS's acquired skills compare to skills learned by unsupervised RL methods? (3) Can BOSS directly be applied on real robot hardware?</p>
<h3>4.1 Experimental Setup</h3>
<p>ALFRED Environment. We test our approach in the ALFRED simulator [50] (see Figure 3a), since its 100+ floorplans with many interactable objects provide a rich environment for learning numberous long-horizon household tasks. We leverage a modified version of the ALFRED simulator [? ] that allows for online RL interactions via a gym interface with $300 \times 300$ egocentric RGB image observations. The action space consists of 12 discrete action choices (e.g. turn left, look up, pick up object), along with 82 discrete object types, first proposed by Pashevich et al. [51]. To train the skills in our initial skill library, we leverage the ALFRED dataset of $73 k$ primitive skill demonstrations with language instructions. For bootstrapping we use four unseen floorplans. In each floorplan we define 10 evaluation tasks, each of which requires 2 to 8 primitive skills to complete.
Real-Robot Kitchen Manipulation. We evaluate our method with a real-robot manipulation setup in which a Kinova Jaco 2 robot arm needs to solve stylized kitchen tasks in a table-top environment (see Figure 3b). The observations consist of concatenated RGB images from a third-person and a wrist-mounted camera. The robot is controlled with continuous end-effector displacements and discrete gripper open/stay/close commands at a frequency of 10 Hz . To train the initial skills, we collect a dataset of $6 k$ language-annotated primitive skill demonstrations via human teleoperation. We perform bootstrapping and evaluate the agents in a table setup with unseen object arrangements.
Training and Evaluation Procedure. We equip the policy with the initial primitive skill library by training it for 150 epochs on the respective pre-collected demonstration datasets using IQL [48] (see Section 3.1). We then perform 500,000 and 15,000 steps ( $\sim 17 \mathrm{~min}$ of robot interaction time) of online skill bootstrapping in the respective unseen eval environments of ALFRED and the real robot setup. Note that for ALFRED we train separate agents for each floorplan, mimicking a scenario in which an agent is dropped into a new household and acquires skills with minimal supervision. After bootstrapping, we evaluate the trained agents zero-shot on the held-out evaluation tasks by conditioning the policy on the respective language instruction. To perform well in this evaluation setting, an agent needs to acquire a large number of useful skills during online environment interactions.</p>
<p>Baselines. We compare BOSS to prior works that can learn a wide range of skills with minimal supervision: (1) unsupervised RL approaches that, like BOSS, learn from environment interactions without additional feedback and (2) large-language model based planners, that leverage the knowledge captured in large pre-trained language models to "bootstrap" given skill libraries into long-horizon behaviors. Concretely, we are comparing to the following approaches:</p>
<ul>
<li>CIC [52]: SoTA method on the unsupervised RL benchmark [53], expands its skill library with a contrastive alignment objective during bootstrapping. For fair comparison, we pre-train CIC's policy on the same primitive skill dataset used in BOSS before unsupervised bootstrapping.</li>
<li>SayCan [12]: Leverages a pre-trained LLM to break down a given task into step-by-step instructions, i.e., "primitive skills", by ranking skills from a given library. We implement SayCan using the same primitive skill policy pre-trained via offline RL as in BOSS. We use the same LLM as our method, and adapt SayCan's LLM prompt for our environment. Notably, SayCan and similar LLM planning work have no mechanism for fine-tuning to new environments.</li>
<li>SayCan+P: To evaluate the effects of online bootstrapping vs. top-down LLM planning in isolation, we evaluate a SayCan variant that uses our LLM-based skill proposal mechanism, which leverages the LLM to generate step-by-step instructions in place of SayCan's original skill ranking method. We found this to perform better than standard SayCan in our evaluation.</li>
<li>SayCan+PF: SayCan+P on policies fine-tuned in the target environments for the same number of steps as BOSS by sampling single skills with the value function and learning to execute them. This compares the effect of BOSS learning to chain skills in the target environments.</li>
</ul>
<p>Additionally, we evaluate (1) an Oracle that finetunes the pre-trained primitive skill policy directly on the target tasks, serving as an upper bound, and (2) a pre-trained primitive skill policy without any bootstrapping (No Bootstrap), serving as a performance lower bound.
All methods utilize the same base primitive skill policy pre-trained on the same demonstration data. We implement a transformer policy and critic architecture based on Pashevich et al. [51] trained with the IQL algorithm [48]. All results reported are inter-quartile means and standard deviations over 5 seeds [54]. Finally, Saycan and BOSS all use the LLaMA-13b open-source, 13-billion parameter LLM [55]. For more baseline implementation and training details, see Appendix B.</p>
<h1>4.2 BOSS Bootstrapping Learns Useful Skills</h1>
<p>ALFRED. Overall, BOSS achieves superior performance to all nonoracle baselines, with better oraclenormalized return at longer, length 3 and 4 tasks than the best baselines, and BOSS is the only method to achieve non-zero success rates across all lengths of tasks. From Table 1, the gap between BOSS and best baselines is largest on the length 4 tasks, indicating the benefit of BOSS' LLM-guided skill bootstrapping in</p>
<p>Table 1: Inter-quartile means (IQMs) and standard deviations of oracle-normalized returns, i.e., number of solved subtasks, broken down by task length, across the ALFRED evaluation tasks. We also report oracle-normalized success rate in the last column. We do not report results for length 6 and 8 tasks since not even the oracle was able to learn these.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Returns by Evaluation Task Length</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Average</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Method</td>
<td style="text-align: center;">Length 2</td>
<td style="text-align: center;">Length 3</td>
<td style="text-align: center;">Length 4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Return</td>
<td style="text-align: center;">Success</td>
</tr>
<tr>
<td style="text-align: left;">No Bootstrap</td>
<td style="text-align: center;">$0.03+-0.02$</td>
<td style="text-align: center;">$0.05+-0.07$</td>
<td style="text-align: center;">$0.08+-0.09$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$0.03+-0.01$</td>
<td style="text-align: center;">$0.00+-0.00$</td>
</tr>
<tr>
<td style="text-align: left;">CIC [52]</td>
<td style="text-align: center;">$0.02+-0.02$</td>
<td style="text-align: center;">$0.25+-0.08$</td>
<td style="text-align: center;">$0.18+-0.07$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$0.11+-0.01$</td>
<td style="text-align: center;">$0.00+-0.00$</td>
</tr>
<tr>
<td style="text-align: left;">SayCan [12]</td>
<td style="text-align: center;">$0.06+-0.02$</td>
<td style="text-align: center;">$0.14+-0.00$</td>
<td style="text-align: center;">$0.10+-0.12$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$0.06+-0.00$</td>
<td style="text-align: center;">$0.00+-0.00$</td>
</tr>
<tr>
<td style="text-align: left;">SayCan + P</td>
<td style="text-align: center;">$0.08+-0.04$</td>
<td style="text-align: center;">$0.28+-0.00$</td>
<td style="text-align: center;">$0.20+-0.15$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$0.12+-0.01$</td>
<td style="text-align: center;">$0.00+-0.00$</td>
</tr>
<tr>
<td style="text-align: left;">SayCan + PF</td>
<td style="text-align: center;">$\mathbf{0 . 6 4 + - 0 . 0 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 9 + - 0 . 2 0}$</td>
<td style="text-align: center;">$0.59+-0.02$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{0 . 5 7 + - 0 . 0 5}$</td>
<td style="text-align: center;">$0.00+-0.00$</td>
</tr>
<tr>
<td style="text-align: left;">BOSS (ours)</td>
<td style="text-align: center;">$\mathbf{0 . 4 7 + - 0 . 1 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 9 + - 0 . 1 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 1 + - 0 . 1 3}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{0 . 5 7 + - 0 . 0 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 7 + - 0 . 1 4}$</td>
</tr>
</tbody>
</table>
<p>learning difficult, longer-horizon tasks without task supervision. CIC can make some progress in some length 3 and 4 tasks, but its contrastive objective generally fails to finetune the primitive skills into meaningful long-horizon skills. Saycan+P performs better than Saycan, indicating that our proposal mechanism better extracts a more meaningful distribution of skills from an LLM, but even Saycan+P greatly falls short of BOSS' performance as it is not robust to execution failures incurred from directly using the pre-trained policy in unseen floor plans. Saycan+PF performs better as it first fine-tunes its policies, but it still achieves a $0 \%$ success rate compared to BOSS' $57 \%$. Additional analyses we perform in Appendix C. 1 demonstrates that in SayCan+P, $95.8 \%$ of all unsuccessful SayCan+P trajectories are caused by policy execution failures. SayCan+PF is only slightly better: $95.0 \%$ are caused by policy execution failures, indicating that naïve fine-tuning in the target en-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Left: The number of subtasks in skills executed during skill bootstrapping by BOSS in one of the unseen ALFRED floorplans. BOSS progressively learns longer skill chains throughout the course of training. Right: The number of newly acquired skills by BOSS throughout training.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">(1) Pick up the pillow off of the seat of the blue chair</th>
<th style="text-align: center;">(1) Go to the area between the cabinets and the toilet</th>
<th style="text-align: center;">(1) Take the apple on the right from the sink</th>
<th style="text-align: center;">(1) Pick up the white pencil on the desk</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">(2) Put the pillow vertically on the couch to the left of the newspaper</td>
<td style="text-align: center;">(2) Pick up the empty toilet paper tube behind the toilet brush</td>
<td style="text-align: center;">(2) Pick up the knife from the counter <br> (3) Cut the apple into pieces</td>
<td style="text-align: center;">(2) Place the white pencil on the desk near the books <br> (3) Pick up the books from the bed</td>
</tr>
<tr>
<td style="text-align: center;">Put the pillow on the couch next to the newspaper.</td>
<td style="text-align: center;">(3) Place the toilet paper tube upright to the left of the full toilet paper roll</td>
<td style="text-align: center;">(4) Put the apple on the right of the statue and in front of the salt</td>
<td style="text-align: center;">(4) Turn on the lamp</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">(4) Close the cabinet door</td>
<td style="text-align: center;">Cut the apple and put it on the right of the statue.</td>
<td style="text-align: center;">Place the white pencil on the desk next to the books and then look at the book from the bed under the lamp light.</td>
</tr>
</tbody>
</table>
<p>Figure 5: Example skill chains (light gray) and new skill summaries (dark grey) learned by BOSS during skill bootstrapping. LLM-guidance ensures meaningful skill chains and summaries.
vironment is ineffective for solving long-horizon tasks. Since BOSS learns to finetune individual primitive skills and transition between skills using a closed-loop policy, it performs much better on complex, long-horizon language-specified tasks in unseen environments.
We display qualitative examples of a length 2 and 3 task in appendix Figure 10, where we can see that BOSS successfully completes the tasks whereas Saycan suffers from execution failures, getting stuck while attempting to manipulate objects, and CIC navigates around performing random behaviors (Figure 10a) or gets stuck navigating around objects (Figure 10b). We show qualitative examples of learned skills in Figure 5 and perform additional experiments and analysis in Appendix C.1.
Real Robot. In our real world experiments, we compare BOSS to ProgPrompt [14], a similar LLM planning method to Saycan that has been extensively evaluated on real-world tabletop robot manipulation environments similar to ours. We also augment it with prompt examples similar to ours and our skill proposal mechanism. Here, we evaluate on 4 tasks, 2 of length 2 and 2 of length 4 after performing bootstrapping. Results in Table 2 demonstrate that both methods perform similarly on length 2 tasks, but only BOSS achieves nonzero success rate on more difficult length 4 tasks as it is able to learn to chain together longhorizon skills in the new environment. See Appendix C. 2 for more detailed task information.</p>
<h1>4.2.1 Ablation Studies</h1>
<p>To better analyze the effect of our core contribution, the usage of LLM guidance during skill bootstrapping, we compare to the following variants of our approach:</p>
<ul>
<li>
<p>BOSS-OPT1: BOSS bootstrapping with a weaker 1-billion parameter LLM, OPT-1 [56].</p>
</li>
<li>
<p>BOSS-Rand: An ablation of our approach BOSS that uses no LLM guidance during skill bootstrapping and simply selects the next skill at random from the current skill library.</p>
</li>
</ul>
<p>We report results in Table 3. The analysis shows the importance of accurate LLM guidance during skill bootstrapping for learning useful skills. Using an LLM with lower performance (OPT1) results in degraded overall performance. Yet, bootstrapping without any LLM guidance performs even worse. Interestingly, the performance gap between BOSS and its variants widens for longer task lengths. Intuitively, the longer the task, the more possible other, less useful tasks of the same length could be learned by the agent during bootstrapping. Thus, particularly for long tasks accurate LLM guidance is helpful.</p>
<p>To further analyze this, we compare the sizes of the learned skill libraries between BOSS bootstrapped with LLaMA-13B guidance vs. random skill selection (BOSS-Rand) in Figure 6. Perhaps surprisingly, the random skill chaining ablation learns more skills than BOSS - its skill library grows faster during bootstrapping. Yet, Table 3 shows that it has lower performance. This indicates, that while BOSS-Rand learns many skills, it learns less meaningful skills. A qualitative analysis supports this intuition: many of the learned skills contain repetitions and meaningless skill chains. This underlines the importance of LLM guidance during skill bootstrapping. Furthermore, the positive correlation between the powerfulness of the used guidance LLM ( $1 \mathrm{~B} \rightarrow 13 \mathrm{~B}$ parameters) and the evaluation task performance suggests that future, even more powerful LLMs can lead to even better skill bootstrapping.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Evaluation Task Length</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Length 2</td>
<td style="text-align: center;">Length 3</td>
<td style="text-align: center;">Length 4</td>
<td style="text-align: center;">Average</td>
</tr>
<tr>
<td style="text-align: center;">BOSS (ours)</td>
<td style="text-align: center;">$0.47 \pm 0.12$</td>
<td style="text-align: center;">$0.59 \pm 0.13$</td>
<td style="text-align: center;">$0.81 \pm 0.13$</td>
<td style="text-align: center;">$\mathbf{0 . 5 7 \pm 0 . 0 6}$</td>
</tr>
<tr>
<td style="text-align: center;">BOSS-OPT1</td>
<td style="text-align: center;">$0.39 \pm 0.08$</td>
<td style="text-align: center;">$0.36 \pm 0.07$</td>
<td style="text-align: center;">$0.56 \pm 0.08$</td>
<td style="text-align: center;">$0.49 \pm 0.07$</td>
</tr>
<tr>
<td style="text-align: center;">BOSS-Rand</td>
<td style="text-align: center;">$0.32 \pm 0.03$</td>
<td style="text-align: center;">$0.29 \pm 0.11$</td>
<td style="text-align: center;">$0.61 \pm 0.16$</td>
<td style="text-align: center;">$0.45 \pm 0.06$</td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 6: Skill library size during bootstrapping.</p>
<h1>5 Discussion</h1>
<p>We propose BOSS, an approach that learns a diverse set of long-horizon tasks with minimal supervision via LLM-guided skill bootstrapping. Starting from an initial library of skills, BOSS acquires new behaviors by practicing to chain skills while using LLMs to guide skill selection. We demonstrate in a complex household simulator and real robot manipulation tasks that BOSS can learn more useful skills during bootstrapping than prior methods.
Limitations. While BOSS learns a large repertoire of skills with minimal supervision, it still has limitations that prevent it from truly fulfilling the vision of agents autonomously acquiring skills in new environments. BOSS requires environment resets between bootstrapping episodes, which are currently performed by a human in our real world experiments. Also, we require success detection for each of the primitive skills during bootstrapping. Future research can investigate using advances in reset-free RL [57, 58] to approach the goal of truly autonomous skill learning. Furthermore, BOSS greedily proposes new skill chains one skill at a time, this greedy skill chaining process may not be optimal for generating consistent long-horizon behaviors beyond a certain length. In future work, we plan to explore mechanisms to propose long-horizon tasks that are broken down to individual skills in conjunction with the greedy skill chaining of BOSS. Finally, BOSS is currently limited to skills that are combinations of skills in its initial skill library. Extending our work with unsupervised RL [59, 52] techniques for learning new low-level skills is an exciting direction for future work.</p>
<h2>Acknowledgments</h2>
<p>We thank Ishika Singh for her assistance with implementing and debugging ProgPrompt. This work was supported by a USC Viterbi Fellowship, Institute of Information \&amp; Communications Technology Planning \&amp; Evaluation (IITP) grants (No.2019-0-00075, Artificial Intelligence Graduate School Program, KAIST; No.2022-0-00077, AI Technology Development for Commonsense Extraction, Reasoning, and Inference from Heterogeneous Data, No.2022-0-00984, Development of Artificial Intelligence Technology for Personalized Plug-and-Play Explanation and Verification of Explanation), a National Research Foundation of Korea (NRF) grant (NRF-2021H1D3A2A03103683) funded by the Korean government (MSIT), the KAIST-NAVER hypercreative AI center, and Samsung Electronics Co., Ltd (IO220816-02015-01). Shao-Hua Sun was supported by the Yushan Fellow Program by the Taiwan Ministry of Education and National Taiwan University.</p>
<h1>References</h1>
<p>[1] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly, M. Kalakrishnan, V. Vanhoucke, and S. Levine. Scalable deep reinforcement learning for vision-based robotic manipulation. In Conference on Robot Learning, 2018.
[2] D. Kalashnkov, J. Varley, Y. Chebotar, B. Swanson, R. Jonschkowski, C. Finn, S. Levine, and K. Hausman. Mt-opt: Continuous multi-task robotic reinforcement learning at scale. arXiv, 2021.
[3] A. X. Lee, C. Devin, Y. Zhou, T. Lampe, K. Bousmalis, J. T. Springenberg, A. Byravan, A. Abdolmaleki, N. Gileadi, D. Khosid, C. Fantacci, J. E. Chen, A. Raju, R. Jeong, M. Neunert, A. Laurens, S. Saliceti, F. Casarini, M. Riedmiller, R. Hadsell, and F. Nori. Beyond pick-andplace: Tackling robotic stacking of diverse shapes. In Conference on Robot Learning, 2021.
[4] A. Gupta, V. Kumar, C. Lynch, S. Levine, and K. Hausman. Relay policy learning: Solving long horizon tasks via imitation and reinforcement learning. In Conference on Robot Learning, 2019.
[5] K. Pertsch, Y. Lee, Y. Wu, and J. J. Lim. Demonstration-guided reinforcement learning with learned skills. In Conference on Robot Learning, 2021.
[6] F. Ebert, Y. Yang, K. Schmeckpeper, B. Bucher, G. Georgakis, K. Daniilidis, C. Finn, and S. Levine. Bridge data: Boosting generalization of robotic skills with cross-domain datasets, 2021.
[7] M. Heo, Y. Lee, D. Lee, and J. J. Lim. Furniturebench: Reproducible real-world benchmark for long-horizon complex manipulation. In Robotics: Science and Systems, 2023.
[8] X. B. Peng, M. Chang, G. Zhang, P. Abbeel, and S. Levine. MCP: Learning composable hierarchical control with multiplicative compositional policies. In Neural Information Processing Systems, 2019.
[9] K. Pertsch, Y. Lee, and J. J. Lim. Accelerating reinforcement learning with learned skill priors. In Conference on Robot Learning, 2020.
[10] A. Ajay, A. Kumar, P. Agrawal, S. Levine, and O. Nachum. Opal: Offline primitive discovery for accelerating offline reinforcement learning. In International Conference on Learning Representations, 2021.
[11] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. arXiv preprint arXiv:2201.07207, 2022.
[12] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman, A. Herzog, D. Ho, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, E. Jang, R. J. Ruano, K. Jeffrey, S. Jesmonth, N. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, K.-H. Lee, S. Levine, Y. Lu, L. Luu, C. Parada, P. Pastor, J. Quiambao, K. Rao, J. Rettinghouse, D. Reyes, P. Sermanet, N. Sievers, C. Tan, A. Toshev, V. Vanhoucke, F. Xia, T. Xiao, P. Xu, S. Xu, M. Yan, and A. Zeng. Do as i can and not as i say: Grounding language in robotic affordances. In arXiv preprint arXiv:2204.01691, 2022.
[13] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, P. Sermanet, N. Brown, T. Jackson, L. Luu, S. Levine, K. Hausman, and B. Ichter. Inner monologue: Embodied reasoning through planning with language models. In arXiv preprint arXiv:2207.05608, 2022.
[14] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg. ProgPrompt: Generating situated robot task plans using large language models. In Neural Information Processing Systems, 2022.</p>
<p>[15] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018.
[16] R. S. Sutton, D. Precup, and S. Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial Intelligence, 112(1):181-211, 1999. ISSN 0004-3702.
[17] M. Pickett and A. G. Barto. Policyblocks: An algorithm for creating useful macro-actions in reinforcement learning. In International Conference on Machine Learning, 2002.
[18] P.-L. Bacon, J. Harb, and D. Precup. The option-critic architecture. In Association for the Advancement of Artificial Intelligence, 2017.
[19] T. Nam, S.-H. Sun, K. Pertsch, S. J. Hwang, and J. J. Lim. Skill-based meta-reinforcement learning. In International Conference on Learning Representations, 2022.
[20] A. Gupta, V. Kumar, C. Lynch, S. Levine, and K. Hausman. Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning, 2019.
[21] A. Mandlekar, F. Ramos, B. Boots, S. Savarese, L. Fei-Fei, A. Garg, and D. Fox. Iris: Implicit reinforcement without interaction at scale for learning control from offline robot manipulation data. In IEEE International Conference on Robotics and Automation, 2020.
[22] S. Schaal. Dynamic movement primitives-a framework for motor control in humans and humanoid robotics. Adaptive Motion of Animals and Machines, 2006.
[23] Y. Lee, S.-H. Sun, S. Somasundaram, E. Hu, and J. J. Lim. Composing complex skills by learning transition policies with proximity reward induction. In International Conference on Learning Representations, 2019.
[24] K. Hausman, J. T. Springenberg, Z. Wang, N. Heess, and M. Riedmiller. Learning an embedding space for transferable robot skills. In International Conference on Learning Representations, 2018.
[25] D. Trivedi, J. Zhang, S.-H. Sun, and J. J. Lim. Learning to synthesize programs as interpretable and generalizable policies. In Neural Information Processing Systems, 2021.
[26] G.-T. Liu, E.-P. Hu, P.-J. Cheng, H.-Y. Lee, and S.-H. Sun. Hierarchical programmatic reinforcement learning via learning to compose programs. In International Conference on Machine Learning, 2023.
[27] L. X. Shi, J. J. Lim, and Y. Lee. Skill-based model-based reinforcement learning. In Conference on Robot Learning, 2022.
[28] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiosity-driven exploration by selfsupervised prediction. In International Conference on Machine Learning, 2017.
[29] A. Sharma, S. Gu, S. Levine, V. Kumar, and K. Hausman. Dynamics-aware unsupervised discovery of skills. arXiv, abs/1907.01657, 2019.
[30] S. Park, K. Lee, Y. Lee, and P. Abbeel. Controllability-aware unsupervised skill discovery. In International Conference on Machine Learning, 2023.
[31] J. Achiam, H. Edwards, D. Amodei, and P. Abbeel. Variational option discovery algorithms. arXiv, 2018.
[32] B. Eysenbach, A. Gupta, J. Ibarz, and S. Levine. Diversity is all you need: Learning skills without a reward function. In International Conference on Learning Representations, 2019.
[33] D. Warde-Farley, T. V. de Wiele, T. Kulkarni, C. Ionescu, S. Hansen, and V. Mnih. Unsupervised control through non-parametric discriminative rewards. In International Conference on Learning Representations, 2019.</p>
<p>[34] K. Gregor, D. J. Rezende, and D. Wierstra. Variational intrinsic control. arXiv, abs/1611.07507, 2016.
[35] J. Zhang, H. Yu, and W. Xu. Hierarchical reinforcement learning by discovering intrinsic options. In International Conference on Learning Representations, 2021.
[36] R. Sekar, O. Rybkin, K. Daniilidis, P. Abbeel, D. Hafner, and D. Pathak. Planning to explore via self-supervised world models. In International Conference on Machine Learning, 2020.
[37] R. Mendonca, O. Rybkin, K. Daniilidis, D. Hafner, and D. Pathak. Discovering and achieving goals via world models. In Neural Information Processing Systems, 2021.
[38] S.-H. Sun, T.-L. Wu, and J. J. Lim. Program guided agent. In International Conference on Learning Representations, 2020.
[39] C. Lynch and P. Sermanet. Language conditioned imitation learning over unstructured data. In Robotics: Science and Systems, 2021.
[40] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn. BC-z: Zero-shot task generalization with robotic imitation learning. In Conference on Robot Learning, 2021.
[41] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. Jackson, S. Jesmonth, N. J. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, I. Leal, K.-H. Lee, S. Levine, Y. Lu, U. Malla, D. Manjunath, I. Mordatch, O. Nachum, C. Parada, J. Peralta, E. Perez, K. Pertsch, J. Quiambao, K. Rao, M. Ryoo, G. Salazar, P. Sanketi, K. Sayed, J. Singh, S. Sontakke, A. Stone, C. Tan, H. Tran, V. Vanhoucke, S. Vega, Q. Vuong, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich. Rt-1: Robotics transformer for real-world control at scale, 2022.
[42] J. Zhang, K. Pertsch, J. Zhang, and J. J. Lim. Sprint: Scalable policy pre-training via language instruction relabeling, 2023.
[43] Z. Liu, J. Zhang, K. Asadi, Y. Liu, D. Zhao, S. Sabach, and R. Fakoor. Tail: Task-specific adapters for imitation learning with large pretrained models, 2023.
[44] D. Shah, B. Osinski, B. Ichter, and S. Levine. Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action. In Conference on Robot Learning, 2022.
[45] Y. Du, O. Watkins, Z. Wang, C. Colas, T. Darrell, P. Abbeel, A. Gupta, and J. Andreas. Guiding pretraining in reinforcement learning with large language models. In International Conference on Machine Learning, 2023.
[46] C. Colas, L. Teodorescu, P.-Y. Oudeyer, X. Yuan, and M.-A. Côté. Augmenting autotelic agents with large language models. In Conference on Lifelong Learning Agents, 2023.
[47] C. Colas, T. Karch, N. Lair, J.-M. Dussoux, C. Moulin-Frier, F. P. Dominey, and P.-Y. Oudeyer. Language as a cognitive tool to imagine goals in curiosity driven exploration. In Neural Information Processing Systems, 2020.
[48] I. Kostrikov, A. Nair, and S. Levine. Offline reinforcement learning with implicit q-learning. In International Conference on Learning Representations, 2022.
[49] N. Reimers and I. Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. In Empirical Methods in Natural Language Processing, 2019.
[50] M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi, L. Zettlemoyer, and D. Fox. ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. In Computer Vision and Pattern Recognition, 2020.</p>
<p>[51] A. Pashevich, C. Schmid, and C. Sun. Episodic Transformer for Vision-and-Language Navigation. In ICCV, 2021.
[52] M. Laskin, H. Liu, X. B. Peng, D. Yarats, A. Rajeswaran, and P. Abbeel. CIC: Contrastive intrinsic control for unsupervised skill discovery, 2022.
[53] M. Laskin, D. Yarats, H. Liu, K. Lee, A. Zhan, K. Lu, C. Cang, L. Pinto, and P. Abbeel. Urlb: Unsupervised reinforcement learning benchmark, 2021.
[54] R. Agarwal, M. Schwarzer, P. S. Castro, A. Courville, and M. G. Bellemare. Deep reinforcement learning at the edge of the statistical precipice. In Neural Information Processing Systems, 2021.
[55] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient foundation language models, 2023.
[56] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, T. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig, P. S. Koura, A. Sridhar, T. Wang, and L. Zettlemoyer. Opt: Open pre-trained transformer language models. 2022.
[57] A. Gupta, J. Yu, T. Z. Zhao, V. Kumar, A. Rovinsky, K. Xu, T. Devlin, and S. Levine. Reset-free reinforcement learning via multi-task learning: Learning dexterous manipulation behaviors without human intervention. In IEEE International Conference on Robotics and Automation. IEEE, 2021.
[58] A. Sharma, A. Gupta, S. Levine, K. Hausman, and C. Finn. Autonomous reinforcement learning via subgoal curricula. In Neural Information Processing Systems, 2021.
[59] A. Sharma, S. Gu, S. Levine, V. Kumar, and K. Hausman. Dynamics-aware unsupervised discovery of skills. In International Conference on Learning Representations, 2020.
[60] X. B. Peng, A. Kumar, G. Zhang, and S. Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning, 2019.
[61] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition, 2016.
[62] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. 2016.
[63] P. J. Ball, L. Smith, I. Kostrikov, and S. Levine. Efficient online reinforcement learning with offline data. In International Conference on Machine Learning, 2023.
[64] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta. R3m: A universal visual representation for robot manipulation. In Conference on Robot Learning, 2022.
[65] T. Z. Zhao, V. Kumar, S. Levine, and C. Finn. Learning fine-grained bimanual manipulation with low-cost hardware. arXiv preprint arXiv:2304.13705, 2023.
[66] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani, S. Brahma, A. Webson, S. S. Gu, Z. Dai, M. Suzgun, X. Chen, A. Chowdhery, A. Castro-Ros, M. Pellat, K. Robinson, D. Valter, S. Narang, G. Mishra, A. Yu, V. Zhao, Y. Huang, A. Dai, H. Yu, S. Petrov, E. H. Chi, J. Dean, J. Devlin, A. Roberts, D. Zhou, Q. V. Le, and J. Wei. Scaling instruction-finetuned language models. 2022.
[67] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. Llm.int8(): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.</p>
<h1>Appendix</h1>
<h2>Algorithm 2 BOSS Algorithm</h2>
<p>Require: Dataset $\mathcal{D}^{L} \mathrm{w} /$ language labels, LLM, Skill Library $Z$, Time limit $T$, max chain length $M$
1: Pre-train policy $\pi(a \mid s, z)$, value function $V(s, z)$ on $\mathcal{D}^{L}$ with offline RL. $\triangleright$ Section 3.1
2: while not converged do
3: SKILLBOOTSTRAPPING(V, Z, LLM, $\pi, \mathcal{D}^{L}, M, T$ ) $\triangleright$ Section 3.2
4:
5: procedure SKILLBOOTSTRAPPING(V, Z, LLM, $\pi, \mathcal{D}^{L}, M, T$ )
6: $\quad s_{1} \leftarrow$ Reset environment
7: RolloutData $\leftarrow[]$
8: $\quad z \leftarrow$ sample from discrete distribution with probs $\left[V\left(s, z_{1}\right), V\left(s, z_{2}\right), \ldots, V\left(s, z_{|Z|}\right)\right]$.
9: $\quad i \leftarrow 0$
10: Success $\leftarrow$ True
11: while $i&lt;M$ and Success do $\quad \triangleright$ If a rollout fails, break the loop.
12: $\quad i \leftarrow i+1$
13: (Success, $\tau$ ) $\leftarrow$ Rollout $\pi(\cdot \mid s, z)$ in Environment for at most $T$ steps.
14: Add $\tau$ to RolloutData
15: if Success then
16: $\quad z \leftarrow$ SAMPLENEXTSKILL(LLM, RolloutData, $Z$ )
17: UpdateBufferANDSKILLREPERTOIRE( $\mathcal{D}^{L}$, RolloutData, LLM)
18: Train $\pi, V$ on $\mathcal{D}^{L}$ with offline RL.
19:
20: procedure SAMPLENEXTSKILL(LLM, RolloutData, $Z$ )
21: AllSkills $\leftarrow$ extract all skill annotations from $Z$.
22: SkillChain $\leftarrow$ extract executed primitive skills from RolloutData.
23: Prompt $\leftarrow$ construct prompt from AllSkills, SkillChain. $\triangleright$ Prompt in Figure 8.
24: $\quad\left(\left[\hat{z}<em N="N">{1}, \ldots, \hat{z}</em>$.
25: Find closest match in $Z$ to each of $\hat{z}}\right],\left[p_{1}, \ldots, p_{N}\right]\right) \leftarrow$ Sample $N$ text generations from LLM(Prompt) with average token probabilities $p_{1}, \ldots, p_{N<em N="N">{1}, \ldots, \hat{z}</em>$ in embedding space $\quad \triangleright$ Embedding model: all-mpnet-base-v2 from Reimers and Gurevych [49].
26: $\quad z \leftarrow$ sample the matches in $Z$ from categorical distribution with parameters $p_{1}, \ldots, p_{N}$.
27: return $z$
28:
29: procedure UpdateBufferANDSKILLREPERTOIRE( $\mathcal{D}^{L}$, RolloutData, $Z$, LLM) $\triangleright$ See Appendix B. 3 for details.
30: $\quad \tau_{1}, \ldots, \tau_{k} \leftarrow$ extract primitive skill trajectories from RolloutData.
31: for $\tau_{i}$ in $\tau_{1}, \ldots, \tau_{k}$ do
32: $\quad \mathcal{D}^{L} \leftarrow \mathcal{D}^{L} \cup\left{\tau_{i, z_{i}}\right} \quad \triangleright$ Add trajectory to $\mathcal{D}^{L}$ with annotation $z_{i}$.
33: $\quad \tau_{1: k} \leftarrow$ concatenate all trajectories together
34: $\quad z_{L L M, 1: k} \leftarrow L L M\left(\tau_{1: k}\right)$ assign name by asking LLM summarize annotations of $\tau_{1: k}$. $\triangleright$ See Appendix A. 2 for prompt.
35: $\quad z_{\text {concat }, 1: k} \leftarrow \text { " }\left{z_{1}\right} \cdot\left{z_{2}\right} \ldots\left{z_{k}\right} .{ }^{\prime} \quad \triangleright$ Assign another label for the trajectory by concatenating primitive skill annotations.
36: $\quad \mathcal{D}^{L} \leftarrow \mathcal{D}^{L} \cup\left{\tau_{L L M, 1: k}, \tau_{\text {concat }, 1: k}\right} \quad \triangleright$ Add to $\mathcal{D}^{L}$ with annotation $z_{L L M, 1: k}$ and $z_{\text {concat }, 1: k}$.
37: Add $z_{L L M, 1: k}$ as a new skill to $Z$.</p>
<h1>A Dataset and Environment Details</h1>
<h2>A. 1 ALFRED</h2>
<h2>A.1.1 Dataset Details</h2>
<p>We base our dataset and environment on the ALFRED benchmark [50]. ALFRED originally contains over 6000 full trajectories collected from an expert planner following a set of 7 high-level tasks with randomly sampled objects (e.g., "pick an object and heat it"). Each trajectory has three crowdsourced annotations, resulting in around 20k distinct language-annotated trajectories. We separate these into only the primitive skill trajectories, resulting in about 141 k language-annotated trajectories. Following Zhang et al. [42], we merge navigation skills (e.g., "Walk to the bed") with the skill immediately following them as these navigation skills make up about half of the dataset, are always performed before another skill, and are difficult to design online RL reward functions for that work across all house floor plans given only the information in the dataset for these skills. After this processing step, the resulting dataset contains 73 k language-annotated primitive skill trajectories.</p>
<h2>A.1.2 RL Environment Details</h2>
<p>We modified ALFRED similarly to Zhang et al. [42], Pashevich et al. [51] to make it suitable for policy learning by modifying the action space to be fully discrete, with 12 discrete action choices and 82 discrete object types.
Furthermore, we rewrote reward functions for all primitive skill types ("CoolObject", "PickupObject", "PutObject", "HeatObject", "ToggleObject", "SliceObject", "CleanObject") so that rewards can be computed independently of a reference expert trajectory. While our rewards depend on the ground truth primitive skill type, no agents are allowed access to what the underlying true primitive skill type is. All of our reward function are sparse, with 1 for a transition that completes primitive skill and 0 for all other transitions.</p>
<h2>A.1.3 Evaluation Tasks</h2>
<p>We generate evaluation tasks by randomly sampling 10 tasks each for 4 unseen ALFRED floor plans, resulting in 40 total tasks unseen tasks requiring anywhere from 2-8 primitive skills to complete. The tasks for each floor plan are sampled randomly from the VALID-UNSEEN ALFRED dataset collected in these plans with the specific object arrangements, and we use the high-level task language descriptions collected by humans for ALFRED as our task descriptions for language-conditioned zero-shot evaluation. See Figure 7 for a histogram of task lengths.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 7: Task lengths regarding the number of primitive skills needed to chain together to solve the task.</p>
<p>Examples of common household tasks and their descriptions:
Task Steps: 1. Pick up the keys on the center table. 2. Put the keys in the box. 3. Pick up the box with keys. 4. Put the box with keys on the sofa close to the newspaper.
Task: Put the box with keys on the sofa.
Task Steps: 1. Pick up the knife from in front of the tomato. 2. Cut the lettuce on the counter. 3. Set the knife down on the counter in front of the toaster. 4. Pick up a slice of the lettuce from the counter. 5. Put the lettuce slice in the refrigerator. take the lettuce slice out of the refrigerator. 6. Set the lettuce slice on the counter in front of the toaster.
Task: Put a cooled slice of lettuce on the counter.
Task Steps: 1. Pick up the book on the table, in front of the chair. 2. Place the book on the left cushion of the couch.
Task: Put a book on the couch.
Task Steps: 1. Pick up the fork from the table. 2. Put the fork in the sink and fill the sink with water, then empty the water from the sink and remove the fork. 3. Put the fork in the drawer.
Task: Put the cleaned fork in a drawer.
Task Steps: 1. Take the box of tissues from the makeup vanity. 2. Put the tissues on the barred rack. 3. Take the box of tissues from the top of the toilet. 4. Put the tissues on the barred rack.
Task: Put the box of tissues on the barred rack.
Task Steps: 1. Pick up the glass from the sink. 2. Heat the glass in the microwave. 3. Put the glass on the wooden rack.
Task: Put a heated glass on the wooden rack.
Task Steps: 1. Pick up the box from the far side of the bed. 2. Hold the box and turn on the lamp. Tasks: Look at the box under the lamp light.</p>
<p>Predict the next skill correctly by choosing from the following skills: [SKILL 1 IN LIBRARY], [SKILL 2 IN LIBRARY], ...</p>
<p>Task Steps: 1. [SKILL 1 EXECUTED SO FAR] 2. [SKILL 2 EXECUTED SO FAR] ... N. $\qquad$
Figure 8: Prompt for the LLM for next skill proposal (Section 3.2). Text is generated after listing out all skills completed so far.</p>
<h1>A. 2 Language Model Prompts</h1>
<p>We use two prompts when using the LLM for two different purposes. The main purpose of the LLM is to propose a distribution over next skills to chain with currently executed skills during skill bootstrapping (Section 3.2). Thus, we pass skills in the given skill library $Z$ into the prompt and ask it to predict the next skill. We also include a fixed set of 7 in-context examples from a random sample of different tasks from the ALFRED training dataset. The prompt for bootstrapping is shown in Figure 8.</p>
<p>We also generate summaries (see Section 3.2 and appendix Appendix B.3) for composite skill annotations with the LLM. These summaries are used to label newly chained longer-horizon skills before adding them back to the skill library. We show the prompt for this in Figure 9.</p>
<h2>B Training Implementation details and Hyperparameters</h2>
<p>We implement IQL [48] as the base offline RL algorithm to pre-train on primitive skill data for all methods, baselines, and ablations, due to its strong offline and finetuning performance on a variety of dense and sparse reward environments.
The IQL policy is trained to maximize the following objective:</p>
<p>$$
e^{\beta(Q(s, a)-V(s))} \log \pi(a \mid s)
$$</p>
<p>which performs advantage-weighted regression [60] with an inverse temperature term $\beta$. $Q$ and $V$ are trained on $\left(s, a, s^{\prime}, r, a^{\prime}\right)$ tuples from the dataset rather than sampling a policy for $a^{\prime}$ to mitigate</p>
<p>Instructions: give a high-level description for the following steps describing common household tasks.
Task Steps: 1. Pick up the keys on the center table. 2. Put the keys in the box. 3. Pick up the box with keys. 4. Put the box with keys on the sofa close to the newspaper.
Summary: Put the box with keys on the sofa.
Task Steps: 1. Pick up the knife from in front of the tomato. 2. Cut the lettuce on the counter. 3. Set the knife down on the counter in front of the toaster. 4. Pick up a slice of the lettuce from the counter. 5. Put the lettuce slice in the refrigerator. take the lettuce slice out of the refrigerator. 6. Set the lettuce slice on the counter in front of the toaster.
Summary: Put a cooled slice of lettuce on the counter.
Task Steps: 1. Pick up the book on the table, in front of the chair. 2. Place the book on the left cushion of the couch.
Summary: Put a book on the couch.
Task Steps: 1. Pick up the fork from the table. 2. Put the fork in the sink and fill the sink with water, then empty the water from the sink and remove the fork. 3. Put the fork in the drawer.
Summary: Put the cleaned fork in a drawer.
Task Steps: 1. Take the box of tissues from the makeup vanity. 2. Put the tissues on the barred rack. 3. Take the box of tissues from the top of the toilet. 4. Put the tissues on the barred rack.
Summary: Put the box of tissues on the barred rack.
Task Steps: 1. Pick up the glass from the sink. 2. Heat the glass in the microwave. 3. Put the glass on the wooden rack.
Summary: Put a heated glass on the wooden rack.
Task Steps: 1. Pick up the box from the far side of the bed. 2. Hold the box and turn on the lamp.
Summary: Look at the box under the lamp light.
Task Steps: 1. [SKILL 1] 2. [SKILL 2] 3. [SKILL 3] ...
Summary:
Figure 9: Prompt for the LLM to summarize completed skills into high-level composite annotations, following Zhang et al. [42].
issues with critic function overestimation common in offline RL. We detail shared training and implementation details below, with method-specific information and hyperparameters in the following subsections.</p>
<h1>B. 1 ALFRED Environment</h1>
<p>We implement the same observation and action space as Zhang et al. [42]. Details are listed below.
Observation space. The observations given to agents are $300 \times 300$ RGB images. For all methods, we first preprocess these images by sending them through a frozen ResNet-18 encoder [61] pretrained on ImageNet, resulting in a $512 \times 7 \times 7$ observation.
Action space. The agent chooses from 12 discrete low-level actions. There are 5 navigation actions: MoveAhead, RotateRight, RotateLeft, LookUp, and LookDown and 7 interaction actions: Pickup, Put, Open, Close, ToggleOn, ToggleOff, and Slice. For interaction actions the agent additionally selects one of 82 object types to interact with, as defined by Pashevich et al. [51]. In total, the action space consists of $5+7 * 82=579$ discrete action choices. For all methods, due to the large discrete action space, we perform the same action masking as Zhang et al. [42] to prevent agents from taking actions that are not possible by using ground truth object properties given by the ALFRED simulator for each object in the scene. For example, we do not allow the agent to Close objects that aren't closeable or ToggleOn objects that can't be turned on.</p>
<p>Policy and critic networks. We use the transformer architecture (and hyperparameters) used by Episodic Transformers (ET) [51] for our policy and critic networks. We implement all critics (two $Q$ functions and one $V$ ) with a shared backbone and separate output heads. Additionally, we use LayerNorms [62] in the MLP critic output heads as recommended by Ball et al. [63]. All networks condition on tokenized representations of input language annotations.
Hyperparameters. Hyperparameters were generally selected from tuning the Oracle baseline to work as best as possible, then carried over to all other methods. Shared hyperparameters for all methods (where applicable) for pre-training on primitive skills are listed below. Any unlisted hyperparameters or implementation details are carried over from Pashevich et al. [51]:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Param</th>
<th style="text-align: left;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Batch Size</td>
<td style="text-align: left;">64</td>
</tr>
<tr>
<td style="text-align: left;"># Training Epochs</td>
<td style="text-align: left;">150</td>
</tr>
<tr>
<td style="text-align: left;">Learning Rate</td>
<td style="text-align: left;">$1 \mathrm{e}-4$</td>
</tr>
<tr>
<td style="text-align: left;">Optimizer</td>
<td style="text-align: left;">AdamW</td>
</tr>
<tr>
<td style="text-align: left;">Dropout Rate</td>
<td style="text-align: left;">0.1</td>
</tr>
<tr>
<td style="text-align: left;">Weight Decay</td>
<td style="text-align: left;">0.1</td>
</tr>
<tr>
<td style="text-align: left;">Discount $\gamma$</td>
<td style="text-align: left;">0.97</td>
</tr>
<tr>
<td style="text-align: left;">Q Update Polyak Averaging Coefficient</td>
<td style="text-align: left;">0.005</td>
</tr>
<tr>
<td style="text-align: left;">Policy and Q Update Period</td>
<td style="text-align: left;">1 per train iter</td>
</tr>
<tr>
<td style="text-align: left;">IQL Advantage Clipping</td>
<td style="text-align: left;">$[0,100]$</td>
</tr>
<tr>
<td style="text-align: left;">IQL Advantage Inverse Temperature $\beta$</td>
<td style="text-align: left;">5</td>
</tr>
<tr>
<td style="text-align: left;">IQL Quantile $\tau$</td>
<td style="text-align: left;">0.8</td>
</tr>
<tr>
<td style="text-align: left;">Maximum Observation Context Length</td>
<td style="text-align: left;">21</td>
</tr>
</tbody>
</table>
<p>When fine-tuning policies (for Oracle, CIC, and BOSS), we keep hyperparameters the same. We fine-tune one policy per floor plan (zero-shot evaluating on 10 tasks in each floor plan) in our ALFRED task set so that the aggregated results are reported over 4 runs per seed. For methods that use a skill library (BOSS, Saycan, Saycan+P), all available primitive skills across all evaluation tasks in each floor plan compose the starting skill library, resulting in anywhere from 15-40 available skills depending on the floor plan.
Additionally, when finetuning the Oracle baseline along with BOSS and its ablations, we sample old data from the offline dataset and newly collected data at equal proportions in the batch, following suggestions from [63]. We do not do this for CIC when finetuning with its unsupervised RL objective because the language embeddings from the old data are not compatible with the online collected data labeled with CIC-learned skill embeddings. Fine-tuning hyperparameters follow:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Param</th>
<th style="text-align: left;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"># Initial Rollouts</td>
<td style="text-align: left;">50</td>
</tr>
<tr>
<td style="text-align: left;"># Training Steps to Env Rollouts Ratio</td>
<td style="text-align: left;">15</td>
</tr>
<tr>
<td style="text-align: left;">$\epsilon$ in $\epsilon$-greedy action sampling</td>
<td style="text-align: left;">0.05</td>
</tr>
<tr>
<td style="text-align: left;">Discrete action sampling</td>
<td style="text-align: left;">True</td>
</tr>
<tr>
<td style="text-align: left;"># Parallel Rollout Samplers</td>
<td style="text-align: left;">10</td>
</tr>
</tbody>
</table>
<h1>B. 2 Real Robot Environment</h1>
<p>The input observation from the environment includes environment RGB input and robot states. The RGB input consists of the third-person view RGB images from a Logitech Pro Webcam C920 cropped to $224 \times 224 \times 3$, and wrist view images from an Intel RealSense D435. We use a pretrained R3M [64] model to get the latent representation for each view. The robot states include the robot's end-effector position, velocity, and gripper state. The end-effector position and velocity are two continuous vectors, and the gripper state is a one-hot vector, which presents OPEN, CLOSE, or NOT MOVE. We concatenate the RGB latent representations and robot states together as environment states.</p>
<p>The policy is language conditioned, and we use a pre-trained sentence encoder to encode the language annotation to a 384-dimensional latent vector. The pretrained sentence encoder we use is all-MiniLM-L12-v2 from the SentenceTransformers package [49].
The total state input dimension is 2048 (third-person R3M) + 2048 (wrist R3M) + 15 (robot state input) +384 (language latent representation) $=4495$.</p>
<p>Action space. The action space of the robot encompasses the difference in the end effector position between each time step, along with discrete open and close signals for the gripper. These actions are transmitted to the robot with 10 HZ and interpreted as desired joint poses using PyBullet's inverse kinematics module.
In line with [65], we adopt the Action Chunking method to train an autoregressive policy. Our policy utilizes an LSTM model to predict the next 15 actions, given the initial observation as input, denoted as $\pi\left(a_{t: t+15} \mid s_{t}\right)$. Both our Q and Value networks are recurrent as well, estimating rewards on a per-timestep basis for each action in the sequence. Similar to the policy, these networks only have access to the observation preceding the action sequence initiation.
Due to the gripper action space is discrete and imbalanced distributed in the dataset, we reweigh gripper loss inversely proportionally to the number of examples in each class.</p>
<h1>B. 3 Additional BOSS Implementation Details</h1>
<p>Here we continue discussion of BOSS in detail. In the main text in Section 3.2 we mention that we add learned skills back to the agent's skill repertoire and then train on collected experience gathered from each rollout. Here, we detail exactly how we do that.</p>
<p>Labeling new composite skills. Finally, after we have finished attempting a composite skill chain, we need a natural language description for it so we can train the language-conditioned policy on this new composite skill. We ask the LLM to generate high-level task descriptions of the annotations of the two skills the agent has just attempted to chain together like proposed by Zhang et al. [42] for offline policy pre-training. Doing so will allow the agent to learn skills at a higher level of text abstraction, allowing the agent to operate on more natural evaluation task specifications. For example, humans are more likely to ask an agent to "Make coffee" than to say "Get a coffee pod. Put the coffee pod in the machine. Fill it up with water..."
We give the LLM a prompt similar to the one for generating next skills. For example, if our agent has just completed two skills: "Pick up the spoon", "Put the spoon on the counter", we ask the LLM to summarize "1. PICK UP THE SPOON. 2. PUT THE SPOON ON THE COUNTER.", and the LLM can generate "put a spoon on the counter." We denote the generated language annotation for this combined skill composed of the annotations of $z^{1}$ and $z^{2}$ as $z^{\prime}$. We then add $z^{\prime}$ as a new composite skill to $Z$ for the agent to possibly sample from again.</p>
<p>Training on new skill data. After the agent has finished a rollout in the environment, it trains on the experience gathered. There are three types of data that we add to the agent's replay buffer from its rollout data:</p>
<ol>
<li>The trajectory of the attempted skill chain which is collected only if the entire first skill is successfully executed (regardless if it is a primitive skill or a chain of them) since only then will another skill be used for chaining. The label for this trajectory is produced by the LLM.</li>
<li>
<p>The trajectory of the composite skill but with a label generated by concatenating the primitive skill annotations as a sequence of sentences of their language annotations. This trajectory ensures that the agent receives a description for the collected composite trajectory that specifies the exact primitive skills that make it up, in order. This is useful because the LLM-generated high-level skill description may not describe certain steps. Those steps are explicitly spelled out in this new label.</p>
</li>
<li>
<p>Trajectories for all lowest-level primitive skills executed during the rollout. These correspond to the original set of skills the policy was equipped with and will help the policy continue to finetune its ability to execute its original primitive skills.</p>
</li>
</ol>
<p>After the rollout, we add these trajectories to the agent's replay buffer.</p>
<p>Other details. When performing skill bootstrapping in the ALFRED environment, we set a max time limit ( $T$ in Algorithm 2) for 40 timesteps per primitive skill. For simplicity, we restrict $M$, the max number of skills to chain, to be 2 during skill bootstrapping rollouts. We also restrict the second skill to be chained to only the set of primitive skills so that the agent can only learn new skill chains that are one primitive skill longer than the first sampled skill. Note that this does not restrict the agent from sampling composite skills it has learned during bootstrapping as first skills upon initialization.
One final implementation detail is with respect to how we map LLM next skill proposals to existing skills in the skill library $Z$. We found that pre-trained sentence embedding models generally seem to put great emphasis on the nouns of skill annotation sentences in ALFRED, instead of the verb. Therefore, all sentence embeddings models we initially experimented with (up to the 11B parameter model FLAN-T5-XXL [66]) would have a tendency to map LLM generations such as "Place the apple in the sink" to skills with different verbs as long as the nouns were the same, such as "Pick up the apple from the sink". These skills are clearly very different, so this presented a problem to us initially. To solve this, we settled on using an NLP library ${ }^{2}$ to extract the main verb of sentences and then added that same verb as a prefix to each sentence before embedding with the sentence embedding model. For example, "Place the apple in the sink" $\rightarrow$ "PLACE: Place the apple in the sink." With this change, the aforementioned issue was addressed in most cases and we could use much smaller sentence embedding models (all-mpnet-v2 from the SentenceTransformers package [49]).</p>
<p>Training Time and Hardware Requirements We perform experiments on a server with 2 AMD EPYC 7763 64-Core Processors, and 8 RTX 3090 GPUs. Pre-training the policies takes around 10 hours with just a single RTX 3090 and 4 CPU threads for parallel dataloading.
Skill bootstrapping experiments require just 1 GPU with sufficient VRAM to run inference with our LLM, along with 4 available CPU threads for parallel dataloading and environment rollouts. In practice, a single RTX 3090 is sufficient for our experiments using LLaMA-13B with 8-bit inference [67] on ALFRED, requiring around 3-5 days of training, mainly due to the speed of the underlying simulator used in ALFRED.</p>
<h1>B. 4 CIC Implementation</h1>
<p>For fairness in our experimental comparison, we implement CIC [52] by using its objective to train a policy pre-trained on the same dataset as BOSS; thus, the CIC agent is first initialized with a set of sensible behaviors. Since CIC operates on a fixed latent space, we modified the critic and policy architectures so that they operate on fixed-length, 768-dimensional embeddings of language inputs from the same sentence embedding model used for skill bootstrapping [49] instead of on variable length tokenized language representations.
CIC-specific hyperparameters follow:</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Param</th>
<th style="text-align: left;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CIC K-means K</td>
<td style="text-align: left;">12</td>
</tr>
<tr>
<td style="text-align: left;">CIC K-means avg</td>
<td style="text-align: left;">True</td>
</tr>
<tr>
<td style="text-align: left;">CIC Hidden Dim</td>
<td style="text-align: left;">1024</td>
</tr>
<tr>
<td style="text-align: left;">CIC Latent Skill Dim</td>
<td style="text-align: left;">768</td>
</tr>
<tr>
<td style="text-align: left;">CIC Temp</td>
<td style="text-align: left;">0.5</td>
</tr>
<tr>
<td style="text-align: left;">CIC Skill Projection Layer</td>
<td style="text-align: left;">True</td>
</tr>
<tr>
<td style="text-align: left;"># Timesteps for each skill rollout before reset</td>
<td style="text-align: left;">200</td>
</tr>
</tbody>
</table>
<h1>B. 5 SayCan Implementation</h1>
<p>We implement SayCan [12] by combining the prompt from SayCan with ours. We use the same in-context examples except but convert them to a human-robot conversation. All other details are the same, including the LLM that we use in this comparison (LLaMa-13b [55]). The Saycan prompt follows below:</p>
<p>Robot: Hi there, I'm a robot operating in a house. Robot: You can ask me to do various tasks and I'll tell you the sequence of actions I would do to accomplish your task.
Human: How would you put the box with keys on the sofa?
Robot: 1. Pick up the keys on the center table. 2. Put the keys in the box. 3. Pick up the box with keys. 4. Put the box with keys on the sofa close to the newspaper.</p>
<p>Human: How would you put a cooled slice of lettuce on the counter?
Robot: 1. Pick up the knife from in front of the tomato. 2. Cut the lettuce on the counter. 3. Set the knife down on the counter in front of the toaster. 4. Pick up a slice of the lettuce from the counter. 5. Put the lettuce slice in the refrigerator. take the lettuce slice out of the refrigerator. 6. Set the lettuce slice on the counter in front of the toaster.</p>
<p>Human: How would you put a book on the couch?
Robot: 1. Pick up the book on the table, in front of the chair. 2. Place the book on the left cushion of the couch.</p>
<p>Human: How would you put the cleaned fork in a drawer?
Robot: 1. Pick up the fork from the table. 2. Put the fork in the sink and fill the sink with water, then empty the water from the sink and remove the fork. 3. Put the fork in the drawer.</p>
<p>Human: How would you put the box of tissues on the barred rack?
Robot: 1. Take the box of tissues from the makeup vanity. 2. Put the tissues on the barred rack. 3. Take the box of tissues from the top of the toilet. 4. Put the tissues on the barred rack.</p>
<p>Human: How would you put a heated glass on the wooden rack?
Robot: 1. Pick up the glass from the sink. 2. Heat the glass in the microwave. 3. Put the glass on the wooden rack.</p>
<p>Human: How would you look at the box under the lamp light?
Robot: 1. Pick up the box from the far side of the bed. 2. Hold the box and turn on the lamp.
Predict the next skill correctly by choosing from the following skills: [SKILL 1 IN LIBRARY], [SKILL 2 IN LIBRARY], ...</p>
<p>Human: How would you [HIGH LEVEL TASK DESCRIPTION]?
Robot: 1. [SKILL 1 EXECUTED SO FAR] 2. [SKILL 2 EXECUTED SO FAR] ... N. $\qquad$</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 10: Qualitative visualizations of zero-shot evaluation rollouts. See the plans SayCan+P generated for these two tasks at the top of Figure 12.</p>
<h1>B. 6 ProgPrompt Implementation</h1>
<p>ProgPrompt [14] converts natural language queries to code and executes the code on a real robot. After consulting with the authors, we converted the examples in our prompt to one suitable for ProgPrompt by converting task descriptions into a code representation by converting spaces into underscores, e.g., "Pick up the milk" into def pick_up_the_milk(). Then, to translate code commands into commands suitable for our pre-trained policy, we prompt ProgPrompt to output pick_and_place (object, object) style code commands that we convert into two separate pick and place natural language commands in the same format as the instructions used for pretraining the policy. We then execute these instructions on the real robot in sequence.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://github.com/chartbeat-labs/textacy&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{1}$ Note that we do not treat invalid LLM skill chain proposals, like asking the agent to "put keys in a safe" when it has not yet picked any keys up, in a special manner. If the proposal is poor, the agent will fail and the value of the skill will drop with training, making it unlikely to sample the skill chain again.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
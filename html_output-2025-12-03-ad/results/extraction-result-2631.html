<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2631 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2631</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2631</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-66.html">extraction-schema-66</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <p><strong>Paper ID:</strong> paper-9ea0757c750ab1222a7442d3485a74d1c526b04c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9ea0757c750ab1222a7442d3485a74d1c526b04c" target="_blank">AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation</a></p>
                <p><strong>Paper TL;DR:</strong> Empirical studies demonstrate the effectiveness of the AutoGen framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.</p>
                <p><strong>Paper Abstract:</strong> AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2631.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2631.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoGen (Multi-Agent Conversation Framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source framework for building LLM applications by orchestrating multiple conversable agents (LLM-, tool-, or human-backed) that communicate via programmable multi-agent conversations to solve complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoGen</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>AutoGen is a general multi-agent conversation infrastructure: developers define conversable agents (backed by LLMs, humans, tools, or combinations) and program their interaction behavior using conversation-centric computation and control flow (via natural language prompts, Python code, or hybrid). Key capabilities demonstrated include: orchestrating multi-turn agent conversations, tool and code execution via user-proxy agents, human-in-the-loop and multi-user workflows, dynamic group chat with speaker selection, grounding agents that inject domain knowledge, retrieval-augmented workflows, auto-reply/auto-reply-function registration, and enhanced LLM inference (caching, error handling). It supports building systems that generate solutions (plans, code), execute code/tools, incorporate execution outputs back into reasoning, solicit human input, and iteratively debug.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Multi-Agent Conversational LLM Application Framework</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Machine learning / NLP systems engineering, mathematics problem solving, code generation and analysis, retrieval-augmented QA, interactive decision-making (text-world), games (conversational interfaces), operations research (supply-chain optimization workflows).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Framework used to construct automated multi-agent systems to solve: (1) hard math problems (MATH dataset, including level-5 problems), (2) retrieval-augmented QA and code generation on Natural Questions and custom codebases, (3) interactive decision-making tasks in ALFWorld, (4) multi-agent coding workflows for optimization analysis (OptiGuide), (5) dynamic group chat tasks and conversational chess. Problems involve generating plans/solutions, writing and executing code, retrieving and integrating external context, and avoiding unsafe or incorrect actions.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Varied by application: includes high-complexity symbolic math (MATH level-5 problems), interactive sequential decision tasks (ALFWorld with long action sequences), code-safety detection across a 100-task benchmark, and open-ended dialogic tasks; complexity factors include multi-step reasoning, program synthesis and execution, multi-agent coordination, dynamic branching in conversations, and grounding/commonsense requirements. Quantitative examples: MATH evaluations used 120 level-5 problems and the full 5000-problem test set; ALFWorld experiments used 134 unseen tasks; coding safety dataset contained 100 tasks (50 safe, 50 unsafe).</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Uses existing benchmark datasets (MATH: full test set ~5000 problems; Natural Questions for QA; ALFWorld task set; a curated 100-task coding safety dataset). Data are pre-existing public datasets or curated task sets; execution requires code-execution environment and optional external retrieval indices (vector DB). No new raw scientific data generation was required in these experiments beyond running LLMs and tool invocations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Not quantitatively specified in the paper (no exact FLOPs, GPU-hours or cost provided). Experiments use GPT-4 and GPT-3.5-turbo as backends; number of LLM calls depends on conversation patterns and dynamic interactions (not reported numerically).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Mixed: some tasks are well-defined with exact verification metrics (math problems with exact correctness, QA with reference answers, code-safety detection with F1), others are open-ended or interactive (ALFWorld, games). Problems include discrete actions (ALFWorld moves), continuous/structured outputs (code, mathematical expressions), and stochastic elements due to LLM nondeterminism. Clear evaluation metrics exist for benchmarked tasks (accuracy, F1, task success), while for interactive features qualitative user-experience metrics were used.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Task-dependent: accuracy on MATH problems (percentage correct), QA performance (retrieval/answering accuracy compared to baselines), task completion rate on ALFWorld, F1 for unsafe-code identification in coding tasks, qualitative/time savings and reduced user interactions for user-experience comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Reported quantitative results in paper: MATH overall accuracy: AutoGen 69.48% vs GPT-4 baseline 55.18% (entire test set). On an evaluation subset of 120 level-5 problems AutoGen outperformed alternative approaches (detailed numbers in Figure 4a). ALFWorld: adding a grounding agent produced ~15% average performance gain over the non-grounded variant on 134 unseen tasks. Multi-agent coding (OptiGuide dataset of 100 tasks): multi-agent design improved F1 for unsafe-code detection by 8% with GPT-4 and by 35% with GPT-3.5-turbo, compared to a single-agent approach. User-experience: AutoGen-based OptiGuide reduced core workflow code (430 -> 100 lines), saved ~3x user's time and reduced user interactions by ~3–5x vs ChatGPT+Code Interpreter (reported qualitatively / as approximate).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Observed limitations include: (1) occasional LLM calculation or reasoning errors (e.g., math or numerical mistakes), (2) code-execution pitfalls when external tool invocation or result parsing is imperfect, (3) commonsense or grounding deficits causing loops in sequential decision tasks (mitigated by adding grounding agent), (4) LLM nondeterminism causing inconsistent outputs across trials, (5) safety concerns when agents autonomously execute code or system-level changes, (6) need for careful prompt/system-message design as LLMs do not obey all instructions perfectly.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Key contributors to success: modular multi-agent decomposition (separating writer/safeguard/executor), ability to combine LLMs with tool execution and humans-in-the-loop, conversation programming (natural-language + code control), interactive retrieval (UPDATE CONTEXT mechanism), grounding agents to inject commonsense, and careful default system messages/prompting techniques. Reuse of built-in agent abstractions reduced development effort and improved robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Across evaluated tasks AutoGen-based systems outperformed or matched strong baselines: on MATH AutoGen achieved 69.48% vs GPT-4 55.18%; AutoGen outperformed Multi-Agent Debate, LangChain ReAct, AutoGPT, and ChatGPT variants on targeted math evaluations. In ALFWorld, introducing a grounding agent in an AutoGen system gave a ~15% boost vs the same system without grounding and was better than or similar to ReAct depending on exact variant. In coding safety, multi-agent AutoGen design improved unsafe-code detection F1 by +8% (GPT-4) and +35% (GPT-3.5) relative to a single-agent approach. Retrieval-augmented Chat (AutoGen) with interactive retrieval improved QA vs a non-interactive retrieval baseline (ablation showed interactive retrieval mattered), exact numeric gains are shown in paper figures.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2631.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2631.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval-augmented Chat (AutoGen RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-augmented Chat (AutoGen implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-agent retrieval-augmented generation system built on AutoGen, combining a vector database-backed user-proxy agent and a retrieval-augmented assistant agent with an interactive retrieval control that can request additional retrievals when context is insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Retrieval-augmented Chat</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Implemented in AutoGen as two extended agents: a Retrieval-augmented UserProxy (manages a vector DB with SentenceTransformers/Chroma) and a Retrieval-augmented Assistant (LLM-backed) that inspects retrieved context and issues an "UPDATE CONTEXT" signal to trigger additional retrieval attempts when needed. Supports both question-answering (Natural Questions benchmark) and code-generation on codebases not present in LLM training data.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Retrieval-Augmented Generation (RAG) conversational system</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Open-domain question answering (NLP), code generation using external codebase context.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Answer natural questions by retrieving relevant document passages and integrating them into LLM responses; generate code grounded in a target codebase that lies outside the LLM's pretraining data.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Knowledge-intensive NLP tasks with potentially large retrieval search spaces; QA tasks require finding and integrating small bits of relevant context, sometimes needing multiple retrieval attempts (interactive retrieval). Complexity arises from breadth of knowledge and need for contextual grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Uses existing corpora and vectorized document stores; Natural Questions dataset used for evaluation. Data are pre-existing and retrieved via vector search; availability is high for evaluated datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Not quantified in the paper; involves vector database retrieval calls and multiple LLM inferences when interactive retrieval is invoked.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined QA tasks with reference answers (Natural Questions); discrete retrieval-control decisions (whether to re-query). Deterministic evaluation via standard QA metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>QA accuracy/answering performance compared to DPR+LLM baselines; ablation of interactive retrieval effect.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Paper reports that the interactive retrieval mechanism materially improves performance on QA (Figure 4b) and that ablation (prompting assistant to say "I don't know" instead of "UPDATE CONTEXT") reduces performance; exact numeric deltas are presented in figures but not repeated verbatim in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>When retrieved context lacks necessary information and the assistant does not trigger additional retrievals (non-interactive behavior), answers fail; dependency on retriever quality and vector DB coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Interactive retrieval control integrated via conversational programming; multi-agent separation allowing the assistant to request context updates; use of robust retriever (SentenceTransformers + Chroma).</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Had better QA performance than a baseline DPR+GPT-3.5 setup reported in prior work (Adlakha et al., 2023) according to paper figures; ablation shows interactive retrieval yields meaningful gains versus a non-interactive variant.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2631.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2631.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OptiGuide (AutoGen-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OptiGuide (AutoGen-based multi-agent implementation of OptiGuide)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent coding workflow implemented in AutoGen that coordinates a Commander, Writer, and Safeguard agents to write, check, execute, and interpret code for optimization-analysis questions (supply-chain scenarios).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models for supply chain optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>OptiGuide (AutoGen implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The Commander agent coordinates the Writer (crafts code) and Safeguard (checks code safety). If safeguards pass, the Commander executes code via external Python tools, returns execution outputs to the Writer for interpretation, and iterates if exceptions or security flags occur. The AutoGen implementation reduced workflow code substantially and added modular safeguards.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Multi-agent coding / automated code-generation-and-execution workflow</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Operations research / supply-chain optimization, interactive code generation and interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Answer user questions about optimization solutions (e.g., counterfactuals in supply-chain decisions) by programmatically generating and executing code that queries optimization outputs and interpreting results for users; includes safety checks on generated code.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Requires correct program synthesis, safe execution, integration with optimization outputs, and interpretation; complexity includes debugging loops, security checks, and potential multi-step iteration until a valid answer is produced. Evaluated on a 100-task dataset balanced for safe vs unsafe tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Used a curated dataset of 100 coding tasks (50 safe, 50 unsafe) to assess code-safety detection; problem instances are task templates from operations-research/code evaluation domain.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Not numerically specified; involves multiple LLM calls for Writer/Safeguard/Commander interactions and code execution time dependent on the executed code.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined coding tasks with binary safety labels for evaluation (safe vs unsafe), and final interpretive numeric outputs for user-facing answers. Deterministic in terms of code execution but non-deterministic from LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>F1 for unsafe-code identification; user-experience metrics (developer code lines, time savings, number of interactions).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Multi-agent design improved F1 for identifying unsafe code by +8% when using GPT-4 and by +35% when using GPT-3.5-turbo versus a single-agent approach on the 100-task dataset. AutoGen-based OptiGuide reduced implementation code from ~430 to ~100 lines and reportedly saved ~3x user time and reduced user interactions by ~3–5x compared to ChatGPT+Code Interpreter (user-experience estimate provided in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Single-agent approaches missed safety checks and had substantially lower unsafe-code detection F1; failures also arise if Safeguard misclassifies or if Writer produces code that the execution environment cannot run (exceptions).</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Explicit separation of writer and safety-checker agents, automated code execution pipelines, and iterative debugging loops orchestrated by the Commander agent.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Multi-agent >> single-agent for safety detection F1 ( +8% GPT-4, +35% GPT-3.5 ); AutoGen-based workflow showed substantial developer productivity gains versus a hand-built workflow and improved user interaction metrics versus ChatGPT+Code Interpreter.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2631.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2631.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-Agent Debate</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Encouraging divergent thinking in large language models through multiagent debate</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent debate approach where multiple LLM-based agents take affirmative/negative roles and debate solutions to encourage divergent thinking and improve reasoning or factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Encouraging divergent thinking in large language models through multiagent debate</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multi-Agent Debate</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructs multiple LLM inference instances as debating agents (e.g., affirmative, negative, moderator) that exchange arguments to converge on solutions; in the paper the authors adapted/ran this approach for math problem solving comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Multi-Agent Debate System</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Mathematics problem solving (MATH dataset) in the paper's evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Applied to multi-step math problems to see if debate among agents yields better correctness versus single-agent baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Evaluated on level-5 (hard) math problems from MATH; tasks require symbolic reasoning and numeric correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Used MATH problem instances (two qualitative problems tested three times each; larger quantitative tests reported elsewhere but in this paper Multi-Agent Debate was assessed on MATH subset).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Not provided numerically in the paper for this method (number of LLM calls increases with number of debating agents).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined math problems with deterministic correct answers for evaluation; inter-agent debate is structured but still relies on LLM outputs that are stochastic.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Correctness on math problems (counts of correct trials out of trials).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>In the qualitative 2-problem trial (3 runs each) reported in Table 2 Multi-Agent Debate achieved 0/3 correct on both example problems; in broader quantitative comparisons it underperformed AutoGen and GPT-4 baselines on MATH (paper summary indicates Multi-Agent Debate underperformed).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Calculation errors, producing multiple different wrong answers across runs; debate did not prevent arithmetic/logic mistakes in these cases.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Not observed in this paper's math experiments; the framework idea may help divergence but requires complementary tool-use or grounding to fix calculation errors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Underperformed AutoGen and GPT-4 in MATH experiments; produced inconsistent/wrong outputs where AutoGen's multi-agent conversation plus tool-execution achieved higher correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2631.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2631.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LangChain ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LangChain ReAct (ReAct-style single-agent with tool use)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A LangChain implementation of the ReAct paradigm where an LLM interleaves reasoning (chain-of-thought-like) and actions (tool/function calls) within a single-agent framework.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LangChain ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Single-agent architecture using ReAct prompting to alternate between reasoning tokens and tool invocations; used as a baseline for math problem solving and as a building block for some online decision-making comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Single-Agent LLM + Tooling system (ReAct)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Mathematics problem solving (MATH), interactive decision tasks (used as baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Solve multi-step math problems by generating code or queries and executing tools when necessary; used out-of-the-box LangChain ReAct Python agent with handle_parsing_errors=True in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Tested on level-5 MATH problems and typical tool-augmented tasks requiring parsing and code execution.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Benchmarks from MATH; standard datasets for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Not specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined math tasks with deterministic correct answers; single-agent tool loop, no multi-agent conversation.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Correctness on MATH problems.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>In the qualitative MATH trials reported in Table 2, LangChain ReAct produced 0/3 correct on each of the two example problems (failed consistently in those trials). In ALFWorld-like decision tasks, ReAct achieved similar performance to a two-agent AutoGen variant when integrated similarly (paper reports comparable performance in such settings).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Produced incorrect answers in math tasks, parsing and execution errors requiring additional handling.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Combines reasoning with tool use in a single agent; can work well when parsing/execution is reliable.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Underperformed AutoGen out-of-the-box on the evaluated math problems; comparable to two-agent AutoGen variants in some online decision tasks when ReAct techniques were integrated.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2631.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2631.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT + Code Interpreter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT with Code Interpreter (OpenAI feature)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A ChatGPT product mode that allows the model to execute Python code (a code-execution sandbox) to aid calculation, data processing, and programmatic steps in problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatGPT + Code Interpreter</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An interactive ChatGPT interface augmented with an execution environment that runs model-suggested Python code and returns outputs for further reasoning; used as a strong commercial baseline for math and coding tasks in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Interactive LLM + execution environment (commercial product)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Mathematics problem solving, code-based problem solving, data analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Solve math problems by generating and executing Python code to compute intermediate results; used in human-in-the-loop and autonomous comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Applied to level-5 math problems; complexity arises from symbolic math and careful numeric/programmatic handling.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Operates on the input problem; no dataset training/specific dataset constraints for usage; evaluated on MATH and example problems.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Execution environment is provided by the service; not quantified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined math tasks with ground-truth answers.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Correctness on example problems and in 3-trial qualitative tests; success with/without human hints.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>In the qualitative evaluation: first example problem 2/3 correct, second example problem 0/3 correct (Table 2). In the human-in-the-loop Scenario 2 for a selected hard problem, ChatGPT+Code Interpreter solved the problem in 2/3 trials.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Occasional wrong numerical results, failures to follow human hints in some trials, and inconsistent decisions between trials.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Direct code execution capability helps verify calculations and produce precise outputs when code is correct.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Performed better than many open-source baselines in some cases but was outperformed by AutoGen's specialized multi-agent setup on the MATH benchmark overall (AutoGen 69.48% vs GPT-4 baseline; ChatGPT+Code Interpreter not reported on full set).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2631.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2631.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT + Wolfram Plugin</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT with Wolfram Alpha plugin (ChatGPT+Plugin)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT augmented with the Wolfram Alpha plugin to query a computation engine for precise mathematical results, used as a commercial baseline in math problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatGPT + Plugin (Wolfram Alpha)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ChatGPT issues Wolfram Alpha queries for computation-heavy subproblems and integrates returned results into answers. Used in evaluations as a commercial competitor for math problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM + external computation plugin (commercial)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Mathematics problem solving (symbolic and numeric computation).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Leverage a computational engine for intermediate calculations in multi-step math problems; evaluate correctness of final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Level-5 math problems; complexity arises when multiple plausible simplifications or interpretations of Wolfram output exist.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Depends on Wolfram Alpha's engine and knowledge base; used on selected MATH problems in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>External plugin handles computation; not quantified in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined math problems.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Correctness on example problems and in repeated trials.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Qualitative trials: first example problem 1/3 correct, second example problem 1/3 correct (Table 2). In the human-in-the-loop Scenario 2, ChatGPT+Plugin solved the selected hard problem in 2/3 trials (paper text).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>When Wolfram returns multiple simplified results or ambiguous outputs, the LLM may choose the incorrect one; can get stuck in query loops in some conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Access to a strong symbolic/numeric computation engine for exact calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Less reliable than AutoGen on the evaluated math tasks; suffered from decision/selection errors even when the correct computation was available from Wolfram.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2631.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2631.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoGPT (open-source autonomous agent project)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source single-agent system that attempts to autonomously achieve a goal by chaining LLM-generated tasks and using tools; evaluated as a baseline in math problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Single-agent pipeline that generates subgoals, plans, and actions using LLM outputs and external tools; the out-of-the-box AutoGPT instance was initialized with purpose 'solve math problems' for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Autonomous single-agent task decomposition system</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>General automated task completion; evaluated on mathematics problem solving in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Attempted to autonomously solve MATH benchmark problems by generating code and plans; external code execution results were expected to be used but AutoGPT in experiments produced code lacking print statements and failed to integrate execution outputs properly.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Level-5 math problems in experiments; requires precise code generation and execution handling.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Operates on input problems; no benchmark-specific training reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Not specified numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Autonomous goal-driven task decomposition; single-agent, static workflow.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Correctness on math example problems.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Qualitative trials: AutoGPT scored 0/3 on each of the two example MATH problems (Table 2). In human-in-the-loop Scenario 2, AutoGPT failed to solve the selected hard problem in all three trials.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Generated code without necessary print statements so execution results weren't returned/checked; could not correct code-execution related issues autonomously in the evaluated setup.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>None demonstrated in this paper's math evaluations; AutoGPT requires careful configuration and tool integration to succeed on such tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Underperformed AutoGen and other evaluated baselines on math tasks in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2631.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2631.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CAMEL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CAMEL (Communicative Agents for Role-play)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A communicative-agent framework where role-playing LLMs interact to complete tasks and record conversational behavior; discussed as related work and preliminarily evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Camel: Communicative agents for "mind" exploration of large scale language model society</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CAMEL</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Role-assigned LLM agents conversing to accomplish tasks and analyze agent capabilities; designed for static conversation patterns and not natively supporting tool execution in its original form.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Multi-Agent communicative framework (research project)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>General multi-agent role-play tasks; compared in related work and preliminary tests.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Role-playing agents converse to solve tasks. In preliminary math problem tests reported in this paper, CAMEL did not solve math problems effectively because it lacks code/tool execution capability.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Not experimentally quantified here beyond qualitative observation that CAMEL struggled on math problems.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Static conversation topologies (pre-defined order), limited tool integration.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Lacks native tool/code execution support, causing inability to solve problems requiring computation; static conversation patterns reduce flexibility.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Role-play prompts may help cooperative behavior in contexts where tool execution is not required.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>AutoGen differs by supporting tool usage, human involvement, dynamic conversation patterns, and execution-capable agents; CAMEL was less suitable for the math tasks evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2631.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2631.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetaGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MetaGPT (multi-agent framework for software development)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A specialized multi-agent system assigning GPT agents different software-development roles to collaboratively build software; discussed as related work and preliminarily evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MetaGPT: Meta programming for multi-agent collaborative framework</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MetaGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A multi-agent system specialized for automatic software development where agents assume specialized roles (e.g., architect, coder, tester). In preliminary math tests in this paper, MetaGPT generally did not solve math problems because it attempted to develop software rather than directly produce math solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Specialized multi-agent software-development system</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Automated software development; not designed for math-problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Designed to organize agents to develop software projects; when applied to math problems it typically started to develop a software solution pipeline rather than solving the math problem directly.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Specialized to software development workflows; static conversation patterns in the version evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Mismatch between system specialization (software development) and target task (math solving) resulted in failure to produce direct solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Specialization to software tasks can be beneficial within the intended domain but is a liability when reused outside that domain.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>AutoGen's generic, execution-capable, and dynamic multi-agent design made it more suitable for math problem solving than MetaGPT in preliminary tests.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2631.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2631.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BabyAGI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BabyAGI (task-management multi-agent script)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lightweight multi-agent task-management script that spawns agents for task creation, prioritization, and execution in a static conversation pattern; discussed in related work as an example of static-pattern multi-agent systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BabyAGI</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Implements multiple LLM-based components (task creation, prioritization, execution) in a static pipeline; does not natively support dynamic conversation patterns, rich tool integration, or flexible human involvement as AutoGen does.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Static multi-agent task-management script</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>General task automation and management.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Automates decomposition and execution of tasks in a static, pre-defined pipeline; not evaluated in-depth in this paper for scientific discovery tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Static topology; sequential task creation/prioritization/execution steps.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Static conversation patterns limit flexibility and adaptability in more complex or interactive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Simple pipeline can be adequate for straightforward task automation where dynamic conversation is unnecessary.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>AutoGen supports both static and dynamic conversation patterns and richer tool/human integration, offering broader applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Encouraging divergent thinking in large language models through multiagent debate <em>(Rating: 2)</em></li>
                <li>Improving factuality and reasoning in language models through multiagent debate <em>(Rating: 2)</em></li>
                <li>React: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>MetaGPT: Meta programming for multi-agent collaborative framework <em>(Rating: 2)</em></li>
                <li>Large language models for supply chain optimization <em>(Rating: 2)</em></li>
                <li>AutoGPT <em>(Rating: 1)</em></li>
                <li>CAMEL: Communicative agents for "mind" exploration of large scale language model society <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2631",
    "paper_id": "paper-9ea0757c750ab1222a7442d3485a74d1c526b04c",
    "extraction_schema_id": "extraction-schema-66",
    "extracted_data": [
        {
            "name_short": "AutoGen",
            "name_full": "AutoGen (Multi-Agent Conversation Framework)",
            "brief_description": "An open-source framework for building LLM applications by orchestrating multiple conversable agents (LLM-, tool-, or human-backed) that communicate via programmable multi-agent conversations to solve complex tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "AutoGen",
            "system_description": "AutoGen is a general multi-agent conversation infrastructure: developers define conversable agents (backed by LLMs, humans, tools, or combinations) and program their interaction behavior using conversation-centric computation and control flow (via natural language prompts, Python code, or hybrid). Key capabilities demonstrated include: orchestrating multi-turn agent conversations, tool and code execution via user-proxy agents, human-in-the-loop and multi-user workflows, dynamic group chat with speaker selection, grounding agents that inject domain knowledge, retrieval-augmented workflows, auto-reply/auto-reply-function registration, and enhanced LLM inference (caching, error handling). It supports building systems that generate solutions (plans, code), execute code/tools, incorporate execution outputs back into reasoning, solicit human input, and iteratively debug.",
            "system_type": "Multi-Agent Conversational LLM Application Framework",
            "problem_domain": "Machine learning / NLP systems engineering, mathematics problem solving, code generation and analysis, retrieval-augmented QA, interactive decision-making (text-world), games (conversational interfaces), operations research (supply-chain optimization workflows).",
            "problem_description": "Framework used to construct automated multi-agent systems to solve: (1) hard math problems (MATH dataset, including level-5 problems), (2) retrieval-augmented QA and code generation on Natural Questions and custom codebases, (3) interactive decision-making tasks in ALFWorld, (4) multi-agent coding workflows for optimization analysis (OptiGuide), (5) dynamic group chat tasks and conversational chess. Problems involve generating plans/solutions, writing and executing code, retrieving and integrating external context, and avoiding unsafe or incorrect actions.",
            "problem_complexity": "Varied by application: includes high-complexity symbolic math (MATH level-5 problems), interactive sequential decision tasks (ALFWorld with long action sequences), code-safety detection across a 100-task benchmark, and open-ended dialogic tasks; complexity factors include multi-step reasoning, program synthesis and execution, multi-agent coordination, dynamic branching in conversations, and grounding/commonsense requirements. Quantitative examples: MATH evaluations used 120 level-5 problems and the full 5000-problem test set; ALFWorld experiments used 134 unseen tasks; coding safety dataset contained 100 tasks (50 safe, 50 unsafe).",
            "data_availability": "Uses existing benchmark datasets (MATH: full test set ~5000 problems; Natural Questions for QA; ALFWorld task set; a curated 100-task coding safety dataset). Data are pre-existing public datasets or curated task sets; execution requires code-execution environment and optional external retrieval indices (vector DB). No new raw scientific data generation was required in these experiments beyond running LLMs and tool invocations.",
            "computational_requirements": "Not quantitatively specified in the paper (no exact FLOPs, GPU-hours or cost provided). Experiments use GPT-4 and GPT-3.5-turbo as backends; number of LLM calls depends on conversation patterns and dynamic interactions (not reported numerically).",
            "problem_structure": "Mixed: some tasks are well-defined with exact verification metrics (math problems with exact correctness, QA with reference answers, code-safety detection with F1), others are open-ended or interactive (ALFWorld, games). Problems include discrete actions (ALFWorld moves), continuous/structured outputs (code, mathematical expressions), and stochastic elements due to LLM nondeterminism. Clear evaluation metrics exist for benchmarked tasks (accuracy, F1, task success), while for interactive features qualitative user-experience metrics were used.",
            "success_metric": "Task-dependent: accuracy on MATH problems (percentage correct), QA performance (retrieval/answering accuracy compared to baselines), task completion rate on ALFWorld, F1 for unsafe-code identification in coding tasks, qualitative/time savings and reduced user interactions for user-experience comparisons.",
            "success_rate": "Reported quantitative results in paper: MATH overall accuracy: AutoGen 69.48% vs GPT-4 baseline 55.18% (entire test set). On an evaluation subset of 120 level-5 problems AutoGen outperformed alternative approaches (detailed numbers in Figure 4a). ALFWorld: adding a grounding agent produced ~15% average performance gain over the non-grounded variant on 134 unseen tasks. Multi-agent coding (OptiGuide dataset of 100 tasks): multi-agent design improved F1 for unsafe-code detection by 8% with GPT-4 and by 35% with GPT-3.5-turbo, compared to a single-agent approach. User-experience: AutoGen-based OptiGuide reduced core workflow code (430 -&gt; 100 lines), saved ~3x user's time and reduced user interactions by ~3–5x vs ChatGPT+Code Interpreter (reported qualitatively / as approximate).",
            "failure_modes": "Observed limitations include: (1) occasional LLM calculation or reasoning errors (e.g., math or numerical mistakes), (2) code-execution pitfalls when external tool invocation or result parsing is imperfect, (3) commonsense or grounding deficits causing loops in sequential decision tasks (mitigated by adding grounding agent), (4) LLM nondeterminism causing inconsistent outputs across trials, (5) safety concerns when agents autonomously execute code or system-level changes, (6) need for careful prompt/system-message design as LLMs do not obey all instructions perfectly.",
            "success_factors": "Key contributors to success: modular multi-agent decomposition (separating writer/safeguard/executor), ability to combine LLMs with tool execution and humans-in-the-loop, conversation programming (natural-language + code control), interactive retrieval (UPDATE CONTEXT mechanism), grounding agents to inject commonsense, and careful default system messages/prompting techniques. Reuse of built-in agent abstractions reduced development effort and improved robustness.",
            "comparative_results": "Across evaluated tasks AutoGen-based systems outperformed or matched strong baselines: on MATH AutoGen achieved 69.48% vs GPT-4 55.18%; AutoGen outperformed Multi-Agent Debate, LangChain ReAct, AutoGPT, and ChatGPT variants on targeted math evaluations. In ALFWorld, introducing a grounding agent in an AutoGen system gave a ~15% boost vs the same system without grounding and was better than or similar to ReAct depending on exact variant. In coding safety, multi-agent AutoGen design improved unsafe-code detection F1 by +8% (GPT-4) and +35% (GPT-3.5) relative to a single-agent approach. Retrieval-augmented Chat (AutoGen) with interactive retrieval improved QA vs a non-interactive retrieval baseline (ablation showed interactive retrieval mattered), exact numeric gains are shown in paper figures.",
            "human_baseline": null,
            "uuid": "e2631.0",
            "source_info": {
                "paper_title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Retrieval-augmented Chat (AutoGen RAG)",
            "name_full": "Retrieval-augmented Chat (AutoGen implementation)",
            "brief_description": "A two-agent retrieval-augmented generation system built on AutoGen, combining a vector database-backed user-proxy agent and a retrieval-augmented assistant agent with an interactive retrieval control that can request additional retrievals when context is insufficient.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Retrieval-augmented Chat",
            "system_description": "Implemented in AutoGen as two extended agents: a Retrieval-augmented UserProxy (manages a vector DB with SentenceTransformers/Chroma) and a Retrieval-augmented Assistant (LLM-backed) that inspects retrieved context and issues an \"UPDATE CONTEXT\" signal to trigger additional retrieval attempts when needed. Supports both question-answering (Natural Questions benchmark) and code-generation on codebases not present in LLM training data.",
            "system_type": "Retrieval-Augmented Generation (RAG) conversational system",
            "problem_domain": "Open-domain question answering (NLP), code generation using external codebase context.",
            "problem_description": "Answer natural questions by retrieving relevant document passages and integrating them into LLM responses; generate code grounded in a target codebase that lies outside the LLM's pretraining data.",
            "problem_complexity": "Knowledge-intensive NLP tasks with potentially large retrieval search spaces; QA tasks require finding and integrating small bits of relevant context, sometimes needing multiple retrieval attempts (interactive retrieval). Complexity arises from breadth of knowledge and need for contextual grounding.",
            "data_availability": "Uses existing corpora and vectorized document stores; Natural Questions dataset used for evaluation. Data are pre-existing and retrieved via vector search; availability is high for evaluated datasets.",
            "computational_requirements": "Not quantified in the paper; involves vector database retrieval calls and multiple LLM inferences when interactive retrieval is invoked.",
            "problem_structure": "Well-defined QA tasks with reference answers (Natural Questions); discrete retrieval-control decisions (whether to re-query). Deterministic evaluation via standard QA metrics.",
            "success_metric": "QA accuracy/answering performance compared to DPR+LLM baselines; ablation of interactive retrieval effect.",
            "success_rate": "Paper reports that the interactive retrieval mechanism materially improves performance on QA (Figure 4b) and that ablation (prompting assistant to say \"I don't know\" instead of \"UPDATE CONTEXT\") reduces performance; exact numeric deltas are presented in figures but not repeated verbatim in the main text.",
            "failure_modes": "When retrieved context lacks necessary information and the assistant does not trigger additional retrievals (non-interactive behavior), answers fail; dependency on retriever quality and vector DB coverage.",
            "success_factors": "Interactive retrieval control integrated via conversational programming; multi-agent separation allowing the assistant to request context updates; use of robust retriever (SentenceTransformers + Chroma).",
            "comparative_results": "Had better QA performance than a baseline DPR+GPT-3.5 setup reported in prior work (Adlakha et al., 2023) according to paper figures; ablation shows interactive retrieval yields meaningful gains versus a non-interactive variant.",
            "human_baseline": null,
            "uuid": "e2631.1",
            "source_info": {
                "paper_title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "OptiGuide (AutoGen-based)",
            "name_full": "OptiGuide (AutoGen-based multi-agent implementation of OptiGuide)",
            "brief_description": "A multi-agent coding workflow implemented in AutoGen that coordinates a Commander, Writer, and Safeguard agents to write, check, execute, and interpret code for optimization-analysis questions (supply-chain scenarios).",
            "citation_title": "Large language models for supply chain optimization",
            "mention_or_use": "use",
            "system_name": "OptiGuide (AutoGen implementation)",
            "system_description": "The Commander agent coordinates the Writer (crafts code) and Safeguard (checks code safety). If safeguards pass, the Commander executes code via external Python tools, returns execution outputs to the Writer for interpretation, and iterates if exceptions or security flags occur. The AutoGen implementation reduced workflow code substantially and added modular safeguards.",
            "system_type": "Multi-agent coding / automated code-generation-and-execution workflow",
            "problem_domain": "Operations research / supply-chain optimization, interactive code generation and interpretation.",
            "problem_description": "Answer user questions about optimization solutions (e.g., counterfactuals in supply-chain decisions) by programmatically generating and executing code that queries optimization outputs and interpreting results for users; includes safety checks on generated code.",
            "problem_complexity": "Requires correct program synthesis, safe execution, integration with optimization outputs, and interpretation; complexity includes debugging loops, security checks, and potential multi-step iteration until a valid answer is produced. Evaluated on a 100-task dataset balanced for safe vs unsafe tasks.",
            "data_availability": "Used a curated dataset of 100 coding tasks (50 safe, 50 unsafe) to assess code-safety detection; problem instances are task templates from operations-research/code evaluation domain.",
            "computational_requirements": "Not numerically specified; involves multiple LLM calls for Writer/Safeguard/Commander interactions and code execution time dependent on the executed code.",
            "problem_structure": "Well-defined coding tasks with binary safety labels for evaluation (safe vs unsafe), and final interpretive numeric outputs for user-facing answers. Deterministic in terms of code execution but non-deterministic from LLM outputs.",
            "success_metric": "F1 for unsafe-code identification; user-experience metrics (developer code lines, time savings, number of interactions).",
            "success_rate": "Multi-agent design improved F1 for identifying unsafe code by +8% when using GPT-4 and by +35% when using GPT-3.5-turbo versus a single-agent approach on the 100-task dataset. AutoGen-based OptiGuide reduced implementation code from ~430 to ~100 lines and reportedly saved ~3x user time and reduced user interactions by ~3–5x compared to ChatGPT+Code Interpreter (user-experience estimate provided in paper).",
            "failure_modes": "Single-agent approaches missed safety checks and had substantially lower unsafe-code detection F1; failures also arise if Safeguard misclassifies or if Writer produces code that the execution environment cannot run (exceptions).",
            "success_factors": "Explicit separation of writer and safety-checker agents, automated code execution pipelines, and iterative debugging loops orchestrated by the Commander agent.",
            "comparative_results": "Multi-agent &gt;&gt; single-agent for safety detection F1 ( +8% GPT-4, +35% GPT-3.5 ); AutoGen-based workflow showed substantial developer productivity gains versus a hand-built workflow and improved user interaction metrics versus ChatGPT+Code Interpreter.",
            "human_baseline": null,
            "uuid": "e2631.2",
            "source_info": {
                "paper_title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Multi-Agent Debate",
            "name_full": "Encouraging divergent thinking in large language models through multiagent debate",
            "brief_description": "A multi-agent debate approach where multiple LLM-based agents take affirmative/negative roles and debate solutions to encourage divergent thinking and improve reasoning or factuality.",
            "citation_title": "Encouraging divergent thinking in large language models through multiagent debate",
            "mention_or_use": "use",
            "system_name": "Multi-Agent Debate",
            "system_description": "Constructs multiple LLM inference instances as debating agents (e.g., affirmative, negative, moderator) that exchange arguments to converge on solutions; in the paper the authors adapted/ran this approach for math problem solving comparisons.",
            "system_type": "Multi-Agent Debate System",
            "problem_domain": "Mathematics problem solving (MATH dataset) in the paper's evaluation.",
            "problem_description": "Applied to multi-step math problems to see if debate among agents yields better correctness versus single-agent baselines.",
            "problem_complexity": "Evaluated on level-5 (hard) math problems from MATH; tasks require symbolic reasoning and numeric correctness.",
            "data_availability": "Used MATH problem instances (two qualitative problems tested three times each; larger quantitative tests reported elsewhere but in this paper Multi-Agent Debate was assessed on MATH subset).",
            "computational_requirements": "Not provided numerically in the paper for this method (number of LLM calls increases with number of debating agents).",
            "problem_structure": "Well-defined math problems with deterministic correct answers for evaluation; inter-agent debate is structured but still relies on LLM outputs that are stochastic.",
            "success_metric": "Correctness on math problems (counts of correct trials out of trials).",
            "success_rate": "In the qualitative 2-problem trial (3 runs each) reported in Table 2 Multi-Agent Debate achieved 0/3 correct on both example problems; in broader quantitative comparisons it underperformed AutoGen and GPT-4 baselines on MATH (paper summary indicates Multi-Agent Debate underperformed).",
            "failure_modes": "Calculation errors, producing multiple different wrong answers across runs; debate did not prevent arithmetic/logic mistakes in these cases.",
            "success_factors": "Not observed in this paper's math experiments; the framework idea may help divergence but requires complementary tool-use or grounding to fix calculation errors.",
            "comparative_results": "Underperformed AutoGen and GPT-4 in MATH experiments; produced inconsistent/wrong outputs where AutoGen's multi-agent conversation plus tool-execution achieved higher correctness.",
            "human_baseline": null,
            "uuid": "e2631.3",
            "source_info": {
                "paper_title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "LangChain ReAct",
            "name_full": "LangChain ReAct (ReAct-style single-agent with tool use)",
            "brief_description": "A LangChain implementation of the ReAct paradigm where an LLM interleaves reasoning (chain-of-thought-like) and actions (tool/function calls) within a single-agent framework.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "LangChain ReAct",
            "system_description": "Single-agent architecture using ReAct prompting to alternate between reasoning tokens and tool invocations; used as a baseline for math problem solving and as a building block for some online decision-making comparisons.",
            "system_type": "Single-Agent LLM + Tooling system (ReAct)",
            "problem_domain": "Mathematics problem solving (MATH), interactive decision tasks (used as baseline).",
            "problem_description": "Solve multi-step math problems by generating code or queries and executing tools when necessary; used out-of-the-box LangChain ReAct Python agent with handle_parsing_errors=True in comparisons.",
            "problem_complexity": "Tested on level-5 MATH problems and typical tool-augmented tasks requiring parsing and code execution.",
            "data_availability": "Benchmarks from MATH; standard datasets for evaluation.",
            "computational_requirements": "Not specified in the paper.",
            "problem_structure": "Well-defined math tasks with deterministic correct answers; single-agent tool loop, no multi-agent conversation.",
            "success_metric": "Correctness on MATH problems.",
            "success_rate": "In the qualitative MATH trials reported in Table 2, LangChain ReAct produced 0/3 correct on each of the two example problems (failed consistently in those trials). In ALFWorld-like decision tasks, ReAct achieved similar performance to a two-agent AutoGen variant when integrated similarly (paper reports comparable performance in such settings).",
            "failure_modes": "Produced incorrect answers in math tasks, parsing and execution errors requiring additional handling.",
            "success_factors": "Combines reasoning with tool use in a single agent; can work well when parsing/execution is reliable.",
            "comparative_results": "Underperformed AutoGen out-of-the-box on the evaluated math problems; comparable to two-agent AutoGen variants in some online decision tasks when ReAct techniques were integrated.",
            "human_baseline": null,
            "uuid": "e2631.4",
            "source_info": {
                "paper_title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "ChatGPT + Code Interpreter",
            "name_full": "ChatGPT with Code Interpreter (OpenAI feature)",
            "brief_description": "A ChatGPT product mode that allows the model to execute Python code (a code-execution sandbox) to aid calculation, data processing, and programmatic steps in problem solving.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "ChatGPT + Code Interpreter",
            "system_description": "An interactive ChatGPT interface augmented with an execution environment that runs model-suggested Python code and returns outputs for further reasoning; used as a strong commercial baseline for math and coding tasks in experiments.",
            "system_type": "Interactive LLM + execution environment (commercial product)",
            "problem_domain": "Mathematics problem solving, code-based problem solving, data analysis.",
            "problem_description": "Solve math problems by generating and executing Python code to compute intermediate results; used in human-in-the-loop and autonomous comparisons.",
            "problem_complexity": "Applied to level-5 math problems; complexity arises from symbolic math and careful numeric/programmatic handling.",
            "data_availability": "Operates on the input problem; no dataset training/specific dataset constraints for usage; evaluated on MATH and example problems.",
            "computational_requirements": "Execution environment is provided by the service; not quantified in the paper.",
            "problem_structure": "Well-defined math tasks with ground-truth answers.",
            "success_metric": "Correctness on example problems and in 3-trial qualitative tests; success with/without human hints.",
            "success_rate": "In the qualitative evaluation: first example problem 2/3 correct, second example problem 0/3 correct (Table 2). In the human-in-the-loop Scenario 2 for a selected hard problem, ChatGPT+Code Interpreter solved the problem in 2/3 trials.",
            "failure_modes": "Occasional wrong numerical results, failures to follow human hints in some trials, and inconsistent decisions between trials.",
            "success_factors": "Direct code execution capability helps verify calculations and produce precise outputs when code is correct.",
            "comparative_results": "Performed better than many open-source baselines in some cases but was outperformed by AutoGen's specialized multi-agent setup on the MATH benchmark overall (AutoGen 69.48% vs GPT-4 baseline; ChatGPT+Code Interpreter not reported on full set).",
            "human_baseline": null,
            "uuid": "e2631.5",
            "source_info": {
                "paper_title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "ChatGPT + Wolfram Plugin",
            "name_full": "ChatGPT with Wolfram Alpha plugin (ChatGPT+Plugin)",
            "brief_description": "ChatGPT augmented with the Wolfram Alpha plugin to query a computation engine for precise mathematical results, used as a commercial baseline in math problem solving.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "ChatGPT + Plugin (Wolfram Alpha)",
            "system_description": "ChatGPT issues Wolfram Alpha queries for computation-heavy subproblems and integrates returned results into answers. Used in evaluations as a commercial competitor for math problem solving.",
            "system_type": "LLM + external computation plugin (commercial)",
            "problem_domain": "Mathematics problem solving (symbolic and numeric computation).",
            "problem_description": "Leverage a computational engine for intermediate calculations in multi-step math problems; evaluate correctness of final answers.",
            "problem_complexity": "Level-5 math problems; complexity arises when multiple plausible simplifications or interpretations of Wolfram output exist.",
            "data_availability": "Depends on Wolfram Alpha's engine and knowledge base; used on selected MATH problems in experiments.",
            "computational_requirements": "External plugin handles computation; not quantified in paper.",
            "problem_structure": "Well-defined math problems.",
            "success_metric": "Correctness on example problems and in repeated trials.",
            "success_rate": "Qualitative trials: first example problem 1/3 correct, second example problem 1/3 correct (Table 2). In the human-in-the-loop Scenario 2, ChatGPT+Plugin solved the selected hard problem in 2/3 trials (paper text).",
            "failure_modes": "When Wolfram returns multiple simplified results or ambiguous outputs, the LLM may choose the incorrect one; can get stuck in query loops in some conditions.",
            "success_factors": "Access to a strong symbolic/numeric computation engine for exact calculations.",
            "comparative_results": "Less reliable than AutoGen on the evaluated math tasks; suffered from decision/selection errors even when the correct computation was available from Wolfram.",
            "human_baseline": null,
            "uuid": "e2631.6",
            "source_info": {
                "paper_title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "AutoGPT",
            "name_full": "AutoGPT (open-source autonomous agent project)",
            "brief_description": "An open-source single-agent system that attempts to autonomously achieve a goal by chaining LLM-generated tasks and using tools; evaluated as a baseline in math problem solving.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "AutoGPT",
            "system_description": "Single-agent pipeline that generates subgoals, plans, and actions using LLM outputs and external tools; the out-of-the-box AutoGPT instance was initialized with purpose 'solve math problems' for evaluation.",
            "system_type": "Autonomous single-agent task decomposition system",
            "problem_domain": "General automated task completion; evaluated on mathematics problem solving in this paper.",
            "problem_description": "Attempted to autonomously solve MATH benchmark problems by generating code and plans; external code execution results were expected to be used but AutoGPT in experiments produced code lacking print statements and failed to integrate execution outputs properly.",
            "problem_complexity": "Level-5 math problems in experiments; requires precise code generation and execution handling.",
            "data_availability": "Operates on input problems; no benchmark-specific training reported in this paper.",
            "computational_requirements": "Not specified numerically.",
            "problem_structure": "Autonomous goal-driven task decomposition; single-agent, static workflow.",
            "success_metric": "Correctness on math example problems.",
            "success_rate": "Qualitative trials: AutoGPT scored 0/3 on each of the two example MATH problems (Table 2). In human-in-the-loop Scenario 2, AutoGPT failed to solve the selected hard problem in all three trials.",
            "failure_modes": "Generated code without necessary print statements so execution results weren't returned/checked; could not correct code-execution related issues autonomously in the evaluated setup.",
            "success_factors": "None demonstrated in this paper's math evaluations; AutoGPT requires careful configuration and tool integration to succeed on such tasks.",
            "comparative_results": "Underperformed AutoGen and other evaluated baselines on math tasks in this study.",
            "human_baseline": null,
            "uuid": "e2631.7",
            "source_info": {
                "paper_title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "CAMEL",
            "name_full": "CAMEL (Communicative Agents for Role-play)",
            "brief_description": "A communicative-agent framework where role-playing LLMs interact to complete tasks and record conversational behavior; discussed as related work and preliminarily evaluated.",
            "citation_title": "Camel: Communicative agents for \"mind\" exploration of large scale language model society",
            "mention_or_use": "mention",
            "system_name": "CAMEL",
            "system_description": "Role-assigned LLM agents conversing to accomplish tasks and analyze agent capabilities; designed for static conversation patterns and not natively supporting tool execution in its original form.",
            "system_type": "Multi-Agent communicative framework (research project)",
            "problem_domain": "General multi-agent role-play tasks; compared in related work and preliminary tests.",
            "problem_description": "Role-playing agents converse to solve tasks. In preliminary math problem tests reported in this paper, CAMEL did not solve math problems effectively because it lacks code/tool execution capability.",
            "problem_complexity": "Not experimentally quantified here beyond qualitative observation that CAMEL struggled on math problems.",
            "data_availability": null,
            "computational_requirements": null,
            "problem_structure": "Static conversation topologies (pre-defined order), limited tool integration.",
            "success_metric": null,
            "success_rate": null,
            "failure_modes": "Lacks native tool/code execution support, causing inability to solve problems requiring computation; static conversation patterns reduce flexibility.",
            "success_factors": "Role-play prompts may help cooperative behavior in contexts where tool execution is not required.",
            "comparative_results": "AutoGen differs by supporting tool usage, human involvement, dynamic conversation patterns, and execution-capable agents; CAMEL was less suitable for the math tasks evaluated.",
            "human_baseline": null,
            "uuid": "e2631.8",
            "source_info": {
                "paper_title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "MetaGPT",
            "name_full": "MetaGPT (multi-agent framework for software development)",
            "brief_description": "A specialized multi-agent system assigning GPT agents different software-development roles to collaboratively build software; discussed as related work and preliminarily evaluated.",
            "citation_title": "MetaGPT: Meta programming for multi-agent collaborative framework",
            "mention_or_use": "mention",
            "system_name": "MetaGPT",
            "system_description": "A multi-agent system specialized for automatic software development where agents assume specialized roles (e.g., architect, coder, tester). In preliminary math tests in this paper, MetaGPT generally did not solve math problems because it attempted to develop software rather than directly produce math solutions.",
            "system_type": "Specialized multi-agent software-development system",
            "problem_domain": "Automated software development; not designed for math-problem solving.",
            "problem_description": "Designed to organize agents to develop software projects; when applied to math problems it typically started to develop a software solution pipeline rather than solving the math problem directly.",
            "problem_complexity": null,
            "data_availability": null,
            "computational_requirements": null,
            "problem_structure": "Specialized to software development workflows; static conversation patterns in the version evaluated.",
            "success_metric": null,
            "success_rate": null,
            "failure_modes": "Mismatch between system specialization (software development) and target task (math solving) resulted in failure to produce direct solutions.",
            "success_factors": "Specialization to software tasks can be beneficial within the intended domain but is a liability when reused outside that domain.",
            "comparative_results": "AutoGen's generic, execution-capable, and dynamic multi-agent design made it more suitable for math problem solving than MetaGPT in preliminary tests.",
            "human_baseline": null,
            "uuid": "e2631.9",
            "source_info": {
                "paper_title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "BabyAGI",
            "name_full": "BabyAGI (task-management multi-agent script)",
            "brief_description": "A lightweight multi-agent task-management script that spawns agents for task creation, prioritization, and execution in a static conversation pattern; discussed in related work as an example of static-pattern multi-agent systems.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "BabyAGI",
            "system_description": "Implements multiple LLM-based components (task creation, prioritization, execution) in a static pipeline; does not natively support dynamic conversation patterns, rich tool integration, or flexible human involvement as AutoGen does.",
            "system_type": "Static multi-agent task-management script",
            "problem_domain": "General task automation and management.",
            "problem_description": "Automates decomposition and execution of tasks in a static, pre-defined pipeline; not evaluated in-depth in this paper for scientific discovery tasks.",
            "problem_complexity": null,
            "data_availability": null,
            "computational_requirements": null,
            "problem_structure": "Static topology; sequential task creation/prioritization/execution steps.",
            "success_metric": null,
            "success_rate": null,
            "failure_modes": "Static conversation patterns limit flexibility and adaptability in more complex or interactive tasks.",
            "success_factors": "Simple pipeline can be adequate for straightforward task automation where dynamic conversation is unnecessary.",
            "comparative_results": "AutoGen supports both static and dynamic conversation patterns and richer tool/human integration, offering broader applicability.",
            "human_baseline": null,
            "uuid": "e2631.10",
            "source_info": {
                "paper_title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Encouraging divergent thinking in large language models through multiagent debate",
            "rating": 2
        },
        {
            "paper_title": "Improving factuality and reasoning in language models through multiagent debate",
            "rating": 2
        },
        {
            "paper_title": "React: Synergizing reasoning and acting in language models",
            "rating": 2
        },
        {
            "paper_title": "MetaGPT: Meta programming for multi-agent collaborative framework",
            "rating": 2
        },
        {
            "paper_title": "Large language models for supply chain optimization",
            "rating": 2
        },
        {
            "paper_title": "AutoGPT",
            "rating": 1
        },
        {
            "paper_title": "CAMEL: Communicative agents for \"mind\" exploration of large scale language model society",
            "rating": 1
        }
    ],
    "cost": 0.024826749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation</h1>
<p>Qingyun $\mathbf{W u}^{\dagger}$, Gagan Bansal<em>, Jieyu Zhang ${ }^{\pm}$, Yiran $\mathbf{W u}^{\dagger}$, Beibin $\mathbf{L i}^{+}$<br>Erkang Zhu</em>, Li Jiang<em>, Xiaoyun Zhang</em>, Shaokun Zhang ${ }^{\dagger}$, Jiale Liu ${ }^{\mp}$<br>Ahmed Awadallah<em>, Ryen W. White</em>, Doug Burger<em>, Chi Wang</em> ${ }^{<em>}$<br></em>Microsoft Research, ${ }^{\dagger}$ Pennsylvania State University<br>${ }^{\text {+ }}$ University of Washington, ${ }^{\mp}$Xidian University</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: AutoGen enables diverse LLM-based applications using multi-agent conversations. (Left) AutoGen agents are conversable, customizable, and can be based on LLMs, tools, humans, or even a combination of them. (Top-middle) Agents can converse to solve tasks. (Right) They can form a chat, potentially with humans in the loop. (Bottom-middle) The framework supports flexible conversation patterns.</p>
<h4>Abstract</h4>
<p>AutoGen ${ }^{2}$ is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic framework for building diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>1 Introduction</h1>
<p>Large language models (LLMs) are becoming a crucial building block in developing powerful agents that utilize LLMs for reasoning, tool usage, and adapting to new observations (Yao et al., 2022; Xi et al., 2023; Wang et al., 2023b) in many real-world tasks. Given the expanding tasks that could benefit from LLMs and the growing task complexity, an intuitive approach to scale up the power of agents is to use multiple agents that cooperate. Prior work suggests that multiple agents can help encourage divergent thinking (Liang et al., 2023), improve factuality and reasoning (Du et al., 2023), and provide validation (Wu et al., 2023). In light of the intuition and early evidence of promise, it is intriguing to ask the following question: how can we facilitate the development of LLM applications that could span a broad spectrum of domains and complexities based on the multi-agent approach?</p>
<p>Our insight is to use multi-agent conversations to achieve it. There are at least three reasons confirming its general feasibility and utility thanks to recent advances in LLMs: First, because chatoptimized LLMs (e.g., GPT-4) show the ability to incorporate feedback, LLM agents can cooperate through conversations with each other or human(s), e.g., a dialog where agents provide and seek reasoning, observations, critiques, and validation. Second, because a single LLM can exhibit a broad range of capabilities (especially when configured with the correct prompt and inference settings), conversations between differently configured agents can help combine these broad LLM capabilities in a modular and complementary manner. Third, LLMs have demonstrated ability to solve complex tasks when the tasks are broken into simpler subtasks. Multi-agent conversations can enable this partitioning and integration in an intuitive manner. How can we leverage the above insights and support different applications with the common requirement of coordinating multiple agents, potentially backed by LLMs, humans, or tools exhibiting different capacities? We desire a multi-agent conversation framework with generic abstraction and effective implementation that has the flexibility to satisfy different application needs. Achieving this requires addressing two critical questions: (1) How can we design individual agents that are capable, reusable, customizable, and effective in multi-agent collaboration? (2) How can we develop a straightforward, unified interface that can accommodate a wide range of agent conversation patterns? In practice, applications of varying complexities may need distinct sets of agents with specific capabilities, and may require different conversation patterns, such as single- or multi-turn dialogs, different human involvement modes, and static vs. dynamic conversation. Moreover, developers may prefer the flexibility to program agent interactions in natural language or code. Failing to adequately address these two questions would limit the framework's scope of applicability and generality.
While there is contemporaneous exploration of multi-agent approaches, ${ }^{3}$ we present AutoGen, a generalized multi-agent conversation framework (Figure 1), based on the following new concepts.
1 Customizable and conversable agents. AutoGen uses a generic design of agents that can leverage LLMs, human inputs, tools, or a combination of them. The result is that developers can easily and quickly create agents with different roles (e.g., agents to write code, execute code, wire in human feedback, validate outputs, etc.) by selecting and configuring a subset of built-in capabilities. The agent's backend can also be readily extended to allow more custom behaviors. To make these agents suitable for multi-agent conversation, every agent is made conversable they can receive, react, and respond to messages. When configured properly, an agent can hold multiple turns of conversations with other agents autonomously or solicit human inputs at certain rounds, enabling human agency and automation. The conversable agent design leverages the strong capability of the most advanced LLMs in taking feedback and making progress via chat and also allows combining capabilities of LLMs in a modular fashion. (Section 2.1)
2 Conversation programming. A fundamental insight of AutoGen is to simplify and unify complex LLM application workflows as multi-agent conversations. So AutoGen adopts a programming paradigm centered around these inter-agent conversations. We refer to this paradigm as conversation programming, which streamlines the development of intricate applications via two primary steps: (1) defining a set of conversable agents with specific capabilities and roles (as described above); (2) programming the interaction behavior between agents via conversationcentric computation and control. Both steps can be achieved via a fusion of natural and programming languages to build applications with a wide range of conversation patterns and agent behaviors. AutoGen provides ready-to-use implementations and also allows easy extension and experimentation for both steps. (Section 2.2)</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>AutoGen also provides a collection of multi-agent applications created using conversable agents and conversation programming. These applications demonstrate how AutoGen can easily support applications of various complexities and LLMs of various capabilities. Moreover, we perform both evaluation on benchmarks and a pilot study of new applications. The results show that AutoGen can help achieve outstanding performance on many tasks, and enable innovative ways of using LLMs, while reducing development effort. (Section 3 and Appendix D)</p>
<h1>2 The AutoGen Framework</h1>
<p>To reduce the effort required for developers to create complex LLM applications across various domains, a core design principle of AutoGen is to streamline and consolidate multi-agent workflows using multi-agent conversations. This approach also aims to maximize the reusability of implemented agents. This section introduces the two key concepts of AutoGen: conversable agents and conversation programming.</p>
<h3>2.1 Conversable Agents</h3>
<p>In AutoGen, a conversable agent is an entity with a specific role that can pass messages to send and receive information to and from other conversable agents, e.g., to start or continue a conversation. It maintains its internal context based on sent and received messages and can be configured to possess a set of capabilities, e.g., enabled by LLMs, tools, or human input, etc. The agents can act according to programmed behavior patterns described next.
Agent capabilities powered by LLMs, humans, and tools. Since an agent's capabilities directly influence how it processes and responds to messages, AutoGen allows flexibility to endow its agents with various capabilities. AutoGen supports many common composable capabilities for agents, including 1) LLMs. LLM-backed agents exploit many capabilities of advanced LLMs such as role playing, implicit state inference and progress making conditioned on conversation history, providing feedback, adapting from feedback, and coding. These capabilities can be combined in different ways via novel prompting techniques ${ }^{4}$ to increase an agent's skill and autonomy. AutoGen also offers enhanced LLM inference features such as result caching, error handling, message templating, etc., via an enhanced LLM inference layer. 2) Humans. Human involvement is desired or even essential in many LLM applications. AutoGen lets a human participate in agent conversation via humanbacked agents, which could solicit human inputs at certain rounds of a conversation depending on the agent configuration. The default user proxy agent allows configurable human involvement levels and patterns, e.g., frequency and conditions for requesting human input including the option for humans to skip providing input. 3) Tools. Tool-backed agents have the capability to execute tools via code execution or function execution. For example, the default user proxy agent in AutoGen is able to execute code suggested by LLMs, or make LLM-suggested function calls.
Agent customization and cooperation. Based on application-specific needs, each agent can be configured to have a mix of basic back-end types to display complex behavior in multi-agent conversations. AutoGen allows easy creation of agents with specialized capabilities and roles by reusing or extending the built-in agents. The yellow-shaded area of Figure 2 provides a sketch of the built-in agents in AutoGen. The ConversableAgent class is the highest-level agent abstraction and, by default, can use LLMs, humans, and tools. The AssistantAgent and UserProxyAgent are two pre-configured ConversableAgent subclasses, each representing a common usage mode, i.e., acting as an AI assistant (backed by LLMs) and acting as a human proxy to solicit human input or execute code/function calls (backed by humans and/or tools).
In the example on the right-hand side of Figure 1, an LLM-backed assistant agent and a tool- and human-backed user proxy agent are deployed together to tackle a task. Here, the assistant agent generates a solution with the help of LLMs and passes the solution to the user proxy agent. Then, the user proxy agent solicits human inputs or executes the assistant's code and passes the results as feedback back to the assistant.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of how to use AutoGen to program a multi-agent conversation. The top subfigure illustrates the built-in agents provided by AutoGen, which have unified conversation interfaces and can be customized. The middle sub-figure shows an example of using AutoGen to develop a two-agent system with a custom reply function. The bottom sub-figure illustrates the resulting automated agent chat from the two-agent system during program execution.</p>
<p>By allowing custom agents that can converse with each other, conversable agents in AutoGen serve as a useful building block. However, to develop applications where agents make meaningful progress on tasks, developers also need to be able to specify and mold these multi-agent conversations.</p>
<h1>2.2 Conversation Programming</h1>
<p>As a solution to the above problem, AutoGen utilizes conversation programming, a paradigm that considers two concepts: the first is computation - the actions agents take to compute their response in a multi-agent conversation. And the second is control flow - the sequence (or conditions) under which these computations happen. As we will show in the applications section, the ability to program these helps implement many flexible multi-agent conversation patterns. In AutoGen, these computations are conversation-centric. An agent takes actions relevant to the conversations it is involved in and its actions result in message passing for consequent conversations (unless a termination condition is satisfied). Similarly, control flow is conversation-driven - the participating agents' decisions on which agents to send messages to and the procedure of computation are functions of the inter-agent conversation. This paradigm helps one to reason intuitively about a complex workflow as agent action taking and conversation message-passing between agents.
Figure 2 provides a simple illustration. The bottom sub-figure shows how individual agents perform their role-specific, conversation-centric computations to generate responses (e.g., via LLM inference calls and code execution). The task progresses through conversations displayed in the dialog box. The middle sub-figure demonstrates a conversation-based control flow. When the assistant receives a message, the user proxy agent typically sends the human input as a reply. If there is no input, it executes any code in the assistant's message instead.</p>
<p>AutoGen features the following design patterns to facilitate conversation programming:</p>
<ol>
<li>Unified interfaces and auto-reply mechanisms for automated agent chat. Agents in AutoGen have unified conversation interfaces for performing the corresponding conversationcentric computation, including a send/receive function for sending/receiving messages and a generate_reply function for taking actions and generating a response based on the received message. AutoGen also introduces and by default adopts an agent auto-reply mechanism to realize conversation-driven control: Once an agent receives a message from another agent, it automatically invokes generate_reply and sends the reply back to the sender unless a termination condition is satisfied. AutoGen provides built-in reply functions based on LLM inference, code or function execution, or human input. One can also register custom reply functions to customize the behavior pattern of an agent, e.g., chatting with another agent before replying to the sender agent. Under this mechanism, once the reply functions are registered, and the conversation is initialized, the conversation flow is naturally induced, and thus the agent conversation proceeds naturally without any extra control plane, i.e., a special module that controls the conversation flow. For example, with the developer code in the blue-shaded area (marked "Developer Code") of Figure 2, one can readily trigger the conversation among the agents, and the conversation would proceed automatically, as shown in the dialog box in the grey shaded area (marked "Program Execution") of Figure 2. The auto-reply mechanism provides a decentralized, modular, and unified way to define the workflow.</li>
<li>Control by fusion of programming and natural language. AutoGen allows the usage of programming and natural language in various control flow management patterns: 1) Naturallanguage control via LLMs. In AutoGen, one can control the conversation flow by prompting the LLM-backed agents with natural language. For instance, the default system message of the built-in AssistantAgent in AutoGen uses natural language to instruct the agent to fix errors and generate code again if the previous result indicates there are errors. It also guides the agent to confine the LLM output to certain structures, making it easier for other tool-backed agents to consume. For example, instructing the agent to reply with "TERMINATE" when all tasks are completed to terminate the program. More concrete examples of natural language controls can be found in Appendix C. 2) Programming-language control. In AutoGen, Python code can be used to specify the termination condition, human input mode, and tool execution logic, e.g., the max number of auto replies. One can also register programmed auto-reply functions to control the conversation flow with Python code, as shown in the code block identified as "ConversationDriven Control Flow" in Figure 2. 3) Control transition between natural and programming language. AutoGen also supports flexible control transition between natural and programming language. One can achieve transition from code to natural-language control by invoking an LLM inference containing certain control logic in a customized reply function; or transition from natural language to code control via LLM-proposed function calls (Eleti et al., 2023).</li>
</ol>
<p>In the conversation programming paradigm, one can realize multi-agent conversations of diverse patterns. In addition to static conversation with predefined flow, AutoGen also supports dynamic conversation flows with multiple agents. AutoGen provides two general ways to achieve this: 1) Customized generate_reply function: within the customized generate_reply function, one agent can hold the current conversation while invoking conversations with other agents depending on the content of the current message and context. 2) Function calls: In this approach, LLM decides whether or not to call a particular function depending on the conversation status. By messaging additional agents in the called functions, the LLM can drive dynamic multi-agent conversation. In addition, AutoGen supports more complex dynamic group chat via built-in GroupChatManager, which can dynamically select the next speaker and then broadcast its response to other agents. We elaborate on this feature and its application in Section 3. We provide implemented working systems to showcase all these different patterns, with some of them visualized in Figure 3.</p>
<h1>3 Applications of AutoGen</h1>
<p>We demonstrate six applications using AutoGen (see Figure 3) to illustrate its potential in simplifying the development of high-performance multi-agent applications. These applications are selected based on their real-world relevance (A1, A2, A4, A5, A6), problem difficulty and solving capabilities enabled by AutoGen (A1, A2, A3, A4), and innovative potential (A5, A6). Together, these criteria showcase AutoGen's role in advancing the LLM-application landscape.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Six examples of diverse applications built using AutoGen. Their conversation patterns show AutoGen's flexibility and power.</p>
<h1>A1: Math Problem Solving</h1>
<p>Mathematics is a foundational discipline and the promise of leveraging LLMs to assist with math problem solving opens up a new plethora of applications and avenues for exploration, including personalized AI tutoring, AI research assistance, etc. This section demonstrates how AutoGen can help develop LLM applications for math problem solving, showcasing strong performance and flexibility in supporting various problem-solving paradigms.
(Scenario 1) We are able to build a system for autonomous math problem solving by directly reusing two built-in agents from AutoGen. We evaluate our system and several alternative approaches, including open-source methods such as Multi-Agent Debate (Liang et al., 2023), LangChain ReAct (LangChain, 2023), vanilla GPT-4, and commercial products ChatGPT + Code Interpreter, and ChatGPT + Plugin (Wolfram Alpha), on the MATH (Hendrycks et al., 2021) dataset and summarize the results in Figure 4a. We perform evaluations over 120 randomly selected level-5 problems and on the entire ${ }^{5}$ test dataset from MATH. The results show that the built-in agents from AutoGen already yield better performance out of the box compared to the alternative approaches, even including the commercial ones. (Scenario 2) We also showcase a human-in-the-loop problem-solving process with the help of AutoGen. To incorporate human feedback with AutoGen, one only needs to set human_input_mode="ALWAYS" in the UserProxyAgent of the system in scenario 1. We demonstrate that this system can effectively incorporate human inputs to solve challenging problems that cannot be solved without humans. (Scenario 3) We further demonstrate a novel scenario where multiple human users can participate in the conversations during the problem-solving process. Our experiments and case studies for these scenarios show that AutoGen enables better performance or new experience compared to other solutions we experimented with. Due to the page limit, details of the evaluation, including case studies in three scenarios are in Appendix D.</p>
<h2>A2: Retrieval-Augmented Code Generation and Question Answering</h2>
<p>Retrieval augmentation has emerged as a practical and effective approach for mitigating the intrinsic limitations of LLMs by incorporating external documents. In this section, we employ AutoGen to build a Retrieval-Augmented Generation (RAG) system (Lewis et al., 2020; Parvez et al., 2021) named Retrieval-augmented Chat. The system consists of two agents: a Retrieval-augmented User Proxy agent and a Retrieval-augmented Assistant agent, both of which are extended from built-in agents from AutoGen. The Retrieval-augmented User Proxy includes a vector database (Chroma,</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Performance on four applications A1-A4. (a) shows that AutoGen agents can be used out of the box to achieve the most competitive performance on math problem solving tasks; (b) shows that AutoGen can be used to realize effective retrieval augmentation and realize a novel interactive retrieval feature to boost performance on Q&amp;A tasks; (c) shows that AutoGen can be used to introduce a three-agent system with a grounding agent to improve performance on ALFWorld; (d) shows that a multi-agent design is helpful in boosting performance in coding tasks that need safeguards.</p>
<p>2023) with SentenceTransformers (Reimers &amp; Gurevych, 2019) as the context retriever. A detailed workflow description of the Retrieval-augmented Chat is provided in Appendix D.</p>
<p>We evaluate Retrieval-augmented Chat in both question-answering and code-generation scenarios. (Scenario 1) We first perform an evaluation regarding natural question answering on the Natural Questions dataset (Kwiatkowski et al., 2019) and report results in Figure 4b. In this evaluation, we compare our system with DPR (Dense Passage Retrieval) following an existing evaluation practice (Adlakha et al., 2023). Leveraging the conversational design and natural-language control, AutoGen introduces a novel interactive retrieval feature in this application: whenever the retrieved context does not contain the information, instead of terminating, the LLM-based assistant would reply “Sorry, I cannot find any information about… UPDATE CONTEXT.” which will invoke more retrieval attempts. We conduct an ablation study in which we prompt the assistant agent to say “I don’t know” instead of “UPDATE CONTEXT.” in cases where relevant information is not found, and report results in Figure 4b. The results show that the interactive retrieval mechanism indeed plays a non-trivial role in the process. We give a concrete example and results using this appealing feature in Appendix D. (Scenario 2) We further demonstrate how Retrieval-augmented Chat aids in generating code based on a given codebase that contains code not included in GPT-4’s training data. Evaluation and demonstration details for both scenarios are included in Appendix D.</p>
<p><sup>6</sup>The results of DPR with GPT-3.5 shown in Figure 4b are from (Adlakha et al., 2023). We use GPT-3.5 as a shorthand for GPT-3.5-turbo.</p>
<h1>A3: Decision Making in Text World Environments</h1>
<p>In this subsection, we demonstrate how AutoGen can be used to develop effective applications that involve interactive or online decision making. We perform the study using the ALFWorld (Shridhar et al., 2021) benchmark, which includes a diverse collection of synthetic language-based interactive decision-making tasks in household environments.</p>
<p>With AutoGen, we implemented a two-agent system to solve tasks from ALFWorld. It consists of an LLM-backed assistant agent responsible for suggesting plans to conduct a task and an executor agent responsible for executing actions in the ALFWorld environments. This system integrates ReAct prompting (Yao et al., 2022), and is able to achieve similar performance. A common challenge encountered in both ReAct and the AutoGen-based two-agent system is their occasional inability to leverage basic commonsense knowledge about the physical world. This deficiency can lead to the system getting stuck in a loop due to repetitive errors. Fortunately, the modular design of AutoGen allows us to address this issue effectively: With AutoGen, we are able to introduce a grounding agent, which supplies crucial commonsense knowledge-such as "You must find and take the object before you can examine it. You must go to where the target object is before you can use it."-whenever the system exhibits early signs of recurring errors. It significantly enhances the system's ability to avoid getting entangled in error loops. We compare the task-solving performance of the two variants of our system with GPT-3.5-turbo and ReAct ${ }^{7}$ on the 134 unseen tasks from ALFWorld and report results in Figure 4c. The results show that introducing a grounding agent could bring in a 15\% performance gain on average. Upon examining the systems' outputs, we observe that the grounding agent, by delivering background commonsense knowledge at the right junctures, significantly mitigated the tendency of the system to persist with a flawed plan, thereby avoiding the creation of error loops. For an example trajectory comparing the systems see Appendix D, Figure 10.</p>
<h2>A4: Multi-Agent Coding</h2>
<p>In this subsection, we use AutoGen to build a multi-agent coding system based on OptiGuide ( Li et al., 2023a), a system that excels at writing code to interpret optimization solutions and answer user questions, such as exploring the implications of changing a supply-chain decision or understanding why the optimizer made a particular choice. The second sub-figure of Figure 3 shows the AutoGen-based implementation. The workflow is as follows: the end user sends questions, such as "What if we prohibit shipping from supplier 1 to roastery 2?" to the Commander agent. The Commander coordinates with two assistant agents, including the Writer and the Safeguard, to answer the question. The Writer will craft code and send the code to the Commander. After receiving the code, the Commander checks the code safety with the Safeguard; if cleared, the Commander will use external tools (e.g., Python) to execute the code, and request the Writer to interpret the execution results. For instance, the writer may say "if we prohibit shipping from supplier 1 to roastery 2, the total cost would increase by $10.5 \%$." The Commander then provides this concluding answer to the end user. If, at a particular step, there is an exception, e.g., security red flag raised by Safeguard, the Commander redirects the issue back to the Writer with debugging information. The process might be repeated multiple times until the user's question is answered or timed-out.</p>
<p>With AutoGen the core workflow code for OptiGuide was reduced from over 430 lines to 100 lines, leading to significant productivity improvement. We provide a detailed comparison of user experience with ChatGPT+Code Interpreter and AutoGen-based OptiGuide in Appendix D, where we show that AutoGen-based OptiGuide could save around 3x of user's time and reduce user interactions by 3 - 5 times on average. We also conduct an ablation showing that multi-agent abstraction is necessary. Specifically, we construct a single-agent approach where a single agent conducts both the code-writing and safeguard processes. We tested the single- and multi-agent approaches on a dataset of 100 coding tasks, which is crafted to include equal numbers of safe and unsafe tasks. Evaluation results as reported in Figure 4d show that the multi-agent design boosts the F-1 score in identifying unsafe code by $8 \%$ (with GPT-4) and $35 \%$ (with GPT-3.5-turbo).</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>A5: Dynamic Group Chat</h1>
<p>AutoGen provides native support for a dynamic group chat communication pattern, in which participating agents share the same context and converse with the others in a dynamic manner instead of following a pre-defined order. Dynamic group chat relies on ongoing conversations to guide the flow of interaction among agents. These make dynamic group chat ideal for situations where collaboration without strict communication order is beneficial. In AutoGen, the GroupChatManager class serves as the conductor of conversation among agents and repeats the following three steps: dynamically selecting a speaker, collecting responses from the selected speaker, and broadcasting the message (Figure 3-A5). For the dynamic speaker-selection component, we use a role-play style prompt. Through a pilot study on 12 manually crafted complex tasks, we observed that compared to a prompt that is purely based on the task, utilizing a role-play prompt often leads to more effective consideration of both conversation context and role alignment during the problem-solving and speaker-selection process. Consequently, this leads to a higher success rate and fewer LLM calls. We include detailed results in Appendix D.</p>
<h2>A6: Conversational Chess</h2>
<p>Using AutoGen, we developed Conversational Chess, a natural language interface game shown in the last sub-figure of Figure 3. It features built-in agents for players, which can be human or LLM, and a third-party board agent to provide information and validate moves based on standard rules.
With AutoGen, we enabled two essential features: (1) Natural, flexible, and engaging game dynamics, enabled by the customizable agent design in AutoGen. Conversational Chess supports a range of game-play patterns, including AI-AI, AI-human, and human-human, with seamless switching between these modes during a single game. An illustrative example of these entertaining game dynamics can be found in Figure 15, Appendix D. (2) Grounding, which is a crucial aspect to maintain game integrity. During gameplay, the board agent checks each proposed move for legality; if a move is invalid, the agent responds with an error, prompting the player agent to re-propose a legal move before continuing. This process ensures that only valid moves are played and helps maintain a consistent gaming experience. As an ablation study, we removed the board agent and instead only relied on a relevant prompt "you should make sure both you and the opponent are making legal moves" to ground their move. The results highlighted that without the board agent, illegitimate moves caused game disruptions. The modular design offered flexibility, allowing swift adjustments to the board agent in response to evolving game rules or varying chess rule variants. A comprehensive demonstration of this ablation study is in Appendix D.</p>
<h2>4 Discussion</h2>
<p>We introduced an open-source library, AutoGen, that incorporates the paradigms of conversable agents and conversation programming. This library utilizes capable agents that are well-suited for multi-agent cooperation. It features a unified conversation interface among the agents, along with an auto-reply mechanisms, which help establish an agent-interaction interface that capitalizes on the strengths of chat-optimized LLMs with broad capabilities while accommodating a wide range of applications. AutoGen serves as a general framework for creating and experimenting with multiagent systems that can easily fulfill various practical requirements, such as reusing, customizing, and extending existing agents, as well as programming conversations between them.
Our experiments, as detailed in Section 3, demonstrate that this approach offers numerous benefits. The adoption of AutoGen has resulted in improved performance (over state-of-the-art approaches), reduced development code, and decreased manual burden for existing applications. It offers flexibility to developers, as demonstrated in A1 (scenario 3), A5, and A6, where AutoGen enables multi-agent chats to follow a dynamic pattern rather than fixed back-and-forth interactions. It allows humans to engage in activities alongside multiple AI agents in a conversational manner. Despite the complexity of these applications (most involving more than two agents or dynamic multi-turn agent cooperation), the implementation based on AutoGen remains straightforward. Dividing tasks among separate agents promotes modularity. Furthermore, since each agent can be developed, tested, and maintained separately, this approach simplifies overall development and code management.</p>
<p>Although this work is still in its early experimental stages, it paves the way for numerous future directions and research opportunities. For instance, we can explore effective integration of existing agent implementations into our multi-agent framework and investigate the optimal balance between automation and human control in multi-agent workflows. As we further develop and refine AutoGen, we aim to investigate which strategies, such as agent topology and conversation patterns, lead to the most effective multi-agent conversations while optimizing the overall efficiency, among other factors. While increasing the number of agents and other degrees of freedom presents opportunities for tackling more complex problems, it may also introduce new safety challenges that require additional studies and careful consideration.</p>
<p>We provide more discussion in Appendix B, including guidelines for using AutoGen and direction of future work. We hope AutoGen will help improve many LLM applications in terms of speed of development, ease of experimentation, and overall effectiveness and safety. We actively welcome contributions from the broader community.</p>
<h1>Ethics statement</h1>
<p>There are several potential ethical considerations that could arise from the development and use of the AutoGen framework.</p>
<ul>
<li>Privacy and Data Protection: The framework allows for human participation in conversations between agents. It is important to ensure that user data and conversations are protected, and that developers use appropriate measures to safeguard privacy.</li>
<li>Bias and Fairness: LLMs have been shown to exhibit biases present in their training data (Navigli et al., 2023). When using LLMs in the AutoGen framework, it is crucial to address and mitigate any biases that may arise in the conversations between agents. Developers should be aware of potential biases and take steps to ensure fairness and inclusivity.</li>
<li>Accountability and Transparency: As discussed in the future work section, as the framework involves multiple agents conversing and cooperating, it is important to establish clear accountability and transparency mechanisms. Users should be able to understand and trace the decision-making process of the agents involved in order to ensure accountability and address any potential issues or biases.</li>
<li>Trust and Reliance: AutoGen leverages human understanding and intelligence while providing automation through conversations between agents. It is important to consider the impact of this interaction on user experience, trust, and reliance on AI systems. Clear communication and user education about the capabilities and limitations of the system will be essential (Cai et al., 2019).</li>
<li>Unintended Consequences: As discussed before, the use of multi-agent conversations and automation in complex tasks may have unintended consequences. In particular, allowing LLM agents to make changes in external environments through code execution or function calls, such as installing packages, could be risky. Developers should carefully consider the potential risks and ensure that appropriate safeguards are in place to prevent harm or negative outcomes.</li>
</ul>
<h2>Acknowledgements</h2>
<p>The work presented in this report was made possible through discussions and feedback from Peter Lee, Johannes Gehrke, Eric Horvitz, Steven Lucco, Umesh Madan, Robin Moeur, Piali Choudhury, Saleema Amershi, Adam Fourney, Victor Dibia, Guoqing Zheng, Corby Rosset, Ricky Loynd, Ece Kamar, Rafah Hosn, John Langford, Ida Momennejad, Brian Krabach, Taylor Webb, Shanka Subhra Mondal, Wei-ge Chen, Robert Gruen, Yinan Li, Yue Wang, Suman Nath, Tanakorn Leesatapornwongsa, Xin Wang, Shishir Patil, Tianjun Zhang, Saehan Jo, Ishai Menache, Kontantina Mellou, Runlong Zhou, Feiran Jia, Hamed Khanpour, Hamid Palangi, Srinagesh Sharma, Julio Albinati Cortez, Amin Saied, Yuzhe Ma, Dujian Ding, Linyong Nan, Prateek Yadav, Shannon Shen, Ankur Mallick, Mark Encarnación, Lars Liden, Tianwei Yue, Julia Kiseleva, Anastasia Razdaibiedina, and Luciano Del Corro. Qingyun Wu would like to acknowledge the funding and research support from the College of Information Science and Technology at Penn State University.</p>
<h1>References</h1>
<p>Vaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy. Evaluating correctness and faithfulness of instruction-following models for question answering. arXiv preprint arXiv:2307.16877, 2023.</p>
<p>Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, et al. Guidelines for human-ai interaction. In Proceedings of the 2019 chi conference on human factors in computing systems, 2019.</p>
<p>Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. Concrete problems in ai safety, 2016.</p>
<p>AutoGPT. Documentation — auto-gpt. https://docs.agpt.co/, 2023.
BabyAGI. Github — babyagi. https://github.com/yoheinakajima/babyagi, 2023.
Carrie J. Cai, Samantha Winter, David F. Steiner, Lauren Wilcox, and Michael Terry. "hello ai": Uncovering the onboarding needs of medical practitioners for human-ai collaborative decisionmaking. Proceedings of the ACM on Human-Computer Interaction, 2019.</p>
<p>Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as tool makers. arXiv preprint arXiv:2305.17126, 2023.</p>
<p>Chroma. Chromadb. https://github.com/chroma-core/chroma, 2023.
Victor Dibia. LIDA: A tool for automatic generation of grammar-agnostic visualizations and infographics using large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Toronto, Canada, July 2023. Association for Computational Linguistics.</p>
<p>Yihong Dong, Xue Jiang, Zhi Jin, and Ge Li. Self-collaboration code generation via chatgpt. arXiv preprint arXiv:2304.07590, 2023.</p>
<p>Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023.</p>
<p>Atty Eleti, Jeff Harris, and Logan Kilpatrick. Function calling and other api updates. https: //openai.com/blog/function-calling-and-other-api-updates, 2023.</p>
<p>Guidance. Guidance. https://github.com/guidance-ai/guidance, 2023.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.</p>
<p>Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 2023.</p>
<p>Eric Horvitz. Principles of mixed-initiative user interfaces. In Proceedings of the SIGCHI conference on Human Factors in Computing Systems, 1999.</p>
<p>HuggingFace. Transformers agent. https://huggingface.co/docs/transformers/ transformers_agents, 2023.</p>
<p>Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. arXiv preprint arXiv:2303.17491, 2023.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 2019.</p>
<p>LangChain. Introduction — langchain. https://python.langchain.com/en/latest/index. html, 2023.</p>
<p>Mike Lewis, Denis Yarats, Yann N Dauphin, Devi Parikh, and Dhruv Batra. Deal or no deal? end-to-end learning for negotiation dialogues. arXiv preprint arXiv:1706.05125, 2017.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 2020.</p>
<p>Beibin Li, Konstantina Mellou, Bo Zhang, Jeevan Pathuri, and Ishai Menache. Large language models for supply chain optimization. arXiv preprint arXiv:2307.03875, 2023a.</p>
<p>Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for "mind" exploration of large scale language model society, 2023b.</p>
<p>Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. Encouraging divergent thinking in large language models through multiagent debate, 2023.</p>
<p>Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement learning on web interfaces using workflow-guided exploration. arXiv preprint arXiv:1802.08802, 2018.</p>
<p>Jerry Liu. LlamaIndex, November 2022. URL https://github.com/jerryjliu/llama_index.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.</p>
<p>Roberto Navigli, Simone Conia, and Björn Ross. Biases in large language models: Origins, inventory and discussion. ACM Journal of Data and Information Quality, 2023.</p>
<p>OpenAI. ChatGPT plugins. https://openai.com/blog/chatgpt-plugins, 2023.
Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442, 2023.</p>
<p>Md Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Retrieval augmented code generation and summarization. arXiv preprint arXiv:2108.11601, 2021.</p>
<p>Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334, 2023.</p>
<p>Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/ abs/1908.10084.</p>
<p>Semantic-Kernel. Semantic kernel. https://github.com/microsoft/semantic-kernel, 2023.</p>
<p>Bokui Shen, Fei Xia, Chengshu Li, Roberto Martín-Martín, Linxi Fan, Guanzhi Wang, Claudia Pérez-D'Arpino, Shyamal Buch, Sanjana Srivastava, Lyne Tchapmi, et al. igibson 1.0: A simulation environment for interactive tasks in large realistic scenes. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2021.</p>
<p>Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: An open-domain platform for web-based agents. In International Conference on Machine Learning. PMLR, 2017.</p>
<p>Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. URL https://arxiv.org/abs/2010.03768.</p>
<p>Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich Küttler, John Agapiou, Julian Schrittwieser, et al. Starcraft ii: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782, 2017.</p>
<p>Chi Wang, Qingyun Wu, Markus Weimer, and Erkang Zhu. Flaml: A fast and lightweight automl library. Proceedings of Machine Learning and Systems, 2021.</p>
<p>Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023a.</p>
<p>Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. arXiv preprint arXiv:2308.11432, 2023b.</p>
<p>Daniel S. Weld and Oren Etzioni. The first law of robotics (a call to arms). In AAAI Conference on Artificial Intelligence, 1994.</p>
<p>Max Woolf. Langchain problem. https://minimaxir.com/2023/07/langchain-problem/, 2023.</p>
<p>Yiran Wu, Feiran Jia, Shaokun Zhang, Qingyun Wu, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, and Chi Wang. An empirical study on challenging math problem solving with gpt-4. arXiv preprint arXiv:2306.01337, 2023.</p>
<p>Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864, 2023.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022 .</p>
<h1>A Related Work</h1>
<p>We examine existing LLM-based agent systems or frameworks that can be used to build LLM applications. We categorize the related work into single-agent and multi-agent systems and specifically provide a summary of differentiators comparing AutoGen with existing multi-agent systems in Table 1. Note that many of these systems are evolving open-source projects, so the remarks and statements about them may only be accurate as of the time of writing. We refer interested readers to detailed LLM-based agent surveys (Xi et al., 2023; Wang et al., 2023b)</p>
<h2>Single-Agent Systems:</h2>
<ul>
<li>AutoGPT: AutoGPT is an open-source implementation of an AI agent that attempts to autonomously achieve a given goal (AutoGPT, 2023). It follows a single-agent paradigm in which it augments the AI model with many useful tools, and does not support multi-agent collaboration.</li>
<li>ChatGPT+ (with code interpreter or plugin): ChatGPT, a conversational AI service or agent, can now be used alongside a code interpreter or plugin (currently available only under the premium subscription plan ChatGPT Plus) (OpenAI, 2023). The code interpreter enables ChatGPT to execute code, while the plugin enhances ChatGPT with a wide range of curated tools.</li>
<li>LangChain Agents: LangChain is a general framework for developing LLM-based applications (LangChain, 2023). LangChain Agents is a subpackage for using an LLM to choose a sequence of actions. There are various types of agents in LangChain Agents, with the ReAct agent being a notable example that combines reasoning and acting when using LLMs (mainly designed for LLMs prior to ChatGPT) (Yao et al., 2022). All agents provided in LangChain Agents follow a single-agent paradigm and are not inherently designed for communicative and collaborative modes. A significant summary of its limitations can be found in (Woolf, 2023). Due to these limitations, even the multi-agent systems in LangChain (e.g., re-implementation of CAMEL) are not based on LangChain Agents but are implemented from scratch. Their connection to LangChain lies in the use of basic orchestration modules provided by LangChain, such as AI models wrapped by LangChain and the corresponding interface.</li>
<li>Transformers Agent: Transformers Agent (HuggingFace, 2023) is an experimental naturallanguage API built on the transformers repository. It includes a set of curated tools and an agent to interpret natural language and use these tools. Similar to AutoGPT, it follows a single-agent paradigm and does not support agent collaboration.</li>
</ul>
<p>AutoGen differs from the single-agent systems above by supporting multi-agent LLM applications.</p>
<h2>Multi-Agent Systems:</h2>
<ul>
<li>BabyAGI: BabyAGI (BabyAGI, 2023) is an example implementation of an AI-powered task management system in a Python script. In this implemented system, multiple LLM-based agents are used. For example, there is an agent for creating new tasks based on the objective and the result of the previous task, an agent for prioritizing the task list, and an agent for completing tasks/sub-tasks. As a multi-agent system, BabyAGI adopts a static agent conversation pattern, i.e., a predefined order of agent communication, while AutoGen supports both static and dynamic conversation patterns and additionally supports tool usage and human involvement.</li>
<li>CAMEL: CAMEL (Li et al., 2023b) is a communicative agent framework. It demonstrates how role playing can be used to let chat agents communicate with each other for task completion. It also records agent conversations for behavior analysis and capability understanding. An Inception-prompting technique is used to achieve autonomous cooperation between agents. Unlike AutoGen, CAMEL does not natively support tool usage, such as code execution. Although it is proposed as an infrastructure for multi-agent conversation, it only supports static conversation patterns, while AutoGen additionally supports dynamic conversation patterns.</li>
<li>
<p>Multi-Agent Debate: Two recent works investigate and show that multi-agent debate is an effective way to encourage divergent thinking in LLMs (Liang et al., 2023) and to improve the factuality and reasoning of LLMs (Du et al., 2023). In both works, multiple LLM inference instances are constructed as multiple agents to solve problems with agent debate. Each agent is simply an LLM inference instance, while no tool or human is involved, and the inter-agent conversation needs to follow a pre-defined order. These works attempt to build LLM applications with multi-agent conversation, while AutoGen, designed as a generic infrastructure, can be used to facilitate this development and enable more applications with dynamic conversation patterns.</p>
</li>
<li>
<p>MetaGPT: MetaGPT (Hong et al., 2023) is a specialized LLM application based on a multi-agent conversation framework for automatic software development. They assign different roles to GPTs to collaboratively develop software. They differ from AutoGen by being specialized solutions to a certain scenario, while AutoGen is a generic infrastructure to facilitate building applications for various scenarios.</p>
</li>
</ul>
<p>There are a few other specialized single-agent or multi-agent systems, such as Voyager (Wang et al., 2023a) and Generative Agents (Park et al., 2023), which we skip due to lower relevance. In Table 1, we summarize differences between AutoGen and the most relevant multi-agent systems.</p>
<p>Table 1: Summary of differences between AutoGen and other related multi-agent systems. infrastructure: whether the system is designed as a generic infrastructure for building LLM applications. conversation pattern: the types of patterns supported by the implemented systems. Under the 'static' pattern, agent topology remains unchanged regardless of different inputs. AutoGen allows flexible conversation patterns, including both static and dynamic patterns that can be customized based on different application needs. execution-capable: whether the system can execute LLMgenerated code; human involvement: whether (and how) the system allows human participation during the execution process of the system. AutoGen allows flexible human involvement in multiagent conversation with the option for humans to skip providing inputs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Aspect</th>
<th style="text-align: center;">AutoGen</th>
<th style="text-align: center;">Multi-agent Debate</th>
<th style="text-align: center;">CAMEL</th>
<th style="text-align: center;">BabyAGI</th>
<th style="text-align: center;">MetaGPT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Infrastructure</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">Conversation pattern</td>
<td style="text-align: center;">flexible</td>
<td style="text-align: center;">static</td>
<td style="text-align: center;">static</td>
<td style="text-align: center;">static</td>
<td style="text-align: center;">static</td>
</tr>
<tr>
<td style="text-align: left;">Execution-capable</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Human involvement</td>
<td style="text-align: center;">chat/skip</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
</tbody>
</table>
<h1>B Expanded Discussion</h1>
<p>The applications in Section 3 show how AutoGen not only enables new applications but also helps renovate existing ones. For example, in A1 (scenario 3), A5, and A6, AutoGen enabled the creation of multi-agent conversations that follow a dynamic pattern instead of a fixed back-and-forth. And in both A5 and A6, humans can participate in the activities together with multiple other AI agents in a conversational manner. Similarly, A1-A4 show how popular applications can be renovated quickly with AutoGen. Despite the complexity of these applications (most of them involve more than two agents or dynamic multi-turn agent cooperation), our AutoGen-based implementation remains simple, demonstrating promising opportunities to build creative applications and a large space for innovation. In reflecting on why these benefits can be achieved in these applications with AutoGen, we believe there are a few reasons:</p>
<ul>
<li>Ease of use: The built-in agents can be used out-of-the-box, delivering strong performance even without any customization. (A1, A3)</li>
<li>Modularity: The division of tasks into separate agents promotes modularity in the system. Each agent can be developed, tested, and maintained independently, simplifying the overall development process and facilitating code management. (A3, A4, A5, and A6)</li>
<li>Programmability: AutoGen allows users to extend/customize existing agents to develop systems satisfying their specific needs with ease. (A1-A6). For example, with AutoGen, the core workflow code in A4 is reduced from over 430 lines to 100 lines, for a 4 x saving.</li>
<li>Allowing human involvement: AutoGen provides a native mechanism to achieve human participation and/or human oversight. With AutoGen, humans can seamlessly and optionally cooperate with AIs to solve problems or generally participate in the activity. AutoGen also facilitates interactive user instructions to ensure the process stays on the desired path. (A1, A2, A5, and A6)</li>
<li>Collaborative/adversarial agent interactions: Like many collaborative agent systems (Dong et al., 2023), agents in AutoGen can share information and knowledge, to complement each other's abilities and collectively arrive at better solutions. (A1, A2, A3, and A4). Analogously, in certain scenarios, some agents are required to work in an adversarial way. Relevant information is shared among different conversations in a controlled manner, preventing distraction or hallucination. (A4, A6). AutoGen supports both patterns, enabling effective utilization and augmentation of LLMs.</li>
</ul>
<h2>B. 1 General Guidelines for Using AutoGen</h2>
<p>Below we give some recommendations for using agents in AutoGen to accomplish a task.</p>
<ol>
<li>Consider using built-in agents first. For example, AssistantAgent is pre-configured to be backed by GPT-4, with a carefully designed system message for generic problem-solving via code. The UserProxyAgent is configured to solicit human inputs and perform tool execution. Many problems can be solved by simply combining these two agents. When customizing agents for an application, consider the following options: (1) human input mode, termination condition, code execution configuration, and LLM configuration can be specified when constructing an agent; (2) AutoGen supports adding instructions in an initial user message, which is an effective way to boost performance without needing to modify the system message; (3) UserProxyAgent can be extended to handle different execution environments and exceptions, etc.; (4) when system message modification is needed, consider leveraging the LLM's capability to program its conversation flow with natural language.</li>
<li>Start with a simple conversation topology. Consider using the two-agent chat or the group chat setup first, as they can often be extended with the least code. Note that the two-agent chat can be easily extended to involve more than two agents by using LLM-consumable functions in a dynamic way.</li>
<li>Try to reuse built-in reply methods based on LLM, tool, or human before implementing a custom reply method because they can often be reused to achieve the goal in a simple way (e.g., the built-in agent GroupChatManager's reply method reuses the built-in LLM-based reply function when selecting the next speaker, ref. A5 in Section 3).</li>
<li>When developing a new application with UserProxyAgent, start with humans always in the loop, i.e., human_input_mode='ALWAYS', even if the target operation mode is more autonomous. This helps evaluate the effectiveness of AssistantAgent, tuning the prompt, discovering corner cases, and debugging. Once confident with small-scale success, consider setting</li>
</ol>
<p>human_input_mode $=$ 'NEVER'. This enables LLM as a backend, and one can either use the LLM or manually generate diverse system messages to simulate different use cases.
5. Despite the numerous advantages of AutoGen agents, there could be cases/scenarios where other libraries/packages could help. For example: (1) For (sub)tasks that do not have requirements for back-and-forth trouble-shooting, multi-agent interaction, etc., a unidirectional (no back-andforth message exchange) pipeline can also be orchestrated with LangChain (LangChain, 2023), LlamaIndex (Liu, 2022), Guidance (Guidance, 2023), Semantic Kernel (Semantic-Kernel, 2023), Gorilla (Patil et al., 2023) or low-level inference API ('autogen.oai' provides an enhanced LLM inference layer at this level) (Dibia, 2023). (2) When existing tools from LangChain etc. are helpful, one can use them as tool backends for AutoGen agents. For example, one can readily use tools, e.g., Wolfram Alpha, from LangChain in AutoGen agent. (3) For specific applications, one may want to leverage agents implemented in other libraries/packages. To achieve this, one could wrap those agents as conversable agents in AutoGen and then use them to build LLM applications through multi-agent conversation. (4) It can be hard to find an optimal operating point among many tunable choices, such as the LLM inference configuration. Blackbox optimization packages like 'flaml.tune' (Wang et al., 2021) can be used together with AutoGen to automate such tuning.</p>
<h1>B. 2 Future Work</h1>
<p>This work raises many research questions and future directions and .
Designing optimal multi-agent workflows: Creating a multi-agent workflow for a given task can involve many decisions, e.g., how many agents to include, how to assign agent roles and agent capabilities, how the agents should interact with each other, and whether to automate a particular part of the workflow. There may not exist a one-fits-all answer, and the best solution might depend on the specific application. This raises important questions: For what types of tasks and applications are multi-agent workflows most useful? How do multiple agents help in different applications? For a given task, what is the optimal (e.g., cost-effective) multi-agent workflow?
Creating highly capable agents: AutoGen can enable the development of highly capable agents that leverage the strengths of LLMs, tools, and humans. Creating such agents is crucial to ensuring that a multi-agent workflow can effectively troubleshoot and make progress on a task. For example, we observed that CAMEL, another multi-agent LLM system, cannot effectively solve problems in most cases primarily because it lacks the capability to execute tools or code. This failure shows that LLMs and multi-agent conversations with simple role playing are insufficient, and highly capable agents with diverse skill sets are essential. We believe that more systematic work will be required to develop guidelines for application-specific agents, to create a large OSS knowledge base of agents, and to create agents that can discover and upgrade their skills (Cai et al., 2023).
Enabling scale, safety, and human agency: Section 3 shows how complex multi-agent workflows can enable new applications, and future work will be needed to assess whether scaling further can help solve extremely complex tasks. However, as these workflows scale and grow more complex, it may become difficult to log and adjust them. Thus, it will become essential to develop clear mechanisms and tools to track and debug their behavior. Otherwise, these techniques risk resulting in incomprehensible, unintelligible chatter among agents (Lewis et al., 2017).
Our work also shows how complex, fully autonomous workflows with AutoGen can be useful, but fully autonomous agent conversations will need to be used with care. While the autonomous mode AutoGen supports could be desirable in many scenarios, a high level of autonomy can also pose potential risks, especially in high-risk applications (Amodei et al., 2016; Weld \&amp; Etzioni, 1994). As a result, building fail-safes against cascading failures and exploitation, mitigating reward hacking, out of control and undesired behaviors, maintaining effective human oversight of applications built with AutoGen agents will become important. While AutoGen provides convenient and seamless involvement of humans through a user proxy agent, developers and stakeholders still need to understand and determine the appropriate level and pattern of human involvement to ensure the safe and ethical use of the technology (Horvitz, 1999; Amershi et al., 2019).</p>
<h1>C Default System Message for Assistant Agent</h1>
<h2>System Message</h2>
<p>You are a helpful AI assistant. Solve tasks using your coding and language skills.
In the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.</p>
<ol>
<li>When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.</li>
<li>When you need to perform some task with code, use the code to perform the task and output the result. Finish the task smartly.</li>
</ol>
<p>Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step does your language skill.</p>
<p>When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can't modify your code. So do not suggest incomplete code which requires users to modify. Don't use a code block if it's not intended to be executed by the user.</p>
<p>If you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block as the first line. Don't include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use 'print' function for the output when relevant. Check the execution result returned by the user.</p>
<p>If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changed. If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.</p>
<p>When you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible.</p>
<p>Reply "TERMINATE" in the end when everything is done.</p>
<p>Prompting techniques color code: Role Play, Control Flow, Output Confine, Facilitate Automation, Grounding
Figure 5: Default system message for the built-in assistant agent in AutoGen (v0.1.1). This is an example of conversation programming via natural language. It contains instructions of different types, including role play, control flow, output confine, facilitate automation, and grounding.</p>
<p>Figure 5 shows the default system message for the built-in assistant agent in AutoGen (v0.1.1), where we introduce several new prompting techniques and highlight them accordingly. When combining these new prompting techniques together, we can program a fairly complex conversation even with the simplest two-agent conversation topology. This approach tries to exploit the capability of LLMs in implicit state inference to a large degree. LLMs do not follow all the instructions perfectly, so the design of the system needs to have other mechanisms to handle the exceptions and faults. Some instructions can have ambiguities, and the designer should either reduce them for preciseness or intentionally keep them for flexibility and address the different situations in other agents. In general, we observe that GPT-4 follows the instructions better than GPT-3.5-turbo.</p>
<h1>D Application Details</h1>
<h2>A1: Math Problem Solving</h2>
<p>Scenario 1: Autonomous Problem Solving. We perform both qualitative and quantitative evaluations in this scenario. For all evaluations, we use GPT-4 as the base model, and pre-install the "sympy" package in the execution environment. We compare AutoGen with the following LLMbased agent systems:</p>
<ul>
<li>AutoGPT: The out-of-box AutoGPT is used. We initialize AutoGPT by setting the purpose to "solve math problems", resulting in a "MathSolverGPT" with auto-generated goals.</li>
<li>ChatGPT+Plugin: We enable the Wolfram Alpha plugin (a math computation engine) in the OpenAI web client.</li>
<li>ChatGPT+Code Interpreter: This is a recent feature in OpenAI web client. Note that the above two premium features from ChatGPT require a paid subscription to be accessed and are the most competitive commercial systems.</li>
<li>LangChain ReAct+Python: We use Python agent from LangChain. To handle parsing errors, we set "handle_parsing_errors=True", and use the default zero-shot ReAct prompt.</li>
<li>Multi-Agent Debate (Liang et al., 2023): We modified the code of the multi-agent debate to perform evaluation. By default, there are three agents: an affirmative agent, a negative agent, and a moderator.</li>
</ul>
<p>We also conducted preliminary evaluations on several other multi-agent systems, including BabyAGI, CAMEL, and MetaGPT. The results indicate that they are not suitable choices for solving math problems out of the box. For instance, when MetaGPT is tasked with solving a math problem, it begins developing software to address the problem, but most of the time, it does not actually solve the problem. We have included the test examples in Appendix E.</p>
<p>Table 2: Qualitative evaluation of two math problems from the MATH dataset within the autonomous problem-solving scenario. Each LLM-based system is tested three times on each of the problems. This table reports the problem-solving correctness and summarizes the reasons for failure.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Correctness</th>
<th style="text-align: center;">Failure Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">AutoGen</td>
<td style="text-align: center;">$3 / 3$</td>
<td style="text-align: center;">N/A.</td>
</tr>
<tr>
<td style="text-align: left;">AutoGPT</td>
<td style="text-align: center;">$0 / 3$</td>
<td style="text-align: center;">The LLM gives code without the print function so the <br> result is not printed.</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT+Plugin</td>
<td style="text-align: center;">$1 / 3$</td>
<td style="text-align: center;">The return from Wolfram Alpha contains 2 simplified <br> results, including the correct answer, but GPT-4 always <br> chooses the wrong answer.</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT+Code Interpreter</td>
<td style="text-align: center;">$2 / 3$</td>
<td style="text-align: center;">Returns a wrong decimal result.</td>
</tr>
<tr>
<td style="text-align: left;">LangChain ReAct</td>
<td style="text-align: center;">$0 / 3$</td>
<td style="text-align: center;">LangChain gives 3 different wrong answers.</td>
</tr>
<tr>
<td style="text-align: left;">Multi-Agent Debate</td>
<td style="text-align: center;">$0 / 3$</td>
<td style="text-align: center;">It gives 3 different wrong answers due to calculation errors.</td>
</tr>
</tbody>
</table>
<p>(a) Evaluation on the first problem that asks to simplify a square root fraction.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Correctness</th>
<th style="text-align: center;">Failure Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">AutoGen</td>
<td style="text-align: center;">$2 / 3$</td>
<td style="text-align: center;">The final answer from code execution is wrong.</td>
</tr>
<tr>
<td style="text-align: left;">AutoGPT</td>
<td style="text-align: center;">$0 / 3$</td>
<td style="text-align: center;">The LLM gives code without the print function so the <br> result is not printed.</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT+Plugin</td>
<td style="text-align: center;">$1 / 3$</td>
<td style="text-align: center;">For one trial, GPT-4 got stuck because it keeps giving <br> wrong queries and has to be stopped. Another trial simply <br> gives a wrong answer.</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT+Code Interpreter</td>
<td style="text-align: center;">$0 / 3$</td>
<td style="text-align: center;">It gives 3 different wrong answers.</td>
</tr>
<tr>
<td style="text-align: left;">LangChain ReAct</td>
<td style="text-align: center;">$0 / 3$</td>
<td style="text-align: center;">LangChain gives 3 different wrong answers.</td>
</tr>
<tr>
<td style="text-align: left;">Multi-Agent Debate</td>
<td style="text-align: center;">$0 / 3$</td>
<td style="text-align: center;">It gives 3 different wrong answers.</td>
</tr>
</tbody>
</table>
<p>(b) Evaluation on the second number theory problem.</p>
<p>For the qualitative evaluation, we utilize two level-5 problems from the MATH dataset, testing each problem three times. The first problem involves simplifying a square root fraction, and the second</p>
<p>problem involves solving a number theory issue. The correctness counts and reasons for failure are detailed in Table 2. For the quantitative evaluation, we conduct two sets of experiments on the MATH dataset to assess the correctness of these systems: (1) an experiment involving 120 level-5 (the most challenging level) problems, including 20 problems from six categories, excluding geometry, and (2) an experiment on the entire test set, which includes 5000 problems. We exclude AutoGPT from this evaluation as it cannot access results from code executions and does not solve any problems in the qualitative evaluation. Our analysis of the entire dataset reveals that AutoGen achieves an overall accuracy of $69.48 \%$, while GPT-4's accuracy stands at $55.18 \%$. From these evaluations, we have the following observations regarding the problem-solving success rate and user experience of these systems:</p>
<ul>
<li>Problem-solving success rate: Results from the quantitative evaluations show that AutoGen can help achieve the highest problem-solving success rate among all the compared methods. The qualitative evaluations elucidate common failure reasons across several alternative approaches. ChatGPT+Code Interpreter fails to solve the second problem, and ChatGPT+Plugin struggles to solve both problems. AutoGPT fails on both problems due to code execution issues. The LangChain agent also fails on both problems, producing code that results in incorrect answers in all trials.</li>
<li>Based on the qualitative evaluation, we analyze the user experience concerning the verbosity of the response and the ability of the LLM-based system to run without unexpected behaviors. ChatGPT+Plugin is the least verbose, mainly because Wolfram queries are much shorter than Python code. AutoGen, ChatGPT+Code Interpreter, and LangChain exhibit similar verbosity, although LangChain is slightly more verbose due to more code execution errors. AutoGPT is the most verbose system owing to predefined steps like THOUGHTS, REASONING, and PLAN, which it includes in replies every time. Overall, AutoGen and ChatGPT+Code Interpreter operate smoothly without exceptions. We note the occurrences of undesired behaviors from other LLM-based systems that could affect user experience: AutoGPT consistently outputs code without the print' statement and cannot correct this, requiring the user to run them manually; ChatGPT with Wolfram Alpha plugin has the potential to become stuck in a loop that must be manually stopped; and Langchain ReAct could exit with a parse error, necessitating the passing of a 'handle.parse_error' parameter.
<img alt="img-4.jpeg" src="img-4.jpeg" /></li>
</ul>
<p>Figure 6: Examples of three settings utilized to solve math problems using AutoGen: (Gray) Enables a workflow where a student collaborates with an assistant agent to solve problems, either autonomously or in a human-in-the-loop mode. (Gray + Orange) Facilitates a more sophisticated workflow wherein the assistant, on the fly, can engage another user termed "expert", who is in the loop with their own assistant agent, to aid in problem-solving if its own solutions are not satisfactory.</p>
<p>Scenario 2: Human-in-the-loop Problem Solving. For challenging problems that these LLM systems cannot solve autonomously, human feedback during the problem-solving process can be</p>
<p>helpful. To incorporate human feedback with AutoGen, one can set human_input_mode='ALWAYS' in the user proxy agent. We select one challenging problem that none of these systems can solve autonomously across three trials. We adhere to the process outlined below to provide human inputs for all the compared methods:</p>
<ol>
<li>Input the problem: Find the equation of the plane which bisects the angle between the planes $3 x-6 y+2 z+5=0$ and $4 x-12 y+3 z-3=0$, and which contains the point $(-5,-1,-5)$. Enter your answer in the form</li>
</ol>
<p>$$
A x+B y+C z+D=0
$$</p>
<p>where $A, B, C, D$ are integers such that $A \quad&gt;\quad 0$ and $\operatorname{gcd}(|A|,|B|,|C|,|D|)=1$.
2. The response from the system does not solve the problem correctly. We then give a hint to the model: Your idea is not correct. Let's solve this together. Suppose $P=(x, y, z)$ is a point that lies on a plane that bisects the angle, the distance from P to the two planes is the same. Please set up this equation first.
3. We expect the system to give the correct distance equation. Since the equation involves an absolute sign that is hard to solve, we would give the next hint: Consider the two cases to remove the abs sign and get two possible solutions.
4. If the system returns the two possible solutions and doesn't continue to the next step, we give the last hint: Use point $(-5,-1,-5)$ to determine which is correct and give the final answer.
5. Final answer is $11 \mathrm{x}+6 \mathrm{y}+5 \mathrm{z}+86=0$.</p>
<p>We observed that AutoGen consistently solved the problem across all three trials. ChatGPT+Code Interpreter and ChatGPT+Plugin managed to solve the problem in two out of three trials, while AutoGPT failed to solve it in all three attempts. In its unsuccessful attempt, ChatGPT+Code Interpreter failed to adhere to human hints. In its failed trial, ChatGPT+Plugin produced an almost correct solution but had a sign discrepancy in the final answer. AutoGPT was unable to yield a correct solution in any of the trials. In one trial, it derived an incorrect distance equation. In the other two trials, the final answer was incorrect due to code execution errors.</p>
<p>Scenario 3: Multi-User Problem Solving. Next-generation LLM applications may necessitate the involvement of multiple real users for collectively solving a problem with the assistance of LLMs. We showcase how AutoGen can be leveraged to effortlessly construct such a system. Specifically, building upon scenario 2 mentioned above, we aim to devise a simple system involving two human users: a student and an expert. In this setup, the student interacts with an LLM assistant to address some problems, and the LLM automatically resorts to the expert when necessary.
The overall workflow is as follows: The student chats with the LLM-based assistant agent through a student proxy agent to solve problems. When the assistant cannot solve the problem satisfactorily, or the solution does not match the expectation of the student, it would automatically hold the conversation and call the pre-defined ask_for_expert function via the function_call feature of GPT in order to resort to the expert. Specifically, it would automatically produce the initial message for the ask_for_expert function, which could be the statement of the problem or the request to verify the solution to a problem, and the expert is supposed to respond to this message with the help of the expert assistant. After the conversation between the expert and the expert's assistant, the final message would be sent back to the student assistant as the response to the initial message. Then, the student assistant would resume the conversation with the student using the response from the expert for a better solution. A detailed visualization is shown in Figure 6.
With AutoGen, constructing the student/expert proxy agent and the assistant agents is straightforward by reusing the built-in UserProxyAgent and AssistantAgent through appropriate configurations. The only development required involves writing several lines of code for the ask_for_expert function, which then becomes part of the configuration for the assistant. Additionally, it's easy to extend such a system to include more than one expert, with a specific ask_for_expert function for each, or to include multiple student users with a shared expert for consultation.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ Results of ReAct are obtained by directly running its official code with default settings. The code uses text-davinci-003 as backend LM and does not support GPT-3.5-turbo or GPT-4.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7593 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7593</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7593</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-ffdbd7f0b03b85747b001b4734d5ee31b5229aa4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ffdbd7f0b03b85747b001b4734d5ee31b5229aa4" target="_blank">The Power of Scale for Parameter-Efficient Prompt Tuning</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work explores “prompt tuning,” a simple yet effective mechanism for learning “soft prompts” to condition frozen language models to perform specific downstream tasks and shows that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient “Prompt ensembling.”</p>
                <p><strong>Paper Abstract:</strong> In this work, we explore “prompt tuning,” a simple yet effective mechanism for learning “soft prompts” to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3’s few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method “closes the gap” and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed “prefix tuning” of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient “prompt ensembling.” We release code and model checkpoints to reproduce our experiments.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7593.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7593.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt Tuning (learned continuous soft prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parameter-efficient adaptation method that prepends a small set of learned continuous token embeddings (soft prompt) to the input while keeping the main pre-trained LM frozen; only the prompt embeddings are trained by backpropagation on downstream labeled data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 1.1 (multiple sizes: Small, Base, Large, XL, XXL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder Transformer (T5 1.1 checkpoints) used as a frozen backbone; prompt tuning updates only a p x e prompt embedding matrix prepended to the input.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Small (≈77M), Base (≈247M), Large (≈783M), XL (≈2.85B), XXL (≈11.1B)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SuperGLUE (8 tasks aggregated)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A suite of 8 challenging English NLU tasks (BoolQ, CB, COPA, MultiRC, ReCoRD, RTE, WiC, WSC) aggregated into a single benchmark score.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Text-to-text conditional generation with a learned continuous prompt prepended to the embedded input (soft prompt of length p tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / prompt representation</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Soft prompt: learned continuous embeddings (prompt length p, default 100 tokens in main config). Prompts are trained end-to-end on supervised examples; variants include different initializations (random, sampled vocabulary, class-label) and lengths. LM-adapted vs span-corruption pretrained backbones evaluated. Evaluation reports mean and stddev across 3 runs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SuperGLUE development aggregate score (mean ± stddev across 3 runs)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Prompt tuning performance increases with model scale and with appropriate prompt design; at T5-XXL prompt tuning matches the strong multi-task model-tuning baseline (qualitative parity reported).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Model tuning (full fine-tuning) on same T5 checkpoints; multi-task model tuning baseline is cited as the strong baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Closing gap with scale: prompt tuning moves from substantially below model tuning at small sizes to matching multi-task model tuning at XXL (no single absolute number stated for prompt-tuned XXL in text; qualitative 'matches').</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Default configuration: LM-adapted T5 (100K steps), prompt length 100, class-label initialization, trained 30K prompt steps, learning rate 0.3, batch size 32; metrics selected via early stopping on dev.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Power of Scale for Parameter-Efficient Prompt Tuning', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7593.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7593.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Length Ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of Soft Prompt Length on Prompt Tuning Performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systematic ablation varying prompt length (1, 5, 20, 100, 150 tokens) showing that longer prompts (≥20 tokens) generally improve performance for most model sizes, but very large models (XXL) remain strong even with single-token prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 1.1 (Small, Base, Large, XL, XXL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Frozen T5 1.1 checkpoints; only prompt embeddings updated.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Small to XXL (see previous entry)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SuperGLUE tasks (per-task prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same as SuperGLUE tasks; each prompt trained for a single SuperGLUE task.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Soft prompt prepended to embedded input; prompt length p varied across experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style (length)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Prompt lengths tested: {1, 5, 20, 100, 150}. Default other hyperparameters. Observed diminishing returns beyond ~20 tokens; very large models (XXL) perform well even with p=1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SuperGLUE dev aggregate and per-task metrics (mean ± stddev across 3 runs)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Increasing prompt length to ≥20 tokens generally gives a large boost in performance for most model sizes; for XXL a single-token prompt still yields strong results (qualitative; plotted means provided in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Single-token prompt (1 token) performance for each model size.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Often large absolute gains when increasing from 1 to 20 tokens for small/medium models; marginal gains beyond 20 tokens; XXL shows little dependence on length (relative improvement vs 1-token is small).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>All other settings default: LM-adapted T5 (100K steps), class-label init, 30K prompt training steps, constant LR 0.3, batch 32.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Power of Scale for Parameter-Efficient Prompt Tuning', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7593.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7593.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Initialization Ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of Prompt Initialization (Random, Sampled Vocabulary, Class-label)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ablation comparing different initializations for soft prompt embeddings: Random uniform, sampled vocabulary embeddings, and initializing prompt tokens using downstream class-label embeddings; class-label initialization performs best at small sizes but differences vanish at XXL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 1.1 (Small to XXL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Frozen T5 1.1 models; only prompt embeddings updated from different initializations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Small to XXL</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SuperGLUE per-task prompts</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Text-to-text mapping for each SuperGLUE dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Soft prompt prepended to input, initialization strategy varied.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style (initialization)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Initialization options: random uniform in [-0.5,0.5]; sampled vocabulary tokens (top 5000 SentencePiece tokens); class-label initialization where prompt tokens seeded with embeddings of class tokens (multi-token labels averaged). When class labels are fewer than prompt length, sampled vocab fills remainder.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SuperGLUE dev aggregate (mean ± stddev over 3 runs)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Class-label initialization performs best for smaller models; random initialization lags. At XXL model size, initialization differences disappear and all initializations give similar performance.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Random uniform initialization baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>At smaller sizes: class-label > sampled-vocab > random (absolute gaps reported in plots; no single-number gap quoted in text). At XXL: ≈0 absolute difference.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Default config otherwise (LM adaptation 100K steps, prompt length 100, 30K prompt training steps).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Power of Scale for Parameter-Efficient Prompt Tuning', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7593.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7593.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pretraining Objective & Sentinels</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of Pretraining Objective (Span Corruption vs LM Adaptation) and Adding Sentinels to Targets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluates how the pretraining objective of T5 influences the ability of frozen models to be controlled by prompts: LM adaptation (further training on causal LM objective) improves prompt-tuning performance compared to using vanilla span-corruption T5; adding sentinels to downstream targets provides little benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 1.1 (Span-corruption pretrained, LM-adapted variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5 1.1 originally trained on a span-corruption objective; LM adaptation continues self-supervised training for up to 100K steps on a language-modeling objective (natural prefix→continuation) producing a frozen model for prompt tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Small to XXL (benefit noted across sizes, but XXL more robust)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SuperGLUE (and per-task failures noted)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Standard SuperGLUE tasks cast text-to-text.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Soft prompt prepended; downstream targets either natural text or prepended with sentinel to mimic span-corruption targets.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>pretraining & prompt-target formatting</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Three settings: (1) Span Corruption (off-the-shelf T5), (2) Span Corruption + Sentinel prepended to downstream targets, (3) LM Adaptation (continue pretraining on causal LM objective for up to 100K steps).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SuperGLUE dev aggregate; task-level failure modes (percentages) also described.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>LM adaptation improves prompt-tuning quality across model sizes; span-corruption models perform poorly and unreliably (many mid-sized models fail to output legal labels and score 0% on some tasks). Adding a sentinel to targets provides little benefit over raw span-corruption models.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Span-corruption pretrained frozen T5 (with and without sentinel in targets).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>LM adaptation yields consistent, sometimes large gains versus span-corruption; exact numeric gains vary by model and task (plots in paper show improvement; adaptation longer up to 100K steps gives larger gains).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>LM adaptation durations varied up to 100K steps; prompt training unchanged. Observed that adaptation of up to 100K steps (≈10% of original pretraining steps) provided gains.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Power of Scale for Parameter-Efficient Prompt Tuning', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7593.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7593.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 Few-shot Prompt (Prompt Design)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 few-shot textual in-context prompt (manual / discrete prompt design)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Discrete human-designed or searched textual prompts that include task descriptions and few demonstration examples prepended to input tokens (in-context learning) used with large autoregressive models like GPT-3, reported to yield competitive few-shot performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (various sizes referenced: XL, 175B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive transformer trained with next-token prediction; evaluated in few-shot in-context learning mode using manually designed or searched discrete prompts (examples + instructions).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3 XL (unspecified here), GPT-3 175B (the 175B parameter model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SuperGLUE (dev aggregate comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same SuperGLUE benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Manual few-shot textual prompt: discrete natural-language prompt consisting of task description and several canonical input-output examples (in-context examples).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style (few-shot in-context)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>GPT-3 few-shot uses 500–2000 token prompt windows in many settings; here the paper references reported few-shot performance on SuperGLUE from Brown et al. (2020). The authors also attempted to use GPT-3 manual prompts with their LM-adapted T5 but performance was far below GPT-3, likely due to model/pretraining differences and shorter context length.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SuperGLUE dev aggregate score (as reported in Brown et al. 2020 and compared qualitatively in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GPT-3 175B few-shot SuperGLUE dev score = 71.8 (reported by Brown et al. 2020).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Fine-tuned T5-XXL (model tuning) dev score = 89.3 (cited), i.e., GPT-3 few-shot is 17.5 points lower than fine-tuned T5-XXL.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>-17.5 points absolute (GPT-3 175B few-shot 71.8 vs fine-tuned T5-XXL 89.3). Additionally, prompt-tuned T5-Small matches GPT-3 XL and prompt-tuned T5-Large beats GPT-3 175B (qualitative claims in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>GPT-3 results taken from Brown et al. (2020) few-shot experiments; prompt-tuned T5 experiments use LM-adapted checkpoints and supervised prompt training (see paper defaults).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Power of Scale for Parameter-Efficient Prompt Tuning', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7593.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7593.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Ensembling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt Ensembling (multiple learned prompts over single frozen model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Train multiple independent soft prompts for the same task on a shared frozen model and ensemble their outputs (majority voting) to boost performance while incurring much smaller storage and inference cost than full-model ensembling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-XXL (frozen)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Single frozen T5-XXL model; ensemble by varying prompt embeddings across replicated inputs in a single batch.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>XXL (≈11.1B)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SuperGLUE per-task ensembles (5 prompts per task)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Each ensemble consists of five independently trained prompts for the same downstream task; majority voting aggregates predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Soft prompt ensembling: multiple learned prompts (each a p-token soft prompt) applied across a batch of replicated inputs; ensemble decision via majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style (ensembling)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Five prompts trained independently on same frozen model; inference uses single forward pass with batch size N where each batch item uses a different prompt for the same input. Reported to beat the average single prompt and match or exceed the best single prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Per-task metrics and SuperGLUE aggregate (table and text summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Five-prompt ensemble built from single frozen T5-XXL exceeds both the average and the best among the five individual prompts on SuperGLUE tasks (no single numeric aggregate included in text for ensemble).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Single learned prompt performance (mean and best among 5 prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Ensemble > average single prompt and >= best single prompt (absolute gains task-dependent; paper reports consistent improvement across tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Five independent prompts trained with default prompt-tuning configs on frozen T5-XXL; majority-vote aggregation at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Power of Scale for Parameter-Efficient Prompt Tuning', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7593.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7593.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Domain-Shift Robustness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt Tuning versus Model Tuning under Domain Shift (zero-shot transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analysis showing that freezing the core language model and only tuning prompts often improves zero-shot transfer to out-of-domain datasets relative to full model fine-tuning, with notable F1 gains on some QA datasets and improvements in paraphrase transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 1.1 (frozen backbone for prompt tuning; tuned backbone for model tuning baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Comparison between frozen T5 with learned soft prompts vs fully fine-tuned T5 (model tuning) on transfer tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Experiments mostly use large appropriate T5 sizes (paper reports results aggregated; specific table results are model-size-agnostic in text).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MRQA out-of-domain QA (SQuAD→others) and paraphrase transfer (QQP↔MRPC)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot evaluation: train on one dataset, evaluate directly on other domain datasets without further adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>For prompt tuning: soft prompt prepended and trained on in-domain training set; for model tuning: standard full fine-tuning. Evaluation zero-shot on out-of-domain dev sets.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>training regime / adaptation format</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>QA: Trained on SQuAD, evaluated on out-of-domain MRQA datasets (TextbookQA, RACE, BioASQ, RE, DuoRC, DROP). Paraphrase: trained on QQP or MRPC, zero-shot evaluate on the other.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>F1 for QA (SQuAD-style), Accuracy and F1 for paraphrase transfer (QQP/MRPC).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>QA: Prompt tuning outperforms model tuning on majority of out-of-domain datasets, with up to +12.5 F1 advantage on TextbookQA. Paraphrase QQP→MRPC: Model tuning = 73.1 ± 0.9 acc / 81.2 ± 2.1 F1; Prompt tuning = 76.3 ± 0.1 acc / 84.3 ± 0.3 F1. MRPC→QQP: Model tuning = 74.9 ± 1.3 acc / 70.9 ± 1.2 F1; Prompt tuning = 75.4 ± 0.8 acc / 69.7 ± 0.3 F1.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Full model tuning (fine-tuning) results as listed in table rows above.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>TextbookQA: prompt tuning +12.5 F1 absolute over model tuning. QQP→MRPC: prompt tuning +3.2 accuracy and +3.1 F1 absolute; MRPC→QQP: prompt tuning +0.5 accuracy and -1.2 F1 (mixed).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot transfer: train on in-domain dataset, select checkpoints on in-domain validation, evaluate on out-of-domain dev sets; means and stddev across 3 runs reported.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Power of Scale for Parameter-Efficient Prompt Tuning', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Prefix-tuning: Optimizing continuous prompts for generation <em>(Rating: 2)</em></li>
                <li>AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts <em>(Rating: 2)</em></li>
                <li>WARP: Word-level Adversarial ReProgramming <em>(Rating: 2)</em></li>
                <li>Exploring the limits of transfer learning with a unified text-to-text transformer <em>(Rating: 2)</em></li>
                <li>GPT understands, too <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7593",
    "paper_id": "paper-ffdbd7f0b03b85747b001b4734d5ee31b5229aa4",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "Prompt Tuning",
            "name_full": "Prompt Tuning (learned continuous soft prompts)",
            "brief_description": "A parameter-efficient adaptation method that prepends a small set of learned continuous token embeddings (soft prompt) to the input while keeping the main pre-trained LM frozen; only the prompt embeddings are trained by backpropagation on downstream labeled data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5 1.1 (multiple sizes: Small, Base, Large, XL, XXL)",
            "model_description": "Encoder-decoder Transformer (T5 1.1 checkpoints) used as a frozen backbone; prompt tuning updates only a p x e prompt embedding matrix prepended to the input.",
            "model_size": "Small (≈77M), Base (≈247M), Large (≈783M), XL (≈2.85B), XXL (≈11.1B)",
            "task_name": "SuperGLUE (8 tasks aggregated)",
            "task_description": "A suite of 8 challenging English NLU tasks (BoolQ, CB, COPA, MultiRC, ReCoRD, RTE, WiC, WSC) aggregated into a single benchmark score.",
            "problem_format": "Text-to-text conditional generation with a learned continuous prompt prepended to the embedded input (soft prompt of length p tokens).",
            "format_category": "prompt style / prompt representation",
            "format_details": "Soft prompt: learned continuous embeddings (prompt length p, default 100 tokens in main config). Prompts are trained end-to-end on supervised examples; variants include different initializations (random, sampled vocabulary, class-label) and lengths. LM-adapted vs span-corruption pretrained backbones evaluated. Evaluation reports mean and stddev across 3 runs.",
            "performance_metric": "SuperGLUE development aggregate score (mean ± stddev across 3 runs)",
            "performance_value": "Prompt tuning performance increases with model scale and with appropriate prompt design; at T5-XXL prompt tuning matches the strong multi-task model-tuning baseline (qualitative parity reported).",
            "baseline_performance": "Model tuning (full fine-tuning) on same T5 checkpoints; multi-task model tuning baseline is cited as the strong baseline.",
            "performance_change": "Closing gap with scale: prompt tuning moves from substantially below model tuning at small sizes to matching multi-task model tuning at XXL (no single absolute number stated for prompt-tuned XXL in text; qualitative 'matches').",
            "experimental_setting": "Default configuration: LM-adapted T5 (100K steps), prompt length 100, class-label initialization, trained 30K prompt steps, learning rate 0.3, batch size 32; metrics selected via early stopping on dev.",
            "statistical_significance": null,
            "uuid": "e7593.0",
            "source_info": {
                "paper_title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Prompt Length Ablation",
            "name_full": "Effect of Soft Prompt Length on Prompt Tuning Performance",
            "brief_description": "Systematic ablation varying prompt length (1, 5, 20, 100, 150 tokens) showing that longer prompts (≥20 tokens) generally improve performance for most model sizes, but very large models (XXL) remain strong even with single-token prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5 1.1 (Small, Base, Large, XL, XXL)",
            "model_description": "Frozen T5 1.1 checkpoints; only prompt embeddings updated.",
            "model_size": "Small to XXL (see previous entry)",
            "task_name": "SuperGLUE tasks (per-task prompts)",
            "task_description": "Same as SuperGLUE tasks; each prompt trained for a single SuperGLUE task.",
            "problem_format": "Soft prompt prepended to embedded input; prompt length p varied across experiments.",
            "format_category": "prompt style (length)",
            "format_details": "Prompt lengths tested: {1, 5, 20, 100, 150}. Default other hyperparameters. Observed diminishing returns beyond ~20 tokens; very large models (XXL) perform well even with p=1.",
            "performance_metric": "SuperGLUE dev aggregate and per-task metrics (mean ± stddev across 3 runs)",
            "performance_value": "Increasing prompt length to ≥20 tokens generally gives a large boost in performance for most model sizes; for XXL a single-token prompt still yields strong results (qualitative; plotted means provided in paper).",
            "baseline_performance": "Single-token prompt (1 token) performance for each model size.",
            "performance_change": "Often large absolute gains when increasing from 1 to 20 tokens for small/medium models; marginal gains beyond 20 tokens; XXL shows little dependence on length (relative improvement vs 1-token is small).",
            "experimental_setting": "All other settings default: LM-adapted T5 (100K steps), class-label init, 30K prompt training steps, constant LR 0.3, batch 32.",
            "statistical_significance": null,
            "uuid": "e7593.1",
            "source_info": {
                "paper_title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Prompt Initialization Ablation",
            "name_full": "Effect of Prompt Initialization (Random, Sampled Vocabulary, Class-label)",
            "brief_description": "Ablation comparing different initializations for soft prompt embeddings: Random uniform, sampled vocabulary embeddings, and initializing prompt tokens using downstream class-label embeddings; class-label initialization performs best at small sizes but differences vanish at XXL.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5 1.1 (Small to XXL)",
            "model_description": "Frozen T5 1.1 models; only prompt embeddings updated from different initializations.",
            "model_size": "Small to XXL",
            "task_name": "SuperGLUE per-task prompts",
            "task_description": "Text-to-text mapping for each SuperGLUE dataset.",
            "problem_format": "Soft prompt prepended to input, initialization strategy varied.",
            "format_category": "prompt style (initialization)",
            "format_details": "Initialization options: random uniform in [-0.5,0.5]; sampled vocabulary tokens (top 5000 SentencePiece tokens); class-label initialization where prompt tokens seeded with embeddings of class tokens (multi-token labels averaged). When class labels are fewer than prompt length, sampled vocab fills remainder.",
            "performance_metric": "SuperGLUE dev aggregate (mean ± stddev over 3 runs)",
            "performance_value": "Class-label initialization performs best for smaller models; random initialization lags. At XXL model size, initialization differences disappear and all initializations give similar performance.",
            "baseline_performance": "Random uniform initialization baseline.",
            "performance_change": "At smaller sizes: class-label &gt; sampled-vocab &gt; random (absolute gaps reported in plots; no single-number gap quoted in text). At XXL: ≈0 absolute difference.",
            "experimental_setting": "Default config otherwise (LM adaptation 100K steps, prompt length 100, 30K prompt training steps).",
            "statistical_significance": null,
            "uuid": "e7593.2",
            "source_info": {
                "paper_title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Pretraining Objective & Sentinels",
            "name_full": "Effect of Pretraining Objective (Span Corruption vs LM Adaptation) and Adding Sentinels to Targets",
            "brief_description": "Evaluates how the pretraining objective of T5 influences the ability of frozen models to be controlled by prompts: LM adaptation (further training on causal LM objective) improves prompt-tuning performance compared to using vanilla span-corruption T5; adding sentinels to downstream targets provides little benefit.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5 1.1 (Span-corruption pretrained, LM-adapted variants)",
            "model_description": "T5 1.1 originally trained on a span-corruption objective; LM adaptation continues self-supervised training for up to 100K steps on a language-modeling objective (natural prefix→continuation) producing a frozen model for prompt tuning.",
            "model_size": "Small to XXL (benefit noted across sizes, but XXL more robust)",
            "task_name": "SuperGLUE (and per-task failures noted)",
            "task_description": "Standard SuperGLUE tasks cast text-to-text.",
            "problem_format": "Soft prompt prepended; downstream targets either natural text or prepended with sentinel to mimic span-corruption targets.",
            "format_category": "pretraining & prompt-target formatting",
            "format_details": "Three settings: (1) Span Corruption (off-the-shelf T5), (2) Span Corruption + Sentinel prepended to downstream targets, (3) LM Adaptation (continue pretraining on causal LM objective for up to 100K steps).",
            "performance_metric": "SuperGLUE dev aggregate; task-level failure modes (percentages) also described.",
            "performance_value": "LM adaptation improves prompt-tuning quality across model sizes; span-corruption models perform poorly and unreliably (many mid-sized models fail to output legal labels and score 0% on some tasks). Adding a sentinel to targets provides little benefit over raw span-corruption models.",
            "baseline_performance": "Span-corruption pretrained frozen T5 (with and without sentinel in targets).",
            "performance_change": "LM adaptation yields consistent, sometimes large gains versus span-corruption; exact numeric gains vary by model and task (plots in paper show improvement; adaptation longer up to 100K steps gives larger gains).",
            "experimental_setting": "LM adaptation durations varied up to 100K steps; prompt training unchanged. Observed that adaptation of up to 100K steps (≈10% of original pretraining steps) provided gains.",
            "statistical_significance": null,
            "uuid": "e7593.3",
            "source_info": {
                "paper_title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "GPT-3 Few-shot Prompt (Prompt Design)",
            "name_full": "GPT-3 few-shot textual in-context prompt (manual / discrete prompt design)",
            "brief_description": "Discrete human-designed or searched textual prompts that include task descriptions and few demonstration examples prepended to input tokens (in-context learning) used with large autoregressive models like GPT-3, reported to yield competitive few-shot performance.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "mention",
            "model_name": "GPT-3 (various sizes referenced: XL, 175B)",
            "model_description": "Large autoregressive transformer trained with next-token prediction; evaluated in few-shot in-context learning mode using manually designed or searched discrete prompts (examples + instructions).",
            "model_size": "GPT-3 XL (unspecified here), GPT-3 175B (the 175B parameter model)",
            "task_name": "SuperGLUE (dev aggregate comparisons)",
            "task_description": "Same SuperGLUE benchmark.",
            "problem_format": "Manual few-shot textual prompt: discrete natural-language prompt consisting of task description and several canonical input-output examples (in-context examples).",
            "format_category": "prompt style (few-shot in-context)",
            "format_details": "GPT-3 few-shot uses 500–2000 token prompt windows in many settings; here the paper references reported few-shot performance on SuperGLUE from Brown et al. (2020). The authors also attempted to use GPT-3 manual prompts with their LM-adapted T5 but performance was far below GPT-3, likely due to model/pretraining differences and shorter context length.",
            "performance_metric": "SuperGLUE dev aggregate score (as reported in Brown et al. 2020 and compared qualitatively in paper)",
            "performance_value": "GPT-3 175B few-shot SuperGLUE dev score = 71.8 (reported by Brown et al. 2020).",
            "baseline_performance": "Fine-tuned T5-XXL (model tuning) dev score = 89.3 (cited), i.e., GPT-3 few-shot is 17.5 points lower than fine-tuned T5-XXL.",
            "performance_change": "-17.5 points absolute (GPT-3 175B few-shot 71.8 vs fine-tuned T5-XXL 89.3). Additionally, prompt-tuned T5-Small matches GPT-3 XL and prompt-tuned T5-Large beats GPT-3 175B (qualitative claims in paper).",
            "experimental_setting": "GPT-3 results taken from Brown et al. (2020) few-shot experiments; prompt-tuned T5 experiments use LM-adapted checkpoints and supervised prompt training (see paper defaults).",
            "statistical_significance": null,
            "uuid": "e7593.4",
            "source_info": {
                "paper_title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Prompt Ensembling",
            "name_full": "Prompt Ensembling (multiple learned prompts over single frozen model)",
            "brief_description": "Train multiple independent soft prompts for the same task on a shared frozen model and ensemble their outputs (majority voting) to boost performance while incurring much smaller storage and inference cost than full-model ensembling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-XXL (frozen)",
            "model_description": "Single frozen T5-XXL model; ensemble by varying prompt embeddings across replicated inputs in a single batch.",
            "model_size": "XXL (≈11.1B)",
            "task_name": "SuperGLUE per-task ensembles (5 prompts per task)",
            "task_description": "Each ensemble consists of five independently trained prompts for the same downstream task; majority voting aggregates predictions.",
            "problem_format": "Soft prompt ensembling: multiple learned prompts (each a p-token soft prompt) applied across a batch of replicated inputs; ensemble decision via majority voting.",
            "format_category": "prompt style (ensembling)",
            "format_details": "Five prompts trained independently on same frozen model; inference uses single forward pass with batch size N where each batch item uses a different prompt for the same input. Reported to beat the average single prompt and match or exceed the best single prompt.",
            "performance_metric": "Per-task metrics and SuperGLUE aggregate (table and text summaries)",
            "performance_value": "Five-prompt ensemble built from single frozen T5-XXL exceeds both the average and the best among the five individual prompts on SuperGLUE tasks (no single numeric aggregate included in text for ensemble).",
            "baseline_performance": "Single learned prompt performance (mean and best among 5 prompts).",
            "performance_change": "Ensemble &gt; average single prompt and &gt;= best single prompt (absolute gains task-dependent; paper reports consistent improvement across tasks).",
            "experimental_setting": "Five independent prompts trained with default prompt-tuning configs on frozen T5-XXL; majority-vote aggregation at inference.",
            "statistical_significance": null,
            "uuid": "e7593.5",
            "source_info": {
                "paper_title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Domain-Shift Robustness",
            "name_full": "Prompt Tuning versus Model Tuning under Domain Shift (zero-shot transfer)",
            "brief_description": "Analysis showing that freezing the core language model and only tuning prompts often improves zero-shot transfer to out-of-domain datasets relative to full model fine-tuning, with notable F1 gains on some QA datasets and improvements in paraphrase transfer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5 1.1 (frozen backbone for prompt tuning; tuned backbone for model tuning baseline)",
            "model_description": "Comparison between frozen T5 with learned soft prompts vs fully fine-tuned T5 (model tuning) on transfer tasks.",
            "model_size": "Experiments mostly use large appropriate T5 sizes (paper reports results aggregated; specific table results are model-size-agnostic in text).",
            "task_name": "MRQA out-of-domain QA (SQuAD→others) and paraphrase transfer (QQP↔MRPC)",
            "task_description": "Zero-shot evaluation: train on one dataset, evaluate directly on other domain datasets without further adaptation.",
            "problem_format": "For prompt tuning: soft prompt prepended and trained on in-domain training set; for model tuning: standard full fine-tuning. Evaluation zero-shot on out-of-domain dev sets.",
            "format_category": "training regime / adaptation format",
            "format_details": "QA: Trained on SQuAD, evaluated on out-of-domain MRQA datasets (TextbookQA, RACE, BioASQ, RE, DuoRC, DROP). Paraphrase: trained on QQP or MRPC, zero-shot evaluate on the other.",
            "performance_metric": "F1 for QA (SQuAD-style), Accuracy and F1 for paraphrase transfer (QQP/MRPC).",
            "performance_value": "QA: Prompt tuning outperforms model tuning on majority of out-of-domain datasets, with up to +12.5 F1 advantage on TextbookQA. Paraphrase QQP→MRPC: Model tuning = 73.1 ± 0.9 acc / 81.2 ± 2.1 F1; Prompt tuning = 76.3 ± 0.1 acc / 84.3 ± 0.3 F1. MRPC→QQP: Model tuning = 74.9 ± 1.3 acc / 70.9 ± 1.2 F1; Prompt tuning = 75.4 ± 0.8 acc / 69.7 ± 0.3 F1.",
            "baseline_performance": "Full model tuning (fine-tuning) results as listed in table rows above.",
            "performance_change": "TextbookQA: prompt tuning +12.5 F1 absolute over model tuning. QQP→MRPC: prompt tuning +3.2 accuracy and +3.1 F1 absolute; MRPC→QQP: prompt tuning +0.5 accuracy and -1.2 F1 (mixed).",
            "experimental_setting": "Zero-shot transfer: train on in-domain dataset, select checkpoints on in-domain validation, evaluate on out-of-domain dev sets; means and stddev across 3 runs reported.",
            "statistical_significance": null,
            "uuid": "e7593.6",
            "source_info": {
                "paper_title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
                "publication_date_yy_mm": "2021-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "rating": 2
        },
        {
            "paper_title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
            "rating": 2
        },
        {
            "paper_title": "WARP: Word-level Adversarial ReProgramming",
            "rating": 2
        },
        {
            "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "rating": 2
        },
        {
            "paper_title": "GPT understands, too",
            "rating": 1
        }
    ],
    "cost": 0.016195,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The Power of Scale for Parameter-Efficient Prompt Tuning</h1>
<p>Brian Lester* Rami Al-Rfou Noah Constant<br>Google Research<br>{brianlester,rmyeid,nconstant}@google.com</p>
<h4>Abstract</h4>
<p>In this work, we explore "prompt tuning," a simple yet effective mechanism for learning "soft prompts" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method "closes the gap" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed "prefix tuning" of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient "prompt ensembling." We release code and model checkpoints to reproduce our experiments. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>With the wide success of pre-trained large language models, a range of techniques has arisen to adapt these general-purpose models to downstream tasks. ELMo (Peters et al., 2018) proposed freezing the pre-trained model and learning a task-specific weighting of its per-layer representations. However, since GPT (Radford et al., 2018) and BERT</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Standard model tuning of T5 achieves strong performance, but requires storing separate copies of the model for each end task. Our prompt tuning of T5 matches the quality of model tuning as size increases, while enabling the reuse of a single frozen model for all tasks. Our approach significantly outperforms fewshot prompt design using GPT-3. We show mean and standard deviation across 3 runs for tuning methods.
(Devlin et al., 2019), the dominant adaptation technique has been model tuning (or "fine-tuning"), where all model parameters are tuned during adaptation, as proposed by Howard and Ruder (2018).</p>
<p>More recently, Brown et al. (2020) showed that prompt design (or "priming") is surprisingly effective at modulating a frozen GPT-3 model's behavior through text prompts. Prompts are typically composed of a task description and/or several canonical examples. This return to "freezing" pre-trained models is appealing, especially as model size continues to increase. Rather than requiring a separate copy of the model for each downstream task, a single generalist model can simultaneously serve many different tasks.</p>
<p>Unfortunately, prompt-based adaptation has several key drawbacks. Task description is error-prone and requires human involvement, and the effectiveness of a prompt is limited by how much condition-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Model tuning requires making a taskspecific copy of the entire pre-trained model for each downstream task and inference must be performed in separate batches. Prompt tuning only requires storing a small task-specific prompt for each task, and enables mixed-task inference using the original pretrained model. With a T5 "XXL" model, each copy of the tuned model requires 11 billion parameters. By contrast, our tuned prompts would only require 20,480 parameters per task-a reduction of over five orders of magnitude-assuming a prompt length of 5 tokens.
ing text can fit into the model's input. As a result, downstream task quality still lags far behind that of tuned models. For instance, GPT-3 175B fewshot performance on SuperGLUE is 17.5 points below fine-tuned T5-XXL (Raffel et al., 2020) (71.8 vs. 89.3) despite using 16 times more parameters.</p>
<p>Several efforts to automate prompt design have been recently proposed. Shin et al. (2020) propose a search algorithm over the discrete space of words, guided by the downstream application training data. While this technique outperforms manual prompt design, there is still a gap relative to model tuning.</p>
<p>Li and Liang (2021) propose "prefix tuning" and show strong results on generative tasks. This method freezes the model parameters and backpropagates the error during tuning to prefix activations prepended to each layer in the encoder stack, including the input layer. Hambardzumyan et al. (2021) simplify this recipe by restricting the trainable parameters to the input and output subnetworks of a masked language model, and show reasonable results on classifications tasks.</p>
<p>In this paper, we propose prompt tuning as a further simplification for adapting language models. We freeze the entire pre-trained model and only allow an additional $k$ tunable tokens per downstream task to be prepended to the input text. This "soft prompt" is trained end-to-end and can condense the signal from a full labeled dataset, allowing our method to outperform few-shot prompts and close the quality gap with model tuning (Figure 1). At
the same time, since a single pre-trained model is recycled for all downstream tasks, we retain the efficient serving benefits of frozen models (Figure 2).</p>
<p>While we developed our method concurrently with Li and Liang (2021) and Hambardzumyan et al. (2021), we are the first to show that prompt tuning alone (with no intermediate-layer prefixes or task-specific output layers) is sufficient to be competitive with model tuning. Through detailed experiments in sections 2-3, we demonstrate that language model capacity is a key ingredient for these approaches to succeed. As Figure 1 shows, prompt tuning becomes more competitive with scale.</p>
<p>We compare with similar approaches in Section 4. Explicitly separating task-specific parameters from the "generalist" parameters needed for general language-understanding has a range of additional benefits. We show in Section 5 that by capturing the task definition in the prompt while keeping the generalist parameters fixed, we are able to achieve better resilience to domain shifts. In Section 6, we show that "prompt ensembling", learning multiple prompts for the same task, can boost quality and is more efficient than classic model ensembling. Finally, in Section 7, we investigate the interpretability of our learned soft prompts. In sum, our key contributions are:</p>
<ol>
<li>Proposing prompt tuning and showing its competitiveness with model tuning in the regime of large language models.</li>
<li>Ablating many design choices, and showing quality and robustness improve with scale.</li>
<li>Showing prompt tuning outperforms model tuning on domain shift problems.</li>
<li>Proposing "prompt ensembling" and showing its effectiveness.</li>
</ol>
<h2>2 Prompt Tuning</h2>
<p>Following the "text-to-text" approach of T5 (Raffel et al., 2020), we cast all tasks as text generation. Instead of modeling classification as the probability of an output class given some input, $\operatorname{Pr}(y \mid X)$, where $X$ is a series of tokens and $y$ is a single class label, we now model it as conditional generation, where $Y$ is a sequence of tokens that represent a class label. T5 models classification as $\operatorname{Pr}_{\theta}(Y \mid X)$, parameterized by the weights, $\theta$, of the transformers (Vaswani et al., 2017) that make up its encoder and decoder.</p>
<p>Prompting is the approach of adding extra information for the model to condition on during its</p>
<p>generation of $Y$. Normally, prompting is done by prepending a series of tokens, $P$, to the input $X$, such that the model maximizes the likelihood of the correct $Y, \operatorname{Pr}<em 1="1">{\theta}(Y \mid[P ; X])$, while keeping the model parameters, $\theta$, fixed. In GPT-3, the representations of the prompt tokens, $P=$ $\left{p</em>}, p_{2}, \ldots, p_{n}\right}$, are part of the model's embedding table, parameterized by the frozen $\theta$. Finding an optimal prompt thus requires the selection of prompt tokens, through either manual search or non-differentiable search methods (Jiang et al., 2020; Shin et al., 2020). Prompt tuning removes the restriction that the prompt $P$ be parameterized by $\theta$; instead the prompt has its own dedicated parameters, $\theta_{P}$, that can be updated. While prompt design involves selecting prompt tokens from a fixed vocabulary of frozen embeddings, prompt tuning can be thought of as using a fixed prompt of special tokens, where only the embeddings of these prompt tokens can be updated. Our new conditional generation is now $\operatorname{Pr<em P="P">{\theta ; \theta</em>$.}}(Y \mid[P ; X])$ and can be trained by maximizing the likelihood of $Y$ via backpropagation, while only applying gradient updates to $\theta_{P</p>
<p>Given a series of $n$ tokens, $\left{x_{1}, x_{2}, \ldots, x_{n}\right}$, the first thing T5 does is embed the tokens, forming a matrix $X_{e} \in \mathbb{R}^{n \times e}$ where $e$ is the dimension of the embedding space. Our soft-prompts are represented as a parameter $P_{e} \in \mathbb{R}^{p \times e}$, where $p$ is the length of the prompt. Our prompt is then concatenated to the embedded input forming a single matrix $\left[P_{e} ; X_{e}\right] \in \mathbb{R}^{(p+n) \times e}$ which then flows though the encoder-decoder as normal. Our models are trained to maximize the probability of $Y$, but only the prompt parameters $P_{e}$ are updated.</p>
<h3>2.1 Design Decisions</h3>
<p>There are many possible ways to initialize the prompt representations. The simplest is to train from scratch, using random initialization. A more sophisticated option is to initialize each prompt token to an embedding drawn from the model's vocabulary. Conceptually, our soft-prompt modulates the frozen network's behavior in the same way as text preceding the input, so it follows that a word-like representation might serve as a good initialization spot. For classification tasks, a third option is to initialize the prompt with embeddings that enumerate the output classes, similar to the "verbalizers" of Schick and Schütze (2021). Since we want the model to produce these tokens in the
output, initializing the prompt with the embeddings of the valid target tokens should prime the model to restrict its output to the legal output classes.</p>
<p>Another design consideration is the length of the prompt. The parameter cost of our method is $E P$, where $E$ is the token embedding dimension and $P$ is the prompt length. The shorter the prompt, the fewer new parameters must be tuned, so we aim to find a minimal length that still performs well.</p>
<h3>2.2 Unlearning Span Corruption</h3>
<p>Unlike autoregressive language models like GPT-3, the T5 models we experiment with use an encoderdecoder architecture and pre-train on a span corruption objective. Specifically, T5 is tasked with "reconstructing" masked spans in the input text, which are marked with unique sentinel tokens. The target output text consists of all the masked content, separated by sentinels, plus a final sentinel. For instance, from the text "Thank you for inviting me to your party last week" we might construct a pre-training example where the input is "Thank you $\langle\mathrm{X}\rangle$ me to your party $\langle\mathrm{Y}\rangle$ week" and the target output is " $\langle\mathrm{X}\rangle$ for inviting $\langle\mathrm{Y}\rangle$ last $\langle\mathrm{Z}\rangle$ ".</p>
<p>While Raffel et al. (2020) find this architecture and pre-training objective more effective than traditional language modeling, we hypothesize that this setup is not a good fit for producing a frozen model that can be readily controlled through prompt tuning. In particular, a T5 model pre-trained exclusively on span corruption, such as T5 1.1, has never seen truly natural input text (free of sentinel tokens), nor has it ever been asked to predict truly natural targets. In fact, due to the details of T5's span corruption preprocessing, every pre-training target will begin with a sentinel. While this "unnatural" tendency to output sentinels is easy to overcome through fine-tuning, we suspect that it would be much harder to override through a prompt alone, as the decoder priors cannot be adjusted.</p>
<p>Given these concerns, we experiment with T5 models in three settings. (1) "Span Corruption": We use pre-trained T5 off-the-shelf as our frozen model, and test its ability to output the expected text for downstream tasks. (2) "Span Corruption + Sentinel": We use the same model, but prepend all downstream targets with a sentinel, so as to more closely resemble the targets seen in pretraining. (3) "LM Adaptation": We continue T5's self-supervised training for a small number of additional steps, but using the "LM" objective dis-</p>
<p>cussed by Raffel et al. (2020); given a natural text prefix as input, the model must produce the natural text continuation as output. Crucially, this adaptation happens only once, producing a single frozen model that we can reuse for prompt tuning across any number of downstream tasks.</p>
<p>Through LM adaptation, we hope to "quickly" transform T5 into a model more similar to GPT-3, which always outputs realistic text, and is known to respond well to prompts as a "few-shot learner". It is not obvious how successful this late-stage transformation will be compared to pre-training from scratch, and it has not been investigated previously to our knowledge. As such, we experiment with various lengths of adaptation up to 100K steps.</p>
<h2>3 Results</h2>
<p>Our frozen models are built on top of pre-trained T5 checkpoints of all sizes (Small, Base, Large, XL, XXL). We leverage the public T5 1.1 checkpoints, which include improvements over the original T5. ${ }^{2}$</p>
<p>Our "default" configuration, plotted with a green ' $\times$ ' $(\rightarrow)$ throughout, uses an LM-adapted version of T5 trained for an additional 100K steps, initializes using class labels (see Section 3.2), and uses a prompt length of 100 tokens. While this is longer than the default 10 -token prefix used by Li and Liang (2021), our method still uses fewer task-specific parameters, as we only tune the input layer, as opposed to overwriting activations in all network layers. See Figure 4 for a detailed comparison. We will also see shortly that even much shorter prompts are viable as model size increases.</p>
<p>We measure performance on the SuperGLUE benchmark (Wang et al., 2019a), a collection of eight challenging English language understanding tasks. ${ }^{3}$ We report metrics on the development set associated with each dataset.</p>
<p>Each of our prompts train on a single SuperGLUE task; there was no multi-task setup or mixing of training data across tasks. We translate each SuperGLUE dataset into a text-to-text format following Raffel et al. (2020), except that we omit the</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>task names prepended to inputs indicating which SuperGLUE task an example belongs to.</p>
<p>We train our prompts for 30,000 steps using T5's standard cross-entropy loss, with a constant learning rate of 0.3 and a batch size of 32 . Checkpoints are selected via early stopping on the development set, where the stopping metric is the default metric for the dataset, or the average of metrics for datasets evaluated with multiple metrics. All experiments were run in JAX (Bradbury et al., 2018) using the Adafactor optimizer (Shazeer and Stern, 2018) with weight decay $1 \epsilon-5, \beta_{2}$ decay 0.8 , and parameter scaling off. The models were implemented in Flax (Heek et al., 2020). More details are available in Appendix A.</p>
<h3>3.1 Closing the Gap</h3>
<p>To compare our method with standard model tuning, we tune the public T5 1.1 checkpoints on SuperGLUE using the default hyperparameters specified in the T5 library (learning rate 0.001 , and Adafactor optimizer with pre-training parameter states restored). We consider two baselines. (1) "Model Tuning": For an apples-to-apples comparison, we tune on each task separately, as in our prompt tuning setup. ${ }^{4}$ (2) "Model Tuning (Multitask)": We use T5's multi-task tuning setup to achieve a more competitive baseline. ${ }^{5}$ In this case, a single model is tuned on all tasks jointly, with a text prefix indicating the task name.</p>
<p>In Figure 1 (p. 1), we see that prompt tuning becomes more competitive with model tuning as scale increases. At the XXL size ( 11 billion parameters), prompt tuning matches even the stronger multi-task model tuning baseline, despite having over 20,000 times fewer task-specific parameters.</p>
<p>To compare with prompt design, we include GPT-3 few-shot performance on the SuperGLUE dev split, as reported by Brown et al. (2020). ${ }^{6}$ Figure 1 shows that prompt tuning beats GPT-3</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Ablations of various hyperparameters on prompt tuning performance (mean and stddev across 3 runs). In our "default" ( $\leftarrow$ ) configuration, quality improves stably with model size. Across all ablations, the largest (XXL) model is the most robust to hyperparameter choice. (a) Prompt length: Increasing to 20+ tokens generally confers a large boost, but XXL performs well even with single-token prompts. (b) Prompt initialization: Random uniform initialization lags behind more "advanced" initializations using sampled vocabulary or class label embeddings, but the difference vanishes at XXL size. (c) Pre-training objective: LM adaptation outperforms span corruption, even when a sentinel is added to downstream task targets, but XXL works well with any method. (d) LM adaptation: Longer adaptation generally gives larger gains, but XXL is robust to even short adaptation.
prompt design by a large margin, with prompttuned T5-Small matching GPT-3 XL (over 16 times larger), and prompt-tuned T5-Large beating GPT-3 175B (over 220 times larger).</p>
<h3>3.2 Ablation Study</h3>
<p>Prompt Length We train prompts for each model size while varying the prompt length in ${1,5,20,100,150}$ and fixing other settings to our default configuration. Figure 3(a) shows that for most model sizes, increasing prompt length beyond a single token is critical to achieve good performance. Notably, the XXL model still gives strong results with a single-token prompt, suggesting that the larger the model, the less conditioning signal is needed to achieve a target behavior. Across all models, increasing beyond 20 tokens only yields marginal gains. ${ }^{7}$</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Prompt Initialization We ablate the effect of prompt initialization by training models at all sizes while fixing other hyperparameters to their default values. For random initialization, we sample uniformly from the range $[-0.5,0.5]$. When initializing from sampled vocabulary, we restrict to the 5,000 most "common" tokens in T5's SentencePiece vocabulary (Kudo and Richardson, 2018), which is ordered by likelihood in the pre-training corpus. For "class label" initialization, we take the embeddings for the string representations of each class in the downstream task and use them to initialize one of the tokens in the prompt. ${ }^{8}$ When a class label is multi-token, we average the token embeddings. At longer prompt lengths, we often run out of class labels before we have initialized all</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>of the prompt tokens. In this case we fall back to our sampled vocab strategy to fill in the prompt.</p>
<p>Figure 3(b) shows our ablation of initialization strategy across model sizes, where we find that the class based initialization performs best. At smaller model sizes, there are large gaps between the different initializations, but once the model is scaled to XXL size, those differences disappear.</p>
<p>With "class label" initialization, we observe that the class labels typically persist in the learned prompts, such that the nearest token embeddings (in cosine distance) match the tokens used for initialization. Beyond this, we did not find our learned prompts to be interpretable, similar to those of <em>Shin et al. (2020)</em>. See Section 7 for details.</p>
<h3>Pre-training Objective</h3>
<p>In Figures 3(c) and 3(d), we see pre-training objective has a clear effect on prompt tuning quality. As hypothesized in Section 2.2, T5's default "span corruption" objective is not well-suited for training frozen models to be later conditioned by prompts. Intuitively, models pre-trained to read and write sentinel tokens are hard to apply directly to tasks of reading and writing text without sentinels. As seen in Figure 3(c), even the "workaround" of adding a sentinel to the downstream targets has little benefit. While LM adaptation adds value across all model sizes, we note our largest XXL model is the most forgiving and gives strong results even with span corruption.</p>
<p>Given the benefit of LM adaptation, we also explore how long of an adaptation is helpful. Figure 3(d) shows that longer adaptation provides additional gains, up to 100K steps. This suggests that the "transition" from span corruption to a language modeling objective is not a trivial change, and making an effective switch takes an investment of training resources (10% of the steps of the original T5 pre-training). At the same time, as in our other ablations, we observe that the XXL model is robust to even non-ideal configurations. At this size, the gains from adaptation are quite modest.</p>
<p>In the non-optimal "span corruption" setting, we observe instability across model sizes, with the Small model outperforming the larger Base, Large, and XL models. On inspection, we find that for many tasks, these mid-sized models never learn to output a legal class label and thus score 0%. The two most common error modes are copying subspans from the input and predicting an empty string. Furthermore, this poor performance is not due to random variance in prompt tuning, as we observe</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p><strong>Figure 4:</strong> Parameter usage of various adaptation techniques, fixing architecture to T5 1.1 and prompt/prefix length to 1–100 tokens (bands show mean and stddev). <strong>Model Tuning:</strong> All parameters are task-specific. <strong>Prefix Tuning:</strong> Activations are tuned in the prefix of each layer, requiring 0.1–1% task-specific parameters for inference, but more are used for training. <strong>WARP:</strong> Task parameters are reduced to under 0.1% by only tuning input and output layers. <strong>Prompt Tuning:</strong> Only prompt embeddings are tuned, reaching under 0.01% for most model sizes. <strong>Prompt Design:</strong> Only a sequence of prompt IDs (500–2000 tokens) is required.</p>
<p>low variance across 3 runs for each size. These results indicate that using models pre-trained with the "span corruption" objective can be unreliable, with only 2 out of 5 models working well, whereas the LM adapted versions work reliably across all model sizes.</p>
<h3>4 Comparison to Similar Approaches</h3>
<p>In this section, we review recent work on learning continuous prompts, and draw comparisons with our method. One important axis of comparison is the number of task-specific parameters each method requires, as shown in Figure 4. Among methods with learnable parameters, prompt tuning is the most parameter efficient, requiring less than 0.01% task-specific parameters for models over a billion parameters.</p>
<p>Li and Liang (2021) propose "prefix tuning": learning a sequence of prefixes that are prepended at every transformer layer. This is akin to learning transformer activations that are fixed across examples.</p>
<p><sup>9</sup>To compare with prompt design, we count each token ID in the prompt as a parameter, and assume a prompt of between 500–2000 tokens to match the GPT-3 setting. While this technique is by far the most parameter efficient, it comes at the cost of task quality.</p>
<p>ples at every network layer. In contrast, prompt tuning uses a single prompt representation that is prepended to the embedded input. Beyond requiring fewer parameters, our approach allows the transformer to update the intermediate-layer task representations, as contextualized by an input example. Their work builds on GPT-2 (Radford et al., 2019) and BART (Lewis et al., 2020), while ours focuses on T5 and examines changes in performance and robustness to design choices as model size increases. When using BART, prefix tuning includes prefixes on both the encoder and decoder network, while prompt tuning only requires prompts on the encoder. Li and Liang (2021) also rely on a reparameterization of the prefix to stabilize learning, which adds a large number of parameters during training, whereas our configuration does not require this reparameterization and is robust across SuperGLUE tasks and model sizes.</p>
<p>Hambardzumyan et al. (2021) propose "WARP", where prompt parameters are added to the input layer. This method works with masked language models, relying on a [MASK] token and a learnable output layer to project the mask to class logits. This formulation restricts the model to producing a single output, limiting it to classification. Prompt tuning does not require any changes to the input or a task-specific head. The performance of prompt tuning is also considerably closer to the strong performance of model tuning.</p>
<p>Liu et al. (2021) propose "P-tuning" where learnable continuous prompts are interleaved throughout the embedded input, using patterns based on human design. Our approach removes this complication by simply prepending the prompt to the input. To achieve strong SuperGLUE results, P-tuning has to be used in conjunction with model tuning, that is, models jointly update both the prompt and the main model parameters, whereas our approach keeps the original language model frozen. ${ }^{10}$</p>
<p>Qin and Eisner (2021) use "soft words" to learn prompts to extract knowledge from pre-trained LMs. Prompts are positioned in relation to the input based on hand-designed prompt prototypes, and a learned $\Delta_{i}^{f}$ parameter is included for each layer, so parameter cost scales with model depth.</p>
<p>Logeswaran et al. (2020) use a learnable prepended token to adapt transformer models to var-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 1: F1 mean and stddev for models trained on SQuAD and evaluated on out-of-domain datasets from the MRQA 2019 shared task. Prompt tuning tends to give stronger zero-shot performance than model tuning, especially on datasets with large domain shifts like TextbookQA.
ious tasks, but focus on small synthetic datasets designed to accommodate a compositional task representation, as opposed to larger real-world datasets. Their base models are small transformers trained from scratch jointly with the task representations, whereas we keep the base model frozen and investigate scaling laws using larger transformers.</p>
<p>More generally, work on task prompts is closely aligned with work on "adapters" (Rebuffi et al., 2017; Houlsby et al., 2019), small bottleneck layers inserted between frozen pre-trained network layers. Adapters offer another means of reducing task-specific parameters, with Houlsby et al. (2019) achieving GLUE performance close to full model tuning when freezing BERT-Large and only adding $2-4 \%$ additional parameters. Pfeiffer et al. (2020) use multiple adapters in a multilingual context to explicitly separate language understanding from task specification, similar to our approach. A core difference between adapters and prompt tuning is how the approaches change model behavior. Adapters modify the actual function that acts on the input representation, parameterized by the neural network, by allowing the rewriting of activations at any given layer. Prompt tuning modifies behavior by leaving the function fixed and adding new input representations that can affect how subsequent input is processed.</p>
<h2>5 Resilience to Domain Shift</h2>
<p>By freezing the core language model parameters, prompt tuning prevents the model from modifying its general understanding of language. Instead, prompt representations indirectly modulate the representation of the input. This reduces the model's ability to overfit to a dataset by memorizing specific lexical cues and spurious correlations. This restriction suggests that prompt tuning may improve</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Train</th>
<th style="text-align: center;">Eval</th>
<th style="text-align: left;">Tuning</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">F1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">QQP</td>
<td style="text-align: center;">MRPC</td>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">$73.1 \pm 0.9$</td>
<td style="text-align: center;">$81.2 \pm 2.1$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;">Prompt</td>
<td style="text-align: center;">$\mathbf{7 6 . 3} \pm 0.1$</td>
<td style="text-align: center;">$\mathbf{8 4 . 3} \pm 0.3$</td>
</tr>
<tr>
<td style="text-align: left;">MRPC</td>
<td style="text-align: center;">QQP</td>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">$74.9 \pm 1.3$</td>
<td style="text-align: center;">$\mathbf{7 0 . 9} \pm 1.2$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;">Prompt</td>
<td style="text-align: center;">$\mathbf{7 5 . 4} \pm 0.8$</td>
<td style="text-align: center;">$69.7 \pm 0.3$</td>
</tr>
</tbody>
</table>
<p>Table 2: Mean and stddev of zero-shot domain transfer between two paraphrase detection tasks.
robustness to domain shifts, where the distribution of inputs differs between training and evaluation.</p>
<p>We investigate zero-shot domain transfer on two tasks: question answering (QA) and paraphrase detection. For question answering, we use the MRQA 2019 shared task on generalization (Fisch et al., 2019). This task collects extractive QA datasets in a unified format and tests how models trained on "in-domain" datasets perform when evaluated on "out-of-domain" datasets. For our experiments, we train on SQuAD (Rajpurkar et al., 2016) and evaluate on each of the out-of-domain datasets. ${ }^{11}$</p>
<p>Table 1 shows that prompt tuning outperforms model tuning on the majority of out-of-domain datasets, with a remarkable 12.5 point F1 gap between the two approaches on TextbookQA. We observe larger gains from prompt tuning in cases of larger domain shifts (e.g. to Biomedical in BioASQ or to Textbooks in TextbookQA). Of the datasets where model tuning is better, we see that DROP shares a domain (Wikipedia) with SQuAD and is thus one of the smallest domain transfers.</p>
<p>As a second test of robustness to domain shift, we explore transfer between two paraphrase detection tasks from GLUE (Wang et al., 2019b). The first task is QQP (Iyer et al., 2017), which asks if two questions from the community Q\&amp;A site Quora are "duplicates". The second task is MRPC (Dolan and Brockett, 2005), which asks if two sentences drawn from news articles are paraphrases. We test transfer in both directions (QQP $\Leftrightarrow$ MRPC). As before, we train on the "in-domain" task, select checkpoints using in-domain validation, and evaluate zero-shot on the "out-of-domain" task.</p>
<p>Table 2 shows that training a lightweight prompt on the QQP data and evaluating on MRPC gives much better performance than tuning the entire model ( +3.2 accuracy and +3.1 F1). The results are much closer in the other direction, with prompt</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 3: Performance of a five-prompt ensemble built from a single frozen T5-XXL model exceeds both the average and the best among the five prompts.
tuning showing a small improvement in accuracy and a small drop in F1. These results support the view that model tuning may be over-parameterized and more prone to overfit the training task, to the detriment of similar tasks in different domains.</p>
<h2>6 Prompt Ensembling</h2>
<p>Ensembles of neural models trained from different initializations on the same data are widely observed to improve task performance (Hansen and Salamon, 1990) and are useful for estimating model uncertainty (Lakshminarayanan et al., 2017). However, as model size increases, ensembling can become impractical. Beyond the space required to store $N$ models (e.g. 42 GiB for each copy of T5-XXL), there is a substantial inference cost to running $N$ distinct models, whether in parallel or in series.</p>
<p>Prompt tuning provides a more efficient way to ensemble multiple adaptations of a pre-trained language model. By training $N$ prompts on the same task, we create $N$ separate "models" for a task, while still sharing the core language modeling parameters throughout. Beyond drastically reducing storage costs, the prompt ensemble makes inference more efficient. To process one example, rather than computing forward passes of $N$ different models, we can execute a single forward pass with a batch size of $N$, replicating the example across the batch and varying the prompt. These savings mirror those seen for multi-tasking in Figure 2.</p>
<p>To demonstrate the viability of prompt ensembling, we train five prompts for each SuperGLUE task, using a single frozen T5-XXL model with our default hyperparameters. We use simple majority voting to compute predictions from the ensemble. Table 3 shows that across all tasks, the ensemble beats the single-prompt average and beats, or matches, the best individual prompt.</p>
<h2>7 Interpretability</h2>
<p>An ideally interpretable prompt would consist of natural language that clearly describes the task at hand, explicitly asks the model for some result or action, and makes it easy to understand why the prompt elicited such behavior from the model.</p>
<p>As prompt tuning works in the continuous embedding space rather than the discrete token space, interpreting prompts becomes more difficult. To test the interpretability of our learned soft prompts, we compute the nearest neighbors to each prompt token from the frozen model's vocabulary. We use cosine distance between the vocabulary embedding vector and the prompt token representation as the similarity metric.</p>
<p>We observe that for a given learned prompt token, the top-5 nearest neighbors form tight semantic clusters. For example, we see lexically similar clusters such as { Technology / technology / Technologies / technological / technologies }, as well as more diverse but still strongly related clusters such as ${$ entirely / completely / totally / altogether / $100 \%$ }. The nature of these clusters suggests that the prompts are in fact learning "word-like" representations. We found that random vectors drawn from the embedding space do not show this sort of semantic clustering.</p>
<p>When initializing the prompts using the "classlabel" strategy, we often find that the class labels persist through training. Specifically, if a prompt token is initialized to a given label, that label is often among the learned token's nearest neighbors after tuning. When initializing with the "Random Uniform" or "Sampled Vocab" methods, the class labels can also be found in the nearest neighbors of the prompts; however they tend to appear as neighbors to multiple prompt tokens. This suggests that the model is learning to store the expected output classes in the prompts as reference, and initializing the prompt to outputs classes makes this easier and more centralized.</p>
<p>When examining longer prompts (e.g. size 100), we often find several prompt tokens with the same nearest neighbors. This suggests there is either excess capacity in the prompt, or that the lack of sequential structure in the prompt representation makes it difficult for the model to localize information to a specific position.</p>
<p>While the learned prompts taken as sequences show little interpretability, we do observe a high frequency of words like science, technology and
engineering as the nearest neighbors for prompts trained on the BoolQ dataset and approximately $20 \%$ of the questions are in the "Nature/Science" category. While more investigation is needed, this suggests that one role of the prompt may be to prime the model to interpret inputs in a specific domain or context (e.g. "scientific").</p>
<h2>8 Conclusion</h2>
<p>In this paper, we showed that prompt tuning is a competitive technique for adapting frozen pretrained language models to downstream tasks. On the popular SuperGLUE benchmark, its task performance rivals that of traditional model tuning, with the gap vanishing as model size increases. On zeroshot domain transfer, we found that prompt tuning leads to improved generalization. This plausibly indicates that freezing general-purpose language understanding parameters and restricting downstream learning to a lightweight parameter footprint can help to avoid overfitting to a specific domain.</p>
<p>Beyond task quality metrics, we discussed the appeal of moving to frozen pre-trained models in terms of storage and serving costs. This move enables both efficient multi-task serving, as well as efficient high-performing prompt ensembling. Looking forward, we believe that factoring out task-defining parameters as distinct from general language-modeling parameters is an exciting step that opens up many avenues for new research.</p>
<h2>Acknowledgements</h2>
<p>We thank Lucas Dixon, Waleed Ammar, Slav Petrov and Sebastian Ruder for comments on an earlier draft, and the following people for helpful discussion: Colin Raffel, Adam Roberts, and Noam Shazeer. We thank Linting Xue for help with the LM adaptation training.</p>
<h2>References</h2>
<p>Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006. The second PASCAL recognising textual entailment challenge. In Proceedings of the second PASCAL challenges workshop on recognising textual entailment, volume 6, pages 6-4. Venice.</p>
<p>Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. 2009. The fifth PASCAL recognizing textual entailment challenge. In TAC.</p>
<p>James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal</p>
<p>Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. 2018. JAX: composable transformations of Python+NumPy programs.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel HerbertVoss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.</p>
<p>Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924-2936, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The PASCAL recognising textual entailment challenge. In Machine Learning Challenges Workshop, pages 177-190. Springer.</p>
<p>Marie-Catherine De Marneff, Mandy Simons, and Judith Tonhauser. 2019. The CommitmentBank: Investigating projection in naturally occurring discourse. Proceedings of Sinn und Bedeutung 23.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>William B Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2368-2378, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen. 2019. MRQA 2019 shared task: Evaluating generalization in reading comprehension. In Proceedings of 2nd Machine Reading for Reading Comprehension (MRQA) Workshop at EMNLP.</p>
<p>Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third PASCAL recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing, pages 1-9. Association for Computational Linguistics.</p>
<p>Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. 2021. WARP: Word-level Adversarial ReProgramming. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4921-4933, Online. Association for Computational Linguistics.
L. K. Hansen and P. Salamon. 1990. Neural network ensembles. IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(10):993-1001.</p>
<p>Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, and Marc van Zee. 2020. Flax: A neural network library and ecosystem for JAX.</p>
<p>Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 2790-2799. PMLR.</p>
<p>Jeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 328-339, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Shankar Iyer, Nikhil Dandekar, and Kornel Csernai. 2017. First Quora dataset release: Question pairs.</p>
<p>Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423-438.
A. Kembhavi, M. Seo, D. Schwenk, J. Choi, A. Farhadi, and H. Hajishirzi. 2017. Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5376-5384.</p>
<p>Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 2018. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings</p>
<p>of North American Chapter of the Association for Computational Linguistics (NAACL).</p>
<p>Vid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas Lukasiewicz. 2019. A surprisingly robust trick for the Winograd schema challenge. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4837-4842, Florence, Italy. Association for Computational Linguistics.</p>
<p>Taku Kudo and John Richardson. 2018. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66-71, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. RACE: Large-scale ReAding comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 785-794, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. 2017. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.</p>
<p>Hector Levesque, Ernest Davis, and Leora Morgenstern. 2012. The Winograd schema challenge. In Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning.</p>
<p>Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. 2017. Zero-shot relation extraction via reading comprehension. In Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 333-342, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.</p>
<p>Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582-4597, Online. Association for Computational Linguistics.</p>
<p>Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021. GPT understands, too. CoRR, abs/2103.10385.</p>
<p>Lajanugen Logeswaran, Ann Lee, Myle Ott, Honglak Lee, Marc'Aurelio Ranzato, and Arthur Szlam. 2020. Few-shot sequence learning with transformers. CoRR, abs/2012.09543.</p>
<p>Vinod Nair and Geoffrey E. Hinton. 2010. Rectified linear units improve restricted Boltzmann machines. In Proceedings of the 27th International Conference on International Conference on Machine Learning, ICML'10, page 807-814, Madison, WI, USA. Omnipress.</p>
<p>Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2227-2237, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, and Sebastian Ruder. 2020. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7654-7673, Online. Association for Computational Linguistics.</p>
<p>Mohammad Taher Pilehvar and Jose CamachoCollados. 2018. WiC: 10,000 example pairs for evaluating context-sensitive representations. CoRR, abs/1808.09121.</p>
<p>Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying LMs with mixtures of soft prompts. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5203-5212, Online. Association for Computational Linguistics.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training.</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI Blog.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-totext transformer. Journal of Machine Learning Research, 21(140):1-67.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of</p>
<p>the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.</p>
<p>Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. 2017. Learning multiple visual domains with residual adapters. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.</p>
<p>Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. 2011. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series.</p>
<p>Amrita Saha, Rahul Aralikatte, Mitesh M. Khapra, and Karthik Sankaranarayanan. 2018. DuoRC: Towards complex language understanding with paraphrased reading comprehension. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1683-1693, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Timo Schick and Hinrich Schütze. 2021. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 255-269, Online. Association for Computational Linguistics.</p>
<p>Noam Shazeer. 2020. GLU variants improve transformer. CoRR, abs/2002.05202.</p>
<p>Noam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive learning rates with sublinear memory cost. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 4596-4604. PMLR.</p>
<p>Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4222-4235, Online. Association for Computational Linguistics.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30, pages 5998-6008.</p>
<p>Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019a. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019b. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In the Proceedings of ICLR.</p>
<p>Michael L. Waskom. 2021. seaborn: statistical data visualization. Journal of Open Source Software, 6(60):3021.</p>
<p>Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. 2018. ReCoRD: Bridging the gap between human and machine commonsense reading comprehension. CoRR, abs/1810.12885.</p>
<h2>A Reproducibility</h2>
<h2>A. 1 Experimental Settings</h2>
<p>We evaluate each GLUE and SuperGLUE dataset using the metric specified in the benchmark. We reuse the evaluation code from the publicly available T5 open-source release to compute metrics. ${ }^{12}$ For the SQuAD and MRQA datasets, we evaluate using F1, one of the metrics used by the SQuAD benchmark, where partial answer spans are considered. Again, we use the T5 open-source release for metric calculation. ${ }^{13}$ All of our models use T5 1.1 as the base frozen model, additional details and pretrained checkpoints can be found on GitHub. ${ }^{14,15}$</p>
<p>All prompts for T5 Small and Base models were trained on 4 TPU v2 chips, while prompts for larger models were trained on 16 TPU v3 chips.</p>
<p>Parameter counts for each prompt can be found in Table 4. Average runtimes until convergence can be found in Table 5.</p>
<h2>A. 2 Hyperparameter Search</h2>
<p>This work used 77 hyperparameter search trials ( 40 for prompt tuning and 37 for single-task model tuning), and 3 training runs (with validation evaluation) for each baseline configuration and ablation setting, for a total of 195 runs for our main result and ablations. There were an additional 18 runs for the domain shift experiments and 24 extra runs to create the ensemble. Hyperparameter bounds can be found in Table 6. Hyperparameter tuning was done via manual tuning and settings were selected based on the SuperGLUE score. All experiments in this work, outside of the hyperparameter being ablated, use our default configuration of 100 K steps of LM Adapation, a prompt length of 100, and "class-label" initialization.</p>
<p>All graphs of our experimental results plot the mean and standard deviation over 3 runs as computed by Seaborn (Waskom, 2021). Some settings have such low variance that the standard deviation</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>is hidden behind the line itself, such as "Model Tuning (Multi-task)" in Figure 1 and the Base, Large, and XL prompts trained on the "Span Corruption" pretraining objective in Figure 3(b). Figure 4 also shows mean and standard deviation for the number of parameters each method uses as the prompt length varies from $1-100$. The "Prefix Tuning (Train)" curves appears to have no standard deviation because the parameter count is so strongly dominated by the cost of the reparameterization parameters that the standard deviation bands are occluded. For our experiments on domain transfer, we report mean and standard deviation over 3 runs.</p>
<h2>A. 3 Datasets</h2>
<p>All datasets used are in English. For the GLUE ${ }^{16,17}$ and SuperGLUE ${ }^{18}$ datasets, we used the training, validation, and test splits that ship with TensorFlow Datasets. We used version 1.0.0 for GLUE and 1.0.2 for SuperGLUE datasets. For SQuAD ${ }^{19}$ we used v1.1:3.0.0 from Tensorflow Datasets and follow the provided training, validation, and test splits. For the out-of-domain datasets we used the development splits distributed as part of the MRQA shared task. ${ }^{20}$ Dataset sizes can be found in Table 7. The label distributions for each dataset can be found in Table 8 (BoolQ), Table 9 (CB), Table 10 (COPA), Table 11 (MultiRC), Table 14 (RTE), Table 12 (WiC), Table 13 (WSC), Table 15 (MRPC) and Table 16 (QQP).</p>
<p>The question answering datasets are extractive datasets with a variety of answers, so there isn't a label distribution to report. Similarly, the ReCoRD dataset is a multiple choice dataset where the model must predict the masked out entity from a list of possible entities. Due to this formulation there isn't a meaningful label distribution.</p>
<p>We followed the open-source T5 preprocessing procedure ${ }^{21}$ for each dataset, except that we omit the dataset prefix denoting which SuperGLUE dataset an example belongs to. For the SQuAD and</p>
<p><sup id="fnref3:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">T5 Size</th>
<th style="text-align: center;">Prompt Length</th>
<th style="text-align: center;">Trainable Parameters</th>
<th style="text-align: center;">Total Parameters</th>
<th style="text-align: center;">Percent Trainable</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Small</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">76,961,664</td>
<td style="text-align: center;">$0.00067 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2,560</td>
<td style="text-align: center;">76,963,712</td>
<td style="text-align: center;">$0.00333 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">10,420</td>
<td style="text-align: center;">76,971,572</td>
<td style="text-align: center;">$0.01330 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">25,600</td>
<td style="text-align: center;">76,986,752</td>
<td style="text-align: center;">$0.03325 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">51,200</td>
<td style="text-align: center;">77,012,352</td>
<td style="text-align: center;">$0.06648 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">76,800</td>
<td style="text-align: center;">77,037,952</td>
<td style="text-align: center;">$0.09969 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">768</td>
<td style="text-align: center;">247,578,624</td>
<td style="text-align: center;">$0.00031 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">3,840</td>
<td style="text-align: center;">247,581,696</td>
<td style="text-align: center;">$0.00155 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">15,360</td>
<td style="text-align: center;">247,593,216</td>
<td style="text-align: center;">$0.00620 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">38,400</td>
<td style="text-align: center;">247,616,256</td>
<td style="text-align: center;">$0.01551 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">76,800</td>
<td style="text-align: center;">247,654,656</td>
<td style="text-align: center;">$0.03101 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">115,200</td>
<td style="text-align: center;">247,693,056</td>
<td style="text-align: center;">$0.04651 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1,024</td>
<td style="text-align: center;">783,151,104</td>
<td style="text-align: center;">$0.00013 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5,120</td>
<td style="text-align: center;">783,155,200</td>
<td style="text-align: center;">$0.00065 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">20,480</td>
<td style="text-align: center;">783,170,560</td>
<td style="text-align: center;">$0.00262 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">51,200</td>
<td style="text-align: center;">783,201,280</td>
<td style="text-align: center;">$0.00654 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">102,400</td>
<td style="text-align: center;">783,252,480</td>
<td style="text-align: center;">$0.01907 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">153,600</td>
<td style="text-align: center;">783,303,680</td>
<td style="text-align: center;">$0.01961 \%$</td>
</tr>
<tr>
<td style="text-align: center;">XL</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2,048</td>
<td style="text-align: center;">2,849,759,232</td>
<td style="text-align: center;">$0.00007 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">10,240</td>
<td style="text-align: center;">2,849,767,424</td>
<td style="text-align: center;">$0.00036 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">40,960</td>
<td style="text-align: center;">2,849,798,144</td>
<td style="text-align: center;">$0.00143 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">102,400</td>
<td style="text-align: center;">2,849,859,584</td>
<td style="text-align: center;">$0.00359 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">204,800</td>
<td style="text-align: center;">2,849,961,984</td>
<td style="text-align: center;">$0.00718 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">307,200</td>
<td style="text-align: center;">2,850,064,384</td>
<td style="text-align: center;">$0.01078 \%$</td>
</tr>
<tr>
<td style="text-align: center;">XXL</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">4,096</td>
<td style="text-align: center;">11,135,336,448</td>
<td style="text-align: center;">$0.00004 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">20,480</td>
<td style="text-align: center;">11,135,352,832</td>
<td style="text-align: center;">$0.00018 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">81,920</td>
<td style="text-align: center;">11,135,414,272</td>
<td style="text-align: center;">$0.00074 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">204,800</td>
<td style="text-align: center;">11,137,380,352</td>
<td style="text-align: center;">$0.00184 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">409,600</td>
<td style="text-align: center;">11,135,741,952</td>
<td style="text-align: center;">$0.00368 \%$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">614,400</td>
<td style="text-align: center;">11,135,946,752</td>
<td style="text-align: center;">$0.00552 \%$</td>
</tr>
</tbody>
</table>
<p>Table 4: Number of parameters used for various prompt lengths and T5 model sizes. Trainable parameters is the number of parameters in the prompt itself, while total parameters includes the prompt plus the original T5 parameters. The T5 parameters are frozen and shared across all tasks, and include the SentencePiece lookup table parameters. The final column is the percentage of total parameters that are trainable.</p>
<p>MRQA datasets we used the T5 SQuAD preprocessing code ${ }^{22}$. By following the T5 preprocessing and text-to-text format, we recast the WSC dataset as a text generation task. Instead of predicting whether a supplied referent is correct for a highlighted span, our model predicts the correct referent directly. As such, we can only learn from training examples where the referent is correct, so WSC training data where the supplied referent is incorrect are omitted.</p>
<p>No new data was collected for this work.</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup>| Prompt Length | T5 Size | Time |
| :--: | :--: | :--: |
| 1 | Large | 3:17 $\pm 02: 10$ |
|  | XL | 3:37 $\pm 02: 11$ |
|  | XXL | 21:23 $\pm 01: 54$ |
| 20 | XL | 49:08 $\pm 18: 53$ |
|  | XXL | 53:03 $\pm 16: 25$ |
| 50 | Small | 09:05 $\pm 05: 07$ |
|  | Base | 55:01 $\pm 27: 48$ |
|  | Large | 1:14:16 $\pm 13: 12$ |
|  | XL | 2:30:10 $\pm 25: 40$ |
|  | XXL | 3:13:13 $\pm 23: 08$ |
| 100 | Small | 16:25 $\pm 01: 15$ |
|  | Base | 29:57 $\pm 00: 18$ |
|  | Large | 1:23:36 $\pm 10: 21$ |
|  | XL | 3:35:00 $\pm 54: 42$ |
|  | XXL | 3:51:15 $\pm 45: 53$ |</p>
<p>Table 5: Mean and standard deviation of the runtime until convergence for the BoolQ dataset and various prompt lengths and model sizes. Convergence is defined as reaching a performance within $1 \%$ of the mean value for that model configuration. A few configurations have been omitted because their runtimes were artificially extended due to preemption.</p>
<table>
<thead>
<tr>
<th>Hyperparameter</th>
<th>Search Space</th>
</tr>
</thead>
<tbody>
<tr>
<td>Learning Rate</td>
<td>$0.001-0.5$</td>
</tr>
<tr>
<td>Parameter Scaling</td>
<td>${\text { True, False }}$</td>
</tr>
<tr>
<td>Batch Size</td>
<td>${32,64,126,256,512}$</td>
</tr>
<tr>
<td>Number of Steps</td>
<td>${10,000,20,000,30,000}$</td>
</tr>
<tr>
<td>Warmup Steps</td>
<td>${\text { off, } 2,000,3,000}$</td>
</tr>
<tr>
<td>Decay Factor</td>
<td>${\text { off, } 0.1,0.5}$</td>
</tr>
<tr>
<td>Steps per Decay</td>
<td>${\text { off, } 4,000,6,000,8,000}$</td>
</tr>
</tbody>
</table>
<p>Table 6: Search space for each hyperparameter considered. Parameter Scaling refers to the Adafactor setting where an update is scaled by the norm of the parameter it will be applied to. Warmup Steps is the number of steps before a linearly increasing learning rate reaches the Learning Rate value, starting from zero. Decay Factor is the reduction in Learning Rate size that occurs every "Steps per Decay" steps.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: right;">Training</th>
<th style="text-align: right;">Validation</th>
<th style="text-align: right;">Testing</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BoolQ</td>
<td style="text-align: right;">9,427</td>
<td style="text-align: right;">3,270</td>
<td style="text-align: right;">3,245</td>
</tr>
<tr>
<td style="text-align: left;">CB</td>
<td style="text-align: right;">250</td>
<td style="text-align: right;">56</td>
<td style="text-align: right;">250</td>
</tr>
<tr>
<td style="text-align: left;">COPA</td>
<td style="text-align: right;">400</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">500</td>
</tr>
<tr>
<td style="text-align: left;">MultiRC</td>
<td style="text-align: right;">27,243</td>
<td style="text-align: right;">4,848</td>
<td style="text-align: right;">9,693</td>
</tr>
<tr>
<td style="text-align: left;">ReCoRD</td>
<td style="text-align: right;">100,730</td>
<td style="text-align: right;">10,000</td>
<td style="text-align: right;">10,000</td>
</tr>
<tr>
<td style="text-align: left;">RTE</td>
<td style="text-align: right;">2,490</td>
<td style="text-align: right;">277</td>
<td style="text-align: right;">3,000</td>
</tr>
<tr>
<td style="text-align: left;">WiC</td>
<td style="text-align: right;">5,428</td>
<td style="text-align: right;">638</td>
<td style="text-align: right;">1,400</td>
</tr>
<tr>
<td style="text-align: left;">WSC</td>
<td style="text-align: right;">$259^{*}$</td>
<td style="text-align: right;">104</td>
<td style="text-align: right;">146</td>
</tr>
<tr>
<td style="text-align: left;">MRPC</td>
<td style="text-align: right;">3,668</td>
<td style="text-align: right;">408</td>
<td style="text-align: right;">1,725</td>
</tr>
<tr>
<td style="text-align: left;">QQP</td>
<td style="text-align: right;">363,849</td>
<td style="text-align: right;">40,430</td>
<td style="text-align: right;">390,965</td>
</tr>
<tr>
<td style="text-align: left;">SQuAD</td>
<td style="text-align: right;">87,599</td>
<td style="text-align: right;">10,570</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">TextbookQA</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">1,504</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">RACE</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">1,503</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">BioASQ</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">1,501</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">RE</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">674</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">DuoRC</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">2,948</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">DROP</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">1,503</td>
<td style="text-align: right;">-</td>
</tr>
</tbody>
</table>
<p>Table 7: Sizes for training, validation, and testing splits of each dataset used. *Following T5, our casting of WSC as a text generation problems means we can only train on examples where the supplied referent is correct. This means our training dataset is smaller than the normal WSC training dataset, which has 554 examples.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Split</th>
<th style="text-align: right;">False</th>
<th style="text-align: right;">True</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Training</td>
<td style="text-align: right;">37.7</td>
<td style="text-align: right;">62.3</td>
</tr>
<tr>
<td style="text-align: left;">Validation</td>
<td style="text-align: right;">37.8</td>
<td style="text-align: right;">62.2</td>
</tr>
</tbody>
</table>
<p>Table 8: Label distribution for the BoolQ dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Split</th>
<th style="text-align: right;">contradiction</th>
<th style="text-align: right;">entailment</th>
<th style="text-align: right;">neutral</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Training</td>
<td style="text-align: right;">47.6</td>
<td style="text-align: right;">46.0</td>
<td style="text-align: right;">6.4</td>
</tr>
<tr>
<td style="text-align: left;">Validation</td>
<td style="text-align: right;">50.0</td>
<td style="text-align: right;">41.1</td>
<td style="text-align: right;">8.9</td>
</tr>
</tbody>
</table>
<p>Table 9: Label distribution for the CB dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Split</th>
<th style="text-align: right;">choice1</th>
<th style="text-align: right;">choice2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Training</td>
<td style="text-align: right;">48.8</td>
<td style="text-align: right;">51.2</td>
</tr>
<tr>
<td style="text-align: left;">Validation</td>
<td style="text-align: right;">55.0</td>
<td style="text-align: right;">45.0</td>
</tr>
</tbody>
</table>
<p>Table 10: Label distribution for the COPA dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Split</th>
<th style="text-align: right;">False</th>
<th style="text-align: right;">True</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Training</td>
<td style="text-align: right;">55.9</td>
<td style="text-align: right;">44.1</td>
</tr>
<tr>
<td style="text-align: left;">Validation</td>
<td style="text-align: right;">57.2</td>
<td style="text-align: right;">42.8</td>
</tr>
</tbody>
</table>
<p>Table 11: Label distribution for the MultiRC dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Split</th>
<th style="text-align: right;">False</th>
<th style="text-align: right;">True</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Training</td>
<td style="text-align: right;">50.0</td>
<td style="text-align: right;">50.0</td>
</tr>
<tr>
<td style="text-align: left;">Validation</td>
<td style="text-align: right;">50.0</td>
<td style="text-align: right;">50.0</td>
</tr>
</tbody>
</table>
<p>Table 12: Label distribution for the WiC dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Split</th>
<th style="text-align: right;">False</th>
<th style="text-align: right;">True</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Training</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">100.0</td>
</tr>
<tr>
<td style="text-align: left;">Validation</td>
<td style="text-align: right;">63.5</td>
<td style="text-align: right;">36.5</td>
</tr>
</tbody>
</table>
<p>Table 13: Label distribution for the WSC dataset. Following T5, we cast the WSC dataset to a free-form text generation task where the model generates the referent to the highlighted span instead predicting if the supplied entity is the correct referent of the highlighted span. Thus, we only use training data where the supplied referent is correct making our training label distribution focused entirely on True.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Split</th>
<th style="text-align: center;">entailment</th>
<th style="text-align: center;">not_entailment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Training</td>
<td style="text-align: center;">51.2</td>
<td style="text-align: center;">49.8</td>
</tr>
<tr>
<td style="text-align: left;">Validation</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">47.3</td>
</tr>
</tbody>
</table>
<p>Table 14: Label distribution for the RTE dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Split</th>
<th style="text-align: right;">equivalent</th>
<th style="text-align: right;">not_equivalent</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Training</td>
<td style="text-align: right;">67.4</td>
<td style="text-align: right;">32.6</td>
</tr>
<tr>
<td style="text-align: left;">Validation</td>
<td style="text-align: right;">68.4</td>
<td style="text-align: right;">31.6</td>
</tr>
</tbody>
</table>
<p>Table 15: Label distribution for the MRPC dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Split</th>
<th style="text-align: right;">duplicate</th>
<th style="text-align: right;">not_duplicate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Training</td>
<td style="text-align: right;">36.9</td>
<td style="text-align: right;">63.1</td>
</tr>
<tr>
<td style="text-align: left;">Validation</td>
<td style="text-align: right;">36.8</td>
<td style="text-align: right;">63.2</td>
</tr>
</tbody>
</table>
<p>Table 16: Label distribution for the QQP dataset.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{22}$ https://github.com/google-research/ text-to-text-transfer-transformer/blob/ master/t5/data/preprocessors.py#L264&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{16}$ https://www.tensorflow.org/datasets/ catalog/glue#gluemrpc
${ }^{17}$ https://www.tensorflow.org/datasets/ catalog/glue#glueqqp
${ }^{18}$ https://www.tensorflow.org/datasets/ catalog/super_glue
${ }^{19}$ https://www.tensorflow.org/datasets/ catalog/squad#squadv11_default_config
${ }^{20}$ https://github.com/mrqa/ MRQA-Shared-Task-2019#out-of-domain
${ }^{21}$ https://github.com/google-research/ text-to-text-transfer-transformer/blob/ master/t5/data/preprocessors.py&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
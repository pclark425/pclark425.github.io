<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6772 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6772</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6772</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-130.html">extraction-schema-130</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <p><strong>Paper ID:</strong> paper-bda605928d6ebe4db906e69ab5d343df75918727</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/bda605928d6ebe4db906e69ab5d343df75918727" target="_blank">Large Language Model Guided Tree-of-Thought</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving and the implemented ToT-based Sudoku solver is available on GitHub.</p>
                <p><strong>Paper Abstract:</strong> In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel approach aimed at improving the problem-solving capabilities of auto-regressive large language models (LLMs). The ToT technique is inspired by the human mind's approach for solving complex reasoning tasks through trial and error. In this process, the human mind explores the solution space through a tree-like thought process, allowing for backtracking when necessary. To implement ToT as a software system, we augment an LLM with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller. In order to solve a given problem, these modules engage in a multi-round conversation with the LLM. The memory module records the conversation and state history of the problem solving process, which allows the system to backtrack to the previous steps of the thought-process and explore other directions from there. To verify the effectiveness of the proposed technique, we implemented a ToT-based solver for the Sudoku Puzzle. Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving. Our implementation of the ToT-based Sudoku solver is available on GitHub: \url{https://github.com/jieyilong/tree-of-thought-puzzle-solver}.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6772.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6772.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToT-Sudoku</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-of-Thought guided Sudoku solver (ToT-based solver)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A solver that augments an auto-regressive LLM with a prompter agent, rule-based checker, memory module (search tree), and a ToT controller to perform LLM-guided tree search with backtracking to solve Sudoku puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Auto-regressive transformer language model accessed via OpenAI API (instruction-following). Used to generate the next intermediate step given a prompt; relied on for short-range reasoning while the ToT framework orchestrates multi-round search.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku (benchmarks of 3x3, 4x4, and 5x5 puzzles as constructed by the authors)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>constraint satisfaction / combinatorial logic (puzzle)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>author-created benchmarks: three sets of 10 Sudoku puzzles each (labeled '3x3', '4x4', '5x5' in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Tree-of-Thought multi-round prompting (prompter agent generates prompts with partial-solution context and in JSON format); temperature=1 for LLM calls</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>LLM-guided tree search with backtracking (Tree-of-Thought); rule-based correctness checker validates intermediate nodes; controller issues backtracks (rule-based in experiments: backtrack if invalid or after exploring >5 children)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Partial Sudoku boards represented as grid configurations stored in a search tree inside the memory module; prompts include the current partial board and request a JSON-formatted next_step; the memory stores conversation history and the tree of partial boards for backtracking.</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Calls to OpenAI's gpt-3.5-turbo via API; implementation in Python includes a local rule-based checker (Sudoku rule validator) and in-memory search tree to manage partial boards and backtracking.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>success rate (fraction of puzzles solved in each benchmark set)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>3x3 benchmark: 100% (10/10); 4x4 benchmark: 90% (9/10); 5x5 benchmark: 80% (8/10). Maximum conversation rounds per run = 100.</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Tree-of-Thought significantly improves solve rates compared to single-shot prompting: LLMs are effective at short-range steps but prone to accumulating errors; adding a checker plus backtracking lets the system recover from incorrect intermediate steps and explore alternative partial solutions. The LLM acts as a heuristic to expand nodes; the rule-based checker reliably detects violations of Sudoku constraints. The authors note LLMs can recognize Sudoku patterns from training data and provide useful local steps, but need external orchestration for long-range trial-and-error.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Compared to three baselines (zero-shot, one-shot CoT, few-shot CoT) on the same benchmarks: ToT outperforms all. Reported numbers: ToT solve rates 100%/90%/80% on 3x3/4x4/5x5; one-shot and few-shot performance drops to ~50% on larger puzzles (4x4/5x5); zero-shot performs worst. Paper also reports ToT improved success rate by ~11% over the second-best on the 3x3 set and reported ToT success rates were ~80% higher than one-shot/few-shot on the 4x4 set (authors' comparative phrasing).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Current implementation uses a rule-based checker specific to Sudoku and a rule-based ToT controller, limiting adaptability and search efficiency; the rule-based controller has no lookahead about solvability and simply backtracks after exploring >5 children, which likely reduced efficiency on some puzzles. Training of neural controllers/prompters and more advanced MARL methods were not implemented here. Maximum conversation rounds (100) can cause failure if not enough exploration occurs. Experiments used a single LLM (gpt-3.5-turbo) and temperature=1; model-size/time tradeoffs and generalization to problems without polynomial-time checkers remain open challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Guided Tree-of-Thought', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6772.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6772.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-few/one-shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought few-shot / one-shot prompting baselines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline prompting strategies that provide chain-of-thought style in-context examples (one-shot or few-shot CoT examples) to elicit step-by-step solutions from the same LLM without tree search or external checker/backtracking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Auto-regressive transformer language model accessed via OpenAI API, prompted with in-context CoT examples to produce stepwise reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku (same author-created benchmarks of 3x3, 4x4, 5x5 puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>constraint satisfaction / combinatorial logic (puzzle)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>author-created benchmarks: three sets of 10 Sudoku puzzles each (3x3,4x4,5x5)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>One-shot or few-shot chain-of-thought (CoT) style in-context examples provided along with the puzzle description; LLM asked to produce step-by-step solution in a single shot.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>Chain-of-Thought in-context reasoning (no external backtracking, no checker intervening during generation)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Puzzle provided inline in prompt; example CoT chains included in prompt. Responses are natural-language step-by-step solutions (not managed in a search tree).</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>LLM accessed through OpenAI API (gpt-3.5-turbo); no additional external solver or checker used in the baseline runs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>success rate (fraction solved per benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported to be strong on the smallest (3x3) puzzles but drops as puzzle size increases; for larger puzzles (4x4/5x5) one-shot and few-shot solvers achieve around ~50% success rate (paper reports ~0.5), substantially lower than ToT.</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Providing CoT-style examples substantially boosts LLM performance on small puzzles that mainly require short-range reasoning, but CoT without trial-and-error/backtracking is far less effective as puzzle complexity increases because errors cannot be corrected during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Compared directly to ToT: CoT baselines underperform ToT on larger puzzles (4x4,5x5) by a substantial margin (~40 percentage points or more according to authors' reported comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Single-shot CoT cannot recover from intermediate mistakes (no explicit verification/backtracking); performance degrades with puzzle size and need for trial-and-error; requires hand-crafted in-context examples and does not expand search beyond what is provided in prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Guided Tree-of-Thought', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6772.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6772.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot LLM solver baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline that directly posts the puzzle description to the LLM with no in-context examples or chain-of-thought prompts, relying on the model's implicit knowledge to produce a full solution in one turn.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Auto-regressive transformer language model accessed via OpenAI API, prompted with only the puzzle description (no examples).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku (author-created 3x3,4x4,5x5 benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>constraint satisfaction / combinatorial logic (puzzle)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>author-created benchmarks: three sets of 10 Sudoku puzzles each (3x3,4x4,5x5)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot prompting (direct puzzle description only)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>Single-shot generation (no explicit reasoning orchestration beyond prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Puzzle provided inline in prompt as natural language / array representation as in examples; no memory or search tree used.</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>LLM accessed through OpenAI API (gpt-3.5-turbo); no additional external checker or controller used in the zero-shot baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>success rate (fraction solved per benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Worst performing baseline across all benchmarks (exact numeric rates not provided beyond 'worst' qualitative statement); markedly lower than CoT and ToT baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Zero-shot performance is poor relative to prompting with CoT examples or ToT multi-round search, indicating the need for explicit prompting or orchestration for such combinatorial puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Serves as lower-bound baseline; both CoT prompting and ToT substantially outperform zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Inadequate for puzzles requiring multi-step verification or trial-and-error; no mechanism to detect or correct mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Model Guided Tree-of-Thought', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 1)</em></li>
                <li>Auto-gpt: An autonomous gpt-4 experiment <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6772",
    "paper_id": "paper-bda605928d6ebe4db906e69ab5d343df75918727",
    "extraction_schema_id": "extraction-schema-130",
    "extracted_data": [
        {
            "name_short": "ToT-Sudoku",
            "name_full": "Tree-of-Thought guided Sudoku solver (ToT-based solver)",
            "brief_description": "A solver that augments an auto-regressive LLM with a prompter agent, rule-based checker, memory module (search tree), and a ToT controller to perform LLM-guided tree search with backtracking to solve Sudoku puzzles.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_description": "Auto-regressive transformer language model accessed via OpenAI API (instruction-following). Used to generate the next intermediate step given a prompt; relied on for short-range reasoning while the ToT framework orchestrates multi-round search.",
            "model_size": null,
            "puzzle_name": "Sudoku (benchmarks of 3x3, 4x4, and 5x5 puzzles as constructed by the authors)",
            "puzzle_type": "constraint satisfaction / combinatorial logic (puzzle)",
            "dataset_name": "author-created benchmarks: three sets of 10 Sudoku puzzles each (labeled '3x3', '4x4', '5x5' in the paper)",
            "prompting_method": "Tree-of-Thought multi-round prompting (prompter agent generates prompts with partial-solution context and in JSON format); temperature=1 for LLM calls",
            "reasoning_technique": "LLM-guided tree search with backtracking (Tree-of-Thought); rule-based correctness checker validates intermediate nodes; controller issues backtracks (rule-based in experiments: backtrack if invalid or after exploring &gt;5 children)",
            "internal_representation": "Partial Sudoku boards represented as grid configurations stored in a search tree inside the memory module; prompts include the current partial board and request a JSON-formatted next_step; the memory stores conversation history and the tree of partial boards for backtracking.",
            "use_of_external_tool": true,
            "external_tool_description": "Calls to OpenAI's gpt-3.5-turbo via API; implementation in Python includes a local rule-based checker (Sudoku rule validator) and in-memory search tree to manage partial boards and backtracking.",
            "evaluation_metric": "success rate (fraction of puzzles solved in each benchmark set)",
            "performance": "3x3 benchmark: 100% (10/10); 4x4 benchmark: 90% (9/10); 5x5 benchmark: 80% (8/10). Maximum conversation rounds per run = 100.",
            "analysis_findings": "Tree-of-Thought significantly improves solve rates compared to single-shot prompting: LLMs are effective at short-range steps but prone to accumulating errors; adding a checker plus backtracking lets the system recover from incorrect intermediate steps and explore alternative partial solutions. The LLM acts as a heuristic to expand nodes; the rule-based checker reliably detects violations of Sudoku constraints. The authors note LLMs can recognize Sudoku patterns from training data and provide useful local steps, but need external orchestration for long-range trial-and-error.",
            "ablation_comparison": "Compared to three baselines (zero-shot, one-shot CoT, few-shot CoT) on the same benchmarks: ToT outperforms all. Reported numbers: ToT solve rates 100%/90%/80% on 3x3/4x4/5x5; one-shot and few-shot performance drops to ~50% on larger puzzles (4x4/5x5); zero-shot performs worst. Paper also reports ToT improved success rate by ~11% over the second-best on the 3x3 set and reported ToT success rates were ~80% higher than one-shot/few-shot on the 4x4 set (authors' comparative phrasing).",
            "limitations": "Current implementation uses a rule-based checker specific to Sudoku and a rule-based ToT controller, limiting adaptability and search efficiency; the rule-based controller has no lookahead about solvability and simply backtracks after exploring &gt;5 children, which likely reduced efficiency on some puzzles. Training of neural controllers/prompters and more advanced MARL methods were not implemented here. Maximum conversation rounds (100) can cause failure if not enough exploration occurs. Experiments used a single LLM (gpt-3.5-turbo) and temperature=1; model-size/time tradeoffs and generalization to problems without polynomial-time checkers remain open challenges.",
            "uuid": "e6772.0",
            "source_info": {
                "paper_title": "Large Language Model Guided Tree-of-Thought",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "CoT-few/one-shot",
            "name_full": "Chain-of-Thought few-shot / one-shot prompting baselines",
            "brief_description": "Baseline prompting strategies that provide chain-of-thought style in-context examples (one-shot or few-shot CoT examples) to elicit step-by-step solutions from the same LLM without tree search or external checker/backtracking.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_description": "Auto-regressive transformer language model accessed via OpenAI API, prompted with in-context CoT examples to produce stepwise reasoning.",
            "model_size": null,
            "puzzle_name": "Sudoku (same author-created benchmarks of 3x3, 4x4, 5x5 puzzles)",
            "puzzle_type": "constraint satisfaction / combinatorial logic (puzzle)",
            "dataset_name": "author-created benchmarks: three sets of 10 Sudoku puzzles each (3x3,4x4,5x5)",
            "prompting_method": "One-shot or few-shot chain-of-thought (CoT) style in-context examples provided along with the puzzle description; LLM asked to produce step-by-step solution in a single shot.",
            "reasoning_technique": "Chain-of-Thought in-context reasoning (no external backtracking, no checker intervening during generation)",
            "internal_representation": "Puzzle provided inline in prompt; example CoT chains included in prompt. Responses are natural-language step-by-step solutions (not managed in a search tree).",
            "use_of_external_tool": true,
            "external_tool_description": "LLM accessed through OpenAI API (gpt-3.5-turbo); no additional external solver or checker used in the baseline runs.",
            "evaluation_metric": "success rate (fraction solved per benchmark)",
            "performance": "Reported to be strong on the smallest (3x3) puzzles but drops as puzzle size increases; for larger puzzles (4x4/5x5) one-shot and few-shot solvers achieve around ~50% success rate (paper reports ~0.5), substantially lower than ToT.",
            "analysis_findings": "Providing CoT-style examples substantially boosts LLM performance on small puzzles that mainly require short-range reasoning, but CoT without trial-and-error/backtracking is far less effective as puzzle complexity increases because errors cannot be corrected during generation.",
            "ablation_comparison": "Compared directly to ToT: CoT baselines underperform ToT on larger puzzles (4x4,5x5) by a substantial margin (~40 percentage points or more according to authors' reported comparisons).",
            "limitations": "Single-shot CoT cannot recover from intermediate mistakes (no explicit verification/backtracking); performance degrades with puzzle size and need for trial-and-error; requires hand-crafted in-context examples and does not expand search beyond what is provided in prompt.",
            "uuid": "e6772.1",
            "source_info": {
                "paper_title": "Large Language Model Guided Tree-of-Thought",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Zero-shot",
            "name_full": "Zero-shot LLM solver baseline",
            "brief_description": "Baseline that directly posts the puzzle description to the LLM with no in-context examples or chain-of-thought prompts, relying on the model's implicit knowledge to produce a full solution in one turn.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_description": "Auto-regressive transformer language model accessed via OpenAI API, prompted with only the puzzle description (no examples).",
            "model_size": null,
            "puzzle_name": "Sudoku (author-created 3x3,4x4,5x5 benchmarks)",
            "puzzle_type": "constraint satisfaction / combinatorial logic (puzzle)",
            "dataset_name": "author-created benchmarks: three sets of 10 Sudoku puzzles each (3x3,4x4,5x5)",
            "prompting_method": "Zero-shot prompting (direct puzzle description only)",
            "reasoning_technique": "Single-shot generation (no explicit reasoning orchestration beyond prompt)",
            "internal_representation": "Puzzle provided inline in prompt as natural language / array representation as in examples; no memory or search tree used.",
            "use_of_external_tool": true,
            "external_tool_description": "LLM accessed through OpenAI API (gpt-3.5-turbo); no additional external checker or controller used in the zero-shot baseline.",
            "evaluation_metric": "success rate (fraction solved per benchmark)",
            "performance": "Worst performing baseline across all benchmarks (exact numeric rates not provided beyond 'worst' qualitative statement); markedly lower than CoT and ToT baselines.",
            "analysis_findings": "Zero-shot performance is poor relative to prompting with CoT examples or ToT multi-round search, indicating the need for explicit prompting or orchestration for such combinatorial puzzles.",
            "ablation_comparison": "Serves as lower-bound baseline; both CoT prompting and ToT substantially outperform zero-shot.",
            "limitations": "Inadequate for puzzles requiring multi-step verification or trial-and-error; no mechanism to detect or correct mistakes.",
            "uuid": "e6772.2",
            "source_info": {
                "paper_title": "Large Language Model Guided Tree-of-Thought",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2
        },
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 1
        },
        {
            "paper_title": "Auto-gpt: An autonomous gpt-4 experiment",
            "rating": 1
        }
    ],
    "cost": 0.01154125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Model Guided Tree-of-Thought</h1>
<p>Jieyi Long<br>Theta Labs, Inc.<br>San Jose, CA 95128<br>jieyi@thetalabs.org</p>
<h4>Abstract</h4>
<p>In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel approach aimed at improving the problem-solving capabilities of auto-regressive large language models (LLMs). The ToT technique is inspired by the human mind's approach for solving complex reasoning tasks through trial and error. In this process, the human mind explores the solution space through a tree-like thought process, allowing for backtracking when necessary. To implement ToT as a software system, we augment an LLM with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller. In order to solve a given problem, these modules engage in a multi-round conversation with the LLM. The memory module records the conversation and state history of the problem solving process, which allows the system to backtrack to the previous steps of the thought-process and explore other directions from there. To verify the effectiveness of the proposed technique, we implemented a ToT-based solver for the Sudoku Puzzle. Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving. Our implementation of the ToT-based Sudoku solver is available on GitHub: https://github.com/jieyilong/tree-of-thought-puzzle-solver.</p>
<h2>1 Introduction</h2>
<p>Self-attention based auto-regressive large language models (LLMs) such as GPT-4 have recently taken the world by storm [1, 2, 3, 4, 5, 6]. These LLMs excel at a variety of tasks that previously thought as extremely difficult or even impossible. For example, they are able to handle various logical and mathematical reasoning tasks, particularly those that entail "short-range reasonings" necessitating only a few steps to arrive at conclusions [6, 7]. Such remarkable capabilities have even led to speculation that an early form of artificial general intelligence (AGI) may have already emerged [7]. However, today's LLMs still exhibit limitations in certain domains, especially for "long-range" reasoning tasks, where long-term planning and solution exploration are necessary [7]. When presenting a LLMs such as GPT-4 with a challenging problem solving task, especially the so called System-2 reasoning problems [8], the model does not always succeed. Although the generated answer may be indicative of the correct direction, the derivation process frequently includes logical errors. We hypothesize that there are two main contributing factors which limits the problem solving ability of LLMs:</p>
<p>Lack of correctness checking: To ensure correctness, a good practice for a human solver is to carry out verification procedures at every step of the problem-solving process, thereby ensuring the credibility of the final solution. In comparison, auto-regressive language models do not explicitly perform logical correctness checks as it generates a new token based on the previous tokens. This limits the model's capacity to rectify its own mistakes. A minor error could be amplified as the model generates more tokens, thereby leading to rapid solution quality deterioration and making it difficult to recover from mistakes.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (a) Details of the Tree-of-Thought search strategy, where a solid arrow means a search step guided by the response from the LLM, and a dashed arrow indicates backtracking commanded by the ToT controller. (b) The software system implementing the Tree-of-Thought search strategy. It enhances the problem solving capability of an LLM by augmenting it with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller.</p>
<p>Solution generated linearly: As mentioned above, LLMs typically generate a token based on the preceding sequence of tokens without backward editing. On the contrary, when a human solver attempts to solve a problem, she might backtrack to previous steps if a derivation step is incorrect, or if she becomes stuck and is unable to make further progress towards arriving at the final answer. Fields Medal winner Terence Tao once shared his experiences solving hard math problems ${ }^{1}$ : "When I was a kid, I had a romanticized notion of mathematics, that hard problems were solved in Eureka moments of inspiration... With me, it's always, Let's try this. That gets me part of the way, or that doesn't work. Now let's try this. Oh, there's a little shortcut here... You work on it long enough and you happen to make progress towards a hard problem by a back door at some point. At the end, it's usually, oh, I've solved the problem." The problem solving process as he described is a tree-like thinking process, rather than a linear chain-of-thought [9]. The limitation of linear response generation is also apparent from a computational complexity perspective. The number of computation steps an auto-regressive LLM can perform is polynomial in terms of its input length. Unless $\mathbf{P}=$ NP holds which contradicts the widely accepted belief, there would be problems in NP that is not solvable by auto-regressive LLMs.
Inspired by these two shortcomings of auto-regressive LLMs, we propose a novel framework which augments an LLM with several additional modules including an automatic "prompter agent". This framework employs a solution search strategy we call the Tree-of-Thought (ToT ${ }^{2}$ ). This strategy solves a problem through a multi-round conversation between the LLM and the prompter agent. Figure la provides a visual description of the ToT search strategy, in which the LLM plays a crucial role in guiding the search for solutions. To make it more concrete, let us assume the problem to be solved is an instance of the Sudoku puzzle. The "root" node represents the initial state, corresponding to when a human mind just reads through the problem description, and begins the thinking process. A blue node in the figure represents a valid partial solution, which can be used by the LLM as a basis to generate the next search step. In the context of Sudoku puzzle solving, this means presenting a partially filled Sudoku board to an LLM and letting the LLM fill in a few more cells. The rationale is that an LLM like GPT-4 has been trained on a vast amount of text corpus which includes many Sudoku puzzle solutions. Given a partially filled board, likely the LLM is able to recognize the pattern, and provide useful insights on how to proceed following the Sudoku rules. Hence, it is highly probable that a search guided by the LLM is significantly more efficient than a brute-force search. In the figure, the search steps guided by the LLM are represented by the solid arrows. However, these steps generated by the LLM are not guaranteed to be always logically correct. Thus, we introduce</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>a "checker module" to perform correctness checks. In Figure 1a, a gray node with an "X" marker represents a "dead-end", i.e. a partial solution that the checker module considers as invalid. For Sudoku, this means the partially filled board violates the Sudoku rules. If the current node is invalid, obviously we need to return to a parent or an ancestor node in order to correct the mistake. This can be coordinated by a module called the "ToT controller" which oversees the ToT search. With the backtracking capability, the system can regenerate the solution and thus recover from errors. In addition, even when the current node is valid, if the system remains stuck at it for too long, the ToT controller could issue a backtrack signal to explore other possible solutions. This is similar to a scenario where a human mind realizes that there is no viable path towards reaching the final solution through a particular direction, prompting her to change course and explore alternative routes. This process continues until either a full solution is found (represented by a green node in the figure), or a pre-specified maximum round of conversations is reached.</p>
<p>Note that while the above discussion utilized Sudoku solving as a tangible example to illustrate our main ideas, the ToT framework can potentially be applied to more general mathematical and logical reasoning tasks. For example, in the context of mathematical theorem proving, a full solution corresponds to the complete proof, encompassing a total of $n$ derivation steps. On the other hand, a partial solution refers to a subset of these steps, specifically the initial $k$ steps, where $k$ is less than $n$. The checker verifies the logically correctness of a given partial proof. In parallel, the prompter agent and the ToT controller can offer hints and suggestions to the LLM, encouraging it to think about the subsequent proving step, or explore different directions for theorem proving when necessary.</p>
<p>To evaluate the effectiveness of the ToT framework, we implemented a ToT-based Sudoku puzzle solver and evaluate it on a suite of Sudoku puzzle benchmarks we created. As shown by the experimental results in Section 4.2, the ToT framework can significantly increase the success rate of Sudoku puzzle solving.</p>
<p>The remainder of the paper is organized as follows. Section 2 reviews the related literature and compared our approach with the most relevant works. Section 3 provides the details of the ToT system architecture. Section 4 describes our implementation of a ToT-based Sudoku puzzle solver and presents the experimental results. Finally, Section 5 discusses the limitation of the present work, and potential future extensions of the ToT framework.</p>
<h1>2 Related Works</h1>
<p>Developing intelligent systems that can reason has long been one of the primary goals of artificial intelligence [10, 11, 12]. Recent advancements in large language models, particularly the discovery of their emergent properties and in-context learning abilities, have opened up a new avenue for machine reasoning [6, 7, 9]. It is discovered that prompting language models using chain-of-thought and other hints can elicit them to output step-by-step solutions for mathematical and logical reasoning tasks [9, 13]. Building on these findings, recent studies have also explored the practice of sampling multiple solutions and using self-consistency or complexity-based criteria to determine the optimal response [14, 15]. Experiments were also conducted to evaluate the performance of different prompts [15]. The self-taught reasoner (STaR) [16] is a technique which asks an LLM to generate reasoning chains and drop those producing incorrect answers. Then, the model is fine-tuned with the remaining valid reasoning chains.</p>
<p>Despite showing high potential, these techniques often necessitate human involvement. For example, chain-of-thought style prompting techniques require carefully hand-crafted examples and is thus difficult to scale. Consequently, researchers have started to explore the possibility of automatic prompt generation. Early exploration in this domain includes AutoPrompt [17], prefix-tuning [18], and parameter-efficient prompt tuning [19]. This research direction received even more attention lately. In a recent study [20], the authors experimented with training verifiers to check if the solution provided by an LLM to an given mathematical problem is logically correct. If the trained verifier can effectively judge the LLM outputs, it would provide another avenue for prompt evaluation. Automatic prompt engineer [21] examines a method to select the best prompt from a set of model-generated candidates. The three-phase augment-prune-select method was suggested in [22]. It first generates multiple chain-of-thought candidates, which was then pruned based on whether the derived answer matches with the ground truths. Finally, a policy gradient based method was used to select the optimal combination of several rationale chains from the pool for CoT prompting.</p>
<p>Very recently researchers have also turned their attention to augmenting LLM with additional agents for various purposes. This is also the research field that is most relevant to our current work. AutoGPT [23] is a program which combines GPT-4 with additional modules including an execution agent and a memory unit. It can chain together LLM "thoughts", in order to autonomously achieve whatever goal the user sets. PromptPG [24] proposes an approach that can learn to select in-context examples from a small amount of training data via policy gradient for prompt learning. The PromptPG agent learns to find optimal in-context examples from a candidate pool, with the goal of maximizing the prediction rewards on given training examples when interacting with the GPT-3 environment. DEPS [25] is a proposal that utilizes multi-step reasoning and sub-task error correction to tackle complex tasks with long-range dependencies. By being able to provide explanations for errors in sub-tasks within a trial, DEPS exhibits remarkable performance. ReAct [26] is an approach that employs emergent properties present in LLMs, such as traces of verbal reasoning, to enable agents to reason and take action, resulting in impressive performance on different text-based benchmarks. Building on top of ReAct, Reflexion [27] is an approach that equips an agent with dynamic memory and self-reflection capabilities, improving its existing reasoning trace and ability to choose task-specific actions. To achieve complete automation, a simple but effective heuristic was designed to enable the agent to identify hallucination instances and prevent repetitive action sequences. Our proposal shares some commonalities with these approaches, for example, the use of a memory module and additional agents for automatic prompt generation. However, our approach is unique in that it introduces a ToT controller which can explicitly conduct backtracking when necessary. This not only allows the system to recover from mistakes, but potentially can also enlarge the solution search space.</p>
<h1>3 Architecture</h1>
<h3>3.1 The Tree-of-Thought Framework</h3>
<p>Figure 1b depicts the software system that implements the ToT Framework. As mentioned earlier, it incorporates several components which enhance the problem solving capability of the LLM, including a prompter agent, a checker module, a memory module, and a ToT controller.</p>
<p>The problem solving process starts with the user inputting the problem description. The prompter agent then relays the problem to the LLM, with additional prompt text which encourages the LLM to come up with an intermediate solution instead of trying to reach the full solution in a single shot. After receiving the response from the LLM, the checker module is invoked to check the validity of the intermediate solution generated. If it passes the correctness check, the intermediate solution will be parsed and stored in the memory module. Then, based on the content of the memory module, the prompter agent generates a prompt to encourage the LLM to generate the next step. Conversely, if the LLM generates an invalid intermediate solution, the ToT controller will activate the prompter to offer hints to the LLM and request it to consider again. Note that in general, a valid intermediate solution does not always leads to the correct final solution. In order to prevent getting stuck, the ToT controller constantly monitors the search process and determines whether to continue trying from the current node or backtrack to a parent or an ancestor node and explore alternative directions.</p>
<p>The ToT strategy can be viewed as a tree-search algorithm using an LLM as a heuristic for generating the search steps. In this setting, the LLM is only used for the "short-range reasoning" tasks, i.e deriving the next intermediate solution, which is a type of tasks that have been shown to have a high success rate for LLMs [7]. On the other hand, by introducing the checker, the system have a higher likelihood to discover the mistakes it makes as it generates the solutions. Moreover, by allowing the system to backtrack from a valid but somewhat "hopeless" intermediate solution, the system is able to explore a larger solution space, which enhances the "long-range reasoning" capability of the system as a whole. The ToT framework thus combines the best of both world. Furthermore, this multi-round conversation technique increases the number of computation steps the system can perform. Thus, based on the time hierarchy theorem in computational complexity theory [28], the ToT framework can expand the range of problems that can potentially be solved compared to relying solely on a single round of conversation with an LLM.</p>
<h3>3.2 ToT Modules</h3>
<p>In this section we provide more details of the components of the ToT software system.</p>
<p>Checker Module. The checker module can either be rule-based or implemented as a deep neural network. For problems that have an explicit polynomial time algorithm for correctness checking (i.e. problems in NP), rule-based checkers can be implemented. Numerous important mathematical and logical problems are in this category, for example, equation solving, polynomial factoring, 3SAT, and puzzles like Sudoku. With a rule-based checker, the ToT software can be viewed as a hybrid system which allows explicitly encoding prior knowledge (e.g. the Sudoku rules) into a neural network powered system. An alternative is to train and use a neural network based classifier as the checker [20]. This is especially useful for problems where a rule-based checker is difficult to implement, e.g. checking whether a mathematical proof is correct.</p>
<p>Memory Module. The memory module can be used to store the entire conversation history between the LLM and the prompter agent, as well as other supplemental data useful for problem solving. The data stored can be served as the information source for the prompter agent to generate helpful hints for the LLM.</p>
<p>ToT Controller. The ToT controller oversees the entire ToT search. It can be implemented in a number of ways. It can be as simple as encoding two rules: 1) if the checker thinks the current partial solution is invalid, backtrack to the parent node, and 2) if the current partial solution is valid, but the ToT search tree has explored its $C$ children and yet failed to find the final solution, then backtrack to the parent node. Here $C$ is an pre-configured integer.</p>
<p>A more advanced version of the ToT controller can employ a policy network to determine the backtracking policy. The network's inputs include the recent search history comprised of the sequence of the last $k+1$ node visited in the search tree $s_{i-k}, . ., s_{i-1}, s_{i}$ ( $k$ is a hyper-parameter). The network also takes in $c_{i}$, a Boolean variable which indicates whether the checker module considers the current node $s_{i}$ is valid. We can sample from the policy to determine the next action $a_{i}$ :</p>
<p>$$
a_{i} \sim \pi_{\rho}^{t}\left(a \mid c_{i}, s_{i}, . ., s_{i-k}\right), a \in A_{\text {cand }}
$$</p>
<p>where $\pi_{\rho}^{t}$ represents the policy network of the ToT controller with parameters $\rho$. The set of candidate actions $A_{\text {cand }}$ includes simply staying at the current node to generate the next step, and backtracking to the parent or an ancestor node at most $L$ levels up in the search tree where $L$ is a hyper-parameter. Thus, we can use one-hot encoding for the actions, where backtracking $j$ levels up is represented by a vector where only the $j^{\text {th }}$ position is set to 1 . The action vector $a$ and checker output $c_{i}$ are processed by a feed-forward network (FFN) to for deep features extraction. A linear layer with learnable parameters $\mathbf{W}<em 1="1">{1}$ and $\mathbf{b}</em>}$ is added on top of the FFN to map its output to a vector $\mathbf{g}\left(a, c_{i}\right)$. The latest $k+1$ visited nodes are concatenated into a string, and then added with position embedding (PE), and finally inputted into a self-attention model [1]. The idea is that by adding position embedding, the attention model will be able to make decisions based on the sequence of the recent node visits. A linear layer with learnable parameters $\mathbf{W<em 2="2">{2}$ and $\mathbf{b}</em>\right)$. Finally, we calculate the inner-products of these two vectors, and use the softmax function to compute the probability of each action candidate:}$ is added on top of the attention model to transform its output to a vector $\mathbf{g}\left(s_{i}, . ., s_{i-k}\right)$ whose dimension matches with that of $\mathbf{g}\left(a, c_{i</p>
<p>$$
\begin{aligned}
\mathbf{g}\left(a, c_{i}\right) &amp; =\mathbf{W}<em i="i">{1} \cdot \operatorname{FFN}\left(a, c</em>}\right)+\mathbf{b<em i="i">{1} \
\mathbf{g}\left(s</em>}, . ., s_{i-k}\right) &amp; =\mathbf{W<em i-k="i-k">{2} \cdot \operatorname{Attention}\left(\operatorname{PE}\left(s</em>}\left||. .| \mid s_{i-1} \mid s_{i}\right)\right)+\mathbf{b<em _rho="\rho">{2}\right. \
\pi</em>
\end{aligned}
$$}^{t}\left(a \mid c_{i}, s_{i}, . ., s_{i-k}\right) &amp; =\frac{\exp \left(\mathbf{g}\left(a, c_{i}\right) \cdot \mathbf{g}\left(s_{i}, . ., s_{i-k}\right)\right)}{\sum_{a^{\prime} \in A_{\text {cand }}} \exp \left(\mathbf{g}\left(a^{\prime}, c_{i}\right) \cdot \mathbf{g}\left(s_{i}, . ., s_{i-k}\right)\right)</p>
<p>In the above formula, " $l$ " is the string concatenation operator. Section 3.3 will discuss the training algorithm for the ToT controller policy network.</p>
<p>Prompter Agent. The prompter agent gives hints to the LLM for it to generate the next search step. The most basic hint can be a generic prompt using the following template: generic_tmpl $=$ "For the given problem: [problem description], we have come up with a partial solution: [partial solution summary]. Please derive the next step on top of this partial solution, and return the next step in the following JSON format [next_step: <next_step>]". Note that the template requires the LLM to respond with a structured JSON string. This is a trick to make it easier for the checker to extract the next step from the LLM response. To create an actual prompt from this template, the prompter needs the [problem description] and the [partial solution summary], both of which can be queried from the memory module.</p>
<p>Algorithm 1 Policy Gradient based Training Algorithm for the ToT System
1: Input: training set $P_{\text {train }}$, num of training epochs $N$
2: procedure REINFORCE( $P_{\text {train }}, N$ )
3: randomly initialized the ToT Controller policy $\pi_{\rho}^{t}$
4: randomly initialized the Prompter agent policy $\pi_{\theta}^{p}$
5: for $e p o c h=1,2, . ., N$ do
6: $\pi_{w} \leftarrow \pi_{\rho}^{t}$ if $e p o c h$ is even, $\pi_{\theta}^{p}$ otherwise $\triangleright$ update the selected policy only, fix the other
7: for $p_{i} \in P_{\text {train }}$ do
8: $\quad r_{i} \leftarrow \operatorname{reward}\left(\operatorname{ToTSystem}\left(p_{i}\right)\right) \quad \triangleright$ attempt to solve problem $p_{i}$ and obtain reward $r_{i}$
9: $\quad w \leftarrow w+\alpha \nabla_{w} \log \pi_{w} r_{i}$
10: end for
11: end for
12: end procedure</p>
<p>Similar to the ToT controller, we can also implement the prompter agent as a policy network, which can generate prompts based on the current partial solution and the conversation history. First we define the prompt template as follows: prompt_tmpl $=$ generic_tmpl || "Here are a few examples: [in-context learning examples],", where || is the string concatenation operator. The variable [in context learning examples] are in-context learning examples for the problem being solved, which can be picked by the prompter policy network from a set of candidates, similar to the PromptPG approach [24]. The rationale is that given the current and recently attempted intermediate solution, some in-context examples might work better than others as hints for the next step. Given the recently visited node sequence $s_{i-k}, . ., s_{i-1}, s_{i}$, our goal is to select $l$ examples $e_{i}=\left{e_{i}^{1}, e_{i}^{2}, \ldots, e_{i}^{l} \mid e_{i}^{j} \in E_{\text {cand }}\right}$ where $E_{\text {cand }}$ is a pool of in-context learning example candidates. The examples are selected according on a policy:</p>
<p>$$
e_{i}^{j} \sim \pi_{\theta}^{p}\left(e \mid s_{i}, . ., s_{i-k}\right), e_{i}^{j} \in E_{\text {cand }} \text { for } j=1,2, \ldots, l
$$</p>
<p>where $\pi_{\theta}^{p}$ represents the policy network of the prompter agent with parameters $\theta$. With the set of selected examples, the prompter agent generates a prompt from the template: $p_{i}=$ prompter $\left(\right.$ prompt_tmpl, $\left.e_{i}, s_{i}\right)$, which can be fed into the LLM to obtain the next intermediate solution $s_{i+1}=L L M\left(p_{i}\right)$. The neural network architecture for the prompter's policy network is similar to that of the ToT controller. The only difference is that since the in-context examples are expressed in natural language, instead of FFN, we use an attention model to process them:</p>
<p>$$
\begin{aligned}
\mathbf{h}(e) &amp; =\mathbf{M}<em 1="1">{1} \cdot \operatorname{Attention}(e)+\mathbf{c}</em> \
\mathbf{h}\left(s_{i}, . ., s_{i-k}\right) &amp; =\mathbf{M}<em i-k="i-k">{2} \cdot \operatorname{Attention}\left(\operatorname{PE}\left(s</em>}\left|..\left|s_{i-1}\right| s_{i}\right)\right)+\mathbf{c<em _theta="\theta">{2}\right. \
\pi</em>
\end{aligned}
$$}^{p}\left(e \mid s_{i}, . ., s_{i-k}\right) &amp; =\frac{\exp \left(\mathbf{h}(e) \cdot \mathbf{h}\left(s_{i}, . ., s_{i-k}\right)\right)}{\sum_{e^{\prime} \in E_{\text {cand }}} \exp \left(\mathbf{h}\left(e^{\prime}\right) \cdot \mathbf{h}\left(s_{i}, . ., s_{i-k}\right)\right)</p>
<p>The prompter policy network can be trained together with the ToT controller using multi-agent reinforcement learning methods. The training algorithm of the prompter's policy network is discussed in Section 3.3.</p>
<h1>3.3 ToT System Training</h1>
<p>In the previous sections, we have described the multi-agent ToT framework. This section dives into how we can train the agents, in particular, the policy networks of the ToT controller and the prompter agent. While there are many multi-agent reinforcement learning algorithms (MARL) proposed in the literature [29, 30, 31], in this work we adopt a relatively simple approach which uses a modified version of the REINFORCE algorithm [32] to train the policy networks of the ToT controller and the prompter agent directly. The more advanced MARL algorithms will be explored in the future.
First, we define a run of the ToT system as the process where a user inputs the problem description, and the ToT system attempts to solve the problem until it thinks the problem is solved, or a prespecified maximum round of conversations is reached. Next, we define the reward $r$ of a run: if the problem is correctly solved, then $r=+1$. Otherwise, if the system outputs an incorrect solution, or the maximum round of conversations is reached, then $r=-1$.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="n">Problem</span><span class="w"> </span><span class="n">Solving</span><span class="w"> </span><span class="n">Using</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">ToT</span><span class="w"> </span><span class="n">System</span>
<span class="w">    </span><span class="n">Input</span><span class="p">:</span><span class="w"> </span><span class="n">problem</span><span class="w"> </span><span class="n">description</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">user</span><span class="w"> </span>\<span class="p">(</span><span class="n">p_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">user</span><span class="w"> </span><span class="p">}}</span>\<span class="p">),</span><span class="w"> </span><span class="nb">max</span><span class="w"> </span><span class="n">num</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">conversation</span><span class="w"> </span><span class="n">rounds</span><span class="w"> </span>\<span class="p">(</span><span class="n">K</span>\<span class="p">)</span>
<span class="w">    </span><span class="n">procedure</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">SOLVE</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">p_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">user</span><span class="w"> </span><span class="p">}},</span><span class="w"> </span><span class="n">K</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">        </span><span class="n">prompt</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">Prompter</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">p_</span><span class="p">{</span>\<span class="n">text</span><span class="w"> </span><span class="p">{</span><span class="n">user</span><span class="w"> </span><span class="p">}}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="nb">round</span><span class="w"> </span>\<span class="p">(</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="o">.</span><span class="w"> </span><span class="o">.</span><span class="p">,</span><span class="w"> </span><span class="n">K</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">            </span><span class="n">response</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">LLM</span><span class="p">}(</span>\<span class="p">)</span><span class="w"> </span><span class="n">prompt</span><span class="w"> </span>\<span class="p">()</span>\<span class="p">)</span>
<span class="w">            </span><span class="n">result</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">Checker</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">result</span><span class="o">.</span><span class="n">isValidFinalSolution</span><span class="p">()</span><span class="w"> </span><span class="n">then</span>
<span class="w">                </span><span class="k">return</span><span class="w"> </span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">solution</span><span class="p">)</span>
<span class="w">            </span><span class="n">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">            </span><span class="n">memory</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="w">            </span><span class="n">ctrl_signal</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">ToTController</span><span class="p">(</span><span class="n">memory</span><span class="p">)</span>
<span class="w">            </span><span class="n">prompt</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">Prompter</span><span class="p">(</span><span class="n">memory</span><span class="p">,</span><span class="w"> </span><span class="n">ctrl_signal</span><span class="p">)</span>
<span class="w">        </span><span class="n">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="p">(</span><span class="n">nil</span><span class="p">)</span>
<span class="w">    </span><span class="n">end</span><span class="w"> </span><span class="n">procedure</span>
</code></pre></div>

<p>The training algorithm is provided in Algorithm 1. The algorithm takes two inputs, the training data set $P_{\text {train }}$, and the number of training epochs $N$ (Line 1-2). The two policy networks $\pi_{p}^{l}\left(a_{i} \mid s_{i}, . ., s_{i-k}\right)$ and $\pi_{p}^{p}\left(e_{i} \mid s_{i}, . ., s_{i-k}\right)$ are randomly initialized (Line 3-4). We train the two policy networks in turns, i.e. training one network with policy gradient while keeping the other fixed (Line 6). To be more specific, when the current epoch is an even number, we select the ToT controller policy $\pi_{p}^{l}$, and keep the parameters of the prompter agent fixed. Otherwise, we select the prompter agent policy $\pi_{p}^{p}$ and fix the ToT controller policy. Next, the algorithm updates the parameters of the selected policy network using the policy gradient method (Line 7-9). For each problem in the training data, we attempt to solve it with a ToT system run. Based on the result, we obtain the reward for that run (Line 8). The entire training algorithm runs for $N$ epochs.</p>
<h1>3.4 Problem Solving Using the ToT System</h1>
<p>After the ToT system is trained, we can use it for inference, i.e. problem solving. Algorithm 2 provides the pseudo code for solving problems using the ToT system. It starts with a user inputting description of the problem (Line 1-2). The prompter module then converts the user input into a prompt (Line 3) using a prompt template for user input, for example: user_input_prompt $=$ "For the given problem: [problem description], please derive the first step, and return the step in the following JSON format [next_step: <next_step>]".</p>
<p>Next, up to $K$ rounds of conversations with the LLM are conducted for problem solving (Line 4). In each round, the LLM first produces a response for the given prompt (Line 5). Then, the checker analyzes the response, and returns a result (Line 6). The result contains the partial solution extracted from the LLM response, as well as information like whether the checker considers the solution as a valid final solution, a valid intermediate solution, or an invalid partial solution, etc. If the solution is a valid final solution, the algorithm simply returns it (Line 7-9). Otherwise, the result is stored in the memory module (Line 10). Based on the content of the memory module, the ToT controller issues control signals, e.g. backtracking for $l$ levels, to the prompter (Line 11). Finally, based on the control signal, the prompter looks up the relevant information from the memory module, and produce the next prompt for the LLM (Line 12). If no valid final solution is found within $K$ rounds of conversations, the algorithm return nil indicating it fails to solve the problem (Line 14).</p>
<h2>4 Evaluation</h2>
<p>This section provides the evaluation methodology and experimental results for our proposed ToT framework. Our evaluation focuses on the ToT-based solver for the Sudoku problem. At the first glance, Sudoku problems seem to be just brain teasers with little practical importance. However, the generalized Sudoku problem on $n^{2} \times n^{2}$ grids of $n \times n$ blocks is known to be NP-complete [33]. If the ToT framework can solve instances of the generalized Sudoku problem (granted that it might takes an exponential number of rounds of conversations), in principle it can handle many</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Experimental results comparing the success rate of different LLM-based Sudoku puzzle solvers across three sets of benchmarks.
other mathematical and logical reasoning tasks. In fact, it is straightforward to re-purpose the implementation described below to solve other puzzles, such as 3SAT, 3-coloring, etc. Below we first describe the implementation details of the solver. Then, we present the test suite used in our evaluation, as well as the experimental results.</p>
<h1>4.1 ToT Solver for Sudoku Puzzles</h1>
<p>The ToT-based Sudoku solver follows the generic framework described in Section 3 with some specific tweaks for the Sudoku problem. It allows a user to input a Sudoku puzzle using natural languages, for example: "Please solve this 4x4 Sudoku puzzle [[3,<em>,</em>,2], $[1, <em>, 3, </em>],[<em>, 1, </em>, 3],[4, <em>, </em>, 1]]$ where * represents a cell to be filled".</p>
<p>We have implemented the ToT-based Sudoku solver as described in Section 4.1 in Python. We adopted a rule-based approach for the checker module since the Sudoku rules are precise and easy to check. The memory module stores the conversation history between the prompter and the LLM, as well as a search tree which maintains all the partially filled Sudoku board the LLM has generated so far. This way, when backtracking happens, the previous board configuration can be retrieved. The ToT controller in our implementation is also rule-based. It returns to the parent node in the search tree if either the current node considered invalid by the checker, or the search algorithm has explored more than 5 children of the current node. Finally the prompter agent uses a variation of the generic template mentioned above, with the [problem description] being the initial configuration of the Sudoku board input by the user, and [partial solution summary] being the partially filled board represented by the current node in the search tree. The LLM utilized in this study is the "gpt-3.5-turbo" model, which is accessible through the OpenAI API suite. The temperature parameter was set to 1 in our experiments.</p>
<h3>4.2 Experimental Results</h3>
<p>We have implemented four LLM-based Sudoku puzzle solvers and compared their performance: 1) zero-shot solver (zs) which directly posts the puzzle description to the LLM, 2) one-shot solver (os) which provides a chain-of-thought style step-by-step solution of a 3x3 Sudoku puzzle as an example in addition to the problem description, 3) few-shot solver (fs) which provides multiple examples with CoT-style solutions, and 4) our proposed Tree-of-Thought solver (tot). We constructed three benchmarks, comprising of ten $3 \times 3,4 \times 4$, and $5 \times 5$ Sudoku puzzles, respectively. The objective of a solver is to fill the $n \times n$ Sudoku grid with digits so that each row and column contain all of the digits from 1 to $n(n=3,4,5$ in our experiments).
Figure 2 compares the success rates of different LLM-based solvers across the three benchmarks. Here the term success rate refers to the fraction of problems in a benchmark set that are successfully solved by a solver. For example, if a solver is able to solve 4 out of 10 problems in the " $3 \times 3$ puzzles" benchmark set, then the success rate of this solver for this benchmark set is 0.4 . As expected, the zero-shot solver has the worst performance across all the three set of benchmarks. Adding CoT-style step-by-step examples significantly boosts the success rate, especially for the $3 \times 3$ puzzles. This is expected, since one can pretty much rely on "short-range" reasoning skills, which is a strength of the</p>
<p>LLM models, to solve a small-sized 3x3 Sudoku puzzle, espcially when CoT-style hints are provided. However, as the puzzle size gets bigger, the success rate of the one-shot and few-shot solver dropped to around 0.5 . This is because solving bigger puzzles requires trial and error, which is a capability LLMs generally lack of as discussed earlier.</p>
<p>In comparison, the ToT-based solver demonstrates superior performance when compared to the other solvers. For the 3x3 benchmark set, it was able to solve all the puzzles. The success rate improves by $11 \%$ compared to the second best for the two benchmark sets. For the $4 \times 4$ benchmark set, the ToT-based solver failed to find the solution for 1 out of the 10 puzzles before reaching the maximum round of conversations (which is set to 100 in our experiments). We suspect it is due to the limited capability of the rule-based ToT controller. In particular, the rule-based controller has no sense of whether the current partially-filled board can be completed without violating the Sudoku rules, which decreases the efficiency of the solution search. We expect a neural network based ToT controller will perform better, which we will verify in the future extension of this work. Despite this, the success rate of the ToT based solver is still $80 \%$ higher compared to that of the one-shot and few-shot based solvers. Finally, for the $5 \times 5$ puzzles, the ToT-based solver failed with 2 puzzles before reaching the maximum round of conversations. Nonetheless, the success rate is $60 \%$ higher compared to that of the one-shot and few-shot based solvers.</p>
<h1>5 Discussions and Future Works</h1>
<p>In this paper, we presented the Tree-of-Thought framework, which enhances LLMs with additional agents and memory modules, resulting in improved performance for mathematical problem-solving tasks. To evaluate the performance of this technique, we implemented a Sudoku puzzle solver based on the ToT framework. One of the limitations of the current implementation is that it utilizes a rule-based checker that contains custom logic, making it less easily adaptable to other problems. For more generic problems, for example, general mathematical and logical reasoning problems, where rule-based solution checking is difficult to implement, a future direction is to explore checkers based on neural network or other probabilistic models. Moreover, the experiments we conducted in this work also uses a rule-based ToT controller, which as we pointed out, has limited capabilities. In the future, we will implement the neural network based ToT controller which can hopefully enhance the system performance. Additionally, the policy-gradient based training algorithm proposed in this work is relatively simple and may be susceptible to training stability issues. To further optimize the ToT system, more advanced multi-agent reinforcement learning algorithms, particularly those designed for cooperative agents, could be adopted.
Another intriguing future direction is to investigate the potential of utilizing the "self-play" technique to enable the ToT system to develop novel problem solving strategies that are not found in the LLM's training text corpus. The self-play training method is a reinforcement learning technique which was popularized by the development of competitive game-playing agents such as AlphaGo and AlphaStar $[34,35,36]$, where an AI agent learns to improve its own strategy by playing against itself. Today's LLMs are typically trained using self-supervised learning techniques. They may have limitations when it comes to problem-solving, as they may not be able to generate samples (i.e. novel problem solving strategies) that fall outside the distribution of the training data. In other words, they may not be able to "think outside the box", which is a crucial human trait that facilitates the discovery of new knowledge. Compared to self-supervised learning, self-play based reinforcement learning enables the system to access a much broader solution space beyond the provided training examples, allowing for greater improvement. AlphaGo and similar systems have demonstrated the ability to devise strategies that surpass even those of human experts. Inspired by these examples, for ToT system training, instead of relying on the training data set $P_{\text {train }}$, we can introduce a "quizzer" module which can come up with problem descriptions on its own to train the ToT controller and the prompter agent. It is worth mentioning that one of the key enablers for training AlphaGo and similar system is that the environment reward can be precisely determined, as it is straightforward to determine whether the gameplay results in a win or a loss. The ToT framework incorporates a checker that can assess the correctness of the solution, functioning similarly to the environment, particularly for problems that have well-defined solution validation rules. Thus, the reinforcement learning training methods can be readily applied. We suspect that this self-driven learning approach, similar to the self-play method, could be an effective means of improving the ToT framework's problem-solving capabilities beyond the solution examples provided in the training text corpus for the LLMs.</p>
<h1>References</h1>
<p>[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2017.
[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019.
[3] Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training. 2018.
[4] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.
[5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.
[6] OpenAI. Gpt-4 technical report, 2023.
[7] Sbastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4, 2023.
[8] Shu Tay, Paul Ryan, and Anthony C Ryan. Systems 1 and 2 thinking processes and cognitive reflection testing in medical students. Canadian Medical Education Journal, 2016:97-103, 112016.
[9] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023.
[10] Larry Wos, Ross Overbeck, Ewing Lusk, and Jim Boyle. Automated Reasoning: Introduction and Applications. Prentice Hall Professional Technical Reference, 1984.
[11] Frederick Hayes-Roth, Donald A. Waterman, and Douglas B. Lenat. Building Expert Systems. AddisonWesley Longman Publishing Co., Inc., USA, 1983.
[12] Ronald Fagin, Joseph Y. Halpern, Yoram Moses, and Moshe Y. Vardi. Reasoning About Knowledge. MIT Press, Cambridge, MA, USA, 2003.
[13] Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin Liu, Linda Chen, Sunny Tran, Newman Cheng, Roman Wang, Nikhil Singh, Taylor L. Patti, Jayson Lynch, Avi Shporer, Nakul Verma, Eugene Wu, and Gilbert Strang. A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level. Proceedings of the National Academy of Sciences, 119(32), aug 2022.
[14] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models, 2023.
[15] Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for multi-step reasoning, 2023.
[16] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning, 2022.
[17] Taylor Shin, Yasaman Razeghi, Robert L. Logan IV au2, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts, 2020.
[18] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation, 2021.
[19] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning, 2021.
[20] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021.
[21] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers, 2023.
[22] KaShun Shum, Shizhe Diao, and Tong Zhang. Automatic prompt augmentation and selection with chain-of-thought from labeled data, 2023.
[23] Auto-gpt: An autonomous gpt-4 experiment, 2023. https://github.com/Significant-Gravitas/ Auto-GPT.</p>
<p>[24] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning, 2023.
[25] Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents, 2023.
[26] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models, 2023.
[27] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection, 2023.
[28] Juris Hartmanis and R. Stearns. On the computational complexity of algorithms. Transactions of The American Mathematical Society - TRANS AMER MATH SOC, 117:285-285, 051965.
[29] Thanh Nguyen, Ngoc Duy Nguyen, and Saeid Nahavandi. Deep reinforcement learning for multiagent systems: A review of challenges, solutions, and applications. IEEE Transactions on Cybernetics, PP:1-14, 032020 .
[30] Afshin OroojlooyJadid and Davood Hajinezhad. A review of cooperative multi-agent deep reinforcement learning, 2021.
[31] Kaiqing Zhang, Zhuoran Yang, and Tamer Baar. Multi-agent reinforcement learning: A selective overview of theories and algorithms, 2021.
[32] Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Proceedings of the 12th International Conference on Neural Information Processing Systems, NIPS'99, page 1057-1063, Cambridge, MA, USA, 1999. MIT Press.
[33] Michael Haythorpe. Reducing the generalised sudoku problem to the hamiltonian cycle problem, 2016.
[34] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016.
[35] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis. Mastering chess and shogi by self-play with a general reinforcement learning algorithm, 2017.
[36] Oriol Vinyals, Igor Babuschkin, Wojciech Czarnecki, Michal Mathieu, Andrew Dudzik, Junyoung Chung, David Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John Agapiou, Max Jaderberg, and David Silver. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575, 112019.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://newsroom.ucla.edu/releases/Terence-Tao-Mozart-of-Math-7252
${ }^{2}$ The word "tot" means a very young child, which is an interesting analogy as this work is a preliminary exploration into the potential for automated problem-solving utilizing language models.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
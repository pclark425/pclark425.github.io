<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3567 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3567</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3567</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-79.html">extraction-schema-79</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <p><strong>Paper ID:</strong> paper-1c7a4e8d9f4fcf19a5d1caa078c66ca39cb75dd2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1c7a4e8d9f4fcf19a5d1caa078c66ca39cb75dd2" target="_blank">A Molecular Multimodal Foundation Model Associating Molecule Graphs with Natural Language</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A molecular multimodal foundation model which is pretrained from molecular graphs and their semantically related textual data via contrastive learning which enhances molecular property prediction and possesses capability to generate meaningful molecular graphs from natural language descriptions is proposed.</p>
                <p><strong>Paper Abstract:</strong> Although artificial intelligence (AI) has made significant progress in understanding molecules in a wide range of fields, existing models generally acquire the single cognitive ability from the single molecular modality. Since the hierarchy of molecular knowledge is profound, even humans learn from different modalities including both intuitive diagrams and professional texts to assist their understanding. Inspired by this, we propose a molecular multimodal foundation model which is pretrained from molecular graphs and their semantically related textual data (crawled from published Scientific Citation Index papers) via contrastive learning. This AI model represents a critical attempt that directly bridges molecular graphs and natural language. Importantly, through capturing the specific and complementary information of the two modalities, our proposed model can better grasp molecular expertise. Experimental results show that our model not only exhibits promising performance in cross-modal tasks such as cross-modal retrieval and molecule caption, but also enhances molecular property prediction and possesses capability to generate meaningful molecular graphs from natural language descriptions. We believe that our model would have a broad impact on AI-empowered fields across disciplines such as biology, chemistry, materials, environment, and medicine, among others.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3567.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3567.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MoMu+MoFlow (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MoMu-based zero-shot text-to-graph molecule generation using MoFlow</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot cross-modal molecule design method that combines a pretrained molecular multimodal foundation model (MoMu: graph encoder GIN + text encoder BERT/Sci-BERT/KV-PLM) with a pretrained flow-based molecular generator (MoFlow). The method optimizes the generator's latent seed to maximize similarity between text embeddings and generated-graph embeddings, producing novel molecular graphs from natural-language property descriptions without additional generator fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Molecular Multimodal Foundation Model Associating Molecule Graphs with Natural Language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MoMu (GIN + BERT variants) + MoFlow</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>MoMu: multimodal foundation model with a 5-layer GIN graph encoder (300-d hidden) and a BERT-base style text encoder (hidden size 768) initialized from Sci-BERT or KV-PLM; projection heads map graph/text features to 256-d shared space. Pretrained with contrastive inter- and intra-modal losses on ~15.6K graph-document pairs. MoFlow: flow-based invertible graph generator pretrained on ZINC250K (learns mapping between Gaussian latent q and molecular graph/bond matrices), limited to max 38 atoms and 9 element types in the used checkpoint.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Differentiable latent optimization: treat the MoFlow latent vector q as the only learnable parameter, generate a (soft) molecular graph from q via MoFlow's reverse flow, pass the soft graph into MoMu's graph encoder to obtain z^G, compute cosine similarity with the text embedding z^T from MoMu's text encoder, and update q by gradient ascent (Adam) to maximize similarity; after optimization, discretize (argmax) atom/bond probabilities to obtain final molecular graph. Formal charges prohibited in final molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General-purpose molecular design from natural-language descriptions (examples: dye discovery, molecules with specified functional groups, drug-like properties such as high water solubility, high membrane permeability, low toxicity); envisioned applications: drug discovery, materials design, catalyst design, targeted molecule/property design.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Qualitative case studies (visual inspection, structural motifs), diversity of generated molecules, validity (implicit via MoFlow and argmax), and downstream property predictions using fine-tuned property-prediction models (for water solubility, barrier permeability, toxicity) cited from literature; reported counts for example-based checks (e.g., number of conditions met in example), and retrieval-confirmation counts in retrieval experiments (e.g., dyes: 4 of top-5 confirmed). No large-scale numeric generation metrics (e.g., novelty %, QED) are reported for generation beyond case examples.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Method generated diverse molecules that qualitatively match high-level vague, functional, and structural natural-language descriptions, whereas MolT5 baseline typically produced a single molecule or failed for vague descriptions. Example outcomes: for one multi-condition description, three out of four specified conditions were met in generated molecules (as assessed by property predictors); for a BBBP/permeability/toxicity/solubility example, MoMu+MoFlow generated molecules predicted to be highly penetrating, low-toxicity, and water-soluble (MolT5 produced a molecule with low penetration). For dye-discovery style retrieval experiments (related retrieval task), MoMu retrieved 4/5 valid dyes in top-5. Results are qualitative and example-driven rather than aggregated quantitative benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared to MolT5 (translation-trained T5 model used as baseline), the MoMu+MoFlow method produced more diverse and more property-aligned molecules from vague/functional text inputs; MolT5 often failed or returned single/invalid outputs for such inputs. The paper contrasts this method conceptually with traditional property-conditioned generators that require supervised property-labeled training, emphasizing zero-shot flexibility. No head-to-head large-scale quantitative benchmark vs other generative models is reported beyond illustrative examples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Acknowledged limitations include (1) dependence on the transfer ability of MoMu and the coverage of the paired graph-text pretraining (only ~15.6K pairs, weakly correlated texts), so text concepts not present or rare in training texts are hard to realize; (2) dependence on the pre-trained generator: the MoFlow checkpoint used restricts molecules to ≤38 atoms and to 9 common elements, limiting chemical space and element diversity; (3) risk of spurious correlations and dataset bias from weakly-correlated retrievaled texts; (4) evaluation is largely qualitative and relies on in-silico property predictors rather than wet-lab validation; (5) the approach optimizes latent vectors and thus is constrained by the generator's latent manifold (may not reach some chemistries); (6) no reported synthesizability/retrosynthesis verification for generated candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>null_field_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Molecular Multimodal Foundation Model Associating Molecule Graphs with Natural Language', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3567.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3567.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolT5 (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolT5 (T5-based molecular translation model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A T5-based encoder-decoder model (MolT5-small/base/large variants) trained for molecule–text translation tasks (e.g., structural description ↔ SMILES); used in this paper as a baseline for generating molecules from natural-language descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Translation between molecules and natural language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolT5 (MolT5-small, MolT5-base, MolT5-large)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5-based encoder-decoder transformer variants adapted to molecular tasks (MolT5 variants), trained to translate SMILES/structural descriptions into natural language and vice versa (reference model from prior work). In this paper MolT5-large is used as the primary baseline for generation comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct text-to-SMILES (or text-to-molecule) translation using a sequence-to-sequence transformer trained on paired structural descriptions and SMILES; in this work MolT5 is applied to attempt to generate molecules from high-level/functional natural-language descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecule captioning and translation tasks; used here as a baseline for text-to-molecule generation from natural-language property/functional descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Standard sequence/translation metrics for captioning tasks in other sections (BLEU, METEOR, ROUGE-L, Text2Mol) were used for captioning; for generation-from-vague-description comparisons the evaluation was qualitative (validity of output, ability to meet functional conditions assessed by property predictors).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MolT5 often produced a single molecule and frequently failed to generate valid or explainable molecules from high-level vague or functional descriptions; in examples it produced fewer valid/diverse candidates and failed outright for some functional prompts where MoMu+MoFlow generated plausible candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Serves as the baseline; MoMu+MoFlow outperformed MolT5 qualitatively on generating diverse and property-aligned molecules from vague/functional text prompts. MolT5 remains appropriate for direct structural-description→SMILES translation but is not well-suited for open-ended property-driven imagination without further adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>MolT5 is trained to translate structural descriptions into specific molecules and thus cannot easily produce diverse candidates that satisfy vague or high-level property descriptions; it lacks a mechanism to explore chemical latent space conditioned on abstract properties, resulting in limited diversity and failure on non-structural prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Molecular Multimodal Foundation Model Associating Molecule Graphs with Natural Language', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Translation between molecules and natural language <em>(Rating: 2)</em></li>
                <li>Moflow: an invertible flow model for generating molecular graphs <em>(Rating: 2)</em></li>
                <li>Language models can learn complex molecular distributions <em>(Rating: 2)</em></li>
                <li>A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3567",
    "paper_id": "paper-1c7a4e8d9f4fcf19a5d1caa078c66ca39cb75dd2",
    "extraction_schema_id": "extraction-schema-79",
    "extracted_data": [
        {
            "name_short": "MoMu+MoFlow (zero-shot)",
            "name_full": "MoMu-based zero-shot text-to-graph molecule generation using MoFlow",
            "brief_description": "A zero-shot cross-modal molecule design method that combines a pretrained molecular multimodal foundation model (MoMu: graph encoder GIN + text encoder BERT/Sci-BERT/KV-PLM) with a pretrained flow-based molecular generator (MoFlow). The method optimizes the generator's latent seed to maximize similarity between text embeddings and generated-graph embeddings, producing novel molecular graphs from natural-language property descriptions without additional generator fine-tuning.",
            "citation_title": "A Molecular Multimodal Foundation Model Associating Molecule Graphs with Natural Language",
            "mention_or_use": "use",
            "model_name": "MoMu (GIN + BERT variants) + MoFlow",
            "model_description": "MoMu: multimodal foundation model with a 5-layer GIN graph encoder (300-d hidden) and a BERT-base style text encoder (hidden size 768) initialized from Sci-BERT or KV-PLM; projection heads map graph/text features to 256-d shared space. Pretrained with contrastive inter- and intra-modal losses on ~15.6K graph-document pairs. MoFlow: flow-based invertible graph generator pretrained on ZINC250K (learns mapping between Gaussian latent q and molecular graph/bond matrices), limited to max 38 atoms and 9 element types in the used checkpoint.",
            "generation_method": "Differentiable latent optimization: treat the MoFlow latent vector q as the only learnable parameter, generate a (soft) molecular graph from q via MoFlow's reverse flow, pass the soft graph into MoMu's graph encoder to obtain z^G, compute cosine similarity with the text embedding z^T from MoMu's text encoder, and update q by gradient ascent (Adam) to maximize similarity; after optimization, discretize (argmax) atom/bond probabilities to obtain final molecular graph. Formal charges prohibited in final molecules.",
            "application_domain": "General-purpose molecular design from natural-language descriptions (examples: dye discovery, molecules with specified functional groups, drug-like properties such as high water solubility, high membrane permeability, low toxicity); envisioned applications: drug discovery, materials design, catalyst design, targeted molecule/property design.",
            "evaluation_metrics": "Qualitative case studies (visual inspection, structural motifs), diversity of generated molecules, validity (implicit via MoFlow and argmax), and downstream property predictions using fine-tuned property-prediction models (for water solubility, barrier permeability, toxicity) cited from literature; reported counts for example-based checks (e.g., number of conditions met in example), and retrieval-confirmation counts in retrieval experiments (e.g., dyes: 4 of top-5 confirmed). No large-scale numeric generation metrics (e.g., novelty %, QED) are reported for generation beyond case examples.",
            "results_summary": "Method generated diverse molecules that qualitatively match high-level vague, functional, and structural natural-language descriptions, whereas MolT5 baseline typically produced a single molecule or failed for vague descriptions. Example outcomes: for one multi-condition description, three out of four specified conditions were met in generated molecules (as assessed by property predictors); for a BBBP/permeability/toxicity/solubility example, MoMu+MoFlow generated molecules predicted to be highly penetrating, low-toxicity, and water-soluble (MolT5 produced a molecule with low penetration). For dye-discovery style retrieval experiments (related retrieval task), MoMu retrieved 4/5 valid dyes in top-5. Results are qualitative and example-driven rather than aggregated quantitative benchmarks.",
            "comparison_to_baselines": "Compared to MolT5 (translation-trained T5 model used as baseline), the MoMu+MoFlow method produced more diverse and more property-aligned molecules from vague/functional text inputs; MolT5 often failed or returned single/invalid outputs for such inputs. The paper contrasts this method conceptually with traditional property-conditioned generators that require supervised property-labeled training, emphasizing zero-shot flexibility. No head-to-head large-scale quantitative benchmark vs other generative models is reported beyond illustrative examples.",
            "limitations_challenges": "Acknowledged limitations include (1) dependence on the transfer ability of MoMu and the coverage of the paired graph-text pretraining (only ~15.6K pairs, weakly correlated texts), so text concepts not present or rare in training texts are hard to realize; (2) dependence on the pre-trained generator: the MoFlow checkpoint used restricts molecules to ≤38 atoms and to 9 common elements, limiting chemical space and element diversity; (3) risk of spurious correlations and dataset bias from weakly-correlated retrievaled texts; (4) evaluation is largely qualitative and relies on in-silico property predictors rather than wet-lab validation; (5) the approach optimizes latent vectors and thus is constrained by the generator's latent manifold (may not reach some chemistries); (6) no reported synthesizability/retrosynthesis verification for generated candidates.",
            "null_field_example": null,
            "uuid": "e3567.0",
            "source_info": {
                "paper_title": "A Molecular Multimodal Foundation Model Associating Molecule Graphs with Natural Language",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "MolT5 (baseline)",
            "name_full": "MolT5 (T5-based molecular translation model)",
            "brief_description": "A T5-based encoder-decoder model (MolT5-small/base/large variants) trained for molecule–text translation tasks (e.g., structural description ↔ SMILES); used in this paper as a baseline for generating molecules from natural-language descriptions.",
            "citation_title": "Translation between molecules and natural language",
            "mention_or_use": "use",
            "model_name": "MolT5 (MolT5-small, MolT5-base, MolT5-large)",
            "model_description": "T5-based encoder-decoder transformer variants adapted to molecular tasks (MolT5 variants), trained to translate SMILES/structural descriptions into natural language and vice versa (reference model from prior work). In this paper MolT5-large is used as the primary baseline for generation comparisons.",
            "generation_method": "Direct text-to-SMILES (or text-to-molecule) translation using a sequence-to-sequence transformer trained on paired structural descriptions and SMILES; in this work MolT5 is applied to attempt to generate molecules from high-level/functional natural-language descriptions.",
            "application_domain": "Molecule captioning and translation tasks; used here as a baseline for text-to-molecule generation from natural-language property/functional descriptions.",
            "evaluation_metrics": "Standard sequence/translation metrics for captioning tasks in other sections (BLEU, METEOR, ROUGE-L, Text2Mol) were used for captioning; for generation-from-vague-description comparisons the evaluation was qualitative (validity of output, ability to meet functional conditions assessed by property predictors).",
            "results_summary": "MolT5 often produced a single molecule and frequently failed to generate valid or explainable molecules from high-level vague or functional descriptions; in examples it produced fewer valid/diverse candidates and failed outright for some functional prompts where MoMu+MoFlow generated plausible candidates.",
            "comparison_to_baselines": "Serves as the baseline; MoMu+MoFlow outperformed MolT5 qualitatively on generating diverse and property-aligned molecules from vague/functional text prompts. MolT5 remains appropriate for direct structural-description→SMILES translation but is not well-suited for open-ended property-driven imagination without further adaptation.",
            "limitations_challenges": "MolT5 is trained to translate structural descriptions into specific molecules and thus cannot easily produce diverse candidates that satisfy vague or high-level property descriptions; it lacks a mechanism to explore chemical latent space conditioned on abstract properties, resulting in limited diversity and failure on non-structural prompts.",
            "uuid": "e3567.1",
            "source_info": {
                "paper_title": "A Molecular Multimodal Foundation Model Associating Molecule Graphs with Natural Language",
                "publication_date_yy_mm": "2022-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Translation between molecules and natural language",
            "rating": 2
        },
        {
            "paper_title": "Moflow: an invertible flow model for generating molecular graphs",
            "rating": 2
        },
        {
            "paper_title": "Language models can learn complex molecular distributions",
            "rating": 2
        },
        {
            "paper_title": "A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals",
            "rating": 2
        }
    ],
    "cost": 0.01246175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Molecular Multimodal Foundation Model Associating Molecule Graphs with Natural Language</h1>
<p>Bing $\mathrm{Su}^{1,2}$, Dazhao $\mathrm{Du}^{3,4}$, Zhao Yang ${ }^{1,2}$, Yujie Zhou ${ }^{1,2}$, Jiangmeng $\mathrm{Li}^{3,4}$, Anyi Rao ${ }^{5}$, Hao Sun ${ }^{1,2}$, Zhiwu Lu ${ }^{1,2}$, Ji-Rong Wen ${ }^{1,2}$<br>${ }^{1}$ Gaoling School of Artificial Intelligence, Renmin University of China, Beijing 100872, China<br>${ }^{2}$ Beijing Key Laboratory of Big Data Management and Analysis Methods<br>${ }^{3}$ Science \&amp; Technology on Integrated Information System Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China<br>${ }^{4}$ University of Chinese Academy of Sciences, Beijing, China<br>${ }^{5}$ The Chinese University of Hong Kong, Hong Kong, China</p>
<h4>Abstract</h4>
<p>Although artificial intelligence (AI) has made significant progress in understanding molecules in a wide range of fields, existing models generally acquire the single cognitive ability from the single molecular modality. Since the hierarchy of molecular knowledge is profound, even humans learn from different modalities including both intuitive diagrams and professional texts to assist their understanding. Inspired by this, we propose a molecular multimodal foundation model which is pretrained from molecular graphs and their semantically related textual data (crawled from published Scientific Citation Index papers) via contrastive learning. This AI model represents a critical attempt that directly bridges molecular graphs and natural language. Importantly, through capturing the specific and complementary information of the two modalities, our proposed model can better grasp molecular expertise. Experimental results show that our model not only exhibits promising performance in cross-modal tasks such as cross-modal retrieval and molecule caption, but also enhances molecular property prediction and possesses capability to generate meaningful molecular graphs from natural language descriptions. We believe that our model would have a broad impact on AI-empowered fields across disciplines such as biology, chemistry, materials, environment, and medicine, among others.</p>
<p>Understanding molecule-related knowledge and discovering molecular properties are critical for scientific exploration across a wide range of disciplines such as biomedicine, chemistry, materials, etc. Traditional methods require professional skills to conduct large amounts of trial-and-error wet biochemical experiments [1-3], which are not only expensive but also time-consuming. With the advancement of deep learning [4], using AI to assist in scientific exploration such as predicting molecular properties and generating molecule candidates has become possible witnessed by many critical signs of progress [5-9].</p>
<p>Different from humans who understand molecules from multiple modalities as shown in Figure 1(a), most existing AI models are specified for a single cognitive ability (e.g., property prediction, molecule generation, or literature comprehension) and a single modality of molecules (e.g., molecular graph, SMILES string, or text), as shown in Figure 1(b). These models are mainly divided into two categories. Firstly, language-based models take natural language about molecular knowledge and/or SMILES strings as input. For example, in [10, 11], molecular property prediction models are designed for SMILES strings of molecules; several works [12-14] focus on literature comprehension learning from biochemical texts; in generation models [1524], the generated molecules are represented as SMILES. Secondly, graph-based models can only process molecular graphs. For example, in [25-31], graph neural network (GNN) [32] based molecular property prediction models are learned from molecular graphs; in [33-39], generation models are learned from graph data to directly generate molecular graphs. Note that extensive manual annotations or collections of specific properties are required for training these models, while other properties of molecules and associated conditions are often omitted and thus cannot be perceived by the models. Therefore, they can only deal with one modality of the molecule and cannot acquire comprehensive molecule understanding.</p>
<p>Professional molecular expertise is usually documented in published papers or books in natural language. Human beings can learn such knowledge by reading the related literature. There have been some works [12, 14] toward making AI models acquire such knowledge from a large number of professional lit-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Conceptual comparison of our MoMu model with the human brain and existing single-modal AI models. (a) The human brain can comprehensively understand molecular knowledge by learning from multiple modalities. (b) Existing AI models generally employ a single network to gain a single cognitive ability from a single modality of molecules. These models mainly fall into two categories: (top) languagebased models take as input natural language texts and/or SMILES strings, which can only be applied to text-related tasks; (bottom) graph-based models take molecular graphs as input, which can only be adapted to graph-related tasks. (c) Our MoMu model learns from weakly-correlated paired text-graph data to associate the molecular graph modality with the natural language modality. It consists of two encoders to tackle the two modalities, respectively, which are jointly trained via contrastive learning. Due to the strong generalization ability of the learned representations, MoMu can be adapted to various downstream tasks such as crossmodality retrieval, molecule caption, property prediction, and text-to-graph molecule generation, and thus effectively facilitates molecular-related scientific exploration.
erature. However, learning knowledge only at the language level may simply associate entities of molecule names with property descriptions, resulting in the challenge in connecting property descriptions with molecular structures. Recently, a deep learning system is developed [40] to jointly learn molecule-related texts and molecular SMILES strings to establish their relationships (as also included in the top of Figure 1(b)). However, this approach has distinct drawbacks: one-dimensional SMILES strings may lose some molecular structure information and cannot capture structural similarities between molecules; moreover, the SMILES of a molecule is not unique.</p>
<p>Different from molecular SMILES strings, molecule graphs are more intuitive and can better reveal the functional structures. Since the molecular knowledge is profound and difficult to understand, even human beings assist their learning process by associating natural language descriptions with molecule graphs. Inspired by this, we propose a molecular multi-modal foundation model (MoMu) that consists of two separate encoders respectively for molecule graphs and texts, as shown in Figure 1(c). To train the model, we collect about 15 K paired molecule graph-text data, where the text of a molecule is retrieved from the SCI paper dataset [41]. The two encoders are jointly trained by contrastive learning so that representations of molecule graphs are as similar as representations of their related texts and as dissimilar as those of unrelated texts. In this way, our MoMu model is able to associate molecule graphs with their biomedical text descriptions. MoMu represents a critical attempt to bridge the two modalities.</p>
<p>Recently, several multi-modal foundation models [42-45] have been developed to connect images and texts. These models are trained with large-scale weakly-related image-text data. Since it is more difficult for humans to learn specialized biological and chemistry literature than general language knowledge, making AI acquire multi-modal molecular knowledge also faces greater challenges. Firstly, much fewer paired molecular graph-text data are available than more general image-text data. Secondly, learning professional molecular knowledge requires basic cognitive abilities, and thus it is difficult to learn a molecular multi-modal model from scratch. To tackle these challenges, we employ the molecular graph model [46] and biomedical text model [12, 40] pre-trained from large-scale single-modality unlabeled data as the initialization of the two encoders in our foundation model, and finetune them with our collected professional graph-text data through contrastive learning. As a result, our foundation model requires much less manual labor and computational resources for training.</p>
<p>By processing molecules in multiple modalities, our foundation model can be applied to a wide range of downstream tasks. For cross-modality tasks, experimental results show that our model achieves a significant improvement in cross-modality retrieval and improves molecule captioning by incorporating the graph feature. More importantly, our model implicitly establishes the connection between structures of molecules and language descriptions, thereby allowing text-to-graph generation. In many realistic applications such as catalyst design and targeted drug discovery, designing new molecules often relies on the knowledge, experience, and inspiration of experts. Although deep learning-based generation models [33-36, 39] have been developed, they rely on large numbers of molecules with specified properties for modeling training. Differently, based on our MoMu model, we design a zero-shot molecule generation method that can directly imagine new molecules from textual descriptions of the desired conditions. For the graph modality, we apply the pre-trained MoMu to molecular property prediction and find that our graph encoder supervised by the weakly related language descriptions outperforms other self-supervised methods trained only from molecule graphs. These findings demonstrate that, due to the strong generalization and imagination abilities, our pre-trained MoMu model can advance scientific exploration and thus has the potential to lead to broad impact in biology, chemistry, materials, medicine, and other molecular-related fields.</p>
<h1>Results</h1>
<p>Our molecular multimodal foundation model, MoMu, is pre-trained based on the paired multi-modal data consisting of molecule graphs and their weakly-related biochemical descriptions retrieved from publicly available Scientific Citation Index (SCI) papers. The pre-trained MoMu exhibits strong generalization abilities in a wide range of downstream tasks, including cross-modal molecule retrieval, molecule caption, zeroshot molecule generation, and molecular property prediction, as shown in this section.</p>
<h2>Data collection and overview of MoMu</h2>
<p>We construct a dataset of molecular graph-text pairs for pre-training our model. We first collect the names, synonyms, and SMILES strings of the top 50K molecular compounds in PubChem [47]. To obtain the molecule graphs of the collected compounds, we employ the smiles2graph function provided by OGB [48], which converts SMILES strings into molecular graphs. As shown in the Supplementary Figure 1(a), for each molecule, we retrieve related texts in published scientific papers in the S2orc [41] database into a document as weak semantic supervision by using its name as the query. S2orc is a corpus database containing 136M+ papers from different fields, and we only retrieve papers in the fields of Medicine, Biology, Chemistry, and Computer Science, as they are more likely to contain molecule-relevant descriptions. Finally, we obtain 15,613 graph-document pairs to form our multi-modal molecular dataset. There are about 37 million paragraphs in all the collected documents.</p>
<p>The overall architecture and the pre-training process of our MoMu model are illustrated in Figure 1(c) and the Supplementary Figure 1(b). MoMu consists of a text encoder and a graph encoder, which encode the molecular graphs and texts into a joint representation space, respectively. We use Graph Isomorphism Network (GIN) [49] and Bert [50] as the graph and text encoders, respectively. We train MoMu based on our collected paired dataset. For each pair within a minibatch, we utilize two different types of graph augmentations to create two separate graphs from the molecular graph, and randomly sample two different sentences from the document. Following the contrastive paradigm in DeClip [51], we use inter-modal and intra-modal contrastive learning [42, 52, 53] as the proxy task for pretraining, which employs the InfoNCE loss [54]. The proxy task aims to bring samples from different modalities with the same semantic information closer together in the feature space while pushing samples with different semantics away.</p>
<p>Compared with general image-text data, there are relatively much fewer molecule-related graph-text data, which is not sufficient to train graph and text encoders from scratch. Just as human beings should also</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Graph-to-text retrieval results. (a) The performance of graph-to-text (G-T) retrieval on the PCdes dataset, where the results of the compared methods for the sentence-level retrieval are reported in [40]. (b) Retrieval results by using an example/SMILES in the test set of PCdes as the query by KV-PLM* and our MoMu-K. (c) The performance of zero-shot graph-to-text (G-T) retrieval on our collected test set.
have the ability to recognize graphs and languages when learning specialized knowledge, enabling AI to learn specialized molecular knowledge also needs to be founded on trained general graph and text encoders. Therefore, we initialize the graph encoder with the self-supervised trained weights of GIN provided in [46], and initialize the text encoder with the pre-trained weights of BERT provided by Sci-BERT [12] and KVPLM [40], respectively. We denote MoMu initialized with the weights of Sci-BERT and KV-PLM by MoMuS and MoMu-K, respectively. Details of the data collection and pretraining method for MoMu are presented in the Methods section. The pre-trained MoMu is able to process molecular graphs and natural language texts in a unified manner, and acquire the common and transferable knowledge from such heterogeneous data that can be easily generalized to different downstream tasks.</p>
<h1>Cross-modality retrieval</h1>
<p>Since our MoMu model is pre-trained by matching weakly-correlated texts to corresponding molecular graphs, it is able to process both the graph and text modalities of molecules. We evaluate its performance in cross-modality retrieval. Given a molecule graph, graph-to-text (G-T) retrieval aims to retrieve the most relevant text descriptions of this molecule. Conversely, given a text paragraph, text-to-graph (T-G) retrieval aims at retrieving the most relevant molecule graph it describes. We evaluate MoMu on the PCdes dataset [40], which contains SMILES and the paired property descriptions of 15 K molecules in PubChem. The dataset has been divided into a training set of 10,500 pairs, a validation set of 1,500 pairs, and a test set of 3,000 pairs $^{1}$. We convert the SMILES string in each pair into the molecule graph. In the G-T/T-G task, we calculate and rank the cosine similarities between the representation of the query graph/text by the graph/text encoder of MoMu and the representations of all texts/graphs by the text/graph encoder of MoMu, as shown in the Supplementary Figure 2. Following [40], we perform retrieval in randomly sampled mini-batches ( 64 pairs per batch) and all test pairs, respectively, and report the average accuracy of the top-1 retrieval result and the recall of the top-20 results, respectively. In [40], one sentence is randomly sampled from the text corresponding to each molecule for retrieval, and we denote this setting by sentence-level retrieval. We further evaluate the setting of using the full description paragraph per molecule, which is denoted by paragraph-level retrieval.</p>
<p>As in [40], the compared methods including Sci-BERT [12], and KV-PLM [40] ${ }^{2}$, as well as our MoMu, are fine-tuned on the training set of PCdes to make a fair comparison. For the G-T task, comparisons of different methods are shown in Figure 2(a). Both MoMu-S and MoMu-K outperform other methods that directly use SMILES for retrieval. We illustrate a retrieval example in Figure 2(b) by using the licodione molecule as the query. None of the top-3 paragraph-level retrieval results of KV-PLM* from the whole test</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Text-to-graph retrieval results. (a) The performance of text-to-graph (T-G) retrieval on the PCdes dataset, where the results of the compared methods for the sentence-level retrieval are reported in [40]. (b) Retrieval results by using a text paragraph in the test set of PCdes as the query by KV-PLM* and our MoMu-K. (c) A case study by using a query text to retrieve molecules that can make dyes. Four of the top-5 molecules retrieved by MoMu-K are confirmed to be effective. (d) The performance of zero-shot text-to-graph (T-G) retrieval on our collected test set.</p>
<p>Set of PCdes match the SMILES of this molecule, while the top-1 result of our MoMu-K accurately hits the corresponding description of the molecular graph. To verify the generalization of MoMu, we conduct zero-shot retrieval, where encoders of the pre-trained MoMu are directly applied without fine-tuning. Considering that some of these 15K pairs in PCdes may have been collected as our pre-training data for MoMu, we collect 5,562 graph-text pairs with compound ids ranging from 50,000 to 100,000 from PubChem that are not used in pre-training. Comparisons with Sci-BERT and KV-PLM on this collected test set for zero-shot retrieval are shown in Figure 2(c). MoMu also outperforms Sci-BERT and KV-PLM significantly, which further demonstrates the generalization ability of MoMu.</p>
<p>For the T-G task, comparisons of different methods are shown in Figure 3(a). Both MoMu-S and MoMu-</p>
<p>K generally outperform other methods. We also illustrate a retrieval example in Figure 3(b) by using a paragraph as the query to retrieve SMILES or graphs from the whole test set of PCdes. The third molecule in the ranking list of KV-PLM* matches the groundtruth, while the top-1 result of MoMu-K hits and the other retrieved molecules are also similar to the groundtruth. In Figure 3(c), we further conduct a case study to discovery dye molecules from the test set of PCdes by our MoMu-K. We use the text "It has a role as a dye." as the query. In the top-5 retrieved molecules, four are confirmed to be valid dyes. It is unclear whether the remaining molecule can be used as a dye, because there is no record of dye properties in PubChem for this molecule. Comparisons with Sci-BERT and KV-PLM on our collected test set for zero-shot retrieval are shown in Figure 3(d). Again, our method outperforms other methods significantly. These results demonstrate that, compared with KV-PLM which jointly models SMILES of molecules and language texts, our MoMu can better bridge molecular structures and natural language descriptions.</p>
<p>On both datasets with different settings, MoMu-S and MoMu-K obtain comparable results, i.e., initializing the text encoder of MoMu with KV-PLM does not lead to better performance than Sci-BERT. This shows that the structural information learned from one-dimensional SMILES strings of molecules can not be easily transferred to structured molecular graphs, while MoMu directly employs the graph neural network to capture structural information with the supervision of language descriptions.</p>
<h1>Molecule caption</h1>
<p>The molecule captioning task is proposed in [55], which aims to generate texts to describe the given molecule. We utilize MolT5 (MolT5-small, MolT5-base, or MolT5-large) [55] as the baseline, which translates the SMILES strings into natural language through the T5-based encoder-decoder transformer architecture. As shown in the Supplementary Figure 3, to better utilize the structural information of the input molecule for translation, we append the graph feature of the molecular graph to the inputs of the MolT5 encoder through a feature mapping module, which is implemented by a multi-layer perceptron. Following [55], we evaluate MolT5 and our MoMu-enhanced MolT5 on the ChEBI-20 dataset [56]. Results are shown in Figure 4(a). The BLEU, METEOR, and Text2Mol metrics are all improved with the additional graph features, but the ROUGEL metric is dropped. These results show that incorporating the structural graph information generally leads to smoother and more accurate captions. MoMu-enhanced MolT5 achieves higher scores on the Text2Mol metric, which indicates that the generated captions are more similar to the ground-truth descriptions. This is also evidenced by several examples shown in Figure 4(b), where MoMu results in more accurate captions for these molecules with complex structures, e.g., long chains and multiple rings.</p>
<h2>Zero-shot text-to-graph molecule generation</h2>
<p>We propose a new task called zero-shot text-to-graph molecule generation. The goal is to design a crossmodality molecule generator that takes as input the natural language description of the desired conditions and imagines new molecules that match the description. The proposed task is different from the text-to-molecule translation task in [55], where the input text is the natural language description of the structure of a groundtruth molecule and the goal is to translate the description into the corresponding molecule. For example, the text "The molecule is a member of the class of monohydroxy-1,4-benzoquinones that is 2-hydroxy-1,4benzoquinone carrying an additional methyl substituent at position 5 . It is a conjugate acid of a 2 -oxido-5methylquinone." exactly describes the molecule " $\mathrm{CC1}=\mathrm{CC}(=\mathrm{O}) \mathrm{C}(=\mathrm{O}) \mathrm{C}=\mathrm{C} 1 \mathrm{O}$ ". Differently, in our zero-shot molecule generation, the natural language text describes the specified properties or conditions of desired new molecules rather than the exact structure of a known molecule. An example of such input description can be "The molecule has high water solubility and barrier permeability with low toxicity". Therefore, there may exist different molecules that fit the description. We aim to generate novel molecules that are not present in existing molecule datasets but match the description as closely as possible.</p>
<p>Based on our pre-trained MoMu and a pre-trained molecule generator, we design a zero-shot molecule generation method that does not require additional training data. The architecture of the method is shown in the Supplementary Figure 4, including a MoMu-based multi-modal similarity measuring module and a differentiable molecule generator. We utilize the flow-based molecular generative model, MoFlow [35], as the generator, which learns a transformation from a Gaussian distribution in the latent space to the distribution of molecules. Given a random sample $\boldsymbol{q}$ from the Gaussian distribution, MoFlow employs the reverse flow to transform $\boldsymbol{q}$ to a molecule. To achieve text-to-graph molecule generation, we set $\boldsymbol{q}$ as the learnable parameter, and freeze the parameters of both the pre-trained MoMu and MoFlow pre-trained from the ZINC250K dataset [57]. We use MoFlow to generate the molecular graph from $\boldsymbol{q}$ and then feed it into the graph encoder of MoMu to obtain the graph representation. The input text description is fed into the text encoder of MoMu to obtain the text representation. We optimize $\boldsymbol{q}$ by maximizing the cosine similarity between the graph representation and the text representation. The optimized $\boldsymbol{q}$ is fed into MoFlow to generate the final molecule.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Molecule caption results. (a) Comparison of MolT5 and our MoMu-enhanced MolT5 on the ChEBI-20 dataset. MolT5 represents the performance with only the MolT5 model, while MoMu+MolT5 represents the performance after adding GIN-extracted graph features to the input of the MolT5 encoder. (b) Example captions generated by different models.</p>
<p>We prohibit the inclusion of formal charges in the resulting molecule. For the detailed algorithm, please refer to the Methods section.</p>
<p>In Figure 5(a), we show the generated molecules from three high-level vague descriptions by MolT5 (large) [55] and our method with MoMu-S and MoMu-K, respectively. Given the input text, MolT5 can only output one molecule. Differently, our method can generate diverse molecules. The results indicate that our method has established an understanding of abstract concepts and, to a certain extent, its own aesthetic. For example, our method tends to regard locally symmetric and stretched molecular graphs as "beautiful", view molecular graphs with more different elements and groups as "versatile", and treat molecular graphs with many irregular connections, cluttered branches, and irregular terminals as "strange". Since MolT5 is trained to translate from a structural description to a specific molecule, it fails to generate explainable molecules from high-level vague descriptions.</p>
<p>In Figure 5(b), we show the generated molecules from descriptions of molecular functionalities. For</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Text-to-graph molecule generation results. (a) Molecules imagined from high-level vague descriptions. (b) Molecules imagined from functional descriptions. (c) Molecules imagined from structural descriptions.
the 4-th description, MolT5 fails to generate a valid molecule, while our methods generate molecules with conjugated double bonds or conjugated molecules. For the 5-th description with four conditions, MoMus successfully generate diverse molecules with hydroxyl groups, high oxygen content, and nitrogen to generate ammonia, so three out of four conditions are met. Different from existing AI-based molecule generation methods that are specified for fixed properties, our method adaptively generates molecular candidates based on the input text which can describe any desired one or several conditions. In the 6-th description, we specify three desired properties including high water solubility, high barrier permeability, and low toxicity, which can be evaluated by fine-tuned property prediction models in [58, 59]. The molecule generated by MolT5 has low</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">BBBP</th>
<th style="text-align: center;">Tox21</th>
<th style="text-align: center;">ToxCast</th>
<th style="text-align: center;">SIDER</th>
<th style="text-align: center;">ClinTox</th>
<th style="text-align: center;">MUV</th>
<th style="text-align: center;">HIV</th>
<th style="text-align: center;">BACE</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">No Pre-Train</td>
<td style="text-align: center;">$65.8 \pm 4.5$</td>
<td style="text-align: center;">$74.0 \pm 0.8$</td>
<td style="text-align: center;">$63.4 \pm 0.6$</td>
<td style="text-align: center;">$57.3 \pm 1.6$</td>
<td style="text-align: center;">$58.0 \pm 4.4$</td>
<td style="text-align: center;">$71.8 \pm 2.5$</td>
<td style="text-align: center;">$75.3 \pm 1.9$</td>
<td style="text-align: center;">$70.1 \pm 5.4$</td>
<td style="text-align: center;">66.96</td>
</tr>
<tr>
<td style="text-align: center;">Infomax</td>
<td style="text-align: center;">$68.8 \pm 0.8$</td>
<td style="text-align: center;">$75.3 \pm 0.5$</td>
<td style="text-align: center;">$62.7 \pm 0.4$</td>
<td style="text-align: center;">$58.4 \pm 0.8$</td>
<td style="text-align: center;">$69.9 \pm 3.0$</td>
<td style="text-align: center;">$75.3 \pm 2.5$</td>
<td style="text-align: center;">$76.0 \pm 0.7$</td>
<td style="text-align: center;">$75.9 \pm 1.6$</td>
<td style="text-align: center;">70.29</td>
</tr>
<tr>
<td style="text-align: center;">EdgePred</td>
<td style="text-align: center;">$67.3 \pm 2.4$</td>
<td style="text-align: center;">$76.0 \pm 0.6$</td>
<td style="text-align: center;">$64.1 \pm 0.6$</td>
<td style="text-align: center;">$60.4 \pm 0.7$</td>
<td style="text-align: center;">$64.1 \pm 3.7$</td>
<td style="text-align: center;">$74.1 \pm 2.1$</td>
<td style="text-align: center;">$76.3 \pm 1.0$</td>
<td style="text-align: center;">$\mathbf{7 9 . 9} \pm 0.9$</td>
<td style="text-align: center;">70.28</td>
</tr>
<tr>
<td style="text-align: center;">AttrMasking</td>
<td style="text-align: center;">$64.3 \pm 2.8$</td>
<td style="text-align: center;">$\mathbf{7 6 . 7} \pm 0.4$</td>
<td style="text-align: center;">$\mathbf{6 4 . 2} \pm 0.5$</td>
<td style="text-align: center;">$\mathbf{6 1 . 0} \pm 0.7$</td>
<td style="text-align: center;">$71.8 \pm 4.1$</td>
<td style="text-align: center;">$74.7 \pm 1.4$</td>
<td style="text-align: center;">$77.2 \pm 1.1$</td>
<td style="text-align: center;">$79.3 \pm 1.6$</td>
<td style="text-align: center;">71.15</td>
</tr>
<tr>
<td style="text-align: center;">ContextPred</td>
<td style="text-align: center;">$68.0 \pm 2.0$</td>
<td style="text-align: center;">$75.7 \pm 0.7$</td>
<td style="text-align: center;">$63.9 \pm 0.6$</td>
<td style="text-align: center;">$60.9 \pm 0.6$</td>
<td style="text-align: center;">$65.9 \pm 3.8$</td>
<td style="text-align: center;">$\mathbf{7 5 . 8} \pm 1.7$</td>
<td style="text-align: center;">$77.3 \pm 1.0$</td>
<td style="text-align: center;">$79.6 \pm 1.2$</td>
<td style="text-align: center;">70.89</td>
</tr>
<tr>
<td style="text-align: center;">GraphCL</td>
<td style="text-align: center;">$69.7 \pm 0.7$</td>
<td style="text-align: center;">$73.9 \pm 0.7$</td>
<td style="text-align: center;">$62.4 \pm 0.6$</td>
<td style="text-align: center;">$\mathbf{6 0 . 5} \pm 0.9$</td>
<td style="text-align: center;">$76.0 \pm 2.7$</td>
<td style="text-align: center;">$69.8 \pm 2.7$</td>
<td style="text-align: center;">$\mathbf{7 8 . 5} \pm 1.2$</td>
<td style="text-align: center;">$75.4 \pm 1.4$</td>
<td style="text-align: center;">70.78</td>
</tr>
<tr>
<td style="text-align: center;">MoMu-S</td>
<td style="text-align: center;">$\mathbf{7 0 . 5} \pm 2.0$</td>
<td style="text-align: center;">$\mathbf{7 5 . 6} \pm 0.3$</td>
<td style="text-align: center;">$\mathbf{6 3 . 4} \pm 0.5$</td>
<td style="text-align: center;">$\mathbf{6 0 . 5} \pm 0.9$</td>
<td style="text-align: center;">$\mathbf{7 9 . 9} \pm 4.1$</td>
<td style="text-align: center;">$70.5 \pm 1.4$</td>
<td style="text-align: center;">$75.9 \pm 0.8$</td>
<td style="text-align: center;">$76.7 \pm 2.1$</td>
<td style="text-align: center;">71.63</td>
</tr>
<tr>
<td style="text-align: center;">MoMu-K</td>
<td style="text-align: center;">$70.1 \pm 1.4$</td>
<td style="text-align: center;">$\mathbf{7 5 . 6} \pm 0.5$</td>
<td style="text-align: center;">$63.0 \pm 0.4$</td>
<td style="text-align: center;">$60.4 \pm 0.8$</td>
<td style="text-align: center;">$77.4 \pm 4.1$</td>
<td style="text-align: center;">$\mathbf{7 1 . 1} \pm 2.7$</td>
<td style="text-align: center;">$76.2 \pm 0.9$</td>
<td style="text-align: center;">$\mathbf{7 7 . 1} \pm 1.4$</td>
<td style="text-align: center;">71.36</td>
</tr>
</tbody>
</table>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Molecule property prediction results. (a) The performance of molecule property prediction under different pre-training schemes on the eight datasets, where the results of the compared methods are reported in [46, 60]. (b) The t-SNE visualization of the pretrained representations by different pre-training schemes before and after fine-tuning on the BBBP dataset.
penetration. Our method with both MoMu-S and MoMu-K can generate different molecules that are highly penetrating, have low toxicity, and are positively water-soluble.</p>
<p>In Figure 5(c), we show the generated molecules from descriptions of molecular structures. For the description of including nucleophilic groups, both MoMu-S and MoMu-K generate diverse molecules with amino groups, hydroxyl groups, or double bond. For the description of including electrophilic groups, both MoMu-S and MoMu-K generate diverse molecules with carbonyl groups, alkyl-like groups, or halogen atoms, even though formal charges are prohibited. For the description of including hydrophilic groups, both MoMu-S and MoMu-K generate molecules containing Hydroxyl, Amino, or Aldehyde with diverse structures. For the description of including lipophilic groups, both MoMu-S and MoMu-K generate molecules containing alkyl-like groups, halogen atoms, or benzene ring with diverse structures.</p>
<h1>Molecule property prediction</h1>
<p>Molecular property prediction is a graph-level prediction task that is usually used to evaluate the transferability of pre-trained graph encoders. We perform experiments on eight widely used datasets from MoleculeNet [61], including BBBP [62], Tox21 [63], ToxCast [64], SIDER [65], ClinTox [66, 67], MUV [68], HIV [69], and BACE [70], ranging from predicting bioactivity to toxicity and pharmacokinetics of molecules. Following [46, 60], we split each dataset into training and testing sets according to the scaffold of molecules, which can better reveal the ability of handling out-of-distribution molecules. To apply MoMu, we use the graph encoder in the pre-trained MoMu-S and MuMu-K as the initialization, respectively. We then fine-tune the graph encoder on the training sets of these datasets for predicting molecular properties, respectively. We compare MoMu with different graph-pretraining methods, including Infomax [71], EdgePred [72], AttrMasking [60], ContextPred [60], and GraphCL [46]. All methods use the same GIN encoder.</p>
<p>We conduct experiments 10 times and report the mean and standard deviation of ROC-AUC scores (\%) following [60]. As shown in Figure 6(a), the proposed MoMu-S and MoMu-K outperform the random initialization on most datasets and achieve the best results on 3 datasets. In particular, the graph encoder of</p>
<p>MoMu is initialized with weights pre-trained by GraphCL during our pre-training process. Compared with GraphCL, the performance of MoMu drops slightly on the HIV dataset, is comparable on the SIDER dataset, but is better on all the other six datasets. On average, MoMu-S and MoMu-K outperform all other compared methods. This indicates that the multimodal joint pre-training of MoMu captures useful information in weakly-correlated text descriptions for a better understanding of the abstract graph. The graph encoders in MoMu-S and MoMu-K obtain comparable results, which shows that the ability to model SMILES strings can not readily generalized to graphs and our MoMu successfully improves the representational ability of the graph encoder from the text modality.</p>
<p>In Figure 6(b), we employ t-SNE [73] to show the visualizations of the pretrained representations by GraphCL, MoMu-S, and MoMu-K before and after fine-tuning, respectively, on the training data of the BBBP dataset. Compared with the pretrained features by GraphCL, representations learned by MoMu-S and MoMu-K are more scattered and more uniformly distributed. Therefore, after fine-tuning, the representations of molecules with different barrier permeability properties are better separated since the overlapping between their distributions is reduced.</p>
<h1>Discussion</h1>
<p>We have presented a molecular multi-modal foundation model, namely MoMu, to bridge molecular graphs and natural language descriptions of molecules. MoMu imitates the human learning process from general knowledge to professional and is pre-trained from our collected weakly correlated data to align graph and text representations in a common space. Extensive experimental results on different downstream tasks such as molecule caption, cross-modality retrieval, and molecular property prediction demonstrate the cross-modality transfer ability and the advantage of joint multi-modal modeling of the proposed MoMu model.</p>
<p>Moreover, we have developed a zero-shot text-to-graph molecule generation method based on MoMu, which learns the generative seeds related to the specified text description for a pre-trained generator via leveraging the model's cross-modal ability. Our method is compatible with any molecule generator that allows gradient back-propagation. The broader the space that a molecular generator can efficiently explore, the more likely our method is to locate the region in which molecules are related to the conditions specified in the text. Since any conditions can be described in the input text, our method allows for convenient custom molecule design. Owing to these advantages, our method may have a significant impact on fields such as drug discovery and materials design that require new molecules with specified properties.</p>
<p>Besides these advantages and potentials, the proposed molecular multi-modal foundation model also suffers from some limitations. Compared with paired image-text data, the amount of our collected graph-text data for pre-training is much smaller. Even though this problem may be mitigated by initializing from pretrained unimodal encoders, MoMu may still not be sufficiently trained to fully establish the common space of the molecular graph modality and the natural language modality. In addition, the retrieved text of a molecule may not describe its properties or structure, although the text contains the name of the molecule. Therefore, MoMu can be misled to learn spurious correlations and stereotyped bias against pre-training data. Our zeroshot text-to-graph molecule generation method is limited by the transfer ability of MoMu and the pre-trained molecular generator. It is difficult to generate reliable molecules for molecular properties that do not appear or appear infrequently in the training texts. Since our method can be regarded as learning to sample from the latent distribution of the generator, both the quality of the produced molecules and the chemical space that can be explored highly depend on the pre-trained generator. The MoFlow pretrained on zinc250k used in this paper can only generate molecules with a maximum of 38 atoms, and atoms can only be one of the common 9 elements. This also limits the molecular space that our method can generate.</p>
<p>In our future work, we intend to improve and better utilize MoMu from the following aspects: (1). Collecting larger-scale molecule data in more modalities (e.g., 3D conformation) and retrieving more related text descriptions, which may enable the training of a more powerful molecular multimodal model. (2). Adopting strongly-correlated paired molecular data and more advanced molecule generators to construct more powerful zero-shot text-to-graph molecule design methods. (3). Developing interpretable tools to reveal the structure of the learned cross-modal common space and how MoMu bridges molecule structures to textural descriptions of molecular properties. (4). Applying the pre-trained MoMu to more downstream tasks and real-world specific cases, e.g., validating the molecules generated by our zero-shot generation method via wet experiments in designing drugs for some special diseases.</p>
<h2>Methods</h2>
<p>Data collection. PubChem [47] contains the basic information of over 150 million chemicals. Only simple physical and chemical properties of molecules are annotated in the PubChem database and there are no exact</p>
<p>language descriptions available for most molecules. We extract papers in the fields of Medicine, Biology, Chemistry, and Computer Science in the S2orc corpus database. To avoid as much as possible the special characters in the text related to the experimental data, we only retrieve from the abstract, introduction, and conclusion sections in each extracted paper. As shown in the Supplementary Figure 1(a), for each molecule, we first use its name as the query to retrieve sentences including this name. Each retrieved sentence and its neighboring sentences are recorded into a document as a paragraph. If less than two pieces of paragraphs are retrieved by name, we then search by the synonyms of the molecule. When 5000 pieces of paragraphs are retrieved or the size of the document exceeds 500 Mb , the retrieval of the molecule is early terminated. Not all 50,000 molecules can be retrieved with corresponding text descriptions. In each retrieved molecule graph-document pair, sentences in the document reveal weakly-correlated semantic information of the corresponding molecule graph.</p>
<p>Graph augmentation. Each molecular graph needs to undergo two different random augmentations into two augmented molecular graphs that retain similar semantics to the original molecule. We utilize the data augmentations introduced by GraphCL [46]. Specifically, we adopt two types of graph augmentations considering priors of molecules, i.e., node dropping and subgraph. Node dropping randomly discards a certain portion of vertices of the original graph. For a molecular graph, missing certain atoms (e.g., some hydrogen atoms in the chemical compound) does not alter its semantic information. Subgraph means sampling a subgraph from the original graph using the random walk. The properties of a molecule have certain similarities to the properties of molecules formed by its subgraphs, e.g., some molecules that contain the same functional group. Therefore, these two augmentations are more suitable for molecular-related tasks, which has been demonstrated in GraphCL [46].</p>
<p>Graph encoder. Graph Isomorphism Network (GIN) [49] is a provably powerful graph neural network (GNN) under the neighborhood aggregation framework, which has become a general backbone network in the graph domain, especially in molecular graph related tasks. Specifically, we utilize atom number and atom chirality as node features to feed into several GIN convolutional layers involving bond type and attribute features. Finally, we read out the feature of the entire molecular graph by performing average pooling of all node features.</p>
<p>Text encoder. The Bert model [50] is widely used as the feature extractor in natural language processing. One BERT variation, Sci-BERT [12], is pre-trained using a total of 1.14 million scientific papers in the domains of biomedicine ( $82 \%$ ) and computer science ( $12 \%$ ) and is better suited for processing texts that describe molecular properties. KV-PLM [40] further jointly models natural languages and SMILES of molecules with BERT. We employ Sci-BERT and KV-PLM as the initialized text encoder to extract the sentence feature sequences, respectively, followed by average pooling to obtain the sentence features.</p>
<p>Cross-modal Supervision. For the graph modal, a mini-batch of $N$ molecular graphs $\left{\mathcal{G}<em N="N">{1}, \ldots, \mathcal{G}</em>}\right}$ are processed by two different augmentations, resulting in $2 N$ augmented graphs. Subsequently, we feed these graphs into the graph encoder to obtain their representation vectors $\left{\boldsymbol{z<em 1="1">{1}^{G}, \tilde{\boldsymbol{z}}</em>}^{G}, \ldots, \boldsymbol{z<em N="N">{N}^{G}, \tilde{\boldsymbol{z}}</em>}^{G}\right}$, where $\boldsymbol{z<em i="i">{i}^{G}$ and $\tilde{\boldsymbol{z}}</em>}^{G}$ denote the representations of two augmented versions from the $i$-th graph $\mathcal{G<em i="i">{i}$. Meanwhile, the representations obtained by passing the two different sentences describing $\mathcal{G}</em>}$ through the text encoder are denoted as $\boldsymbol{z<em i="i">{i}^{T}, \tilde{\boldsymbol{z}}</em>}^{T}$. There are two different sentences corresponding to each molecular graph in a mini-batch, resulting in $2 N$ text representations $\left{\boldsymbol{z<em 1="1">{1}^{T}, \tilde{\boldsymbol{z}}</em>}^{T}, \ldots, \boldsymbol{z<em N="N">{N}^{T}, \tilde{\boldsymbol{z}}</em>}^{T}\right}$. Therefore, for the $i$-th graph $\mathcal{G<em i="i">{i}$, the total multi-view loss includes four contrastive losses between four paris of representations from multi-modals, i.e., $\left(\boldsymbol{z}</em>}^{G}, \boldsymbol{z<em i="i">{i}^{T}\right),\left(\tilde{\boldsymbol{z}}</em>}^{G}, \boldsymbol{z<em i="i">{i}^{T}\right)$, $\left(\boldsymbol{z}</em>}^{G}, \tilde{\boldsymbol{z}<em i="i">{i}^{T}\right)$ and $\left(\tilde{\boldsymbol{z}}</em>}^{G}, \tilde{\boldsymbol{z}<em i="i">{i}^{T}\right)$. For the sake of simplicity, we only denote the contrastive loss for $\left(\boldsymbol{z}</em>\right)$ as:}^{G}, \boldsymbol{z}_{i}^{T</p>
<p>$$
\ell_{i}^{\left(\boldsymbol{z}<em i="i">{i}^{G}, \boldsymbol{z}</em>}^{T}\right)}=-\log \frac{\exp \left(\operatorname{sim}\left(\boldsymbol{z<em i="i">{i}^{G}, \boldsymbol{z}</em>}^{T}\right) / \tau\right)}{\sum_{j=1}^{N} \exp \left(\operatorname{sim}\left(\boldsymbol{z<em j="j">{i}^{G}, \boldsymbol{z}</em>
$$}^{T}\right) / \tau\right)</p>
<p>where $\tau$ is the temperature parameter and $\operatorname{sim}\left(\boldsymbol{z}<em i="i">{i}^{G}, \boldsymbol{z}</em>}^{T}\right)$ first passes $\boldsymbol{z<em i="i">{i}^{G}$ and $\boldsymbol{z}</em>$ into two separate projection heads to project them into the same dimension, and then calculates the cosine similarity between the projected vectors. The other three cross-modal contrastive losses have the same form.}^{T</p>
<p>Graph-modal Self-Supervision. To enhance the representational power of the graph encoder further, we utilize contrastive learning within the graph modality. Specifically, we pull in the features of positive pairs while pushing those of negative pairs away by minimizing the normalized temperature-scaled cross-entropy loss $[46,74,75]$, where positive pairs are two augmentations of the same molecular graph and negative pairs come from different molecular graphs. Based on the previous definitions, we derive the graph-modal contrastive loss for the $i$-th graph as:</p>
<p>$$
\ell_{i}^{\left(\boldsymbol{z}<em i="i">{i}^{G}, \tilde{\boldsymbol{z}}</em>}^{G}\right)}=-\log \frac{\exp \left(\operatorname{sim}\left(\boldsymbol{z<em i="i">{i}^{G}, \tilde{\boldsymbol{z}}</em>}^{G}\right) / \tau\right)}{\sum_{j=1}^{N} \exp \left(\operatorname{sim}\left(\boldsymbol{z<em j="j">{i}^{G}, \tilde{\boldsymbol{z}}</em>
$$}^{G}\right) / \tau\right)</p>
<p>where $\tau$ is the temperature parameter and the final loss is computed across all samples in the mini-batch.</p>
<p>Implementation details. Following GraphCL [46], we use the GIN with 5 layers and a 300-dimensional hidden size as the graph encoder. We select the base BERT ${ }^{3}$, whose hidden size is 768 , for the text encoder. Graph features and sentence features are projected into the same feature space using two multi-layer perceptrons, each of whose output dimension is 256. Prior to the pre-training procedure, we initialize the GIN model using the GraphCL checkpoint ${ }^{4}$ and the BERT model using the checkpoints of Sci-BERT [12] or KV-PLM [40]. Then we pre-train the models using the dataset we have gathered, which includes pairs of molecular graphs and texts. For two graph augmentations, the node dropping ratio is $10 \%$ and the size of the sampled subgraph is $80 \%$ of the original graph. The input data for the text modal is two sentences randomly selected from the document of the corresponding graph data. We employ the AdamW optimizer with a learning rate of 0.0001 and a weight decay of 1e-5 for 300 epochs to pre-train our models. $\tau$ is set to 0.1 and the batch size is set to 256. The entire pre-training process is implemented with PyTorch [76] and conducted on eight NVIDIA Tesla V100 PCIe 32GB GPUs.</p>
<p>Formalization of zero-shot molecule generation. As shown in the Supplementary Figure 3, our zeroshot text-to-graph molecule generation method consists of the MoMu-based similarity-measuring module and the MoFlow-based molecule generator. MoFlow defines a parameterized invertible mapping flow from a Gaussian distribution to the distribution of molecues. A molecule graph $\mathcal{G}$ is a pair of an atom matrix $\boldsymbol{V} \in \mathbb{R}^{N \times C_{a}}$ and a bond matrix $\boldsymbol{E} \in \mathbb{R}^{N \times N \times C_{b}}$, where $N$ is the number of atoms in the molecule, $C_{a}$ and $C_{b}$ are the number of atom types and bond types. $\boldsymbol{V}<em c="c" n_="n,">{n, c}=1$ if the $n$-th atom belongs to the $c$-th atom type and $\boldsymbol{V}</em>}=0$ otherwise. $\boldsymbol{E<em n_="n," n_prime="n^{\prime">{n, n^{\prime}, c^{\prime}}=1$ if the bond between the $n$-th atom and the $n^{\prime}$-th atom belongs to the $c^{\prime}$-th bond type and $\boldsymbol{E}</em>}, c^{\prime}}=0$ otherwise. MoFlow contains a graph conditional flow $\boldsymbol{q<em c="c">{v}=f</em>}(\boldsymbol{V} \mid \boldsymbol{E})$ for encoding the atom matrix $\boldsymbol{V}$ given the bond matrix $\boldsymbol{E}$ into a latent variable $\boldsymbol{q<em e="e">{v}$ and a gflow $\boldsymbol{q}</em>}=f_{g}(\boldsymbol{E})$ for encoding the bond matrix $\boldsymbol{E}$ into a latent variable $\boldsymbol{q<em c="c">{e} . f</em>}$ and $f_{g}$ are implemented by graph coupling layerbased graph convolution neural networks. The concatenation $\boldsymbol{q}=\left[\boldsymbol{q<em e="e">{v} ; \boldsymbol{q}</em>}\right]$ of $\boldsymbol{q<em e="e">{v}$ and $\boldsymbol{q}</em>}$ follows a Gaussian distribution $P(\boldsymbol{q})$. Once MoFlow is trained, a variable $\boldsymbol{q}$ can be sampled from $P(\boldsymbol{q})$ and decomposed into two parts $\boldsymbol{q<em e="e">{v}$ and $\boldsymbol{q}</em>$ to obtain the probability matrices:}$, which are fed into the reverse graph conditional flow $f_{c}^{-1}$ and the reverse gflow $f_{g}^{-1</p>
<p>$$
\begin{gathered}
\hat{\boldsymbol{E}}=f_{g}^{-1}\left(\boldsymbol{q}<em b="b">{v}\right) \in \mathbb{R}^{N \times N \times C</em> \
\hat{\boldsymbol{V}}=f_{c}^{-1}\left(\boldsymbol{q}}<em a="a">{v} \mid G N(\hat{\boldsymbol{E}})\right) \in \mathbb{R}^{N \times C</em>
\end{gathered}
$$}</p>
<p>where $\hat{\boldsymbol{E}}<em c="c" n_="n,">{n, n^{\prime}, c^{\prime}}$ is the predicted probability that the bond between the $n$-th atom and the $n^{\prime}$-th atom belongs to the $c^{\prime}$-th bond type, and $\hat{\boldsymbol{V}}</em>)$, MoFlow can generate different novel and valid molecules.}$ is the probability that $n$-th atom belongs to the $c$-th atom type. $\hat{\boldsymbol{V}}$ and $\boldsymbol{E}$ can be obtained from $\hat{\boldsymbol{V}}$ and $\hat{\boldsymbol{E}}$ by performing the argmax operation to the last dimension, respectively. $G N$ is the graph normalization layer in [35]. By sampling different $\boldsymbol{q}$ from $P(\boldsymbol{q</p>
<p>Our zero-shot text-to-graph molecule generation method takes a text description $\boldsymbol{x}^{T}$ as input. The only learnable parameter in our method is $\boldsymbol{q}$ which is initialized by randomly sampling from $P(\boldsymbol{q})$. All parameters of the pre-trained MoMu and MoFlow are freezed. The input $\boldsymbol{x}^{T}$ is fed into the text encoder of MoMu to obtain the text representation $\boldsymbol{z}^{T} . \boldsymbol{q}$ is fed into MoFlow to obtain $\hat{\boldsymbol{V}}$ and $\hat{\boldsymbol{E}}$. To make all operations differentiable and thus allow gradient backpropagation, we fed $\hat{\boldsymbol{V}}$ instead of $\boldsymbol{V}$ into the graph encoder of MoMu to obtain the graph representation $\boldsymbol{z}^{G}$. The trained graph encoder, GIN, contains embeddings for all atom and bond types. Originally, $\boldsymbol{V}$ is used as the indicator to select the corresponding embeddings with respect to the atom types in the first layer. When $\hat{\boldsymbol{V}}$ is used, for each atom, the representation is actually the weighted sum of all atom embeddings. The probabilities among atom types per node in $\hat{\boldsymbol{V}}$ serve as attention scores. The loss function is the cosine similarity between the projected $\boldsymbol{z}^{T}$ and $\boldsymbol{z}^{G}$ :</p>
<p>$$
\ell_{q}=-\operatorname{sim}\left(\boldsymbol{z}^{G}, \boldsymbol{z}^{T}\right) / \tau
$$</p>
<p>where $\operatorname{sim}\left(\boldsymbol{z}^{G}, \boldsymbol{z}^{T}\right)$ is the module for calculating the similarity between the projected representations in MoMu. $\boldsymbol{q}$ can be updated through gradient backpropagation with respect to $\ell_{q}$. We use the Adam optimizer for updating. After repeating the update for a maximize of 500 iterations, we obtain the optimized $\boldsymbol{q}^{*}$, which is then fed into MoFlow to obtain $\hat{\boldsymbol{V}}$ and $\hat{\boldsymbol{E}}$. Finally, the molecule graph $(\mathcal{G}=\boldsymbol{V}, \boldsymbol{E})$ is obtained by performing the argmax operations to the last dimension of $\hat{\boldsymbol{V}}$ and $\hat{\boldsymbol{E}}$. The pipeline is summarized in Algorithm 1.</p>
<h1>Data availability</h1>
<p>For collecting the pretraining data, the PubChem dataset is available at https://pubchem.ncbi.nlm. nih.gov/, and the S2orc dataset is available at https://github.com/allenai/s2orc. Our col-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Algorithm 1 Zero-shot text-to-graph molecule generation.
Input: The pretrained graph encoder $f^{G}$, text encoder $f^{T}$, and similarity calculation module sim of MoMu; The pretrained graph conditional flow network $f_{c}$, the gflow network $f_{g}$, and Gaussian distribution $P(\boldsymbol{q})$ of MoFlow; The input text description $\boldsymbol{x}^{T}$; Hyper-parameters: the learning rate $l_{r}$, the maximum number of iteration $T_{m}$, the temperature $\tau$.
Initialize The learnable parameter $\boldsymbol{q}$ randomly sampled from $P(\boldsymbol{q})$.
Obtain the text representation $\boldsymbol{z}^{T}=f^{T}\left(\boldsymbol{x}^{T}\right)$
for iteration = $1, \cdots, T_{m}$ do
Separate $\boldsymbol{q}$ into $\boldsymbol{q}<em e="e">{v}$ and $\boldsymbol{q}</em>$
$\hat{\boldsymbol{E}}=f_{g}^{-1}\left(\boldsymbol{q}<em c="c">{e}\right)$
$\boldsymbol{E}=\operatorname{argmax}(\hat{\boldsymbol{E}})$
$\hat{\boldsymbol{V}}=f</em>}^{-1}\left(\boldsymbol{q<em q="q">{v} \mid G N(\hat{\boldsymbol{E}})\right)$
Obtain the graph representation $\boldsymbol{z}^{G}=f^{G}(\hat{\boldsymbol{V}}, \boldsymbol{E})$
Calculate the loss $\ell</em>\right) / \tau$
Update $\boldsymbol{q}$ by the Adam optimizer with $l_{r}$
end for
Separate $\boldsymbol{q}$ into $\boldsymbol{q}}=-\operatorname{sim}\left(\boldsymbol{z}^{G}, \boldsymbol{z}^{T<em e="e">{v}$ and $\boldsymbol{q}</em>$
$\hat{\boldsymbol{E}}=f_{g}^{-1}\left(\boldsymbol{q}<em c="c">{e}\right)$
$\boldsymbol{E}=\operatorname{argmax}(\hat{\boldsymbol{E}})$
$\hat{\boldsymbol{V}}=f</em>)\right)$
$\boldsymbol{V}=\operatorname{argmax}(\hat{\boldsymbol{V}})$
Return the generated molecule graph $\mathcal{G}=\boldsymbol{V}, \boldsymbol{E}$
lected dataset consists of two folders holding molecular graphs and texts, respectively. The dataset can be downloaded in https://pan.baidu.com/s/1aHJoYTTZWDHPCcRuu9I7Fg. For cross-modality retrieval, the PCdes dataset is available at https://github.com/thunlp/KV-PLM. For molecule caption, the ChEBI-20 dataset is available at https://github.com/cnedwards/text2mol. For text-to-molecule generation, the pre-trained MoFlow is available at https://github.com/calvin-zcx/ moflow. For molecule property prediction, the eight datasets from MoleculeNet are available at https: //github.com/deepchem/deepchem/tree/master/datasets and the processed datasets are available at: http://snap.stanford.edu/gnn-pretrain/data/chem_dataset.zip.}^{-1}\left(\boldsymbol{q}_{v} \mid G N(\hat{\boldsymbol{E}</p>
<h1>Code availability</h1>
<p>The code for training the proposed molecular multimodal foundation model and employing the trained model in molecule caption, zero-shot text-to-graph generation, and molecule property prediction will be available at https://github.com/BingSu12/MoMu. The code for data collection and cross-modality retrieval will be available at https://github.com/yangzhao1230/GraphTextRetrieval.</p>
<h2>References</h2>
<p>[1] Philip J Hajduk and Jonathan Greer. A decade of fragment-based drug design: strategic advances and lessons learned. Nature reviews Drug discovery, 6(3):211-219, 2007.
[2] Matthew A Clark, Raksha A Acharya, Christopher C Arico-Muendel, Svetlana L Belyanskaya, Dennis R Benjamin, Neil R Carlson, Paolo A Centrella, Cynthia H Chiu, Steffen P Creaser, John W Cuozzo, et al. Design, synthesis and selection of dna-encoded small-molecule libraries. Nature chemical biology, 5(9):647-654, 2009.
[3] Tiago Rodrigues, Daniel Reker, Petra Schneider, and Gisbert Schneider. Counting on natural products for drug design. Nature chemistry, 8(6):531-541, 2016.
[4] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436-444, 2015.
[5] Shuaihua Lu, Qionghua Zhou, Yixin Ouyang, Yilv Guo, Qiang Li, and Jinlan Wang. Accelerated discovery of stable lead-free hybrid organic-inorganic perovskites via machine learning. Nature communications, 9(1):1-8, 2018.
[6] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Multi-objective molecule generation using interpretable substructures. In International conference on machine learning, pages 4849-4859. PMLR, 2020.
[7] W Patrick Walters and Regina Barzilay. Applications of deep learning in molecule generation and molecular property prediction. Accounts of chemical research, 54(2):263-270, 2020.</p>
<p>[8] Panagiotis-Christos Kotsias, Josep Arús-Pous, Hongming Chen, Ola Engkvist, Christian Tyrchan, and Esben Jannik Bjerrum. Direct steering of de novo molecular generation with descriptor conditional recurrent neural networks. Nature Machine Intelligence, 2(5):254-265, 2020.
[9] Omar Mahmood, Elman Mansimov, Richard Bonneau, and Kyunghyun Cho. Masked graph modeling for molecule generation. Nature communications, 12(1):1-12, 2021.
[10] Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, and Junzhou Huang. Smiles-bert: large scale unsupervised pre-training for molecular property prediction. In Proceedings of the 10th ACM international conference on bioinformatics, computational biology and health informatics, pages 429-436, 2019.
[11] Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. Chemberta: large-scale self-supervised pretraining for molecular property prediction. arXiv preprint arXiv:2010.09885, 2020.
[12] Iz Beltagy, Kyle Lo, and Arman Cohan. Scibert: A pretrained language model for scientific text. pages 3615-3620, 2019.
[13] Diya Li, Lifu Huang, Heng Ji, and Jiawei Han. Biomedical event extraction based on knowledge-driven treeIstm. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1421-1430, 2019.
[14] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):12341240, 2020.
[15] Matt J Kusner, Brooks Paige, and José Miguel Hernández-Lobato. Grammar variational autoencoder. In International conference on machine learning, pages 1945-1954. PMLR, 2017.
[16] Hanjun Dai, Yingtao Tian, Bo Dai, Steven Skiena, and Le Song. Syntax-directed variational autoencoder for structured data. In International Conference on Learning Representations, 2018.
[17] Marwin HS Segler, Thierry Kogej, Christian Tyrchan, and Mark P Waller. Generating focused molecule libraries for drug discovery with recurrent neural networks. ACS central science, 4(1):120-131, 2018.
[18] Rafael Gómez-Bombarelli, Jennifer N Wei, David Duvenaud, José Miguel Hernández-Lobato, Benjamín SánchezLengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Alán AspuruGuzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS central science, $4(2): 268-276,2018$.
[19] Shion Honda, Shoi Shi, and Hiroki R Ueda. Smiles transformer: Pre-trained molecular fingerprint for low data drug discovery. arXiv preprint arXiv:1911.04738, 2019.
[20] Mariya Popova, Mykhailo Shvets, Junier Oliva, and Olexandr Isayev. Molecularrnn: Generating realistic molecular graphs with optimized properties. arXiv preprint arXiv:1905.13372, 2019.
[21] Jike Wang, Chang-Yu Hsieh, Mingyang Wang, Xiaorui Wang, Zhenxing Wu, Dejun Jiang, Benben Liao, Xujun Zhang, Bo Yang, Qiaojun He, et al. Multi-constraint molecular generation based on conditional transformer, knowledge distillation and reinforcement learning. Nature Machine Intelligence, 3(10):914-922, 2021.
[22] Jeff Guo, Vendy Fialková, Juan Diego Arango, Christian Margreitter, Jon Paul Janet, Kostas Papadopoulos, Ola Engkvist, and Atanas Patronov. Improving de novo molecular design with curriculum learning. Nature Machine Intelligence, pages 1-9, 2022.
[23] Daniel Flam-Shepherd, Kevin Zhu, and Alán Aspuru-Guzik. Language models can learn complex molecular distributions. Nature Communications, 13(1):1-10, 2022.
[24] Samuel C Hoffman, Vijil Chenthamarakshan, Kahini Wadhawan, Pin-Yu Chen, and Payel Das. Optimizing molecules using efficient queries from property evaluations. Nature Machine Intelligence, 4(1):21-31, 2022.
[25] Truong Son Hy, Shubhendu Trivedi, Horace Pan, Brandon M Anderson, and Risi Kondor. Predicting molecular properties with covariant compositional networks. The Journal of chemical physics, 148(24):241745, 2018.
[26] Connor W Coley, Wengong Jin, Luke Rogers, Timothy F Jamison, Tommi S Jaakkola, William H Green, Regina Barzilay, and Klavs F Jensen. A graph-convolutional neural network model for the prediction of chemical reactivity. Chemical science, 10(2):370-377, 2019.
[27] Sheheryar Zaidi, Michael Schaarschmidt, James Martens, Hyunjik Kim, Yee Whye Teh, Alvaro Sanchez-Gonzalez, Peter Battaglia, Razvan Pascanu, and Jonathan Godwin. Pre-training via denoising for molecular property prediction. arXiv preprint arXiv:2206.00133, 2022.</p>
<p>[28] Yuquan Li, Chang-Yu Hsieh, Ruiqiang Lu, Xiaoqing Gong, Xiaorui Wang, Shuo Liu, Yanan Tian, Dejun Jiang, Jiaxian Yan, Qifeng Bai, et al. Glam: An adaptive graph learning method for automated molecular interactions and properties predictions. Nature Machine Intelligence, 2022.
[29] Yuyang Wang, Jianren Wang, Zhonglin Cao, and Amir Barati Farimani. Molecular contrastive learning of representations via graph neural networks. Nature Machine Intelligence, 2022.
[30] Hongwei Wang, Weijiang Li, Xiaomeng Jin, Kyunghyun Cho, Heng Ji, Jiawei Han, and Martin Burke. Chemical-reaction-aware molecule representation learning. In International Conference on Learning Representations, 2021.
[31] Dong Chen, Kaifu Gao, Duc Duy Nguyen, Xin Chen, Yi Jiang, Guo-Wei Wei, and Feng Pan. Algebraic graphassisted bidirectional transformers for molecular property prediction. Nature Communications, 12(1):1-9, 2021.
[32] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pages 1263-1272. PMLR, 2017.
[33] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for molecular graph generation. In International conference on machine learning, pages 2323-2332. PMLR, 2018.
[34] Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang. Graphaf: a flow-based autoregressive model for molecular graph generation. In International Conference on Learning Representations, 2019.
[35] Chengxi Zang and Fei Wang. Moflow: an invertible flow model for generating molecular graphs. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining, pages 617-626, 2020.
[36] Changsheng Ma and Xiangliang Zhang. Gf-vae: A flow-based variational autoencoder for molecule generation. In Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management, pages 11811190, 2021.
[37] Youzhi Luo, Keqiang Yan, and Shuiwang Ji. Graphdf: A discrete flow model for molecular graph generation. In International Conference on Machine Learning, pages 7192-7203. PMLR, 2021.
[38] Ziqi Chen, Martin Renqiang Min, Srinivasan Parthasarathy, and Xia Ning. A deep generative model for molecule optimization via one fragment modification. Nature Machine Intelligence, 3(12):1040-1049, 2021.
[39] Myeonghun Lee and Kyoungmin Min. Mgcvae: Multi-objective inverse design via molecular graph conditional variational autoencoder. Journal of Chemical Information and Modeling, 2022.
[40] Zheni Zeng, Yuan Yao, Zhiyuan Liu, and Maosong Sun. A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals. Nature communications, 13(1):1-11, 2022.
[41] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Michael Kinney, and Daniel S. Weld. S2orc: The semantic scholar open research corpus. In ACL, 2020.
[42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748-8763. PMLR, 2021.
[43] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pages 4904-4916. PMLR, 2021.
[44] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision, pages 121-137. Springer, 2020.
[45] Nanyi Fei, Zhiwu Lu, Yizhao Gao, Guoxing Yang, Yuqi Huo, Jingyuan Wen, Haoyu Lu, Ruihua Song, Xin Gao, Tao Xiang, et al. Towards artificial general intelligence via a multimodal foundation model. Nature Communications, 13(1):1-13, 2022.
[46] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. Advances in Neural Information Processing Systems, 33:5812-5823, 2020.
[47] Yanli Wang, Jewen Xiao, Tugba O Suzek, Jian Zhang, Jiyao Wang, and Stephen H Bryant. Pubchem: a public information system for analyzing bioactivities of small molecules. Nucleic acids research, 37(suppl.2):W623W633, 2009.
[48] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems, 33:22118-22133, 2020.</p>
<p>[49] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations, 2018.
[50] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. pages 4171-4186, 2019.
[51] Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and Junjie Yan. Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm. In International Conference on Learning Representations, 2021.
[52] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 1597-1607. PMLR, 2020.
[53] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729-9738, 2020.
[54] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. In International Conference on Learning Representations, 2018.
[55] Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, and Heng Ji. Translation between molecules and natural language. arXiv preprint arXiv:2204.11817, 2022.
[56] Carl Edwards, ChengXiang Zhai, and Heng Ji. Text2mol: Cross-modal molecule retrieval with natural language queries. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 595-607, 2021.
[57] John J Irwin, Teague Sterling, Michael M Mysinger, Erin S Bolstad, and Ryan G Coleman. Zinc: a free tool to discover chemistry for biology. Journal of chemical information and modeling, 52(7):1757-1768, 2012.
[58] Susheel Suresh, Pan Li, Cong Hao, and Jennifer Neville. Adversarial graph augmentation to improve graph contrastive learning. Advances in Neural Information Processing Systems, 34:15920-15933, 2021.
[59] Hang Gao, Jiangmeng Li, Wenwen Qiang, Lingyu Si, Changwen Zheng, and Fuchun Sun. Bootstrapping informative graph augmentation via a meta learning approach. arXiv preprint arXiv:2201.03812, 2022.
[60] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. In International Conference on Learning Representations, 2020.
[61] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chemical science, 9(2):513530, 2018.
[62] Ines Filipa Martins, Ana L Teixeira, Luis Pinheiro, and Andre O Falcao. A bayesian approach to in silico bloodbrain barrier penetration modeling. Journal of chemical information and modeling, 52(6):1686-1697, 2012.
[63] Tox21 data challenge 2014. URL https://tripod.nih.gov/tox21/challenge/, 2014.
[64] Ann M Richard, Richard S Judson, Keith A Houck, Christopher M Grulke, Patra Volarath, Inthirany Thillainadarajah, Chihae Yang, James Rathman, Matthew T Martin, John F Wambaugh, et al. Toxcast chemical landscape: paving the road to 21st century toxicology. Chemical research in toxicology, 29(8):1225-1251, 2016.
[65] Michael Kuhn, Ivica Letunic, Lars Juhl Jensen, and Peer Bork. The sider database of drugs and side effects. Nucleic acids research, 44(D1):D1075-D1079, 2016.
[66] Paul A Novick, Oscar F Ortiz, Jared Poelman, Amir Y Abdulhay, and Vijay S Pande. Sweetlead: an in silico database of approved drugs, regulated chemicals, and herbal isolates for computer-aided drug discovery. PloS one, 8(11):e79568, 2013.
[67] Aact database. URL https://www.ctti-clinicaltrials.org/aact-database, 2017.
[68] Eleanor J Gardiner, Caroline Holliday, John D andØD owd, and Peter Willett. Effectiveness of 2d fingerprints for scaffold hopping. Future medicinal chemistry, 3(4):405-414, 2011.
[69] Aids antiviral screen data. URL https://wiki.nci.nih.gov/display/NCIDTPdata/AIDS+Antiviral+Screen+Data.
[70] Govindan Subramanian, Bharath Ramsundar, Vijay Pande, and Rajiah Aldrin Denny. Computational modeling of $\beta$-secretase 1 (bace-1) inhibitors using ligand based approaches. Journal of chemical information and modeling, 56(10):1936-1949, 2016.</p>
<p>[71] Petar Velickovic, William Fedus, William L Hamilton, Pietro Liò, Yoshua Bengio, and R Devon Hjelm. Deep graph infomax. In International Conference on Learning Representations, 2019.
[72] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. Advances in neural information processing systems, 30, 2017.
[73] Van Der Maaten Laurens and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, $9(2605): 2579-2605,2008$.
[74] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages $3733-3742,2018$.
[75] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.
[76] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems, pages 8026-8037, 2019.</p>
<h1>Author contributions</h1>
<p>Ji-Rong Wen managed the study. Bing Su, Dazhao Du, and Ji-Rong Wen designed the methodology of the study. Zhao Yang and Yujie Zhou collected the paired dataset. Dazhao Du performed the pretraining of the model. Dazhao Du, Zhao Yang, Bing Su, and Jiangmeng Li performed experiments on down-stream tasks. Bing Su, Dazhao Du, and Zhao Yang wrote the original draft. All co-authors discussed the results, performed analyses, revised and reviewed the manuscript.</p>
<h2>Corresponding author</h2>
<p>Correspondence to Ji-Rong Wen (jrwen@ruc.edu.cn).</p>
<h2>Competing interests statement</h2>
<p>The authors declare no competing interests.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://huggingface.co/bert-base-uncased.
${ }^{4}$ https://github.com/Shen-Lab/GraphCL/tree/master/transferLearning_MoleculeNet_PPI/chem/ models_graphcl&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
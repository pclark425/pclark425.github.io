<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5437 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5437</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5437</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365" target="_blank">Enabling Large Language Models to Generate Text with Citations</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work proposes ALCE, the first benchmark for Automatic LLMs' Citation Evaluation, and develops automatic metrics along three dimensions -- fluency, correctness, and citation quality -- and demonstrates their strong correlation with human judgements.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, our aim is to allow LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare different modeling approaches. We propose ALCE, the first benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We develop automatic metrics along three dimensions -- fluency, correctness, and citation quality -- and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvement -- For example, on the ELI5 dataset, even the best models lack complete citation support 50% of the time. Our analyses further highlight promising future directions, including developing better retrievers, advancing long-context LLMs, and improving the ability to synthesize information from multiple sources.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5437.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5437.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RERANK</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sample-and-rerank using automatic citation-recall</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A post-generation selection method: generate multiple candidate answers (n_sample=4) and choose the best response according to the automatic citation-recall metric (ALCE's NLI-based citation recall). Used to improve citation quality by selecting the candidate best-supported by retrieved passages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned GPT-3.5 Chat model used with a 4K context window; sampling temperature 0.5 in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>RERANK (sample-and-select)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate n_sample = 4 independent answers for each question; evaluate each candidate with the automatic citation-recall metric (using the NLI model TRUE on cited passages) and select the candidate with the highest automated citation-recall score as the final output.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ASQA, QAMPARI, ELI5 (ALCE benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Long-form and list-style open-domain QA requiring retrieved evidence and citations; ALCE evaluates fluency, correctness, and citation quality over ASQA (ambiguous long-form), QAMPARI (entity lists), and ELI5 (long-form explanations).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>ASQA (ChatGPT): citation recall 84.8%, citation precision 81.6%, fluency (MAUVE) 77.0, correctness (EM recall) 40.2. QAMPARI (ChatGPT): Rec-5 22.8%, citation recall 21.2%, citation precision 21.4%. ELI5 (ChatGPT): citation recall 69.3%, citation precision 67.8%, fluency 56.1, claim recall 11.4.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>ASQA (ChatGPT VANILLA): citation recall 73.6%, citation precision 72.5%, fluency 66.6, correctness 40.4. QAMPARI VANILLA: Rec-5 20.8%, citation recall 20.5%, citation precision 20.9%. ELI5 VANILLA: citation recall 51.1%, citation precision 50.0%, fluency 57.2, claim recall 12.0.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Automated metrics show consistent increases in citation recall and precision when applying RERANK versus VANILLA (e.g., ASQA citation recall increased from 73.6% to 84.8% for ChatGPT; ELI5 citation recall from 51.1% to 69.3%). Human evaluation corroborates gains: human-annotated citation recall for ASQA increased (Vanilla 74.7% → Rerank 79.3%), and for ELI5 (Vanilla 50.8% → Rerank 59.7%). The paper reports consistent improvement on citation quality across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>RERANK relies on the same automatic citation metric used to select candidates, which can bias selection toward the metric's weaknesses; authors note potential bias but also show human agreement. RERANK increases compute cost (multiple generations). Improvements are mainly in citation quality — correctness (coverage) often changes little. Reported gains vary by dataset and model (smaller gains on QAMPARI).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enabling Large Language Models to Generate Text with Citations', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5437.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5437.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>POSTCITE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Post-hoc citation assignment (PostCite)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A post-processing method that, after generating an answer without retrieval, finds the best-matching retrieved passage(s) for each statement and attaches citations (using GTR to match passages).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (closed-book configuration)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ChatGPT prompted without any retrieved passages (closed-book generation); in evaluation the model's output is post-processed by searching top-100 retrieved passages with GTR to assign citations.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>PostCite (post-hoc citation matching)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate an answer in closed-book mode (no retrieved context); for each statement, search the top-100 retrieved passages using GTR to find the best matching passage(s) and attach those passages as citations to the statements.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ASQA, QAMPARI, ELI5 (ALCE benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same ALCE tasks: response generation with supporting citations from a large corpus, evaluated for fluency, correctness, and citation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported results include closed-book with PostCite (the paper treats ClosedBook as using PostCite): ASQA (ChatGPT CLOSEDBOOK+POSTCITE): fluency MAUVE 52.7, correctness (EM recall) 38.3, citation recall 26.7%, citation precision 26.7%. QAMPARI CLOSEDBOOK+POSTCITE: Rec-5 32.9%, citation recall 10.0%. ELI5 CLOSEDBOOK+POSTCITE: claim recall 18.6%, citation recall 15.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Compared to VANILLA (open-book with passages in context): ASQA VANILLA citation recall 73.6% and correctness 40.4; QAMPARI VANILLA Rec-5 20.8% and citation recall 20.5%; ELI5 VANILLA claim recall 12.0% and citation recall 51.1%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>PostCite (closed-book+post-cite) sometimes yields higher perceived correctness (e.g., ClosedBook correctness > VANILLA on QAMPARI and ELI5 in some cases), indicating closed-book generation can produce answers matching ground truth. However, automated citation metrics reveal post-hoc citations poorly support those answers (substantially lower citation recall and precision compared to VANILLA).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>PostCite often fails because closed-book generations are not phrased similarly to retrieved passages, making it hard to find matching supporting passages; authors report large drops in citation recall and precision (e.g., ASQA citation recall lower by ~47% compared to VANILLA). Thus post-hoc citation does not reliably make outputs verifiable; open-book approaches remain superior for citation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enabling Large Language Models to Generate Text with Citations', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5437.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5437.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>INTERACT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>INTERACT (interactive checking of full passages)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interactive prompting scheme that lets the model request to 'check' the full texts of specified retrieved passages during generation, then produce output statements, enabling a stepwise generate-and-check workflow within the same session.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-following ChatGPT with 4K context window; used with summarized/snippet representations of many retrieved passages and allowed to 'check' up to 3 full passages during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>INTERACT (generate/check/output loop)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Expose many (summarized) retrieved passages in context; the model can issue actions at each step: (1) 'Check: Document [ids]' to fetch and read the full document text for those ids, (2) 'Output:' to write a statement citing passages, and (3) 'End.' to finish. Full passages are removed after being checked to save context. The system forces an 'Output' after each 'Check' to avoid indefinite checking.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ASQA, ELI5 (ALCE benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same citation-generation tasks; INTERACT aims to let the model inspect full texts selectively to improve faithfulness while keeping many summaries in context.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>ASQA (ChatGPT, SUMM 10-psg w/ INTERACT): fluency MAUVE 69.0, correctness (EM) 39.1, citation recall 73.4%, citation precision 66.5% (compare to SUMM-only below). ELI5 (ChatGPT w/ INTERACT): fluency 68.0, claim recall 13.3, citation recall 47.8%, precision 45.0.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>ASQA (ChatGPT, SUMM 10-psg, no INTERACT): fluency 70.0, correctness 43.3, citation recall 68.9%, citation precision 61.8. ELI5 VANILLA (no INTERACT): citation recall 51.1%, claim recall 12.0.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Mixed: INTERACT increased citation recall for ASQA when added to SUMM (68.9% → 73.4%), suggesting selective checking can help support statements; however, correctness dropped (43.3% → 39.1%). Across datasets the paper concludes interactive retrieval approaches do not reliably improve overall performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Authors observe models are not yet proficient at interactive usage: INTERACT sometimes reduced correctness and required engineering (forcing 'Output' after checks). Checking full passages gave limited benefit in many cases; interactive methods did not broadly outperform simpler VANILLA prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enabling Large Language Models to Generate Text with Citations', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5437.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5437.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>INLINESEARCH</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>INLINESEARCH (search-on-the-fly during generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generation-time retrieval interface: the model can call a 'Search: {query}' action during generation to retrieve the best passage from the top-100 retrieval set and immediately use it (then the passage is removed to save context).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ChatGPT with ability to perform retrieval actions during generation over the pre-retrieved top-100 passages (GTR used for ranking returned passage per search action).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>INLINESEARCH (on-the-fly retrieval + output actions)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>During generation, the model can emit 'Search: {query}' which triggers a search among top-100 passages (GTR) and places the best-matching passage in context; model can then 'Output:' statements citing the returned passage and continue searching as needed.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ASQA, ELI5 (ALCE)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Allows the model to decide when and what to retrieve during multi-step answer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>ASQA (ChatGPT INLINESEARCH): fluency MAUVE 58.7, correctness (EM) 32.4, citation recall 58.3%, citation precision 58.2% (worse than VANILLA). ELI5 INLINESEARCH: citation recall 45.6%, claim recall ~13.4, fluency 49.7.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>VANILLA (ChatGPT 5-psg) ASQA: fluency 66.6, correctness 40.4, citation recall 73.6%, precision 72.5. ELI5 VANILLA citation recall 51.1%, claim recall 12.0.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>INLINESEARCH performed worse than VANILLA on both citation quality and correctness in the reported experiments; the authors report that retrieving text on the fly did not improve performance and often degraded citation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Authors find it hard for the model to formulate effective intermediate queries without seeing passage content; INLINESEARCH performed worse possibly because the model cannot easily craft detailed retrieval queries from the question alone. Additional context or improved querying heuristics may be required.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enabling Large Language Models to Generate Text with Citations', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5437.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5437.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generate-then-revise (RARR / related)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generate-then-revise approaches (e.g., RARR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that first produce an initial answer without retrieval, then retrieve relevant documents and revise the answer to be consistent with evidence (sometimes using LMs to perform the revision). Mentioned in related work but not implemented in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RARR: Researching and revising what language models say, using language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified language model(s) in referenced work</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced works (e.g., Gao et al., 2023) empirically use large LMs to both generate initial outputs and to conduct subsequent retrieval-and-revision steps; exact model variants depend on those papers.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Generate-then-revise / iterative retrieval-based revision</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>First generate a draft answer without external documents; then retrieve supporting documents relevant to the draft and instruct the model (or another model) to revise the draft to be consistent with retrieved evidence (a generate-then-reflect/revise pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General factual generation and verification (mentioned in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used for improving faithfulness of LM outputs by aligning them to external evidence via revision steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Paper cites Gao et al. (2023) and He et al. (2022) as proposing generate-then-revise pipelines; ALCE paper itself did not implement these and therefore provides no in-paper empirical comparison, but references suggest this line of work aims to improve factuality by revision.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>ALCE authors note these approaches are in prior work and did not evaluate them in this benchmark; they emphasize that training models to incorporate citations remains challenging due to lack of supervised data. No direct failure cases are reported here for those methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enabling Large Language Models to Generate Text with Citations', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>RARR: Researching and revising what language models say, using language models. <em>(Rating: 2)</em></li>
                <li>Rethinking with retrieval: Faithful large language model inference. <em>(Rating: 2)</em></li>
                <li>WebGPT: Browser-assisted question-answering with human feedback. <em>(Rating: 2)</em></li>
                <li>Teaching language models to support answers with verified quotes. <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing reasoning and acting in language models. <em>(Rating: 1)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5437",
    "paper_id": "paper-e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "RERANK",
            "name_full": "Sample-and-rerank using automatic citation-recall",
            "brief_description": "A post-generation selection method: generate multiple candidate answers (n_sample=4) and choose the best response according to the automatic citation-recall metric (ALCE's NLI-based citation recall). Used to improve citation quality by selecting the candidate best-supported by retrieved passages.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo-0301)",
            "model_description": "Instruction-tuned GPT-3.5 Chat model used with a 4K context window; sampling temperature 0.5 in these experiments.",
            "reflection_method_name": "RERANK (sample-and-select)",
            "reflection_method_description": "Generate n_sample = 4 independent answers for each question; evaluate each candidate with the automatic citation-recall metric (using the NLI model TRUE on cited passages) and select the candidate with the highest automated citation-recall score as the final output.",
            "num_iterations": 4,
            "task_name": "ASQA, QAMPARI, ELI5 (ALCE benchmark)",
            "task_description": "Long-form and list-style open-domain QA requiring retrieved evidence and citations; ALCE evaluates fluency, correctness, and citation quality over ASQA (ambiguous long-form), QAMPARI (entity lists), and ELI5 (long-form explanations).",
            "performance_with_reflection": "ASQA (ChatGPT): citation recall 84.8%, citation precision 81.6%, fluency (MAUVE) 77.0, correctness (EM recall) 40.2. QAMPARI (ChatGPT): Rec-5 22.8%, citation recall 21.2%, citation precision 21.4%. ELI5 (ChatGPT): citation recall 69.3%, citation precision 67.8%, fluency 56.1, claim recall 11.4.",
            "performance_without_reflection": "ASQA (ChatGPT VANILLA): citation recall 73.6%, citation precision 72.5%, fluency 66.6, correctness 40.4. QAMPARI VANILLA: Rec-5 20.8%, citation recall 20.5%, citation precision 20.9%. ELI5 VANILLA: citation recall 51.1%, citation precision 50.0%, fluency 57.2, claim recall 12.0.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Automated metrics show consistent increases in citation recall and precision when applying RERANK versus VANILLA (e.g., ASQA citation recall increased from 73.6% to 84.8% for ChatGPT; ELI5 citation recall from 51.1% to 69.3%). Human evaluation corroborates gains: human-annotated citation recall for ASQA increased (Vanilla 74.7% → Rerank 79.3%), and for ELI5 (Vanilla 50.8% → Rerank 59.7%). The paper reports consistent improvement on citation quality across datasets.",
            "limitations_or_failure_cases": "RERANK relies on the same automatic citation metric used to select candidates, which can bias selection toward the metric's weaknesses; authors note potential bias but also show human agreement. RERANK increases compute cost (multiple generations). Improvements are mainly in citation quality — correctness (coverage) often changes little. Reported gains vary by dataset and model (smaller gains on QAMPARI).",
            "uuid": "e5437.0",
            "source_info": {
                "paper_title": "Enabling Large Language Models to Generate Text with Citations",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "POSTCITE",
            "name_full": "Post-hoc citation assignment (PostCite)",
            "brief_description": "A post-processing method that, after generating an answer without retrieval, finds the best-matching retrieved passage(s) for each statement and attaches citations (using GTR to match passages).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (closed-book configuration)",
            "model_description": "ChatGPT prompted without any retrieved passages (closed-book generation); in evaluation the model's output is post-processed by searching top-100 retrieved passages with GTR to assign citations.",
            "reflection_method_name": "PostCite (post-hoc citation matching)",
            "reflection_method_description": "Generate an answer in closed-book mode (no retrieved context); for each statement, search the top-100 retrieved passages using GTR to find the best matching passage(s) and attach those passages as citations to the statements.",
            "num_iterations": null,
            "task_name": "ASQA, QAMPARI, ELI5 (ALCE benchmark)",
            "task_description": "Same ALCE tasks: response generation with supporting citations from a large corpus, evaluated for fluency, correctness, and citation quality.",
            "performance_with_reflection": "Reported results include closed-book with PostCite (the paper treats ClosedBook as using PostCite): ASQA (ChatGPT CLOSEDBOOK+POSTCITE): fluency MAUVE 52.7, correctness (EM recall) 38.3, citation recall 26.7%, citation precision 26.7%. QAMPARI CLOSEDBOOK+POSTCITE: Rec-5 32.9%, citation recall 10.0%. ELI5 CLOSEDBOOK+POSTCITE: claim recall 18.6%, citation recall 15.5%.",
            "performance_without_reflection": "Compared to VANILLA (open-book with passages in context): ASQA VANILLA citation recall 73.6% and correctness 40.4; QAMPARI VANILLA Rec-5 20.8% and citation recall 20.5%; ELI5 VANILLA claim recall 12.0% and citation recall 51.1%.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "PostCite (closed-book+post-cite) sometimes yields higher perceived correctness (e.g., ClosedBook correctness &gt; VANILLA on QAMPARI and ELI5 in some cases), indicating closed-book generation can produce answers matching ground truth. However, automated citation metrics reveal post-hoc citations poorly support those answers (substantially lower citation recall and precision compared to VANILLA).",
            "limitations_or_failure_cases": "PostCite often fails because closed-book generations are not phrased similarly to retrieved passages, making it hard to find matching supporting passages; authors report large drops in citation recall and precision (e.g., ASQA citation recall lower by ~47% compared to VANILLA). Thus post-hoc citation does not reliably make outputs verifiable; open-book approaches remain superior for citation quality.",
            "uuid": "e5437.1",
            "source_info": {
                "paper_title": "Enabling Large Language Models to Generate Text with Citations",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "INTERACT",
            "name_full": "INTERACT (interactive checking of full passages)",
            "brief_description": "An interactive prompting scheme that lets the model request to 'check' the full texts of specified retrieved passages during generation, then produce output statements, enabling a stepwise generate-and-check workflow within the same session.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo-0301)",
            "model_description": "Instruction-following ChatGPT with 4K context window; used with summarized/snippet representations of many retrieved passages and allowed to 'check' up to 3 full passages during generation.",
            "reflection_method_name": "INTERACT (generate/check/output loop)",
            "reflection_method_description": "Expose many (summarized) retrieved passages in context; the model can issue actions at each step: (1) 'Check: Document [ids]' to fetch and read the full document text for those ids, (2) 'Output:' to write a statement citing passages, and (3) 'End.' to finish. Full passages are removed after being checked to save context. The system forces an 'Output' after each 'Check' to avoid indefinite checking.",
            "num_iterations": null,
            "task_name": "ASQA, ELI5 (ALCE benchmark)",
            "task_description": "Same citation-generation tasks; INTERACT aims to let the model inspect full texts selectively to improve faithfulness while keeping many summaries in context.",
            "performance_with_reflection": "ASQA (ChatGPT, SUMM 10-psg w/ INTERACT): fluency MAUVE 69.0, correctness (EM) 39.1, citation recall 73.4%, citation precision 66.5% (compare to SUMM-only below). ELI5 (ChatGPT w/ INTERACT): fluency 68.0, claim recall 13.3, citation recall 47.8%, precision 45.0.",
            "performance_without_reflection": "ASQA (ChatGPT, SUMM 10-psg, no INTERACT): fluency 70.0, correctness 43.3, citation recall 68.9%, citation precision 61.8. ELI5 VANILLA (no INTERACT): citation recall 51.1%, claim recall 12.0.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Mixed: INTERACT increased citation recall for ASQA when added to SUMM (68.9% → 73.4%), suggesting selective checking can help support statements; however, correctness dropped (43.3% → 39.1%). Across datasets the paper concludes interactive retrieval approaches do not reliably improve overall performance.",
            "limitations_or_failure_cases": "Authors observe models are not yet proficient at interactive usage: INTERACT sometimes reduced correctness and required engineering (forcing 'Output' after checks). Checking full passages gave limited benefit in many cases; interactive methods did not broadly outperform simpler VANILLA prompting.",
            "uuid": "e5437.2",
            "source_info": {
                "paper_title": "Enabling Large Language Models to Generate Text with Citations",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "INLINESEARCH",
            "name_full": "INLINESEARCH (search-on-the-fly during generation)",
            "brief_description": "A generation-time retrieval interface: the model can call a 'Search: {query}' action during generation to retrieve the best passage from the top-100 retrieval set and immediately use it (then the passage is removed to save context).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo-0301)",
            "model_description": "ChatGPT with ability to perform retrieval actions during generation over the pre-retrieved top-100 passages (GTR used for ranking returned passage per search action).",
            "reflection_method_name": "INLINESEARCH (on-the-fly retrieval + output actions)",
            "reflection_method_description": "During generation, the model can emit 'Search: {query}' which triggers a search among top-100 passages (GTR) and places the best-matching passage in context; model can then 'Output:' statements citing the returned passage and continue searching as needed.",
            "num_iterations": null,
            "task_name": "ASQA, ELI5 (ALCE)",
            "task_description": "Allows the model to decide when and what to retrieve during multi-step answer generation.",
            "performance_with_reflection": "ASQA (ChatGPT INLINESEARCH): fluency MAUVE 58.7, correctness (EM) 32.4, citation recall 58.3%, citation precision 58.2% (worse than VANILLA). ELI5 INLINESEARCH: citation recall 45.6%, claim recall ~13.4, fluency 49.7.",
            "performance_without_reflection": "VANILLA (ChatGPT 5-psg) ASQA: fluency 66.6, correctness 40.4, citation recall 73.6%, precision 72.5. ELI5 VANILLA citation recall 51.1%, claim recall 12.0.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "INLINESEARCH performed worse than VANILLA on both citation quality and correctness in the reported experiments; the authors report that retrieving text on the fly did not improve performance and often degraded citation quality.",
            "limitations_or_failure_cases": "Authors find it hard for the model to formulate effective intermediate queries without seeing passage content; INLINESEARCH performed worse possibly because the model cannot easily craft detailed retrieval queries from the question alone. Additional context or improved querying heuristics may be required.",
            "uuid": "e5437.3",
            "source_info": {
                "paper_title": "Enabling Large Language Models to Generate Text with Citations",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Generate-then-revise (RARR / related)",
            "name_full": "Generate-then-revise approaches (e.g., RARR)",
            "brief_description": "Approaches that first produce an initial answer without retrieval, then retrieve relevant documents and revise the answer to be consistent with evidence (sometimes using LMs to perform the revision). Mentioned in related work but not implemented in this paper's experiments.",
            "citation_title": "RARR: Researching and revising what language models say, using language models.",
            "mention_or_use": "mention",
            "model_name": "unspecified language model(s) in referenced work",
            "model_description": "Referenced works (e.g., Gao et al., 2023) empirically use large LMs to both generate initial outputs and to conduct subsequent retrieval-and-revision steps; exact model variants depend on those papers.",
            "reflection_method_name": "Generate-then-revise / iterative retrieval-based revision",
            "reflection_method_description": "First generate a draft answer without external documents; then retrieve supporting documents relevant to the draft and instruct the model (or another model) to revise the draft to be consistent with retrieved evidence (a generate-then-reflect/revise pipeline).",
            "num_iterations": null,
            "task_name": "General factual generation and verification (mentioned in related work)",
            "task_description": "Used for improving faithfulness of LM outputs by aligning them to external evidence via revision steps.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "evidence_of_improvement": "Paper cites Gao et al. (2023) and He et al. (2022) as proposing generate-then-revise pipelines; ALCE paper itself did not implement these and therefore provides no in-paper empirical comparison, but references suggest this line of work aims to improve factuality by revision.",
            "limitations_or_failure_cases": "ALCE authors note these approaches are in prior work and did not evaluate them in this benchmark; they emphasize that training models to incorporate citations remains challenging due to lack of supervised data. No direct failure cases are reported here for those methods.",
            "uuid": "e5437.4",
            "source_info": {
                "paper_title": "Enabling Large Language Models to Generate Text with Citations",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "RARR: Researching and revising what language models say, using language models.",
            "rating": 2
        },
        {
            "paper_title": "Rethinking with retrieval: Faithful large language model inference.",
            "rating": 2
        },
        {
            "paper_title": "WebGPT: Browser-assisted question-answering with human feedback.",
            "rating": 2
        },
        {
            "paper_title": "Teaching language models to support answers with verified quotes.",
            "rating": 2
        },
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models.",
            "rating": 1
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools.",
            "rating": 1
        }
    ],
    "cost": 0.022962999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Enabling Large Language Models to Generate Text with Citations</h1>
<p>Tianyu Gao Howard Yen Jiatong Yu Danqi Chen<br>Department of Computer Science \&amp; Princeton Language and Intelligence<br>Princeton University<br>{tianyug, hyen, jiatongy, danqic}@cs.princeton.edu</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, our aim is to allow LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare different modeling approaches. We propose ALCE, the first benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We develop automatic metrics along three dimensions-fluency, correctness, and citation quality-and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvement-For example, on the ELI5 dataset, even the best models lack complete citation support $50 \%$ of the time. Our analyses further highlight promising future directions, including developing better retrievers, advancing long-context LLMs, and improving the ability to synthesize information from multiple sources. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs; Brown et al., 2020; OpenAI, 2023) have gained increasing popularity as a tool for information seeking. While they generate engaging and coherent responses, their outputs are prone to hallucination and often contain factually incorrect information (Ji et al., 2023). This makes it harder for users to trust and verify LLMgenerated outputs without any supporting evidence.</p>
<p>In this work, we study a new generation paradigm for LLMs, in which we require LLMs</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The task setup of ALCE. Given a question, the system generates text while providing citing passages from a large retrieval corpus. Each statement may contain multiple citations (e.g., [1][2]).
to provide citations to one or a few text passages for any statement they generate (Figure 1). Incorporating citations brings several benefits: (1) users can easily verify LLMs' claims with the provided citations; (2) LLMs can generate text that faithfully follows cited passages, which has the promise to improve correctness and alleviate hallucination.</p>
<p>Multiple commercial systems have adopted this paradigm: Bing Chat ${ }^{2}$ and perplexity.ai ${ }^{3}$ respond to user questions in natural language with references to Web pages. Nakano et al. (2021); Menick et al. (2022) share a similar motivation, but they mainly experiment with commercial search engines and closed-source models, making their results difficult to evaluate. Retrieval-augmented LMs (Borgeaud et al., 2022; Izacard et al., 2022) incorporate retrieved passages during both training and inference, but do not guarantee faithfulness to retrieved passages or explicitly provide citations. Additionally, previous studies mostly rely on human evaluation (Nakano et al., 2021; Menick et al., 2022; Liu et al., 2023), which is expensive and difficult to reproduce. We argue that the absence of automated evaluation hinders the advances of such systems.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Corpus (#passages)</th>
<th>Question type</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>ASQA</td>
<td>Wikipedia (21M)</td>
<td>Factoid (ambiguous)</td>
<td>Q: When did the US break away from England?</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>A: The US declared independence on July 2, 1776 [1][2] ... The</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>Treaty of Paris was later signed on September 3, 1783 [3].</td>
</tr>
<tr>
<td>QAMPARI</td>
<td>Wikipedia (21M)</td>
<td>Factoid (list)</td>
<td>Q: Which films have Gong Li as a member of their cast?</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>A: The Story of Qiu Ju [1], Farewell My Concubine [2], The</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>Monkey King 2 [3], Mulan [3], Saturday Fiction [3] ...</td>
</tr>
<tr>
<td>ELI5</td>
<td>Sphere (899M)</td>
<td>Why/How/What</td>
<td>Q: How do student loans affect getting a mortgage?</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>A: Student loans can affect the debt to income ratio [1], which is</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>a key factor in determining the amount that ... [2][3]</td>
</tr>
</tbody>
</table>
<p>Table 1: The three datasets used in our ALCE benchmark. These datasets cover a wide range of question types and the corresponding corpora span from Wikipedia to Web-scale document collection.</p>
<p>We present ALCE, the first reproducible benchmark for automatically evaluating LLMs’ generations with citations. ALCE assumes a naturallanguage question and a retrieval corpus, and requires building end-to-end systems to retrieve relevant passages from the corpus, generate a response to the question, and cite corresponding supporting passages. We compile three datasets that cover different types of questions and corporaASQA (Stelmakh et al., 2022), QAMPARI (Rubin et al., 2022), and ELI5 (Fan et al., 2019)—as shown in Table 1. Different from previous benchmarks (Lee et al., 2019; Bohnet et al., 2022), ALCE evaluates long-text generation, focusing on automatically evaluating citation quality, and allows citing multiple passages for individual statements.</p>
<p>We design automatic evaluation methods in three dimensions: fluency, correctness, and citation quality. Specifically, we use MAUVE (Pillutla et al., 2021) to measure fluency, propose tailored correctness metrics for each dataset, and adopt a natural language inference (NLI) model (Honovich et al., 2022) to measure citation quality. We showcase how the three dimensions together contribute to a robust evaluation, preventing systems from exploiting shortcuts. Additionally, we conduct human evaluation and demonstrate a strong correlation with our automatic metrics.</p>
<p>We experiment on multiple systems with state-of-the-art LLMs and retrievers and also propose novel prompting strategies to synthesize retrieved text into text generation. Although all systems are capable of providing fluent and coherent responses, there remains substantial room for improvement in terms of correctness and citation quality: For example, on the ELI5 dataset, around 50% generations of our ChatGPT and GPT-4 baselines are not fully supported by the cited passages. Additionally, we find that (1) a closed-book model (generating answers without accessing any retrieved documents) with post-hoc citing achieves good correctness but much worse citation quality; (2) although interactive retrieval approaches (Yao et al., 2023; Schick et al., 2023) offer more flexibility in when/what to retrieve, they do not improve the performance on this challenging benchmark; (3) summarizing the retrieved passages in a shorter text improves correctness but not citation quality; (4) reranking multiple generations boosts citation quality measured by human evaluation; (5) incorporating more retrieved passages in context does not help ChatGPT but improves GPT-4 performance.</p>
<p>Our extensive analyses highlight three major challenges of building LLMs to generate text with citations: (1) the retrieval quality is crucial to the final performance and has substantial room for improvement; (2) LLMs’ limited context window restricts the number of passages they can incorporate; (3) current LLMs struggle to synthesize multiple documents in context without being distracted by irrelevant ones, although better instruction tuning brings significant improvement. These challenges pose promising research directions for developing better systems integrating retrieval and LLMs.</p>
<h2>2 Task Setup and Datasets</h2>
<p>Our task is formalized as follows: Given a query $q$ and a corpus of text passages $\mathcal{D}$, the system is required to return an output $\mathcal{S}$, which consists of $n$ statements $s_{1}, \ldots, s_{n}$, and each statement $s_{i}$ cites a list of passages $\mathcal{C}<em 1="1" i_="i,">{i}=\left{c</em>$ While LLMs may include sentences that do not require a citation, such as "I'm happy to help", we observe that almost all sentences that LLMs output provide}, c_{i, 2}, \ldots\right}^{4}$, where $c_{i, j} \in \mathcal{D}$. In this work, we segment LLMs’ output into statements by sentence boundaries. ${ }^{5</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>valuable information and require citations, similar to findings in Liu et al. (2023). In this work, citations are enclosed by box brackets such as [1][2].</p>
<p>We divide the corpus $\mathcal{D}$ into 100-word passages following previous works on open-domain question answering (Karpukhin et al., 2020; Petroni et al., 2021; Piktus et al., 2021), in contrast to commercial systems like Bing Chat, which cite entire Web pages. We take 100-word passages because it is easier for humans to verify, and allows for more retrieved passages to fit in LLMs' limited context.</p>
<p>We choose QA datasets so that (1) they contain factual questions, in which references are important; (2) questions require long-text answers that cover multiple aspects; (3) answering the questions requires synthesizing multiple sources. We select three datasets (Table 1) and introduce them below. See $\S B$ for additional statistics.</p>
<p>ASQA (Stelmakh et al., 2022) is a long-form factoid dataset. As shown in Figure 1, each question is an ambiguous question from AmbigQA (Min et al., 2020) that requires multiple short answers to cover different aspects, and the dataset provides a longform answer that covers all short answers. Since most questions can be answered by Wikipedia, we use the 2018-12-20 Wikipedia snapshot as $\mathcal{D}$.</p>
<p>QAMPARI (Rubin et al., 2022) is a factoid QA dataset constructed from Wikipedia, where the answer is a list of entities that are drawn from different passages. Same as ASQA, we use the 2018-1220 Wikipedia as the corpus.</p>
<p>ELI5 (Fan et al., 2019) is a long-form QA dataset built on the Reddit forum "Explain Like I'm Five". ${ }^{6}$ Most ELI5 questions are how/why/what questions that require long answers and multiple passages as evidence. Due to the diverse topics discussed in the questions, we use Sphere (Piktus et al., 2021)—a filtered version of Common Crawl ${ }^{7}$ —as the corpus. The ELI5 dataset is widely used in related work due to its challenging nature (Nakano et al., 2021; Menick et al., 2022; Liu et al., 2023).</p>
<p>We randomly select 1,000 examples from the development set of each dataset for ALCE. Our benchmark primarily assesses the citation capabilities of existing LLMs and does not provide training data, as there are no available examples that provide supervision for citations in these datasets.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>3 Automatic Evaluation</h2>
<p>Our benchmark measures the following three dimensions of system responses:</p>
<ul>
<li>Fluency: whether the model's generated text is fluent and coherent.</li>
<li>Correctness: whether the answer is accurate and covers all aspects of interest.</li>
<li>Citation quality: whether the answer is well supported by the cited passages and no irrelevant passages are cited.
In the following, we present automatic metrics for each dimension and discuss why the combination of the three metrics provides a robust evaluation.</li>
</ul>
<h3>3.1 Fluency</h3>
<p>We use MAUVE (Pillutla et al., 2021) to evaluate the fluency of the output (§C). We deploy MAUVE for ASQA and ELI5 and omit it for QAMPARI, as QAMPARI only requires a list of short answers as the response and LLMs consistently adhere to the format in our experiments. As MAUVE is sensitive to output length and text style, and most LLMs are capable of producing fluent text, we mainly employ it as a sanity check as long as the MAUVE scores are high enough.</p>
<h3>3.2 Correctness</h3>
<p>Our objective is to measure the informativeness and utility of the generation to the question. Liu et al. (2023) propose to directly evaluate perceived utility by humans, a process difficult to automate. Therefore, we use correctness-whether the response is accurate compared to a ground truth answer-as a proxy. Evaluating the correctness of long-form generation is a challenging task (Krishna et al., 2021), and we describe our strategy for each dataset below. Figure 2 illustrates the metrics and we include additional implementation details in $\S C$.</p>
<p>For ASQA, we follow Stelmakh et al. (2022) and calculate the recall of correct short answers by checking whether the short answers (provided by the dataset) are exact substrings of the generation (exact match recall; EM recall).</p>
<p>For QAMPARI, we follow Rubin et al. (2022) and calculate the precision and recall of the model prediction, by checking the exact match to the gold answer list. We add one additional adjustment: considering that users often want to know only a few example answers of the question, our evaluation considers recall to be $100 \%$ if the prediction includes at least 5 correct answers (recall-5).</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Evaluation of correctness (details in §3.2).
Unlike ASQA and QAMPARI, the ELI5 dataset does not provide short entity answers. Fan et al. (2019) use ROUGE for evaluation, which does not reflect the correctness well (Krishna et al., 2021; §A). Inspired by works in summarization evaluation (Zhang and Bansal, 2021; Kamoi et al., 2023; Wang et al., 2020), we use InstructGPT (text-davinci-003; Ouyang et al., 2022) to generate three "sub-claims". Then we use TRUE $^{\mathrm{S}}$ (Honovich et al., 2022), a T5-11B (Raffel et al., 2020) model fine-tuned on a collection of natural language inference (NLI) datasets, to check whether the model output entails the sub-claims (claim recall). TRUE targets factual correctness and has been used by previous works in similar context (Bohnet et al., 2022; Gao et al., 2023). We demonstrate that claim recall provides a more accurate measure of correctness than existing metrics (more details in §A).</p>
<h3>3.3 Citation Quality</h3>
<p>We evaluate citation qualities using two metrics: (1) citation recall, which determines if the output is entirely supported by cited passages, and (2) citation precision, which identifies any irrelevant citations. Although we prioritize citation recall as it entails a well-supported and truthful answer, enhancing precision is crucial for better user satisfaction, reducing the need for human review of extraneous</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Evaluation of citation quality (details in §3.3). We use an NLI model to verify whether a statement is supported by its citations.
passages. Figure 3 provides an illustrated example.
We use the NLI model TRUE (Honovich et al., 2022) again to automatically examine whether the cited passages entail the model generation. We conduct human evaluation (§6) to demonstrate strong human correlation of our metric.</p>
<p>Citation recall. We calculate the citation recall of each statement ( 0 or 1 ) and average over all statements in the model response. For each statement $s_{i}$, its citation recall is 1 if and only if there is at least one citation $\left(\mathcal{C}<em i="i">{i} \neq \emptyset\right)$ and $\phi\left(\operatorname{concat}\left(\mathcal{C}</em>}\right), s_{i}\right)=1$, where $\phi$ (premise, hypothesis) is the NLI model that outputs 1 if the premise entails the hypothesis, and 0 otherwise; concat $\left(\mathcal{C<em i="i">{i}\right)$ concatenates all passages in $\mathcal{C}</em>}$ together (details in §C). The NLI evaluation is in accordance with the attributable to identified sources (AIS) framework (Rashkin et al., 2023): $\phi\left(\operatorname{concat}\left(\mathcal{C<em i="i">{i}\right), s</em>\right)$.}\right)=1$ implies that $s_{i}$ is true based solely on concat $\left(\mathcal{C}_{i</p>
<p>Citation precision. Our citation precision evaluation detects citations that are irrelevant, but it does not require citing a minimal set. We follow this design because human writing often cites redundant sources to enhance credibility; human readers may also appreciate multiple citations, especially when it pertains to critical claims such as medical advice.</p>
<p>We calculate the citation precision for each citation ( 0 or 1 ) and average over all citations in the</p>
<table>
<thead>
<tr>
<th>Instruction: Write an accurate, engaging, and</th>
</tr>
</thead>
<tbody>
<tr>
<td>concise answer for ...</td>
</tr>
<tr>
<td><Retrieve for the question></td>
</tr>
<tr>
<td>Document <a href="Title: American Decolonization">1</a></td>
</tr>
<tr>
<td>...</td>
</tr>
<tr>
<td>Document <a href="Title: Decolonization">2</a> ...</td>
</tr>
<tr>
<td>Document <a href="Title: American Revolution">3</a> ...</td>
</tr>
<tr>
<td>...</td>
</tr>
<tr>
<td>Question: When did US break away from England?</td>
</tr>
<tr>
<td>Answer: The United States took the first step</td>
</tr>
<tr>
<td>towards gaining independence ... [1][2]. The</td>
</tr>
<tr>
<td>Treaty of Paris was later signed ... [3].</td>
</tr>
</tbody>
</table>
<p>Table 2: An example of our VANILLA method. Different colors represent prompt, model generation, and <actions>. We also provide two in-context demonstrations before the test example.
response. We first define if a citation is "irrelevant". Intuitively, a citation $c_{i, j}$ is "irrelevant" if (a) $c_{i, j}$ itself cannot support $s_{i}$ and (b) removing $c_{i, j}$ does not affect the rest of the citations to support $s_{i}$. Formally, $c_{i, j}$ is "irrelevant" if and only if
(a) $\phi\left(c_{i, j}, s_{i}\right)=0$, AND
(b) $\phi\left(\operatorname{concat}\left(\mathcal{C}<em i_="i," j="j">{i} \backslash\left{c</em>\right)=1$.
$c_{i, j}$ has a precision of 1 if $s_{i}$ has recall $=1$ and $c_{i, j}$ is not irrelevant. For example (Figure 3), when $s_{3}$ cites three references [2][4][5] and recall $=1$, [2] is "irrelevant" if $\phi\left([2], s_{3}\right)=0$ and $\phi\left([4][5], s_{3}\right)=1$. For condition (b) to work, we set recall $=1$ as a prerequisite for precision $=1$. Note that this algorithm overlooks the scenario when one citation partially supports the statement. We discuss the details in §E.}\right}\right), s_{i</p>
<h3>3.4 ALCE is Robust to Shortcut Cases</h3>
<p>We showcase how the ALCE evaluation is robust to two possible shortcuts in §D: (1) using the top-1 retrieved passage as the response and citing itself, and (2) using the first two sentences of the top-1 passage. Both cases have almost-perfect citation scores, but (1) has low fluency due to its unnaturally long length compared to human answers, and (2) has low correctness due to low coverage.</p>
<h2>4 Modeling</h2>
<p>In this section, we discuss three major modeling components for an ALCE system—retrieval, synthesis, and post-editing.</p>
<h3>4.1 Retrieval</h3>
<p>We explore simple, off-the-shelf retrievers. We use dense retrievers for Wikipedia, including GTR (Ni et al., 2022) and DPR (Karpukhin et al., 2020); we use BM25 for Sphere. For each question, we retrieve the top-100 passages.</p>
<h3>4.2 Synthesis</h3>
<p>We focus on how to prompt an LLM to interact with the retriever, and synthesize and cite the evidence (without fine-tuning internal parameters). One noteworthy challenge is that existing LLMs all have limited context window and thus can only fit a handful of passages.</p>
<p>Vanilla. We simply provide the model with the top- $k^{9}$ passages and instruct the model to cite accordingly (Table 2). We also use in-context learning (Brown et al., 2020) and prepend two demonstrations. The complete instruction is in Table 23.</p>
<p>Summ/SNIPPET. With a 4K context window, we can at most safely fit $k=5$ passages. As shown in Figure 4, top-5 retrieved passages can only cover 56.8% percent of the answers in ASQA.</p>
<p>To tackle this limitation, we propose to provide summaries or snippets of passages instead of the full text (summaries are abstractive but snippets are spans from passages). We acquire summaries and snippets by prompting ChatGPT with instructions (prompts in Table 25 and 26). ${ }^{10}$ Then we replace all passages with summaries/snippets. Summaries or snippets significantly reduce the passage length, allowing for more passages to fit in: for ASQA, they reduce passage length by $6 \times$ on average.</p>
<p>Though Summ/SNIPPET allows for more retrieved passages, they are lossy compressions. To alleviate this problem, we propose INTERACT, an interactive prompting scheme to allow the model to check the full text of certain passages. At each step, the model can execute one of three actions: (1) "Check: Document [1][2]" to check the full text of the corresponding documents; (2) "Output:" to output a statement of the answer; (3) "End." to end the generation. §C provides more details.</p>
<p>InlineSearch. The above methods all display retrieval results at the beginning. In InLinESEARCH, we allow LLMs to call "search" during the generation process (Yao et al., 2023; Press et al., 2022; Jiang et al., 2023). At each step, the model can execute one of three actions: "Search:</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Instruction: ...</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><Retrieve for question "..."></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Question: When did US break away from England?</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Search: Declaration of Independence</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><Search the query among the top-100 passages></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Document <a href="Title: ...">1</a> ...</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Output: The United States ... [1].</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><Remove Document [1] from context></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Search: Treaty of Paris</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><Search the query among the top-100 passages></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Document <a href="Title: ...">3</a> ...</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Output: The Treaty of Paris ... [3].</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><Remove Document [3] from context></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">End.</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 3: An example of InLineSearch.
{query}" to search among the top-100 passages ${ }^{11}$ by using GTR; the "Output" and "End" actions are the same as Interact. For each "Search" action, we display the best retrieved passage in the context. The passage is removed after one action to save context space. Table 3 shows an example.</p>
<p>ClosedBook. We also add a simple closedbook baseline, where the model is only prompted with the instruction and the question, without any retrieved passages provided. Consequently, this variant does not cite any evidences.</p>
<h3>4.3 Post-editing</h3>
<p>In this section we discuss two strategies for refining the output to further improve its quality.</p>
<p>Rerank. We randomly sample $n_{\text {sample }}=4$ responses for each question, and select the best response using the automatic citation recall score. we expect RERANK to improve the citation quality.</p>
<p>PostCite. For each statement, we find the best matching passage among the top-100 retrieved passages using GTR and cite it. We combine this with CLOSEDBOOK in our experiments.</p>
<h2>5 Experiments</h2>
<p>We describe experiment details in §C. We use ChatGPT (gpt-3.5-turbo-0301) with a 4 K context window for most main experiments and ablations. We also report results with ChatGPT-16K (gpt-3.5-turbo-16k-0613) and GPT-4 (gpt-4-0613; 8 K context window). For open-source models, we test LLaMA (Touvron et al., 2023a) and its instruction-tuned versions, including Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023), and</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup><code>Fluency Correct. Citation
(MAUVE) (EM Rec.) Rec. Prec.
ChatGPT
VANILLA (5-psg) 66.6 40.4 73.6 72.5
w/ RERANK 77.0 40.2 84.8 81.6
SUMM (10-psg) 70.0 43.3 68.9 61.8
w/ INTERACT 69.0 39.1 73.4 66.5
SNIPPET (10-psg) 69.8 41.4 65.3 57.4
INLINESEARCH 58.7 32.4 58.3 58.2
CLOSEDBOOK 52.7 38.3 26.7 26.7
GPT-4 (VANILLA prompting)
GPT-4 (5-psg) 67.1 41.3 68.5 75.6
GPT-4 (20-psg) 64.9 44.4 73.0 76.5
LLaMA (VANILLA prompting)
LLaMA-13B (3-psg) 68.4 26.9 10.6 15.4
Vicuna-13B (3-psg) 82.6 31.9 51.1 50.1
Chat-13B (5-psg) 72.4 35.2 38.4 39.4
Chat-70B (5-psg) 88.3 41.5 62.9 61.3</code></p>
<p>Table 4: Experiments on ASQA. For CLOSEDBOOK, we use PostCite to get citations. $k$-psg: putting top$k$ passages from the retrieval results into the context. Chat-13B and Chat-70B refer to LLaMA-2-Chat.</p>
<p>Oasst (Köpf et al., 2023). They all have a 2K context window. We use short instructions for LLaMA (Table 24) to save context budget. Additionally, we test LLaMA-2-Chat, which were also trained to follow instructions (Touvron et al., 2023b). These models have a context window of 4 K tokens, which allows for 5 passages per question.</p>
<h3>5.1 Main Results</h3>
<p>We present the main results on three datasets in Table 4, 5, and 6 respectively (full results in §G.6). We first note that all models achieve good fluency scores (except some models on ELI5 mainly due to their longer generations). We summarize the main takeaways from the experiments below.</p>
<p>VANILLA achieves strong performance. Despite its simplicity, VANILLA (putting retrieved passages in context) achieves close-to-the-best performance among all prompting strategies.</p>
<p>Using summaries or snippets improves correctness. We see a universal trend that SUMM or SNIPPET improves correctness, though on ASQA and ELI5, such an improvement comes at a cost of citation quality due to the lossy compression. Combining INTERACT with SUMM/SNIPPET does not bring improvement, and we hypothesize that checking the full passages offers limited benefit and current LLMs are not proficient in an interactive usage.</p>
<p>Retrieving text on the fly does not improve performance. All datasets show that VANILLA outperforms InLineSEARCH on citation quality (and</p>
<table>
<thead>
<tr>
<th></th>
<th>Correctness</th>
<th></th>
<th>Citation</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Rec.-5</td>
<td>Prec.</td>
<td>Rec.</td>
<td>Prec.</td>
</tr>
<tr>
<td>ChatGPT</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>VANILLA (5-psg)</td>
<td>20.8</td>
<td>20.8</td>
<td>20.5</td>
<td>20.9</td>
</tr>
<tr>
<td>w/ RERANK</td>
<td>22.8</td>
<td>21.4</td>
<td>21.2</td>
<td>21.4</td>
</tr>
<tr>
<td>SUMM (10-psg)</td>
<td>23.6</td>
<td>21.2</td>
<td>23.6</td>
<td>25.7</td>
</tr>
<tr>
<td>SNIPPET (10-psg)</td>
<td>24.5</td>
<td>21.5</td>
<td>22.9</td>
<td>24.9</td>
</tr>
<tr>
<td>w/ INTERACT</td>
<td>21.9</td>
<td>23.0</td>
<td>21.9</td>
<td>23.4</td>
</tr>
<tr>
<td>INLINESEARCH</td>
<td>17.2</td>
<td>20.4</td>
<td>14.9</td>
<td>14.9</td>
</tr>
<tr>
<td>CLOSEDBOOK</td>
<td>32.9</td>
<td>19.8</td>
<td>10.0</td>
<td>10.0</td>
</tr>
<tr>
<td>GPT-4 (VANILLA prompting)</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-4 (5-psg)</td>
<td>22.2</td>
<td>25.0</td>
<td>25.9</td>
<td>27.0</td>
</tr>
<tr>
<td>GPT-4 (20-psg)</td>
<td>29.6</td>
<td>26.2</td>
<td>27.4</td>
<td>28.5</td>
</tr>
<tr>
<td>LLaMA (VANILLA prompting)</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LLaMA-13B (3-psg)</td>
<td>9.7</td>
<td>9.1</td>
<td>6.7</td>
<td>7.1</td>
</tr>
<tr>
<td>Vicuna-13B (5-psg)</td>
<td>14.0</td>
<td>15.9</td>
<td>12.5</td>
<td>13.4</td>
</tr>
<tr>
<td>Chat-13B (5-psg)</td>
<td>21.1</td>
<td>18.2</td>
<td>9.6</td>
<td>9.7</td>
</tr>
<tr>
<td>Chat-70B (5-psg)</td>
<td>21.8</td>
<td>18.4</td>
<td>15.1</td>
<td>15.6</td>
</tr>
</tbody>
</table>
<p>Table 5: Experiments on QAMPARI. “Rec.-5”: we set the recall to be 100% if the prediction includes at least 5 correct answers.</p>
<p>on correctness for ASQA and ELI5). By manually examining the examples, we find that it is challenging to ask detailed questions without seeing any passages. To improve INLINESEARCH, one may need to provide more context about the questions in advance or encourage the model to call retrievers with more detailed and diverse queries.</p>
<p>RERANK boosts citation quality. We observe that RERANK leads to consistent improvement in citation quality (on ASQA and ELI5). As the automatic scores may be biased in RERANK, we also conduct human evaluation (§6) and verify its effectiveness.</p>
<h4>CLOSEDBOOK+POSTCITE delivers strong correctness but poor citation quality. CLOSEDBOOK outperforms VANILLA in correctness on ELI5 and QAMPARI, and has only a 2% gap on ASQA. However, CLOSEDBOOK cannot provide any citation; when combined with POSTCITE, the citation quality remains inadequate. For instance, citation recall of CLOSEDBOOK+POSTCITE is lower than VANILLA by 47% on ASQA.</h4>
<p>To understand why CLOSEDBOOK achieves better correctness and why POSTCITE cannot deliver satisfying citation quality, we manually examine model outputs and find that: (1) open-book models are easily distracted by irrelevant passages and generate responses with lower correctness, a phenomenon also observed by <em>Shi et al. (2023)</em>; (2) CLOSEDBOOK often generates texts that are correct but not similar to any retrieved passages, making it difficult to match a citation post-hoc.</p>
<table>
<thead>
<tr>
<th></th>
<th>Fluency</th>
<th>Correct.</th>
<th>Citation</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>(MAUVE)</td>
<td>(Claim)</td>
<td>Rec.</td>
<td>Prec.</td>
</tr>
<tr>
<td>ChatGPT</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>VANILLA (5-psg)</td>
<td>57.2</td>
<td>12.0</td>
<td>51.1</td>
<td>50.0</td>
</tr>
<tr>
<td>w/ RERANK</td>
<td>56.1</td>
<td>11.4</td>
<td>69.3</td>
<td>67.8</td>
</tr>
<tr>
<td>SUMM (10-psg)</td>
<td>40.3</td>
<td>12.5</td>
<td>51.5</td>
<td>48.2</td>
</tr>
<tr>
<td>SNIPPET (10-psg)</td>
<td>62.9</td>
<td>14.3</td>
<td>50.4</td>
<td>45.0</td>
</tr>
<tr>
<td>w/ INTERACT</td>
<td>68.0</td>
<td>13.3</td>
<td>47.8</td>
<td>45.0</td>
</tr>
<tr>
<td>INLINESEARCH</td>
<td>49.7</td>
<td>13.4</td>
<td>45.6</td>
<td>43.7</td>
</tr>
<tr>
<td>CLOSEDBOOK</td>
<td>32.6</td>
<td>18.6</td>
<td>15.5</td>
<td>15.5</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-4 (VANILLA prompting)</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-4 (5-psg)</td>
<td>38.4</td>
<td>14.2</td>
<td>44.0</td>
<td>50.1</td>
</tr>
<tr>
<td>GPT-4 (20-psg)</td>
<td>41.5</td>
<td>18.3</td>
<td>48.5</td>
<td>53.4</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LLaMA (VANILLA prompting)</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LLaMA-13B (3-psg)</td>
<td>50.0</td>
<td>3.9</td>
<td>3.1</td>
<td>5.3</td>
</tr>
<tr>
<td>Vicuna-13B (3-psg)</td>
<td>58.2</td>
<td>10.0</td>
<td>15.6</td>
<td>19.6</td>
</tr>
<tr>
<td>Chat-13B (5-psg)</td>
<td>34.7</td>
<td>13.4</td>
<td>17.3</td>
<td>15.8</td>
</tr>
<tr>
<td>Chat-70B (5-psg)</td>
<td>38.6</td>
<td>12.8</td>
<td>38.3</td>
<td>37.9</td>
</tr>
</tbody>
</table>
<p>Table 6: Experiments on ELI5. We use <em>claim recall</em> for the correctness evaluation. Chat-13B and Chat-70B refer to LLaMA-2-Chat.</p>
<h4>GPT-4 brings limited improvement but is better at using long context. We evaluate GPT-4 with VANILLA and different numbers of passages (more results in §G.6). GPT-4 brings consistent (but limited) improvement on correctness, but often at a cost of citation quality. GPT-4 can also incorporate more passages due to its longer context window, which boosts both correctness and citation quality. On the contrary, including more passages with ChatGPT-16K does not improve the results (Table 7), suggesting that processing more passages is non-trivial and GPT-4 is better at synthesizing information from its long context than ChatGPT.</h4>
<h3>5.2 Comparison of Different LLMs</h3>
<p>Table 7 compares different LLMs on ASQA using VANILLA (more results in §G.6). Notably, instruction-tuned models (Vicuna-13B and LLaMA-2-Chat) outperform the original LLaMA models in correctness and considerably enhance the citation quality. We observe that while the original LLaMA models are able to copy facts from the context, they struggle with accurately citing the sources or simply do not cite. Notably, the best open-source model, LLaMA-2-70B-Chat, achieves comparable correctness score as the OpenAI models, but still lags behind in citation quality.</p>
<h3>5.3 Retrieval Analysis</h3>
<p>The retrieval results play a crucial role to the correctness and the citation quality. Figure 4 presents the retrieval recall@$k$ with different datasets and</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Retrieval recall@k on ASQA (<em>EM recall</em>), QAMPARI (<em>recall-5</em>), and ELI5 (<em>claim recall</em>). Retrieval recall serves as an upper bound for model performance, and we compare them with two models' correctness results in the figure (dashed lines): "Vanilla (5-psg)" is ChatGPT VANILLA with top-5 passages in context; "Oracle" is the same model except that it uses 5 gold passages (§G.1), whose recall matches Recall@100 on all three datasets.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Fluency</td>
<td>Correct.</td>
<td>Citation</td>
<td></td>
</tr>
<tr>
<td></td>
<td>(MAUVE)</td>
<td>(EM Rec.)</td>
<td>Rec.</td>
<td>Prec.</td>
<td></td>
</tr>
<tr>
<td>Open-source (max #tokens=2K-4K)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LLaMA-13B (3-psg)</td>
<td>68.4</td>
<td>26.9</td>
<td>10.6</td>
<td>15.4</td>
<td></td>
</tr>
<tr>
<td>Vicuna-13B (3-psg)</td>
<td>82.6</td>
<td>31.9</td>
<td>51.1</td>
<td>50.1</td>
<td></td>
</tr>
<tr>
<td>Chat-13B (5-psg)</td>
<td>72.4</td>
<td>35.2</td>
<td>38.4</td>
<td>39.4</td>
<td></td>
</tr>
<tr>
<td>Chat-70B (5-psg)</td>
<td>88.3</td>
<td>41.5</td>
<td>62.9</td>
<td>61.3</td>
<td></td>
</tr>
<tr>
<td>ChatGPT (max #tokens=4K)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>ChatGPT (3-psg)</td>
<td>66.6</td>
<td>39.6</td>
<td>72.8</td>
<td>73.9</td>
<td></td>
</tr>
<tr>
<td>ChatGPT (5-psg)</td>
<td>66.6</td>
<td>40.4</td>
<td>73.6</td>
<td>72.5</td>
<td></td>
</tr>
<tr>
<td>ChatGPT-16K (max #tokens=16K)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>ChatGPT (5-psg)</td>
<td>60.3</td>
<td>36.1</td>
<td>76.2</td>
<td>76.5</td>
<td></td>
</tr>
<tr>
<td>ChatGPT (10-psg)</td>
<td>56.3</td>
<td>36.7</td>
<td>75.3</td>
<td>75.0</td>
<td></td>
</tr>
<tr>
<td>ChatGPT (20-psg)</td>
<td>56.7</td>
<td>36.1</td>
<td>73.7</td>
<td>73.5</td>
<td></td>
</tr>
<tr>
<td>GPT-4 (max #tokens=8K)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPT-4 (5-psg)</td>
<td>67.1</td>
<td>41.3</td>
<td>68.5</td>
<td>75.6</td>
<td></td>
</tr>
<tr>
<td>GPT-4 (10-psg)</td>
<td>71.5</td>
<td>43.1</td>
<td>72.0</td>
<td>75.5</td>
<td></td>
</tr>
<tr>
<td>GPT-4 (20-psg)</td>
<td>64.9</td>
<td>44.4</td>
<td>73.0</td>
<td>76.5</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 7: Comparison of different LLMs on ASQA (GTR+VANILLA). LLaMA-13B and Vicuna-13B have a context limit of 2,048 tokens, and thus can only use a short version of instructions and at most top-3 passages. Chat-13B and Chat-70B refer to LLaMA-2-Chat.</p>
<p>retrievers. As the number of passages increases, retrieval recall steadily improves. Additionally, Figure 4 shows the correctness performance of two models: (1) ChatGPT VANILLA with top-5 passages (our primary baseline); (2) an oracle version of the same model employing 5 gold passages (§G.1; the 5 gold passages match the retrieval recall@100). Notably, both models' correctness lags behind the corresponding retrieval recall (except for ELI5 top-5). The discrepancy suggests that despite the presence of accurate answers in context, LLMs struggle to utilize them in their outputs.</p>
<p>We compare the impact of different retrievers and different numbers of passages to LLMs. Figure 4 (right) shows that GTR outperforms DPR in both correctness and citation quality, emphasizing the importance of deploying better retrievers. Contrary to the retrieval recall trend in Figure 4, more passages in context do not yield substantial improvement for ChatGPT. Specifically, correctness plateaus at top-1 passage and citation quality plateaus at top-3. GPT-4 (Table 7) exhibits an increasing trend with more passages, but the improvement is not proportional to the retrieval performance. This indicates the limited ability of LLMs in utilizing multiple passages within context.</p>
<h3>5.4 Other Ablations</h3>
<p>We provide additional ablations in §G. In summary, we find that (1) using comprehensive instructions enhances the citation quality of instruction-tuned models (§G.2); (2) including at least one demonstration improves the performance (§G.3); (3) finetuned models (FiD; Izacard and Grave, 2021) with POSTCITE lag behind LLMs in both correctness and citation quality and fail to generalize (§G.4).</p>
<h2>6 Human Evaluation</h2>
<p>To verify that our automatic evaluation correlates with human judgement, we conduct human evaluation on selected models and request workers to judge model generations on three dimensions similar to Liu et al. (2023)—(1) utility: a 1-to-5 score indicating whether the generation helps answer the question; (2) citation recall: the annotator is given a sentence and all passages that the sentence cited, and is asked to judge whether the passages fully support the sentence; (3) citation precision: given a sentence and one of its citations, the annotator is asked to judge whether the citation "fully supports", "partially supports", or "does not support" the sentence. Each citation gets a precision score 1 if the output sentence has a citation recall of 1 and this citation at least "partially supports" it. See Appendix F for more details.</p>
<p><strong>Model outputs score high utility.</strong> The utility scores do not differ significantly between models, ranging 3.7-3.9 for ASQA and 3.5-3.6 for ELI5. Upon inspection, all tested models are mostly able</p>
<table>
<thead>
<tr>
<th></th>
<th>Human scores</th>
<th>ALCE scores</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Rec.</td>
<td>Prec.</td>
</tr>
<tr>
<td>ChatGPT Vanilla</td>
<td>74.7</td>
<td>76.6</td>
</tr>
<tr>
<td>w/ Rerank</td>
<td>79.3</td>
<td>81.9</td>
</tr>
<tr>
<td>Vicuna-13B Vanilla</td>
<td>51.6</td>
<td>51.5</td>
</tr>
</tbody>
</table>
<p>Table 8: Human citation quality evaluation vs. ALCE citation quality evaluation on ASQA.</p>
<table>
<thead>
<tr>
<th></th>
<th>Human scores</th>
<th>ALCE scores</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Rec.</td>
<td>Prec.</td>
</tr>
<tr>
<td>ChatGPT Vanilla</td>
<td>50.8</td>
<td>52.4</td>
</tr>
<tr>
<td>w/ Rerank</td>
<td>59.7</td>
<td>60.6</td>
</tr>
<tr>
<td>Vicuna-13B Vanilla</td>
<td>13.4</td>
<td>19.2</td>
</tr>
</tbody>
</table>
<p>Table 9: Human citation quality evaluation vs. ALCE citation quality evaluation on ELI5.
to output fluent answers that are related to the question, despite differences in factual correctness.</p>
<p>Our automatic evaluation of citation quality strongly correlates with human judgements. As shown in Table 8 (ASQA) and Table 9 (ELI5), the relative rankings induced by human and our automatic metrics are consistent. The absolute citation scores from human and ALCE are very close except for Rerank (which uses the automated citation recall for reranking). This suggests that an improvement on ALCE citation metrics translates to improvement on human preferences. Furthermore, the Cohen's kappa coefficient between human and ALCE suggests substantial agreement for citation recall ( 0.698 ) and moderate agreement for citation precision ( 0.525 ). We also show in $\S$ G. 5 that our automatic evaluation achieves high accuracy when treating human annotations as gold labels ( $85.1 \%$ for citation recall and $77.6 \%$ for citation precision).</p>
<h2>7 Related Work</h2>
<p>Evaluating citations. Generating text with citations is closely related to attribution. Rashkin et al. (2023) define the "attributable to identified sources" (AIS) score to measure how faithful a generated text is to its sources. Bohnet et al. (2022) apply AIS scores on a single-document short-answer QA dataset. Honovich et al. (2022); Yue et al. (2023) study automatic evaluations for the AIS score. A concurrent work (Liu et al., 2023) conduct human evaluation on commercial generative search engines to examine their citation qualities.</p>
<p>Scientific citation text generation (Funkquist et al., 2022) is a related task to ALCE where the model is provided the papers-to-cite and context and is required to recover the citing text. It is different from ALCE as all citations are provided and the model only needs to perform the summarization.</p>
<p>Retrieval-augmented LMs. Many studies have explored augmenting LMs with externally retrieved information. Guu et al. (2020); Borgeaud et al. (2022); Izacard et al. (2022) pre-train language models with retrieved passages, while Khandelwal et al. (2020); Zhong et al. (2022) augment LLMs' output by interpolating it with a $k \mathrm{NN}$ module; though none of them explicitly provide citations to the retrieved sources. Other works prompt or fine-tune LLMs to "retrieve on-the-fly" (Parisi et al., 2022; Schick et al., 2023; Shuster et al., 2022; Jiang et al., 2023; Yao et al., 2023; Press et al., 2022), which offers flexibility of when and what to search. Gao et al. (2023); He et al. (2022) propose to first generate text without accessing external documents and then retrieve relevant documents and revise the generation to be consistent.</p>
<p>Among previous explorations, Nakano et al. (2021); Menick et al. (2022) are the closest to our setting, where LLMs are trained to answer questions while providing citations. However, they do not explore retrieval strategies and simply use commercial search engines, which are not reproducible, and their models and training data are closedsource. To the best of our knowledge, we are the first to implement end-to-end systems that retrieve, synthesize, and cite documents with LLMs.</p>
<h2>8 Conclusion</h2>
<p>We propose ALCE, the first automatic benchmark for evaluating LLM generations with citations. We deploy automatic metrics to measure fluency, correctness, and citation quality, and verify their efficacy via human evaluation. We explore a variety of strategies for incorporating citations in LLMs and demonstrate that current systems have considerable room for improvement on ALCE.</p>
<p>Our experiments highlight a number of promising research directions, including (1) enhancing retrieval and refining retrieval integrations in LLMs, (2) developing long-context LLMs, and (3) advancing LLMs' ability to synthesize multiple sources. What's even more intriguing is that these research proposals extend beyond the ALCE setup (for example, long-context LLMs have numerous exciting applications), and ALCE can serve as a valuable testbed for their development.</p>
<h2>Limitations</h2>
<p>Our evaluation still has room for improvement: (1) MAUVE is found to be sensitive to output length and may provide unstable results; (2) for the ELIS's correctness evaluation, the automatically generated claims may not cover all possible answers due to the open-ended nature of the questions; (3) our citation quality evaluation is limited by the accuracy of the NLI model; for citation precision, the NLI model cannot detect the case of "partially support" and thus leads to a lower citation precision score than the human evaluation.</p>
<p>Although we believe our curated datasets closely resemble the distribution of real-world user questions, we acknowledge that they do not cover more challenging scenarios, such as multi-hop reasoning, math reasoning, and code completion.</p>
<p>In our experiments, we focus on prompting LLMs without updating their model weights. Training a model directly to incorporate citations remains challenging due to the lack of supervised data. However, we observe that certain humaninstruction datasets contain examples similar to our task setup. We leave the exploration of training LLMs to generate citations for future work.</p>
<h2>Acknowledgments</h2>
<p>We appreciate the helpful feedback from the members of the Princeton NLP group. We thank Alexander Wettig, Nelson Liu, Tianyi Zhang, Yu Meng, Sadhika Malladi, Yangsibo Huang, Zhiyuan Zeng, and Dan Friedman for the valuable discussion. We thank Surge AI (especially Anna Folinsky and Edwin Chen) for their support with the human evaluation. Tianyu Gao is supported by an IBM PhD Fellowship. This research is supported by an NSF CAREER award (IIS-2239290), a Sloan Research Fellowship, and Microsoft Azure credits through the "Accelerate Foundation Models Academic Research" Initiative.</p>
<h2>References</h2>
<p>Bernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, et al. 2022. Attributed question answering: Evaluation and modeling for attributed large language models. arXiv preprint arXiv:2212.08037.</p>
<p>Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste</p>
<p>Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and Laurent Sifre. 2022. Improving language models by retrieving from trillions of tokens. In International Conference on Machine Learning (ICML), volume 162, pages 2206-2240.</p>
<p>Samuel Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. A large annotated corpus for learning natural language inference. In Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with $90 \%$ * chatgpt quality.</p>
<p>Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. ELI5: Long form question answering. In Association for Computational Linguistics (ACL), pages 3558-3567.</p>
<p>Martin Funkquist, Ilia Kuznetsov, Yufang Hou, and Iryna Gurevych. 2022. CiteBench: A benchmark for Scientific Citation Text Generation. arXiv preprint arXiv:2212.09577.</p>
<p>Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Y Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al. 2023. RARR: Researching and revising what language models say, using language models. In Association for Computational Linguistics (ACL).</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. REALM: Retrievalaugmented language model pre-training. In International Conference on Machine Learning (ICML).</p>
<p>Hangfeng He, Hongming Zhang, and Dan Roth. 2022. Rethinking with retrieval: Faithful large language model inference. arXiv preprint arXiv:2301.00303.</p>
<p>Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In International Conference on Learning Representations (ICLR).</p>
<p>Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias. 2022. TRUE: Re-evaluating factual</p>
<p>consistency evaluation. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 3905-3920.</p>
<p>Gautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874-880, Online. Association for Computational Linguistics.</p>
<p>Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane DwivediYu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022. Atlas: Few-shot learning with retrieval augmented language models. arXiv preprint arXiv:2208.03299.</p>
<p>Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1-38.</p>
<p>Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active retrieval augmented generation. arXiv preprint arXiv:2305.06983.</p>
<p>Ryo Kamoi, Tanya Goyal, Juan Diego Rodriguez, and Greg Durrett. 2023. WiCE: Real-World Entailment for Claims in Wikipedia. arXiv preprint arXiv:2303.01432.</p>
<p>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769-6781, Online. Association for Computational Linguistics.</p>
<p>Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations (ICLR).</p>
<p>Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018. Scitail: A textual entailment dataset from science question answering. In Conference on Artificial Intelligence (AAAI), volume 32.</p>
<p>Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021. Hurdles to progress in long-form question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4940-4957, Online. Association for Computational Linguistics.</p>
<p>Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Mattick. 2023. Openassistant conversations - democratizing large language model alignment. arXiv preprint arXiv:2304.07327.</p>
<p>Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Association for Computational Linguistics (ACL), pages 6086-6096.</p>
<p>Nelson F Liu, Tianyi Zhang, and Percy Liang. 2023. Evaluating verifiability in generative search engines. arXiv preprint arXiv:2304.09848.</p>
<p>Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy CampbellGillingham, Geoffrey Irving, et al. 2022. Teaching language models to support answers with verified quotes. arXiv preprint arXiv:2203.11147.</p>
<p>Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2020. AmbigQA: Answering ambiguous open-domain questions. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 57835797, Online. Association for Computational Linguistics.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. WebGPT: Browser-assisted questionanswering with human feedback. arXiv preprint arXiv:2112.09332.</p>
<p>Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. 2022. Large dual encoders are generalizable retrievers. In Empirical Methods in Natural Language Processing (EMNLP), pages 9844-9855.</p>
<p>OpenAI. 2023. GPT-4 Technical Report.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems (NeurIPS), 35:2773027744.</p>
<p>Aaron Parisi, Yao Zhao, and Noah Fiedel. 2022. TALM: Tool augmented language models. arXiv preprint arXiv:2205.12255.</p>
<p>Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian</p>
<p>Riedel. 2021. KILT: a benchmark for knowledge intensive language tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2523-2544, Online. Association for Computational Linguistics.</p>
<p>Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Dmytro Okhonko, Samuel Broscheit, Gautier Izacard, Patrick Lewis, Barlas Oğuz, Edouard Grave, Wen-tau Yih, et al. 2021. The Web Is Your Oyster-Knowledge-Intensive NLP against a Very Large Web Corpus. arXiv preprint arXiv:2112.09924.</p>
<p>Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, and Zaid Harchaoui. 2021. MAUVE: Measuring the gap between neural text and human text using divergence frontiers. In Advances in Neural Information Processing Systems.</p>
<p>Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. 2022. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text Transformer. The Journal of Machine Learning Research (JMLR), 21(140).</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Empirical Methods in Natural Language Processing (EMNLP), pages 2383-2392.</p>
<p>Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, and David Reitter. 2023. Measuring Attribution in Natural Language Generation Models. Computational Linguistics, pages 1-64.</p>
<p>Samuel Joseph Amouyal Ohad Rubin, Ori Yoran, Tomer Wolfson, Jonathan Herzig, and Jonathan Berant. 2022. QAMPARI: An Open-domain Question Answering Benchmark for Questions with Many Answers from Multiple Paragraphs. arXiv preprint arXiv:2205.12665.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761.</p>
<p>Tal Schuster, Adam Fisch, and Regina Barzilay. 2021. Get your vitamin $\mathrm{C}^{3}$ robust fact verification with contrastive evidence. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 624643.</p>
<p>Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Schärli, and Denny Zhou. 2023. Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning (ICML).</p>
<p>Kurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, and Jason Weston. 2022. Language models that seek for knowledge: Modular search \&amp; generation for dialogue and prompt completion. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 373-393.</p>
<p>Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and MingWei Chang. 2022. ASQA: Factoid questions meet long-form answers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 8273-8288, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: An Instruction-following LLaMA model.</p>
<p>James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extraction and VERification. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 809-819.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. LLaMA: Open and Efficient Foundation Language Models. arXiv preprint arXiv:2302.13971.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models.</p>
<p>Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the factual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5008-5020, Online. Association for Computational Linguistics.</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR).</p>
<p>Xiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su, and Huan Sun. 2023. Automatic evaluation of attribution by large language models. arXiv preprint arXiv:2305.06311.</p>
<p>Shiyue Zhang and Mohit Bansal. 2021. Finding a balanced degree of automation for summary evaluation. In Empirical Methods in Natural Language Processing (EMNLP), pages 6617-6632.</p>
<p>Yuan Zhang, Jason Baldridge, and Luheng He. 2019. PAWS: Paraphrase adversaries from word scrambling. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 1298-1308.</p>
<p>Zexuan Zhong, Tao Lei, and Danqi Chen. 2022. Training language models with memory augmentation. In Empirical Methods in Natural Language Processing (EMNLP), pages 5657-5673.</p>
<table>
<thead>
<tr>
<th></th>
<th>ROUGE-L</th>
<th>Claim recall</th>
</tr>
</thead>
<tbody>
<tr>
<td>ChatGPT VANILLA</td>
<td>20.6</td>
<td>12.0</td>
</tr>
<tr>
<td>ChatGPT ORACLE</td>
<td>21.2</td>
<td>21.3</td>
</tr>
<tr>
<td>LLaMa-13B VANILLA</td>
<td>16.2</td>
<td>3.9</td>
</tr>
<tr>
<td>Top-1 passage</td>
<td>19.1</td>
<td>3.0</td>
</tr>
</tbody>
</table>
<p>Table 10: Comparison between ROUGE-L and claim recall scores on ELI5.</p>
<h2>Appendix A Generating Claims for ELI5</h2>
<p>We elect not to use ROUGE-L as our main correctness metrics since it does not account for the different ways of expressing the same answer and it can be easily gamed <em>Krishna et al. (2021)</em>. We further illustrate this issue in Table 10. A system can easily achieve high ROUGE-L score by retrieving and returning the top passage from a BM25 index. However, the claims evaluation metric does not reward this approach since the output often lacks different aspects of the answers.</p>
<p>Table 10: Comparison between ROUGE-L and claim recall scores on ELI5.</p>
<p>Instead, we leverage the original answers to generate sub-claims and use them to serve as an estimate of the different aspects of the answers that we expect the model to cover. This approach is inspired by works in summarization evaluation and claim verification <em>Zhang and Bansal (2021); Kamoi et al. (2023); Wang et al. (2020)</em>.</p>
<p>Specifically, we use text-davinci-003 to generate the sub-claims. We first manually annotate three question and answer pairs from the original ELI5 training set with 3 sub-claims each. Then, we prompt text-davinci-003 with these pairs as demonstrations. The full prompt with an example is shown in Table 22.</p>
<p>InstructGPT generates coherent and faithful sub-claims. To ensure that the generated sub-claims are of good quality, we manually inspect a random sample of 40 answers and their generated sub-claims (totaling to 120 sub-claims). For each sub-claim, we assign a score of 1 if it is relevant to the question and faithful to the facts presented in the ground truth, and 0 otherwise. We found that 112 out of the 120 (93.33%) sub-claims received a score of 1, meaning that our generated sub-claims are of high quality and faithful to the ground truth. Furthermore, the average number of words in the generated sub-claims is 14 words, and they are typically just one sentence long. This is aligned with the intent behind the metric—to capture short factual claims made by the original answer.</p>
<p>NLI model accurately predicts the entailment of sub-claims. We further analyze our sub-claim evaluation metrics by checking the error rate of the final prediction of the NLI model. To this end, we first manually annotate the entailment scores between 40 outputs and their sub-claims (in total of 120 pairs; these are the same questions from the previous analysis). We then use the NLI model to obtain the entailment scores for the output and sub-claims. Using the human annotations as the ground truth label, we found that the NLI model achieved an accuracy of 80.0%.</p>
<h2>Appendix B Dataset Statistics</h2>
<p>For ASQA, human answers have an average length of 65 words. For QAMPARI, each question has on average 13 answers. For ELI5, human answers have an average length of 131 words.</p>
<h2>Appendix C Implementation Details</h2>
<p>NLI model. We use the version of TRUE model from https://huggingface.co/google/ t5_xxl_true_nli_mixture, which is trained on SNLI <em>Bowman et al. (2015)</em>, MNLI <em>Williams et al. (2018)</em>, Fever <em>Thorne et al. (2018)</em>, Scitail <em>Khot et al. (2018)</em>, PAWS <em>Zhang et al. (2019)</em>, and VitaminC <em>Schuster et al. (2021)</em>. This model uses the following prompt: "premise: {PREMISE} hypothesis: {}" and outputs "1" if the premise entails the hypothesis. We format each passage (when used as premise) by the format of "Title: {TITLE}\n{TEXT}" and concatenate all passages with "\n" as a separator.</p>
<p>MAUVE. When running MAUVE, we concatenate the question and the model output (or human answer) by space. We truncate both the references and the model generations to 100 words, as we found MAUVE results are unstable beyond this length for ELI5 (this is due to that ELI5 has a lot of extremely long human answers).</p>
<p>Exact match for ASQA and QAMPARI. Both ASQA and QAMPARI provide aliases for their short answers. We normalize the response and the short answers similarly to Rajpurkar et al. (2016) and report the score with the best-matching aliases. For ASQA, Stelmakh et al. (2022) also propose a QA-based evaluation which we found to be not as stable, and thus we do not report it in our paper.</p>
<p>Output truncation. Before evaluation, we trun-</p>
<p>cate model output by new lines, as non-instructiontuned models may generate more content after new lines that are irrelevant.</p>
<p>INTERACT. Empirically, we found that models tend to execute too many consecutive "check" actions, so we force the model to always "output" after each "check". We limit the maximum number of passages to check as 3 to avoid exceeding the length limit. The full passages are removed from the context after one action to save context space. Table 27 provides an example for INTERACT.</p>
<p>Main experiments. For all experiments except ChatGPT RERANK, we run each model three times with different seeds and each time we sample two demonstrations from a pool of four. We report the averaged scores for all experiments in the main paper and we report the standard deviations in Appendix G.6.</p>
<p>Decoding methods. Based on preliminary experiments we choose the following decoding methods: For ChatGPT and GPT-4, we use sampling with temperature 0.5 ; for all open-source models, we use Nucleus sampling [holtzman2020nucleus] and set top_p $=0.95$.</p>
<h2>D ALCE Catches Shortcut Cases</h2>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Fluency</th>
<th style="text-align: center;">Correct.</th>
<th style="text-align: center;">Citation</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">(MAUVE)</td>
<td style="text-align: center;">(EM Rec.)</td>
<td style="text-align: center;">Rec.</td>
<td style="text-align: center;">Prec.</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">63.0</td>
</tr>
<tr>
<td style="text-align: left;">Top-1 passage</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">99.4</td>
</tr>
<tr>
<td style="text-align: left;">First 2 sents</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">98.7</td>
<td style="text-align: center;">98.7</td>
</tr>
</tbody>
</table>
<p>Table 11: ASQA cheating cases. "ChatGPT": the ChatGPT VANILLA model with GTR-retrieved top-5 passages. "Top-1 passage": use the top-1 retrieved passage as the response. "First 2 sents": use the first 2 sentences of the top-1 retrieved passage.</p>
<p>Table 11 demonstrates the experiments to show that ALCE is robust to shortcut cases. Using the top-1 passages or first two sentences of the top-1 passages induces almost perfect citation quality, but fluency and correctness are dramatically lower.</p>
<h2>E Citation Recall Discussion</h2>
<p>Our citation precision evaluation cannot detect a citation that partially supports the statement and hence will falsely penalize it. Consider a statement $s_{3}$ and its citations [2][4][5]: if [2] entails partial information of $s_{3}$ that [4][5] also entails, [2] will be counted as "irrelevant" while it should not be penalized. Liu et al. (2023) conduct human evaluation on citation precision in a different way: For each citation, they ask annotators to judge whether the citation (1) fully support, (2) partially support, or (3) does not support $s_{i}$. One citation $c_{i, j}$ is precise if (a) $c_{i, j}$ fully supports $s_{i}$ or (b) $\mathcal{C}<em i="i">{i}$ fully supports $s</em>}, c_{i, j}$ partially supports $s_{i}$, and no $c \in \mathcal{C<em i="i">{i}$ alone fully supports $s</em>$. This evaluation solved the corner case we mentioned in the main paper (one citation partially supports the claim but is identified as "irrelevant"). However, it is challenging to conduct such evaluation automatically, as there is no existing model that can judge whether a citation "partially" supports a claim. We also explore prompting ChatGPT to conduct such a task, which yields poor results. We defer it to future work to collect supervised data to train a better $\phi$ that can detect "partial support".</p>
<h2>F Human Evaluation</h2>
<p>We employ Surge AI (https://www.surgehq. ai/) for our human evaluation. The average pay to workers is 20 USD per hour. We randomly sample 100 examples from ASQA and ELI5 and annotate outputs of selected models: ChatGPT VANILLA, ChatGPT RERANK, and Vicuna-13B VANILLA.</p>
<h2>F. 1 Utility</h2>
<p>To check if the model output is useful to downstream users, we measure the utility of the response $\mathcal{S}$. We first show the query $q$ and model response $\mathcal{S}$ to the worker and ask them to rate their agreement with the statement "The response is a helpful and informative answer to the query" on a Likert scale of 1-5, corresponding to Strongly Disagree, Disagree, Neutral, Agree, and Strongly Agree.</p>
<h2>F. 2 Citation Recall</h2>
<p>The annotators are shown the question $q$, the statement $s_{i}$, and all of its citations $\mathcal{C}_{i}$, and they rate if the joint set of citations fully support the statement (recall=1) or if they do not support all the claims (recall=0). We calculate the overall recall score for the generation by taking an average of all the statements' recall scores.</p>
<h2>F. 3 Citation Precision</h2>
<p>We show the question $q$ and a pair of a statement $s_{i}$ and one of its citation $c_{i, j} \in \mathcal{C}_{i}$ to the annotator. We ask the annotator if the citation fully supports,</p>
<table>
<thead>
<tr>
<th></th>
<th>R@1</th>
<th>R@3</th>
<th>R@5</th>
<th>R@20</th>
<th>R@100</th>
</tr>
</thead>
<tbody>
<tr>
<td>DPR</td>
<td>29.6</td>
<td>44.5</td>
<td>51.5</td>
<td>64.6</td>
<td>74.1</td>
</tr>
<tr>
<td>GTR</td>
<td>35.1</td>
<td>50.7</td>
<td>56.8</td>
<td>70.3</td>
<td>78.4</td>
</tr>
<tr>
<td>Oracle</td>
<td>63.8</td>
<td>72.8</td>
<td>78.4</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>Table 12: Retrieval results for ASQA (EM recall).</p>
<table>
<thead>
<tr>
<th></th>
<th>R@1</th>
<th>R@3</th>
<th>R@5</th>
<th>R@20</th>
<th>R@100</th>
</tr>
</thead>
<tbody>
<tr>
<td>DPR</td>
<td>6.7</td>
<td>13.5</td>
<td>17.6</td>
<td>30.4</td>
<td>47.6</td>
</tr>
<tr>
<td>GTR</td>
<td>14.6</td>
<td>24.7</td>
<td>31.6</td>
<td>49.7</td>
<td>65.6</td>
</tr>
<tr>
<td>Oracle</td>
<td>44.3</td>
<td>58.7</td>
<td>65.6</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>Table 13: Retrieval results for QAMPARI (recall-5).
partially supports, or does not support the factual claims in $s_{i}$. Citation $c_{i,j}$ has a citation precision of 1 if $s_{i}$ has a recall of 1 , and $c_{i,j}$ fully or partially supports $s_{i}$. Finally, we take an average of precision scores of all citations in the statement $\mathcal{S}$ to obtain the citation precision score.</p>
<h2>G More Experiments</h2>
<h2>G. 1 Retrieval Analysis</h2>
<p>Oracle. Since the original datasets do not contain gold passages at the same granularity level as our setting (100-word passages), we approximate gold passages by running the following algorithm on the top-100 retrieved passages. We first calculate the recall score for each passage. Then, we sort the passages using their recall score and take the top 5 passages as our initial oracle set. Finally, we iterate through all passages that were not initially in the oracle set and try to replace the passages in the oracle set in a greedy fashion: we calculate the change in the recall score of the oracle set for every possible replacement and proceed with the replacement that results in the largest recall improvement. The set of 5 oracle passages were able to match the recall scores of the top-100 retrieved passages.</p>
<p>Detailed retrieval results. We show detailed retrieval results in Tables 12, 13, and 14.</p>
<h2>G. 2 Effect of Instructions</h2>
<p>Table 15 shows results of using a full instruction (Table 23) and a short version of the instruction (Table 24). We see that the full version induces stronger correctness and citation recall, while the two instructions lead to similar citation precision.</p>
<table>
<thead>
<tr>
<th></th>
<th>R@1</th>
<th>R@3</th>
<th>R@5</th>
<th>R@20</th>
<th>R@100</th>
</tr>
</thead>
<tbody>
<tr>
<td>BM25</td>
<td>3.0</td>
<td>6.6</td>
<td>9.6</td>
<td>19.3</td>
<td>31.8</td>
</tr>
<tr>
<td>Oracle</td>
<td>25.3</td>
<td>29.7</td>
<td>31.8</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>Table 14: Retrieval results for ELI5 (claim recall).</p>
<table>
<thead>
<tr>
<th></th>
<th>Fluency</th>
<th>Correct.</th>
<th>Citation</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>(MAUVE)</td>
<td>(EM Rec.)</td>
<td>Rec. Prec.</td>
</tr>
<tr>
<td>ChatGPT (VANILLA, 5-doc)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Short instruction</td>
<td>64.1</td>
<td>39.5</td>
<td>69.6 73.2</td>
</tr>
<tr>
<td>Full instruction</td>
<td>66.6</td>
<td>40.4</td>
<td>73.6 72.5</td>
</tr>
</tbody>
</table>
<p>Table 15: Effect of different instructions on ASQA.</p>
<h2>G. 3 Effect of Demonstrations</h2>
<p>Table 16 shows results on effect of different numbers of demonstrations. We see that numbers of demonstrations do not affect ChatGPT's correctness but using at least one demonstration ensures high citation recall. For the original LLaMA model, Table 16 shows the trend that more demonstrations lead to better performance.</p>
<h2>G. 4 Fine-tuned Models</h2>
<p>To better understand the differences between finetuned models and prompted large language models, we train state-of-the-art question answering model, Fusion-in-Decoder (FiD; [izacard-grave-2021]), and evaluate it in conjunction with PostCite. Due to the lack of training data with citation annotation, we first train a T5-base FiD model for 5 epochs on the ASQA training set with a batch size of 64 and a learning rate of 1e-4. During evaluation, we use PostCite to add citations to the output. We also use $k=5$ passages during both training and evaluation of the FiD model.</p>
<p>Then, we evaluate this model on both ASQA (in-domain) and ELI5 (out-of-domain), and the results can be found in Tables 17 and 18. Note that this is not a direct comparison, as ALCE assumes only evaluation data available and uses only fewshot data for prompting. As the results show, the FiD baseline still significantly lags behind prompting ChatGPT in both correctness and citation quality (even though it is trained on 4000+ examples). When tested on another dataset (ELI5), FiD performs even worse, showing that it is challenging to solve the problem by fine-tuning a small pretrained model.</p>
<table>
<thead>
<tr>
<th></th>
<th>Fluency</th>
<th>Correct.</th>
<th>Citation</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>(MAUVE)</td>
<td>(EM)</td>
<td>Rec.</td>
<td>Prec.</td>
</tr>
<tr>
<td></td>
<td>ChatGPT (Vanilla)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>0.0pt $\neq$</td>
<td>74.5</td>
<td>41.9</td>
<td>69.3</td>
<td>73.4</td>
</tr>
<tr>
<td>0.0pt $\neq$</td>
<td>68.9</td>
<td>39.8</td>
<td>74.6</td>
<td>73.2</td>
</tr>
<tr>
<td>0.0pt $\neq$</td>
<td>66.6</td>
<td>40.4</td>
<td>73.6</td>
<td>72.5</td>
</tr>
</tbody>
</table>
<p>Table 16: Different demonstrations on ASQA.</p>
<table>
<thead>
<tr>
<th></th>
<th>Fluency</th>
<th>Correct.</th>
<th>Citation</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>(MAUVE)</td>
<td>(EM Rec.)</td>
<td>Rec.</td>
<td>Prec.</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>66.6</td>
<td>40.4</td>
<td>73.6</td>
<td>72.5</td>
</tr>
<tr>
<td>FiD + POSTCITE</td>
<td>75.8</td>
<td>28.4</td>
<td>58.1</td>
<td>58.0</td>
</tr>
</tbody>
</table>
<p>Table 17: Comparison of Fusion-in-Decoder with ChatGPT on ASQA. Both models use top-5 GTR passages.</p>
<h3>G.5 More Human Evaluation</h3>
<p>We evaluate the accuracy of our automatic metrics by treating the human annotations as gold labels. For citation recall, ALCE achieves an accuracy of $85.1 \%$; for citation precision, ALCE has an accuracy of $77.6 \%$. Regarding detecting insufficient citations, ALCE has a recall of $82.3 \%$ and a precision of $84.2 \%$; regarding detecting "irrelevant" citations, ALCE has a recall of $75.6 \%$ and a precision of $66.1 \%$-ALCE is effective in detecting "irrelevant" citations, but due to the limitation of the NLI model (cannot detect "partial support"), it has a relatively high false positive rate.</p>
<h3>G.6 Main Results</h3>
<p>We show full results of our experiments along with the standard deviation in Tables 19, 20, and 21. We repeat all experiments with three different random seeds. However, for ChatGPT Rerank, we use only one seeded run since each run repeats the generation step four times, and more experiments would incur significant costs. Similarly, due to the cost of running ChatGPT-16K and GPT-4, we only use one seeded run for each model.</p>
<h3>G.7 Open-source Models</h3>
<p>In addition to the open-source models discussed in the main text, we also show the results of LLaMA7B, Alpaca-7B, Vicuna-7B, LLaMA-33B, Oasst33B, and Stable Beluga 2 in the Tables 19, 20, and 21. For selected models, we also tested them using approaches beyond VANILLA.</p>
<p>Although open-source models generally lag behind the state-of-the-art models (i.e. GPT-4) in</p>
<table>
<thead>
<tr>
<th></th>
<th>Fluency</th>
<th>Correct.</th>
<th>Citation</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>(MAUVE)</td>
<td>(Claim)</td>
<td>Rec.</td>
<td>Prec.</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>57.2</td>
<td>12.0</td>
<td>51.1</td>
<td>50.0</td>
</tr>
<tr>
<td>FiD + POSTCITE</td>
<td>25.2</td>
<td>4.4</td>
<td>39.3</td>
<td>39.3</td>
</tr>
</tbody>
</table>
<p>Table 18: Comparison of Fusion-in-Decoder with ChatGPT on ELI5. Both models use top-5 GTR passages.
both correctness and citation quality, the largest instruction-following models (i.e. LLaMA-2-70BChat and Stable Beluga 2) can sometimes achieve competitive correctness to the SoTA.</p>
<p>Furthermore, we found that the open-source models follow a similar trend between different approaches as ChatGPT. Specifically, using SumM and SNIPPET improves correctness and RERANK boosts citation quality.</p>
<h2>H Prompts</h2>
<p>We show detailed prompts used in our paper in Tables 23, 24, 25, 26, 27, 28, and 29.</p>
<h2>I Examples</h2>
<p>In Tables 30 and 31 we show some examples of questions and model generated outputs.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Fluency <br> (MAUVE)</th>
<th style="text-align: center;">Correct. <br> (EM Rec.)</th>
<th style="text-align: center;">Citation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ROUGE-L</th>
<th style="text-align: center;">Length</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rec.</td>
<td style="text-align: center;">Prec.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">VANILLA (5-psg)</td>
<td style="text-align: center;">$66.8_{(2.0)}$</td>
<td style="text-align: center;">$40.4_{(0.6)}$</td>
<td style="text-align: center;">$73.6_{(1.1)}$</td>
<td style="text-align: center;">$72.5_{(1.8)}$</td>
<td style="text-align: center;">$37.0_{(0.4)}$</td>
<td style="text-align: center;">$40.0_{(3.1)}$</td>
</tr>
<tr>
<td style="text-align: center;">w/ Rerank</td>
<td style="text-align: center;">$77.0_{(-)}$</td>
<td style="text-align: center;">$40.2_{(-)}$</td>
<td style="text-align: center;">$84.8_{(-)}$</td>
<td style="text-align: center;">$81.6_{(-)}$</td>
<td style="text-align: center;">$36.9_{(-)}$</td>
<td style="text-align: center;">$40.8_{(-)}$</td>
</tr>
<tr>
<td style="text-align: center;">Summ (10-psg)</td>
<td style="text-align: center;">$70.0_{(1.2)}$</td>
<td style="text-align: center;">$43.3_{(0.8)}$</td>
<td style="text-align: center;">$68.8_{(0.6)}$</td>
<td style="text-align: center;">$61.8_{(1.1)}$</td>
<td style="text-align: center;">$36.9_{(0.2)}$</td>
<td style="text-align: center;">$49.8_{(4.3)}$</td>
</tr>
<tr>
<td style="text-align: center;">w/ Interact</td>
<td style="text-align: center;">$69.0_{(2.7)}$</td>
<td style="text-align: center;">$39.1_{(0.5)}$</td>
<td style="text-align: center;">$73.4_{(0.2)}$</td>
<td style="text-align: center;">$66.5_{(4.9)}$</td>
<td style="text-align: center;">$35.7_{(0.2)}$</td>
<td style="text-align: center;">$34.0_{(0.9)}$</td>
</tr>
<tr>
<td style="text-align: center;">SnIPPET (10-psg)</td>
<td style="text-align: center;">$69.8_{(2.5)}$</td>
<td style="text-align: center;">$41.4_{(0.6)}$</td>
<td style="text-align: center;">$65.3_{(0.6)}$</td>
<td style="text-align: center;">$57.4_{(0.9)}$</td>
<td style="text-align: center;">$36.4_{(0.4)}$</td>
<td style="text-align: center;">$43.0_{(3.5)}$</td>
</tr>
<tr>
<td style="text-align: center;">InLINESearch</td>
<td style="text-align: center;">$58.7_{(1.3)}$</td>
<td style="text-align: center;">$32.4_{(0.6)}$</td>
<td style="text-align: center;">$58.3_{(1.3)}$</td>
<td style="text-align: center;">$58.3_{(1.3)}$</td>
<td style="text-align: center;">$58.2_{(1.1)}$</td>
<td style="text-align: center;">$23.7_{(1.1)}$</td>
</tr>
<tr>
<td style="text-align: center;">ClosedBook</td>
<td style="text-align: center;">$52.7_{(4.9)}$</td>
<td style="text-align: center;">$38.2_{(0.1)}$</td>
<td style="text-align: center;">$26.7_{(1.1)}$</td>
<td style="text-align: center;">$26.7_{(1.1)}$</td>
<td style="text-align: center;">$37.1_{(0.3)}$</td>
<td style="text-align: center;">$61.1_{(4.5)}$</td>
</tr>
<tr>
<td style="text-align: center;">Oracle(5-psg)</td>
<td style="text-align: center;">$64.4_{(0.6)}$</td>
<td style="text-align: center;">$48.9_{(1.2)}$</td>
<td style="text-align: center;">$74.5_{(0.6)}$</td>
<td style="text-align: center;">$72.7_{(1.0)}$</td>
<td style="text-align: center;">$38.2_{(1.0)}$</td>
<td style="text-align: center;">$37.4_{(3.0)}$</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT-16K</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">VANILLA (5-psg)</td>
<td style="text-align: center;">$60.3_{(-)}$</td>
<td style="text-align: center;">$36.1_{(-)}$</td>
<td style="text-align: center;">$76.2_{(-)}$</td>
<td style="text-align: center;">$76.5_{(-)}$</td>
<td style="text-align: center;">$36.2_{(-)}$</td>
<td style="text-align: center;">$24.7_{(-)}$</td>
</tr>
<tr>
<td style="text-align: center;">VANILLA (10-psg)</td>
<td style="text-align: center;">$56.3_{(-)}$</td>
<td style="text-align: center;">$36.7_{(-)}$</td>
<td style="text-align: center;">$75.3_{(-)}$</td>
<td style="text-align: center;">$75.0_{(-)}$</td>
<td style="text-align: center;">$35.6_{(-)}$</td>
<td style="text-align: center;">$23.5_{(-)}$</td>
</tr>
<tr>
<td style="text-align: center;">VANILLA (20-psg)</td>
<td style="text-align: center;">$56.7_{(-)}$</td>
<td style="text-align: center;">$36.1_{(-)}$</td>
<td style="text-align: center;">$73.7_{(-)}$</td>
<td style="text-align: center;">$73.5_{(-)}$</td>
<td style="text-align: center;">$35.5_{(-)}$</td>
<td style="text-align: center;">$23.1_{(-)}$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">VANILLA (5-psg)</td>
<td style="text-align: center;">$67.1_{(-)}$</td>
<td style="text-align: center;">$41.3_{(-)}$</td>
<td style="text-align: center;">$68.5_{(-)}$</td>
<td style="text-align: center;">$75.6_{(-)}$</td>
<td style="text-align: center;">$39.2_{(-)}$</td>
<td style="text-align: center;">$31.8_{(-)}$</td>
</tr>
<tr>
<td style="text-align: center;">VANILLA (10-psg)</td>
<td style="text-align: center;">$71.5_{(-)}$</td>
<td style="text-align: center;">$43.1_{(-)}$</td>
<td style="text-align: center;">$72.0_{(-)}$</td>
<td style="text-align: center;">$75.5_{(-)}$</td>
<td style="text-align: center;">$39.7_{(-)}$</td>
<td style="text-align: center;">$33.8_{(-)}$</td>
</tr>
<tr>
<td style="text-align: center;">VANILLA (20-psg)</td>
<td style="text-align: center;">$64.9_{(-)}$</td>
<td style="text-align: center;">$44.4_{(-)}$</td>
<td style="text-align: center;">$73.0_{(-)}$</td>
<td style="text-align: center;">$76.5_{(-)}$</td>
<td style="text-align: center;">$40.1_{(-)}$</td>
<td style="text-align: center;">$34.3_{(-)}$</td>
</tr>
<tr>
<td style="text-align: center;">Open-source</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-7B VANILLA (3-psg)</td>
<td style="text-align: center;">$69.8_{(2.0)}$</td>
<td style="text-align: center;">$22.6_{(0.9)}$</td>
<td style="text-align: center;">$6.2_{(2.7)}$</td>
<td style="text-align: center;">$9.2_{(2.9)}$</td>
<td style="text-align: center;">$29.1_{(0.2)}$</td>
<td style="text-align: center;">$61.3_{(14.3)}$</td>
</tr>
<tr>
<td style="text-align: center;">Alpaca-7B VANILLA (3-psg)</td>
<td style="text-align: center;">$84.2_{(2.7)}$</td>
<td style="text-align: center;">$32.1_{(1.7)}$</td>
<td style="text-align: center;">$12.3_{(7.2)}$</td>
<td style="text-align: center;">$14.1_{(7.0)}$</td>
<td style="text-align: center;">$33.1_{(0.8)}$</td>
<td style="text-align: center;">$51.7_{(12.8)}$</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-7B VANILLA (3-psg)</td>
<td style="text-align: center;">$82.9_{(5.0)}$</td>
<td style="text-align: center;">$34.6_{(0.7)}$</td>
<td style="text-align: center;">$40.3_{(0.5)}$</td>
<td style="text-align: center;">$42.6_{(1.0)}$</td>
<td style="text-align: center;">$35.9_{(0.7)}$</td>
<td style="text-align: center;">$48.9_{(6.6)}$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-13B VANILLA (3-psg)</td>
<td style="text-align: center;">$68.4_{(6.4)}$</td>
<td style="text-align: center;">$26.9_{(0.4)}$</td>
<td style="text-align: center;">$10.6_{(4.7)}$</td>
<td style="text-align: center;">$15.4_{(5.2)}$</td>
<td style="text-align: center;">$29.8_{(0.5)}$</td>
<td style="text-align: center;">$67.1_{(19.1)}$</td>
</tr>
<tr>
<td style="text-align: center;">w/ Rerank</td>
<td style="text-align: center;">$60.9_{(14.5)}$</td>
<td style="text-align: center;">$25.2_{(2.5)}$</td>
<td style="text-align: center;">$28.1_{(9.3)}$</td>
<td style="text-align: center;">$37.0_{(7.2)}$</td>
<td style="text-align: center;">$27.9_{(2.4)}$</td>
<td style="text-align: center;">$50.5_{(14.3)}$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-13B Summ (10-psg)</td>
<td style="text-align: center;">$76.8_{(4.7)}$</td>
<td style="text-align: center;">$33.3_{(0.7)}$</td>
<td style="text-align: center;">$19.6_{(3.9)}$</td>
<td style="text-align: center;">$23.7_{(4.7)}$</td>
<td style="text-align: center;">$32.1_{(0.3)}$</td>
<td style="text-align: center;">$54.4_{(1.5)}$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-13B SnIPPET (10-psg)</td>
<td style="text-align: center;">$72.0_{(0.8)}$</td>
<td style="text-align: center;">$31.3_{(1.1)}$</td>
<td style="text-align: center;">$18.2_{(3.1)}$</td>
<td style="text-align: center;">$21.1_{(3.6)}$</td>
<td style="text-align: center;">$30.8_{(0.4)}$</td>
<td style="text-align: center;">$50.5_{(4.5)}$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-13B Oracle (3-psg)</td>
<td style="text-align: center;">$69.5_{(11.4)}$</td>
<td style="text-align: center;">$34.3_{(0.9)}$</td>
<td style="text-align: center;">$10.8_{(4.9)}$</td>
<td style="text-align: center;">$15.8_{(5.9)}$</td>
<td style="text-align: center;">$30.6_{(0.1)}$</td>
<td style="text-align: center;">$67.3_{(17.9)}$</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-13B VANILLA (3-psg)</td>
<td style="text-align: center;">$82.6_{(9.4)}$</td>
<td style="text-align: center;">$31.9_{(3.9)}$</td>
<td style="text-align: center;">$51.1_{(1.4)}$</td>
<td style="text-align: center;">$50.1_{(2.5)}$</td>
<td style="text-align: center;">$34.9_{(1.3)}$</td>
<td style="text-align: center;">$39.1_{(6.6)}$</td>
</tr>
<tr>
<td style="text-align: center;">w/ Rerank</td>
<td style="text-align: center;">$73.5_{(2.1)}$</td>
<td style="text-align: center;">$32.9_{(1.3)}$</td>
<td style="text-align: center;">$71.9_{(1.9)}$</td>
<td style="text-align: center;">$65.4_{(1.5)}$</td>
<td style="text-align: center;">$34.6_{(0.3)}$</td>
<td style="text-align: center;">$35.7_{(4.2)}$</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-13B Summ (10-psg)</td>
<td style="text-align: center;">$67.7_{(0.3)}$</td>
<td style="text-align: center;">$43.2_{(0.1)}$</td>
<td style="text-align: center;">$52.7_{(2.6)}$</td>
<td style="text-align: center;">$50.0_{(2.1)}$</td>
<td style="text-align: center;">$36.7_{(0.2)}$</td>
<td style="text-align: center;">$66.0_{(1.2)}$</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-13B SnIPPET (10-psg)</td>
<td style="text-align: center;">$81.4_{(3.0)}$</td>
<td style="text-align: center;">$42.1_{(1.2)}$</td>
<td style="text-align: center;">$53.4_{(1.9)}$</td>
<td style="text-align: center;">$48.7_{(1.6)}$</td>
<td style="text-align: center;">$36.9_{(0.4)}$</td>
<td style="text-align: center;">$61.2_{(7.4)}$</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-13B Oracle (3-psg)</td>
<td style="text-align: center;">$72.9_{(3.5)}$</td>
<td style="text-align: center;">$42.5_{(1.6)}$</td>
<td style="text-align: center;">$52.2_{(0.8)}$</td>
<td style="text-align: center;">$50.7_{(1.6)}$</td>
<td style="text-align: center;">$36.5_{(0.9)}$</td>
<td style="text-align: center;">$38.7_{(3.5)}$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-33B VANILLA (3-psg)</td>
<td style="text-align: center;">$83.7_{(5.4)}$</td>
<td style="text-align: center;">$31.0_{(0.8)}$</td>
<td style="text-align: center;">$19.5_{(5.3)}$</td>
<td style="text-align: center;">$23.0_{(5.3)}$</td>
<td style="text-align: center;">$32.3_{(0.6)}$</td>
<td style="text-align: center;">$44.1_{(9.3)}$</td>
</tr>
<tr>
<td style="text-align: center;">w/ Rerank</td>
<td style="text-align: center;">$82.1_{(3.0)}$</td>
<td style="text-align: center;">$31.3_{(1.1)}$</td>
<td style="text-align: center;">$41.3_{(6.4)}$</td>
<td style="text-align: center;">$44.7_{(5.5)}$</td>
<td style="text-align: center;">$32.5_{(0.9)}$</td>
<td style="text-align: center;">$39.4_{(8.0)}$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-33B Summ (10-psg)</td>
<td style="text-align: center;">$72.0_{(3.0)}$</td>
<td style="text-align: center;">$33.1_{(1.9)}$</td>
<td style="text-align: center;">$34.7_{(5.8)}$</td>
<td style="text-align: center;">$35.2_{(6.0)}$</td>
<td style="text-align: center;">$31.1_{(0.8)}$</td>
<td style="text-align: center;">$43.7_{(5.0)}$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-33B SnIPPET (10-psg)</td>
<td style="text-align: center;">$70.8_{(3.1)}$</td>
<td style="text-align: center;">$30.9_{(1.4)}$</td>
<td style="text-align: center;">$31.4_{(4.2)}$</td>
<td style="text-align: center;">$31.5_{(5.3)}$</td>
<td style="text-align: center;">$30.1_{(0.7)}$</td>
<td style="text-align: center;">$42.8_{(3.6)}$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-33B Oracle (3-psg)</td>
<td style="text-align: center;">$82.6_{(7.1)}$</td>
<td style="text-align: center;">$39.3_{(2.9)}$</td>
<td style="text-align: center;">$20.2_{(6.2)}$</td>
<td style="text-align: center;">$23.9_{(6.3)}$</td>
<td style="text-align: center;">$33.1_{(0.9)}$</td>
<td style="text-align: center;">$42.0_{(0.3)}$</td>
</tr>
<tr>
<td style="text-align: center;">Oasst-33B VANILLA (3-psg)</td>
<td style="text-align: center;">$82.9_{(2.7)}$</td>
<td style="text-align: center;">$34.8_{(1.5)}$</td>
<td style="text-align: center;">$36.2_{(1.7)}$</td>
<td style="text-align: center;">$38.3_{(2.7)}$</td>
<td style="text-align: center;">$35.5_{(0.7)}$</td>
<td style="text-align: center;">$45.2_{(6.3)}$</td>
</tr>
<tr>
<td style="text-align: center;">w/ Rerank</td>
<td style="text-align: center;">$83.2_{(2.4)}$</td>
<td style="text-align: center;">$35.1_{(1.4)}$</td>
<td style="text-align: center;">$66.7_{(0.2)}$</td>
<td style="text-align: center;">$64.3_{(1.0)}$</td>
<td style="text-align: center;">$35.0_{(0.6)}$</td>
<td style="text-align: center;">$41.8_{(6.0)}$</td>
</tr>
<tr>
<td style="text-align: center;">Oasst-33B Summ (10-psg)</td>
<td style="text-align: center;">$74.3_{(4.6)}$</td>
<td style="text-align: center;">$40.9_{(1.1)}$</td>
<td style="text-align: center;">$45.5_{(1.9)}$</td>
<td style="text-align: center;">$44.0_{(2.9)}$</td>
<td style="text-align: center;">$35.8_{(0.6)}$</td>
<td style="text-align: center;">$54.3_{(4.8)}$</td>
</tr>
<tr>
<td style="text-align: center;">Oasst-33B SnIPPET (10-psg)</td>
<td style="text-align: center;">$79.3_{(1.0)}$</td>
<td style="text-align: center;">$40.1_{(0.9)}$</td>
<td style="text-align: center;">$45.0_{(1.3)}$</td>
<td style="text-align: center;">$43.3_{(2.2)}$</td>
<td style="text-align: center;">$35.8_{(0.2)}$</td>
<td style="text-align: center;">$50.9_{(4.1)}$</td>
</tr>
<tr>
<td style="text-align: center;">Oasst-33B Oracle (3-psg)</td>
<td style="text-align: center;">$85.1_{(2.8)}$</td>
<td style="text-align: center;">$44.3_{(2.4)}$</td>
<td style="text-align: center;">$37.0_{(1.0)}$</td>
<td style="text-align: center;">$39.6_{(1.5)}$</td>
<td style="text-align: center;">$36.5_{(1.1)}$</td>
<td style="text-align: center;">$44.2_{(5.8)}$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2-7B-Chat VANILLA (5-psg)</td>
<td style="text-align: center;">$80.1_{(6.5)}$</td>
<td style="text-align: center;">$33.9_{(2.1)}$</td>
<td style="text-align: center;">$50.9_{(4.5)}$</td>
<td style="text-align: center;">$47.5_{(3.7)}$</td>
<td style="text-align: center;">$35.1_{(0.9)}$</td>
<td style="text-align: center;">$42.3_{(10.1)}$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2-13B-Chat VANILLA (5-psg)</td>
<td style="text-align: center;">$72.4_{(6.3)}$</td>
<td style="text-align: center;">$35.2_{(1.2)}$</td>
<td style="text-align: center;">$38.4_{(5.9)}$</td>
<td style="text-align: center;">$39.4_{(4.8)}$</td>
<td style="text-align: center;">$35.8_{(0.9)}$</td>
<td style="text-align: center;">$38.0_{(0.4)}$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2-70B-Chat VANILLA (5-psg)</td>
<td style="text-align: center;">$88.3_{(4.1)}$</td>
<td style="text-align: center;">$41.5_{(0.8)}$</td>
<td style="text-align: center;">$62.9_{(1.4)}$</td>
<td style="text-align: center;">$61.3_{(2.1)}$</td>
<td style="text-align: center;">$37.1_{(0.4)}$</td>
<td style="text-align: center;">$52.9_{(9.5)}$</td>
</tr>
<tr>
<td style="text-align: center;">Stable Beluga 2 VANILLA (5-psg)</td>
<td style="text-align: center;">$59.7_{(3.3)}$</td>
<td style="text-align: center;">$37.2_{(1.0)}$</td>
<td style="text-align: center;">$63.6_{(0.7)}$</td>
<td style="text-align: center;">$63.5_{(0.1)}$</td>
<td style="text-align: center;">$35.3_{(0.7)}$</td>
<td style="text-align: center;">$23.8_{(3.2)}$</td>
</tr>
</tbody>
</table>
<p>Table 19: ASQA full results.</p>
<table>
<thead>
<tr>
<th></th>
<th>Correctness</th>
<th></th>
<th>Citation</th>
<th></th>
<th>Num Pred.</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Rec.-5</td>
<td>Prec.</td>
<td>Rec.</td>
<td>Prec.</td>
<td></td>
</tr>
<tr>
<td>ChatGPT</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Vanilla (5-psg)</td>
<td>$20.8_{(2.2)}$</td>
<td>$20.8_{(0.2)}$</td>
<td>$20.5_{(0.7)}$</td>
<td>$20.9_{(0.7)}$</td>
<td>$5.0_{(0.5)}$</td>
</tr>
<tr>
<td>w/ Rerank</td>
<td>$22.8(-)$</td>
<td>$21.4(-)$</td>
<td>$21.2(-)$</td>
<td>$21.4(-)$</td>
<td>$5.4(-)$</td>
</tr>
<tr>
<td>Summ (10-psg)</td>
<td>$23.6_{(0.9)}$</td>
<td>$21.2_{(0.5)}$</td>
<td>$23.6_{(0.7)}$</td>
<td>$25.7_{(0.8)}$</td>
<td>$6.7_{(0.4)}$</td>
</tr>
<tr>
<td>Snippet (10-psg)</td>
<td>$24.5_{(1.4)}$</td>
<td>$21.5_{(1.8)}$</td>
<td>$22.9_{(1.6)}$</td>
<td>$24.9_{(0.4)}$</td>
<td>$7.2_{(0.9)}$</td>
</tr>
<tr>
<td>w/ Interact</td>
<td>$21.9_{(0.9)}$</td>
<td>$23.0_{(0.4)}$</td>
<td>$21.9_{(1.2)}$</td>
<td>$23.4_{(0.9)}$</td>
<td>$6.7_{(0.4)}$</td>
</tr>
<tr>
<td>InLineSearch</td>
<td>$17.2_{(1.1)}$</td>
<td>$20.4_{(0.8)}$</td>
<td>$14.9_{(0.8)}$</td>
<td>$14.9_{(0.8)}$</td>
<td>$6.7_{(0.2)}$</td>
</tr>
<tr>
<td>ClosedBook</td>
<td>$32.9_{(1.1)}$</td>
<td>$19.8_{(1.6)}$</td>
<td>$10.0_{(0.4)}$</td>
<td>$10.0_{(0.4)}$</td>
<td>$17.0_{(2.9)}$</td>
</tr>
<tr>
<td>Oracle</td>
<td>$37.0_{(3.1)}$</td>
<td>$36.9_{(0.6)}$</td>
<td>$24.1_{(1.2)}$</td>
<td>$24.6_{(1.3)}$</td>
<td>$5.3_{(0.6)}$</td>
</tr>
<tr>
<td>ChatGPT-16K</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Vanilla (5-psg)</td>
<td>$21.1(-)$</td>
<td>$22.0(-)$</td>
<td>$20.7(-)$</td>
<td>$21.2(-)$</td>
<td>$4.9(-)$</td>
</tr>
<tr>
<td>Vanilla (10-psg)</td>
<td>$23.4(-)$</td>
<td>$21.9(-)$</td>
<td>$21.6(-)$</td>
<td>$22.0(-)$</td>
<td>$5.7(-)$</td>
</tr>
<tr>
<td>Vanilla (20-psg)</td>
<td>$26.4(-)$</td>
<td>$21.1(-)$</td>
<td>$19.4(-)$</td>
<td>$19.7(-)$</td>
<td>$7.6(-)$</td>
</tr>
<tr>
<td>GPT-4</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Vanilla (5-psg)</td>
<td>$22.2(-)$</td>
<td>$25.0(-)$</td>
<td>$25.9(-)$</td>
<td>$27.0(-)$</td>
<td>$4.4(-)$</td>
</tr>
<tr>
<td>Vanilla (10-psg)</td>
<td>$26.8(-)$</td>
<td>$25.1(-)$</td>
<td>$26.2(-)$</td>
<td>$27.2(-)$</td>
<td>$5.7(-)$</td>
</tr>
<tr>
<td>Vanilla (20-psg)</td>
<td>$29.6(-)$</td>
<td>$26.2(-)$</td>
<td>$27.4(-)$</td>
<td>$28.5(-)$</td>
<td>$6.8(-)$</td>
</tr>
<tr>
<td>Open-source</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LLaMA-7B Vanilla (3-psg)</td>
<td>$7.8_{(3.4)}$</td>
<td>$7.4_{(2.7)}$</td>
<td>$5.1_{(0.5)}$</td>
<td>$5.7_{(0.8)}$</td>
<td>$5.7_{(0.6)}$</td>
</tr>
<tr>
<td>Alpaca-7B Vanilla (3-psg)</td>
<td>$9.4_{(3.7)}$</td>
<td>$9.5_{(3.6)}$</td>
<td>$6.4_{(0.5)}$</td>
<td>$6.8_{(0.5)}$</td>
<td>$5.1_{(0.1)}$</td>
</tr>
<tr>
<td>Vicuna-7B Vanilla (3-psg)</td>
<td>$11.3_{(1.4)}$</td>
<td>$13.3_{(2.3)}$</td>
<td>$10.1_{(0.6)}$</td>
<td>$10.9_{(0.5)}$</td>
<td>$3.9_{(0.3)}$</td>
</tr>
<tr>
<td>LLaMA-13B Vanilla (3-psg)</td>
<td>$9.7_{(3.6)}$</td>
<td>$9.1_{(3.1)}$</td>
<td>$6.7_{(0.9)}$</td>
<td>$7.1_{(0.9)}$</td>
<td>$5.9_{(0.6)}$</td>
</tr>
<tr>
<td>w/ Rerank</td>
<td>$10.0_{(3.3)}$</td>
<td>$10.7_{(3.3)}$</td>
<td>$9.9_{(1.2)}$</td>
<td>$10.2_{(1.1)}$</td>
<td>$5.4_{(0.5)}$</td>
</tr>
<tr>
<td>LLaMA-13B Summ (10-psg)</td>
<td>$14.8_{(2.5)}$</td>
<td>$12.6_{(1.5)}$</td>
<td>$7.4_{(0.5)}$</td>
<td>$8.0_{(0.6)}$</td>
<td>$8.1_{(0.9)}$</td>
</tr>
<tr>
<td>LLaMA-13B Snippet (10-psg)</td>
<td>$17.7_{(1.4)}$</td>
<td>$15.7_{(0.9)}$</td>
<td>$8.8_{(0.7)}$</td>
<td>$9.9_{(0.6)}$</td>
<td>$8.2_{(0.4)}$</td>
</tr>
<tr>
<td>LLaMA-13B Oracle (3-psg)</td>
<td>$16.8_{(6.6)}$</td>
<td>$15.4_{(5.6)}$</td>
<td>$7.7_{(1.0)}$</td>
<td>$8.3_{(1.1)}$</td>
<td>$5.7_{(0.7)}$</td>
</tr>
<tr>
<td>Vicuna-13B Vanilla (3-psg)</td>
<td>$14.0_{(0.6)}$</td>
<td>$15.9_{(1.7)}$</td>
<td>$12.5_{(0.8)}$</td>
<td>$13.4_{(0.7)}$</td>
<td>$4.7_{(0.3)}$</td>
</tr>
<tr>
<td>w/ Rerank</td>
<td>$13.0_{(0.7)}$</td>
<td>$17.2_{(2.2)}$</td>
<td>$17.3_{(0.8)}$</td>
<td>$17.7_{(0.6)}$</td>
<td>$4.4_{(0.3)}$</td>
</tr>
<tr>
<td>Vicuna-13B Summ (10-psg)</td>
<td>$21.1_{(1.4)}$</td>
<td>$17.1_{(0.3)}$</td>
<td>$15.7_{(0.2)}$</td>
<td>$17.8_{(0.1)}$</td>
<td>$6.9_{(0.7)}$</td>
</tr>
<tr>
<td>Vicuna-13B Snippet (10-psg)</td>
<td>$21.9_{(0.8)}$</td>
<td>$18.2_{(0.3)}$</td>
<td>$16.8_{(0.3)}$</td>
<td>$19.7_{(0.6)}$</td>
<td>$7.5_{(0.4)}$</td>
</tr>
<tr>
<td>Vicuna-13B Oracle (3-psg)</td>
<td>$25.9_{(1.6)}$</td>
<td>$28.4_{(2.6)}$</td>
<td>$15.8_{(1.4)}$</td>
<td>$16.8_{(1.4)}$</td>
<td>$4.9_{(0.5)}$</td>
</tr>
<tr>
<td>LLaMA-33B Vanilla (3-psg)</td>
<td>$14.7_{(3.3)}$</td>
<td>$12.0_{(2.2)}$</td>
<td>$7.9_{(0.7)}$</td>
<td>$8.3_{(0.6)}$</td>
<td>$7.2_{(0.7)}$</td>
</tr>
<tr>
<td>w/ Rerank</td>
<td>$14.0_{(3.4)}$</td>
<td>$13.9_{(2.6)}$</td>
<td>$10.7_{(0.6)}$</td>
<td>$11.1_{(0.5)}$</td>
<td>$6.4_{(0.7)}$</td>
</tr>
<tr>
<td>LLaMA-33B Summ (10-psg)</td>
<td>$19.0_{(1.9)}$</td>
<td>$14.8_{(0.8)}$</td>
<td>$12.5_{(0.2)}$</td>
<td>$15.0_{(0.3)}$</td>
<td>$7.6_{(0.6)}$</td>
</tr>
<tr>
<td>LLaMA-33B Snippet (10-psg)</td>
<td>$19.6_{(1.1)}$</td>
<td>$15.7_{(0.1)}$</td>
<td>$12.8_{(1.1)}$</td>
<td>$15.2_{(1.2)}$</td>
<td>$7.8_{(0.5)}$</td>
</tr>
<tr>
<td>LLaMA-33B Oracle (3-psg)</td>
<td>$23.9_{(6.9)}$</td>
<td>$20.3_{(5.2)}$</td>
<td>$9.8_{(1.2)}$</td>
<td>$10.4_{(1.2)}$</td>
<td>$6.8_{(0.9)}$</td>
</tr>
<tr>
<td>Oasst-33B Vanilla (3-psg)</td>
<td>$15.5_{(1.5)}$</td>
<td>$14.9_{(1.4)}$</td>
<td>$9.0_{(1.6)}$</td>
<td>$10.1_{(1.8)}$</td>
<td>$5.6_{(0.3)}$</td>
</tr>
<tr>
<td>w/ Rerank</td>
<td>$14.1_{(1.1)}$</td>
<td>$15.8_{(1.0)}$</td>
<td>$15.0_{(1.6)}$</td>
<td>$15.9_{(1.6)}$</td>
<td>$4.7_{(0.3)}$</td>
</tr>
<tr>
<td>Oasst-33B Summ (10-psg)</td>
<td>$21.0_{(0.6)}$</td>
<td>$17.5_{(1.0)}$</td>
<td>$12.9_{(1.2)}$</td>
<td>$16.6_{(1.2)}$</td>
<td>$7.1_{(0.4)}$</td>
</tr>
<tr>
<td>Oasst-33B Snippet (10-psg)</td>
<td>$22.0_{(0.4)}$</td>
<td>$17.4_{(0.3)}$</td>
<td>$13.6_{(1.7)}$</td>
<td>$17.7_{(1.6)}$</td>
<td>$7.5_{(0.1)}$</td>
</tr>
<tr>
<td>Oasst-33B Oracle (3-psg)</td>
<td>$26.9_{(3.7)}$</td>
<td>$26.0_{(3.3)}$</td>
<td>$11.7_{(1.0)}$</td>
<td>$12.9_{(1.2)}$</td>
<td>$5.6_{(0.4)}$</td>
</tr>
<tr>
<td>LLaMA-2-7B-Chat Vanilla (5-psg)</td>
<td>$16.2_{(1.3)}$</td>
<td>$15.3_{(1.6)}$</td>
<td>$10.6_{(0.9)}$</td>
<td>$10.9_{(1.0)}$</td>
<td>$5.5_{(0.0)}$</td>
</tr>
<tr>
<td>LLaMA-2-13B-Chat Vanilla (5-psg)</td>
<td>$21.1_{(0.9)}$</td>
<td>$18.2_{(0.5)}$</td>
<td>$9.6_{(1.5)}$</td>
<td>$9.7_{(1.5)}$</td>
<td>$6.5_{(0.3)}$</td>
</tr>
<tr>
<td>LLaMA-2-70B-Chat Vanilla (5-psg)</td>
<td>$21.8_{(0.7)}$</td>
<td>$18.4_{(0.1)}$</td>
<td>$15.1_{(1.2)}$</td>
<td>$15.6_{(1.3)}$</td>
<td>$7.1_{(0.2)}$</td>
</tr>
<tr>
<td>Stable Beluga 2</td>
<td>$19.7_{(0.7)}$</td>
<td>$21.2_{(1.4)}$</td>
<td>$18.6_{(1.2)}$</td>
<td>$20.7_{(1.1)}$</td>
<td>$5.1_{(0.3)}$</td>
</tr>
</tbody>
</table>
<p>Table 20: QAMPARI full results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Fluency <br> (MAUVE)</th>
<th style="text-align: center;">Correct. <br> (Claim)</th>
<th style="text-align: center;">Citation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ROUGE-L</th>
<th style="text-align: center;">Length</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rec.</td>
<td style="text-align: center;">Prec.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Vanilla (5-psg)</td>
<td style="text-align: center;">$57.2_{(1.6)}$</td>
<td style="text-align: center;">$12.0_{(0.6)}$</td>
<td style="text-align: center;">$51.1_{(4.2)}$</td>
<td style="text-align: center;">$50.0_{(4.8)}$</td>
<td style="text-align: center;">$20.6_{(0.2)}$</td>
<td style="text-align: center;">$91.5_{(6.5)}$</td>
</tr>
<tr>
<td style="text-align: center;">w/ Rerank</td>
<td style="text-align: center;">$56.1(-)$</td>
<td style="text-align: center;">$11.4(-)$</td>
<td style="text-align: center;">$69.3(-)$</td>
<td style="text-align: center;">$67.8(-)$</td>
<td style="text-align: center;">$20.3(-)$</td>
<td style="text-align: center;">$103.4(-)$</td>
</tr>
<tr>
<td style="text-align: center;">Summ (10-psg)</td>
<td style="text-align: center;">$40.2_{(1.2)}$</td>
<td style="text-align: center;">$12.5_{(0.2)}$</td>
<td style="text-align: center;">$51.5_{(1.1)}$</td>
<td style="text-align: center;">$48.2_{(2.0)}$</td>
<td style="text-align: center;">$20.3_{(0.1)}$</td>
<td style="text-align: center;">$90.0_{(6.6)}$</td>
</tr>
<tr>
<td style="text-align: center;">Snippet (10-psg)</td>
<td style="text-align: center;">$62.9_{(2.2)}$</td>
<td style="text-align: center;">$14.3_{(0.1)}$</td>
<td style="text-align: center;">$50.4_{(1.1)}$</td>
<td style="text-align: center;">$45.0_{(2. w)}$</td>
<td style="text-align: center;">$21.0_{(0.1)}$</td>
<td style="text-align: center;">$100.0_{(6.8)}$</td>
</tr>
<tr>
<td style="text-align: center;">w/ Interact</td>
<td style="text-align: center;">$68.0_{(5.8)}$</td>
<td style="text-align: center;">$13.3_{(0.5)}$</td>
<td style="text-align: center;">$47.8_{(3.3)}$</td>
<td style="text-align: center;">$45.0_{(3.1)}$</td>
<td style="text-align: center;">$20.1_{(0.2)}$</td>
<td style="text-align: center;">$99.8_{(8.1)}$</td>
</tr>
<tr>
<td style="text-align: center;">InLineSearch</td>
<td style="text-align: center;">$49.7_{(4.6)}$</td>
<td style="text-align: center;">$13.4_{(1.1)}$</td>
<td style="text-align: center;">$45.6_{(2.5)}$</td>
<td style="text-align: center;">$43.7_{(3.9)}$</td>
<td style="text-align: center;">$20.4_{(0.3)}$</td>
<td style="text-align: center;">$103.0_{(18.1)}$</td>
</tr>
<tr>
<td style="text-align: center;">ClosedBook</td>
<td style="text-align: center;">$32.6_{(1.1)}$</td>
<td style="text-align: center;">$18.6_{(0.5)}$</td>
<td style="text-align: center;">$15.4_{(0.3)}$</td>
<td style="text-align: center;">$15.4_{(0.3)}$</td>
<td style="text-align: center;">$22.8_{(0.1)}$</td>
<td style="text-align: center;">$108.3_{(8.9)}$</td>
</tr>
<tr>
<td style="text-align: center;">Oracle (5-psg)</td>
<td style="text-align: center;">$59.4_{(4.1)}$</td>
<td style="text-align: center;">$21.3_{(0.2)}$</td>
<td style="text-align: center;">$57.8_{(3.7)}$</td>
<td style="text-align: center;">$56.0_{(3.8)}$</td>
<td style="text-align: center;">$21.2_{(0.3)}$</td>
<td style="text-align: center;">$93.0_{(7.8)}$</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT-16K</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Vanilla (5-psg)</td>
<td style="text-align: center;">$31.6(-)$</td>
<td style="text-align: center;">$14.4(-)$</td>
<td style="text-align: center;">$44.6(-)$</td>
<td style="text-align: center;">$44.1(-)$</td>
<td style="text-align: center;">$21.4(-)$</td>
<td style="text-align: center;">$87.6(-)$</td>
</tr>
<tr>
<td style="text-align: center;">Vanilla (10-psg)</td>
<td style="text-align: center;">$26.6(-)$</td>
<td style="text-align: center;">$14.4(-)$</td>
<td style="text-align: center;">$45.5(-)$</td>
<td style="text-align: center;">$43.3(-)$</td>
<td style="text-align: center;">$21.5(-)$</td>
<td style="text-align: center;">$87.5(-)$</td>
</tr>
<tr>
<td style="text-align: center;">Vanilla (20-psg)</td>
<td style="text-align: center;">$31.6(-)$</td>
<td style="text-align: center;">$15.9(-)$</td>
<td style="text-align: center;">$43.4(-)$</td>
<td style="text-align: center;">$40.9(-)$</td>
<td style="text-align: center;">$21.7(-)$</td>
<td style="text-align: center;">$92.6(-)$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Vanilla (5-psg)</td>
<td style="text-align: center;">$38.4(-)$</td>
<td style="text-align: center;">$14.2(-)$</td>
<td style="text-align: center;">$44.0(-)$</td>
<td style="text-align: center;">$50.1(-)$</td>
<td style="text-align: center;">$20.6(-)$</td>
<td style="text-align: center;">$79.6(-)$</td>
</tr>
<tr>
<td style="text-align: center;">Vanilla (10-psg)</td>
<td style="text-align: center;">$39.9(-)$</td>
<td style="text-align: center;">$15.7(-)$</td>
<td style="text-align: center;">$49.5(-)$</td>
<td style="text-align: center;">$54.2(-)$</td>
<td style="text-align: center;">$21.2(-)$</td>
<td style="text-align: center;">$88.2(-)$</td>
</tr>
<tr>
<td style="text-align: center;">Vanilla (20-psg)</td>
<td style="text-align: center;">$41.5(-)$</td>
<td style="text-align: center;">$18.3(-)$</td>
<td style="text-align: center;">$48.5(-)$</td>
<td style="text-align: center;">$53.4(-)$</td>
<td style="text-align: center;">$22.2(-)$</td>
<td style="text-align: center;">$97.0(-)$</td>
</tr>
<tr>
<td style="text-align: center;">Open-source</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-7B Vanilla (3-psg)</td>
<td style="text-align: center;">$28.6_{(17.9)}$</td>
<td style="text-align: center;">$1.6_{(0.9)}$</td>
<td style="text-align: center;">$1.2_{(0.0)}$</td>
<td style="text-align: center;">$2.7_{(0.1)}$</td>
<td style="text-align: center;">$12.2_{(1.3)}$</td>
<td style="text-align: center;">$46.9_{(1.2)}$</td>
</tr>
<tr>
<td style="text-align: center;">Alpaca-7B Vanilla (3-psg)</td>
<td style="text-align: center;">$45.9_{(5.3)}$</td>
<td style="text-align: center;">$9.2_{(0.1)}$</td>
<td style="text-align: center;">$4.5_{(1.6)}$</td>
<td style="text-align: center;">$5.2_{(1.9)}$</td>
<td style="text-align: center;">$18.8_{(0.3)}$</td>
<td style="text-align: center;">$67.1_{(1.2)}$</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-7B Vanilla (3-psg)</td>
<td style="text-align: center;">$43.2_{(3.9)}$</td>
<td style="text-align: center;">$10.0_{(0.5)}$</td>
<td style="text-align: center;">$12.6_{(2.3)}$</td>
<td style="text-align: center;">$16.3_{(2.6)}$</td>
<td style="text-align: center;">$19.1_{(0.4)}$</td>
<td style="text-align: center;">$68.7_{(2.0)}$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-13B Vanilla (3-psg)</td>
<td style="text-align: center;">$50.0_{(2.0)}$</td>
<td style="text-align: center;">$3.9_{(0.4)}$</td>
<td style="text-align: center;">$3.1_{(0.9)}$</td>
<td style="text-align: center;">$5.3_{(1.3)}$</td>
<td style="text-align: center;">$16.1_{(0.5)}$</td>
<td style="text-align: center;">$63.3_{(2.0)}$</td>
</tr>
<tr>
<td style="text-align: center;">w/ Rerank</td>
<td style="text-align: center;">$46.7_{(2.9)}$</td>
<td style="text-align: center;">$4.3_{(0.4)}$</td>
<td style="text-align: center;">$9.7_{(2.1)}$</td>
<td style="text-align: center;">$15.0_{(2.2)}$</td>
<td style="text-align: center;">$16.1_{(0.7)}$</td>
<td style="text-align: center;">$63.0_{(2.3)}$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-13B Summ (10-psg)</td>
<td style="text-align: center;">$28.6_{(1.8)}$</td>
<td style="text-align: center;">$2.9_{(0.1)}$</td>
<td style="text-align: center;">$2.5_{(0.8)}$</td>
<td style="text-align: center;">$3.8_{(0.8)}$</td>
<td style="text-align: center;">$8.5_{(0.3)}$</td>
<td style="text-align: center;">$33.1_{(0.6)}$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-13B Snippet (10-psg)</td>
<td style="text-align: center;">$48.4_{(3.1)}$</td>
<td style="text-align: center;">$5.7_{(0.9)}$</td>
<td style="text-align: center;">$5.8_{(0.6)}$</td>
<td style="text-align: center;">$7.6_{(0.9)}$</td>
<td style="text-align: center;">$15.1_{(1.1)}$</td>
<td style="text-align: center;">$60.2_{(3.2)}$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-13B Oracle (3-psg)</td>
<td style="text-align: center;">$49.5_{(2.4)}$</td>
<td style="text-align: center;">$6.4_{(0.6)}$</td>
<td style="text-align: center;">$3.7_{(0.7)}$</td>
<td style="text-align: center;">$6.5_{(1.0)}$</td>
<td style="text-align: center;">$16.8_{(0.5)}$</td>
<td style="text-align: center;">$64.5_{(1.7)}$</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-13B Vanilla (3-psg)</td>
<td style="text-align: center;">$58.2_{(25.1)}$</td>
<td style="text-align: center;">$10.0_{(0.3)}$</td>
<td style="text-align: center;">$15.6_{(2.2)}$</td>
<td style="text-align: center;">$19.6_{(2.0)}$</td>
<td style="text-align: center;">$19.1_{(0.3)}$</td>
<td style="text-align: center;">$69.6_{(0.6)}$</td>
</tr>
<tr>
<td style="text-align: center;">w/ Rerank</td>
<td style="text-align: center;">$45.9_{(4.3)}$</td>
<td style="text-align: center;">$9.2_{(0.0)}$</td>
<td style="text-align: center;">$31.7_{(2.9)}$</td>
<td style="text-align: center;">$38.2_{(1.6)}$</td>
<td style="text-align: center;">$18.6_{(0.5)}$</td>
<td style="text-align: center;">$69.7_{(1.0)}$</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-13B Summ (10-psg)</td>
<td style="text-align: center;">$22.4_{(3.0)}$</td>
<td style="text-align: center;">$4.9_{(0.1)}$</td>
<td style="text-align: center;">$9.7_{(1.3)}$</td>
<td style="text-align: center;">$12.2_{(1.2)}$</td>
<td style="text-align: center;">$9.3_{(0.4)}$</td>
<td style="text-align: center;">$33.0_{(3.7)}$</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-13B Snippet (10-psg)</td>
<td style="text-align: center;">$48.1_{(5.3)}$</td>
<td style="text-align: center;">$11.2_{(1.4)}$</td>
<td style="text-align: center;">$27.2_{(3.6)}$</td>
<td style="text-align: center;">$27.9_{(1.9)}$</td>
<td style="text-align: center;">$18.4_{(1.9)}$</td>
<td style="text-align: center;">$76.8_{(8.7)}$</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-13B Oracle (3-psg)</td>
<td style="text-align: center;">$41.6_{(3.1)}$</td>
<td style="text-align: center;">$17.1_{(0.4)}$</td>
<td style="text-align: center;">$20.2_{(3.0)}$</td>
<td style="text-align: center;">$26.5_{(3.0)}$</td>
<td style="text-align: center;">$20.0_{(0.3)}$</td>
<td style="text-align: center;">$72.0_{(0.3)}$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-33B Vanilla (3-psg)</td>
<td style="text-align: center;">$58.8_{(4.3)}$</td>
<td style="text-align: center;">$6.2_{(0.0)}$</td>
<td style="text-align: center;">$9.3_{(3.0)}$</td>
<td style="text-align: center;">$12.1_{(4.2)}$</td>
<td style="text-align: center;">$16.9_{(0.2)}$</td>
<td style="text-align: center;">$60.0_{(1.3)}$</td>
</tr>
<tr>
<td style="text-align: center;">w/ Rerank</td>
<td style="text-align: center;">$65.9_{(2.5)}$</td>
<td style="text-align: center;">$6.0_{(0.7)}$</td>
<td style="text-align: center;">$22.5_{(5.2)}$</td>
<td style="text-align: center;">$26.1_{(6.9)}$</td>
<td style="text-align: center;">$17.5_{(0.4)}$</td>
<td style="text-align: center;">$61.0_{(1.2)}$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-33B Summ (10-psg)</td>
<td style="text-align: center;">$23.3_{(2.0)}$</td>
<td style="text-align: center;">$3.0_{(0.2)}$</td>
<td style="text-align: center;">$6.2_{(0.5)}$</td>
<td style="text-align: center;">$8.2_{(0.7)}$</td>
<td style="text-align: center;">$7.5_{(0.4)}$</td>
<td style="text-align: center;">$26.2_{(2.3)}$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-33B Snippet (10-psg)</td>
<td style="text-align: center;">$53.2_{(4.0)}$</td>
<td style="text-align: center;">$7.4_{(1.3)}$</td>
<td style="text-align: center;">$13.7_{(0.5)}$</td>
<td style="text-align: center;">$15.1_{(0.4)}$</td>
<td style="text-align: center;">$14.4_{(1.7)}$</td>
<td style="text-align: center;">$53.3_{(8.5)}$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-33B Oracle (3-psg)</td>
<td style="text-align: center;">$63.7_{(2.8)}$</td>
<td style="text-align: center;">$11.4_{(0.5)}$</td>
<td style="text-align: center;">$11.9_{(2.6)}$</td>
<td style="text-align: center;">$15.4_{(3.6)}$</td>
<td style="text-align: center;">$17.9_{(0.2)}$</td>
<td style="text-align: center;">$61.7_{(2.6)}$</td>
</tr>
<tr>
<td style="text-align: center;">Oasst-33B Vanilla (3-psg)</td>
<td style="text-align: center;">$46.8_{(7.6)}$</td>
<td style="text-align: center;">$9.5_{(0.2)}$</td>
<td style="text-align: center;">$16.0_{(2.5)}$</td>
<td style="text-align: center;">$21.6_{(3.5)}$</td>
<td style="text-align: center;">$18.6_{(0.3)}$</td>
<td style="text-align: center;">$67.8_{(1.5)}$</td>
</tr>
<tr>
<td style="text-align: center;">w/ Rerank</td>
<td style="text-align: center;">$52.1_{(6.1)}$</td>
<td style="text-align: center;">$8.5_{(0.5)}$</td>
<td style="text-align: center;">$34.4_{(2.9)}$</td>
<td style="text-align: center;">$41.5_{(2.5)}$</td>
<td style="text-align: center;">$18.2_{(0.3)}$</td>
<td style="text-align: center;">$67.0_{(1.5)}$</td>
</tr>
<tr>
<td style="text-align: center;">Oasst-33B Summ (10-psg)</td>
<td style="text-align: center;">$24.8_{(2.8)}$</td>
<td style="text-align: center;">$3.9_{(0.3)}$</td>
<td style="text-align: center;">$12.3_{(0.2)}$</td>
<td style="text-align: center;">$16.3_{(0.3)}$</td>
<td style="text-align: center;">$9.1_{(0.3)}$</td>
<td style="text-align: center;">$31.6_{(1.5)}$</td>
</tr>
<tr>
<td style="text-align: center;">Oasst-33B Snippet (10-psg)</td>
<td style="text-align: center;">$50.7_{(4.6)}$</td>
<td style="text-align: center;">$10.7_{(1.2)}$</td>
<td style="text-align: center;">$25.8_{(3.3)}$</td>
<td style="text-align: center;">$26.7_{(2.3)}$</td>
<td style="text-align: center;">$17.8_{(1.8)}$</td>
<td style="text-align: center;">$69.6_{(8.6)}$</td>
</tr>
<tr>
<td style="text-align: center;">Oasst-33B Oracle (3-psg)</td>
<td style="text-align: center;">$50.7_{(12.1)}$</td>
<td style="text-align: center;">$15.8_{(0.1)}$</td>
<td style="text-align: center;">$20.8_{(2.8)}$</td>
<td style="text-align: center;">$28.0_{(3.2)}$</td>
<td style="text-align: center;">$19.4_{(0.1)}$</td>
<td style="text-align: center;">$70.3_{(1.1)}$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2-7B-Chat Vanilla (5-psg)</td>
<td style="text-align: center;">$27.8_{(3.0)}$</td>
<td style="text-align: center;">$10.9_{(0.2)}$</td>
<td style="text-align: center;">$19.8_{(1.2)}$</td>
<td style="text-align: center;">$15.0_{(1.4)}$</td>
<td style="text-align: center;">$20.5_{(0.2)}$</td>
<td style="text-align: center;">$87.8_{(8.1)}$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2-13B-Chat Vanilla (5-psg)</td>
<td style="text-align: center;">$34.7_{(1.5)}$</td>
<td style="text-align: center;">$13.4_{(0.4)}$</td>
<td style="text-align: center;">$17.3_{(1.3)}$</td>
<td style="text-align: center;">$15.8_{(1.4)}$</td>
<td style="text-align: center;">$20.9_{(0.2)}$</td>
<td style="text-align: center;">$88.3_{(6.3)}$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2-70B-Chat Vanilla (5-psg)</td>
<td style="text-align: center;">$38.6_{(4.8)}$</td>
<td style="text-align: center;">$12.8_{(1.0)}$</td>
<td style="text-align: center;">$38.3_{(2.4)}$</td>
<td style="text-align: center;">$37.9_{(1.9)}$</td>
<td style="text-align: center;">$21.3_{(0.1)}$</td>
<td style="text-align: center;">$110.8_{(5.6)}$</td>
</tr>
<tr>
<td style="text-align: center;">StableBeluga2</td>
<td style="text-align: center;">$33.0_{(4.4)}$</td>
<td style="text-align: center;">$14.0_{(0.5)}$</td>
<td style="text-align: center;">$27.9_{(2.7)}$</td>
<td style="text-align: center;">$29.0_{(2.7)}$</td>
<td style="text-align: center;">$20.6_{(0.2)}$</td>
<td style="text-align: center;">$75.6_{(5.0)}$</td>
</tr>
</tbody>
</table>
<p>Table 21: ELI5 full results.</p>
<p>Read the original question and passage, and generate 3 additional claims that are supported by the passage and answer the question.</p>
<p>Original question: What's the difference between Shia vs. Sunni Islam?
Passage: The main difference between Shia and Sunni Muslim is related to ideological heritage and issues of leadership. This difference is first formed after the death of the Prophet Muhammad in 632 A.D. The ideological practice of the Sunni branch strictly follows Prophet Muhammad and his teachings, while the Shia branch follows Prophet Muhammad's son-in-law Ali. Nowadays, Sunni and Shia are the major branches of Islam.
Claim 1: The major branches of Islam are Sunni and Shia.
Claim 2: Prophet Muhammad died in 632 A.D.
Claim 3: The ideological practice of the Sunni branch strictly follows Prophet Muhammad and his teachings.</p>
<p>Original question: What causes Bi-polar disorder?
Passage: Bipolar disorder is an emotional disorder that causes extreme mood swings between excitement and depression. The spectrum of mood swing may span from days to months. We are still not certain of the exact factors that cause such disorder, but genetics is considered a major factor. Claim 1: One symptom of Bi-polar disorder is extreme mood swings between excitement and depression. Claim 2: Genetics could be one of the major factors that causes Bi-polar disorder.
Claim 3: The mood swing from Bi-polar disorder can last days to months.
Original question: How do we hear differences in sound besides volume and pitch?
Passage: Pitch refers to the frequency of soundwave, and volumn refers to the amplitude of the soundwave. Besides volumn and pitch, we can also tell the difference between sounds based on the tone of sound. For example, we can differentiate the sound of different instruments based on the tone of the sounds.
Claim 1: Volume of sound is the amplitude of the soundwave.
Claim 2: Pitch is the frequency of soundwave.
Claim 3: We can use the tone of the sounds to differentiate the sound of different instruments.
Original question: How are we able to discern whether a sound is coming from in front of us or behind us?
Passage: There are multiple explanations for why we can localize sounds. One explanation is that sounds travelling to the corresponding side of one's ear will be slightly louder. Another explanation is that there is a slight difference in the hitting time to one's left and right ear based on the sound's direction. However, these explanation means that when a sound is exactly in front of someone or exactly behind someone, he or she can not tell the difference.
Claim 1: We can localize sounds by recognizing that the sound travelling to the corresponding side of one's ear will be slightly louder.
Claim 2: We can also localize sounds by recognizing the difference in hitting time to one's left and right ear based on the sound's direction.
Claim 3: We cannot tell the difference between a sound that is exactly in front of us or exactly behind us.</p>
<p>Table 22: Prompt used to generate the sub-claims for ELI5 questions. Blue text is model generation. Brown text is the ELI5 example that we want to generate sub-claims for. We construct the prompt by manually writing the sub-claims for three questions from the training set.</p>
<p>Instruction: Write an accurate, engaging, and concise answer for the given question using only the provided search results (some of which might be irrelevant) and cite them properly. Use an unbiased and journalistic tone. Always cite for any factual claim. When citing several search results, use [1][2][3]. Cite at least one document and at most three documents in each sentence. If multiple documents support the sentence, only cite a minimum sufficient subset of the documents.</p>
<p>Table 23: Instruction for VANILLA.</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 24: Short instruction for VANILLA.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Instruction: Write a high-quality answer for the given question using only the provided search results and cite them properly using [1][2][3].&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{2}$ https://www.bing.com/new
${ }^{3}$ https://www.perplexity.ai&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
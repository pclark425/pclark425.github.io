<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7681 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7681</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7681</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-143.html">extraction-schema-143</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <p><strong>Paper ID:</strong> paper-269930374</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.12264v1.pdf" target="_blank">Directed Metric Structures arising in Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models are transformer neural networks which are trained to produce a probability distribution on the possible next words to given texts in a corpus, in such a way that the most likely word predicted is the actual word in the training text. In this paper we find what is the mathematical structure defined by such conditional probability distributions of text extensions. Changing the view point from probabilities to -log probabilities we observe that the subtext order is completely encoded in a metric structure defined on the space of texts $\mathcal{L}$, by -log probabilities. We then construct a metric polyhedron $P(\mathcal{L})$ and an isometric embedding (called Yoneda embedding) of $\mathcal{L}$ into $P(\mathcal{L})$ such that texts map to generators of certain special extremal rays. We explain that $P(\mathcal{L})$ is a $(\min,+)$ (tropical) linear span of these extremal ray generators. The generators also satisfy a system of $(\min+)$ linear equations. We then show that $P(\mathcal{L})$ is compatible with adding more text and from this we derive an approximation of a text vector as a Boltzmann weighted linear combination of the vectors for words in that text. We then prove a duality theorem showing that texts extensions and text restrictions give isometric polyhedra (even though they look a priory very different). Moreover we prove that $P(\mathcal{L})$ is the lattice closure of (a version of) the so called, Isbell completion of $\mathcal{L}$ which turns out to be the $(\max,+)$ span of the text extremal ray generators. All constructions have interpretations in category theory but we don't use category theory explicitly. The categorical interpretations are briefly explained in an appendix. In the final appendix we describe how the syntax to semantics problem could fit in a general well known mathematical duality.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7681.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7681.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Probabilistic language model → directed metric (d) / P(L)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Probabilistic language model induced directed metric and metric polyhedron P(L)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper defines how an autoregressive LLM's next-token conditional probabilities Pr(a_j | a_i) are converted via d(a_i,a_j) = -log Pr(a_j|a_i) into a directed metric on the corpus L, and how that metric determines a tropical/(min,+) linear polyhedron P(L) whose generators and (min,+) linear relations encode structural quantitative relations among texts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>DIRECTED METRIC STRUCTURES ARISING IN LARGE LANGUAGE MODELS</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_type</strong></td>
                            <td>conceptual use of autoregressive transformer outputs (next-token probability distributions); no named model run in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Convert model-computed conditional next-token probabilities into pairwise conditional probabilities Pr(a_j|a_i) for text extensions, then apply -log to obtain directed distances d; construct P(L) defined by inequalities x_i ≤ x_j + d(a_i,a_j) (equivalently d_min projector Fix), yielding (min,+) linear relations among Yoneda/co‑Yoneda text vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>structural/tropical algebraic relations between conditional-probability-derived distances and text vectors (abstract functional relations encoding semantics)</td>
                        </tr>
                        <tr>
                            <td><strong>law_representation</strong></td>
                            <td>Directed metric (d), (min,+) linear equations (tropical linear relations), generator/extremal-ray characterizations, and Boltzmann-weighted ordinary linear approximations (via temperature limit to min).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Mathematical derivations and proofs relating Pr→d→P(L) (Propositions/Theorems). Paper cites external empirical experiments for related vectors but performs no empirical validation here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>The construction assumes access to conditional probabilities for text extensions computed by an autoregressive model and (in some results) multiplicativity Pr(a_k|a_i)=Pr(a_k|a_j)Pr(a_j|a_i); authors note this multiplicative assumption may not hold for real transformers and that infinite distances (zero probabilities) and corpus size/representation issues complicate practical application; no empirical accuracy or approximation error bounds provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Directed Metric Structures arising in Large Language Models', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7681.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7681.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Boltzmann approximation of tropical (min,+) combinations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Boltzmann-weighted ordinary linear approximation of (min,+) combinations (temperature T → 0 limit)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper shows that (min,+) tropical linear combinations (min of affine terms) can be approximated by ordinary Boltzmann-weighted linear combinations using a temperature parameter T via the identity lim_{T→0} −T log Σ exp(−·/T) = min{·}, and uses this to relate tropical expressions for text vectors to transformer-style softmax-weighted combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>DIRECTED METRIC STRUCTURES ARISING IN LARGE LANGUAGE MODELS</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_type</strong></td>
                            <td>theoretical mapping between tropical (min,+) expressions and Boltzmann-weighted linear combinations; linked conceptually to transformer attention softmax</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Analytic transform/limit: convert tropical (min,+) expressions for x = min_j (d_{i,j} + x_j) into −T log Σ_j exp(−(d_{i,j}+x_j)/T) and take T→0 to recover min, yielding an ordinary linear combination with Boltzmann weights for small T (Equation 90).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>functional approximation / asymptotic equivalence between tropical minima and Boltzmann-weighted sums</td>
                        </tr>
                        <tr>
                            <td><strong>law_representation</strong></td>
                            <td>Asymptotic identity: e^{-R(Y(b))/T} ≈ Σ_i w_i(b) e^{-d(w_i,b)/T} v_i (Equation 90); tropical (min,+) equations equivalently represented as temperature-limited softmax-weighted linear combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Mathematical limit identity and derivations; heuristically connected to transformer self-attention computations but not empirically validated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>No quantitative bounds on approximation error for finite T provided; practical fidelity depends on temperature schedule and numerical conditioning; mapping is asymptotic and does not produce explicit finite‑T guarantees for real models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Directed Metric Structures arising in Large Language Models', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7681.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7681.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Co‑Yoneda sampling experiments (ref.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Empirical estimation of co‑Yoneda vectors by sampling continuations from a Transformer LLM (experimental evidence referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites prior experiments (reference [12]) where co‑Yoneda embedding vectors (variations of d(·,a_k) or d(a_k,·)) were constructed by sampling continuations from an actual Transformer LLM and evaluated on semantic tasks, reported as 'in general very good' results; the present paper uses those empirical observations as supporting evidence for its semantic interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>DIRECTED METRIC STRUCTURES ARISING IN LARGE LANGUAGE MODELS</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_type</strong></td>
                            <td>sampling-based estimation of embedding vectors from an autoregressive transformer (sampling continuations to estimate conditional probabilities / co‑Yoneda vectors); referenced but not reproduced here</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Sampling continuations from an autoregressive transformer conditioned on prompt texts to estimate conditional extension probabilities and build co‑Yoneda/distance vectors (described in the cited experimental work, not carried out in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>empirical estimation of semantic embeddings (no explicit physical/biological law), used to validate semantic interpretability of theoretical constructions</td>
                        </tr>
                        <tr>
                            <td><strong>law_representation</strong></td>
                            <td>Estimated co‑Yoneda vectors (d(a_k,·) or d(·,a_k)) derived from sampled conditional probabilities; subsequently interpreted in the paper via tropical algebraic relations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_dataset</strong></td>
                            <td>not specified in this paper (experiments reside in cited reference [12])</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>not specified here (cited work reportedly used several semantic tests; details not presented in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>qualitative statement: 'results were in general very good' (no numeric metrics provided in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Reference to external empirical experiments (sampling + semantic tests) in [12]; this paper provides theoretical derivations and points to those empirical results as supporting evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>No experimental details, model identities, datasets, or quantitative metrics provided in this paper; reliance on an external reference for empirical claims prevents assessing reproducibility or robustness here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Directed Metric Structures arising in Large Language Models', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Meaning representations from trajectories in autoregressive models <em>(Rating: 2)</em></li>
                <li>An enriched category theory of language: From syntax to semantics <em>(Rating: 1)</em></li>
                <li>Tropical convexity <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7681",
    "paper_id": "paper-269930374",
    "extraction_schema_id": "extraction-schema-143",
    "extracted_data": [
        {
            "name_short": "Probabilistic language model → directed metric (d) / P(L)",
            "name_full": "Probabilistic language model induced directed metric and metric polyhedron P(L)",
            "brief_description": "The paper defines how an autoregressive LLM's next-token conditional probabilities Pr(a_j | a_i) are converted via d(a_i,a_j) = -log Pr(a_j|a_i) into a directed metric on the corpus L, and how that metric determines a tropical/(min,+) linear polyhedron P(L) whose generators and (min,+) linear relations encode structural quantitative relations among texts.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "paper_title": "DIRECTED METRIC STRUCTURES ARISING IN LARGE LANGUAGE MODELS",
            "llm_name": null,
            "llm_type": "conceptual use of autoregressive transformer outputs (next-token probability distributions); no named model run in this paper",
            "input_corpus_description": null,
            "extraction_method": "Convert model-computed conditional next-token probabilities into pairwise conditional probabilities Pr(a_j|a_i) for text extensions, then apply -log to obtain directed distances d; construct P(L) defined by inequalities x_i ≤ x_j + d(a_i,a_j) (equivalently d_min projector Fix), yielding (min,+) linear relations among Yoneda/co‑Yoneda text vectors.",
            "law_type": "structural/tropical algebraic relations between conditional-probability-derived distances and text vectors (abstract functional relations encoding semantics)",
            "law_representation": "Directed metric (d), (min,+) linear equations (tropical linear relations), generator/extremal-ray characterizations, and Boltzmann-weighted ordinary linear approximations (via temperature limit to min).",
            "evaluation_dataset": null,
            "evaluation_metrics": null,
            "performance_results": null,
            "baseline_comparison": false,
            "baseline_performance": null,
            "validation_method": "Mathematical derivations and proofs relating Pr→d→P(L) (Propositions/Theorems). Paper cites external empirical experiments for related vectors but performs no empirical validation here.",
            "limitations": "The construction assumes access to conditional probabilities for text extensions computed by an autoregressive model and (in some results) multiplicativity Pr(a_k|a_i)=Pr(a_k|a_j)Pr(a_j|a_i); authors note this multiplicative assumption may not hold for real transformers and that infinite distances (zero probabilities) and corpus size/representation issues complicate practical application; no empirical accuracy or approximation error bounds provided.",
            "uuid": "e7681.0",
            "source_info": {
                "paper_title": "Directed Metric Structures arising in Large Language Models",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Boltzmann approximation of tropical (min,+) combinations",
            "name_full": "Boltzmann-weighted ordinary linear approximation of (min,+) combinations (temperature T → 0 limit)",
            "brief_description": "The paper shows that (min,+) tropical linear combinations (min of affine terms) can be approximated by ordinary Boltzmann-weighted linear combinations using a temperature parameter T via the identity lim_{T→0} −T log Σ exp(−·/T) = min{·}, and uses this to relate tropical expressions for text vectors to transformer-style softmax-weighted combinations.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "paper_title": "DIRECTED METRIC STRUCTURES ARISING IN LARGE LANGUAGE MODELS",
            "llm_name": null,
            "llm_type": "theoretical mapping between tropical (min,+) expressions and Boltzmann-weighted linear combinations; linked conceptually to transformer attention softmax",
            "input_corpus_description": null,
            "extraction_method": "Analytic transform/limit: convert tropical (min,+) expressions for x = min_j (d_{i,j} + x_j) into −T log Σ_j exp(−(d_{i,j}+x_j)/T) and take T→0 to recover min, yielding an ordinary linear combination with Boltzmann weights for small T (Equation 90).",
            "law_type": "functional approximation / asymptotic equivalence between tropical minima and Boltzmann-weighted sums",
            "law_representation": "Asymptotic identity: e^{-R(Y(b))/T} ≈ Σ_i w_i(b) e^{-d(w_i,b)/T} v_i (Equation 90); tropical (min,+) equations equivalently represented as temperature-limited softmax-weighted linear combinations.",
            "evaluation_dataset": null,
            "evaluation_metrics": null,
            "performance_results": null,
            "baseline_comparison": false,
            "baseline_performance": null,
            "validation_method": "Mathematical limit identity and derivations; heuristically connected to transformer self-attention computations but not empirically validated in this paper.",
            "limitations": "No quantitative bounds on approximation error for finite T provided; practical fidelity depends on temperature schedule and numerical conditioning; mapping is asymptotic and does not produce explicit finite‑T guarantees for real models.",
            "uuid": "e7681.1",
            "source_info": {
                "paper_title": "Directed Metric Structures arising in Large Language Models",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Co‑Yoneda sampling experiments (ref.)",
            "name_full": "Empirical estimation of co‑Yoneda vectors by sampling continuations from a Transformer LLM (experimental evidence referenced)",
            "brief_description": "The paper cites prior experiments (reference [12]) where co‑Yoneda embedding vectors (variations of d(·,a_k) or d(a_k,·)) were constructed by sampling continuations from an actual Transformer LLM and evaluated on semantic tasks, reported as 'in general very good' results; the present paper uses those empirical observations as supporting evidence for its semantic interpretation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "paper_title": "DIRECTED METRIC STRUCTURES ARISING IN LARGE LANGUAGE MODELS",
            "llm_name": null,
            "llm_type": "sampling-based estimation of embedding vectors from an autoregressive transformer (sampling continuations to estimate conditional probabilities / co‑Yoneda vectors); referenced but not reproduced here",
            "input_corpus_description": null,
            "extraction_method": "Sampling continuations from an autoregressive transformer conditioned on prompt texts to estimate conditional extension probabilities and build co‑Yoneda/distance vectors (described in the cited experimental work, not carried out in this paper).",
            "law_type": "empirical estimation of semantic embeddings (no explicit physical/biological law), used to validate semantic interpretability of theoretical constructions",
            "law_representation": "Estimated co‑Yoneda vectors (d(a_k,·) or d(·,a_k)) derived from sampled conditional probabilities; subsequently interpreted in the paper via tropical algebraic relations.",
            "evaluation_dataset": "not specified in this paper (experiments reside in cited reference [12])",
            "evaluation_metrics": "not specified here (cited work reportedly used several semantic tests; details not presented in this paper)",
            "performance_results": "qualitative statement: 'results were in general very good' (no numeric metrics provided in this paper)",
            "baseline_comparison": null,
            "baseline_performance": null,
            "validation_method": "Reference to external empirical experiments (sampling + semantic tests) in [12]; this paper provides theoretical derivations and points to those empirical results as supporting evidence.",
            "limitations": "No experimental details, model identities, datasets, or quantitative metrics provided in this paper; reliance on an external reference for empirical claims prevents assessing reproducibility or robustness here.",
            "uuid": "e7681.2",
            "source_info": {
                "paper_title": "Directed Metric Structures arising in Large Language Models",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Meaning representations from trajectories in autoregressive models",
            "rating": 2,
            "sanitized_title": "meaning_representations_from_trajectories_in_autoregressive_models"
        },
        {
            "paper_title": "An enriched category theory of language: From syntax to semantics",
            "rating": 1,
            "sanitized_title": "an_enriched_category_theory_of_language_from_syntax_to_semantics"
        },
        {
            "paper_title": "Tropical convexity",
            "rating": 1,
            "sanitized_title": "tropical_convexity"
        }
    ],
    "cost": 0.0151385,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DIRECTED METRIC STRUCTURES ARISING IN LARGE LANGUAGE MODELS
May 22, 2024</p>
<p>S T Éphane 
Yiannis Vlassopoulos 
DIRECTED METRIC STRUCTURES ARISING IN LARGE LANGUAGE MODELS
May 22, 202442588563378AA6B4E0324145920A9586arXiv:2405.12264v1[cs.LG]
Large Language Models are transformer neural networks which are trained to produce a probability distribution on the possible next words to given texts in a corpus, in such a way that the most likely word predicted is the actual word in the training text.In this paper we find what is the mathematical structure defined by such conditional probability distributions of text extensions.Changing the view point from probabilities to -log probabilities we observe that the subtext order is completely encoded in a metric structure defined on the space of texts L, by -log probabilities.We then construct a metric polyhedron P (L) and an isometric embedding (called Yoneda embedding) of L into P (L) such that texts map to generators of certain special extremal rays.We explain that P (L) is a (min, +) (tropical) linear span of these extremal ray generators.The generators also satisfy a system of (min +) linear equations.We then show that P (L) is compatible with adding more text and from this we derive an approximation of a text vector as a Boltzmann weighted linear combination of the vectors for words in that text.We then prove a duality theorem showing that texts extensions and text restrictions give isometric polyhedra (even though they look a priory very different).Moreover we prove that P (L) is the lattice closure of (a version of) the so called, Isbell completion of L which turns out to be the (max, +) span of the text extremal ray generators.All constructions have interpretations in category theory but we don't use category theory explicitly.The categorical interpretations are briefly explained in an appendix.In the final appendix we describe how the syntax to semantics problem could fit in a general well known mathematical duality.</p>
<p>Large Language Models (LLM) are transformer neural networks that are trained to compute the probability of the possible next words to a text in such a way that the most probable next word predicted by the network, is the actual next word in the training text [20,15,16,2].</p>
<p>They are often characterized as "just statistical models".In this paper, continuing the approach introduced in [1], this time without explicitly using categories 1 , we would like to make a proposal for what is the underlying mathematical structure these probability distributions actually encode and show the evidence and possible consequences.We find that a rich structure is revealed if we change the point of view from probabilities to negative log probabilities.While it is entirely equivalent, this point of view can be reinterpreted as an asymmetric metric on texts.</p>
<p>Indeed consider a set L := {a 1 , . . ., a n } whose elements are texts in the language.Equip L with a poset structure, where a i a j if and only if a i is a subtext of a j .Denote by Pr(a j |a i ) the probability of extending a text a i to a text a j .The probability is 0 exactly when a i is not a subtext of a j .Our main assumption is that conditional probabilities of extensions of texts multiply i.e. (1) if a i a j a k then Pr(a j |a i ) Pr(a k |a j ) = Pr(a k |a i ).</p>
<p>We call the triple (L, , Pr) a probabilistic language model.Next, recall the notion of a directed metric δ on a set X.It is defined to be a function δ : X × X → (−∞, ∞] which satisfies the triangle inequality and δ(a, a) = 0. Nevertheless, unless stated otherwise it does not have to be symmetric, δ(a, b) = 0 does not necessarily imply a = b and δ can take the value ∞ and can also take negative values. 2  Now we notice that the probabilistic language model (L, , Pr) defines a directed metric d on the poset L by (2) d(a i , a j ) = − log Pr(a j |a i ) if a i a j , ∞ if a i and a j are not comparable.</p>
<p>(L, d) is then a directed metric space.Indeed</p>
<p>(3) if a i a j a k then d(a i , a k ) = d(a i , a j ) + d(a j , a k ) and otherwise the triangle inequality is satisfied with at least one side being ∞ (Proposition 1).We see also that the metric d fully determines the poset structure on L (Corollary 1).Although d satisfies a rather degenerate form of the triangle inequality, it is enough to define a non trivial directed metric polyhedron P (L) in (R ∪ {∞}) n , inside which L is isometrically embedded as a remarkable set of extremal rays.</p>
<p>Indeed, in Section 3 we define (4) P (L) := {x ∈ (R ∪ {∞}) n (∞, . . ., ∞)|x i x j + d(a i , a j )} and thus the finite part of P (L) is a polyhedron in R n .Define the Funk directed metric D on (R ∪ {∞}) n (∞, . . ., ∞) by ( 5)
D(x, x ′ ) := max i {x ′ i − x i |x i = ∞}.
Then (P (L), D) becomes a directed metric space.The Funk directed metric originates from Hilbert geometry [14].</p>
<p>To understand the relevance of P (L) note first (Proposition 4) that (6) Y : (L, d) ֒→ (P (L), D), given by Y (a k ) := d(−, a k ), is an isometric embedding (called the Yoneda embedding).Moreover, each Y (a k ) is a generator of an extremal ray of P (L) (Theorem 1).</p>
<p>To be precise about the term extremal ray here, consider the image Q(L) of P (L), under the coordinate wise map x i → z i := e −xi , i.e.We see that (8) Q(L) := {z = (z 1 , . . .z n ) ∈ [0, ∞) n (0, . . ., 0)|z i Pr(a j |a i )z j }</p>
<p>Denote by e −Y (a k ) , the image of Y (a k ) in Q(L).Then (Theorem 1) e −Y (a k ) is a generator of a usual extremal ray in the polyhedral cone Q(L).When we speak of extremal rays of P (L) we always mean the subsets of P (L) that map to extremal rays in the polyhedral cone Q(L) by the map (x i ) → (e −xi ).The polyhedral cone Q(L) is a generalization of the order polytope studied by Stanley [18].The order polytope corresponds to the case where Pr(a j |a i ) takes only the values 0 or 1 and L is simply a poset.Moreover, Stanley does not consider a cone, but rather the intersection of this cone with the unit box.Lam and Postnikov [10] defined an alcoved polytope to be a bounded cell of a Coxeter arrangement (of type A n ).The definition of P (L) is similar, but we do not require the cell to be bounded.Alcoved polytopes have been studied in tropical geometry, in relation with metric spaces, see e.g.[9,19].</p>
<p>Moving on, we prove in Proposition 5 that (9) if x = (x 1 , . . ., x n ) ∈ P (L) then x k = D(Y (a k ), x).</p>
<p>We now see that the defining equations for P (L) are exactly the triangle inequalities for D. Indeed (10) x i x j + d(a i , a j ) ⇐⇒ D(Y (a i ), x) D(Y (a j ), x) + D(Y (a i ), Y (a j )).</p>
<p>Note also (Proposition 3) that we can think of the points x ∈ P (L) as functions on L (just as we can think of usual vectors as functions on a set).Indeed, if we denote by d R the Funk metric on R ∪ {∞}, namely d R (s, t) = t − s and d R (∞, ∞) = max ∅ = −∞ 3 then
P (L) = {x : (L, d t ) → ((−∞, ∞], d F )|x is non-expansive.}
We then have that x(a i ) = x i = D(Y (a i ), x) (Proposition 5).</p>
<p>Therefore P (L) can also be thought of as the set of maps x : L → (−∞, ∞] which satisfy the triangle inequalities for the metric D with respect to all the maps Y (a k ) := d(−, a k ) : L → (−∞, ∞] for k = 1, . . ., n.It is therefore a kind of convex metric span of the Y (a k ) := d(−, a k ).</p>
<p>From the point of view of language semantics now, we consider Y (a k ) = d(−, a k ) to represent the meaning of a text a k in terms of all the texts it contains, (Section 4.1) in accordance with the statistical semantics principal and as was advocated in [1].</p>
<p>Dually, we can consider the meaning of a text a k to be given by all the texts extending a k , namely d(a k , −).This is then encoded in (11) P (L) = {y ∈ (R ∪ {∞}) n |y i y j + d(a j , a i )}.</p>
<p>Indeed we have the isometric co-Yoneda embedding (12) Y : L ֒→ P (L) given by Y (a k ) := d(a k , −) and moreover y i = D( Y (a i ), y).</p>
<p>Remark 1.We said that Y (a k ) := d(−, a k ) or Y (a k ) := d(a k , −) represent the meaning of a text a k but it's also the "location" of these vectors in the whole space P (L) and P (L) respectively.In particular if w := a k is a word then Y (w) := d(−, w) is supported only on w so it does not seem to contain much information.However the relevant semantic information is in D(Y (w), −).Moreover we will see shortly that the fact that the vector d(−, w) is in P (L) means that it satisfies a whole system of equations (Eq 19, 20, Proposition 11) with respect to other texts.We will explain this system of equations, later in this overview when we describe section 4.</p>
<p>We already saw that Y (a k ) is an extremal ray in P (L) but it turns out that P (L) has generally exponentially many additional extremal rays -that are not in the image of Y .We explicitly characterize the extremal rays of P (L) as corresponding to connected lower sets of (L, ) in Proposition 6 and Theorem 2. In fact, if x is such an extremal ray and l(x) denotes the corresponding lower set then we show that after a diagonal change of variables the new coordinates of the extremal ray give the characteristic function of the lower set l(x).</p>
<p>What distinguishes the lower sets corresponding to elements Y (a i ) is that they are principal.</p>
<p>Therefore P (L) can be considered as a space parameterizing semantics in the language.However, only the extremal rays corresponding to principal lower sets of L, correspond to texts.</p>
<p>Notice now that the Funk metric D on P (L) defines a metric D Q on the polyhedral cone Q(L) where, if z, z ′ ∈ Q(L), then (13) D Q (z, z ′ ) := max i {log(
z i z ′ i )|z ′ i = 0}.
By definition we have
D Q (z, z ′ ) = D(− log z, − log z ′ ) and D(x, x ′ ) = D Q (e −x , e −x ′ ).
Then the fact that Y is an isometric embedding into P (L) implies that
(14) e −Y : (L, d) → (Q(L), D Q )
is an isometric embedding.</p>
<p>In section 4 we explain the construction of P (L) in terms of tropical or (min, +) algebra.Recall that the (min, +) semifield R min , is defined as ((−∞, ∞], min, +).We show in Section 4 that P (L) is generated by the vectors Y (a k ) = d(−, a k ) as a (min, +) module.To see that, note first that d is a directed metric if and only if it is a (min, +) projector.This is because if we let d i,j := d(a i , a j ) and define (15) d min (x) i := min
j {d i,j + x j },
then the triangle inequality and the fact that d i,i = 0, are equivalent to d i,k = min{d i,j + d j,k } which is equivalent to d 2 min = d min .We then note that P (L) = {x|d min x = x}.Indeed:
x = d min x ⇐⇒ x i = min j {d i,j + x j } ⇐⇒ x ∈ P (L).
Introduce the notation Fix(d min ) := {x|d min x = x}.Since d min is a projection Im(d min ) = Fix(d min ).Therefore ( 16)
P (L) = Fix(d min ) = Im(d min ).
We see that P (L) is the (min, +) (tropical) span of the columns of the matrix d.</p>
<p>Analogously, if we denote by d t the transpose of d, we see that P (L) = Im(d t min ) = Fix(d t min ) and therefore P (L) is the (min, +) row span of d.We let (u ⊕ v) i := min{u i , v i } and (λ ⊙ v) i := λ + v i .Then for x ∈ P (L) we have (17)
x = ⊕ j x j ⊙ d(−, a j ) = ⊕ j D(Y (a j ), x) ⊙ d(−, a j ) = ⊕ j D(Y (a j ), x) ⊙ Y (a j ).
and an analogous formula holds for z ∈ P (L) = Im(d t min ).From these we get that (Proposition 10)
(18) Y (a k ) = d(−, a k ) = ⊕ aj a k d j,k ⊙ Y (a j ) and (19) Y (a k ) = d(a k , −) = ⊕ a k a l d k,l ⊙ Y (a l ).
These are the systems of equations we referred to in Remark 1.</p>
<p>In section 5 we study how P (L) changes when we enlarge the language corpus L. We prove that if a probabilistic language model (L
1 , d 1 ) is extended to (L 2 , d 2 ), namely if there is an isometric embedding φ : (L 1 , d 1 ) ֒→ (L 2 , d 2 ) then there is an isometric embedding φ : (P (L 1 )), D 1 ) ֒→ (P (L 2 ), D 2 ) such that φ(Y 1 (a)) = Y 2 (φ(a)
). Moreover there is a non-expansive, (min, +) projection R :
P (L 2 ) → P (L 2 ) such that Im(R) = φ(P (L 1 )).
Using this we show that if L 1 := {w 1 , . . ., w l } is the set of words in the language and b is a text in
L then R(Y (b)) = wi b d 2 (w i , b) ⊙ Y 2 (w i ) Introducing a temperature parameter T we get (20) R(Y (b)) = lim T →0 −T log( wi b e − d(w i ,b) T e − Y (w i ) T )
Therefore for small T we have
e − R(Y (b)) T ≈ wi b e − d(w i ,b) T e − Y (w i ) T . Putting v i := e −Y (wi) we have (21) e − R(Y (b)) T ≈ i e −d(Y (wi),b)/T v i
This approximation of text vectors is similar to the one calculated by transformer neural networks in the self attention module.</p>
<p>In section 6 we describe a duality between semantics via texts extensions and text restriction.Indeed we have already seen that P (L) = {x|d min x = x} is the (min, +) column span of d and
P (L) = {x|d t min x = x} is the (min, +) row span of d. It is easy to see that d min x = x ⇐⇒ d t min (−x) = −x (Proposition 19
). Indeed it follows from the fact that x i d i,j + x j ⇐⇒ −x j d i,j − x i .However this requires extending the (min, +) semifield, as well as the values of the directed metric, to [−∞, ∞].This results in the definition of extended (min, +) modules P − (L) and P − (L).</p>
<p>We show that (P − (L), D) and ( P − (L), D t ) are isometric (and in fact tropically anti-isomorphic).We interpret this as saying that the semantic space P − (L), defined by extensions of texts is isomorphic to the semantic space P − (L) defined by restrictions of texts.We note that this is quite a non-trivial isomorphism as P (L) and P (L) don't even have the same number of extremal rays in general.We give an example (Example 1) illustrating the polyhedra P (L) and P (L).In fact we consider the corresponding polyhedral cones Q(L) and Q(L).We then define
(22) Q 0 (L) := Q(L) ∩ ∆
to be the intersection of Q(L) with the unit simplex ∆.Points in Q 0 (L) are normalized to probability distributions.Analogously define Q 0 (L)) to be the intersection of Q(L) with the unit simplex.Extremal rays of Q(L) and Q(L)) define vertices of Q 0 (L) and Q 0 (L) respectively.We show the correspondence of extremal rays to lower sets and upper sets.The example also showcases the difference between a probabilistic language model and a general directed metric space where infinite distances are approximated uniformly by a big number M .Section 7 further explores the extremal rays of P (L).</p>
<p>In section 8 we explore the relation with the so called Isbell completions I(L) and I(L).This is similar to the duality of section 6.The Isbell adjunction is defined over the extended ring [−∞, ∞] and the fixed parts of the adjunction turn out to be I(L) = Im(d max ) where d max (x) i := max j {d i,j + x j } and I(L) = Im(d t max ).In (Proposition 29) it is shown that P (L) is the lattice completion of the so called Isbell completion.</p>
<p>When restricting coefficients to [0, ∞], the Isbell completion has been studied by Willerton [21] where it is proven to be isomorphic to the directed tight span DT S(L) of Hirai and Koichi [7] (inspired by the undirected tight span defined by Isbell [8] and Dress [5]).It also generalizes the Dedekind-MacNeille completion of a poset.</p>
<p>Informally speaking I(L) can be though of as the minimal space in which we can isometrically embed L. Unlike P (L) though, it is far from being convex.A simple example is shown in section 8.</p>
<p>Section 9 is a collection of observations about probabilistic language models and their relation to transformers.</p>
<p>As mentioned already, all constructions and results in this paper have categorical interpretations and though we have avoided using categorical language in the main text we explain briefly in Appendix A, for the benefit of readers familiar with categories, these categorical interpretations.</p>
<p>Finally in Appendix B we present a general perspective which locates the language syntax and semantics problems in the realm of a basic duality in mathematics which in its simplest form appears as a duality between algebra and geometry.This allows us to locate future directions of research.</p>
<p>Experimental evidence for the semantic meaning of the (co-)Yoneda embedding vectors Y (a k ) has been provided in [12].There experiments based on (a slight variation of) the co-Yoneda embedding vectors, were performed using an actual Transformer LLM, by sampling over continuations of texts.Several semantic tests were conducted and the results were in general very good.</p>
<p>1.1.Acknowledgements.YV would like to thank Tai-Danae Bradley, Michael Douglas, Ioannis Emiris, Harris Papageorgiou, Alex Takeda, John Terilla, Matthew Trager, Maxim Kontsevich, Matilde Marcolli, Jack Morava, Stefano Soatto, and Elias Zafiris for useful conversations.He also would like to thank Anna Geneveaux for computing several useful examples of polyhedra for probabilistic language models during her internship.SG and YV thank Gleb Koshevoy and Panayotis Mertikopoulos for useful conversations.Finally YV would like to thank IHES for providing excellent working conditions.</p>
<p>From probabilities of text extensions to distances</p>
<p>Consider a language with a set of words W := {w 1 , . . ., w l }.Consider also a set of training texts from the language, L := {a 0 , a 1 , . . ., a n } where a i := w i1 . . .w i k i .We endow L with a poset structure where a i a j if and only if a i is a subtext of a j .We consider two possibilities for the notion of subtext.The first is
(23) a i 1 a j ⇐⇒ ∃a k ∈ L such that a j = a i a k
and we refer to this as the one sided subtext order and the second is (24) a i 2 a j ⇐⇒ ∃a k1 , a k2 ∈ L such that a j = a k1 a i a k2</p>
<p>and we refer to that as the two sided subtext order.</p>
<p>We define always a 0 to be the empty text and a 0 is the only text such that a 0 a i ∀i in either order.(However see remark (1) bellow for how a 0 interacts with the probabilities we will soon add to the model.)</p>
<p>If a i 1 a j in the one sided subtext order then a i 2 a j .The results and constructions that follow hold equally for both orders so we will simply write a i a j and when there is need to separate the two orders we will make a special comment.</p>
<p>If a i a j then denote by Pr(a j |a i ) the probability of extension from a i to a j .</p>
<p>It is important to note that these probabilities are not calculated from a corpus of texts, as any probability for a sufficiently long text would be vanishingly small.Instead we are talking about the probabilities that the large language model (LLM) computes.Namely prompted with a text a i the model outputs a probability distribution Pr(a i w j1 |a i )∀w j1 ∈ W and this is the probabilities we are referring to, above.To continue extending to a i w j1 w j2 we simply have (25) Pr(a i w j1 w j2 |a i ) = Pr(a i w j1 |a i ) Pr(a i w j1 w j2 |a i w j1 )</p>
<p>And continuing this way Pr(a j |a i ) is computed.If a i is not a subtext of a j then we put Pr(a j |a i ) = 0.</p>
<p>Recall that the LLM is trained to produce the probability distribution of the next word to a text, in such a way that the most likely next word predicted by the model is the one in the training text.</p>
<p>As a consequence of Equation (25) we make our fundamental assumption that
(26) a i a j a k =⇒ Pr(a k |a i ) = Pr(a k |a j ) Pr(a j |a i )
Note that the transformer LLM produces the probabilities for the one-sided subtext order.However in the attention layers of the transformer, two sided extensions are used in order to construct the text vector.We consider therefore the case of the two sided order as well.Indeed in section 5 we will see that the text vector we define, is expressed in terms of word vectors when we consider the two sided subtext order.We put these together in the following Definition 1.A probabilistic language model is a triple (L, , P r) where, L := {a 0 , a 1 , . . ., a n } is a collection of texts, is the subtext order and Pr : L×L → [0, 1] is a function such that a i a j a k =⇒ Pr(a k |a i ) = Pr(a k |a j ) Pr(a j |a i ).</p>
<p>Recall now the following
Definition 2. (X, δ) is called a directed metric space if X is a set and δ : X × X → (−∞, ∞] satisfies the triangle inequality (27) δ(a, c) δ(a, b) + δ(b, c)
for all a, b, c ∈ X and δ(a, a) = 0, ∀a ∈ X Note that this generalises usual metrics in that we don't require δ(a, b) = δ(b, a), δ(a, b) = 0 does not necessarily imply a = b and moreover we allow negative values.This definition of a directed metric, in the special case of positive valued δ, has appeared in [21,11] and is also known as a generalised metric or a pseudo quasi metric.</p>
<p>Remark 2. We need the following technical specification: In three cases (proposition 3 and the duality theorems of sections 6 and 8) we will need to extend definition 2, of a directed metric to allow the value −∞ so that we will have.
δ : X × X → [−∞, ∞].
In that case the definition is the same but we need to specify that we use the convention that +∞ is the absorbing element so that s + (+∞) = +∞ for all s and in particular −∞ + ∞ = +∞.This will be needed in Proposition 2. In section 6 we will explain further that this is the so called (min, +) convention, as this is the only one compatible with the structure of (min, +) semiring; there is also a dual (max, +) convention.</p>
<p>We define now a directed metric space structure on the underlying poset of a probabilistic language model (L, , Pr).Definition 3. Given the probabilistic language model (L, , Pr) where is the subtext order and Pr(a j |a i ) are the probabilities of extension, define the directed metric d :
L × L → [0, ∞] by (28) d(a i , a j ) = − log Pr(a j |a i ) if a i a j , ∞
if a i and a j are not comparable.</p>
<p>It is clear that d(a i , a i ) = 0. To verify that d is a directed metric we have the following: Proposition 1.The map d satisfies the triangle inequality:
(29) d(a i , a k ) d(a i , a j ) + d(a j , a k ) ,
and equality holds if and only if a i a j a k or a i a k .</p>
<p>Proof.Indeed, if a i a j a k then a i a k , and the equality holds in (29), since by our main assumption (the standard property of conditional probabilities), Pr(a k |a i ) = Pr(a k |a j ) Pr(a j |a i ).If a i a k , then, d(a i , a k ) = ∞, and either a i a j or a j a k , which entails that both sides of (29) are equal to infinity.Finally, if a i a k but a i a j or a j a k , the left-hand side of (29) is finite whereas the right-hand side is +∞.</p>
<p>We then have Corollary 1.The following statements are equivalent:</p>
<p>(1)
a i a j a k (2) d(a i , a k ) = d(a i , a j ) + d(a j , a k ) and d(a i , a k ) &lt; ∞ Remark 3.
Note that from Proposition 1 and Corollary 1 it follows that the partial order on L can be fully recovered by the directed metric d or equivalently the conditional probabilities Pr(a j |a i ), therefore we will also denote the probabilistic language model (L, , Pr) as (L, Pr) or (L, d).</p>
<p>Remark 4. In a Large Language model probabilities are normalized to add up to one, over all extensions of a given text by a word.</p>
<p>Remark 5. Note that the probabilistic language model (L, d) is a special case of a directed metric space.Whenever it is possible we will prove results for a general directed metric space and derive the language case as a corollary.Moreover, it is possible to imagine that even the main assumption a i a j a k =⇒ Pr(a k |a i ) = Pr(a k |a j ) Pr(a j |a i ) should be generalized to Pr(a k |a i ) Pr(a k |a j ) Pr(a j |a i ), namely this of a general directed metric space with d(a i , a k ) d(a i , a j ) + d(a j , a k ).This is a reasonable assumption by itself and can be interpreted a saying that the shortest path to go from a i to a k is at least as short as a path that is forced to go from a i to a k but passing though a j .As we will see in what follows, the only result that requires the main assumption of conditional probabilities multiplying is Theorem 2 and all the rest are valid for general directed metric spaces.It is a matter of experimental verification to check for a given LLM if the multiplicative assumption is best or the general case.Remark 6.Note that we can slightly modify the definition of the Probabilistic Language model so that instead of Pr taking values in [0, 1] we put Pr : L × L → [0, ∞).Then definition 3 will again produce a directed metric space.In fact we develop most of the theory using the more general extended assumption since most results are valid for general directed metric spaces as we mentioned in the previous remark.</p>
<p>Remark 7. Since the machine produces probabilities for all possible next words it is natural to assume it is learning probabilities of extension for the free monoid generated by words.Obviously most strings of words will have vanishing probability and only those which are part of the language should have big probability.</p>
<p>We can then consider L to contain the whole free monoid and it is natural to grade it by the word length of each text.</p>
<p>Remark 8.Note that if we assume that there exists a 0 such that a 0 a k ∀a k ∈ L then a i a j implies a 0 a i a j and therefore d(a 0 , a j ) = d(a 0 , a i ) + d(a i , a j ) and thus d(a i , a j ) = d(a 0 , a j )−d(a 0 , a i ).This is equivalent to the statement that there is a globally defined probability distribution for absolute probabilities of texts, giving rise to all the conditional probabilities.Namely if a i a j then Pr(a j |a i ) = Pr(aj |a0) Pr(ai|a0) .The element a 0 can be considered to be the empty text and from this point of view it is natural to assume it exists in L. However the fact that it implies all conditional probabilities come from a global probability distribution shows that the inclusion of a 0 in the probabilistic language model, is not an entirely trivial assumption.</p>
<p>It would be a matter for experimental verification to see if it applies in the transformer Large Language Models.Therefore we will not assume it by default.We will specify explicitly whenever we assume a 0 ∈ L.</p>
<p>Next, we illustrate what the main assumption implies by the following:</p>
<p>Proposition 2. Consider a probabilistic language model (L, , Pr) then on every connected component C of the Hasse diagram of L, there is a function
P C : C → [0, ∞) such that if a i , a j ∈ C and a i a j then Pr(a j |a i ) = PC (aj )
PC (aj ) .The function P C is unique up to multiplication by a positive number.</p>
<p>Proof.The fact that (L, , Pr) is a probabilistic language model means that
a i a j a k is equivalent to Pr(a k |a i ) = Pr(a k |a j ) Pr(a j |a i ) and Pr(a k |a i ) &lt; ∞.
Let G denote the directed graph which is the Hasse diagram of C. We construct a new weighted graph G as follows: If a i a j , we draw an arrow from node a i to node a j with weight Pr(a j |a i ).If a j a i , we draw an arrow from node a i to node a j with weight Pr(a j |a i ) −1 .We now choose arbitrarily an element c ∈ C. If a i ∈ C, we define P C (a i ) to be the weight in the graph G of an arbitrary path from the point c to a i .Owing to our main assumption, (1), the weight is independent of the choice of the path from c to a i .Moreover, for all i, j such that a i a j , we have P C (a i ) Pr(a
j |a i )P C (a j ) −1 = 1 therefore Pr(a j |a i ) = PC (aj ) PC (ai) .
Picking a different reference element c ′ ∈ C scales P C (a i ) by P C (c ′ ) therefore the ratio stays the same.</p>
<p>3.</p>
<p>From the text metric space L to the polyhedra P (L) and Q(L)</p>
<p>First notice that we can also equip L with the transpose directed metric d t where d t (a i , a j ) := d(a j , a i ).</p>
<p>We now construct two directed metric, polyhedra P (L) and P (L) in which the directed metric space (L, d) is isometrically embedded as a special set of extremal rays.</p>
<p>To that end, we equip {R∪{∞}} n {(∞, . . ., ∞)} for n 2, with the Funk metric D defined by
(30) D(x, y) := inf{λ ∈ R ∪ {+∞} | λ + x y} = max i {y i − x i | x i = ∞} .
This is a directed metric.Note that it takes possibly negative values, and that it can also take the value ∞.</p>
<p>We also denote by D t the transpose directed metric with D t (x, y) := D(y, x).</p>
<p>Definition 4. Let (P (L), D) be the directed metric polyhedron
(31) P (L) := {x = (x 1 , . . . , x n ) ∈ {R ∪ {∞}} n {(∞, . . . , ∞)}|x i x j + d i,j }.
Moreover let ( P (L), D t ) be the directed metric polyhedron
(32) P (L) := {y = (y 1 , . . . , y n ) ∈ {R ∪ {∞}} n {(∞, . . . , ∞)}|y i y j + d j,i }.
For the following proposition, we need to extend the Funk metric in eq.30 to n = 1.For this we will use the fact that max ∅ = −∞.From this it follows that for
n = 1 the Funk metric d R : (−∞, ∞] 2 → [−∞, ∞] is given by (33) d R (s, t) := t − s if s = ∞ and d R (∞, t) = −∞ . In particular d R (∞, ∞) = −∞.
Notice that d R can also take the value −∞ and this case of a directed metric, was explained in Remark 2.
Proposition 3. P (L) is the set of non expansive maps x : (L, d t ) → ((−∞, ∞], d R ). Namely x satisfies (34) d R (x(a j ), x(a i )) d t (a j , a i ) Moreover P (L) is the set of non expansive maps y : (L, d) → ((−∞, ∞], d R ). Namely y satisfies (35) d F (y(a j ), y(a i )) d(a j , a i )
Proof.To see this description of P (L), let
x i := x(a i ). Then d R (x(a j ), x(a i )) d t (a j , a i ) ⇐⇒ x(a i ) − x(a j ) d(a i , a j ) ⇐⇒ x i − x j d i,j .
Likewise to see this description of P (L), let y i := y(a i ).Then
d R (y(a j ), y(a i )) d(a j , a i ) ⇐⇒ y(a i ) − y(a j ) d(a j , a i ) ⇐⇒ y i − y j d j,i .
Remark 9. Following Proposition 3 we see that we can view P (L) as a space of functions on the metric space L and we will see in Section 4 that it is similar to considering real vectors as real valued functions on a set.</p>
<p>Proposition 4. The map
(36) Y : (L, d) ֒→ (P (L), D) given by Y (a k ) := d(−, a k )
is called the Yoneda embedding 4 and is an isometric embedding.Moreover the map
(37) Y : (L, d) ֒→ ( P (L), D t ) given by Y (a k ) := d(a k , −)
is also an isometric embedding and is called the co-Yoneda embedding.</p>
<p>Proof.First note that for any
a k ∈ L the function Y (a k ) := d(−, a k ) is in P (L) and the function Y := d(a k , −) : L → [0, ∞] is in P (L). Indeed by the triangle inequality, d(a i , a k ) d(a i , a j ) + d(a j , a k ), in other words if x := d(−, a k ) and x i := d(a i , a k ) then x i x j + d i,j proving that Y (a k ) ∈ P (L).
Analogously d(a k , a i ) d(a k , aj) + d(a j , a i ) and therefore if y := d(a k , −) then y i y j + d j,i proving that Y (a k ) ∈ P (L). 4 The reason for the name Yoneda embedding comes from its appearance in category theory and was explained in [1].It is similar to the so called, Kuratowski embedding of a metric space.</p>
<p>Moreover, the inequality Equation (29) entails that d(−, a i )+d(a i , a j ) d(−, a j ), and so, D(d(−, a i ), d(−, a j )) d(a i , a j ).On the other hand, if x, y ∈ P (L) then D(x, y) y l − x l for all l and thus
D(d(−, a i ), d(−, a j ) d(a i , a j ) − d(a i , a i ) = d(a i , a j ). Consequently D(d(−, a i ), d(−, a j )) = d(a i , a j ) i.e. a → d(−, a) is an isometry. Likewise we have d(a j , a i ) + d(a i , −) d(a j , −) and d(a j , −) − d(a i , −) d j,i which implies that D(d(a i , −), d(a j , −)) d j,i . Moreover D(d(a i , −), d(a j , −)) d(a j , a i ) − d(a i , a i ) = d(a j , a i ). Thus D(d(a i , −), d(a j , −)) = d j,i .
To further understand the polyhedron P (L) we consider the change of variables z i := e −xi and introduce the following:
Definition 5. Let Q(L) be the polyhedral cone (38) Q(L) := {z = (z 1 , . . . z n ) ∈ [0, ∞) n {(0, . . . , 0)}|z i Pr(a j |a i )z j } Moreover let Q(L) be the polyhedral cone (39) Q(L) := {u = (u 1 , . . . u n ) ∈ [0, ∞) n {(0, . . . , 0)}|u i Pr(a i |a j )u j } Note that if z is in Q(L) then λz ∈ Q(L) for λ ∈ [0, ∞) therefore Q(L)
is indeed a polyhedral cone in the positive orthant and so is Q(L).</p>
<p>To simplify notation we introduce the convention that if
v = (v 1 , . . . , v n ) ∈ R n then (40)
e v := (e v1 , . . ., e vn ) and log(v
) := (log(v 1 ), . . . , log(v n ))
We see that
(41) Q(L) = {z ∈ [0, ∞) n |z := e −x for x ∈ P (L)} and (42) Q(L) = {u ∈ [0, ∞) n |u := e −y for y ∈ P (L)}
And vice versa (43)
P (L) = {x ∈ (−∞, ∞] n |x := − log(z) for z =∈ Q(L)} and (44) P (L) = {y ∈ (−∞, ∞] n |y := − log(u) for u ∈ Q(L)}.
Using the map − log : Q(L) → P (L) we can define a directed metric D Q on Q(L) using the Funk metric D on P (L).We put
(45) D Q (z, z ′ ) := max i {log( z i z ′ i )|z ′ i = 0}.
By definition we have
(46) D Q (z, z ′ ) = D(− log z, − log z ′ ) and D(x, x ′ ) = D Q (e −x , e −x ′ ).
Clearly the transpose D t Q defines a directed metric on Q(L).Then Proposition 4 implies that Corollary 2. The maps
(47) e −Y : (L, d) → (Q(L), D Q ) and e − Y : (L, d) → ( Q(L), D t Q ) are isometric embeddings Proof. It follows from Proposition 2 since d(a k , a l ) = D(Y (a k ), Y (a l )) = D Q (e −Y (a k ) , e −Y (a l ) ) .
We define the unit simplex
∆ := {z ∈ [0, 1] n | i z i = 1} . Definition 6. Define the polyhedron Q 0 (L) by (48) Q 0 (L) := Q(L) ∩ ∆. Then (Q 0 (L), D Q ) is a directed metric polyhedron and points in Q 0 (L) are proba- bility distributions.
Analogously we define the polyhedron Q 0 (L) as the intersection of Q(L) with the unit simplex and D t Q is a directed metric on it.</p>
<p>Remark 10.The polyhedra Q 0 (L) and Q 0 (L) define a normalization to probability distributions of our probabilistic language model L. Indeed in the definition of (L, , Pr) we only ask for the conditional probabilities multiplicative property but there is no normalization to a probability distribution.Now if we consider the vertex of Q 0 (L), corresponding to the ray generated by e −Y (a k ) , it will be
1 n(a k ) e −Y (a k )
where
n(a k ) := aj a k (e −Y (a k ) ) j = aj a k Pr(a k |a j )
is the normalization factor.While the vertex of Q 0 (L)corresponding to the ray generated by e − Y (a k ) will be
1 n(a k ) e −Y (a k )
where
n(a k ) := a k a l (e − Y (a k ) ) j = a k a l Pr(a l |a k )
is the normalization factor.</p>
<p>Remark 11.The polyhedral cone Q(L) is a generalization of the order polytope defined by Stanley [18].The order polytope corresponds to the case where Pr(a j |a i ) takes only the values 0 or 1, moreover, Stanley adds the "box constraint"
z i ∈ [0, 1] which translates to x i ∈ [0, ∞].
Up to the box constraint, Q(L) corresponds to the order polytope of the poset (L op , ) where L op is the opposite poset.</p>
<p>Stanley [18] proves that vertices of an order polytope correspond to upper sets of the poset.We will prove a generalization of that result in Theorem 2 in section 3.2.</p>
<p>We now explain what is the geometric meaning of the coordinates of a point x = (x 1 , x 2 , . . ., x n ) ∈ P (L) and a point y = (y 1 , y 2 , . . ., y n ) ∈ P (L).
Proposition 5. If x ∈ P (L) then (49) x i = D(d(−, a i ), x) = D(Y (a i ), x).
Moreover if y ∈ P (L) then (50)
y i = D t (y, d(a i , −)) = D( Y (a i ), y). Proof. We have D(d(−, a i ), x)) x i − d(a i , a i ) = x i .
On the other hand D(d(−, a i ), x)) = max j {x j − d j,i } x i where the last inequality follows because x ∈ P (L) ⇐⇒ x j x i + d j,i which implies x j − d j,i x i .</p>
<p>Moreover if y ∈ P (L) then D t (y, d(a i , −))
y i − d(a i , a i ) = y i
On the other hand D t (y, d(a i , −)) = max j {y j − d i,j } y i since y ∈ P (L) ⇐⇒ y j y i + d i,j which implies y j − d i,j y i .</p>
<p>Remark 12.</p>
<p>(1) We note that using the previous proposition, the defining inequalities,
x i x j + d i,j of P (L) become (51) D(Y (a i ), x) D(Y (a i ), Y (a j )) + D(Y (a j ), x).
Namely they are triangle inequalities for maps
x : L → ((−∞, ∞], d R ) and for the maps Y (a k ) = d(−, a k ) : L → ((−∞, ∞], d R ).
Therefore we can think of P (L) as the space of all maps x = L → ((−∞, ∞], d R ) that satisfy the triangle inequalities for the metric D, with respect to all the maps
Y (a k ) = d(−, a k ) : L → ((−∞, ∞], d R ). Thus P (L) is a kind of convex metric span of the maps Y (a k ) = d(−, a k ) : L → ((−∞, ∞], d R ). (2) Analogously, the defining inequalities, y i y j + d j,i of P (L) become (52) D t (y, Y (a i )) D t (y, Y (a j )) + D t ( Y (a j ), Y (a i )),
namely the triangle inequalities for maps y : L → ((−∞, ∞], d R ).This implies that P (L) is the space of all maps y = L → ((−∞, ∞], d R ) that satisfy the triangle inequalities for the metric D t , with respect to all the maps
Y (a k ) = d(a k , −) : L → ((−∞, ∞], d R )
. Again we see P (L) as a kind of convex metric span of Y (a k ) = d(a k , −) (3) A restatement of (51) is to say that the shortest path that connects Y (a i ) and x is at most as long as the shortest path that connects them but has to also go though a j .Analogously for (52).( 4) Note that all constructions and results in this section work for a general directed metric space and not just for the special case of a probabilistic Language model 3.1.Texts define special Extremal rays of P (L) and Q(L).</p>
<p>Definition 7.An extremal ray of a polyhedral cone in R n is a ray generated by a vector that cannot be expressed as a positive linear combination of two nonproportional vectors in the polyhedral cone.</p>
<p>Recall that a vector in a polyhedral cone in R n generates an extremal ray if and only if it saturates n − 1 linearly independent inequalities [17].Definition 8.An additive extremal ray of P (L) (respectively P (L)) is defined to be the image under − log of a usual extremal ray of the polyhedral cone Q(L) (respectively Q(L)).</p>
<p>Note that the name additive extremal ray is chosen since a usual extremal ray in Q(L) is invariant under scaling by λ and therefore its image under − log is invariant under translation by − log λ.Note that the extremal rays of Q(L) and of Q(L) have generators which have in general some zero coordinates.Then, their − log-images have vectors such that some of their coordinates are ∞, which is why we speak of "additive extremal rays" of P (L) and P (L) (they are not extremal rays in the usual sense).</p>
<p>From now on though we will simply refer to the additive extremal rays of P (L) as extremal rays, when there is no chance of confusion.</p>
<p>We now define a directed graph associated to any point x ∈ P (L) which encodes the saturated inequalities satisfied by the point x and which we call its saturation graph S(x).Definition 9. Let x ∈ P (L).Define S(x), the saturation graph of x, to be the graph whose vertices are the elements of L and whose set of directed edges E(x) is the set of saturated inequalities that coordinates of x satisfy, namely E(x) := {(a i , a j ) :
x i = x j + d i,j }. When (a i , a j ) ∈ E(x) we introduce a directed edge from a i to a j .
The graph always contains trivial arcs (a i , a i ) (loops), for i ∈ [n], since d i,i = 0.It contains non-trivial arcs if and only if x is on the boundary of P .</p>
<p>The graph S(x) and in particular its support Supp(x) encodes all the hyperplanes on which x lies.</p>
<p>Note that the graph can be disconnected.Proof.We have Y (a k ) := d(−, a k ), and therefore
Y (a k ) i = d(a i , a k ), i = 1 . . . |L|.
Define the support of Y (a k ), Supp(Y (a k )), to be the set of texts a i such that Y (a k ) i is finite.We recall that a vector in a cone in R n defined by finitely many linear constraints generates an extreme ray of the cone if, and only if, the family of gradients of active constraints at this point is of rank n − 1.</p>
<p>Let x := Y (a k ), and y ∈ Q(L) denote the image of x by the map which applies exp(−•) entrywise.Each edge (a i , a j ) of the saturation graph S(Y (a k )) yields x i = d(a i , a j )+x j , and so the vector y induces the active inequality y i = Pr(a j |a i )y j with gradient e i − Pr(a j |a i )e j where e i denotes the ith vector of the canonical basis of R n .Moreover, each text a i in L\Supp(Y (a k )) yields the active inequality y i = 0, with gradient e i .</p>
<p>The saturation graph S(Y (a k )) has a connected component which is a directed tree with a k as its root since
Y (a k ) i = Y (a k ) j + d i,j ⇐⇒ d(a i , a k ) = d(a i , a j ) + d(a j ,
a k ) and from corollary 1 it follows that a i a j a k , namely a j extends a i and a k extends a j .It has also trivial connected components, reduced to loops at the vertices a i such that a i ∈ Supp(Y (a k )).Using the fact that the non-trivial connected component of S(Y (A k ) is a tree, we see that any vector z satisfying the saturated equalities is uniquely defined by its value on the root of the tree.Hence, the space orthogonal to the family e i − Pr(a j |a i )e i with (i, j) ∈ S(y k ) and e l with a l ∈ L \ Supp(Y (a k )) is of dimension one, which entails that this family is of rank
|L| − 1, showing that y is an extreme ray of Q(L). Likewise let Y (a k ) := d(a k , −) and Y (a k ) i = d(a k , a i ). If a k a i a j then d(a k , a j ) = d(a k , a i ) + d(a i , a j ), i.e. Y (a k ) j = Y (a k ) i + d i,j .
The saturation graph for Y (a k ) is the same as for Y (a k ) with all arrow reversed.Therefore the same proof applies.</p>
<p>It follows from Theorem 1, that we can identify the texts in L with some of the extremal rays of P (L) and also with some of the extremal rays in P (L).</p>
<p>However there are many other extremal rays of P (L), which we next characterize.</p>
<p>3.2.All Extremal rays correspond to connected lower sets of L. Consider the equations y i Pr(a j |a i )y j which define Q(L).We denote P i,j := Pr(a j |a i ).If we assume that a 0 ∈ L is the empty text, we have a 0 a i a j for any a i a j , and then Pr(a j |a 0 ) = Pr(a i |a 0 ) Pr(a j |a i ).</p>
<p>(53)</p>
<p>Define P i := Pr(a i |a 0 ) then P i,j = P j P i .</p>
<p>Therefore y i P i,j y j becomes P i y i P j y j .</p>
<p>(54) Define ỹ = (ỹ 1 , . . .ỹn ) where ỹi := P i y i .</p>
<p>Notice that this change of coordinates maps extremal rays to extremal rays.We get then a new polyhedral cone (55) Q(L) := {ỹ = (ỹ 1 , . . ., ỹn ) ∈ [0, ∞) n {(0, . . ., 0)}| ỹi ỹj whenever a i a j }.The change of variables mapping Q(L) to Q(L) can be also done under our more general assumption of a Probabilistic language model without assuming the existence of a global minimum a 0 ∈ L. Indeed Proposition 6.Let (L, , Pr) be a probabilistic language model then there is a diagonal change of variables mapping Q(L) to Q(L).
Therefore Q(L) := {ỹ|y ∈ Q(L)}. Q(L)
Proof.The fact that (L, , Pr) is a probabilistic language model means that a i a j a k is equivalent to Pr(a k |a i ) = Pr(a k |a j ) Pr(a j |a i ) and Pr(a k |a i ) &lt; ∞.</p>
<p>We define the directed graph G whose nodes are the texts a 1 , . . ., a n .If a i a j , we draw an arrow from node a i to node a j with weight Pr(a j |a i ).If a j a i , we draw an arrow from node a i to node a j with weight Pr(a j |a i ) −1 .Consider the Hasse diagram of L. We make use of the observation in Proposition 2. For every connected component C m , let us select arbitrarily an element c m .If a i ∈ C m , we define w i to be the weight in the graph G of an arbitrary path from the point c m to a i .Owing to our main assumption, (1), the weight is independent of the choice of the path from c m to a i .Moreover, for all i, j such that a i a j , we have w i Pr(a j |a i )w −1 j = 1.Setting ỹi = w i y i , we rewrite the constraint y i Pr(a j |a i )y j as ỹi ỹj .</p>
<p>In this way, we transformed Q(L) to Q(L) by a diagonal scaling.</p>
<p>We then have the following Proof.Let {λ 1 , . . ., λ s } be the distinct values taken by ỹi , ordered so that 0
λ 1 &lt; λ 2 &lt; • • • &lt; λ s . Let L m = {i|ỹ i = λ m }.
We define the rank of a family of affine inequalities to be the rank of the family of gradients of the affine forms defining these inequalities.For ỹ to be an extremal ray it has to saturate a family of inequalities of rank n − 1, where |L| = n, see [17].</p>
<p>Let us first assume λ 1 = 0.The rank of the family of saturated constraints given by L 1 is then r 1 = |L 1 | since we get equations of the form ỹi = 0 which is a hyperplane normal to e i for i ∈ L 1 .</p>
<p>Moreover, we claim that the rank r k of the family of saturated constraints given by any L k for k = 1 given by r k = |L k | − c k where c k is the number of connected components of the Hasse diagram of (L k , ).To see this, it suffices to observe that a solution h ∈ R L k of the system of saturated inequalities h i = h j for a i a j and a i , a j ∈ L k is uniquely determined by fixing precisely one coordinate of h on every connected component of the Hasse diagram (in other words, we have c k degrees of freedom for the choice of h).</p>
<p>Therefore the rank of the family of saturated constraints at the point ỹ is less than
or equal to |L 1 |+|L 2 |−1+• • •+|L s |−1. We also have that |L 1 |+|L 2 |+• • •+|L s | = n.
We know though that ỹ is an extremal ray if and only if the rank of the family of saturated constraints is n − 1. Therefore we must have n
− 1 |L 1 | + (|L 2 | − 1) + • • • + (|L s | − 1) = n − s + 1.
This is only possible if s 2 but, for ỹ to generate an extremal ray, not all coordinates of ỹ can be 0, and then our assumption λ 1 = 0 excludes the case s = 1.This entails that s 2 and therefore s = 2.We then have n
− 1 = |L 1 | + |L 2 | − 1.
In that case ỹi = 0 for i ∈ L 1 and ỹj = λ 2 for j ∈ L 2 .We then scale ỹ by 1 λ2 so as to get a representative vector of the same the ray with ỹi = 0 for i ∈ L 1 and ỹj = 1 for j ∈ L 2 .Therefore ỹ is the characteristic function of L 2 .</p>
<p>Moreover L 2 is a lower set.Indeed if a j ∈ L 2 and a i a j then a i ∈ L 2 .This holds because a j ∈ L 2 implies ỹ(a j ) = 1 and a i a j implies ỹ(a i ) = ỹ(a j ) = 1 therefore a i ∈ L 2 .</p>
<p>If now
λ 1 &gt; 0 then n − 1 (|L 1 | − 1) + (|L 2 | − 1) + • • • + (|L s | − 1) = n − s therefore s 1 which implies s = 1 and |L 1 | = n.
In that case we have a single extremal ray ỹ = (1, 1, . . ., 1) which is the characteristic function of the maximal lower set L 1 = L.</p>
<p>Conversely let C be a lower set in L and let ỹ : L → {0, 1} be the characteristic function of U .Consider a j ∈ U then ỹ(a j ) = 1.Now if a i a j then a i ∈ U and therefore ỹ(a i ) = 1 which means ỹ(a j ) = ỹ(a i ).</p>
<p>Remark 13.Note that if L admits a bottom element a 0 then any lower set is connected since it must include a 0 , and the Hasse diagram of L contains a path from a 0 to every element of L.</p>
<p>Notice that Stanley in [18] has proven that vertices of his order polytope correspond to upper sets.In contrast, rays of Q(L) correspond only to connected lower sets.Notwithstanding the order reversal, there is a discrepancy which arises because Stanley considers the intersection of Q(L) with a box, with creates additional vertices, not associated to rays of Q(L).</p>
<p>Remark 14.Note that a vector Y (a k ) ∈ P (L) corresponds to the principal lower set generated by a k .We will therefore call the extremal rays generated by images of the Yoneda embedding, principal extremal rays.Corollary 3. Assume L includes the bottom element a 0 and recall from (53) that P i := Pr(a i |a 0 ).If C is a lower set, the extremal ray corresponding to C is generated by y in Q(L) with coordinates (56)
y i = 1 Pi if a i ∈ C, 0 if a i not in C.
Proof.It follows from Theorem 2 and the change of coordinates ỹi = P i y i in eq 53.</p>
<p>Remark 15.Notice that Corollary 3 is consistent with the coordinates of an extremal ray y in Q(L), corresponding to a text a j ∈ L. Indeed according to corollary 2, any element y on the extremal ray has y i = λ Pi for λ ∈ [0, ∞).We also have y j = Pr(a j |a j ) = 1, therefore λ = P j .This implies
y i = Pj Pi = Pr(a j |a i ) (57) y i = Pr(a j |a i ) if a i a j , 0 if a i not a subtext of a j .y i = 1 w c i , for a i ∈ C, y i = 0 for a i ∈ L \ {C}
generates an extreme ray of Q(L), and all the extreme rays arise in this way.</p>
<p>Proof.We showed in Theorem 2 that ỹ is a positive scalar multiple of the characteristic function of C. If a i belongs to C, we have y i = (w c i ) −1 ỹi , from which (58) follows.We note that a change of the reference point c in C only modifies the vector w c by a positive scalar multiple.Indeed, for all c and c ′ ∈ C, we have w c = µw c ′ where µ is the weight of any path from c to c ′ in the directed graph G.</p>
<p>Proposition 8.If y generates an extremal ray of Q(L) corresponding to a lower set C in L then the saturation graph of y has an edge from a i to a j if and only if a i a j for a i , a j ∈ C.</p>
<p>Proof.This follows from (58), using the main assumption (1).Indeed if a i a j in C then we have y i = 1 w c i and y j = 1 w c j .Therefore y i = w c j wi y j and therefore y i = Pr(a j |a i )y j which means that there is an edge from a i to a j in the saturation graph of y.Proof.We have Q(L) = Q(L op ) and upper sets of L correspond to lower sets of L op therefore the result follows from Proposition 7.</p>
<p>Remark 16.Note that Proposition 6, and Theorem 2 are only valid for a probabilistic language model and not for general directed metric space.In the latter case there will still be exponentially many extremal rays not coming from the Yoneda embedding, but we the characterization in terms of connected lower sets no longer holds.</p>
<p>Corollary 5.If the empty text a 0 is in L then the set P 0 (L) of extremal rays of P (L) is identified with the lower set completion of the poset L.</p>
<p>Proof.Since a 0 ∈ L, every lower set of L is connected so P 0 (L) is identified with the set of lower sets of L.</p>
<p>Remark 17.Note that having explicit equations for the polyhedral cone Q(L), the extremal rays of Q(L) can be computed, for instance by the double description method [6].</p>
<p>The polyhedron P (L) as a (min, +) linear space</p>
<p>To further understand the polyhedra P (L) and P (L) we need to consider their description in terms of tropical algebra.</p>
<p>Consider the metric space (L, d).Recall the (min, +) (tropical) semifield R min defined as
R min := ((−∞, ∞], ⊕ min , ⊙) where for s, t ∈ (−∞, ∞],(59)
s ⊕ min t := min{s, t} and s ⊙ t := s + t.</p>
<p>We denote by d min : R n → R n the (min, +) linear operator defined by x j + d i,j .Likewise for Im(d t min ) we get x i x j + d j,i .</p>
<p>Since we use much more often the (min, +) semifield than the (max, +) that will appear later on, to simplify notation we denote ⊕ min by ⊕.</p>
<p>In particular we introduce the notation, for u, v ∈ R n , (u ⊕ v) i := min{u i , v i }.We then have Corollary 6.If x ∈ P (L) then
(61) x = ⊕ j D(Y (a j ), x) ⊙ Y (a j ).
Proof.From Proposition 10, x ∈ P (L) = Im(d min ) = Fix(d min ) ⇐⇒ d min (x) = x.Therefore we have the (min, +) linear expression for x in terms of the columns of d:
x = ⊕ j x j ⊙ d(−, a j ) = ⊕ j x j ⊙ Y (a j ) = ⊕ j D(Y (a j ), x) ⊙ Y (a j ).
It is known that an order polytope, and more generally, an alcoved polytope (of A n type) is closed under min and max, (61) expresses this fact for our metric case for min.In Proposition 29 we will see that P (L) is also closed under max.</p>
<p>Proposition 11.We have
(62) Y (a k ) = ⊕ aj a k d j,k ⊙ Y (a j )
and
(63) Y (a k ) = ⊕ a k a l d k,l ⊙ Y (a l ) Proof. The fact that d 2 min = d min is equivalent to (64) d i,k = min j {d i,j + d j,k }.
We have Y (a k ) := d(−, a k ) and Y (a j ) := d(−, a j ) therefore eq.64 implies
Y (a k ) i = ⊕ j d j,k ⊙ Y (a j ) i which means Y (a k ) = ⊕ j d j,k ⊙ Y (a j )
. Since d j,k = ∞ unless a j a k we have
Y (a k ) = ⊕ aj a k d j,k ⊙ Y (a j )
.</p>
<p>Analogously for d t we have
d t i,k = min l {d t i,l + d t l,k } ⇐⇒ d k,i = min l {d k,l + d l.i }. Recall that Y (a k ) := d(a k , −) and Y (a l ) := d(a l , −). This implies Y (a k ) = ⊕ l d k,l ⊙ Y (a l ). Since d k,l = ∞ unless a k a l we have Y (a k ) = ⊕ a k a l d k,l ⊙ Y (a l ) .
Finally we have the following Proposition 12.The Funk metric D(x, y) := max i {y i − x i |x i = ∞} has the property that D(−, w) is tropically antilinear, namely
(65) D(λ 1 ⊙ x ⊕ min λ 2 ⊙ y, z) = −λ 1 ⊙ D(x, z) ⊕ max −λ 2 ⊙ D(y, z) while D(w, −) is linear, namely (66) D(x, λ 1 ⊙ y ⊕ max λ 2 ⊙ z) = λ 1 ⊙ D(x, z) ⊕ max λ 2 ⊙ D(y, z). Proof. We have D(λ ⊙ x, y) = max i {y i − λ − x i } = D(x, y) − λ. We calculate D(x⊕ min y, z) = max i {z i −min{x i , y i }} = max i {z i +max i {−x i , −y i }} = = max{max i {z i − x i }, max{z i − y i }} = D(x, z) ⊕ max D(y, z). Moreover D(x, λ ⊙ y) = max i {λ + y i − x i } = λ − D(x, y). Finally, D(x, y ⊕ max z) = D(x, max{y, z}) = max i {max{y i , z i } − x i } = = max i {max{y i − x i }, max{z i − x i }} = max{max i {y i − x i }, max i {z i − x i }} = = D(x, y) ⊕ max D(x, z)
This means that we can think of D as a tropical inner product.</p>
<p>Remark 18.All the results in this section hold for a general directed metric space.</p>
<p>4.1.P (L) and P (L) as Semantic spaces.We already mentioned in the overview that we consider Y (a k ) := d(−, a k ) as well as Y (a k ) := d(a k , −) as encoding the meanings of text a k in accordance with the statistical semantics principal namely that texts that appear in similar contexts have similar meaning.The function d(a k , −) is supported on extensions of a k while d(−, a k ) is supported on restrictions of a k .</p>
<p>However it is also the position of these vectors in P (L) and P (L) that contains semantic information since for example, Y (a k ) = d(−, a k ) for a k a word is supported only on that word while D(Y (a k ), −) is supported on all extensions of a k .</p>
<p>Therefore more generally, we think of (P (L), D) and ( P (L), D t ) as "semantic spaces" giving mathematical substance to the statistical semantics hypothesis.This point if view was advocated in [1].</p>
<p>We further explain our view about the syntax to semantics problem in Appendix B and show that it is located in the realm of a deep and general duality in mathematics which in some cases appears as a duality between algebra and geometry.</p>
<p>It is interesting that even though the whole space P (L) (or Q(L)) and P (L) (or Q(L)) appear as a spaces of meanings, texts appear only as special extremal rays.They are the "observable" variables while other points of P (L) are like "hidden" variables.</p>
<p>The systems of equations Proposition 11 Equation (62), Equation (63) express the (min, +) linear relations satisfied by the Yoneda and co-Yoneda embedding text vectors.They are reminiscent of vector equations between word vectors as appeared first in [13].</p>
<p>Another way to think about them is as equations that implement the constraints imposed by the probabilities of extension.It is common to consider constraints giving rise to equations defining a geometric object and here we have something analogous but in (min, +) algebra.</p>
<p>Moreover we note that any (min, +) linear combination can be transformed into a Boltzmann weighted usual linear combination using a small temperature parameter and the identity Equation (91) lim T →0 −T log(e −y/T + e −z/T ) = min{y, z}.</p>
<p>Using this we will also show in Corollary 9, Equation ( 90) that e −Y (a k ) can be approximated by a Boltzmann weighted linear combinations of word vectors for the words that make up that text.We note the similarity of this with the expression of a value vector for a text in terms of word vectors, in the attention layer of a transformer.</p>
<p>Notice also that from the formulation of probabilistic language models, vectors arise naturally, first in the (min, +) context but later in Boltzmann weighted usual linear combinations (section 5.1).</p>
<p>Moreover we will show in Section 6 that there is a duality relating P (L) and P (L) as well as Q(L) and Q(L) (they are isometric and tropically anti-isomorphic).This shows that given a corpus, the semantic information given by extensions of texts is equivalent to that given by restrictions.</p>
<p>We note that if the transformer is computing an approximation to P (L) then the fact that it is a convex space could explain why the gradient descent during training converges nicely.</p>
<p>Since the transformer computes probabilities for all possible next words to a text it is natural to think that the corresponding probabilistic language model (L, , Pr) contains the whole free monoid generated by words and all texts appear as extremal rays of P (L) corresponding to principal upper sets.Wrong texts are very far away from correct texts as they are very unlikely.</p>
<p>In that case the neural network should then learn an effective representation of P (L), which a priori has a huge dimension.How the neural network is able to construct an effective approximation of such a huge dimensional space is not clear to us.</p>
<p>From another point of view we see that if we consider that the transformer neural network is learning P (L) then we can think of training the transformer as finding a solution to the huge (min, +) system of Equation ( 63), Proposition 11, given the coefficients d j,k .</p>
<p>We will see a small example of the polyhedra Q(L) and Q(L) as well as the dualities, in section 6.</p>
<p>Further evidence for these spaces as semantic spaces is provided by the fact that they have a Heyting algebra structure (which is a generalization of a Boolean algebra) as explained in [1].</p>
<p>As already mentioned, experiments using (a slight variant of ) the co-Yoneda vectors d(−, a k ) were performed in [12] where an actual transformer neural network was used to sample continuations of texts and construct the co Yoneda vectors.The authors tested these vectors on several semantic tasks and obtained very good results.</p>
<p>4.2.</p>
<p>From one word text extensions to longer extensions.We now explain how to go from one word extension probabilities to the metric d.</p>
<p>However d constructed in this way does not satisfy the main assumption of probabilistic language model Pr(a k |a i ) = Pr(a j |a i ) Pr(a k |a j ).It does satisfy that d is a directed metric and therefore d is a (min, +) projector.</p>
<p>Indeed, let C be the matrix of one word extensions.Namely for texts a i and a j we put (67) C(a i , a j ) =    − log Pr(a j |a i ) if a i a j and a j extends a i by a single word, ∞ if a i a j and a j extends a i by more than a single word, ∞ if a i and a j are not comparable.</p>
<p>Let Id denote the matrix with Id i,i = 0 and Id i,j = ∞ for i = j.Id is the identity matrix in the (min, +) matrix semiring.Indeed C Id = Id C = C.We note that C i,i = 0 and therefore C ⊕ Id = C.</p>
<p>In that case the tropical power C l computes distances for up to l word extensions.</p>
<p>If we bound the number of words in the extension to say k then C k = C k+1 and d = C k is our metric.
Proposition 13. Let C be such that C i,i = 0. If d = C k = C k+1 then (68) x = dx ⇐⇒ x = Cx Proof. If x = dx = C k x then Cx = C k+1 x = C k x = dx = x therefore solutions of x = dx are also solutions of x = Cx. On the other hand if x = Cx then C k x = x i.e. dx = x.
Since the diagonal entries of d, and C, are equal to 0, the equations x = dx and x = Cx are equivalent to x dx and x Cx, respectively.These two systems of inequalities describe the same polyhedron.</p>
<p>Compatibility of P (L) with adding more texts</p>
<p>When training the neural network to learn by predicting continuations of texts we add more and more text.Moreover we have already mentioned in Remark 7 that it is natural to grade L by word length of texts.It is therefore important to understand how P (L) changes as we add more and more text.</p>
<p>We have the following: We will prove this in full generality and derive the probabilistic language model case as a special case.Theorem 3. Let φ : (X 1 , δ 1 ) ֒→ (X 2 , δ 2 ) be an isometric embedding of discrete, finite, directed metric spaces, then there is an isometric embedding φ : (P (X 1 ), ∆ 1 ) ֒→ (P (X 2 ), ∆ 2 ) compatible with the Yoneda isometric embeddings Y 1 : X 1 → P (X 1 ) and Y 2 : X 2 → P (X 2 ), namely φ(Y 1 (a)) = Y 2 (φ(a)).Moreover φ(P (X 1 )) is a retraction (i.e. a non-expansive (min, +) projection) of P (X 2 ).</p>
<p>Proof. Say X
1 := {a 1 . . . a n } and X 2 := {b 1 . . . b n , b n+1 , . . . b n+k }, where b j = φ(a j ) for j = 1 . . . n. Recall that we have P (X 1 ) = Im(δ 1 ) is the span of Y 1 (a j ) := δ 1 (−, a j ) and P (X 2 ) = Im(δ 2 ) is the span of Y 2 (b j ) := δ 2 (−, b j ).
Let e m := (∞, . . ., 0, . . ., ∞) for m = 1, . . .n, so that e 1 , . . ., e n is a basis (free and generating family) of the module (R min ) n of the (min,+) semifield R min .We define
φ(⊕ n m=1 x m ⊙ e m ) := ⊕ n m=1 x m ⊙ δ 2 (−, b m ) We now show that φ(δ 1 (−, a i )) = δ 2 (−, b i ). for i = 1, . . . , n. Indeed, φ(δ 1 (−, a i )) = φ(⊕ n j=1 δ 1 (a j , a i ) ⊙ e j ) = ⊕ n j=1 δ 1 (a j , a i )δ 2 (−, b j ) = δ 2 (−, b i ).
Indeed, the last equality holds since
⊕ n j=1 δ 1 (a j , a i ) ⊙ δ 2 (b l , b j ) = ⊕ n j=1 δ 2 (b j , b i ) ⊙ δ 2 (b l , b j ) = δ 2 (b l , b i
), in which the last equality follows from the fact that δ 2 is a (min,+) idempotent.</p>
<p>Note that φ is well defined since any x ∈ (R min ) n has a unique expression in the basis e k , k = 1, . . ., n.If we attempted to define it directly on the (min, +) module spanned by the vectors δ 1 (−, a i ) we would have to deal with the complication that x ∈ R n does not always have a unique expression as a (min, +) combination of these vectors.In fact, one can show that only vectors in the interior of P (L 1 ) would have such unique expressions.</p>
<p>We now check that φ is an isometric embedding.We want to check that
(69) ∆ 2 ( φ(x), φ(y)) = ∆ 1 (x, y) Recall that ∆ 1 (x, y) = max n j=1 {y j − x j |x j = ∞}. Moreover ∆ 2 ( φ(x), φ(y)) = max n+k j=1 { y j − x j |x j = ∞}.
From the definition of the x j the result follows.Finally we define the retraction R : P (X 2 ) → P (X 2 ) by ( 70)
R := n j=1 ∆ 2 (−, Y (b j )) ⊙ ∆ 2 (Y (b j ), −)
Note that as a matrix
(71) R i,k = R(Y (b i ), Y (b k ) = n j=1 δ 2 (b i , b j ) ⊙ δ 2 (b j , b k ) .
We need to check that R 2 = R, Im(R) = φ(P (X 1 )) and R is non-expansive.Let us check first that R 2 = R:
R 2 (Y 2 (b k ), Y 2 (b l )) = ⊕ n m=1 R(Y 2 (b k ), Y 2 (b m )) ⊙ R(Y 2 (b m ), Y 2 (b l )) = ⊕ n m,j1,j2=1 δ 2 (b k .b j1 ) + δ 2 (b j1 .b m ) + δ 2 (b m .b j2 ) + δ 2 (b m , b j2 ) + δ 2 (b j2 , b l ) = δ 2 (b k , b l ) = R(b k , b l ).
Where we have used the fact that
⊕ n l=1 δ 2 (b k , b l ) + δ 2 (b l , b m ) = ⊕ n l=1 δ 1 (a k , a l ) + δ 1 (a l , a m ) = δ 1 (a k , a m ) = δ 2 (b k , b m ). Next notice that clearly Im(R) ⊂ Span n j=1 {δ 2 (−, b j )} = φ(P (L 1 ). Moreover we claim that R(δ 2 (−, b k )) = δ 2 (−, b k ) Indeed R(δ 1 (−, b k )) = ⊕ n j=1 δ 2 (b j , b k ) ⊙ δ 2 (−, b j ). and thus R(δ 1 (b l , b k )) = ⊕ n j=1 δ 2 (b j , b k ) + δ 2 (b l , b j ) = δ 2 (b l , b k ), proving the claim. Therefore Span n j=1 {δ 2 (−, b j )} ⊂ φ(P (L 1 )) ⊂ Im(R) showing that Im(R) = φ(P (L 1 ).
Finally we check that R is non-expansive, namely that
∆ 2 (R(x), R(y)) ∆ 2 (x, y)
To that end note that R is order preserving and also R(α ⊙ x) = α ⊙ R(x).Indeed both of these statements follow from the (min, +) linearity of R.</p>
<p>In particular x y ⇐⇒ x ⊕ y = x which implies that R(x ⊕ y) = R(x) and therefore R(x) ⊕ R(y) = R(x) which means R(x) R(y).</p>
<p>Recall now that ∆ 2 (x, y) = inf{λ :
x λ ⊙ y} = max i {y i − x i |x i = ∞}. Then x ∆ 2 (x, y) ⊙ y =⇒ R(x) R(∆ 2 (x, y) ⊙ y) =⇒ R(x) ∆ 2 (x, y) ⊙ R(y) therefore ∆ 2 (R(x), R(y) ∆ 2 (x, y) Remark 19. Since φ(Y 1 (a j )) := Y 2 (φ(a j ) we have, if x := ⊕ n j=1 x j ⊙ Y 1 (a j ), φ(x) := ⊕ n j=1 x j ⊙ Y 2 (φ(a j )) = ⊕ n j=1 x j ⊙ Y 2 (b j ) = ⊕ n+k j=1 xj ⊙ Y 2 (b j )
, where x j := x j for j = 1, . . ., n and x j = ∞ for j = n + 1, . . ., n + k.So in these coordinates P (X 1 ) is cut out inside P (X 2 ) by the equations xj = ∞ for j = n + 1, . . ., n + k.In this sense, it constitutes a "face" of P (X 2 ) of (projective) dimension
|X 1 | − 1. Remark 20. Note that if x ∈ P (L 2 ) where x : L 2 → (−∞, ∞] and x i := x(b i ) then (72) R(x) = n j=1 ∆ 2 (Y 2 (b j ), x) ⊙ ∆ 2 (−, Y (b j )) : L 2 → (−∞, ∞]
and for i = 1 . . ., n + l (73)
R(x) i := R(x)(b i ) = n j=1 ∆ 2 (Y 2 (b i ), Y 2 (b j )) ⊙ ∆ 2 (Y 2 (b j ), x) = n j=1 d 2 (b i , b j ) ⊙ x j Therefore R(x) = n+l i=1 R(x) i ⊙ Y 2 (b i ) = n+l i=1 n j=1 d 2 (b i , b j ) ⊙ x j ⊙ Y 2 (b i ) = n+l i=1 n j=1 d 2 (b i , b j ) ⊙ ∆ 2 (Y (b j ), x) ⊙ Y 2 (b i ) (74)
5.1.Approximation of a text vector in terms of word vectors.Let us see how Theorem 3 applies to the probabilistic language model case.
j = φ(a j ) for j = 1 . . . n. Let Y 1 : (L 1 , d 1 ) → (P (L 1 ), D 1 ) and Y 2 : (L 2 , d 2 ) → (P (L 2 ), D 2 ) be the Yoneda isometric embeddings.
Let R : P (L 2 ) → P (L 2 ) be the non-expansive projection of Theorem 3 given by
(75) R := n j=1 D 2 (−, Y 2 (b j )) ⊙ D 2 (Y 2 (b j ), −).
Then for i, k = 1, . . ., n + l
(76) R(Y 2 (b k )) i = n j=1 d 2 (b i , b j ) ⊙ d 2 (b j , b k ) and (77) R(Y 2 (b k )) = n+l i=1 R(Y 2 (b k )) i ⊙ Y 2 (b i ) = n+l i=1 n j=1 d 2 (b i , b j ) ⊙ d 2 (b j , b k ) ⊙ Y 2 (b i ).
or equivalently
(78) R(Y 2 (b k )) = bi bj b k d 2 (b i , b j ) ⊙ d 2 (b j , b k ) ⊙ Y 2 (b i ).
Proof.We have
R = n j=1 D 2 (−, Y 2 (b j )) ⊙ D 2 (Y 2 (b j ), −), Applying to Y 2 (b k ) for k = 1, . . . n + l we get R(Y 2 (b k )) = n j=1 D 2 (−, Y 2 (b j )) ⊙ D 2 (Y 2 (b j ), Y 2 (b k )).
Since Y 2 is an isometric embedding we have
R(Y 2 (b k )) = n j=1 D 2 (−, Y 2 (b j )) ⊙ d 2 (b j , b k ) : L 2 → (−∞, ∞]. Therefore for i = 1, . . . , n + l R(Y 2 (b k )) i = R(Y 2 (b k ))(b i ) = n j=1 D 2 (Y 2 (b i ), Y (b j )) ⊙ d 2 (b j , b k ) = n j=1 d 2 (b i , b j ) ⊙ d 2 (b j , b k ) (79) Consequently (80) R(Y 2 (b k )) = ⊕ n+l i=1 R(Y 2 (b k )) j ⊙ Y (b i ) = n+l i=1 n j=1 d 2 (b i , b j )⊙ d 2 (b j , b k )⊙ Y 2 (b i ).
or equivalently
(81) R(Y 2 (b k )) = bi bj b k d 2 (b i , b j ) ⊙ d 2 (b j , b k ) ⊙ Y 2 (b i ). Remark 21. We see from eq 78, R(Y 2 (b k )) i = n j=1 d 2 (b i , b j ) ⊙ d 2 (b j , b k )
, that only summands such that b i b j b k , will be finite.</p>
<p>Remark 22. Recall that, according to Theorem 3, R is a non-expansive map therefore
(82) D(R(Y (b k )), R(Y (b l ))) D(Y (b k ), Y (b l )).
We can use the previous proposition in order to approximate a text vector by the vectors corresponding to words making up that text.Corollary 8. Let L := {b 1 , . . ., b N } be a probabilistic language model and let W := {w 1 . . ., w m } be the set of words identified with b 1 , . . ., b m and considered as a probabilistic language model with all pairwise distances equal to infinity.Let Y : L → P (L) be the Yoneda embedding.Let R : P (L) → P (L) be the nonexpansive projection given by
(83) R := m j=1 D(−, Y (w j )) ⊙ D(Y (w j ), −). Consider Y (b k ) ∈ P (L), then for i, k = 1, . . . , N (84) R(Y 2 (b k )) i = d 2 (w i , b k ) and (85) R(Y 2 (b k )) = N i=1 d 2 (w i , b k ) ⊙ Y 2 (w i ) = wi b k d 2 (w i , b k ) ⊙ Y 2 (w i )
Proof.Consider the projection R :
P (L) → P (L) given R = m j=1 D(−, Y (w j )) ⊙ D(Y (w j ), −),
where Im(R) = φ(P (W )).</p>
<p>We have identified b j with w j for j = 1, . . ., m therefore from corollary 5 we have for i, k = 1, . . ., N
(86) R(Y 2 (b k )) i = l j=1 d 2 (b i , w j ) ⊙ d 2 (w j , b k ). However d 2 (b i , w j ) is finite only of j=i and w j = b i . In that case d 2 (b i , w i ) = 0. Therefore for i = 1, . . . , N (87) R(Y 2 (b k )) i = d(w i , b k ). Consequently (88) R(Y 2 (b k )) = N i=1 d 2 (w i , b k ) ⊙ Y 2 (w i ) = wi b k d 2 (w i , b k ) ⊙ Y 2 (w i ) .
Corollary 9. Let L := {b 1 , . . ., b N } be a probabilistic language model and let W := {w 1 . . ., w m } be the set of words identified with b 1 , . . ., b m and considered as a probabilistic language model with all pairwise distances equal to infinity.Let Y : L → P (L) be the Yoneda embedding.Let T 0 be a parameter (which is usually called temperature), then we have
(89) R(Y (b k )) = lim T →0 −T log( wi b k e − d(w i ,b k ) T e − Y (w i ) T )
Therefore for small T we have
(90) e − R(Y (b k )) T ≈ wi b k e − d(w i ,b k ) T e − Y (w i ) T Proof. Recall the identity (91) lim T →0
−T log(e −y/T + e −z/T ) = min{y, z}.</p>
<p>Then eq. ( 88) implies the result.</p>
<p>Remark 23.Equation ( 90) is similar to the expression for a text value vector in terms of word value vectors as computed in the attention module of a transformer.</p>
<p>Remark 24.As already mentioned, it is natural to filter the probabilistic language L by the word length of texts.Define L k to be the set of texts on L that have word length up to k. L 1 will be the set of words.Each L k inherits the structure of a probabilistic language model from L. The inclusions define isometric embeddings φ k : L k → L k+1 .Then we can consider the non-expansive projections R k : P (L k+1 ) → P (L k+1 ) where Im(R k+1 ) = φ k (P (L k )).</p>
<p>Duality between text extensions and restrictions</p>
<p>We have already considered the (min, +) semifield R min := ((−∞, +∞], ⊕ min , ⊙).To express duality results though, it will be convenient to work with the completed (min, +) semiring Rmin := ([−∞, +∞], ⊕ min , ⊙) where as before s ⊕ min t := min{s, t} and s ⊙ t := s + t but we need to further determine how −∞ and ∞ interact.</p>
<p>Indeed we specify that the element +∞ remains absorbing, so +∞ + s = +∞ holds for all element s, and in particular (+∞) + (−∞) = +∞.The definition of d min in (60) extends to this semiring.We also need to extend definitions of P (L) and P (L): Definition 10.Let P − (L), D) be the directed metric polyhedron (92)
P − (L) := {x = (x 1 , . . . , x n ) ∈ {R ∪ {∞, −∞}} n {(∞, . . . , ∞)}|x i x j + d i,j }.
Moreover let P − (L), D t ) be the directed metric polyhedron
(93) P − (L) := {y = (y 1 , . . . , y n ) ∈ {R ∪ {∞, −∞} n {(∞, . . . , ∞)}|y i y j + d j,i }.
Remark 25.Recall that we added the case of a directed metric which can also take the value −∞ in Remark 1 and now D is such a metric.Moreover we specified that +∞ is absorbing in R min .However the Funk metric D is defined using max so we have to specify further our convention to cover expressions that contain both min and max.For that, we simply use the relation max(s, t) = − min{−s, −t} to transform any max in the expression to min so that we end up with an expression containing only min.Then we compute using the (min, +) convention that +∞ is absorbing.</p>
<p>Equivalently we can use the same relation to transform any expression to one that contains only max.Then using −∞ as the absorbing element gives the same answer.We also denote by D t the transpose metric with D t (x, y) := D(y, x).</p>
<p>In fact we will see that A and B on non-expansive maps with respect to these metrics.</p>
<p>The pair (A, B) forms an adjunction in the categorical or metric sense:
Proposition 16. If x : L → [−∞, ∞] and y : L → [−∞, ∞] then we have (96) D(Ay, x) = D t (y, Bx) Proof. D(Ay, x) = max i {x i − min j {d i,j − y j }} = − min i {min j {d i,j − y j } − x i } = − min j {min i {d i,j − x i } − y j } = max{y j − min i {d i,j − x i }} = D(Bx, y) = D t (y, Bx).
Remark 26.</p>
<p>(1) Note the resemblance of the pair of adjoint maps (A, B) with the Legendre-Fenchel transform where the metric is replaced by the inner product of a vector space.</p>
<p>(2) We note, for purposes of developing intuition, that the pair of adjoint maps (A, B) is similar to a pair of adjoint linear maps (A, A * ) on a vector space with inner product −, − .Indeed in the usual linear algebra case Au, v = u, A * v .Moreover we have already seen in Proposition 24 that D is a kind of tropical inner product.There is a crucial difference though that v, v = |v| 2 while D(x, x) = 0.This reflects the fact that to go from usual algebra to tropical algebra we apply − log.</p>
<p>We now have the following Proposition 17.We have ABA = A and BAB = B which implies that AB and BA are idempotent.</p>
<p>Proof.This follows from the fact that D(Ay, x) = D t (y, Bx).Indeed D(ABAy, Ay) = D t (BAy, BAy) = 0 and D(Ay, ABAy) = D t (BAy, BAy) = 0. Therefore ABAx = Ax.The equality BAB = B is shown analogously.</p>
<p>Let us now compute the fixed parts of the adjunction Fix(AB) and Fix(BA).Remark 27.Note that we can directly check the adjunction of Proposition 16 using our explicit formula from Proposition 19.Indeed D(Ax, y) = D(−x, y) = max i {y i + x i |x i = −∞}.Moreover D t (x, By) = D(−y, x) = max i {x i + y i |y i = −∞}.Since we have a max expression, −∞ is absorbing (see Remark 25) and consequently if
x i = −∞ of if y i = −∞ then x i + y i = −∞
therefore both these conditions can be ignored for taking the max and we get D(−x, y) = D(−y, x).</p>
<p>The following theorem has been proved in [4] and [3] from different points of view and in different generalities.Another approach using category theory was used in Willerton [21].</p>
<p>Here we take advantage of the explicit computation in Proposition 19 which is true because d
′ ) = max i {y ′ i − y i } = D((y ′ , y) = D t (y, y ′ ). Furthermore, A(λ ⊙ y) i = −(λ + y i ) = −λ ⊙ A(y) i A(y ⊕ max y ′ ) i = − max{y i , y ′ i } = min{−y i , −y ′ i } = A(y) i ⊕ min A(y ′ ) i . A(y ⊕ min y ′ ) i = − min{y i , y ′ i } = max{−y i , −y ′ i } = A(y) i ⊕ max A(y ′ ) i .
(Note that Im(d min ) is (max, +) closed; this follows from Proposition 29.)</p>
<p>We have then that the (min +) column span P − (L), of d min is anti isomorphic to the (min, +) row span P − (L) of d min (as Rmin modules) by the two inverse maps A and B and moreover they are isometric when considered with the directed metrics D and D t respectively.(Recall also that in Proposition 12 we saw that D can be considered as tropical inner product.)We will see an example of this bellow.First though we would like to make this map more explicit with respect to the rows and columns of the matrix d.Proposition 20.Consider x ∈ Im(d min ).We have that (104)</p>
<p>x = ⊕ j x j ⊙ d(−, a j ) and then B(x) = −x = ⊕ j − x j ⊙ d(a j , −).
In particular if x = d(−, a k ) then (105) d(−, a k ) = ⊕ aj a k d(a j , a k ) ⊙ d(−, a j )
and
(106) −d(−, a k ) = ⊕ aj a k − d(a j , a k ) ⊙ d(a j , −)
Analogously for y ∈ Im(d t ) we have
(107) y = ⊕ i y i ⊙ d(a i , −) and then A(y) = −y = ⊕ i − y i ⊙ d(−, a i ).
In particular if y = d(a k , −) then Remark 28.Note that all results in this section hold for a general directed metric space.</p>
<p>Example 1.We now show a simple example of a probabilistic language model (L, d 1 ) along with P (L) and P (L).We will also see the correspondence of extremal rays with connected lower sets for P (L) and connected upper sets for P (L) as described in Theorem 2.</p>
<p>We will actually consider the corresponding polyhedral cones Q(L) and Q(L) and show in the figures the polyhedra Q 0 (L) and Q 0 (L) (Definition 6) which are their intersections with the unit simplex.</p>
<p>We will further illustrate the duality between completions P − (L) and P − (L) by making a uniform approximation of infinities in d with a big number M .</p>
<p>Indeed consider the corpus to be L := {red, colour, red colour}.Denote "red" by "r", "colour" by "c" and "red colour" by "rc".</p>
<p>Let the metric d be given by eq. ( 110):
d =    r c rc r 0 ∞ log 2 c ∞ 0 log 3 rc ∞ ∞ 0    (110)
Recall that in general e −di,j = Pr(a j |a i ) and thus the corresponding matrix of probabilities of extensions is
Pr =    r c rc r 1 0 1 2 c 0 1 1 3 rc 0 0 1    (111)
This means for example that Pr(rc|r) = 1  2 and Pr(c|r) = 0, while P (r|r) = 1.Recall that the equations for P (L) = Im(d min ) are (Definition 4): x i d i,j + x j .Letting z i := e −xi we have that the equations for Q(L) are (Definition 5): z i e −di,j z j .</p>
<p>Therefore in our case we get that the polyhedral cone Q(L) is defined by inequalities
(112) z 1 1 2 z 3 , z 2 1 3 z 3 , z 1 0, z 2 0, z 3 0. The intersection Q 0 (L) of Q(L)
with the unit simplex is shown on the right in Figure 1.Notice that it has three vertices.Analogously, the equations for P (L) = Im(d t min ) (Definition 4) are y j d i,j + y i .Letting u i := e −yi we have that the equations for Q(L) are (Definition 4) u j e −di,j u i .</p>
<p>Therefore in our case we get that the polyhedral cone Q(L) is defined by inequalities
(113) u 3 1 2 u 1 , u 3 1 3 u 2 , u 1 0, u 2 0, u 3 0.
The intersection Q 0 (L) of Q(L) with the unit simplex is shown on the left in Figure 1.Notice that it has four vertices.</p>
<p>Denote the lower set generated by "a" by (a) l and the upper set generated by a by (a) u .</p>
<p>From Theorem 2, extremal rays of Q(L) correspond to connected lower sets of L. There are three and they are all principal: (r) l = {r}, (c) l = {c}, (rc) l = {r, c, rc}.These give rise to the three vertices of Q 0 (L) as we can see in Fig 1 .(Note that (r, c) l is not connected so it does not correspond to an extremal ray of Q(L).</p>
<p>From Corollary 4, extremal rays of Q(L) correspond to connected upper sets of L. The principal ones are (r) u = {r, rc}, (c) u = {c, rc}, (rc) u = {rc} and a non-principal one (r, c) u = {r, c, rc}.This extremal ray is not in the image of the Yoneda embedding.The corresponding four vertices of Q 0 (L) are shown on the left in Figure 1.</p>
<p>Notice that the number of extremal rays of P (L) and P (L) are actually different.Now note that in general the R min module P (L) = Im(d min ) is a geometric object.In fact Q(L) is a polyhedral cone and Q 0 (L) is a polyhedron.However the Rmin module P − (L) is not obviously geometric.In order to approximate with a geometric object and be able to visualize the duality between the P − (L) and P − (L) it is natural to "truncate" the matrix d, replacing the +∞ entries by a sufficiently large number M , leading to the new matrix:
d M =    r c rc r 0 M log 2 c M 0 log 3 rc M M 0    (114)
This matrix is sill a directed metric, satisfying (d M min ) 2 = d M min but it does not any more represent a probabilistic language model.</p>
<p>We can consider P M (L) := Im(d M min ) as in (Definition 4) and Q M (L) as in (Definition 5).Then the intersection Q M,0 (L) of Q M (L) with the unit simplex is depicted in Figure 2 on the right.</p>
<p>Moreover we consider P M (L) := Im((d M min ) t ) as in (Definition 4) and Q M (L) as in (Definition 5).Then the intersection Q M,0 (L) of Q M (L) with the unit simplex is depicted in Figure 2 on the right.</p>
<p>Observe that the duality preserves the number of extreme points inside the interior of the simplex, and that the sets of Figure 2 converge to the sets of Figure 1 as M → ∞.</p>
<p>Also note that the duality map between P (L) an P (L) is x i → y i = −x i for i = 1, 2, 3. We also have z i := e −xi and u i := e −yi Therefore the map between the polyhedra Q(L) and Q Remark 29.Note that approximating uniformly infinities in the matrix d with a big number M can be done in general.This is helpful since the duality theorem is easier to illustrate for matrices with finite entries.Of course, the proof of the duality theorem in section 6 goes through with coefficients in (−∞, ∞).(Note also that the Develin-Sturmfels version [4] of the adjunction between the tropical column span and the tropical row span is exactly about matrices with finite entries, whereas the version of [3] deals with matrices with possibly infinite entries.)However as we already saw in the example replacing ∞ with M in a directed metric d that defines a probabilistic language model gives a metric that is no longer a language model.The limit of polyhedra for M → ∞ will give the polyhedra for the original metric.Remark 30.We have said that we can encode the meaning of a k by d(−, a k ).If a k is a word this contains very little information.We already addressed the solution to this problem in section Section 4.1.Another solution is, using the duality between P (L) and P (L) explained previously and in particular Equation (105), Equation (106).
(L) is z i → u i = 1 zi for i = 1, 2, 3.</p>
<p>Extremal Rays in terms of text vectors</p>
<p>We have seen in Theorem 1 that the original texts in L, mapped by the Yoneda isometric embedding Y : L → P (L), appear as extremal rays (corresponding to principal lower sets) in the polyhedron P (L) and the polyhedral cone Q(L).As proven in Proposition 7, there are in general many other extremal rays of P (L) corresponding to connected lower sets of L. Nevertheless extremal rays in the image of Y , (min, +) generate P (L) as we have already seen in Corollary 6 where we showed that (115)
x ∈ P (L) ⇐⇒ x = ⊕ j D(Y (a j ), x) ⊙ Y (a j ).
Recall that we think of Y (a k ) := d(−, a k ) ∈ P (L) as encoding the meaning of text a k according to the statistical semantics principal.</p>
<p>Recall Definition 9, where we introduced the saturation graph S(x) for x ∈ P (L).We shall also consider the undirected saturation graph, obtained by forgetting the orientation of the edges in S(x).and if we put v j := e Y (bj ) , then for small T , we get (123) e −x/T ≈ j e −D(Y (bj ),x)/T v j .</p>
<p>P − (L) as the lattice completion of the Isbell completion</p>
<p>We have seen that P (L) and P (L) generalize the lower set and upper set completions respectively from the poset L to the directed metric space (L, d), at least in the case where L contains the empty text a 0 which is the bottom element.</p>
<p>However there is another completion of a poset, called the Dedekind-MacNeille completion (which also generalizes the so called notion of formal concepts).</p>
<p>It is known that the generalization of the Dedekind MacNeille completion from posets to directed metric spaces is the so called Isbell completion, which is the fixed part of the Isbell adjunction.This is also relevant to our situation as it turns out to be defined by d max .This was studied in [11,21] with [0, ∞] coefficients.In that case the Isbell completion is identified with the directed tight span of Hirai and Koichi [7].</p>
<p>We will instead define the Isbell adjunction using the extended semi ring [−∞, ∞] as we did with the d min adjunction in section 6.</p>
<p>Recall that in section 6 Remark 25 we explained the conventions for working with (min, +) and (max, +) on [−∞, ∞].We use the same here.Proof.D t (Lx, y) = D(y, Lx) = max i {max j {d i,j − x j } − y i } = max j {max i {d i,j − y i } − x j } = D(x, Ry).</p>
<p>We now have the following Proposition 25.We have LRL = L and RLR = R which implies that LR and RL are idempotent.</p>
<p>Proof.This follows from the fact that D t (Lx, y) = D(x, Ry).Indeed L(λ⊙x) i = max j {d i,j −λ−x j } = max j {d i,j −x j }−λ = L(x) i −λ = (−λ⊙L(x)) i .Moreover L(x ⊕ min y) i = max j {d i,j − min{x j , y j }} = max j {d i,j + max{−x j , −y j } = max j {max{d i,j −x j , d i,j −y j }} = max{max j {d i,j −x j }, max j {d i,j −y j }} = (L(x)⊕ max L(y)) i .</p>
<p>We have that the tropical linear space Im(d max ) is anti isomorphic to Im(d t max ) by the two inverse maps As mentioned earlier according to a theorem of Willerton [21] Theorem 5.The directed tight span of Hirai and Koichi [7] is the same as the fixed parts of the Isbell adjunction when using [0, ∞] coefficients and the truncated max operations .</p>
<p>We It follows that x = dx ⇐⇒ x dx.</p>
<p>We want to show that if x dx and y dy then max{x, y} d(max{x, y})</p>
<p>which will imply that max{x, y} ∈ Im(d min ).Indeed if max{x, y} = x then d(max{x, y}) = dx x y and if max{x, y} = y then d(max{x, y}) = dy y x, therefore x d(max{x, y}) and y d(max{x, y}) which implies that max{x, y} d(max{x, y}).</p>
<p>We have shown therefore that Im(d min ) is closed under the max operation.Both Im(d min ) and Im(d max ) are generated by the vectors d(−, a i ) therefore the result is proved.</p>
<p>Example 2. To illustrate the difference between Im(d min ) and Im(d max ) (albeit for a symmetric and finite metric) we provide the following example: Consider the discrete metric on three points
d 2 =   0 1 1 1 0 1 1 1 0   (131)
In the monoid case we consider that the meaning of a text eg "red" is given by the ideal generated by red which contains all texts containg red.</p>
<p>In the poset case it is the same, where ideals and filters correspond to principal lower and upper sets.This is a mathematical incarnation of the distributional semantics principle.Now there is a very general and basic concept of duality in mathematics that in the commutative case takes the form of a duality between algebra and geometry.</p>
<p>The most basic case is, given a commutative algebra, to consider the space of (prime) ideals.This is called the spec and can be thought of as a space on which the algebra of functions is the commutative algebra we started with.For example if we consider the algebra C[x, y] of complex polynomials in two variables then prime ideas are ideals generated by monomials (x−a)(y −b) for any a, b ∈ C and therefore the space of ideals is C 2 i.e. the complex plane.The duality then is between the commutative algebra C[x, y] and the space of ideals C 2 .(This so called spec construction is the cornerstone of algebraic geometry.)</p>
<p>We can try to extend this kind of duality for monoids, posets, and for our enriched category.Ideals in a monoid are modules over the monoid and in general we have to consider modules.Now moving to the case of an algebra, a module over the algebra (a representation) is a presheaf over the corresponding category.This is the category with one object and arrows given by the elements of the algebra.In general in a category the presheaves play the role of modules.</p>
<p>In our case modules i.e. presheaves are the non-expansive maps (Proposition 3) and the space P (L) is the category of modules (the Hom is given by the metric D as already mentioned in Appendix A).</p>
<p>The original category defines the syntax and the presheaf category can be considered to reflect semantics (see also [1]).</p>
<p>In fact just like C[x, y] gives coordinates on the space of ideals C 2 , we could think that the language category (the syntax category) provides coordinates on the category of presheaves (modules) which can be though as the semantic category (in this particular case, for example because the Hom which is the metric D measures semantic similarity).Now since we can translate between languages, namely the semantics of languages are in some sense the same (approximately) we expect that the categories of presheaves on different language categories, should be equivalent.This is a well known notion called Morita equivalence.We would then expect that enriched categories corresponding to different languages should be Morita equivalent.Moreover in that case there are associated invariants (Hochschild homology) which should be semantic invariants.</p>
<p>Investigating and developing this, is a future direction of research.</p>
<p>( 7 )
7
Q(L) := {z := (z 1 , . . .z n ) ∈ [0, ∞) n |z i := e −xi for x = (x 1 , . . ., x n ) ∈ P (L)}</p>
<p>Theorem 1 .
1
The isometric embedding Y : L ֒→ P (L), maps points of L to extremal rays of the polyhedronP (L) namely Y (a k ) = d(−, a k ) is an extremal ray in P (L).Moreover the isometric embedding Y : L ֒→ P (L), maps points of L to extremal rays of the polyhedron P (L) namely Y (a k ) = d(a k , −) is an extremal ray in P (L).</p>
<p>is a polyhedral cone variant of Stanley's order polytope for the opposite poset L op -the latter is the intersection of Q(L) with the box [0, 1] n[18].</p>
<p>Theorem 2 .
2
The vector ỹ := (ỹ 1 , . . .ỹn ) ∈ Q(L) generates an extremal ray of Q(L) if and only if the function a i → ỹ(a i ) := y i is a positive scalar multiple of the characteristic function of a lower set in L whose Hasse diagram is connected.</p>
<p>Now we want a general version of the Corollary 3.For any subset C of L, selecting an element c ∈ C, for every element a i in the connected component of C in the graph induced by the Hasse diagram of L, we denote by w c i the weight of any path from c to a i in the directed graph constructed in the proof of Proposition 6. Proposition 7. Let C be a connected lower set of the Hasse diagram of (L, ).Let c denote any element of C.Then, the vector (58)</p>
<p>Corollary 4 .
4
Extremal rays of Q(L) and P (L) correspond to connected upper sets of L.</p>
<p>Lemma 1 .
1
(60) d min (x) i := min j {d i,j + x j } Proposition 9. (L, d) is a directed metric if and only if d 2 min = d min , namely d min is a (min, +) projector.Proof.We have d 2 min = d min ⇐⇒ d i,k = min j {d i,j + d j,k } which is the same as the triangle inequality d i,k d i,j + d j,k .Let us denote Im(d min ), the image of d min , namely the (min, +) column span of d.We have Im(d min ) = Fix(d min ), where Fix(d min ) is the (min, +) module Fix(d min ) := {x : d min (x) = x}.Proof.It follows from d 2 min = d min .We note now that there is a very natural description of our polyhedra as follows: Proposition 10.The polyhedron P (L) is equal to Im(d min ) = Fix(d min ) and the polyhedron P (L) is equal to Im(d t min ) = Fix(d t min ).Proof.Since d 2 min = d min we have that x ∈ Im(d min ) ⇐⇒ d min x = x which means that x i = min j {d i,j + x j } and thus x i</p>
<p>Proposition 14 .
14
If a probabilistic language model (L 1 , d 1 ) is extended to (L 2 , d 2 ), namely if there is an isometric embedding φ : (L 1 , d 1 ) ֒→ (L 2 , d 2 ) then there is an isometric embedding φ : (P (L 1 )), D 1 ) ֒→ (P (L 2 ), D 2 ) such that φ(Y 1 (a)) = Y 2 (φ(a)).Moreover φ(P (L 1 )) is a retraction (i.e. a non-expansive (min, +) projection) of P (L 2 )</p>
<p>Corollary 7 .
7
Let L 1 := {a 1 . . .a n } and L 2 := {b 1 . . .b n , b n+1 , . . .b n+l }, be probabilistic language models and φ : L 1 → L 2 an isometric embedding where b</p>
<p>Now, analogously to Proposition 10, if we consider d min and d t min acting on {R ∪ {∞, −∞}} n {(∞, . . ., ∞)} then we have Proposition 15.The polyhedron P − (L) is equal to Im(d min ) = Fix(d min ) and the polyhedron P − (L) is equal to Im(d t min ) = Fix(d t min ).Definition 11.Define the pair of maps (A, B) as follows.If y : L → [−∞, ∞] and x : L → [−∞, ∞] then (94) A(y) := d min (−y), B(x) := d t min (−x) Or in coordinates (95) A(y) i := min j {d i,j − y j }, B(x) j := min i {d i,j − x i }</p>
<p>Proposition 18 .
18
We have Fix(AB) = Im(A) = Im(d min ) and Fix(BA) = Im(B) = Im(d t min ).Proof.This follows from the fact that ABA = A. Indeed clearly Im(A) ⊂ Fix(AB).Moreover Fix(AB) ⊂ Im(A) since AB(x) = x says that x ∈ Im(A).Analogously for BA.In this case, due to the fact that d min is an idempotent we can more explicitly compute the maps A and B Proposition 19.We have that (97) A : Im(d t min ) = P − (L) → Im(d min ) = P − (L) is given by A(y) = −y and (98) B : Im(d min ) = P − (L) → Im(d t min ) = P − (L) is given by B(x) = −x.Proof.Consider x ∈ Im(d min ) = F ix(d min ).We have d min (x) = x ⇐⇒ x i = min j {d i,j + x j } ⇐⇒ −x j = min j {d i,j − x i } ⇐⇒ d t min (−x) = −x.Therefore B(y) = d t min (−x) = −x.Analogously consider y ∈ Im(d t min ).We have d t min (y) = y ⇐⇒ d min (−y) = −y.Therefore A(y) = d min (−y) = −y.</p>
<p>2 min = d min . Theorem 4 .
4
We have that A : Fix(BA) = Im(B) = Im(d t min ) = P − (L) → Fix(AB) = Im(A) = Im(d min ) = P (L) and B : Fix(AB) = Im(A) = Im(d min ) = P (L) → Fix(BA) = Im(B) = Im(d t min ) = P − (L) are anti-isomorphisms.In other words they are one to one and onto and inverses.They are isometries, namely D(Ay, Ay ′ ) = D t (y, y ′ ).Finally we have (99) A(λ ⊙ y) = −λ ⊙ A(y), (100) A(y ⊕ min y ′ ) = A(y) ⊕ max A(y ′ ) and (101) A(y ⊕ max y ′ ) = A(y) ⊕ min A(y ′ ) and similarly for B. Proof.From Proposition 16 (102) A : Im(d t min ) = P (L) → Im(d min ) = P (L) is given by A(y) = −y and (103) B : Im(d min ) = P (L) → Im(d t min ) = P (L) is given by B(x) = −x.therefore A and B are one on one and onto and inverses.Moreover D(Ay, Ay ′ ) = D(−y, −y</p>
<p>k , −) = ⊕ a k ai d(a k , a i ) ⊙ d(a i , −) and (109) −d(a k , −) = ⊕ a k ai − d(a k , a i ) ⊙ d(−, a i ).Proof.We have x ∈ Im(d min ) ⇐⇒ d min x = x ⇐⇒ x = ⊕ j x j ⊙ d(−, a j ).From Proposition 19 we then haved t min (−x) = −x which is equivalent to −x = ⊕ j − x i ⊙ d(a j , −).This proves (104).Now if x := d(−, a k ) then x j = x(a j ) = d(a j , a k ) = d j,k .Then from Proposition 11 we have d(−, a k ) = ⊕ aj a k d j,k ⊙ d(−, a j ) therefore from (104) it follows that −d(−, a k ) = ⊕ aj a k − d(a j , a k ) ⊙ d(a j , −).The proof for y ∈ Im(d t min ) and for y := d(a k , −) is analogous.</p>
<p>Figure 1 .
1
Figure 1.The cross section Q 0 (L) of the polyhedral cone Q(L) arising from the metric of d (left).Every vector d(r, −), d(c, −), d(rc, −) determines an extreme point of the cross section, denoted by r, c, or rc.There is a fourth extreme point (shown in gray) corresponding to a non-principal upper set.The cross section Q 0 (L) (right).There are three extreme points, which correspond to the vectors d(−, r), d(−, c), d(−, rc).</p>
<p>Figure 2 .
2
Figure 2. The duality between the columns and row spaces of metric matrices (Proposition 19 and Theorem 4) illustrated.On the right Im(d M min ) and on the left Im((d M min ) t )</p>
<p>Proposition 21 .
21
A vector x ∈ P (L) can be written as a tropical linear combination of terminal elements of its saturation graph S(x).Specifically if b 1 , . . .b k are the terminal elements in S(x), then(116) x = ⊕ j D(Y (b j ), x) ⊙ Y (b j ).and therefore, for small T (120) e −x/T ≈ j e −D(Y (bj ),x)/T v j Proof.Recall the identity (121) lim T →0 −T log(e −y/T + e −z/T ) = min{y, z}.If x ∈ P (L), by the previous proposition x = ⊕ j D(Y (b j ), x) ⊙ Y (b j ) where b j are terminal elements.Then we have (122) x = lim T →0 −T log j e −D(Y (bj ),x)/T e Y (bj )</p>
<p>Given x : L → [−∞, ∞] and y : L → [−∞, ∞] define d max and d t max by (124)d max (x) i := max j {d i,j + x j } and d t max (y) j := max i {d i,j + y i }Extending the definition in[11,21] by using [−∞, ∞] coefficients we have that Definition 12.The Isbell adjunction is the pair of maps (L, R) defined as follows.If x : L → [−∞, ∞] and y : L → [−∞, ∞] then (125) L(x) := d max (−x) and R(x) := d t max (−y) Or in coordinates (126) L(x) i := max j {d i,j − x j } and R(x) j := max i {d i,j − y i }Recall from section 6 that the Funk metric D, is still well defined by D(x, y) := max i {y i −x i |x i = ∞}.We also denoted by D t the transpose metric with D t (x, y) := D(y, x).Remark 33.Note that (127) L(x) i := max j {d i,j − x j } = D(x, d(a i , −)) = D(x, Y (a i )) and (128) R(y) j := max i {d i,j − y i } = D(y, d(−, a j )) = D(y, Y (a j )) The pair (L, R) forms an adjunction in the categorical or metric sense: Proposition 24.If x : L → [−∞, ∞] and y : L → [−∞, ∞] then we have D t (Lx, y) = D(x, Ry).</p>
<p>D t (LRLx, Lx) = D(RLx, RLx) = 0 and D t (Lx, LRLx) = D(RLx, RLx) = 0. Therefore LRLx = Lx.The equality RLR = R is shown analogously.Let us now compute the fixed parts of the adjunction Fix(LR) and Fix(RL).Proposition 26.We have Fix(LR) = Im(L) = Im(d max ) and Fix(RL) = Im(R) = Im(d t max ).Proof.This follows from the fact that LRL = L. Indeed clearly Im(L) ⊂ Fix(LR).Moreover Fix(LR) ⊂ Im(L) since LR(y) = y says that y ∈ Im(L).As before we have the following Proposition 27.We have thatL : Fix(RL) = Im(R) = Im(d t max ) → Fix(LR) = Im(L) = Im(d max ) and R : Fix(LR) = Im(L) = Im(d max ) → Fix(RL) = Im(R) = Im(d tmax ) are anti-isomorphisms.In other words they are one to one and onto and inverses.They are isometries, namely D(Lx, Lx ′ ) = D t (x, x ′ ).Finally we have(129) L(λ ⊙ x) = −λ ⊙ L(x) and L(x ⊕ min y) = L(x) ⊕ max L(y)and similarly for R.Proof.First let us check that L and R are one to one and onto.Consider x, x ′ ∈ Fix(RL).If L(x) = L(x ′ ) then RL(x) = RL(x ′ ) and therefore x = x ′ .Also if y ∈ Fix(LR) then y = L(R(y)).Moreover D(Lx, Lx ′ ) = D op (RLx, x ′ ) = D op (x, x ′ ) = D(x ′ , x) Next we check the tropical antilinearity.</p>
<dl>
<dt>R</dt>
<dd>
<p>Im(d max ) → Im(d t max ) and L : Im(d t max ) → Im(d max ).Proposition 28.The Yoneda isometric embedding Y : L → P (L) given by Y (a) := d(−, a) and the co-Yoneda isometric embedding Y : L → P (L) given by Y (a) := d(a, −), are compatible with the anti-isomorphisms L and R above, in the sense that (130) Y (a) = R(Y (a)) and Y (a) = L( Y (a)).Proof.We haved i,j d i,k + d k,j , therefore L( Y (a k )) i = L(d(a k , −)) i = max j {d ij − d k,j } = d i,k = d((−, a k ) i = Y (a k ) i .Analogously R(Y (a k )) j = R(d(−, a k )) j = max i {d ij − d i,k } = d k,j = d((a k , −) j = Y (a k ) j .</p>
</dd>
</dl>
<p>denote the Isbell completion with [−∞, ∞] coefficients by Ĩ(L).Let us finally explore the relation between P (L) = Im(d min ) and Ĩ(L) = Im(d max ).Proposition 29.The polyhedron P (L) = Im(d min ) is the lattice completion of Im(d max ) when using (−∞, ∞] coefficients and P − (L) is the lattice completion of Im(d max ) when using [−∞, ∞] coefficients.Proof.Recall that since d 2 min = d min we have Im(d min ) = {x|dx = x}.Moreover if Id is the (min, +) identity matrix, namely Id i,i = 0 and Id i,j = ∞ for i = j, then, since d i, = 0, we have d Id and therefore we always have dx x.</p>
<p>We explain later in Remark 2 that it is possible and useful in some cases to extend the values of a directed metric to [−∞, ∞] and this is one of the cases we do so.
Moreover, if the saturation graph of x has s undirected connected components, this vector belongs to a face of P (L) of dimension s.Proof.Let x ∈ P (L).If i is not terminal in the saturation graph S(x), then, there is a j 1 = i such that x i = d i,j1 + x j1 .Similarly, if j 1 is not terminal, there is j 2 = j 1 such that x j1 = d j1,j2 + x j2 and thus x i = d i,j1 + d j1,j2 + x j2 .Continuing this way we get x i = d i,j1 + d j1,j2 + . . .d jn−1,jn + x jn where a jn is a terminal element of the saturation graph S(x).We know that this stops at a terminal element because there are no cycles of positive weight in the digraph of d. (This is the graph that has an edge from i to j with weight d i,j when d i,j is not infinity.)Using the triangular inequality, we deduce that x i ⊕ j∈T d i,j ⊙x j , where the sum is taken over the set T of terminal nodes of S(x), and so, xwhere now the sum is taken over all the indices k (possibly non terminal).This entails that (116) holds.Finally, arguing as in the proof of Theorem 2, we get the rank of the family of active constraints at point x is given by the number s of connected component of the undirected saturation graph of x.Hence, x belongs to a face of dimension s.Remark 31.Note that Proposition 21 holds for a general directed metric space L and not just for (L, d) a probabilistic language model.We can now find explicit (min, +) expressions for generators of extremal rays corresponding to non principal lower sets.Proposition 22.Let L be a probabilistic language model with the empty text a 0 included.Let x denote an extremal ray corresponding to the lower set generated by {b 1 , . . .b n }.ThenProof.From Proposition 21 we haveFrom corollary 3 we have x(b j ) = − log 1 Pr(bj ) = log Pr(b j ).This proves the result.Remark 32.We point out that the terminal elements b 1 , . . .b k of S(x) function like an orthonormal basis with respect to D, namelySo for example if we know that there are λ j such thatProposition 23.Let T ∈ [0, ∞) be a parameter which will be called temperature.Consider x ∈ P (L) an extremal ray and let) where b j are the terminal elements of the saturation graph S(x).Let v j := e Y (bj ) .Then we haveThe associated (min,+)-module P(d 2 ) is shown on Figure3, left and the (max, +) module on the right.Some comments about Probabilistic Language ModelsWe would finally like to gather some comments about how to interpret probabilistic language models (L, , Pr) and what they imply.Some of these were stated already in section Section 4.1(1) We note that the construction of P (L) explains why it is natural to have vectors in a problem of language.In fact we naturally get Boltzmann weighted linear combinations Equation (90), Equation (62), Equation (62) which is what is introduced by hand in the attention layers of the transformer and the final layer where the distribution over possible next words is determined (2) If the transformer is learning P (L) or equivalently Q(L) it would be learning a convex body which could explain why its training is efficient in the first place.(3) Assuming that the transformer is learning the polyhedron P (L) or equivalently Q(L) it would be learning an effective representation of Yoneda embeddings of texts.This can then be interpreted as solving the huge (min, +) linear systems in Equation (62), Equation (63), (Proposition 11).(4) The duality explained in section 6 between P (L) and P (L) shows how to resolve the paradox that both d(−, a k ) and d(a k , −) should equally well encode the meaning of a text a k , given a probabilistic language model (L, , Pr).This is most striking when a k is a single word.In that case d(−, a k ) is supported only on a k , but we have that −d(−, a k ) = −d(a k , −).This was explained in Section 6, Proposition 20, Remark 30.It also showcases the notion that the meaning of a text a k is not just encoded by d(−, a k ) or d(a k , −) but by the whole ambient spaces P (L) and P (L) respectively.Appendix A. Categorical interpretationThe metric polyhedra (P (L), D) and ( P (L), D t ), as well as the polyhedral cones Q(L) and Q(L), arise from a categorical point of view[11,21,1].In fact all constructions have a categorical interpretations and we will briefly explain these here.To begin with, we can consider the probabilistic language model (L, d) to be a category enriched over the monoidal closed category (−∞, ∞], with monoidal structure given by addition and Hom given by considering (−∞, , ∞] as a poset with the opposite of the usual order of numbers.The Hom between objects a i and a j in L, is d(a i , a j ) and the triangle inequality is the composition of morphisms.This construction (using [0, ∞] instead of (−∞, ∞]) was explained in[1]) Then P (L) is the category of presheaves on L namely the category of enriched functors L op → (−∞, ∞] where (−∞, ∞]) is considered as a category enriched over itself with internal Hom given by the directed metric d R on (−∞, ∞] where d R (s, t) = t − s.This follows from the fact that we can think of the points x ∈ P (L) as non-expansive functions on L as we have seen in Proposition 4. Indeed (132)Moreover, the Funk directed metric D on P (L) is the Hom on presheaves.The isometric embedding Y : L ֒→ P (L) is the Yoneda embedding, Y (a k ) is a representable presheaf and the fact that x i = x(a i ) = D(Y (a i ), x), is the Yoneda lemma.On the other hand P (L) is the category of co-presheaves and Y is the co-Yoneda embedding.The tropical anti-isomorphims between P (L) and P (L) as already explained follows from an adjunction between A(x) := d min (−x) and B(y) = d t min (−y).Finally it was proven in[21]that the directed tight span DT S(L) (defined in[7]) is the Isbell completion, with [0, ∞] as enriching category, of the enriched category L. Namely the fixed part of the Isbell adjunction which is given by (L(x)) i := max j {d i,j − x j } and (R(y)) j := max i {d i,j − y i } (where we use trancated difference so the result is always positive).We instead define the Isbell adjunction with enriching category [−∞, ∞].The fact that the category of presheaves P (L) is the (min, +) span of the images of the Yoneda embedding reflects the fact that colimits are given by min and every presheaf is a weighted colimit of representables.On the other hand the Isbell completion is given by presheaves which are weighted limits of representables since limits are given by max and it is smaller that P (L) since in general not every presheaf is such a weighted limit.Appendix B. Syntax to Semantics and Morita equivalenceThe problem of encoding allowed (with some probability) sequences of symbols, by some mathematical structure can be located in the realm of a very basic duality in mathematics.Traditionally language has been modeled as a monoid generated by words.We can go from the monoid to a poset by considering the monoid as a category with one object and arrows corresponding to texts and constructing the factorization category (also called the twisted arrow category).This produces exactly the poset of texts with the subtext order as we have used in our probabilistic language model.Considering the subtext poset makes it easier to add probabilities and we are led naturally to the probabilistic language model we defined which is a special case of a directed metric space.In Appendix A we saw that this is an enriched category.
An enriched category theory of language: From syntax to semantics. Tai-Danae Bradley, John Terilla, Yiannis Vlassopoulos, arXiv:2106.07890La Matematica. 12022</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordCurran Associates, Inc202033Ilya Sutskever, and Dario Amodei</p>
<p>Duality and separation theorems in idempotent semimodules. G Cohen, S Gaubert, J P Quadrat, Linear Algebra and Appl. 3792004</p>
<p>. M Develin, B Sturmfels, Tropical convexity. Documenta Mathematica. 92004</p>
<p>Trees, tight extensions of metric spaces, and the cohomological dimension of certain groups: A note on combinatorial properties of metric spaces. Andreas W M Dress, Advances in Mathematics. 533September 1984</p>
<p>Double description method revisited. Komei Fukuda, Alain Prodon, Selected papers from the 8th Franco-Japanese and 4th Franco-Chinese Conference on Combinatorics and Computer Science. London, UKSpringer-Verlag1996</p>
<p>On tight spans for directed distances. Hiroshi Hirai, Shungo Koichi, Annals of Combinatorics. 163May 2012</p>
<p>Six theorems about injective metric spaces. J R Isbell, Commentarii Mathematici Helvetici. 391December 1964</p>
<p>Tropical and ordinary convexity combined. Michael Joswig, Katja Kulas, Adv. Geom. 102March 2010</p>
<p>. T Lam, A Postnikov, Alcoved polytopes. I. Discrete Comput. Geom. 3832007</p>
<p>Metric spaces, generalized logic, and closed categories. F William Lawvere, December 197343Rendiconti del Seminario Matematico e Fisico di Milano</p>
<p>Meaning representations from trajectories in autoregressive models. Yu Tian, Matthew Liu, Alessandro Trager, Pramuditha Achille, Luca Perera, Stefano Zancato, Soatto, arXiv:2310.183482023</p>
<p>Distributed representations of words and phrases and their compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, Jeff Dean, arXiv:1310.4546Advances in Neural Information Processing Systems. C J Burges, L Bottou, M Welling, Z Ghahramani, K Q Weinberger, Curran Associates, Inc201326</p>
<p>From Funk to Hilbert geometry. Athanase Papadopoulos, Marc Troyanov, December 2014EMS Press</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2019160025533</p>
<p>Improving language understanding by genetrative pre-training. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2018</p>
<p>Theory of Linear and Integer Programming. A Schrijver, 1986Wiley</p>
<p>Two poset polytopes. R P Stanley, Discrete Comput. Geom. 11986</p>
<p>Enumerating polytropes. Ngoc Mai, Tran , Journal of Combinatorial Theory, Series A. 151October 2017</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, arXiv:1706.03762Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17. the 31st International Conference on Neural Information Processing Systems, NIPS'17Red Hook, NY, USACurran Associates Inc2017</p>
<p>Tight spans, Isbell completions and semi-tropical modules. Simon Willerton, ; Inria, Cmap , École Polytechnique, Paris Ip, CNRS Email address: Stephane.Gaubert@inria.fr YV: ATHENA Research Center, Institute for Language and Speech Processing. Athens, Greece and IHES, Bures-sur-Yvette, France Email address201328</p>            </div>
        </div>

    </div>
</body>
</html>
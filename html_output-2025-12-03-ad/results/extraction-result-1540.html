<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1540 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1540</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1540</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-30.html">extraction-schema-30</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-860bc4f071f35d6d8529a52c2c1858d030779a6a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/860bc4f071f35d6d8529a52c2c1858d030779a6a" target="_blank">In-context Reinforcement Learning with Algorithm Distillation</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> It is demonstrated that AD can reinforcement learn in-context in a variety of environments with sparse rewards, combinatorial task structure, and pixel-based observations, and it is found that AD learns a more data-efficient RL algorithm than the one that generated the source data.</p>
                <p><strong>Paper Abstract:</strong> We propose Algorithm Distillation (AD), a method for distilling reinforcement learning (RL) algorithms into neural networks by modeling their training histories with a causal sequence model. Algorithm Distillation treats learning to reinforcement learn as an across-episode sequential prediction problem. A dataset of learning histories is generated by a source RL algorithm, and then a causal transformer is trained by autoregressively predicting actions given their preceding learning histories as context. Unlike sequential policy prediction architectures that distill post-learning or expert sequences, AD is able to improve its policy entirely in-context without updating its network parameters. We demonstrate that AD can reinforcement learn in-context in a variety of environments with sparse rewards, combinatorial task structure, and pixel-based observations, and find that AD learns a more data-efficient RL algorithm than the one that generated the source data.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1540",
    "paper_id": "paper-860bc4f071f35d6d8529a52c2c1858d030779a6a",
    "extraction_schema_id": "extraction-schema-30",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.004871250000000001,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>In-CONTEXT REINFORCEMENT LEARNING WITH ALGORITHM DISTILLATION</h1>
<p>Michael Laskin<em> Luyu Wang</em> Junhyuk Oh Emilio Parisotto Stephen Spencer<br>Richie Steigerwald DJ Strouse Steven Hansen Angelos Filos Ethan Brooks<br>Maxime Gazeau Himanshu Sahni Satinder Singh Volodymyr Mnih<br>DeepMind</p>
<h4>Abstract</h4>
<p>We propose Algorithm Distillation (AD), a method for distilling reinforcement learning (RL) algorithms into neural networks by modeling their training histories with a causal sequence model. Algorithm Distillation treats learning to reinforcement learn as an across-episode sequential prediction problem. A dataset of learning histories is generated by a source RL algorithm, and then a causal transformer is trained by autoregressively predicting actions given their preceding learning histories as context. Unlike sequential policy prediction architectures that distill post-learning or expert sequences, AD is able to improve its policy entirely in-context without updating its network parameters. We demonstrate that AD can reinforcement learn in-context in a variety of environments with sparse rewards, combinatorial task structure, and pixel-based observations, and find that AD learns a more data-efficient RL algorithm than the one that generated the source data.</p>
<h2>Data Generation</h2>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Algorithm Distillation (AD) has two steps - (i) a dataset of learning histories is collected from individual single-task RL algorithms solving different tasks; (ii) a causal transformer predicts actions from these histories using across-episodic contexts. Since the RL policy improves throughout the learning histories, by predicting actions accurately AD learns to output an improved policy relative to the one seen in its context. AD models state-action-reward tokens, and does not condition on returns.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>1 INTRODUCTION</h1>
<p>Transformers have emerged as powerful neural network architectures for sequence modeling Vaswani et al., 2017). A striking property of pre-trained transformers is their ability to adapt to downstream tasks through prompt conditioning or in-context learning. After pre-training on large offline datasets, large transformers have been shown to generalize to downstream tasks in text completion Brown et al., 2020), language understanding Devlin et al., 2018), and image generation (Yu et al., 2022).</p>
<p>Recent work demonstrated that transformers can also learn policies from offline data by treating offline Reinforcement Learning (RL) as a sequential prediction problem. While Chen et al. (2021) showed that transformers can learn single-task policies from offline RL data via imitation learning, subsequent work showed that transformers can also extract multi-task policies in both same-domain (Lee et al., 2022) and cross-domain settings Reed et al., 2022). These works suggest a promising paradigm for extracting generalist multi-task policies - first collect a large and diverse dataset of environment interactions, then extract a policy from the data via sequential modeling. We refer to the family of approaches that learns policies from offline RL data via imitation learning as Offline Policy Distillation, or simply Policy Distillation ${ }^{1}$ (PD).</p>
<p>Despite its simplicity and scalability, a substantial drawback of PD is that the resulting policy does not improve incrementally from additional interaction with the environment. For instance, the MultiGame Decision Transformer (MGDT, Lee et al., 2022) learns a return-conditioned policy that plays many Atari games while Gato Reed et al., 2022) learns a policy that solves tasks across diverse environments by inferring tasks through context, but neither method can improve its policy in-context through trial and error. MGDT adapts the transformer to new tasks by finetuning the model weights while Gato requires prompting with an expert demonstration to adapt to a new task. In short, Policy Distillation methods learn policies but not Reinforcement Learning algorithms.</p>
<p>We hypothesize that the reason Policy Distillation does not improve through trial an error is that it trains on data that does not show learning progress. Current methods either learn policies from data that contains no learning (e.g. by distilling fixed expert policies) or data with learning (e.g. the replay buffer of an RL agent) but with a context size that is too small to capture policy improvement.</p>
<p>Our key observation is that the sequential nature of learning within RL algorithm training could, in principle, make it possible to model the process of reinforcement learning itself as a causal sequence prediction problem. Specifically, if a transformer's context is long enough to include policy improvement due to learning updates it should be able to represent not only a fixed policy but a policy improvement operator by attending to states, actions and rewards from previous episodes. This opens the possibility that any RL algorithm can be distilled into a sufficiently powerful sequence model such as a transformer via imitation learning, converting it into an in-context RL algorithm.</p>
<p>We present Algorithm Distillation (AD), a method that learns an in-context policy improvement operator by optimizing a causal sequence prediction loss on the learning histories of an RL algorithm. AD has two components. First, a large multi-task dataset is generated by saving the training histories of an RL algorithm on many individual tasks. Next, a transformer models actions causally using the preceding learning history as its context. Since the policy improves throughout the course of training of the source RL algorithm, AD is forced to learn the improvement operator in order to accurately model the actions at any given point in the training history. Crucially, the transformer context size must be sufficiently large (i.e. across-episodic) to capture improvement in the training data. The full method is shown in Fig. 1.</p>
<p>We show that by imitating gradient-based RL algorithms using a causal transformer with sufficiently large contexts, AD can reinforcement learn new tasks entirely in-context. We evaluate AD across a number of partially observed environments that require exploration, including the pixel-based Watermaze Morris, 1981) from DMLab Beattie et al., 2016). We show that AD is capable of in-context exploration, temporal credit assignment, and generalization. We also show that AD learns a more data-efficient algorithm than the one that generated the source data for transformer training. To the best of our knowledge, AD is the first method to demonstrate in-context reinforcement learning via sequential modeling of offline data with an imitation loss.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>2 BACKGROUND</h1>
<p>Partially Observable Markov Decision Processes: A Markov Decision Process (MDP) consists of states $s \in \mathcal{S}$, actions $a \in \mathcal{A}$, rewards $r \in \mathcal{R}$, a discount factor $\gamma$, and a transition probability function $p\left(s_{t+1} \mid s_{t}, a_{t}\right)$, where $t$ is an integer denoting the timestep and $(\mathcal{S}, \mathcal{A})$ are state and action spaces. In environments described by an MDP, at each timestep $t$ the agent observes the state $s_{t}$, selects an action $a_{t} \sim \pi\left(\cdot \mid s_{t}\right)$ from its policy, and then observes the next state $s_{t+1} \sim p\left(\cdot \mid s_{t}, a_{t}\right)$ sampled from the transition dynamics of the environment. In this work, we operate in the Partially Observable Markov Decision Process (POMDP) setting where instead of states $s \in \mathcal{S}$ the agent receives observations $o \in \mathcal{O}$ that only have partial information about the true state of the environment.</p>
<p>Full state information may be incomplete due to missing information about the goal in the environment, which the agent must instead infer through rewards with memory, or because the observations are pixel-based, or both.</p>
<p>Online and Offline Reinforcement Learning: Reinforcement Learning algorithms aim to maximize the return, defined as the cumulative sum of rewards $\sum_{t} \gamma^{t} r_{t}$, throughout an agent's lifetime or episode of training. RL algorithms broadly fall into two categories: on-policy algorithms (Williams, 1992) where the agent directly maximizes a Monte-Carlo estimate of the total returns or off-policy (Mnih et al., 2013; 2015) where an agent learns and maximizes a value function that approximates the total future return. Most RL algorithms maximize returns through trial-and-error by directly interacting with the environment. However, offline RL (Levine et al., 2020) has recently emerged as an alternate and often complementary paradigm for RL where an agent aims to extract return maximizing policies from offline data gathered by another agent. The offline dataset consists of $(s, a, r)$ tuples which are often used to train an off-policy agent, though other algorithms for extracting return maximizing policies from offline data are also possible.</p>
<p>Self-Attention and Transformers The self-attention (Vaswani et al., 2017) operation begins by projecting input data $X$ with three separate matrices onto $D$-dimensional vectors called queries $Q$, keys $K$, and values $V$. These vectors are then passed through the attention function:</p>
<p>$$
\text { Attention }(Q, K, V)=\operatorname{softmax}\left(Q K^{T} / \sqrt{D}\right) V
$$</p>
<p>The $Q K^{T}$ term computes an inner product between two projections of the input data $X$. The inner product is then normalized and projected back to a $D$-dimensional vector with the scaling term $V$. Transformers (Vaswani et al., 2017; Devlin et al., 2018; Brown et al., 2020) utilize self-attention as a core part of the architecture to process sequential data such as text sequences. Transformers are usually pre-trained with a self-supervised objective that predicts tokens within the sequential data. Common prediction tasks include predicting randomly masked out tokens (Devlin et al., 2018) or applying a causal mask and predicting the next token (Radford et al., 2018).</p>
<p>Offline Policy Distillation: We refer to the family of methods that treat offline Reinforcement Learning as a sequential prediction problem as Offline Policy Distillation, or Policy Distillation (PD) for brevity. Rather than learning a value function from offline data, PD extracts policies by predicting actions in the offline data (i.e. behavior cloning) with a sequence model and either return conditioning (Chen et al., 2021; Lee et al., 2022) or filtering out suboptimal data (Reed et al., 2022). Initially proposed to learn single-task policies (Chen et al., 2021; Janner et al., 2021), PD was recently extended to learn multi-task policies from diverse offline data (Lee et al., 2022; Reed et al., 2022).</p>
<p>In-Context Learning: In-context learning refers to the ability to infer tasks from context. For example, large language models like GPT-3 (Brown et al., 2020) or Gopher (Rae et al., 2021) can be directed at solving tasks such as text completion, code generation, and text summarization by specifying the task through language as a prompt. This ability to infer the task from prompt is often called in-context learning. We use the terms 'in-weights learning' and 'in-context learning' from prior work on sequence models (Brown et al., 2020; Chan et al., 2022) to distinguish between gradient-based learning with parameter updates and gradient-free learning from context, respectively.</p>
<h2>3 Method</h2>
<p>Over the course of its lifetime a capable reinforcement learning (RL) agent will exhibit complex behaviours, such as exploration, temporal credit assignment, and planning. Our key insight is that an</p>
<p>agent's actions, regardless of the environment it inhabits, its internal structure, and implementation, can be viewed as a function of its past experience, which we refer to as its history. Formally, we write:</p>
<p>$$
\mathcal{H} \ni h_{t}:=\left(o_{0}, a_{0}, r_{0}, \ldots, o_{t-1}, a_{t-1}, r_{t-1}, o_{t}, a_{t}, r_{t}\right)=\left(o_{\leq t}, r_{\leq t}, a_{\leq t}\right)
$$</p>
<p>and we refer to a long ${ }^{2}$ history-conditioned policy as an algorithm:</p>
<p>$$
P: \mathcal{H} \cup \mathcal{O} \rightarrow \Delta(\mathcal{A})
$$</p>
<p>where $\Delta(\mathcal{A})$ denotes the space of probability distributions over the action space $\mathcal{A}$. Eqn. (3) suggests that, similar to a policy, an algorithm can be unrolled in an environment to generate sequences of observations, rewards, and actions. For brevity, we denote the algorithm as $P$ and environment (i.e. task) as $\mathcal{M}$, such that the history of learning for any given task $\mathcal{M}$ is generated by the algorithm $P_{\mathcal{M}}$.</p>
<p>$$
\left(O_{0}, A_{0}, R_{0}, \ldots, O_{T}, A_{T}, R_{T}\right) \sim P_{\mathcal{M}}
$$</p>
<p>Here, we're denoting random variables with uppercase Latin letters, e.g. $O, A, R$, and their values with lowercase Latin letters, e.g. $o, a, r$. By viewing algorithms as long history-conditioned policies, we hypothesize that any algorithm that generated a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. Next, we present a method that, provided agents' lifetimes, learns a sequence model with behavioral cloning to map long histories to distributions over actions.</p>
<h1>3.1 Algorithm Distillation</h1>
<p>Suppose the agents' lifetimes, which we also call learning histories, are generated by the source algorithm $P^{\text {source }}$ for many individual tasks $\left{\mathcal{M}<em n="1">{n}\right}</em>$ :}^{N}$, producing the dataset $\mathcal{D</p>
<p>$$
\mathcal{D}:=\left{\left(o_{0}^{(n)}, a_{0}^{(n)}, r_{0}^{(n)}, \ldots, o_{T}^{(n)}, a_{T}^{(n)}, r_{T}^{(n)}\right) \sim P_{\mathcal{M}<em n="1">{n}}^{\text {source }}\right}</em>
$$}^{N</p>
<p>Then we distill the source algorithm's behaviour into a sequence model that maps long histories to probabilities over actions with a negative log likelihood (NLL) loss and refer to this process as algorithm distillation (AD). In this work, we consider neural network models $P_{\theta}$ with parameters $\theta$ which we train by minimizing the following loss function:</p>
<p>$$
\mathcal{L}(\theta):=-\sum_{n=1}^{N} \sum_{t=1}^{T-1} \log P_{\theta}\left(A=a_{t}^{(n)} \mid h_{t-1}^{(n)}, o_{t}^{(n)}\right)
$$</p>
<p>Intuitively, a sequence model with fixed parameters that is trained with AD should amortise the source RL algorithm $P^{\text {source }}$ and by doing so exhibit similarly complex behaviours, such as exploration and temporal credit assignment. Since the RL policy improves throughout the learning history of the source algorithm, accurate action prediction requires the sequence model to not only infer the current policy from the preceding context but also infer the improved policy, therefore distilling the policy improvement operator.</p>
<h3>3.2 Practical Implementation</h3>
<p>In practice, we implement AD as a two-step procedure. First, a dataset of learning histories is collected by running an individual gradient-based RL algorithm on many different tasks. Next, a sequence model with multi-episodic context is trained to predict actions from the histories. We describe these two steps below and detail the full practical implementation in Algorithm 1.
Dataset Generation: A dataset of learning histories is collected by training $N$ individual single-task gradient-based RL algorithms. To prevent overfitting to any specific task during sequence model training, a task $\mathcal{M}$ is sampled randomly from a task distribution for each RL run. The data generation step is RL algorithm agnostic - any RL algorithm can be distilled. We show results distilling UCB exploration (Lai \&amp; Robbins, 1985), an on-policy actor-critic (Mnih et al., 2016), and an off-policy DQN (Mnih et al., 2013), in both distributed and single-stream settings. We denote the dataset of learning histories as $\mathcal{D}$ in Eq. 5. Training the Sequence Model: Once a dataset of learning histories $\mathcal{D}$ is collected, a sequential prediction model is trained to predict actions given the preceding</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mh">1</span><span class="w"> </span><span class="n">Algorithm</span><span class="w"> </span><span class="n">Distillation</span>
<span class="nl">Require:</span><span class="w"> </span><span class="n">Train</span><span class="w"> </span><span class="n">\(\left\{\mathcal{M}^{\text</span><span class="w"> </span><span class="p">{</span><span class="n">train</span><span class="w"> </span><span class="p">}}</span><span class="n">\right\}\)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="n">\(\left\{\mathcal{M}^{\text</span><span class="w"> </span><span class="p">{</span><span class="n">test</span><span class="w"> </span><span class="p">}}</span><span class="n">\right\}\)</span><span class="w"> </span><span class="n">tasks</span><span class="p">,</span><span class="w"> </span><span class="n">observations</span><span class="w"> </span><span class="n">\(o</span><span class="w"> </span><span class="n">\in</span><span class="w"> </span><span class="n">\mathcal{O}\),</span><span class="w"> </span><span class="n">actions</span><span class="w"> </span><span class="n">\(a</span><span class="w"> </span><span class="n">\in</span><span class="w"> </span><span class="n">\mathcal{A}\),</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">rewards</span><span class="w"> </span><span class="n">\(r</span><span class="w"> </span><span class="n">\in</span><span class="w"> </span><span class="n">\mathcal{R}\).</span>
<span class="nl">Require:</span><span class="w"> </span><span class="n">Network</span><span class="w"> </span><span class="n">parameters</span><span class="w"> </span><span class="n">\(\phi_{i}\)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">\(i=1,</span><span class="w"> </span><span class="n">\ldots,</span><span class="w"> </span><span class="n">N\)</span><span class="w"> </span><span class="n">source</span><span class="w"> </span><span class="n">RL</span><span class="w"> </span><span class="n">algorithms</span><span class="p">.</span>
<span class="nl">Require:</span><span class="w"> </span><span class="n">Network</span><span class="w"> </span><span class="n">parameters</span><span class="w"> </span><span class="n">\(\theta\)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">causal</span><span class="w"> </span><span class="n">transformer</span><span class="w"> </span><span class="n">\(P_{\theta}\)</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">predicts</span><span class="w"> </span><span class="n">actions</span><span class="p">.</span>
<span class="nl">Require:</span><span class="w"> </span><span class="n">An</span><span class="w"> </span><span class="n">empty</span><span class="w"> </span><span class="n">buffer</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">store</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="n">\(\mathcal{D}\).</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">\(i=1</span><span class="w"> </span><span class="n">\ldots</span><span class="w"> </span><span class="n">N\)</span><span class="w"> </span><span class="k">do</span>
<span class="w">        </span><span class="n">\(\triangleright\)</span><span class="w"> </span><span class="n">Part</span><span class="w"> </span><span class="mh">1</span><span class="o">:</span><span class="w"> </span><span class="n">Dataset</span><span class="w"> </span><span class="n">Generation</span>
<span class="w">        </span><span class="n">Sample</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="k">task</span><span class="w"> </span><span class="n">\(\mathcal{M}_{i}^{\text</span><span class="w"> </span><span class="p">{</span><span class="n">train</span><span class="w"> </span><span class="p">}}</span><span class="n">\)</span><span class="w"> </span><span class="n">randomly</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">train</span><span class="w"> </span><span class="k">task</span><span class="w"> </span><span class="n">distribution</span><span class="p">.</span>
<span class="w">        </span><span class="n">Train</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">source</span><span class="w"> </span><span class="n">RL</span><span class="w"> </span><span class="n">algorithm</span><span class="w"> </span><span class="n">\(\phi_{i}\)</span><span class="w"> </span><span class="n">until</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">converges</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">optimal</span><span class="w"> </span><span class="n">policy</span><span class="p">.</span>
<span class="w">        </span><span class="n">Save</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">learning</span><span class="w"> </span><span class="n">history</span><span class="w"> </span><span class="n">\(h_{T}^{(i)}=\left(o_{0},</span><span class="w"> </span><span class="n">a_</span><span class="p">{</span><span class="mh">0</span><span class="p">},</span><span class="w"> </span><span class="n">r_</span><span class="p">{</span><span class="mh">0</span><span class="p">},</span><span class="w"> </span><span class="n">\ldots,</span><span class="w"> </span><span class="n">o_</span><span class="p">{</span><span class="n">T</span><span class="p">},</span><span class="w"> </span><span class="n">a_</span><span class="p">{</span><span class="n">T</span><span class="p">},</span><span class="w"> </span><span class="n">r_</span><span class="p">{</span><span class="n">T</span><span class="p">}</span><span class="n">\right)_{i}\)</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">dataset</span><span class="w"> </span><span class="n">\(\mathcal{D}</span><span class="w"> </span><span class="n">\leftarrow</span><span class="w"> </span><span class="n">\mathcal{D}</span><span class="w"> </span><span class="n">\cup</span><span class="w"> </span><span class="n">h_</span><span class="p">{</span><span class="n">T</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="n">i</span><span class="p">)}</span><span class="n">\).</span>
<span class="w">    </span><span class="k">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="k">while</span><span class="w"> </span><span class="n">\(P_{\theta}\)</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">converged</span><span class="w"> </span><span class="k">do</span>
<span class="w">        </span><span class="n">\(\triangleright\)</span><span class="w"> </span><span class="n">Part</span><span class="w"> </span><span class="mh">2</span><span class="o">:</span><span class="w"> </span><span class="n">Algorithm</span><span class="w"> </span><span class="n">Distillation</span>
<span class="w">        </span><span class="n">Randomly</span><span class="w"> </span><span class="n">sample</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">multi</span><span class="o">-</span><span class="n">episodic</span><span class="w"> </span><span class="n">subsequence</span><span class="w"> </span><span class="n">\(\tilde{h}_{i}^{(i)}=\left(o_{j},</span><span class="w"> </span><span class="n">a_</span><span class="p">{</span><span class="n">j</span><span class="p">},</span><span class="w"> </span><span class="n">r_</span><span class="p">{</span><span class="n">j</span><span class="p">},</span><span class="w"> </span><span class="n">\ldots,</span><span class="w"> </span><span class="n">o_</span><span class="p">{</span><span class="n">j</span><span class="o">+</span><span class="n">c</span><span class="p">},</span><span class="w"> </span><span class="n">a_</span><span class="p">{</span><span class="n">j</span><span class="o">+</span><span class="n">c</span><span class="p">},</span><span class="w"> </span><span class="n">r_</span><span class="p">{</span><span class="n">j</span><span class="o">+</span><span class="n">c</span><span class="p">}</span><span class="n">\right)_{i}\)</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">length</span><span class="w"> </span><span class="n">\(c\).</span>
<span class="w">        </span><span class="n">Autoregressively</span><span class="w"> </span><span class="n">predict</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">actions</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">\(P_{\theta}\)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">compute</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">NLL</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">Eq</span><span class="p">.</span><span class="w"> </span><span class="mf">6.</span>
<span class="w">        </span><span class="n">Backpropagate</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">update</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">transformer</span><span class="w"> </span><span class="n">parameters</span><span class="p">.</span>
<span class="w">    </span><span class="k">end</span><span class="w"> </span><span class="k">while</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">\(k=1</span><span class="w"> </span><span class="n">\ldots</span><span class="w"> </span><span class="n">M_</span><span class="p">{</span><span class="n">\text</span><span class="w"> </span><span class="p">{</span><span class="n">amb</span><span class="w"> </span><span class="p">}}</span><span class="n">\)</span><span class="w"> </span><span class="k">do</span>
<span class="w">        </span><span class="n">\(\triangleright\)</span><span class="w"> </span><span class="n">Part</span><span class="w"> </span><span class="mh">3</span><span class="o">:</span><span class="w"> </span><span class="n">Autoregressive</span><span class="w"> </span><span class="n">Evaluation</span>
<span class="w">        </span><span class="n">Sample</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="k">task</span><span class="w"> </span><span class="n">\(\mathcal{M}_{k}^{\text</span><span class="w"> </span><span class="p">{</span><span class="n">test</span><span class="w"> </span><span class="p">}}</span><span class="n">\)</span><span class="w"> </span><span class="n">randomly</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="k">task</span><span class="w"> </span><span class="n">distribution</span><span class="p">.</span><span class="w"> </span><span class="n">Initialize</span><span class="w"> </span><span class="n">empty</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="n">queue</span><span class="w"> </span><span class="n">\(C\).</span>
<span class="w">        </span><span class="n">Unroll</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">transformer</span><span class="w"> </span><span class="n">\(P_{\theta}(\cdot</span><span class="w"> </span><span class="n">\mid</span><span class="w"> </span><span class="n">C</span><span class="p">)</span><span class="n">\)</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">environment</span><span class="w"> </span><span class="n">storing</span><span class="w"> </span><span class="n">sequential</span><span class="w"> </span><span class="n">transitions</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="p">.</span><span class="n">e</span><span class="p">.</span><span class="w"> </span><span class="n">histories</span><span class="p">)</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">\(C\).</span>
<span class="w">        </span><span class="n">Measure</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">accumulated</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">agent</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="n">episode</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">evaluation</span><span class="p">.</span>
<span class="w">    </span><span class="k">end</span><span class="w"> </span><span class="k">for</span>
</code></pre></div>

<p>histories. We utilize the GPT (Radford et al., 2018) causal transformer model for sequential action prediction, but AD is compatible with any sequence model including RNNs (Williams \&amp; Zipser, 1989). For instance, we show in Appendix $K$ that AD can also be achieved with an LSTM (Hochreiter \&amp; Schmidhuber, 1997), though less effectively than AD with causal transformers. Since causal transformer training and inference are quadratic in the sequence length, we sample across-episodic subsequences $\tilde{h}<em j="j">{j}=\left(o</em>$ rather than training full histories.}, r_{j}, a_{j} \ldots, o_{j+c}, r_{j+c}, a_{j+c}\right)$ of length $c&lt;T$ from $\mathcal{D</p>
<h1>4 EXPERIMENTAL SETUP</h1>
<h3>4.1 ENVIRONMENTS</h3>
<p>To investigate the in-context RL capabilities of AD and the baselines (see next section), we focus on environments that cannot be solved through zero-shot generalization after pre-training. Specifically, we require that each environment supports many tasks, that the tasks cannot be inferred easily from the observation, and that episodes are short enough to feasibly train across-episodic causal transformers - for more details regarding environments see Appendix A. We list the evaluation environments below:</p>
<p>Adversarial Bandit: a multi-armed bandit with 10 arms and 100 trials similar to the environment considered in $\mathrm{RL}^{2}$ (Duan et al., 2016). However, during evaluation the reward is out of distribution. Reward is more likely distributed under odd arms $95 \%$ of the time during training. At evaluation, the opposite happens reward appears more often under even arms $95 \%$ of the time.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Agent view from the DMLab Watermaze environment. The task is to find a hidden platform that elevates once found.</p>
<p>Dark Room: a 2D discrete POMDP where an agent spawns in a room and must find a goal location. The agent only knows its own $(x, y)$ coordinates but does not know the goal location and must infer it from the reward. The room size is $9 \times 9$, the possible actions are one step left, right, up, down, and no-op, the episode length is 20 , and the agent resets at the center of the map. We test two environment variants - Dark Room where the agent receives $r=1$ every time the goal is reached and Dark Room Hard, a hard exploration variant with a $17 \times 17$ size and sparse reward ( $r=1$ exactly once for reaching the goal). When not $r=1$, then $r=0$.
Dark Key-to-Door: similar to Dark Room but this environment requires an agent to first find an invisible key upon which it receives a reward of $r=1$ once and then open an invisible door upon</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Adversarial Bandit (Section 5): AD, $\mathrm{RL}^{2}$, and ED evaluated on a 10-arm bandit with 100 trials. The source data for AD comes from learning histories from UCB ( [Lai &amp; Robbins, 1985]). During training, the reward is distributed under odd arms $95 \%$ of the time and under even arms $95 \%$ of the time during evaluation. Both AD and $\mathrm{RL}^{2}$ can in-context learn in-distribution tasks, but AD generalizes better out of distribution. Running $\mathrm{RL}^{2}$ with a transformer generally doesn't offer an advantage over the original LSTM variant. ED performs poorly both in and out of distribution relative to AD and $\mathrm{RL}^{2}$. Scores are normalized relative to UCB.
which it receives a reward of $r=1$ once again. Otherwise, the reward is $r=0$. The room size is $9 \times 9$ making the task space combinatorial with $81^{2}=6561$ possible tasks. This environment is similar to the one considered in <em>Chen et al. (2021)</em> except the key and door are invisible and the reward is semisparse ( $r=1$ for both key and the door). The agent is randomly reset. The episode length is 50 steps.
DMLab Watermaze: a partially observable 3D visual DMLab environment based on the classic Morris Watermaze ( [Morris, 1981]). The task is to navigate the water maze to find a randomly spawned trap-door. The maze walls have color patterns that can be used to remember the goal location. Observations are pixel images of size $72 \times 96 \times 3$. There are 8 possible actions in total, including going forward, backward, left, or right, rotating left or right, and rotating left or right while going forward. The episode length is 50, and the agent resets at the center of the map. Similar to Dark Room, the agent cannot see the location of the goal from the observations and must infer it through the reward of $r=1$ if reached and $r=0$ otherwise; however, the goal space is continuous and therefore there are an infinite number of goals.</p>
<h1>4.2 BASELINES</h1>
<p>The main aim of this work is to investigate to what extent AD reinforcement learns in-context relative to prior related work. AD is mostly closely related to Policy Distillation, where a policy is learned with a sequential model from offline interaction data. In-context online meta-RL is also related though not directly comparable to AD , since AD is an in-context offline meta-RL method. Still, we consider both types of baselines to better contextualize our work. For a more detailed discussion of these baseline choices we refer the reader to Appendix B. Our baselines include:</p>
<p>Expert Distillation (ED): this baseline is exactly the same as AD but the source data consists of expert trajectories only, rather than learning histories. ED is most similar to Gato ( [Reed et al., 2022]) except ED models state-action-reward sequences like AD, while Gato models state-action sequences.</p>
<p>Source Algorithm: we compare AD to the gradient-based source RL algorithm that generates the training data for distillation. We include running the source algorithm from scratch as a baseline to compare the data-efficiency of in-context RL to the in-weights source algorithm.
$R L^{2}$ (Duan et al., 2016): an online meta-RL algorithm where exploration and fast in-context adaptation are learned jointly by maximizing a multi-episodic value function. $\mathrm{RL}^{2}$ is not directly comparable to AD for similar reasons to why online and offline RL algorithms are not directly comparable $-\mathrm{RL}^{2}$ gets to interact with the environment during training while AD does not. We use $\mathrm{RL}^{2}$ asymptotic performance as an approximate upper bound for AD .</p>
<h3>4.3 EVALUATION</h3>
<p>After pre-training, the AD transformer $P_{\theta}$ can reinforcement learn in-context. Evaluation is exactly the same as with an in-weights RL algorithm except the learning happens entirely in-context without updating the transformer network parameters. Given an MDP (or POMDP), the transformer interacts with the environment and populates its own context (i.e. without demonstrations), where the context is a queue containing the last $c$ transitions. The transformer's performance is then evaluated in terms</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Main results: we evaluate $\mathrm{AD}, \mathrm{RL}^{2}$, ED, and the source algorithm on environments that require memory and exploration. In these environments, an agent must reach a goal location that can only be inferred through a binary reward. AD is consistently able to in-context reinforcement learn across all environments and is more data-efficient than the A3C ("Dark" environments) [mnih2016asynchronous] or DQN (Watermaze) [mnih2013dqn] source algorithm it distilled. We report the mean return $\pm 1$ standard deviation over 5 training seeds with 20 test seeds each.
of its ability to maximize return. For all evaluation runs, we average results across 5 training seeds with 20 evaluation seeds each for a total of 100 seeds. A task $\mathcal{M}$ is sampled uniformly from the test task distribution and fixed for each evaluation seed. The aggregate statistics reported reflect multi-task performance. We evaluate for 1000 and 160 episodes for the Dark and Watermaze environments respectively and plot performance as a function of total environment steps at test-time.</p>
<h1>5 EXPERIMENTS</h1>
<p>The main research question of this work is whether an in-weights RL algorithm can be amortized into an in-context one via Algorithm Distillation. The in-context RL algorithm should behave in a similar way as the in-weights one and exhibit exploration, credit assignment, and generalization capabilities. We begin our analysis in a clean and simple experimental setting where all three properties are required to solve the task - the Adversarial Bandit described in Sec. 4.
To generate the source data, we sample a set of training tasks $\left{\mathcal{M}<em j="1">{j}\right}</em>$ can reliably in-context learn tasks sampled from the training distribution while ED cannot, though ED does do better than random guessing when evaluated in-distribution. However, AD can also in-context learn to solve out of distribution tasks whereas the other methods cannot. This experiment shows that AD can explore the bandit arms, can assign credit by exploiting an arm once reached, and can generalize well out of distribution nearly as well as UCB. We now move beyond the bandit setting and investigate similar research questions in more challenging RL environments and present our results as answers to a series of research questions.}^{N}$, run the Upper Confidence Bound algorithm [Lai \&amp; Robbins (1985)], and save its learning histories. We then train a transformer to predict actions as described in Alg. 1. We evaluate AD, ED, and $\mathrm{RL}^{2}$ and normalize their scores relative to UCB and a random policy $\left(r-r_{\text {rand. }}\right) /\left(r_{U C B}-r_{\text {rand. }}\right)$. The results are shown in Fig. 3. We find that both AD and $\mathrm{RL}^{2</p>
<p>Does Algorithm Distillation exhibit in-context reinforcement learning? To answer this question, we first generate source data for Algorithm Distillation. In the Dark Room and Dark Key-toDoor environments we collect 2000 learning histories with an Asynchronous Advantage Actor-Critic (A3C) [mnih2016asynchronous] with 100 actors, while in DMLab Watermaze we collect 4000 learning histories with a distributed DQN with 16 parallel actors (see Appendix E for asymptotic learning curves of the source algorithm and Appendix M for hyperparameters). Shown in Fig. 4, AD in-context reinforcement learns in all of the environments. In contrast, ED fails to explore and learn in-context in most settings. We use $\mathrm{RL}^{2}$ trained for 1 billion environment steps as a proxy for the upper bound of performance for a meta-RL method. Despite learning to reinforcement learn from offline data, AD matches asymptotic $\mathrm{RL}^{2}$ on the Dark environments and approaches it (within $13 \%$ ) on Watermaze.
Credit-assignment: In Dark Room, the agent receives $r=1$ each time it visits the goal location. Even though AD is trained to condition only on single timestep reward and not episodic return tokens, it is still able to maximize the reward, which suggests that AD has learned to do credit assignment.
Exploration: Dark Room (Hard) tests the agents exploration capability. Since the reward is sparse ( $r=1$ exactly once), most of the learning history has reward values of $r=0$. Nevertheless, AD infers the goal from previous episodes in its context which means it has learned to explore and exploit.</p>
<p>Generalization: Dark Key-to-Door tests in-distribution generalization with a combinatorial task space. While the environment has a total of $\sim 6.5$k tasks, less than 2k were seen during training. During evaluation, AD both generalizes and achieves near-optimal performance on mostly unseen tasks.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: $A D$ and $E D$ conditioned on partial demonstrations: We compare the performance of AD and ED when prompted with a demonstration from the source algorithm’s training history on Dark Room (semi-dense). While ED slightly improves and then maintains performance from the input policy, AD is able to improve it in-context until the policy is optimal or nearly optimal.
Can Algorithm Distillation learn from pixel-based observations? DMLab Watermaze is a pixel-based environment that is larger than the Dark environments with tasks sampled from a continuous uniform distribution. The environment is partially observable in two ways - the goal is invisible until the agent has reached it and the firstperson view limits the agent’s field of vision. Shown in Fig. 4, AD maximizes the episodic return with in-context RL while ED does not learn.
Can AD learn a more data-efficient RL algorithm than the one that produced the source data? In Fig. 4, AD is significantly more data-efficient than the source algorithm. This gain is a byproduct of distilling a multi-stream algorithm into a single-stream one. The source algorithms (A3C and DQN) are distributed, which means they run many actors in parallel to achieve good performance. A distributed RL algorithm may not be very data-efficient in aggregate but each individual actor can be data-efficient. Since the learning history for each actor is saved separately, AD achieves similar performance to the multi-stream distributed RL algorithm, but is more data-efficient as a single-stream method.</p>
<p>These data-efficiency gains are also evident for distilling single-stream algorithms. In Fig 6 we show that by subsampling every $k$-th episode (where $k=10$ ) from a single stream A3C learning history, AD can still learn a more data-efficient in-context RL algorithm (for more detail, see Appendix I). Therefore, AD can be more data-efficient than both a multi and single-stream source RL algorithm.</p>
<p>While AD is more data-efficient, the source algorithm achieves slightly higher asymptotic performance (see Appendix E). However, the source algorithm produces many single-task agents with a unique set of weights $\phi_{n}$ per task $\mathcal{M}_{n}$, while AD produces a single generalist agent with weights $\theta$ that are fixed across all tasks.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Single-stream Algorithm Distillation: AD trained on the learning history from an A3C agent with only one actor (i.e. single-stream). By training on subsampled learning histories (see Sec. 5), AD learns are more data-efficient in-context RL algorithm.</p>
<p>Is it possible to accelerate AD by prompting it with demonstrations? Although AD can reinforcement learn without relying on demonstrations, it has the added benefit that, unlike the source algorithm, it can be conditioned on or prompted with external data. To answer the research question, we sample</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>policies from the hold-out test-set data along different points of the source algorithm history - from a near-random policy to a near-expert policy. We then pre-fill the context for both AD and ED with this policy data, and run both methods in the environment in Dark Room and plot the results in Fig. 5. While ED maintains the performance of the input policy, AD improves every policy in-context until it is near-optimal. Importantly, the more optimal the input policy the faster AD improves it until it is optimal.</p>
<p>What context size is required for in-context RL to emerge? We've hypothesized that AD requires sufficiently long (i.e. across-episodic) contexts to in-context reinforcement learn. We test this hypothesis by training several AD variants with different context sizes on the Dark Room environment. We plot the learning curves of these different variants in Fig. 7 and find that multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. Initial signs of in-context RL begin to emerge when the context size is roughly the length of an episode. The reason for this is likely that the context is large enough to retrain across-episodic information - e.g., at the start of a new episode, the context will be filled with transitions from most of the previous episode.</p>
<h1>6 Related Work</h1>
<p>Offline Policy Distillation: Most closely related to our work are the recent advances in learning policies from offline environment interaction data with transformers, which we have been referring to as Policy Distillation (PD). Initial PD architectures such as Decision Transformer (Chen et al., 2021) and Trajectory Transformer (Janner et al., 2021) showed that transformers can learn single-task policies from offline data. Subsequently the Multi-Game Decision Transformer (MGDT) (Lee et al., 2022) and Gato (Reed et al., 2022) showed that PD architectures can also learn multi-task same domain and cross-domain policies, respectively. Importantly, these prior methods use contexts substantially smaller than an episode length, which is likely the reason in-context RL was not observed in these works. Instead, they rely on alternate ways to adapt to new tasks - MGDT finetunes the model parameters while Gato gets prompted with expert demonstrations to adapt to downstream tasks. AD adapts in-context without finetuning and does not rely on demonstrations. Finally, a number of recent works have explored more generalized PD architectures (Furuta et al., 2021), prompt conditioning (Xu et al., 2022), and online gradient-based RL (Zheng et al., 2022).
Meta Reinforcement Learning: AD falls into the category of methods that learn to reinforcement learn, also known as meta-RL. Specifically, AD is an in-context offline meta-RL method. This general idea of learning the policy improvement process has a long history in reinforcement learning, but has been limited to meta-learning hyper-parameters until recently (Ishii et al., 2002). In-context deep meta-RL methods introduced by Wang et al. (2016) and Duan et al. (2016) are usually trained in the online setting by maximizing multi-episodic value functions with memory-based architectures through environment interactions. Another common approach to online meta-RL includes optimization-based methods that find good network parameter initializations for meta-RL (Hochreiter et al., 2001; Finn et al., 2017; Nichol et al., 2018) and adapt by taking additional gradient steps. Like other in-context meta-RL approaches, AD is gradient-free - it adapts to downstream tasks without updating its network parameters. Recent works have proposed learning to reinforcement learn from offline datasets, or offline meta-RL, using Bayesian RL (Dorfman et al., 2021) and optimization-based meta-RL (Mitchell et al., 2021). Given the difficulty of offline meta-RL, Pong et al. (2022) proposed a hybrid offline-online strategy for meta-RL.</p>
<p>In-Context Learning with Transformers: In this work, we make the distinction between in-context learning and incremental or in-context learning. In-context learning involves learning from a provided prompt or demonstration while incremental in-context learning involves learning from one's own behavior through trial and error. While many recent works have demonstrated the former, it is much less common to see methods that exhibit the latter. Arguably, the most impressive demonstrations of in-context learning to date have been shown in the text completion setting (Radford et al., 2018; Chen et al., 2020; Brown et al., 2020) through prompt conditioning. Similar methodology was recently extended to show powerful composability properties in text-conditioned image generation (Yu et al., 2022). Recent work showed that transformers can also learn simple algorithm classes, such as linear regression, in-context in a small-scale setting (Garg et al., 2022). Like prior in-context learning methods, Garg et al. (2022) required initializing the transformer prompt with expert examples. While the aforementioned approaches were examples of in-context learning, a recent work (Chen et al.,</p>
<p>2022) demonstrated incremental in-context learning for hyperparameter optimization by treating hyperparameter optimization as a sequential prediction problem with a score function.</p>
<h1>7 CONCLUSION</h1>
<p>We have demonstrated that Algorithm Distillation can distill an in-weights RL algorithm into an in-context RL algorithm by modeling RL learning histories with a causal transformer and that AD can learn more data-efficient algorithms than those that generated the source data. The main limitation of AD is that most RL environments of interest have long episodes and modeling multi-episodic context requires more powerful long-horizon sequential models than the ones considered in this work. We believe this is a promising direction for future research and hope that AD inspires further investigation into in-context reinforcement learning from the research community.</p>
<h2>REFERENCES</h2>
<p>Charles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Küttler, Andrew Lefrancq, Simon Green, Víctor Valdés, Amir Sadik, Julian Schrittwieser, Keith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King, Demis Hassabis, Shane Legg, and Stig Petersen. DeepMind Lab. CoRR, abs/1612.03801, 2016.</p>
<p>Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.</p>
<p>Stephanie CY Chan, Adam Santoro, Andrew K Lampinen, Jane X Wang, Aaditya Singh, Pierre H Richemond, Jay McClelland, and Felix Hill. Data Distributional Properties Drive Emergent In-Context Learning in Transformers. arXiv preprint arXiv:2205.05055, 2022.</p>
<p>Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084-15097, 2021.</p>
<p>Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In International Conference on Machine Learning, pp. 16911703. PMLR, 2020.</p>
<p>Yutian Chen, Xingyou Song, Chansoo Lee, Zi Wang, Qiuyi Zhang, David Dohan, Kazuya Kawakami, Greg Kochanski, Arnaud Doucet, Marc'aurelio Ranzato, Sagi Perel, and Nando de Freitas. Towards Learning Universal Hyperparameter Optimizers with Transformers, 2022.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Ron Dorfman, Idan Shenfeld, and Aviv Tamar. Offline Meta Reinforcement Learning-Identifiability Challenges and Effective Data Collection Strategies. Advances in Neural Information Processing Systems, 34:4607-4618, 2021.</p>
<p>Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. RL²: Fast Reinforcement Learning via Slow Reinforcement Learning, 2016.</p>
<p>Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pp. 1126-1135. PMLR, 2017.</p>
<p>Hiroki Furuta, Yutaka Matsuo, and Shixiang Shane Gu. Generalized decision transformer for offline hindsight information matching. arXiv preprint arXiv:2111.10364, 2021.</p>
<p>Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What Can Transformers Learn In-Context? A Case Study of Simple Function Classes, 2022.</p>
<p>Matteo Hessel, Ivo Danihelka, Fabio Viola, Arthur Guez, Simon Schmitt, Laurent Sifre, Theophane Weber, David Silver, and Hado van Hasselt. Muesli: Combining Improvements in Policy Optimization. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 4214-4226. PMLR, 2021.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. Long Short-Term Memory. Neural Computation, 9(8): $1735-1780,1997$.</p>
<p>Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent. In International conference on artificial neural networks, pp. 87-94. Springer, 2001.</p>
<p>Shin Ishii, Wako Yoshida, and Junichiro Yoshimoto. Control of exploitation-exploration metaparameter in reinforcement learning. Neural networks, 15(4-6):665-687, 2002.</p>
<p>Michael Janner, Qiyang Li, and Sergey Levine. Reinforcement Learning as One Big Sequence Modeling Problem. arXiv preprint arXiv:2106.02039, 2021.
T.L Lai and Herbert Robbins. Asymptotically Efficient Adaptive Allocation Rules. Adv. Appl. Math., 6(1):4-22, mar 1985. ISSN 0196-8858. doi: 10.1016/0196-8858(85)90002-8.</p>
<p>Kuang-Huei Lee, Ofir Nachum, Mengjiao Yang, Lisa Lee, Daniel Freeman, Winnie Xu, Sergio Guadarrama, Ian Fischer, Eric Jang, Henryk Michalewski, and Igor Mordatch. Multi-Game Decision Transformers, 2022.</p>
<p>Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.</p>
<p>Eric Mitchell, Rafael Rafailov, Xue Bin Peng, Sergey Levine, and Chelsea Finn. Offline metareinforcement learning with advantage weighting. In International Conference on Machine Learning, pp. 7780-7791. PMLR, 2021.</p>
<p>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.</p>
<p>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529-533, 2015.</p>
<p>Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous Methods for Deep Reinforcement Learning. In Maria-Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, volume 48 of JMLR Workshop and Conference Proceedings, pp. 1928-1937. JMLR.org, 2016.</p>
<p>Richard G.M. Morris. Spatial localization does not require the presence of local cues. Learning and Motivation, 12(2):239-260, 1981. doi: 10.1016/0023-9690(81)90020-5.</p>
<p>Rafael Müller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help? In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/ f1748d6b0fd9d439f71450117eba2725-Paper.pdf.</p>
<p>Alex Nichol, Joshua Achiam, and John Schulman. On First-Order Meta-Learning Algorithms, 2018.
Vitchyr H Pong, Ashvin V Nair, Laura M Smith, Catherine Huang, and Sergey Levine. Offline meta-reinforcement learning with online self-supervision. In International Conference on Machine Learning, pp. 17811-17829. PMLR, 2022.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training, 2018.</p>
<p>Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling Language Models: Methods, Analysis \&amp; Insights from Training Gopher, 2021.</p>
<p>Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A Generalist Agent, 2022.</p>
<p>Andrei A. Rusu, Sergio Gomez Colmenarejo, Çaglar Gülçehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy distillation. In Yoshua Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1511.06295.</p>
<p>Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy P. Lillicrap, and David Silver. Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model. CoRR, abs/1911.08265, 2019.</p>
<p>Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2818-2826, 2016. doi: 10.1109/CVPR.2016.308.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, 2017.</p>
<p>Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn, 2016.</p>
<p>Ronald J. Williams. Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning. Mach. Learn., 8:229-256, 1992. doi: 10.1007/BF00992696.
Ronald J Williams and David Zipser. A learning algorithm for continually running fully recurrent neural networks. Neural computation, 1(2):270-280, 1989.</p>
<p>Mengdi Xu, Yikang Shen, Shun Zhang, Yuchen Lu, Ding Zhao, Joshua Tenenbaum, and Chuang Gan. Prompting decision transformer for few-shot policy generalization. In International Conference on Machine Learning, pp. 24631-24645. PMLR, 2022.</p>
<p>Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling Autoregressive Models for Content-Rich Text-to-Image Generation, 2022.</p>
<p>Qinqing Zheng, Amy Zhang, and Aditya Grover. Online decision transformer. arXiv preprint arXiv:2202.05607, 2022.</p>
<h1>A EnVIRONMENT CONSIDERATIONS</h1>
<p>In this work, we consider environments where zero-shot generalization is difficult, so the agent must learn through trial and error. We also want environments where overfitting to any particular task is difficult to ensure our method is general. A final practical consideration is that we consider environments wher across-episodic histories can be feasibly modeled with a causal transformer. Given these considerations, our evaluation environments need to satisfy three criteria:</p>
<ol>
<li>Supports many tasks: The environments must be multi-task to ensure that our agent and the baselines do not overfit to any single task and instead is able to in-context reinforcement learn across many tasks within a given domain.</li>
<li>Task must be hard to infer: To ensure that the downstream tasks are hard to generalize to in zero-shot, we use environments that require exploration. Namely, we require environments where either the task can only be inferred from the reward and not the observation, or tasks that are partially observable.</li>
<li>Supports multi-episodic contexts: Lastly, we impose a practical constraint - the environment episodes must be short enough such that a normal GPT-like transformer can fit multiple episodes in its context. Since this work introduces AD as a method, we wish to investigate it in the cleanest possible setting using a canonical architecture. We leave investigating AD with more complex architectures that scale to longer sequences for future work.</li>
</ol>
<p>Prior related works (Chen et al., 2021; Lee et al., 2022; Janner et al., 2021; Reed et al., 2022) evaluated on Atari, OpenAI gym, and as well as other environments. However, Atari and OpenAI gym don't satisfy at least one of the above criteria. Atari and OpenAI gym episodes are often long and can contain thousands or more transitions per episode, so it's technically challenging to populate a causal transformer's context with across-episode histories. Indeed, the prior related works only considered within-episode context lengths. Additionally, it is often easy to infer the task from either the observation or the dense reward alone in both Atari and OpenAI gym, which reduces the need for exploration. For these reasons, we evaluate in environments that satisfy all three criteria instead.</p>
<h2>B Closely Related Prior Methods</h2>
<p>In our main results we use Expert Distillation (ED) as a baseline. Here, we discuss how the most closely related methods differ from AD and why ED is sufficient to support the paper's claims.
Expert Distillation (ED): ED is most similar to Gato (Reed et al., 2022), which models expert sequences from a converged RL policy using a causal transformer. ED also trains a causal transformer to predict actions using expert policy data. There are two key differences between ED and Gato. First, unlike Gato which utilizes small (relative to an episode length) within-episode contexts, ED is trained on the same across-episode contexts as AD, so the architectures used by ED and AD are the same. The benefits of AD cannot therefore be attributed to across-episode contexts alone but also learning progress in the offline data used to train AD. Second, ED models state-action-reward sequences while Gato models only state-action sequences. The main difference between ED and AD is that AD is trained on full multi-task learning histories rather than expert policy data.
Decision Transformer (DT) (Chen et al., 2021) and Multi-Game Decision Transformer (Lee et al., 2022): DTs learn return-conditioned policies from single-task offline data collected by an RL agent. While the training data itself (an RL agent's replay buffer) contains learning, the context sizes used in DT are too small to capture any learning progress or identify the task using across-episode information. For instance, the Atari experiments use a context of length $30-50$ tokens, or $10-17$ transitions. Atari games can have hundreds or thousands of transitions in a single episode, which means these contexts capture mostly within-episode information. Additionally, very little learning progress happens in the underlying replay buffer data within that many transitions.
Another difference between DT and AD / ED is that DT learns a return-conditioned model whereas AD / ED are both reward-conditioned. In our setting return-conditioning alone cannot yield an optimal policy since the agent does not know the task until after it explores the environment and can identify it using across-episode contexts. Since (i) DT uses small within-episodic contexts and (ii) return-conditioning would not help in the environments considered, this baseline is similar to ED</p>
<p>with a small within-episode context which is strictly weaker than the long across-episode context variant of ED we consider.</p>
<p>Trajectory Transformer (TTO) (Janner et al., 2021): Like AD, TTO also models state-action-reward tokens but in addition to predicting actions it also learns a world model by predicting states and rewards. To maximize return, TTO then uses beam search to select high-reward actions. However, in our setting, TTO will run into the same problem as DT. To model rewards accurately it will need longer across-episodic contexts since one environment supports many tasks. Similar to DT, MGDT, and Gato, TTO uses smaller within-episode contexts. For this reason, TTO will fare no better than DT, MGDT, or ED in the settings we consider. We also note that in contrast to TTO, AD is model-free. In AD, actions are sampled from the transformer history-conditioned predictions and return maximization emerges from modeling the learning histories of an RL algorithm.</p>
<p>To summarize, AD differs from prior methods mainly because its context is across-episodic and hence large enough to capture learning progress and task information. AD could further be augmented by learning world models like TTO or conditioning on returns like DT, but these investigations would be well suited for future work since they are tangential to the main research question addressed in this work - whether in-context RL can emerge by imitating the learning histories of an RL algorithm with long across-episodic contexts.</p>
<p>AD is also closely related to prior work in in-context meta-RL. While both AD and in-context meta-RL model across-episodic histories with memory-based architectures, prior in-context meta-RL algorithms, such as RL2 (Duan et al., 2016) are trained online and rely on learning multi-episodic value functions with TD learning while AD is trained offline and uses a supervised imitation learning objective.</p>
<h1>C EXPERT DISTILLATION MAIN RESULTS</h1>
<p>We elaborate further on the main results in Fig. 4 and provide intuition regarding the behaviors of the ED baseline. In Dark Room, Dark Room (Hard), and Watermaze, ED performance is either flat or it degrades. The reason for this is that ED only saw expert trajectories during training, but during evaluation it needs to first explore (i.e. perform non-expert behavior) to identify the task. This required behavior is out-of-distribution for ED and for this reason it does not reinforcement learning in-context. In Dark Key-to-Door the agent is reset randomly at the beginning of each episode, whereas in all of the environments the agent's starting position is fixed. Due to random resets, the ED agent is sometimes reset by the first goal in Dark Key-to-Door which allows it to occasionally identify the first goal of the task, which is why it shows slight improvement.</p>
<h2>D MODEL SIZE</h2>
<p>We investigate how transformer capacity affects performance in Fig. 8. While in-context RL emerges across all model sizes investigated, we find that increasing the model depth, the model width in terms of embedding dimension, and (to a lesser extent) the number of attention heads improves performance on Dark Key-to-Door.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Model size investigations: We investigate how increasing model capacity affects AD. While in-context RL with AD emerges regardless of the model capacity, increasing the model depth and width helps improve AD until it achieves near-optimal performance.</p>
<h1>E Source Algorithm Training Runs</h1>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Asymptotic performance of the A3C (Mnih et al., 2016) and a Q- $\lambda$ variant of the DQN (Mnih et al., 2013) RL algorithms used to produce learning histories for the Dark and Watermaze environments. These curves show the learning histories AD is trained on. The source algorithms plotted in Fig. 4 are the same as in these plots.</p>
<h2>F Label Smoothing Ablation</h2>
<p>For the harder exploration task of Dark Room (Hard), we found that adding label smoothing regularization (Szegedy et al., 2016; Müller et al., 2019) improved the in-context learning ability of AD . In Figure 10 we ablate the benefit of using label smoothing for 3 different $\alpha$ values as well as with it turned off. Each curve in the figure denotes average performance over 5 training seeds. We can see that adding label smoothing up to a point improves the in-context learning ability of Algorithm Distillation, with performance continually increasing with the number of evaluation episodes.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: AD trained with different amounts of label smoothing on Dark Room (Hard).</p>
<h2>G RL ${ }^{2}$ NETWORK ARCHITECTURE: TRANSFORMER VS LSTM</h2>
<p>We compared using a transformer as the architecture for $\mathrm{RL}^{2}$ instead of an LSTM. In Figure 11, we ran both transformer and LSTM RL ${ }^{2}$ agents over the Dark Room environment. The curves shown are the best from a sweep over learning rate and unroll length hyperparameters. The transformer architecture is 4 -layers with a model size of 256 and pre-norm layer normalization placement. While both agents reached a similar level of final performance, all $\mathrm{RL}^{2}$ transformer models trained tended to be more unstable with the average return not as consistent as with an LSTM architecture. Given the poor performance of the transformer-based $\mathrm{RL}^{2}$ on the simpler Dark Room setting, our other experimental settings used the LSTM.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 11: Comparison of LSTM and Transformer architecture for RL ${ }^{2}$ agent on Dark Room. Each curve is averaged over 5 training seeds with the shaded area representing the standard error.</p>
<h1>H Number of Training Tasks in Source Data</h1>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 12: Algorithm distillation trained on different numbers of training tasks on Dark Key-to-Door evaluated on a fixed set of test tasks for 300 episodes of evaluation.</p>
<p>One interesting question is how many tasks AD needs to be trained on to learn an algorithm that generalizes to held out tasks. We trained AD on different numbers of Dark Key-to-Door training tasks and evaluated the resulting models on the same set of test tasks. Figure 12 shows the incontext learning plots for the resulting AD models on the set of test tasks. As a reminder, there are $81^{2}=6561$ unique Dark Key-to-Door tasks. Models trained on 1, 9 or 18 training tasks did not show any in-context learning on test tasks. While models trained on 37, 75 and 151 tasks did not achieve good performance overall, they did exhibit some in-context learning over the course of 300 episodes. The best models were trained on 1212 and 2424 tasks which corresponds to roughly $18 \%$ and $37 \%$ of the total number of tasks in the Dark Key-to-Door domain.</p>
<h2>I Single-Stream Algorithm Distillation</h2>
<p>We provide more details around the experimental setup for the single-stream result shown in Fig. 6. We showed in Fig. 4 that when AD is trained on data from a subset of the actors of a distributed source RL algorithm, the resulting model is more data efficient than the source algorithm. Here we confirm that AD can produce a faster algorithm than the one it was trained on in the single-stream setting. For this experiment we trained A3C on 2048 Dark Key-to-Door tasks for 2000 episodes each. We then trained AD on the resulting data while subsampling the learning histories by a factor of 10. More concretely, we took every 10th episode from each of the learning history, which resulted in a 200 episode compressed learning trajectory for each task. Figure 6 compares the resulting AD model evaluated on a set of test tasks to the performance of the source algorithm on these tasks. The model learned by AD learns much faster than the source algorithm confirming that Algorithm Distillation can turn a slow gradient-based algorithm into a much more data efficient in-context learning algorithm.</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 13: Downstream performance of Algorithm Distillation with different values of random masking during training in 9x9 1 goal gridworld.</p>
<p>During training, input tokens were randomly masked to avoid overfitting to training data. This plot shows the downstreams results on a 9x9 Dark Key-to-Door domain with different values of this random masking. Values of $0.3-0.5$ perform the best with the value of 0.3 chosen for all experiments.</p>
<h1>K AD NETWORK ARCHITECTURE: TRANSFORMER VS LSTM</h1>
<p>Here we consider the importance of the Transformer architecture to the success of algorithm distillation (AD) by comparing to AD based off of an LSTM (Hochreiter \&amp; Schmidhuber, 1997). Specifically, the LSTM receives the concatenated embeddings of $\left(o_{i}, a_{i}, r_{i}\right)$ triplets up to the most recent time step $t-1$. The output of the LSTM is then concatenated with the current observation $o_{t}$ embedding and both are then fed through a multi-layer perceptron (MLP) policy torso to produce a distribution over the present action $a_{t}$. The LSTM hidden size (512), MLP depth (2), and MLP width (256) were swept and tuned by grid search based on downstream reward attainment.</p>
<p>Comparing Transformer AD and LSTM AD on the Dark Key-to-Door task, we find that both agents are capable of in-context learning, demonstrating that the success of AD is not tied to the underlying network architecture. However, we also find that the Transformer variant consistently outperforms the LSTM variant, which is why all other experiments in this paper employ the Transformer variant. This finding is consistent with the recent wider success of Transformer-based architectures over recurrent neural network (RNN)-based architectures in sequence prediction tasks.</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 14: Comparison between algorithm distillation with a Transformer and LSTM architecture on Dark Key-to-Door. Mean $\pm 1$ standard deviation over 5 training seeds and 20 evaluation seeds. 300 episodes corresponds to 15 k environment steps.</p>
<h1>L Algorithm Distillation Hyperparameters</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: center;">Dark Room</th>
<th style="text-align: center;">Dark Room (Hard)</th>
<th style="text-align: center;">Dark Key-to-Door</th>
<th style="text-align: center;">Watermaze</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Embedding Dim.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">64</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Number of Layers</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Number of Heads</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Feedforward Dim.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2048</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Position Encodings</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Absolute</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Layer Norm Placement</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Post Norm</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Dropout Rate</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Attention Dropout Rate</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: left;">Sequence Mask Prob</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.3</td>
</tr>
<tr>
<td style="text-align: left;">Label Smoothing $\alpha$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>Table 1: Algorithm Distillation Architecture Hyperparameters.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Batch Size</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: left;">Optimizer</td>
<td style="text-align: center;">Adam</td>
</tr>
<tr>
<td style="text-align: left;">$\beta_{1}$</td>
<td style="text-align: center;">0.9</td>
</tr>
<tr>
<td style="text-align: left;">$\beta_{2}$</td>
<td style="text-align: center;">0.99</td>
</tr>
<tr>
<td style="text-align: left;">Gradient Clip Norm Threshold</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">Learning Rate Schedule</td>
<td style="text-align: center;">Cosine Decay</td>
</tr>
<tr>
<td style="text-align: left;">Initial Value</td>
<td style="text-align: center;">$2 \mathrm{e}-6$</td>
</tr>
<tr>
<td style="text-align: left;">Peak Value</td>
<td style="text-align: center;">$3 \mathrm{e}-4$</td>
</tr>
</tbody>
</table>
<p>Table 2: Algorithm Distillation Optimization Hyperparameters.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Layer</th>
<th style="text-align: center;">Hyperparameter</th>
<th style="text-align: right;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Conv Block</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Channel</td>
<td style="text-align: right;">128</td>
</tr>
<tr>
<td style="text-align: left;">Conv</td>
<td style="text-align: center;">Kernel</td>
<td style="text-align: right;">5</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Stride</td>
<td style="text-align: right;">2</td>
</tr>
<tr>
<td style="text-align: left;">BatchNorm</td>
<td style="text-align: center;">Decay Rate</td>
<td style="text-align: right;">0.999</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">eps</td>
<td style="text-align: right;">$1 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: left;">Activation</td>
<td style="text-align: center;">-</td>
<td style="text-align: right;">ReLU</td>
</tr>
<tr>
<td style="text-align: left;">Max Pooling</td>
<td style="text-align: center;">Kernel</td>
<td style="text-align: right;">2</td>
</tr>
<tr>
<td style="text-align: left;">Dropout</td>
<td style="text-align: center;">Stride</td>
<td style="text-align: right;">2</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Rate</td>
<td style="text-align: right;">0.2</td>
</tr>
<tr>
<td style="text-align: left;">Network</td>
<td style="text-align: center;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">Conv Blocks</td>
<td style="text-align: center;">-</td>
<td style="text-align: right;">3</td>
</tr>
<tr>
<td style="text-align: left;">Final Linear Layer</td>
<td style="text-align: center;">Units</td>
<td style="text-align: right;">256</td>
</tr>
</tbody>
</table>
<p>Table 3: Watermaze Image Encoder Hyperparameters.</p>
<h1>M SOURCE RL ALGORITHM HYPERPARAMETERS</h1>
<h2>M. 1 DARK ENVIRONMENTS</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Batch Size (Num. Actors)</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;">$\lambda$</td>
<td style="text-align: center;">0.95</td>
</tr>
<tr>
<td style="text-align: left;">Agent Discount</td>
<td style="text-align: center;">0.99</td>
</tr>
<tr>
<td style="text-align: left;">Entropy Bonus Weight</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr>
<td style="text-align: left;">MLP Layers</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: left;">MLP Hidden Dim</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: left;">Optimizer</td>
<td style="text-align: center;">Adam</td>
</tr>
<tr>
<td style="text-align: left;">$\beta_{1}$</td>
<td style="text-align: center;">0.9</td>
</tr>
<tr>
<td style="text-align: left;">$\beta_{2}$</td>
<td style="text-align: center;">0.999</td>
</tr>
<tr>
<td style="text-align: left;">$\epsilon$</td>
<td style="text-align: center;">$1 \mathrm{e}-6$</td>
</tr>
<tr>
<td style="text-align: left;">Learning Rate</td>
<td style="text-align: center;">$1 \mathrm{e}-4$</td>
</tr>
</tbody>
</table>
<p>Table 4: Source A3C Algorithm Hyperparameters for Dark Environments.</p>
<h2>M. 2 DMLAB WATERMAZE</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Batch Size</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: left;">Rollout Length</td>
<td style="text-align: center;">40</td>
</tr>
<tr>
<td style="text-align: left;">Rollout Overlap</td>
<td style="text-align: center;">31</td>
</tr>
<tr>
<td style="text-align: left;">Number of Actors</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: left;">Reply Buffer Capacity</td>
<td style="text-align: center;">1 e 5</td>
</tr>
<tr>
<td style="text-align: left;">Offline Data Fraction</td>
<td style="text-align: center;">0.7</td>
</tr>
<tr>
<td style="text-align: left;">$\lambda$</td>
<td style="text-align: center;">0.75</td>
</tr>
<tr>
<td style="text-align: left;">$\epsilon$</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr>
<td style="text-align: left;">Agent Discount</td>
<td style="text-align: center;">0.9</td>
</tr>
<tr>
<td style="text-align: left;">Target Update Period</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: left;">ResNet Channels</td>
<td style="text-align: center;">$[32,64,64]$</td>
</tr>
<tr>
<td style="text-align: left;">ResNet Kernels</td>
<td style="text-align: center;">$[3,3,3]$</td>
</tr>
<tr>
<td style="text-align: left;">ResNet Strides</td>
<td style="text-align: center;">$[1,1,1]$</td>
</tr>
<tr>
<td style="text-align: left;">Pool Kernels</td>
<td style="text-align: center;">$[3,3,3]$</td>
</tr>
<tr>
<td style="text-align: left;">Pool Strides</td>
<td style="text-align: center;">$[2,2,2]$</td>
</tr>
<tr>
<td style="text-align: left;">Optimizer</td>
<td style="text-align: center;">Adam</td>
</tr>
<tr>
<td style="text-align: left;">$\beta_{1}$</td>
<td style="text-align: center;">0.9</td>
</tr>
<tr>
<td style="text-align: left;">$\beta_{2}$</td>
<td style="text-align: center;">0.999</td>
</tr>
<tr>
<td style="text-align: left;">$\epsilon$</td>
<td style="text-align: center;">$1 \mathrm{e}-6$</td>
</tr>
<tr>
<td style="text-align: left;">Gradient Clip Norm Threshold</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: left;">Learning Rate</td>
<td style="text-align: center;">$1 \mathrm{e}-4$</td>
</tr>
</tbody>
</table>
<p>Table 5: Source DQN(Q- $\lambda$ ) Algorithm Hyperparameters for Watermaze.</p>
<h1>N RL ${ }^{2}$ HYPERPARAMETERS</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RL Algorithm</td>
<td style="text-align: center;">A3C</td>
</tr>
<tr>
<td style="text-align: left;">Learning Rate</td>
<td style="text-align: center;">$3 \mathrm{e}-4$</td>
</tr>
<tr>
<td style="text-align: left;">Batch Size</td>
<td style="text-align: center;">256</td>
</tr>
<tr>
<td style="text-align: left;">Unroll Length</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: left;">LSTM Hidden Dim.</td>
<td style="text-align: center;">256</td>
</tr>
<tr>
<td style="text-align: left;">LSTM Number of Layers</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">Episodes Per Trial</td>
<td style="text-align: center;">10</td>
</tr>
</tbody>
</table>
<p>Table 6: $\mathrm{RL}^{2}$ Hyperparameters used in "Dark" Environments.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RL Algorithm</td>
<td style="text-align: center;">DQN(Q- $\lambda$ )</td>
</tr>
<tr>
<td style="text-align: left;">Learning Rate</td>
<td style="text-align: center;">$1 \mathrm{e}-4$</td>
</tr>
<tr>
<td style="text-align: left;">Batch Size</td>
<td style="text-align: center;">96</td>
</tr>
<tr>
<td style="text-align: left;">Unroll Length</td>
<td style="text-align: center;">40</td>
</tr>
<tr>
<td style="text-align: left;">LSTM Hidden Dim.</td>
<td style="text-align: center;">256</td>
</tr>
<tr>
<td style="text-align: left;">LSTM Number of Layers</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">Episodes Per Trial</td>
<td style="text-align: center;">30</td>
</tr>
</tbody>
</table>
<p>Table 7: $\mathrm{RL}^{2}$ Hyperparameters used in the Watermaze Environment.</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 15: Attention maps for AD from five separate seeds. White and gray colors correspond to low and high attention. Red and blue colors indicate that those transitions correspond to an episode restart and a positive reward token, respectively. The left column plots attention for an AD transformer after 200 time-steps of evaluation (when the context is initially filled). The right column plots attention after 1900 steps ( 38 episodes) of evaluation. Each episode has a length of 50 steps. From these patterns, it is evident that AD attends to tokens across several episodes to predict its next action.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Indeed, current state-of-the-art RL algorithms such as MuZero (Schrittwieser et al., 2019) and Muesli (Hessel et al., 2021) rely on distributed actors.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
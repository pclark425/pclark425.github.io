<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9408 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9408</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9408</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-164.html">extraction-schema-164</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-277271746</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.17604v1.pdf" target="_blank">OmniScience: A Domain-Specialized LLM for Scientific Reasoning and Discovery</a></p>
                <p><strong>Paper Abstract:</strong> ,</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9408.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9408.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OmniScience Battery Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OmniScience Reasoning model used in a dual-agent battery molecular screening agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An application of the OmniScience Reasoning LLM to rank and grade candidate electrolyte solvent/additive molecules via a generator-reflector agent pipeline with RAG support; outputs numeric grades (1-10) and ranks to prioritize molecules for experimental follow-up.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OmniScience Reasoning (OmniScience)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Domain-adapted and reasoning-distilled LLaMA-3.1-based large reasoning model: LLaMA 3.1 70B weights were domain-adaptively pretrained on ~35B scientific tokens, supervised fine-tuned on 160k+ instruction samples, and further fine-tuned on the s1K-1.1 reasoning distillation traces (derived from DeepSeek-R1) to improve multi-step reasoning. The battery agent uses this model as the generator and GPT-o1 as a reflector, with a RAG pipeline and short-term memory module.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Rank and grade molecules by their suitability/effectiveness as electrolyte solvents or additives for lithium metal batteries (i.e., prioritize candidates likely to be experimentally effective).</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Direct generation of numeric grades (1–10) and textual explanations by the OmniScience Reasoning model (generator); iterative autonomous refinement of grades by a reflector agent (GPT-o1). The pipeline uses retrieval-augmented generation (RAG) to fetch supporting documents and a short-term memory module to maintain context during iterative feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Numeric grades on a 1–10 scale and derived rankings (Mean Grade, Mean Rank, Hits@K). No explicit probabilistic (percentage/log-odds) outputs reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Retrospective ranking evaluation against a curated ground-truth set: 71 known 'good' solvent molecules embedded in a pool of ~700 molecules. Metrics computed: Mean Grade (average model grade for the 71 good molecules), Mean Rank (average ranked position of the 71 good molecules when all 700 are sorted by grade), and Hits@K (fraction of the 71 good molecules appearing in top K = 5,10,20,50). No calibration metrics (e.g., Brier score, calibration plots) or prospective experimental validation reported.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>OmniScience Reasoning substantially outperformed the LLaMA-3.1-70B baseline and OmniScience Chat in this retrospective ranking task: OmniScience Reasoning achieved Mean Grade = 8.0/10, Mean Rank = 81.26, and Hits@50 ≈ 32/50; OmniScience Chat achieved Mean Grade ≈ 7.0/10 and Mean Rank ≈ 153.46; baseline LLaMA 3.1 70B achieved Mean Grade = 6.4/10 and Mean Rank ≈ 166.70 with Hits@50 ≈ 12/50. These numeric improvements indicate stronger prioritization of known good molecules by the reasoning-distilled model.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>The model emits ordinal/graded scores rather than calibrated probabilities; no explicit calibration or uncertainty quantification was reported. Evaluation is retrospective on a curated dataset (71 good molecules among ~700) rather than prospective experimental validation, so real-world predictive value for discovery is unproven. Performance depends on RAG and the external reflector (GPT-o1) — dependencies and potential dataset/bias in retrieval sources are not fully characterized. The paper does not report calibration metrics (e.g., Brier, reliability diagrams) or analyses of overconfidence, false positives/negatives in a discovery context.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared to LLaMA-3.1-70B baseline, OmniScience Chat, and other models tested, OmniScience Reasoning gave substantially higher Mean Grade and much better Mean Rank and Hits@K; e.g., Hits@50 nearly tripled versus LLaMA (32/50 vs 12/50). The paper also notes GPT-o1 was a strong performer across some battery tasks (and is used as the reflector); OmniScience was competitive with larger proprietary reasoning models on unrelated general benchmarks but the ranking experiment compares primarily to the LLaMA baseline and OmniScience Chat.</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Authors propose creating domain-specific reasoning-distillation datasets (battery-specific s1K-style dataset) to better teach domain reasoning strategies, and exploring reinforcement learning on reasoning-distilled models (per DeepSeek guidance) to further improve performance. Also, the paper uses RAG and dual-agent iterative refinement as methods to improve outputs; no post-hoc calibration or explicit uncertainty estimation methods were applied in the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'OmniScience: A Domain-Specialized LLM for Scientific Reasoning and Discovery', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning <em>(Rating: 2)</em></li>
                <li>Gpt4battery: An llm-driven framework for adaptive state of health estimation of raw li-ion batteries <em>(Rating: 2)</em></li>
                <li>Domain-adapted llms for chip design <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9408",
    "paper_id": "paper-277271746",
    "extraction_schema_id": "extraction-schema-164",
    "extracted_data": [
        {
            "name_short": "OmniScience Battery Agent",
            "name_full": "OmniScience Reasoning model used in a dual-agent battery molecular screening agent",
            "brief_description": "An application of the OmniScience Reasoning LLM to rank and grade candidate electrolyte solvent/additive molecules via a generator-reflector agent pipeline with RAG support; outputs numeric grades (1-10) and ranks to prioritize molecules for experimental follow-up.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OmniScience Reasoning (OmniScience)",
            "model_description": "Domain-adapted and reasoning-distilled LLaMA-3.1-based large reasoning model: LLaMA 3.1 70B weights were domain-adaptively pretrained on ~35B scientific tokens, supervised fine-tuned on 160k+ instruction samples, and further fine-tuned on the s1K-1.1 reasoning distillation traces (derived from DeepSeek-R1) to improve multi-step reasoning. The battery agent uses this model as the generator and GPT-o1 as a reflector, with a RAG pipeline and short-term memory module.",
            "model_size": "70B",
            "prediction_target": "Rank and grade molecules by their suitability/effectiveness as electrolyte solvents or additives for lithium metal batteries (i.e., prioritize candidates likely to be experimentally effective).",
            "prediction_method": "Direct generation of numeric grades (1–10) and textual explanations by the OmniScience Reasoning model (generator); iterative autonomous refinement of grades by a reflector agent (GPT-o1). The pipeline uses retrieval-augmented generation (RAG) to fetch supporting documents and a short-term memory module to maintain context during iterative feedback.",
            "probability_format": "Numeric grades on a 1–10 scale and derived rankings (Mean Grade, Mean Rank, Hits@K). No explicit probabilistic (percentage/log-odds) outputs reported.",
            "evaluation_method": "Retrospective ranking evaluation against a curated ground-truth set: 71 known 'good' solvent molecules embedded in a pool of ~700 molecules. Metrics computed: Mean Grade (average model grade for the 71 good molecules), Mean Rank (average ranked position of the 71 good molecules when all 700 are sorted by grade), and Hits@K (fraction of the 71 good molecules appearing in top K = 5,10,20,50). No calibration metrics (e.g., Brier score, calibration plots) or prospective experimental validation reported.",
            "results": "OmniScience Reasoning substantially outperformed the LLaMA-3.1-70B baseline and OmniScience Chat in this retrospective ranking task: OmniScience Reasoning achieved Mean Grade = 8.0/10, Mean Rank = 81.26, and Hits@50 ≈ 32/50; OmniScience Chat achieved Mean Grade ≈ 7.0/10 and Mean Rank ≈ 153.46; baseline LLaMA 3.1 70B achieved Mean Grade = 6.4/10 and Mean Rank ≈ 166.70 with Hits@50 ≈ 12/50. These numeric improvements indicate stronger prioritization of known good molecules by the reasoning-distilled model.",
            "limitations_or_challenges": "The model emits ordinal/graded scores rather than calibrated probabilities; no explicit calibration or uncertainty quantification was reported. Evaluation is retrospective on a curated dataset (71 good molecules among ~700) rather than prospective experimental validation, so real-world predictive value for discovery is unproven. Performance depends on RAG and the external reflector (GPT-o1) — dependencies and potential dataset/bias in retrieval sources are not fully characterized. The paper does not report calibration metrics (e.g., Brier, reliability diagrams) or analyses of overconfidence, false positives/negatives in a discovery context.",
            "comparison_to_baselines": "Compared to LLaMA-3.1-70B baseline, OmniScience Chat, and other models tested, OmniScience Reasoning gave substantially higher Mean Grade and much better Mean Rank and Hits@K; e.g., Hits@50 nearly tripled versus LLaMA (32/50 vs 12/50). The paper also notes GPT-o1 was a strong performer across some battery tasks (and is used as the reflector); OmniScience was competitive with larger proprietary reasoning models on unrelated general benchmarks but the ranking experiment compares primarily to the LLaMA baseline and OmniScience Chat.",
            "methods_for_improvement": "Authors propose creating domain-specific reasoning-distillation datasets (battery-specific s1K-style dataset) to better teach domain reasoning strategies, and exploring reinforcement learning on reasoning-distilled models (per DeepSeek guidance) to further improve performance. Also, the paper uses RAG and dual-agent iterative refinement as methods to improve outputs; no post-hoc calibration or explicit uncertainty estimation methods were applied in the reported experiments.",
            "uuid": "e9408.0",
            "source_info": {
                "paper_title": "OmniScience: A Domain-Specialized LLM for Scientific Reasoning and Discovery",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
            "rating": 2,
            "sanitized_title": "deepseekr1_incentivizing_reasoning_capability_in_llms_via_reinforcement_learning"
        },
        {
            "paper_title": "Gpt4battery: An llm-driven framework for adaptive state of health estimation of raw li-ion batteries",
            "rating": 2,
            "sanitized_title": "gpt4battery_an_llmdriven_framework_for_adaptive_state_of_health_estimation_of_raw_liion_batteries"
        },
        {
            "paper_title": "Domain-adapted llms for chip design",
            "rating": 2,
            "sanitized_title": "domainadapted_llms_for_chip_design"
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 1,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        }
    ],
    "cost": 0.01135625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>OmniScience: A Domain-Specialized LLM for Scientific Reasoning and Discovery
March 21, 2025</p>
<p>Vignesh Prabhakar 
MdAmirul Islam 
Adam Atanas 
Kai Liu kai.liu@ses.ai 
Ses Ai </p>
<p>SES AI</p>
<p>SES AI</p>
<p>SES AI Yao-Ting Wang SES AI Joah Han SES AI Aastha Jhunjhunwala NVIDIA Rucha Apte NVIDIA Robert Clark NVIDIA Kang Xu SES AI Zihan Wang NVIDIA</p>
<p>OmniScience: A Domain-Specialized LLM for Scientific Reasoning and Discovery
March 21, 2025ED5F79656D11DD01AE8ACD52A392F08FarXiv:2503.17604v4[cs.AI]
Large Language Models (LLMs) have demonstrated remarkable potential in advancing scientific knowledge and addressing complex challenges.In this work, we introduce OmniScience, a specialized large reasoning model for general science, developed through three key components:(1) domain adaptive pretraining on a carefully curated corpus of scientific literature, (2) instruction tuning on a specialized dataset to guide the model in following domain-specific tasks, and (3) reasoning-based knowledge distillation through fine-tuning to significantly enhance its ability to generate contextually relevant and logically sound responses.We demonstrate the versatility of OmniScience by developing a battery agent that efficiently ranks molecules as potential electrolyte solvents or additives.Comprehensive evaluations reveal that OmniScience is competitive with state-of-the-art large reasoning models on the GPQA Diamond and domain-specific battery benchmarks, while outperforming all public reasoning and non-reasoning models with similar parameter counts.We further demonstrate via ablation experiments that domain adaptive pretraining and reasoning-based knowledge distillation are critical to attain our performance levels, across benchmarks.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) (OpenAI, 2022;Dubey et al., 2024;Team et al., 2023;Anthropic, 2023;Alayrac et al., 2022;Bai et al., 2023;Guo et al., 2025;OpenAI, 2024;Liu et al., 2025;OpenAI, 2025) have demonstrated widespread success across diverse scientific fields (Taylor et al., 2022;Feng et al., 2024;Bolton et al., 2024;Chithrananda et al., 2020;Zhang et al., 2025Zhang et al., , 2024;;Tang et al., 2025), showcasing remarkable capabilities in extracting, processing, and synthesizing complex information.Their versatility allows them to handle a wide range of tasks, from summarizing intricate research papers and answering specialized queries to generating innovative hypotheses.In the context of general science, science focused LLMs offer significant potential for accelerating discovery processes.For example, in battery research, a science LLM could rapidly screen large molecular datasets, explore expansive chemical spaces, and identify promising candidate molecules for use as solvents or additives.Such capabilities not only streamline research workflows but also pave the way for breakthrough innovations across multiple scientific disciplines.However, developing a highly specialized LLM for general science poses unique challenges.General-purpose foundation models (Dubey et al., 2024;OpenAI, 2022;Team et al., 2023;Alayrac et al., 2022;Bai et al., 2023;Guo et al., 2025) often lack the domain-specific vocabulary and contextual understanding needed to tackle complex scientific topics, such as molecular structures, electrochemical properties, experimental data, and gene expression patterns.While domain-specific language models like ChipNeMo (Liu et al., 2023) for chip design and BloombergGPT (Wu et al., 2023) for financial data have shown the benefits of specialized adaptation, similar efforts in general science remain limited (Taylor et al., 2022;Feng et al., 2024;Zhang et al., 2024;Frey et al., 2022;Li &amp; Jiang, 2021;Yunusoglu et al., 2025;Zhang et al., 2025;Tang et al., 2025).This gap highlights the need for a dedicated general science LLM to address the specific demands of the field.</p>
<p>Training an LLM from scratch using only domain-specific data is both computationally intensive and costly.A more practical and efficient approach is Domain Adaptive Pre-Training (DAPT) (Gururangan et al., 2020), which involves further training a pre-existing foundation model on a targeted dataset from a specific domain (Shen et al., 2024;dos Santos Junior et al., 2024;Liu et al., 2023).This method allows researchers to leverage the broad knowledge of a general-purpose LLM while enhancing its expertise in a specific domain (e.g., general science).By continuously pretraining on in-domain texts, DAPT achieves significant improvements in domain-specific tasks with only a fraction of the computational resources required for full-scale training.This approach not only retains the model's general language understanding but also optimizes its performance for domain-related applications.</p>
<p>In this work, we present OmniScience (see Fig. 1), an adaptation of the LLaMA 3.1 70B model (Dubey et al., 2024) specifically tailored for science exploration.Our approach leverages DAPT on a carefully curated dataset that includes peer-reviewed articles, arXiv papers, journals, and textbooks covering general science and electrochemistry.To ensure high-quality input, we develop a robust data-processing pipeline that cleans and organizes the text effectively.We then perform DAPT on this domain-specific corpus, enabling the model to acquire a deep understanding of scientific language and concepts.</p>
<p>General-purpose LLMs widely adopt instruction-based supervised fine-tuning (OpenAI, 2022;Dubey et al., 2024) as a key strategy to ensure they accurately follow user instructions.In line with this trend, we refine our base OmniScience through instruction tuning after domain adaptive pretraining, resulting in our OmniScience Chat model.This supervised process incorporates both science-specific task instructions and general chat instructions, enhancing the model's ability to generate accurate and contextually relevant responses across a broad range of queries and domainspecific prompts.</p>
<p>While domain adaptive pretraining on scientific literature and instruction-based fine-tuning enhance the model's ability to address general and domain-specific questions, they remain insufficient for tackling complex scientific problems requiring multi-step reasoning, contextual synthesis, or domain knowledge integration.To address these limitations, recent work (Team et al., 2025;Guo et al., 2025;OpenAI, 2024;Liu et al., 2025;OpenAI, 2025) has explored enabling models to engage in extended internal thinking, where it performs iterative reasoning processes such as hypothesis generation, chain-of-thought analysis, and self-correction, before generating responses.This technique has significantly boosted reasoning capabilities.These large reasoning models (e.g., GPT-o1, o3-mini, Grok 3, DeepSeek-R1, Gemini 2.0 flash thinking) have demonstrated impressive performance on complex tasks and questions.To leverage this insight, we add an additional knowledge distillation stage on top of the OmniScience Chat model using the s1K-1.1 dataset (Muennighoff et al., 2025), originally derived from DeepSeek-R1 (Guo et al., 2025) reasoning traces.This reasoning-based knowledge distillation in the form of fine-tuning bridges the gap between basic instruction alignment and advanced inferential performance, enabling our model to achieve competitive results on science reasoning tasks.OmniScience achieves 0.72 on the GPQA-Diamond benchmark, which is state-of-the-art performance among similarly-sized models.</p>
<p>The primary objective of OmniScience is to demonstrate its adaptability to general scientific tasks.In this work, we apply OmniScience to battery-related applications, such as ranking molecules and explaining their suitability as electrolyte solvents or additives, to support the broader battery research community.By integrating large scale language modeling with specialized scientific knowledge, we provide a powerful tool that accelerates the discovery of new electrolytes and streamlines molecular design processes.This work bridges the gap between general purpose LLMs and the specific needs of general science, offering a pathway to more efficient and targeted research in this critical field.</p>
<p>Related work</p>
<p>Recent advancements in large language models (LLMs) have accelerated growing interest in developing domain-specific LLMs, driven by the availability of vast public and proprietary datasets.Early efforts include models, such as BloombergGPT (Wu et al., 2023) for finance, BioMedLLM (Bolton et al., 2024) for biomedical applications, and Galactica (Taylor et al., 2022) for general scientific research.These models were trained from scratch on raw, domain-specific datasets, and although effective, they require billions of tokens and substantial computational resources to achieve acceptable performance.In parallel, domain-specific models in the chemistry space have emerged, including ChemBERTa (Chithrananda et al., 2020), MolBERT (Li &amp; Jiang, 2021), ChemGPT (Frey et al., 2022), and ChemLLM (Zhang et al., 2024).These models leveraged transformer architectures and specialized pretraining to capture chemical knowledge, enabling tasks like property prediction and reaction modeling, though they too often require extensive computational budgets.Despite these promising advances, relatively little effort has been devoted to developing a science-focused LLM that can be easily adapted to specific scientific tasks.Our work seeks to address this gap by creating a model that not only excels in general scientific reasoning but can also be tailored to specialized applications, such as battery research.</p>
<p>While our work focuses mostly on training a science model checkpoint, we recognize that recent studies (Liu et al., 2023;Lewis et al., 2020;Borgeaud et al., 2021;Izacard et al., 2022) have shown that integrating Retrieval Augmented Generation (RAG) techniques can significantly boost performance.For example, ChipNeMo (Liu et al., 2023) used RAG to dynamically fetch up-to-date information from external sources, enabling even smaller models to outperform larger ones that lack this capability.Various retrieval methods have been explored, highlighting RAG's versatility in enhancing LLM outputs.In general, retrieval-augmented models address the limitations of relying solely on model weights.Models like RAG (Lewis et al., 2020), RETRO (Borgeaud et al., 2021), and Atlas (Izacard et al., 2022) achieved lower capacity requirements by leveraging external retrieval mechanisms, although they do require additional infrastructure.In this work, while we mostly focus on exploring the limits of model weights alone, we also demonstrate the power of our model in a full agentic setup with RAG on the real-world scientific task of ranking molecules by electrolyte suitability.</p>
<p>OmniScience: Domain Adaptive Reasoning Model</p>
<p>We describe our three-step training pipeline for building OmniScience.First, we perform continuous pretraining (Sec.3.1) on a large corpus of scientific literature to provide the model a solid foundation in domain-specific language and concepts.Next, we perform supervised fine-tuning (Sec.3.2) using a carefully curated instruction dataset, which aligns the model's responses with high-quality, contextually relevant information.Finally, we apply an extra fine-tuning step (Sec.3.3) on a high-quality reasoning dataset to further improve the model's reasoning ability to handle complex scientific tasks.</p>
<p>Domain Adaptive Pretraining</p>
<p>Here, we describe our methodology for domain adaptive pretraining (see Fig. 10 in the Appendix for more details).We first discuss the composition and preprocessing of the pretraining data, curated from diverse scientific sources for broad domain coverage.Next, we outline the model architecture.Finally, we discuss training details, including hyper-parameters, optimization strategies.</p>
<p>Pretraining Data.Our pretraining corpus consists of 35 billion tokens collected from a diverse array of scientific sources, including large number of peer-reviewed articles, as well as preprints from arXiv (Clement et al., 2019), ChemRxiv (Kiessling et al., 2016), PubChem (Kim et al., 2016), Semantic Scholar (Lo et al., 2020) and other such open research platforms.Additionally, the dataset integrates full text from academic textbooks.By combining natural language data from research papers and textbooks with structured scientific information, we capture a broad and comprehensive spectrum of scientific knowledge.This diversified data collection approach ensures a robust and representative foundation for modeling and analysis across scientific domains.Additional details about the domain adaptive pretraining data are provided in Sec.A.2 of the Appendix.</p>
<p>Data Processing.We begin our data processing workflow by collecting PDF documents from multiple sources and converting them to plain text using Unstructured's PDF to text extraction library (Unstructured.IO, 2022).Next, we apply fuzzy de-duplication using MinHash (Broder, 1997) and Locality Sensitive Hashing (Gionis et al., 1999) across the entire dataset to remove near duplicate documents, ensuring uniqueness in the dataset.We then employ the NeMo Curator (Kuchaiev et al., 2019) to perform a series of preprocessing steps, such as heuristic filtering to remove additional low-quality documents, outliers, and documents with excessive repetitions.Finally, the cleaned and curated text is tokenized using the base LLaMA 3.1 70B tokenizer (Dubey et al., 2024), transforming the processed data into a format suitable for domain adaptive pretraining.</p>
<p>Model Architecture.We conduct Domain Adaptive Pretraining (DAPT) on large-scale pretrained foundation models, specifically leveraging the LLaMA 3.1 architecture (Dubey et al., 2024) with 70B parameter configurations.The domain adapted model is initialized using the pretrained weights of LLaMA 3.1 70B base model, and the resulting model is collectively referred to as OmniScience base model.For efficient training, we utilize the NVIDIA NeMo framework (Guillaume &amp; Rougemont, 2006), incorporating state-of-the-art optimization techniques such as tensor parallelism (Shoeybi et al., 2019) and flash attention (Dao et al., 2022) to enhance computational performance and scalability.We also use context parallelism in NeMo framework to boost memory efficiency as we scale context length, enabling training on long sequences up to 8K tokens.</p>
<p>Training Details.We use a learning rate of 1 × 10 −5 and the AdamW optimizer (Kingma &amp; Ba, 2014), with a weight decay of 0.0001.We set the global batch size to 64, and use a input sequence length of 8,192 tokens, resulting in a total batch size of 524288 tokens in each forward pass.So, to train on all of the 35 billion tokens of the dataset, we train our model for 66,000 steps, which is roughly one full pass through the dataset.To ensure training convergence, we perform gradient accumulation in bfloat16 precision during the backward pass.The training takes around 6 days on 128 NVIDIA H100 GPUs.Additional details about the training procedure are provided in Sec.A.1 of the Appendix.</p>
<p>Model Alignment with Supervised Fine-Tuning (SFT)</p>
<p>SFT Data Generation.We randomly sample 50,000 papers from our domain adaptive pretraining dataset in order to synthetically generate instruction dataset for SFT.Each paper is processed using GPT-4o-mini API, which is prompted to produce four distinct task instructions (question answering, summarization, reading comprehension, and multiple choice questions).This process yields 200,000 instruction samples derived from scientific literature.For training, we retain 160,000 samples and reserve 40,000 as a held-out test set for evaluation.In addition, we incorporate 90,000 general chat instruction samples from the publicly available daring-anteater dataset (Wang et al., 2024), resulting in a final SFT dataset comprising 250,000 samples.This dataset provides a robust resource for aligning our model with both domain-specific and general instructional tasks.</p>
<p>SFT Training.Following Domain Adaptive Pretraining (DAPT), we further align our model through supervised fine tuning (SFT) using the NeMo framework (see Fig. 12 in the Appendix for illustration).We retain the same hyperparameter settings as in DAPT, reducing only the learning rate to 1 × 10 −6 .Using a standard auto-regressive objective, the model learns to predict the next token in a sequence.We fine tune our OmniScience base model for 1400 steps with a global batch size of 64 on our SFT training set, which comprises both domain-specific instructional data and general chat instructions.The resulting model is referred to as the OmniScience Chat model.The SFT training takes around 32 hours on 128 NVIDIA H100 GPUs.These hyperparameter settings and the use of the NeMo framework ensure stable convergence, reduce overfitting, and improve the model's ability to produce accurate, contextually relevant responses for both general and domain-specific queries.</p>
<p>Model Alignment with High-Quality Reasoning Dataset</p>
<p>While Domain-Adaptive Pretraining (DAPT) on scientific literature and instruction-based fine tuning enhance the model's capacity to address general and domain-specific queries, it still lacks the ability to reason about complex scientific problems.To address this, we introduce an additional fine tuning step through reasoning-based distillation using the publicly available s1K dataset (Muennighoff et al., 2025).This extra phase bridges the gap between domain adaptation and advanced reasoning, enabling the model to produce contextually grounded responses with improved inferential capabilities.</p>
<p>s1K Dataset Collection.The s1K dataset, as described in (Muennighoff et al., 2025), is a carefully curated subset derived from an initial collection of 59,000 samples.The refinement process involved three key stages.First, low quality samples, such as those with formatting issues, were removed, reducing the dataset to 54,000 samples.Next, in the difficulty filtering stage, each sample was evaluated using two large language models, Qwen2.5 7B Instruct (Yang et al., 2024) and Qwen2.5 32B Instruct (Yang et al., 2024), with correctness verified by Claude 3.5 Sonnet.</p>
<p>Only questions that both models answered incorrectly were retained, narrowing the dataset to approximately 25,000 samples.Additionally, in the diversity filtering stage, samples were clustered into thematic categories (for example, math and science) using an LLM, and uniform sampling was applied with a preference for samples that exhibited longer reasoning traces in order to capture complexity.Finally, the s1K samples were decontaminated against several popular benchmarks, which crucially include the GPQA Diamond benchmark we use extensively in this paper.Notably, we use the s1K-1.1 dataset, which was generated using DeepSeek-R1 traces rather than Gemini traces, further enhancing its quality for model alignment.</p>
<p>Fine-tuning with s1K Reasoning Dataset.We fine tune our OmniScience Chat model on the s1K-1.1 dataset (Muennighoff et al., 2025) to obtain OmniScience Reasoning model.We preprocess the s1K dataset to align with NeMo training framework.As shown in (Muennighoff et al., 2025), longer context windows yield superior results, likely due to the s1K training samples containing long reasoning traces that would not fit in a shorter context window.Therefore, during training and inference, we use a longer context window of 16k tokens by reducing the global batch size.This context window fits over 94% of all s1K reasoning traces in their entirety.We use the following fine-tuning hyper-parameters: We fine-tune the OmniScience Chat model for 5 epochs with a batch size of 2, totaling 2500 gradient steps.The model is trained in bfloat16 precision with a learning rate of 1 × 10 −5 , which is warmed up linearly for the first 128 steps and then decayed to zero following a cosine schedule.We employ the AdamW optimizer with β 1 set to 0.9, β 2 to 0.95, and a weight decay of 0.0001.The distillation training takes around 12 hours on 64 NVIDIA H100 GPUs.This configuration ensures that our models effectively leverage the dataset's complexity and diversity, resulting in more accurate and contextually grounded responses.Since the s1K dataset is derived from DeepSeek-R1 traces, fine tuning on it naturally transfers DeepSeek-R1's reasoning capacity into our compact OmniScience Chat model.This enables our smaller model to gain strong reasoning abilities with fewer parameters.Our experiments show that this reasoning distillation based fine-tuning step not only improves factual accuracy but also enhances the model's ability to combine and explain complex ideas, marking a clear improvement over models that rely solely on DAPT and task-specific fine tuning.</p>
<p>Experiments</p>
<p>In this section, we present a comprehensive evaluation of our model's performance on both public (Sec.4.1) and domain-specific (Sec.4.2) benchmarks.We benchmark against state-of-the-art models using our OmniScience Reasoning model and rigorously assess its capabilities across diverse tasks and datasets.Additionally, we perform ablation studies (Sec.4.3) to demonstrate the necessity of our continuous pretraining and reasoning alignment steps.Finally, we show the adaptability of our OmniScience to solve battery related tasks in Sec.4.4.</p>
<p>Results on Public Benchmarks</p>
<p>In Table 1, we compare our results with recent state-of-the-art reasoning and non-reasoning models on GPQA Diamond benchmark.GPQA Diamond (Rein et al., 2024) is particularly relevant for our science-focused LLM, as our post-training was specifically tailored to domain-specific scientific tasks.Our OmniScience Reasoning model achieves a score of 0.720, narrowly surpassing DeepSeek-R1 (0.715), despite having only 10% of DeepSeek-R1's parameters and not requiring pretraining from scratch.Notably, our model remains competitive with much larger reasoning models, such as Claude 3.7 sonnet (0.782), Grok 3 (0.802), and GPT-o1 (0.757).Our model also outperforms most non-reasoning models, even those with huge sets of parameters such as Llama-3.1-405B,DeepSeek-v3, and GPT-4.5.We believe that further scaling of our architecture will yield even greater performance gains.Additionally, Fig. 2 compares the GPQA Diamond scores of our OmniScience Reasoning model with several top performing models in the 10 to 100B parameter range.Among comparably-sized models, OmniScience Reasoning model surpasses well-established baselines, including DeepSeek-R1 distill variants.Notably, the DeepSeek-R1 Distill LLaMA 70B model uses the LLaMA-3.1-70Binstruct model, yet our domain-adaptive approach still achieves a higher score (0.720 vs. 0.652) despite using the base Llama-3.1-70Bmodel as a starting point.This substantial improvement underscores the effectiveness of continuous pretraining and reasoning-based distillation in elevating the model's performance beyond what can be achieved through instruction alignment alone.OmniScience Reasoning performs on par with the strongest models, which also demonstrates its robust generalization capabilities beyond the scientific domain.In addition to GPQA Diamond, we benchmark OmniScience Reasoning on four widely used public benchmarks that assess general language understanding and reasoning, namely MMLU (Hendrycks et al., 2020), Winogrande (Sakaguchi et al., 2021), Hellaswag (Zellers et al., 2019), and ARC-E (Clark et al., 2018).We include these public benchmarks to demonstrate that OmniScience, while optimized for scientific reasoning, retains strong generalization capabilities.Table 2 summarizes the comparative performance on these benchmarks, and Figure 3 visualizes these results.Both the OmniScience Chat and Reasoning models consistently outperform the LLaMA-3.1-70Bbaseline, and OmniScience Reasoning remains competitive with significantly larger proprietary models, highlighting the effectiveness of our domain-adaptive and reasoning-based alignment strategies.
L L a M A 3 .1 7 0 B Q W -3 2 B P r e v ie w p h i-4 s 1 -3 2 B o 1 -m in i D S -R 1 D is t il l Q w e n 3 2 B D S -R 1 D is t il l L L a M A</p>
<p>Results on Domain-Specific Benchmarks</p>
<p>We further evaluate our model using an SFT held-out test set (as detailed in Section 3.2) and an internally curated battery-specific reasoning benchmark.To ensure comprehensive comparison, we extend our analysis to large reasoning models with publicly accessible APIs, including GPT-o1, LLaMA 3.1 70B, Claude 3.7 Sonnet, and Gemini.As shown in Table 3 and Figure 4, our OmniScience Reasoning model achieves robust performance across tasks such as Battery Q/A, multiple-choice questions (MCQ), reading comprehension, summarization, and reasoning, outperforming all baseline models except GPT-o1.Notably, while GPT-o1 (used to generate task instruction data for supervised fine-tuning) remains the strongest performer, our model achieves competitive results despite its significantly smaller parameter size and lower computational training costs.These findings highlight the efficacy of domain-adaptive pretraining and reasoning-focused distillation for specialized task performance.</p>
<p>Ablation Studies</p>
<p>To evaluate the impact of domain adaptive pretraining and supervised fine-tuning on our model's performance, we isolate the reasoning distillation-based fine-tuning step.As discussed in Sec.  4 show that both the CPT and Chat Distillation models outperform the LLaMA 70B Distillation variant on the GPQA Diamond benchmark.Notably, on tasks requiring complex scientific reasoning, such as GPQA Diamond, the CPT and Chat Distillation models achieve a score of 0.72 compared to 0.58 for the LLaMA 70B Distillation model, highlighting the importance of continuous pretraining and supervised fine tuning for effective domain specific reasoning.</p>
<p>In addition to the public GPQA Diamond benchmark, we evaluate the models on battery specific tasks (see Fig. 5), where the CPT and Chat Distillation models again demonstrate superior performance.For instance, the CPT distillation model scores 86% on the battery reasoning benchmark compared to 78% for the LLaMA 70B Distillation model.Although the CPT Distillation model, which relies solely on continuous pretraining, generally achieves superior performance, there are cases where the Chat Distillation model, our OmniScience Reasoning model, outperforms CPT Distillation (as seen in the Battery Q/A task) or achieves similar performance (as seen on the GPQA Diamond benchmark).</p>
<p>These results underscore the critical role of domain adaptive pretraining in achieving superior performance across both general and specialized scientific benchmarks.Notably, we observed consistent performance gaps between the OmniScience Chat and OmniScience Reasoning models, and the CPT/Chat Distillation and LLaMA 70B Distillation models.This highlights the necessity of combining domain adaptive pretraining with reasoning-based model alignment to optimize performance in scientific reasoning tasks.</p>
<p>Battery Agent for Molecular Screening</p>
<p>In this section, we demonstrate the adaptability of our OmniScience Reasoning model for batteryspecific tasks.We develop a battery agent using our OmniScience Reasoning model to rank molecules as potential electrolyte solvents or additives.</p>
<p>Battery Agent Framework.Figure 6 illustrates our dual-agent framework for grading and ranking molecules that are effective as electrolyte solvents or additives in Lithium Metal Batteries.Reasoning: Containing sulfone groups, this solvent boasts exceptional thermal and electrochemical stability.The presence of cyclic structures and multiple hydroxyl groups enhances its solvation properties, making it highly effective and safe for use in lithium metal batteries.</p>
<p>Solvent Molecule</p>
<p>Figure 6: Dual-agent framework for ranking and explaining molecule efficacy as electrolyte solvents or additives.The Generator (OmniScience Reasoning) proposes initial outputs, while the Reflector (GPT-o1) refines and provides feedback autonomously.</p>
<p>Generator Reflector Mean Grade Mean Rank Hits@5 Hits@10 Hits@20 Hits@50 LLaMA 3. Table 5: Performance of various battery agent configurations for the ranking and selection of effective electrolyte solvents.Evaluation metrics: Mean Gradeaverage of the grades assigned to the 71 well-known/good solvent molecules from the full set of 700 molecule, Mean Rankwhen the full list of 700 molecules is ranked by grade, the average position of the 71 good molecules in this full ranked list, Hits@K (k=5, 10, 20, 50)-the fraction of molecules within the top-ranked 5, 10, 20, and 50 predictions (respectively) that are in the set of good molecules.</p>
<p>The generator agent (OmniScience Reasoning) proposes initial grades, while the reflector agent (GPT-o1) autonomously refines the output through iterative feedback.The system integrates a Retrieval-Augmented Generation (RAG) pipeline, which interacts with textbooks and scientific resources, alongside a short-time memory module to maintain context and optimize decision-making.This interactive approach ensures robust, scientifically grounded evaluations by leveraging the strengths of both agents and dynamic knowledge integration.Molecular Ranking Results using our Battery Agent Framework.We evaluate the performance of our battery agent (see Table 5) by varying the generator agent while keeping the reflector agent fixed as GPT-o1.The evaluation is conducted on a set of 71 well-known solvent molecules, selected from a diverse pool of approximately 700 molecules that include good, bad, and reasonable electrolyte solvents.We can then sort all 700 molecules by their LLM-assigned grades to assign a unique rank for each molecule.We then compute three key metrics for these 71 molecules to report the quality of our ranking (see Sec. A.3 in the Appendix for further details).Table 5 compares the performance of the OmniScience Chat and OmniScience Reasoning models with the baseline LLaMA 3.1 70B.While the Chat model already outperforms LLaMA, improving the Mean Grade of good-quality molecules from 6.4/10 to 7.0/10 and lowering the Mean Rank of good molecules from 166.70 to 153.46, the Reasoning model delivers an even greater boost.In particular, the OmniScience Reasoning model achieves a Mean Grade of 8/10 and a Mean Rank of 81.26, with average Hits@50 reaching 32/50, nearly triple LLaMA's 12/50.These results highlight the advanced capabilities of our reasoning model in accurately ranking and selecting molecules, demonstrating its superior effectiveness over the baseline.</p>
<p>Conclusion and Future Work</p>
<p>We introduce OmniScience, a 70B large reasoning model that achieves state-of-the-art performance on scientific tasks among models in its size category.We demonstrate a compute-efficient training strategy to convert a base model into a highly-performing domain-specific model.Our training paradigm combines domain adaptive pretraining with reasoning-based distillation via supervised fine-tuning on a dataset curated from a much larger reasoning model.Our results show that this approach significantly outperforms the base LLaMA model with only about 1% additional pretraining compute cost.We also demonstrate that this combination of post-training techniques is essential, as our approach substantially outperforms using either domain-adaptive pretraining or reasoning-based distillation on their own.We demonstrated that our OmniScience Reasoning model outperforms nearly all non-reasoning models and attains competitive results with much larger state-of-the-art reasoning models on the GPQA-Diamond benchmark.Moreover, it exceeds the performance of much larger reasoning and non-reasoning models on battery-related tasks.</p>
<p>In future work, we will focus on further refining OmniScience using domain-specific reasoning distillation to continue advancing its capabilities in specialized scientific applications.Specifically, the s1K-1.1 dataset we are currently using for reasoning distillation is a general reasoning dataset spanning many fields of math and science.We hypothesize that creating a similar dataset of questions in the battery domain will better enable our model to learn proper reasoning strategies for problems specific to that domain.Additionally, DeepSeek mentioned that applying reinforcement learning to reasoning-distilled models can yield further performance improvements (Guo et al., 2025).We expect this to hold true for domain-specific tasks as well, and aim to test it via a domain-specific reinforcement learning dataset.</p>
<p>Our hope is to demonstrate the possibility of cost-efficient training of domain-expert mediumsized (&lt;100B parameter) LLMs with state-of-the-art performance in their domain.This work is a step towards realizing that goal, and potentially moving towards an era of highly-customizable specialist LLMs with domain expertise.</p>
<p>A Appendix</p>
<p>A.1 OmniScience Training Details</p>
<p>Domain Adaptive Pretraining.Figure 7 shows the training and validation loss curves during domain-adaptive continuous pretraining of OmniScience.The training loss declines rapidly in the initial steps, indicating that the model quickly learns domain-specific knowledge.Over the course of training, the loss continues to decrease and eventually stabilizes, suggesting effective convergence without notable overfitting.The validation loss follows a similar downward trajectory, demonstrating that the model generalizes well to unseen data.Collectively, these results highlight the effectiveness of domain-adaptive pretraining in enabling the model to learn specialized scientific content while retaining robust performance on validation sets.</p>
<p>SFT Training.Figure 8 displays the training and validation loss curves during our supervised fine tuning stage.The training loss drops sharply in the early steps, indicating that the model quickly adjusts to the instruction data.As training progresses, the loss continues to decrease and eventually stabilizes, suggesting effective convergence without substantial overfitting.Meanwhile, the validation loss follows a similar pattern, demonstrating strong generalization to unseen samples.</p>
<p>Reasoning-based Knowledge Distillation Training.Figure 9 presents the training loss curve observed during our reasoning-based knowledge distillation phase on the s1K dataset.The training loss begins at a relatively high value and drops at a steady pace, showing that the model quickly adapts to the high-quality s1K data.As training proceeds, the loss continues to decrease and ultimately approaches near zero, indicating effective convergence.We note that owing to the very small size of this dataset, we chose to allocate the entire dataset as training data, so our validation loss measurements are performed on a copy of the training dataset and are therefore not independent for this phase of training.This decision follows the example of (Muennighoff et al., 2025).These results underscore the success of our reasoning-based distillation approach in refining the model's performance.</p>
<p>A.2 Domain Adaptive Data Collection</p>
<p>Table 6 further provides an overview of the diverse data sources used to compile our domain-adaptive pretraining corpus, which totals 35 billion tokens.We collect documents from peer-reviewed journals, arXiv preprints, ChemRxiv, open research platforms, PubChem, academic books, and PLOS articles.This inclusive approach ensures broad coverage of scientific domains, ranging from fundamental chemistry and material science to interdisciplinary research.By drawing on such a variety of literature, we provide the model with a comprehensive understanding of scientific terminology, methodologies, and contextual nuances, ultimately enhancing its ability to handle complex, domainspecific tasks.The figure illustrates our data collection, curation, and domain adaptive pretraining process.We gather raw papers, textbooks, and preprints, apply de-duplication and heuristic filtering, then tokenize the curated dataset using the LLaMA 3.1 70B tokenizer.Finally, the domain adaptive pretraining is carried out on a multi-node system using the NeMo framework, resulting in the OmniScience base model.We randomly sample 50,000 preprocessed documents from our pretraining corpus and pass them to the GPT-4o-mini API, which is prompted to produce four distinct task instructions: question answering, summarization, reading comprehension, and multiple choice questions.In addition, we incorporate 90,000 chat instruction samples from the Daring Anteater dataset, yielding a combined SFT dataset of 250,000 samples.This dataset is tokenized using the LLaMA 3.1 70B tokenizer, and the supervised fine tuning is performed on a multi node system using the NeMo framework, resulting in the final OmniScience Chat model.</p>
<p>A.3 Evaluation Metrics</p>
<p>Mean Grade: The Mean Grade is the average of the grades assigned to each solvent molecule in the ranked list.Grades are based on a predefined scale (e.g., 1 to 10 in our case, where 1 is the Energy Storage Calculation: The energy stored in the anode is proportional to the specific capacities of silicon (Si) and graphite, and the mass of each component.For a 12% Si anode:</p>
<p>• Silicon Mass (m Si ) = 12% of total mass (m total ) • Graphite Mass (m graphite ) = 88% of total mass (m total ) • Specific Capacity of Si (SC Si ) = 4200 mAh/g • Specific Capacity of Graphite (SC graphite ) = 372 mAh/g • Voltage (V ) = 3.7 V (typical for Li-ion batteries)
Energy (E): E = m Si × SC Si + m graphite × SC graphite × V E = 0.12 × 4200 + 0.88 × 372 × 3.7 E = 3081.12 Wh
Mass and Volume Relationship:</p>
<p>• Total Mass (m total ) = m Si + m graphite = 0.12 m total + 0.88 m total = m total (redundant).</p>
<p>• Total Volume (V total ) = V Si + V graphite = 0.12 m total 2.33 + 0.88 m total 2.09 ≈ 0.4725 m total .</p>
<p>Gravimetric Energy Density (GED):
GED = E m total GED = 3081.
12 m total = 320 Wh/kg → m total = 3081.12320 ≈ 9.6325.</p>
<p>Volumetric Energy Density (VED):
V ED = E V total V ED =
3081.12 5.1352 ≈ 600 Wh/L.</p>
<p>Calculating GED and VED for 30% Si Anode: Assumptions:</p>
<p>1.The anode's mass and volume are the same as the current 12% Si anode (unrealistic but required for calculation as per problem statement).2. Silicon content is 30% by mass, so:
• m Si = 0.3 × m total • m graphite = 0.7 × m total
Energy (E 30 ): 1.The anode's mass and volume are the same as the current 12% Si anode.
E 30 = m Si × SC Si + m graphite × SC graphite × V E 30 =
2. Silicon content is 100% by mass.</p>
<p>Energy (E Si ):
E Si = 1 × 4200 × 3.7 = 15,720 Wh
Gravimetric Energy Density (GED Si ):
GED Si = E Si m total GED Si = 15,720 9.6325 ≈ 1,626 Wh/kg
Volumetric Energy Density (V ED Si ):
V ED Si = E Si V total
V ED Si = 15,720 5.1352 ≈ 3,050 Wh/L</p>
<p>Conclusion:</p>
<p>The expected GED and VED values for a 30% Si-content anode are approximately 582 Wh/kg and 294 Wh/L, respectively.For a pure Si anode, they are about 1,626 Wh/kg and 3,050 Wh/L.However, these values are likely overestimated because they assume the same mass and volume as the current 12% Si anode, which is unrealistic for higher Si contents.</p>
<p>DETAILED THINKING</p>
<p>Okay, so I need to calculate the expected gravimetric and volumetric energy densities for lithiumion batteries (LIBs) with two different anode materials: one with 30% silicon (Si) content in a Si/graphite hybrid and the other with pure silicon.The current 12% Si-content anode achieves 320 Wh/kg and 600 Wh/L.Let me start by recalling what these terms mean.</p>
<p>Gravimetric energy density (GED) is the amount of energy stored per unit mass of the battery, usually in Wh/kg.Volumetric energy density (VED) is the energy per unit volume, in Wh/L.Both are important metrics for evaluating battery performance, especially when considering the weight and size constraints of portable devices.</p>
<p>First, let me note that the 12% Si-content anode is a hybrid of silicon and graphite.The performance of such a hybrid anode is likely due to the combination of the high theoretical specific capacity of silicon (around 4200 mAh/g) and the good conductivity and stability of graphite.However, increasing the silicon content might affect the overall density and volume.Wait, but the question is about replacing the 12% Si anode with ones that have 30% Si or pure Si.It doesn't specify whether the anode is a hybrid or just silicon.Hmm.</p>
<p>Wait, the question says "LIBs equipped with 30% Si-content Si/graphite hybrid anodes and those with pure Si anodes."Wait, but the original anode is a Si/graphite hybrid with 12% Si.So the user is asking to compare two scenarios: one where the anode is a hybrid with 30% Si and the other where it's pure Si.Got it.</p>
<p>So I need to take the current performance (320 Wh/kg and 600 Wh/L) for the 12% Si hybrid and estimate what the GED and VED would be for a 30% Si hybrid and a pure Si anode, given that the current 12% Si hybrid is a combination of Si and graphite.The challenge here is to figure out how the energy density (both gravimetric and volumetric) changes with different Si contents and different anode materials.</p>
<p>Let me start by breaking down the current 12% Si hybrid anode.Let's assume that the anode is a physical mixture of silicon and graphite.The performance of a battery anode is usually determined by its active material, which in this case is a combination of Si and graphite.The mass and volume of the anode will depend on the amounts of Si and graphite.However, the energy density (both gravimetric and volumetric) is calculated based on the total mass and volume of the anode plus any other components, but I think the problem might be simplifying it to just the anode material.Wait, no.For a battery, the gravimetric energy density is the total energy divided by the total mass, and volumetric is total energy divided by total volume.So if the anode is a hybrid of Si and graphite, the mass is m Si + m graphite, and the volume is V Si + V graphite (assuming no volume change on mixing, which might not be true, but maybe for estimation purposes, they use the rule of mixtures).But the problem here is that the 12% Si anode -the Si content is 12% by weight.So m Si = 0.12 × m total, m graphite = 0.88 × m total.Similarly, the volume would be 0.12 × V total + 0.88 × V total, but only if the densities of Si and graphite are the same, which they aren't.Wait, silicon has a density of around 2.33 g/cm 3 , while graphite has a density of about 2.09 g/cm 3 .So volume percentages would be mass percentages divided by relative densities.This is getting complicated.Maybe the problem expects us to ignore volume changes and just take mass ratios?But how would that affect GED and VED?</p>
<p>Wait, perhaps the problem is considering only the mass and volume of the anode materials (Si and graphite) when calculating GED and VED.However, in reality, the mass of the anode includes the current collector, binders, etc., but maybe for this estimation, they are considering only the active materials.The problem says "the expected gravimetric energy density (GED) and volumetric energy density (VED) for LIBs equipped with 30% Si-content Si/graphite hybrid anodes and those with pure Si anodes."So maybe the GED is calculated as (Mass of Si + Mass of graphite) × (Specific capacity of Si + Specific capacity of graphite) divided by (Mass of Si + Mass of graphite), and then divided by the total volume (Volume of Si + Volume of graphite).Wait, no.Wait, energy density (both gravimetric and volumetric) is (Mass × Specific capacity) / Volume.But when considering a hybrid anode, how does the specific capacity factor in? Because the specific capacity of the anode is the mass ratio of Si to the total mass.For example, the specific capacity of a 12% Si anode would be 0.12 × 4200 mAh/g (assuming 100% Coulombic efficiency, which isn't the case, but maybe for estimation) plus the specific capacity of graphite.But graphite's theoretical specific capacity is about 372 mAh/g.However, in practice, due to the low density and good conductivity, the specific capacity of the anode would be some combination of the two.But the problem here might be oversimplified, as the current 12% Si anode has a certain GED and VED, and we need to calculate what happens if we change the Si content to 30% or use pure Si.</p>
<p>Wait, the current 12% Si anode has GED = 320 Wh/kg and VED = 600 Wh/L.To compute these values, they must have used the total mass and volume of the anode.So perhaps we can model the anode as a mixture of Si and graphite where the mass of Si is 12% of the total anode mass.Let's denote the total anode mass as m total.So m Si = 0.12×m total, and m graphite = 0.88×m total.The specific capacity is then (0.12 × 4200 + 0.88 × 372) mAh/g.Then the capacity (C) is SC ×m total.Then energy is C × V , but wait, no.Wait, the energy (E) is capacity (C) multiplied by voltage (V).But since this is a lithium-ion battery, the voltage is roughly 3.7 V.But maybe for the purposes of calculating GED and VED, the voltage is considered as 3.7 V.So E = C × V .</p>
<p>Then GED would be GED = E m total = (0.12 × 4200 + 0.88 × 372) × 3.7 m total .</p>
<p>But m total is also the denominator, so GED = (0.12 × 4200 + 0.88 × 372) × 3.7.Similarly, VED is VED = E V total = (0.12 × 4200 + 0.88 × 372) × 3.7 V total , where V total is the volume of the anode.But here is where it gets tricky: V total is the volume of the Si and the volume of the graphite.Since they are mixed together, but the densities are different.So</p>
<p>V Si = m Si ρ Si = 0.12 × m total 2.33 and V graphite = m graphite ρ graphite = 0.88 × m total 2.09 .</p>
<p>Then total volume</p>
<p>V total = V Si + V graphite ≈ 0.12 2.33 + 0.88 2.09 × m total.</p>
<p>Let me compute that: 0.12/2.33≈ 0.0515 and 0.88/2.09≈ 0.421, so total ≈ 0.4725.Thus, V total ≈ 0.4725 × m total.Therefore, the VED would be GED divided by this factor.</p>
<p>. . .</p>
<p>The text continues with the detailed reasoning of how one might compute the GED and VED for a 30% Si anode and a pure Si anode, comparing the changes in mass, volume, and energy due to different Si contents, and discussing the assumptions and potential discrepancies in the calculations.</p>
<p>. . .Now, the user needs to calculate the expected GED and VED for a 30% Si hybrid anode (Si/graphite) and a pure Si anode.The current anode is 12% Si hybrid with GED 320 and VED 600.The problem likely wants the answer expressed in terms of the same variables, so:</p>
<p>For 30% Si hybrid:</p>
<p>GED = [0.3× 4200 + 0.7 × 372] × 3.7 m total But m total is not given.However, from the original values: For the 12% Si anode: E = (0.12 × 4200 + 0.88 × 372) × 3.7 = 3081.12m total = E GED = 3081.12320 ≈ 9.6325</p>
<p>V total = E VED = 3081.12600 ≈ 5.1352 L Now, assuming that the 30% Si anode has the same mass and volume as the 12% Si anode (which is not realistic, but the problem might assume it for calculation), then: But in reality, since the 30% Si anode would have a higher silicon content, the mass and volume would be higher, so the actual GED would be lower.However, the problem might ignore that and just use the same mass and volume.Similarly for VED: But actual pure silicon anode would be even heavier, so GED would be even lower.However, if assuming same mass and volume, then: VED = 15720 5.135 ≈ 3050 Wh/L Therefore, the expected GED for a 30% Si hybrid anode is approximately 582 Wh/kg and VED approximately 294 Wh/L.For pure Si, GED is about 1626 Wh/kg and VED about 3050 Wh/L.But these values are likely much higher than feasible because in reality, the mass and volume would be much higher with increased Si content.However, given the problem's phrasing, this might be the intended answer.
GED</p>
<p>QUESTION 2</p>
<p>Five 2 V cells, each having an internal resistance of 0.2 Ω, are connected in series to a load of resistance 14 Ω.</p>
<p>The current flowing in the circuit is: (a) 10 A (b) 1.4 A (c) 1.5 A (d) 2/3 A ANSWER Given five 2 V cells connected in series to a load of resistance 14 Ω, each cell has an internal resistance of 0.2 Ω.To find the current flowing in the circuit:</p>
<p>1.The total voltage of the five cells is 5 × 2 V = 10 V. which is not an answer.However, if they considered that each cell has internal resistance, then the total internal resistance is 1 Ω, so the total resistance becomes 15 Ω.Therefore, 10 15 = 0.6667, which is 2 3 .So the correct answer is 2 3 A. Thus, the correct answer is option (d) 2 3 A.</p>
<p>Figure 1 :
1
Figure 1: Illustration of our OmniScience training pipeline.We begin with a LLaMA 3.1 70B foundation model, apply domain adaptive pretraining to obtain the OmniScience base model, and then perform model alignment and reasoning-based knowledge distillation to produce the final OmniScience Reasoning model.for use as solvents or additives.Such capabilities not only streamline research workflows but also pave the way for breakthrough innovations across multiple scientific disciplines.However, developing a highly specialized LLM for general science poses unique challenges.General-purpose foundation models(Dubey et al., 2024;OpenAI, 2022;Team et al., 2023;Alayrac et al., 2022;Bai et al., 2023;Guo et al., 2025) often lack the domain-specific vocabulary and contextual understanding needed to tackle complex scientific topics, such as molecular structures, electrochemical properties, experimental data, and gene expression patterns.While domain-specific language models like ChipNeMo(Liu et al., 2023) for chip design and BloombergGPT(Wu et al., 2023) for financial data have shown the benefits of specialized adaptation, similar efforts in general science remain limited(Taylor et al., 2022;Feng et al., 2024;Zhang et al., 2024; Frey et al., 2022;Li &amp; Jiang, 2021;Yunusoglu et al., 2025;Zhang et al., 2025;Tang et al., 2025).This gap highlights the need for a dedicated general science LLM to address the specific demands of the field.Training an LLM from scratch using only domain-specific data is both computationally intensive and costly.A more practical and efficient approach is Domain Adaptive Pre-Training (DAPT)(Gururangan et al., 2020), which involves further training a pre-existing foundation model on a targeted dataset from a specific domain(Shen et al., 2024;dos Santos Junior et al., 2024;Liu et al., 2023).This method allows researchers to leverage the broad knowledge of a general-purpose LLM while enhancing its expertise in a specific domain (e.g., general science).By continuously pretraining on in-domain texts, DAPT achieves significant improvements in domain-specific tasks with only a fraction of the computational resources required for full-scale training.This approach not only retains the model's general language understanding but also optimizes its performance for domain-related applications.In this work, we present OmniScience (see Fig.1), an adaptation of the LLaMA 3.1 70B model(Dubey et al., 2024) specifically tailored for science exploration.Our approach leverages DAPT on a carefully curated dataset that includes peer-reviewed articles, arXiv papers, journals, and textbooks covering general science and electrochemistry.To ensure high-quality input, we develop a robust data-processing pipeline that cleans and organizes the text effectively.We then perform DAPT on this domain-specific corpus, enabling the model to acquire a deep understanding of scientific language and concepts.General-purpose LLMs widely adopt instruction-based supervised fine-tuning(OpenAI, 2022;Dubey et al., 2024) as a key strategy to ensure they accurately follow user instructions.In line with this trend, we refine our base OmniScience through instruction tuning after domain adaptive</p>
<p>Figure 2 :
2
Figure 2: Comparison of GPQA Diamond scores with top 10-100B parameter models.Our model outperforms all the baselines including DeepSeek-R1 distill variants.</p>
<p>Figure 3 :
3
Figure 3: Bar chart visualizing the performance of various LLMs on MMLU, Winogrande, Hellaswag, and ARC-E.OmniScience Reasoning consistently matches or exceeds stronger proprietary models across benchmarks.</p>
<p>Figure 4 :
4
Figure 4: Bar chart visualization of battery-specific task performance for various LLMs.This figure corresponds to Table3and highlights comparative accuracy across Q/A, MCQ, reading comprehension, summarization, and reasoning.</p>
<p>Figure 7 :Figure 8 :Figure 9 :
789
Figure 7: Training and validation loss curves during domain adaptive pretraining of OmniScience.</p>
<p>Overview of the data sources used for domain-adaptive pretraining, totaling 35 billion tokens.</p>
<p>Figure 10 :
10
Figure10: Workflow of domain adaptive pretraining.The figure illustrates our data collection, curation, and domain adaptive pretraining process.We gather raw papers, textbooks, and preprints, apply de-duplication and heuristic filtering, then tokenize the curated dataset using the LLaMA 3.1 70B tokenizer.Finally, the domain adaptive pretraining is carried out on a multi-node system using the NeMo framework, resulting in the OmniScience base model.</p>
<p>training on DGX (128 H100s) using NeMo Framework 160k Task Instructions</p>
<p>Figure 11 :
11
Figure 11: Workflow of supervised fine tuning on the instruction dataset.The figure outlines our process for generating task based instruction data and fine tuning the OmniScience base model.We randomly sample 50,000 preprocessed documents from our pretraining corpus and pass them to the GPT-4o-mini API, which is prompted to produce four distinct task instructions: question answering, summarization, reading comprehension, and multiple choice questions.In addition, we incorporate 90,000 chat instruction samples from the Daring Anteater dataset, yielding a combined SFT dataset of 250,000 samples.This dataset is tokenized using the LLaMA 3.1 70B tokenizer, and the supervised fine tuning is performed on a multi node system using the NeMo framework, resulting in the final OmniScience Chat model.</p>
<p>training on DGX (64 H100s) using NeMo Framework</p>
<p>Figure 12 :
12
Figure 12: Workflow of reasoning based knowledge distillation on the s1K(Muennighoff et al., 2025) dataset.The dataset derived from DeepSeek-R1 traces is tokenized using the LLaMA 3.1 70B tokenizer, and the reasoning based knowledge distillation in the form of fine tuning is performed on a multi-node system using the NeMo framework, resulting in the final OmniScience Reasoning model.</p>
<p>Table 2 :
2
Performance of SOTA reasoning models on four core public benchmarks -MMLU, Winogrande, Hellaswag, and ARC-E evaluating expert knowledge, commonsense, and reasoning.</p>
<p>Table 3 :
3
Performance comparison on battery-specific tasks, including Q/A, MCQ, Reading Comprehension, Summarization, and Reasoning.</p>
<p>worst and 10 is the best).Mean Grade = 1 NN i=1Grade iMean Rank: The Mean Rank is the average position of relevant molecules in the ranked list.Lower values indicate better performance, as the desired molecules are ranked higher.Rank jWhere:• N is the total number of molecules.• Grade i is the grade of the i-th molecule.• M is the number of relevant molecules.• Rank j is the rank of the j-th relevant molecule.Hits@k: The Hits@k metric measures the fraction of molecules in the top k positions of the ranked list that are desired.For k = 5, 10, 20, 50, it is defined as: Hits@k = Number of desired items in top k kA.4 OmiScience Reasoning Model Example ResponsesWe present two examples generated by the OmiScience Reasoning Model.Each example is composed of a clearly stated question, a detailed answer, and a corresponding reasoning trace that captures the model's step-by-step thought process.These examples demonstrate how the model systematically breaks down complex problems, integrates relevant information, and constructs comprehensive responses.QUESTION 1The 12% Si-content Si/graphite hybrid anode in LIBs currently achieves around 320 Wh/kg and 600 Wh/L.Could you calculate the expected gravimetric energy density (GED) and volumetric energy density (VED) for LIBs equipped with 30% Si-content Si/graphite hybrid anodes and those with pure Si anodes?ANSWERTo calculate the expected gravimetric energy density (GED) and volumetric energy density (VED) for lithium-ion batteries (LIBs) with 30% Si-content Si/graphite hybrid anodes and pure Si anodes, we start by analyzing the current 12% Si-content anode.Current 12% Si Anode:• Gravimetric Energy Density (GED) = 320 Wh/kg • Volumetric Energy Density (VED) = 600 Wh/L 2. The total internal resistance of the five cells is 5 × 0.2 Ω = 1 Ω.3. The total resistance in the circuit is the sum of the internal resistance and the load resistance:1 Ω + 14 Ω = 15 Ω.4. The current through the circuit is calculated by dividing the total voltage by the total resistance:Thus, the current flowing in the circuit is (d) 2 3 A.DETAILED THINKINGOkay, so I have this problem where there are five 2 V cells connected in series to a load resistance of 14 Ω.Each cell has an internal resistance of 0.2 Ω.I need to find the current flowing in the circuit.The options are 10 A, 1.4 A, 1.5 A, or 2 3 A. Hmm, let me think.First, I remember that when batteries are connected in series, their voltages add up.So, if each cell is 2 V , then the total voltage would be 5 × 2 = 10 V .But the load resistance is 14 Ω.Also, each cell has an internal resistance of 0.2 Ω. Wait, is that per cell or total?The problem says "each having an internal resistance of 0.2 Ω."So each cell has an internal resistance of 0.2 Ω.Therefore, for all five cells, the total internal resistance would be 5 × 0.2 = 1 Ω, right?But the load is 14 Ω.So the total resistance in the circuit is the internal resistance of the cells plus the load resistance.Let me make sure.The internal resistance is the resistance inside the battery that causes the actual voltage to be less than the nominal voltage.So when the cells are connected to the load, the total resistance they encounter is the internal resistance (which is 0.2 Ω per cell) in series with the load resistance (14 Ω).Wait, but if each cell has an internal resistance of 0.2 Ω, then for all five cells in series, the total internal resistance would be 5 × 0.2 = 1 Ω.Then the total resistance in the circuit is 1 Ω (internal) + 14 Ω (load) = 15 Ω.So the total resistance is 15 Ω.The current through the circuit is determined by the total voltage divided by the total resistance.The total voltage is 10 V (5 cells ×2 V each).So the current is 10 V /15 Ω.Let me compute that: 10 divided by 15 is 0.666..., so 0.6667 A. But that's not one of the options.Wait, but the options are 10 A, 1.4 A, 1.5 A, and 2 3 A. 0.6667 A is closest to 2 3 A, which is 0.6666 A. So maybe I made a calculation mistake.Wait, hold on.Let me check again.If the total voltage is 10 V , and the total resistance is 15 Ω, then the current is 10 V /15 Ω = (10/15) A = 0.666... A. Which is approximately 0.6667 A. But the answer options have 2 3 A as one of them. 2 3 is approximately 0.6666.So that would match.But wait, why is there a discrepancy?Let me verify my steps again.Each cell is 2 V with an internal resistance of 0.2 Ω.So each cell's actual voltage when connected to a load would be 2 V − 0.2 V (due to internal resistance) = 1.8 V .But when connected in series, the voltages add.So five cells would produce 5 × 1.8 = 9 V .Then the load is 14 Ω.Wait, but hold on, the internal resistance is in series with the load.So the total voltage drop is across the internal resistance and the load.Wait, I think there's a confusion here.Let me clarify.The internal resistance is inside the battery.When you connect the battery to a load, the current flows through the battery's internal resistance and then through the load.So the total resistance in the circuit is the sum of the internal resistance and the load resistance.So if each cell has an internal resistance of 0.2 Ω, and you have five cells in series, the total internal resistance is 5 × 0.2 = 1 Ω.Then the load is 14 Ω.So the total resistance is 1 + 14 = 15 Ω.Therefore, the current through the circuit is the total voltage divided by the total resistance.But wait, the total voltage here is a bit different.Wait, each cell is a battery.So the actual voltage when considering internal resistance is the nominal voltage minus the voltage drop across the internal resistance.Wait, the nominal voltage of each cell is 2 V .The actual voltage when connected to a load would be 2 V − (I × 0.2 Ω).But when they are connected in series, how does that work?Wait, if you have five cells connected in series, each producing 2 V , but each also has an internal resistance of 0.2 Ω.So the first cell will produce 2 V , but the actual voltage that the second cell sees is 2 V minus the voltage drop across the first cell's internal resistance.Wait, this is getting complicated.Maybe another approach is necessary.Alternatively, the total internal resistance is 1 Ω (5 cells ×0.2 Ω each).Then the total voltage applied is 10 V (5 × 2 V ).However, the actual voltage across the entire circuit (including internal resistances) is 10 V .But the current through the circuit is determined by the total resistance (15 Ω) and the actual voltage.Wait, but the actual voltage here is after considering the internal resistances.Wait, this is confusing.Wait, perhaps there is an easier way.If each cell is considered as a voltage source with internal resistance, then connecting five of them in series would result in a total voltage of 10 V (since each contributes 2 V ) and a total internal resistance of 1 Ω (since each contributes 0.2 Ω).Then, the load is 14 Ω.So the total resistance in the circuit is 1 Ω + 14 Ω = 15 Ω.Then, the current through the circuit is 10 V 15 Ω ≈ 0.6667 A.However, the options only include 2 3 A, which is approximately 0.6666 A. So that must be the correct answer.But wait, one of the options is 1.5 A. If someone miscalculates, thinking that the total voltage is 10 V and the total resistance is 14 Ω (not accounting for internal resistance), then they would get 10 14 ≈ 0.714 A, which is close to 0.6667 A, but not exactly.Alternatively, if someone thinks the total resistance is 1 Ω (only internal) plus 14 Ω load, totaling 15 Ω, then the current is 10 V 15 Ω ≈ 0.6667 A.That matches 2 3 A. Alternatively, maybe the person forgets to account for internal resistance and just takes the load resistance as 14 Ω, so 10 V 14 A ≈ 0.714,
Flamingo: a visual language model for few-shot learning. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Advances in neural information processing systems. 202235Anthropic. Claude 3.5 sonnet</p>
<p>. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, arXiv:2309.166092023Qwen technical report. arXiv preprint</p>
<p>Elliot Bolton, Abhinav Venigalla, Michihiro Yasunaga, David Hall, Betty Xiong, Tony Lee, Roxana Daneshjou, Jonathan Frankle, Percy Liang, Michael Carbin, arXiv:2403.18421A 2.7 b parameter language model trained on biomedical text. 2024arXiv preprint</p>
<p>Improving language models by retrieving from trillions of tokens. Sébastien Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las, Aurelia Casas, Jacob Guy, Roman Menick, Tom Ring, Saffron Hennigan, Lisa Anne Huang, Catalina Hendricks, Daniel Dadarlat, Chris Ziegler, Albin Jones, Andy Cassirer, Akbir Brock, Vlad Khan, Oriol Firoiu, Andrew Vinyals, Nicolas Trask, Matthew Carrazza, David Botvinick, Choi, arXiv:2112.044262021arXiv preprint</p>
<p>On the resemblance and containment of documents. Andrei Z Broder, Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171). Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171)IEEE1997</p>
<p>Chemberta: large-scale selfsupervised pretraining for molecular property prediction. Seyone Chithrananda, Gabriel Grand, Bharath Ramsundar, arXiv:2010.098852020arXiv preprint</p>
<p>Think you have solved question answering? try arc, the ai2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, arXiv:1803.054572018arXiv preprint</p>
<p>On the use of arxiv as a dataset. Colin B Clement, Matthew Bierbaum, Kevin P O'keeffe, Alexander A Alemi, 2019</p>
<p>Flashattention: Fast and memory-efficient exact attention with io-awareness. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, Christopher Ré, Advances in neural information processing systems. 202235</p>
<p>Domain-driven llm development: Insights into rag and fine-tuning practices. José Cassio, Santos Junior, Rachel Hu, Richard Song, Yunfei Bai, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024arXiv preprint</p>
<p>Gpt4battery: An llm-driven framework for adaptive state of health estimation of raw li-ion batteries. Yuyuan Feng, Guosheng Hu, Zhihong Zhang, arXiv:2402.000682024arXiv preprint</p>
<p>Chemgpt: Neural scaling of deep chemical models. Nathan Frey, Ryan Soklaski, Simon Axelrod, Siddharth Samsi, Rafael Gomez-Bombarelli, Connor Coley, Vijay Gadepally, 10.26434/chemrxiv-2022-3s512ChemRxiv. 2022</p>
<p>Similarity search in high dimensions via hashing. Aristides Gionis, Piotr Indyk, Rajeev Motwani, Proceedings of the 25th International Conference on Very Large Data Bases (VLDB). the 25th International Conference on Very Large Data Bases (VLDB)VLDB Endowment1999</p>
<p>Nemo: an evolutionary and population genetics programming framework. Frédéric Guillaume, Jacques Rougemont, Bioinformatics. 22202006</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Don't stop pretraining: Adapt language models to domains and tasks. Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, Noah A Smith, arXiv:2004.109642020arXiv preprint</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Atlas: Few-shot learning with retrieval augmented language models. Gautier Izacard, Lucas Hosseini, Fabio Petroni, Patrick Lewis, Ledell Wu, Wen Tau Yih, Sonal Gupta, Nicola De Cao, Antoine Bordes, Stuart Shieber, Sebastian Riedel, Douwe Kiela, arXiv:2208.032992022arXiv preprint</p>
<p>Chemrxiv: A chemistry preprint server. L L Kiessling, L E Fernandez, A P Alivisatos, P S Weiss, ACS Nano. 10102016</p>
<p>Pubchem substance and compound databases. Sunghwan Kim, Paul A Thiessen, Evan E Bolton, Jie Chen, Gang Fu, Asta Gindulyte, Lianyi Han, Jane He, Siqian He, Benjamin A Shoemaker, Nucleic acids research. 2016</p>
<p>P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980Adam: A method for stochastic optimization. 2014arXiv preprint</p>
<p>Oleksii Kuchaiev, Jason Li, Huyen Nguyen, Oleksii Hrinchuk, Ryan Leary, Boris Ginsburg, Samuel Kriman, Stanislav Beliaev, Vitaly Lavrukhin, Jack Cook, arXiv:1909.09577a toolkit for building ai applications using neural modules. Nemo2019arXiv preprint</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen Tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela, arXiv:2005.114012020arXiv preprint</p>
<p>Mol-bert: An effective molecular representation with bert for molecular property prediction. Juncai Li, Xiaofei Jiang, 10.1155/2021/7181815Complexity. 2021</p>
<p>Mingjie Liu, Teodor-Dumitru Ene, Robert Kirby, Chris Cheng, Nathaniel Pinckney, Rongjian Liang, Jonah Alben, Himyanshu Anand, Sanmitra Banerjee, Ismet Bayraktaroglu, arXiv:2311.00176Domain-adapted llms for chip design. 2023arXiv preprint</p>
<p>Grok 3: A 2700b parameter model for advanced reasoning. Wei Liu, Mark Johnson, Li Zhang, Raj Kumar, Xiaoming Chen, 2025</p>
<p>S2ORC: The semantic scholar open research corpus. Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, Daniel Weld, 10.18653/v1/2020.acl-main.447Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 2020</p>
<p>Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang , Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, Tatsunori Hashimoto, arXiv:2501.19393Simple test-time scaling. 20251arXiv preprint</p>
<p>Optimizing language models for dialogue. Openai, Chatgpt, 2023-03-04November 2022</p>
<p>Optimizing language models for dialogue. Openai, Chatgpt, 2024</p>
<p>Optimizing language models for dialogue. Openai, Chatgpt, 2025</p>
<p>Gpqa: A graduate-level google-proof q&amp;a benchmark. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R Bowman, First Conference on Language Modeling. 2024</p>
<p>Winogrande: An adversarial winograd schema challenge at scale. Keisuke Sakaguchi, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, Communications of the ACM. 6492021</p>
<p>Tag-llm: Repurposing general-purpose llms for specialized domains. Junhong Shen, Neil Tenenholtz, Brian James, David Hall, Nicolo Alvarez-Melis, Fusi, arXiv:2402.051402024arXiv preprint</p>
<p>Megatron-lm: Training multi-billion parameter language models using model parallelism. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick Legresley, Jared Casper, Bryan Catanzaro, arXiv:1909.080532019arXiv preprint</p>
<p>Matterchat: A multi-modal llm for material science. Yingheng Tang, Wenbin Xu, Jie Cao, Jianzhu Ma, Weilu Gao, Steve Farrell, Benjamin Erichson, Michael W Mahoney, Andy Nonaka, Zhi Yao, arXiv:2502.131072025arXiv preprint</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, arXiv:2312.118052023arXiv preprint</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, arXiv:2312.118052025arXiv preprint</p>
<p>Unstructured: A python library for processing unstructured data. Unstructured, Io, 2022. February 27, 2025</p>
<p>Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy J Zhang, Makesh Narsimhan Sreedhar, Oleksii Kuchaiev, Helpsteer2: Open-source dataset for training top-performing reward models. 2024</p>
<p>Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, Gideon Mann, arXiv:2303.17564Bloomberggpt: A large language model for finance. 2023arXiv preprint</p>
<p>. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, arXiv:2412.1511520245 technical report. arXiv preprint</p>
<p>Battery state of health estimation using llm framework. Aybars Yunusoglu, Dexter Le, Karn Tiwari, Murat Isik, Dikmen, arXiv:2501.181232025arXiv preprint</p>
<p>Hellaswag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, arXiv:1905.078302019arXiv preprint</p>
<p>Di Zhang, Yutong Li, Yixuan Li, A chemical large language model. 2024Preprint or manuscript in preparation</p>
<p>Scientific large language models: A survey on biological &amp; chemical domains. Qiang Zhang, Keyan Ding, Tianwen Lv, Xinda Wang, Qingyu Yin, Yiwen Zhang, Jing Yu, Yuhao Wang, Xiaotong Li, Zhuoyi Xiang, ACM Computing Surveys. 5762025</p>            </div>
        </div>

    </div>
</body>
</html>
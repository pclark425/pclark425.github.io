<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9613 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9613</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9613</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-167.html">extraction-schema-167</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-278394651</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.05428v2.pdf" target="_blank">Empowering Scientific Workflows with Federated Agents</a></p>
                <p><strong>Paper Abstract:</strong> Agentic systems, in which diverse agents cooperate to tackle challenging problems, are exploding in popularity in the AI community. However, the agentic frameworks used to build these systems have not previously enabled use with research cyberinfrastructure. Here we introduce Academy, a modular and extensible middleware designed to deploy autonomous agents across the federated research ecosystem, including HPC systems, experimental facilities, and data repositories. To meet the demands of scientific computing, Academy supports asynchronous execution, heterogeneous resources, high-throughput data flows, and dynamic resource availability. It provides abstractions for expressing stateful agents, managing inter-agent coordination, and integrating computation with experimental control. We present microbenchmark results that demonstrate high performance and scalability in HPC environments. To demonstrate the breadth of applications that can be supported by agentic workflow designs, we also present case studies in materials discovery, decentralized learning, and information extraction in which agents are deployed across diverse HPC systems.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9613.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9613.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agentic MCQ workflow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agentic Multiple-Choice Question Generation and Validation Workflow (Information Extraction case study)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agentic pipeline implemented in Academy that extracts text from PDFs and uses multiple LLM-backed Generator, Answerer, Selector, and Scorer agents to produce and validate multiple-choice questions (MCQs) from scientific publications; demonstrates integration of different LLMs as interchangeable agents and concurrent execution across federated resources.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B-Instruct-v0.3; Meta-Llama-3-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two instruction-tuned LLMs used as interchangeable agent backends: Mistral-7B-Instruct-v0.3 (instruct-tuned Mistral family) and Meta-Llama-3-70B-Instruct (instruction-tuned Llama 3 family). The paper denotes these models as A (Mistral-7B-Instruct-v0.3) and B (Meta-Llama-3-70B-Instruct) and runs them in separate Generator / Answerer / Scorer agents.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B; 70B</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Research publications (PDFs) parsed by a PDFParser agent; the demonstration execution trace processed 10 manuscripts concurrently to generate and validate MCQs.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>10</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Generate and validate multiple-choice questions (MCQs) and their answers from individual scientific publications (i.e., extract salient facts, methods, and findings from each paper to form question/answer pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Pipeline agentic method: (1) PDFParser agent extracts text from a manuscript, (2) two Generator agents (using different LLMs) independently produce MCQs from the extracted text, (3) an MCQSelector agent selects subsets of generated questions for evaluation, (4) two MCQAnswerer agents (each using an LLM) generate answers, and (5) two AnswerScorer agents (each with a different LLM) validate/rank the generated answers. Agents run concurrently and communicate via Academy's distributed exchange; the architecture emphasizes modular swapping of LLMs and scaling of agents across federated compute resources.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured MCQs with generated answers and validation scores (a dataset of question/answer pairs and associated validation metadata).</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Execution trace demonstrating end-to-end concurrent processing (agents running concurrently to generate and validate MCQs for 10 manuscripts). No quantitative accuracy metrics or human expert evaluation reported in the paper for this case study.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>No quantitative evaluation results provided; the paper reports an execution trace showing that the pipeline processed 10 manuscripts over a 15-minute interval using the specified agents and models, but does not report precision/recall, agreement with human labels, or other gold-standard comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Demonstrates modularity and interchangeability of LLMs as agents, concurrent and federated execution across heterogeneous resources, easy integration of alternate LLMs and agent roles, and automatic scaling of agents to workload; shows feasibility of automating extraction and creation of structured artifacts (MCQs) from papers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Demonstration is small-scale (10 manuscripts) and exploratory; the paper does not report quantitative evaluation of correctness/faithfulness of generated MCQs or of hallucination rates; no gold-standard human evaluation or large-corpus synthesis results are presented.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>None specifically reported for this workflow in the paper; the authors do not present characterized failure cases (e.g., hallucinations, incorrect extractions) or error rates for MCQ generation/validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Scientific Workflows with Federated Agents', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9613.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9613.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based literature synthesis (conceptual)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conceptual use of LLM-backed agents for literature synthesis and knowledge extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper discusses, at a conceptual level, that LLM-backed agentic workflows can be used to identify and synthesize key findings, methodologies, and datasets across the exponentially growing scientific literature to produce structured datasets and support downstream tasks like fine-tuning, summarization, and hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Conceptual: the paper does not specify a concrete model architecture or training details in this discussion; it frames LLMs as components that can be integrated into agentic workflows to perform information extraction and synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Described generally as the rapidly growing body of scientific publications; no specific corpus, sources, selection criteria, or corpus size are provided in the conceptual discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Broad literature analysis tasks: identify and synthesize key findings, methodologies, data sources across disciplines to enable cross-disciplinary insights and structured datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Not specified in detail; discussed as a potential application of agentic workflows where specialized agents (e.g., parsers, generators, validators) and LLMs collaborate to extract and synthesize information. No concrete prompting, RAG, or chain-of-thought methods are described for large-scale synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Proposed outputs include structured datasets of key concepts/findings, summaries, and training data for fine-tuning models; described at a high level rather than demonstrated.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not specified; the paper frames this as a promising capability but does not present an evaluation protocol or empirical results for large-scale literature synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Highlights potential for automation at scale, cross-disciplinary discovery, and creation of structured datasets to accelerate scientific progress; positions agentic architectures as suitable infrastructure for deploying such LLM-based synthesis across federated resources.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Discussion is conceptual only — the paper does not demonstrate large-scale synthesis, nor does it address empirical evaluation, factuality, or mitigation of LLM errors in that context.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Scientific Workflows with Federated Agents', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9613.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9613.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaperQA (cited)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaperQA: Retrieval-Augmented Generative Agent for Scientific Research</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced system (cited in related work) that applies retrieval-augmentation and generative agents to support scientific research and question-answering over papers; included in the paper's related-work context as an example of LLM-based systems for scientific research assistance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PaperQA: Retrieval-Augmented Generative Agent for Scientific Research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not described in this paper beyond the citation; PaperQA (cited work) is known by title only in the references and is discussed as related work in applying retrieval-augmented generative agents for scientific research.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Scientific Workflows with Federated Agents', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>PaperQA: Retrieval-Augmented Generative Agent for Scientific Research <em>(Rating: 2)</em></li>
                <li>Autonomous LLM-Driven Research-from Data to Human-Verifiable Research Papers <em>(Rating: 2)</em></li>
                <li>Creating and Scoring Multiple Choice Questions (MCQs) from Papers <em>(Rating: 2)</em></li>
                <li>Improving Factuality and Reasoning in Language Models through Multiagent Debate <em>(Rating: 1)</em></li>
                <li>Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate <em>(Rating: 1)</em></li>
                <li>Empowering biomedical discovery with AI agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9613",
    "paper_id": "paper-278394651",
    "extraction_schema_id": "extraction-schema-167",
    "extracted_data": [
        {
            "name_short": "Agentic MCQ workflow",
            "name_full": "Agentic Multiple-Choice Question Generation and Validation Workflow (Information Extraction case study)",
            "brief_description": "An agentic pipeline implemented in Academy that extracts text from PDFs and uses multiple LLM-backed Generator, Answerer, Selector, and Scorer agents to produce and validate multiple-choice questions (MCQs) from scientific publications; demonstrates integration of different LLMs as interchangeable agents and concurrent execution across federated resources.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-7B-Instruct-v0.3; Meta-Llama-3-70B-Instruct",
            "model_description": "Two instruction-tuned LLMs used as interchangeable agent backends: Mistral-7B-Instruct-v0.3 (instruct-tuned Mistral family) and Meta-Llama-3-70B-Instruct (instruction-tuned Llama 3 family). The paper denotes these models as A (Mistral-7B-Instruct-v0.3) and B (Meta-Llama-3-70B-Instruct) and runs them in separate Generator / Answerer / Scorer agents.",
            "model_size": "7B; 70B",
            "input_corpus_description": "Research publications (PDFs) parsed by a PDFParser agent; the demonstration execution trace processed 10 manuscripts concurrently to generate and validate MCQs.",
            "input_corpus_size": 10,
            "topic_query_description": "Generate and validate multiple-choice questions (MCQs) and their answers from individual scientific publications (i.e., extract salient facts, methods, and findings from each paper to form question/answer pairs).",
            "distillation_method": "Pipeline agentic method: (1) PDFParser agent extracts text from a manuscript, (2) two Generator agents (using different LLMs) independently produce MCQs from the extracted text, (3) an MCQSelector agent selects subsets of generated questions for evaluation, (4) two MCQAnswerer agents (each using an LLM) generate answers, and (5) two AnswerScorer agents (each with a different LLM) validate/rank the generated answers. Agents run concurrently and communicate via Academy's distributed exchange; the architecture emphasizes modular swapping of LLMs and scaling of agents across federated compute resources.",
            "output_type": "Structured MCQs with generated answers and validation scores (a dataset of question/answer pairs and associated validation metadata).",
            "output_example": null,
            "evaluation_method": "Execution trace demonstrating end-to-end concurrent processing (agents running concurrently to generate and validate MCQs for 10 manuscripts). No quantitative accuracy metrics or human expert evaluation reported in the paper for this case study.",
            "evaluation_results": "No quantitative evaluation results provided; the paper reports an execution trace showing that the pipeline processed 10 manuscripts over a 15-minute interval using the specified agents and models, but does not report precision/recall, agreement with human labels, or other gold-standard comparisons.",
            "strengths": "Demonstrates modularity and interchangeability of LLMs as agents, concurrent and federated execution across heterogeneous resources, easy integration of alternate LLMs and agent roles, and automatic scaling of agents to workload; shows feasibility of automating extraction and creation of structured artifacts (MCQs) from papers.",
            "limitations": "Demonstration is small-scale (10 manuscripts) and exploratory; the paper does not report quantitative evaluation of correctness/faithfulness of generated MCQs or of hallucination rates; no gold-standard human evaluation or large-corpus synthesis results are presented.",
            "failure_cases": "None specifically reported for this workflow in the paper; the authors do not present characterized failure cases (e.g., hallucinations, incorrect extractions) or error rates for MCQ generation/validation.",
            "uuid": "e9613.0",
            "source_info": {
                "paper_title": "Empowering Scientific Workflows with Federated Agents",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "LLM-based literature synthesis (conceptual)",
            "name_full": "Conceptual use of LLM-backed agents for literature synthesis and knowledge extraction",
            "brief_description": "The paper discusses, at a conceptual level, that LLM-backed agentic workflows can be used to identify and synthesize key findings, methodologies, and datasets across the exponentially growing scientific literature to produce structured datasets and support downstream tasks like fine-tuning, summarization, and hypothesis generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": "Conceptual: the paper does not specify a concrete model architecture or training details in this discussion; it frames LLMs as components that can be integrated into agentic workflows to perform information extraction and synthesis.",
            "model_size": null,
            "input_corpus_description": "Described generally as the rapidly growing body of scientific publications; no specific corpus, sources, selection criteria, or corpus size are provided in the conceptual discussion.",
            "input_corpus_size": null,
            "topic_query_description": "Broad literature analysis tasks: identify and synthesize key findings, methodologies, data sources across disciplines to enable cross-disciplinary insights and structured datasets.",
            "distillation_method": "Not specified in detail; discussed as a potential application of agentic workflows where specialized agents (e.g., parsers, generators, validators) and LLMs collaborate to extract and synthesize information. No concrete prompting, RAG, or chain-of-thought methods are described for large-scale synthesis.",
            "output_type": "Proposed outputs include structured datasets of key concepts/findings, summaries, and training data for fine-tuning models; described at a high level rather than demonstrated.",
            "output_example": null,
            "evaluation_method": "Not specified; the paper frames this as a promising capability but does not present an evaluation protocol or empirical results for large-scale literature synthesis.",
            "evaluation_results": null,
            "strengths": "Highlights potential for automation at scale, cross-disciplinary discovery, and creation of structured datasets to accelerate scientific progress; positions agentic architectures as suitable infrastructure for deploying such LLM-based synthesis across federated resources.",
            "limitations": "Discussion is conceptual only — the paper does not demonstrate large-scale synthesis, nor does it address empirical evaluation, factuality, or mitigation of LLM errors in that context.",
            "failure_cases": null,
            "uuid": "e9613.1",
            "source_info": {
                "paper_title": "Empowering Scientific Workflows with Federated Agents",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "PaperQA (cited)",
            "name_full": "PaperQA: Retrieval-Augmented Generative Agent for Scientific Research",
            "brief_description": "A referenced system (cited in related work) that applies retrieval-augmentation and generative agents to support scientific research and question-answering over papers; included in the paper's related-work context as an example of LLM-based systems for scientific research assistance.",
            "citation_title": "PaperQA: Retrieval-Augmented Generative Agent for Scientific Research",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": "Not described in this paper beyond the citation; PaperQA (cited work) is known by title only in the references and is discussed as related work in applying retrieval-augmented generative agents for scientific research.",
            "model_size": null,
            "input_corpus_description": null,
            "input_corpus_size": null,
            "topic_query_description": null,
            "distillation_method": null,
            "output_type": null,
            "output_example": null,
            "evaluation_method": null,
            "evaluation_results": null,
            "strengths": null,
            "limitations": null,
            "failure_cases": null,
            "uuid": "e9613.2",
            "source_info": {
                "paper_title": "Empowering Scientific Workflows with Federated Agents",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "PaperQA: Retrieval-Augmented Generative Agent for Scientific Research",
            "rating": 2,
            "sanitized_title": "paperqa_retrievalaugmented_generative_agent_for_scientific_research"
        },
        {
            "paper_title": "Autonomous LLM-Driven Research-from Data to Human-Verifiable Research Papers",
            "rating": 2,
            "sanitized_title": "autonomous_llmdriven_researchfrom_data_to_humanverifiable_research_papers"
        },
        {
            "paper_title": "Creating and Scoring Multiple Choice Questions (MCQs) from Papers",
            "rating": 2,
            "sanitized_title": "creating_and_scoring_multiple_choice_questions_mcqs_from_papers"
        },
        {
            "paper_title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
            "rating": 1,
            "sanitized_title": "improving_factuality_and_reasoning_in_language_models_through_multiagent_debate"
        },
        {
            "paper_title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
            "rating": 1,
            "sanitized_title": "encouraging_divergent_thinking_in_large_language_models_through_multiagent_debate"
        },
        {
            "paper_title": "Empowering biomedical discovery with AI agents",
            "rating": 1,
            "sanitized_title": "empowering_biomedical_discovery_with_ai_agents"
        }
    ],
    "cost": 0.012885500000000001,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Empowering Scientific Workflows with Federated Agents
27 May 2025</p>
<p>J Gregory Pauloski 
University of Chicago</p>
<p>Yadu Babuji 
University of Chicago</p>
<p>Ryan Chard 
Argonne National Laboratory</p>
<p>Mansi Sakarvadia 
University of Chicago</p>
<p>Kyle Chard 
University of Chicago Argonne National Laboratory</p>
<p>Ian Foster 
Argonne National Laboratory University of Chicago</p>
<p>Empowering Scientific Workflows with Federated Agents
27 May 2025EB603FFBF270D4865E1A8D4BEB64AE63arXiv:2505.05428v2[cs.MA]Computational WorkflowsDistributed ComputingFederated ComputingMulti-Agent SystemsOpen-Source Software Agentic Infrastructure Experimental Facilities Data Storage Compute Agentic Workflows
Agentic systems, in which diverse agents cooperate to tackle challenging problems, are exploding in popularity in the AI community.However, the agentic frameworks used to build these systems have not previously enabled use with research cyberinfrastructure.Here we introduce Academy, a modular and extensible middleware designed to deploy autonomous agents across the federated research ecosystem, including HPC systems, experimental facilities, and data repositories.To meet the demands of scientific computing, Academy supports asynchronous execution, heterogeneous resources, high-throughput data flows, and dynamic resource availability.It provides abstractions for expressing stateful agents, managing interagent coordination, and integrating computation with experimental control.We present microbenchmark results that demonstrate high performance and scalability in HPC environments.To demonstrate the breadth of applications that can be supported by agentic workflow designs, we also present case studies in materials discovery, decentralized learning, and information extraction in which agents are deployed across diverse HPC systems.</p>
<p>Introduction</p>
<p>The desire to automate scientific processes has led to advancements in many fields, from artificial intelligence (AI) [72] and computational workflows [22] to research data management [4] and self-driving laboratories (SDL) [1], but humans typically remain responsible for core aspects of the iterative research cycle, including hypothesis generation, experimental design, code development, and data analysis.Often, the human-in-the-loop is the rate-limiting step in discovery.This friction increases as the scale and ambition of computational science endeavors grow and leads to inefficient use of research cyberinfrastructure-the federated ecosystem of experimental and observational facilities, data repositories, and high-performance computing (HPC) systems [50].</p>
<p>Intelligent agents, either as an individual system or composing larger multi-agent systems (MAS), rather than humans, can be the driving entities of discovery.Agents are independent, persistent, stateful, and cooperative-working together to achieve a predefined goal with only intermittent human oversight.The contemporaneous explosion of interest in multi-agent systems is largely a consequence of advancements in reasoning capabilities of the large language models (LLMs) often used to back AI agents.Expressing components of scientific applications as agents-programs that can perform tasks independently or semi-autonomously on behalf of a client or another agent-is powerful.An agent manages its own local state and exposes a well-defined behavior.Agents can perform human roles in iterative scientific processes [65] or encapsulate research cyberinfrastructure (e.g., computational resources and procedures, experimental instruments, data repositories) [30].</p>
<p>Significant progress has been made towards developing AI agents that can act on behalf of humans for such tasks as literature synthesis [43], hypothesis generation [35], and data analysis and publication [39].However, existing agent frameworks (e.g., AutoGen [69]) are not ready to build and deploy agents that employ federated research cyberinfrastructure.New middleware is needed to enable agentic workflows that seamlessly integrate experiment, observation, theory, simulation, AI, analysis, and more, as in Figure 1.</p>
<p>Frameworks for building agentic workflows are limited in scope and generally target conversational, cloud-native applications (e.g., LLM-based AI chatbots) [44,54,69].The federated nature of research infrastructure poses unique challenges: distributed resources have diverse access protocols, interactions between computational and experimental entities are asynchronous, and the dynamic availability of resources requires fault-tolerant and adaptive systems.Existing frameworks fail to address these intricacies.They lack abstractions and mechanisms tailored to support autonomous multiagent workflows that integrate computation, data management, and experimental control, which leads to brittle, ad hoc integrations that are ill-suited for the demands of modern science.Moreover, the inherent complexity of such workflows is compounded by the need to balance efficiency with scientific rigor, especially in applications involving real-time decision-making, iterative exploration, and multi-agent coordination.</p>
<p>These challenges are often orthogonal and span many levels of abstraction, but achieving this vision where intelligent agents serve as driving entities in scientific discovery requires a paradigm shift in how workflows are designed, orchestrated, and executed.We introduce a novel framework for building agentic workflows, emphasizing modularity, statefulness, and interoperability across the diverse research infrastructure.Specifically, this work contributes:</p>
<p>• Academy, a novel, modular, and extensible middleware for expressing agentic workflows and deploying multi-agent systems across federated resources.Academy addresses unique challenges in scientific applications, such as high data volumes, variable resource availability, and the heterogeneous nature of experimental and computational systems (Section 3).• Performance analysis of Academy in diverse scenarios yielding insights into the scalability and practical considerations of deploying agentic workflows (Section 4).• Case studies demonstrating the utility of agentic workflow design and highlighting improvements in automation, resource utilization, and discovery acceleration (Section 5).These contributions advance the state of the art in multi-agent systems for scientific discovery and establish a foundation for future innovations in autonomous research workflows.</p>
<p>Background</p>
<p>Agents encompass a rapidly expanding front for AI research, yet agent paradigms can address a breadth of challenges across the computational sciences.We begin with a definition of an agentinspired by prior work-that is sufficiently generic to encompass the various semantic uses of the term.Then, we enumerate common high-level classes of agents and formalize agentic workflows, both of which we aim to support in the design of Academy in Section 3.</p>
<p>An agent is a program that can perform actions independently or semi-autonomously on behalf of a client or another agent.This definition is imprecise but presents a powerful conceptual model for distributed computing.The agent concept originates from the actor model, a concurrent computing paradigm in which actors encapsulate a local state and communicate through message passing [37].Agents extend the actor model with the notion of agency-the ability of the agent to engage independently with its environment.</p>
<p>An agent  is defined by its behavior  and local state .The behavior of an agent encompasses a set of actions  ∈  (procedures that the agent can perform), and a set of control loops  ∈  that define the autonomous behavior [33].Agents are often longrunning, but may also be ephemeral-created to complete a specific task and then exiting.Clients and agents can request another agent to perform an action on their behalf through message passing.An action can be atomic or composite, invoking other actions on the same or remote agents.An agent with actions but no control loops (i.e., | | &gt; 0 and | | = 0) reduces to an actor.</p>
<p>Agents come in many flavors.Intelligent (deliberative) agents are goal-oriented and reason about what actions to take using internal models and external perception [68].AI agents, a subset of intelligent agents, use AI methods to make decisions or perform actions.In contrast, reactive (observer) agents simply perceive their external environment and react to changes [53].Service agents provide predefined services in response to action requests and come in many forms: resource agents manage and grant access to resources, such as compute or storage, and embodied agents can act in the world, such as through physical actions when paired with a robot body.Learning agents adapt their behavior over time to improve performance, often through reinforcement learning [51].Composite agents exhibit two or more of these behaviors.For example, deliberative learning agents improve their reasoning or planning capabilities over time, and reactive service agents perform services in response to environmental changes.A multi-agent system can enable more complex behaviors than monolithic programs [55], which can lead to powerful emergent behavior [21].</p>
<p>An agentic workflow can be formalized as a graph of actionsrather than tasks, as in typical DAG-based workflows-where agents request and perform actions on behalf of one another, enabling dynamic coordination and the collective pursuit of complex, distributed objectives.Let the environment  represent the external state space, influencing and influenced by the actions of agents and other entities.Agents in the environment are represented by a deployment  (, ) :  →  of agents  ∈  on to resources  ∈ .Each agent implements a behavior, and an agent that knows the behavior of a peer agent can request the peer to perform an action through message passing.Thus, there exists a directed graph representing the peer relationships between the agents; an edge  = (  ,   ) in this graph implies that   knows of   and can request actions from   .Sink nodes in the graph represent agents that only perform atomic actions, whereas source nodes may represent deliberative or reactive agents that trigger actions on other agents.A cut vertex (articulation point) in the graph can represent an agent that serves as an interface or gateway to another, possibly more complex, multi-agent system.</p>
<p>The deployment of agents can execute workflows.An agentic workflow  is modeled as a directed graph where nodes are a tuple (, ) of an action to perform and the agent performing the action, and edges representing the source agent and action that triggered the subsequent action.A workflow is typically implicitly encoded within the agent behaviors of a multi-agent system.I.e., the graph  is not explicitly materialized and agents do not need to know  in order to execute.An agent only needs to be concerned with its local view of executing requested actions and requesting actions from peers.Thus, workflows may often be highly dynamic as agent behaviors react to changing states.</p>
<p>Academy Design</p>
<p>Designing a middleware that can express the diverse demands of scientific applications and leverage federated research infrastructure is challenging.In the design of Academy, we aim to address the following high-level challenges: How to represent, in code, the declaration of and interaction between agents?How to deploy agents across federated infrastructure?How to achieve performance across heterogeneous systems, networks, and storage?Thus, we begin by outlining key requirements, before we introduce the highlevel architecture and detailed implementation choices.The name Academy alludes to societies of artists and scholars that, while independent, collaborate and share similar goals.</p>
<p>Academy Requirements</p>
<p>Writing scientific applications as agentic workflows, rather than using traditional workflow models, can require a considerable shift in conceptual thinking.To reduce developer friction, our design emphasizes familiarity-using well-known programming patternsand simplicity-providing a small set of primitives and inviting users to invent new patterns and techniques.With these principles in mind, we define requirements in four areas:</p>
<p>• Representation: Agent behavior must be expressed in code, supporting control loops, actions that can be performed, and local state.Multiple agents may be instantiated with the same behavior.Agents should not share state.• Interaction: Agents and clients must be able to interact.They must be able to address a specific agent, perform one or more actions, modify local state, or create and terminate agents.• Communication: Agents and clients communicate asynchronously and are temporally decoupled (e.g., a message sent to an offline agent should be read when that agent is next online).</p>
<p>Agents may be deployed in diverse environments with heterogeneous network environments (e.g., asymmetric networks and firewalls restricting connections).• Execution: An agent performs actions in response to requests from clients or other agents.Agent control loops may run in perpetuity or exit before the end of the agent's lifetime.</p>
<p>Agents may be launched using different mechanisms that are dependent on the application or environment.These requirements are an extension of actor systems; therefore, our system inherits properties including simplified concurrency, message processing ordering, loose coupling, error isolation, and modularity [37].</p>
<p>Our implementation focuses on mechanism rather than policy.That is, we discuss how applications can use Academy to achieve certain outcomes without prescribing what agents, or more generally, applications should do.This avoids constraining, or worse, alienating possible use cases and results in a flexible framework suitable for solving many disparate problems.Further, we describe the components within the architecture in terms of abstract interfaces (i.e., without mandating implementation details such as message protocols, state formats, or ordering) to enable further experimentation and optimization, but we still aim to provide implementations that are suitable for most use cases (as demonstrated in the evaluation).Features such as fault tolerance and resilience, resource allocation, and authentication and authorization, while important, are not listed as explicit requirements because applications have varying demands that preclude one-size-fits-all solutions.</p>
<p>Academy Architecture</p>
<p>Academy is a middleware for expressing agentic workflows and deploying multi-agent systems across federated resources.Its architecture strongly decouples the implementation of agent behavior from execution and communication to simplify the development of new agents while maintaining flexibility in deployment.</p>
<p>As depicted at a high level in Figure 2, an Academy deployment includes one or more agents and zero or more clients.An agent is a process that executes a behavior, where a behavior is defined by a local state, a set of actions, and a set of control loops.Agents are  executed remotely using a launcher.Once running, an agent concurrently executes all of its control loops and listens for messages from clients, which can be other agents or programs.</p>
<p>A client interacts with an agent through a handle, a term we borrow from actor frameworks.A handle acts like a reference to the remote agent and translates method calls into action request messages.Each entity (i.e., client or agent) has an associated mailbox that maintains a queue of messages sent to that entity by other entities.Mailboxes are maintained by an exchange such that any client with access to a given exchange can send messages to the mailbox of another agent in the exchange and receive a response through its own mailbox.</p>
<p>Academy Implementation Details</p>
<p>Academy is implemented as an open-source Python library, available on GitHub and PyPI. 1 We target Python for its broad compatibility with scientific workflow codes and libraries, but both the architecture and individual components could be implemented in other languages.</p>
<p>3.3.1</p>
<p>Behavior.An agent behavior is implemented as a Python class that inherits from the base Behavior type, as shown in Listing 1.This class-based approach is simple, so existing code can be easily transformed into agents, and extensible through inheritance and polymorphism.Instance attributes maintain the agent's state, and methods define the actions and control loops.</p>
<p>The @action decorator marks a method as an action, allowing other entities to invoke it remotely.(In the future, we plan to support adding metadata to the @action behavior to aid discovery discussed in Section 3.3.4.)A behavior can invoke actions on itself, as actions are simply Python methods.Methods not decorated as @action are private to the behavior.The @loop decorator marks a method as a control loop.Control loops are executed in separate threads, so a shared threading.Event is passed as an argument to each loop that signals when the agent is shutting down so that control loops can gracefully exit.A control loop can terminate early and the agent will remain running.Commonly, control loops are used to execute a routine on a regular interval, such as to check the state of the environment, or in response to an event.We provide two special control loop decorators, @timer and @event, that simplify behavior implementations for these scenarios.</p>
<p>Two special methods, on_setup() and on_shutdown(), allow behaviors to define callbacks when starting or shutting down, such as to load/store state or initialize/destroy resources.Multiple inheritance of behaviors enables the creation of composite agents.</p>
<p>3.3.2</p>
<p>Agent.An Agent is a multithreaded entity that executes a behavior and manages communication with other entities.It is instantiated with a behavior, unique identifier (the address of the agent's mailbox in the exchange), and exchange interface.An agent is a callable object that when run: (1) invokes the on_setup() callback of the behavior, (2) starts each @loop method in a separate thread, (3) spawns a thread to listen for new messages in the agent's mailbox, and (4) waits for the agent to be shut down.An @action method is executed in a thread pool when requested remotely so as to not block the handling of other messages.Behaviors can optionally specify the maximum action concurrency.</p>
<p>Agents are designed to be long-running, but can be terminated by sending a shutdown request.Upon shutdown, the shutdown Event, passed to each @loop, is set; running threads are instructed to shutdown and waited on; and the on_shutdown() callback is invoked.Alternatively, an agent can terminate itself by setting the shutdown event.Similarly, an exception raised in an @loop method will shutdown the agent by default but can optionally be suppressed to keep the agent alive.Exceptions raised when executing @action methods are caught and returned to the remote caller.</p>
<p>The use of multi-threading means that behavior implementations must be aware of the caveats of Python's global interpreter lock (GIL).Compute-heavy actions can dispatch work to other parallel executors, such as process pools, Dask Distributed [60], Parsl [9], or Ray [52].We discuss these patterns in more detail in Section 3.4.4.In the future, we would like to support async behaviors and exchanges for improved I/O performance, but scientific computing libraries in Python are not typically async compatible.In Python 3.13 and later, we provide experimental support for free-threading builds, which disable the GIL, enabling full multi-core performance.At this time, however, third-party library support for free-threading builds is limited.</p>
<p>Our decision to decouple behavior definitions from agent execution is deliberate.As behaviors encode application-level logic, we want them to be easily testable and reusable, independent of deployment details.Existing code bases can trivially transition an existing class definition into an agent by inheriting from Behavior and decorating with @action as needed, and behavior classes can still be used independently (i.e., not as a running agent).</p>
<p>Handles.</p>
<p>Interacting with an agent is asynchronous; an entity sends a message to the agent's mailbox and waits to receive a response message in its own mailbox.A handle is a client interface to a remote agent used to invoke actions, ping, and shutdown the agent.Each handle acts as a reference to that agent, translating each method call into a request message that is sent via the exchange and returning a Future.The handle also listens for response messages and accordingly sets the result on the appropriate Future.Rather than creating a return mailbox and listener thread for each handle that a client or agent may have, Academy will multiplex communication for multiple handles within a single process through a single mailbox.This multiplexing ensures that only one mailbox listener thread is needed per process (i.e., agent or client).</p>
<p>Exchange.</p>
<p>Entities communicate by sending and receiving messages to and from mailboxes.An exchange hosts these mailboxes, and the Exchange protocol defines the interface to an exchange.Namely, the Exchange defines methods for registering new agent or client mailboxes, sending and receiving messages, and creating handles to remote agents.Registering an agent or client involves creating a unique ID for the entity, which is also the address of its mailbox, and initializing that mailbox within the exchange.</p>
<p>A mailbox has two states: open and closed.Open indicates that the entity is accepting messages, even if, for example, an agent has not yet started or is temporarily offline.Closed indicates permanent termination of the entity and will cause MailboxClosedError to be raised by subsequent send or receive operations to that mailbox.</p>
<p>Exchanges also provide mechanisms for agent discovery by querying based on agent behaviors.This also works with superclasses of behaviors.Consider, for example, a behavior Protein-Folder that can fold proteins [5] and another behavior OpenProt-einFolder that inherits from ProteinFolder and specifically uses the OpenFold model [2].Querying for ProteinFolder would return the IDs of all agents inheriting from ProteinFolder whereas querying for OpenProteinFolder would return only specific agents using the OpenFold model.In the future, agents could provide additional metadata to enhance discovery.</p>
<p>Users can define custom exchanges to address specific hardware or application characteristics.We provide two exchange implementations for local and distributed agent deployments.The thread exchange stores messages in-memory and is suitable for agents running in separate threads of a single process, such as when testing.</p>
<p>The distributed exchange enables communication between entities across wide-area networks.Core to the distributed exchange is an object store that persists information about registered entities.A hybrid approach is used for message passing: direct messaging is preferred, and indirect message passing via the object store is available as a fallback.Upon startup, an entity writes its location (i.e., address and port) to the object store; peers that want to send a message can attempt to send directly to the entity's address.If the peer is offline or a direct connection fails, such as in the presence of NAT (network address translation) or firewall restrictions, messages are appended to the list of pending messages in the object store.Entities continuously listen to incoming messages from peers and pending messages in the object store.Entities cache successful communication routes locally to reduce queries to the object store.Our implementation use TCP (transmission control protocol) sockets for direct messaging and a Redis server as the object store.Redis provides low-latency communication and optional replication, but applications that need greater fault-tolerance could consider DHT-based (distributed hash table) object stores.</p>
<p>We optimize the exchange for low latency, as control messages are typically small: (100) bytes.However, action request and response messages can contain arbitrarily sized serialized values for arguments and results that can induce considerable overheads when messages are sent indirectly via the object store.To alleviate these overheads, we pass large values by reference and perform out-of-band data transfers by using ProxyStore [57,58], which provides pass-by-reference semantics in distributed computing through proxy objects.Proxy objects act like references (cheap to serialize and communicate) and automatically de-reference themselves to the true object using performant data storage and communication methods.For example, ProxyStore can leverage RDMA (remote direct memory access) transfers via Mochi [61] and UCX [62], GridFTP via Globus Transfer [17], and reliable peer-to-peer UDP (user datagram protocol) through NAT hole-punching.ProxyStore also provides two key optimizations useful within Academy: proxies can be forwarded to actions executed on other agents without incurring additional data transfers and proxies can be asynchronously resolved to overlap communication and computation.</p>
<p>3.3.5</p>
<p>Launcher.An agent can be run manually, but the intended method of execution is via a launcher, which manages the initialization and execution of agents on remote resources.The Launcher protocol defines a launch() method with parameters for the behavior, exchange, and agent ID and returns a handle to the launched agent.Users can create custom implementations; we provide the following four that cover most use cases:</p>
<p>• Thread: Runs agents in separate threads of the same process.Useful for local development and testing or for light-weight or I/O bound agents.• Process: Runs agents in separate processes on same machine.</p>
<p>• Parsl: Runs agents across the workers of a Parsl Executor [9].</p>
<p>Parsl supports execution across local, remote, and batch compute systems.• Globus Compute: Runs agents across Globus Compute Endpoints [18].Globus Compute is a cloud-managed function-asa-service (FaaS) platform which can execute Python functions across federated compute systems.The last three launchers support mechanisms to automatically restart agents if they exit unexpectedly.It is common for different agents in an application to be executed with different launchers, but all agents must be registered to the same exchange to interact.</p>
<p>A Manager combines an exchange and one or more launchers to provide a single interface for launching, using, and managing agents.Each manager has a single mailbox in the exchange and multiplexes that mailbox across handles to all of the agents that it manages.This reduces boilerplate code, improves communication efficiency, and ensures stateful resources and threads are appropriately cleaned up.An end-to-end example is provided in Listing 2.</p>
<p>Common Patterns</p>
<p>We have introduced basic building blocks necessary to build multiagent systems and deploy agents across federated infrastructure.Now we discuss several common patterns that highlight features of Academy and guide users in building new agentic workflows.</p>
<p>State</p>
<p>Checkpoints.Research infrastructure can fail; thus, agents may want to perform periodic state checkpointing.The framework does not enforce a specific checkpointing mechanism, as the format, location, and frequency of checkpoints are highly application specific, but on_startup() callbacks can be used to restore state automatically.For convenience, we provide a State API that provides a dictionary-like interface and persists values to the local file system.</p>
<p>Migration.</p>
<p>Research infrastructure is typically static, so Academy does not require that the launcher provide mechanisms for automatic agent migration.Some launchers, such as Parsl, will restart agents on different workers if node-level failures cause agents to crash.Applications can also manually migrate agents across different launchers using agent shutdown and checkpointing mechanisms.These features are sufficient for users to implement custom launchers that enable automatic migration, such as to load-balance across resource pools.</p>
<p>Agent Hierarchies.</p>
<p>Agents may dynamically need to create and manage child agents, either to offload tasks or to access new behaviors.A parent agent can create new child agents by using the same launcher used to create the parent, or by creating a new launcher.The use of different launchers is common in scenarios where parent agents want to initialize a local multi-agent system.For example, a client may launch an initial set of agents across federated resources using Globus Compute, and then those initial agents spawn more agents on local resources through Parsl.</p>
<p>Resource Pools.</p>
<p>High-performance workflows may need to distribute work across many computers.In an agentic model, resource pool allocation can take two forms: agent managed resource pools or agents as resource pools.In the former, an agent allocates a pool of resources using a parallel computing framework, such as Parsl or Ray, and the agent's actions dispatch work to resources in the pool.In the second pattern, we deploy identical agents across a set of resources and then route action requests across this agent pool (akin to worker pools in HTTP frameworks).</p>
<p>3.4.5Process-as-a-Service. FaaS systems, such as Globus Compute, provide optimized execution of short-lived, stateless, and ephemeral functions.Academy agents can extend FaaS systems with processas-a-service capabilities [19], enabling applications to utilize longerlived, stateful, and isolated processes on-demand.</p>
<p>Evaluation</p>
<p>We studied the performance characteristics of Academy to answer key questions including: How well does the system scale?How fast can agents be deployed?What is the messaging latency?We also make comparisons to Dask and Ray, two popular frameworks with support for distributed actors in Python.Although Academy agents provide a superset of features provided by actors, these evaluations contextualize the performance of the framework.In these comparisons, we use the terms agent and actor interchangeably.</p>
<p>We conducted experiments using the Aurora supercomputer at the Argonne Leadership Computing Facility (ALCF), unless otherwise stated.Aurora has 10 624 nodes interconnected by an HPE Slingshot 11 network and a high performance DAOS storage system.Aurora nodes contain two Intel Xeon Max CPUs, each with 52 physical cores and 64 GB of high-bandwidth memory; 512 GB of DDR5 memory per socket; and six 128 GB Intel Data Center Max GPUs.In some cases we also use the Polaris supercomputer at ALCF and the compute-zen-3 nodes of Chameleon Cloud's CHI@TACC cluster [42].Polaris has 560 nodes interconnected by an HPE Slingshot 11 network.Polaris nodes contains one AMD EPYC Milan processor with 32 physical cores, 512 GB of DDR4 memory, and four 40 GB NVIDIA A100 GPUs.Each compute-zen-3 node contains two 64-core CPUs and 256 GB memory.Experiments were performed using Python 3.10, AutoGen 0.5.1,Dask 2025.2.0, Globus Compute 3.5.0,Parsl 2025.03.03, and Ray 2.43.0.</p>
<p>Weak Scaling</p>
<p>We measure weak scaling performance from two aspects: agent startup and action completion time.The object store of the exchange is located on the head node of the Aurora batch job to best match the behavior of Dask and Ray.</p>
<p>Agent Startup Time.</p>
<p>We measure the time to spawn  agents in Figure 3 (top).We pre-warm the worker processes by starting and stopping  agents, then record the average startup time over five runs.Specifically, we measure the time between submitting the first agent to receiving a ping from all agents to ensure that they have finished their startup sequence.We configured Academy to use Parsl's High-throughput Executor as the launcher.Ray always spawns a new process per actor and thus does not benefit from pre-warmed workers leading to high startup overheads at smaller  scales.The cold start time with Academy and Dask is comparable to that of Ray and dominated by loading libraries from the shared file system.With warm starts, Academy starts a single actor in 5.5 ms, 2.8× faster than Dask.Academy scales well, starting 3328 actors in 7.6 s compared to Dask's 23.4 s, but Ray demonstrates an advantage at this scale with a 3.2 s startup.Since Academy can leverage many launcher types, applications requiring frequent startup of agents can utilize Parsl for low-latency, and applications launching thousands of long-running agents could use Ray.</p>
<p>Action Completion Time.</p>
<p>In Figure 3 (bottom), we execute 30 sleep tasks (1 s) per agent and record the total completion time.We set the maximum concurrency to 1 for all agents to ensure that tasks are processed sequentially.Completion time remains constant for Academy and Ray up to 3328 agents while the performance of Dask degrades starting at 104 actors.</p>
<p>Academy Distributed Exchange</p>
<p>Next, we study the performance of the distributed exchange.</p>
<p>Data Transfer. We first investigate the pass-by-reference and direct communication optimizations of the distributed exchange.</p>
<p>In baseline, all message data are communicated indirectly between peers via the exchange's object store.The object store is located remotely on a Chameleon Cloud node.In pass-by-ref, messages are still communicated with the object store, but action arguments and results are replaced with references using ProxyStore.ProxyStore is configured to use ZeroMQ and ProxyStore's P2P endpoints for intra-site and inter-site transfer of referenced objects, respectively.In direct, messages are communicated directly between peers, circumventing the cloud-hosted object store; this is only possible when peers are located within the same site.In Figure 4 (top), we measure the time it takes for a client to invoke a no-op action on an agent as a function of input and output payload size.We compare baseline, pass-by-ref, and direct across two scenarios: Aurora → Aurora, where the client and the agent are located on two different Aurora nodes, and Workstation → Aurora, where the client is located on a personal workstation and the agent is located on an Aurora node.The latencies between the three sites are Aurora to Chameleon: 31 ms; Aurora to Workstation: 12 ms; and Workstation to Chameleon: 42 ms.The workstation is limited to an 800 Mbps internet connection.</p>
<p>We observe that network latency to the exchange object store limits performance at smaller payload sizes (≤ 100 KB).Direct, which is possible only in the intra-site scenario, circumvents these latencies.In both scenarios, pass-by-ref alleviates overheads of data transfer to and from the object store by communicating data directly between the client and agent via ProxyStore.For intra-site transfers, pass-by-ref and direct reduce action latency compared to the baseline by 91.2% and 97.6%, respectively, with 100 MB payloads.For inter-site transfers, pass-by-ref reduces action latency by 78.8%.</p>
<p>Pass-by-ref also reduces overheads when actions pass data to subsequent actions, a common pattern in multi-agent systems.We evaluated this optimization by measuring the round-trip time of action chains in which data are passed through  actions, each invoked on a separate agent, and results are returned through each agent as well.Pass-by-ref reduces the size of messages communicated via the exchange, as indicated by the shallower slope in Figure 4 (bottom).Data are only communicated once to the agent that uses the data (here, the last agent in the chain).</p>
<p>Handle Multiplexing.</p>
<p>As described in Section 3.3.3,the communication of multiple handles within a process is multiplexed through a single mailbox.Without this optimization, each handle held by a client process or agent would create a thread for communication.We evaluated this optimization in Figure 5 by creating one agent that submits a bag-of-tasks to  worker agents and comparing the task throughput with (multiplex) and without (baseline) mailbox multiplexing.Multiplexing improves throughput by 41.7% with 52 worker agents due to reduced threading overheads.</p>
<p>Agent Messaging</p>
<p>Here, we investigate the performance of agent messaging.As in Section 4.1, the object store of the exchange is located on the head node of the Aurora batch job.</p>
<p>Action Latency.</p>
<p>In Figure 6 (top), we show action latencythe time between sending an action request and receiving a resultbetween two agents on different nodes.We vary the input/output payload size to understand data transfer overheads.The mean and</p>
<p>Action</p>
<p>Throughput.We measure the maximum action throughput for a single agent by submitting a bag of no-op tasks to a pool of worker agents (following the "agents as process pools" pattern from Section 3.4.4).The pool contains 208 agents across four nodes to ensure that each worker agent is not over-saturated with work.That is, the single submitter agent is the limiting factor for performance.Academy, Dask, and Ray achieve maximum throughputs of 3.4K, 185, and 14.1K action/s, respectively.Academy is 18× faster than Dask but 4× slower than Ray; however, this is a worst case scenario with no-op tasks and we believe &gt;3K actions/s to be sufficient for real-world agents.</p>
<p>Agent Conversations.</p>
<p>In Figure 6 (bottom), we simulate a common pattern in LLM agents where two agents have a back-andforth conversation.We compare Academy to AutoGen, a popular framework for creating multi-agent AI applications.Each agent is run in a different process on the same node.Agents send ten messages back-and-forth, and we repeat with varying message sizes to simulate different kinds of conversations (i.e., text-only versus multi-modal).AutoGen's distributed agent runtime uses gRPC which has a maximum message size of 4 MB.Academy has comparatively lower overhead messaging in distributed settings.</p>
<p>Memory Overhead</p>
<p>We show memory used as a function of number of agents in Figure 7; for Academy, we compare two launchers: a low-overhead but single-node process-pool and Parsl's High-throughput Executor.</p>
<p>For fairness, we disable features in Dask and Ray that may reduce performance, such as dashboards, and set the initial Ray object store size to the smallest possible value.Academy agents have low memory overheads, making them suitable for memory-constrained devices, such as when deployed across edge devices via the Globus Compute launcher.The Ray cluster head worker has high memory overhead, but that initial overhead is amortized as the number of actors is increased, indicating that each actor has modest overhead.</p>
<p>Case Studies</p>
<p>We use three applications to demonstrate the practicality, generality, and robustness of our system in real-world settings.These examples illustrate how Academy integrates with existing research infrastructure, supports a range of computational patterns, and adapts to the varying demands of scientific applications.They validate key design choices, uncover integration challenges, and provide guidance to researchers building agentic workflows.</p>
<p>Materials Discovery</p>
<p>MOFA [71] is an online learning application for generating, screening, and evaluating metal organic frameworks (MOFs) that couples generative AI methods with computational chemistry.MOFs are polymers composed of inorganic metal clusters and organic ligands; their porosity and large surface area make them suitable for gas adsorption applications such as carbon capture [29].The goal of MOFA is to generate high-performing candidates by intelligently navigating the intractable combinatorial space of possible MOF structures.MOFA is representative of a broad class of scientific workflows that require careful integration of heterogeneous tasks spanning AI and simulation.MOFA involves five stages: (1) a generative AI model produces candidate ligands; (2) these ligands are combined with predefined metal clusters to assemble candidate MOFs; (3) the candidates undergo iterative screening and validation using a series of molecular dynamics simulations; (4) CO 2 adsorption properties of the most promising structures are simulated and recorded in a database; and (5) the generative model is periodically retrained on the accumulated results to enhance its performance over time.MOFA utilizes Colmena [66] to coordinate the flow of data between stages and to distribute computations across CPU and GPU resources within a single batch job.However, this design has key limitations: stages cannot be deployed across heterogeneous resources, such as to leverage hardware best optimized for the specific computations; stages cannot independently scale in or out-resources are bound by the size of a single job; integrating new components within tightly coupled code is challenging; and integration with asynchronous processes, such as synthesis in a real laboratory, are infeasible.</p>
<p>MOFA is an excellent candidate for an agentic workflow, as we demonstrate by porting MOFA to use Academy and deploying the workflow across federated resources: see Figure 8.We express MOFA through six agents: Database, Generator, Assembler, Validator, Optimizer, and Estimator.Each agent is responsible for a different component of the workflow and manages its own resources (i.e., storage and compute).Agents are remotely deployed across Chameleon Cloud nodes and the login nodes of Aurora and Polaris via Globus Compute, and communicate via the distributed exchange backed by a Redis instance in Chameleon Cloud.</p>
<p>An execution trace of the agentic MOFA workflow (Figure 9) shows how each agent scales out its allocated resources as work becomes available, and in the case of Assembler and Estimator, scale down when their workload decreases.The Generator, Validator, and Optimizer consistently have work to do but their batch jobs within which workers run have 60 minute wall times that expire and then must be resubmitted, causing temporary drops the the number of workers.Active tasks that are killed are automatically restarted in the next job.This separation of concerns is key to enabling long-running workflows-resource infrastructure is not persistently available and agents will need to be able to adapt to that varying availability.A second benefit of this model is the loose coupling between agents.For example, the specific implementation of a given agent can be trivially swapped provided the behavior (i.e., the API that agents expose) remains the same.In addition, it becomes easier to integrate future agents, such as to incorporate embodied agents that interact with self-driving labs to synthesize and evaluate the best-performing MOFs in the real-world.While automated MOF synthesis is not yet practical, the capabilities of self-driving labs are rapidly improving [1,64], and it is tangible to envision a future where these loosely coupled agentic workflows incorporate services provided by self-driving labs through embodied agents.</p>
<p>Decentralized Learning</p>
<p>In decentralized machine learning a set of models learn collaboratively across distributed datasets [36].This paradigm is particularly relevant today as data are increasingly generated in decentralized settings and transfer to a centralized location can be infeasible for cost and privacy reasons.Each device in a decentralized learning workflow performs three steps: (1) train a model on local data for a set number of iterations; (2) receive models from neighboring devices and send its own model to neighbors; and (3) update the local model via an all reduce operation performed across its own and received models.Reframing the decentralized learning workflow as an agentic workflow is a natural and powerful extension.</p>
<p>We implement a decentralized and asynchronous machine learning exemplar using Academy.The agents and the communication channels between them can be represented as a graph where nodes are agents and edges are communication channels.We choose a powerlaw cluster graph to approximate real-world networks [38].Each agent is responsible for training its local model, receiving neighboring agents' models, and aggregating received models with its own model on a periodic basis.Each agent uses a copy of the MNIST dataset [24].The agents are configured to use pass-by-ref with ProxyStore as the transfer backend.Thus data communication between agents follows the network topology.We investigate the cost of distributing updates from all agents as we scale the size of the graph for different model sizes in Figure 10.We do not show  training and aggregation time as it is approximately the same for all model sizes and does not increase with the number of agents.The agents are deployed on Aurora using Parsl, where each agent is pinned to a single GPU tile (two tiles per physical GPU), allowing 12 agents per node.Our results demonstrate Academy's ability to support more than 1500 autonomous agents working collaboratively with no client coordination (as can be seen by the constant time in Figure 10).</p>
<p>Information Extraction</p>
<p>Exponential growth in scientific publications [12] creates potential for cross-disciplinary insights that are largely untapped due to the limitations of manual literature review.Automating information extraction from this vast and varied body of work using AI is crucial to accelerate scientific progress.AI methods can be employed to identify and synthesize key findings, methodologies, and datasets across fields and thus to identify connections and facilitate the crosspollination of ideas that would otherwise go unnoticed [14,63].</p>
<p>Agentic workflows that leverage LLMs present a transformative new approach to engage with scientific literature.Employing autonomous agents with specific roles and capabilities makes it possible to automate the extraction of information and generation of structured datasets that represent key concepts and findings.Such datasets can be used to fine-tune models and enhance their ability to understand scientific text, answer domain-specific queries, and potentially contribute to tasks like hypothesis generation or literature summarization.</p>
<p>To explore the potential of agentic workflows for thus analyzing the scientific literature we used Academy to implement a system for generating and validating multi-choice questions (MCQs) from research publications [15,16].The workflow includes a PDFParser agent to extract text from a manuscript; two Generator agents that use different LLMs to generate MCQs; an MCQSelector to choose subsets of questions to evaluate; and two MCQAnswerers and two AnswerScorers (again, each with a different LLM) to generate and validate, respectively, answers to questions.The agents use the Mistral-7B-Instruct-v0.3 [41] and Meta-Llama-3-70B-Instruct [8] models, denoted A and B, respectively.</p>
<p>The beauty of this architecture is that alternative tasks and LLMs are easily integrated by defining new agents; agents can scale up and down in response to demand; and different agents can run concurrently or at different times.We show in Figure 11 an execution trace from a run in which the agents just listed were run concurrently to generate and validate MCQs for 10 publications.</p>
<p>Related Work</p>
<p>A workflow is a structured sequence of tasks, typically a directed acyclic graph (DAG), designed to achieve a specific goal, often involving data transformation, analysis, or computational modeling.Frameworks for building workflows take many forms.Parallel computing libraries, such as Dask [60] and Ray [52], provide mechanisms for executing functions in parallel across local resources or distributed systems.Similarly, workflow management systems (WMSs) can execute tasks in parallel but also provide mechanisms for defining, optimizing, and monitoring DAG execution (e.g., Airflow [7], Fireworks [40], Makeflow [3], Nextflow [25], Parsl [9], Pegasus [23], Swift [67]).WMSs can be differentiated by how dependency graphs are defined [56]: static configurations files, such as CWL [20], XML, or YAML; general purpose languages (GPLs); domain specific languages (DSLs); or procedurally through the dynamic execution of a program.The class of workflows supported by these frameworks have two key limitations that we address: tasks are assumed to be pure (i.e., no side-effects) and programs are static, i.e., they cannot adapt to changing environments over time.</p>
<p>Actors are computational entities that enable concurrent computing through message passing [37].In response to a message, an actor can alter its local state, send messages to other actors, and create new actors.No global state means locks and other synchronization primitives are not required.Actors can enable stateful computations within traditionally stateless programming models, and are supported in parallel computing frameworks (e.g., Akka [47], Dask, Orleans [10], Ray) and function-as-a-service (FaaS) platforms (e.g., Abaco [31], Azure Service Fabric [49], PraaS [19]).Actor models have been investigated as alternatives for designing computational workflows where communication and coordination are decoupled [13].Our system extends the actor model to support autonomous behaviors and federated deployments.</p>
<p>Multi-agent systems can enhance or automate scientific processes.Early work investigated cooperative agent environments for distributed problem solving with minimal human intervention [26,27].Recent work focuses on improving the reasoning capabilities of LLM-backed agents through ontological knowledge graphs and multi-agent systems [32] and tool-augmented LLMs [48].Increasingly popular is the use of multi-agent conversations, in which multiple role-specialized agents interact to collaborate, coordinate, or compete towards goals [69].These systems enhance LLM-based tools through better reasoning [28], validation [70], and divergent thinking [46], prompting rapid development of frameworks such as LangGraph [44], Microsoft AutoGen [69], OpenAI Swarm [54], and Pydantic Agents [59].Subsequently, interest in standardizing agent protocols has developed.Anthropic's Model Context Protocol (MCP) [6] defines structured interaction between humans/tools and AI models.Google's Agent2Agent (A2A) Protocol [34] focuses on structured interaction between autonomous agents; each agent serves an HTTP endpoint which is impracticable for many scientific workflows.Multi-agent conversations can proxy scientists in iterative scientific processes-brainstorming ideas, planning experiments, and reasoning about results [11,30,35,65]-but these aforementioned systems are designed for local or cloud-native applications and lack the features necessary to deploy agents across federated research infrastructure.We focus on the systems-level challenges of representing and deploying diverse agent types and agentic workflows across heterogeneous environments rather than the applied use of LLMs for workflow steering.</p>
<p>Conclusion &amp; Future Work</p>
<p>Advancements in AI, coupled with concurrent advancements in selfdriving laboratories, high performance computing, and research data management, open the door for truly autonomous scientific discovery.Realizing this grand vision requires mechanisms for the seamless and dynamic integration of research software and infrastructure.To that end, we introduced Academy, a middleware for developing agentic workflows that engage multi-agent systems spanning federated research infrastructure.This framework enables scalable and flexible orchestration of intelligent agents across heterogeneous resources.We presented solutions to three key challenges: representing and programming agents; communicating among agents; and executing agents across diverse resources.Our evaluations demonstrate that Academy can support high-performance workflows, and three case studies highlight the advantages of agentic workflow design.</p>
<p>In future work, we will explore scoped authentication to control which agents can invoke others, enabling the creation of agent marketplaces where access can be granted, revoked, or delegated.We also plan to expand agent discovery with additional metadata to support AI-steered workflows in which LLMs autonomously identify and use available agents.Recording the relative ordering of agent events (i.e., messages received and state transitions), as in Instant Replay [45], can support provenance within agentic workflows.Last but not least, we will work across scientific research communities to assemble agents for different purposes, and with research facilities to identify obstacles to agent use that may motivate further developments in Academy.</p>
<p>Figure 1 :
1
Figure 1: Cooperative agents, spanning federated research infrastructure (experimental facilities, computational systems, data storage), can enable agentic workflows that autonomously steer discovery.</p>
<p>Figure 2 :
2
Figure 2: Agents and clients in Academy interact via handles to invoke actions asynchronously.Agents implement a behavior, defined by their actions, control loops, and state.Academy decouples the control and data planes through the launcher and exchange components that manage spawning agents and communication, respectively.</p>
<p>1Listing 1 :
1
import time , threading 2 from academy .behavior import Behavior , action , Example agent behavior definition.</p>
<p>1 4 5
4
from academy .exchange .thread import ThreadExchange 2 from academy .launcher .thread import ThreadLauncher 3 from academy .manager import Manager () # Or via the manager 16 manager .shutdown ( handle .agent_id , blocking = True )Listing 2: Example of initialization, spawning, using, and shutting down an agent using the Manager interface.</p>
<p>Figure 3 :
3
Figure 3: (Top) Warm-start time for  agents/actors between Academy (using the Parsl launcher), Dask Actors, and Ray Actors.Ray does not benefit from warm-starts because a new process is spawned for each actor.(Bottom) Time to execute 30 actions per agent/actor (weak scaling).Each action sleeps for 1 s.Note the Academy and Ray lines are overlapped.</p>
<p>Figure 4 :
4
Figure 4: (Top) Time for a client to invoke a no-op action on an actor as a function of input and output payload size with different optimizations enabled on the distributed exchange.Two scenarios are considered: client and agent are at the same site (left) and different sites (right).(Bottom) Time for a client to invoke a chain of  actions across  agents with a payload size of 10 MB.Each action in the chain is a no-op that passes the input data along to the next agent, and returns the resulting data.The pass-by-reference optimization reduces communication costs among intermediate actions.</p>
<p>Figure 5 :
5
Figure 5: Maximum no-op action throughput for a single agent requesting actions from  worker agents.The handle multiplexing optimization improves performance by reducing the number of mailbox listener threads from  to 1.</p>
<p>Figure 6 :
6
Figure 6: (Top) No-op action latency between two agents/actors running on separate Aurora nodes versus action input and output payload size.(Bottom) Completion time for a simulated two agent chat where agents send ten messages back and forth with varied message sizes.Academy is compared to AutoGen's distributed runtime.</p>
<p>Figure 8 :Figure 9 :
89
Figure 8: We execute MOFA by deploying agents across federated infrastructure with Globus Compute.The Assembler, Database, and Estimator run on Chameleon Cloud nodes with fast single-core performance; the Generator and Validator run on Aurora login nodes and execute AI and simulation tasks, respectively, on Aurora compute nodes; and the Optimizer runs on a Polaris login node and executes simulation tasks on Polaris compute nodes.Each agent is responsible for a single MOFA stage, and agents cooperate through message passing, such as to request more work and trigger periodic events.The agents on Aurora and Polaris use Parl to scale resources up and down based on workload needs.</p>
<p>Figure 11 :
11
Figure 11: Execution trace of the agentic MCQ workflow processing 10 manuscripts to generate and validate questions and answers over 15 minutes.The figure shows the active agents and the duration of their tasks.Agents employ either the Mistral-7B-Instruct-v0.3 or Meta-Llama-3-70B-Instruct model, denoted A and B, respectively.</p>
<p>Memory used by  agents/actors.We encountered Ray crashes when deploying 104 actors on a single Aurora node (i.e., all cores on both sockets).standarddeviation roundtrip latencies are 385±301 s in Academy, 1186±1059 s in Dask, and 526±308 s in Ray for the smallest 10 KB payloads, with latencies increasing with payload size.
Memory Used (GB)0.2 0.5 1 2 5 10 20Academy+ProcessPool Academy+ParslDask RaySingle node Lower is better Error bands are std dev1248 Agents/Actors 1632 52104Figure 7:
J. Gregory Pauloski, Yadu Babuji, Ryan Chard, Mansi Sakarvadia, Kyle Chard, and Ian Foster
https://github.com/proxystore/academy
AcknowledgmentsThis research was supported in part by the National Science Foundation under Grants 2004894 and 2209919.We used resources provided by the Argonne Leadership Computing Facility (ALCF), a U.S. Department of Energy (DOE) Office of Science user facility at Argonne National Laboratory supported under Contract DE-AC02-06CH11357, and the Chameleon testbed supported by the National Science Foundation.
The rise of self-driving labs in chemical and materials sciences. Milad Abolhasani, Eugenia Kumacheva, Nature Synthesis. 22023. 2023</p>
<p>OpenFold: Retraining AlphaFold2 yields new insights into its learning mechanisms and capacity for generalization. Gustaf Ahdritz, Nazim Bouatta, Christina Floristean, Sachin Kadyan, Qinghui Xia, William Gerecke, J O' Timothy, Daniel Donnell, Ian Berenberg, Niccolò Fisk, Zanichelli, Nature Methods. 212024. 2024</p>
<p>Makeflow: A portable abstraction for data intensive computing on clusters, clouds, and grids. Michael Albrecht, Patrick Donnelly, Peter Bui, Douglas Thain, 10.1145/2443416.2443417SWEET '12Proceedings of the 1st ACM SIGMOD Workshop on Scalable Workflow Execution Engines and Technologies. the 1st ACM SIGMOD Workshop on Scalable Workflow Execution Engines and TechnologiesScottsdale, Arizona, USA; New York, NY, USA, Article 1Association for Computing Machinery201213</p>
<p>Software as a Service for Data Scientists. Bryce Allen, John Bresnahan, Lisa Childers, Ian Foster, Gopi Kandaswamy, Raj Kettimuthu, Jack Kordas, Mike Link, Stuart Martin, Karl Pickett, Steven Tuecke, 10.1145/2076450.2076468Commun. ACM. 5522012. feb 2012</p>
<p>Principles that govern the folding of protein chains. Christian B Anfinsen, Science. 1811973. 1973</p>
<p>Model Context Protocol (MCP). Anthropic, 2024. Apr 2025</p>
<p>. Apache , 2015. Sep 2024</p>
<p>arXiv:2407.21783[cs.AIThe Llama Team at Meta. 2024. The Llama 3 Herd of Models. </p>
<p>Parsl: Pervasive Parallel Programming in Python. Yadu Babuji, Anna Woodard, Zhuozhao Li, Daniel S Katz, Ben Clifford, Rohan Kumar, Lukasz Lacinski, Ryan Chard, Justin M Wozniak, Ian Foster, Michael Wilde, Kyle Chard, 10.1145/3307681.3325400Proceedings of the 28th International Symposium on High-Performance Parallel and Distributed Computing. the 28th International Symposium on High-Performance Parallel and Distributed ComputingPhoenix, AZ, USA; New York, NY, USAAssociation for Computing Machinery2019HPDC '19)</p>
<p>Orleans: Distributed Virtual Actors for Programmability and Scalability. Phil Bernstein, Sergey Bykov, Alan Geller, Gabriel Kliot, Jorgen Thelin, MSR-TR-2014-412014MicrosoftTechnical Report</p>
<p>Autonomous chemical research with large language models. Robert Daniil A Boiko, Ben Macknight, Gabe Kline, Gomes, Nature. 6242023. 2023</p>
<p>Growth rates of modern science: A latent piecewise growth curve approach to model publication numbers from established and new literature databases. Robin Lutz Bornmann, Rüdiger Haunschild, Mutz, Humanities and Social Sciences Communications. 82021. 2021</p>
<p>Actor-Oriented Design of Scientific Workflows. Shawn Bowers, Bertram Ludäscher, Conceptual Modeling -ER 2005. Christian Kop, Heinrich C Mayr, John Mylopoulos, Oscar Pastor, Lois Delcambre; Berlin Heidelberg; Berlin, HeidelbergSpringer2005</p>
<p>The application of artificial intelligence technologies as a substitute for reading and to support and enhance the authoring of scientific review articles. Rüdiger Buchkremer, Alexander Demund, Stefan Ebener, Fabian Gampfer, David Jägering, Andreas Jürgens, Sebastian Klenke, Dominik Krimpmann, Jasmin Schmank, Markus Spiekermann, Michael Wahlers, Markus Wiepke, IEEE access. 72019. 2019</p>
<p>Creating and Scoring Multiple Choice Questions (MCQs) from Papers. Charlie Catlett, Ian Foster, 2025. Mar 2025</p>
<p>Automatic multiple choice question generation from text: A survey. Dhawaleswar Rao, Ch , Sujan Kumar Saha, IEEE Transactions on Learning Technologies. 132018. 2018</p>
<p>Efficient and Secure Transfer, Synchronization, and Sharing of Big Data. Kyle Chard, Steven Tuecke, Ian Foster, 10.1109/MCC.2014.52IEEE Cloud Computing. 12014. 2014</p>
<p>funcX: A Federated Function Serving Fabric for Science. Ryan Chard, Yadu Babuji, Zhuozhao Li, Tyler Skluzacek, Anna Woodard, Ben Blaiszik, Ian Foster, Kyle Chard, 10.1145/3369583.3392683Proceedings of the 29th International Symposium on High-Performance Parallel and Distributed Computing. the 29th International Symposium on High-Performance Parallel and Distributed ComputingStockholm, Sweden; New York, NY, USAAssociation for Computing Machinery2020HPDC '20)</p>
<p>Marcin Copik, Alexandru Calotoiu, Rodrigo Bruno, Gyorgy Rethy, Roman Böhringer, Torsten Hoefler, Process-as-a-Service: Elastic and Stateful Serverless with Cloud Processes. ess-as-a-Service: Elastic and Stateful Serverless with Cloud essesETH Zürich2022Technical Report</p>
<p>Methods included: standardizing computational reuse and portability with the Common Workflow Language. R Michael, The CWL CommunitySanne Crusoe, The CWL CommunityAlexandru Abeln, The CWL CommunityPeter Iosup, The CWL CommunityJohn Amstutz, The CWL CommunityNebojša Chilton, The CWL CommunityHervé Tijanić, The CWL CommunityStian Ménager, The CWL CommunityBogdan Soiland-Reyes, The CWL CommunityCarole Gavrilović, The CWL CommunityGoble, The CWL Community10.1145/3486897Commun. ACM. 6562022. May 2022</p>
<p>On the mathematics of emergence. Felipe Cucker, Steve Smale, Japanese Journal of Mathematics. 22007. 2007</p>
<p>Workflows and e-Science: An overview of workflow system features and capabilities. Ewa Deelman, Dennis Gannon, Matthew Shields, Ian Taylor, Future Generation Computer Systems. 252009. 2009</p>
<p>Pegasus, a workflow management system for science automation. Ewa Deelman, Karan Vahi, Gideon Juve, Mats Rynge, Scott Callaghan, Philip J Maechling, Rajiv Mayani, Weiwei Chen, Rafael Ferreira Da Silva, Miron Livny, Kent Wenger, 10.1016/j.future.2014.10.008Future Generation Computer Systems. 462015. 2015</p>
<p>The MNIST Database of Handwritten Digit Images for Machine Learning Research. Li Deng, IEEE Signal Processing Magazine. 292012. 2012</p>
<p>Nextflow enables reproducible computational workflows. Paolo Di Tommaso, Maria Chatzou, Evan W Floden, Pablo Prieto Barja, Emilio Palumbo, Cedric Notredame, Nature Biotechnology. 352017. 2017</p>
<p>Networked agents for scientific computing. Tzvetan Drashansky, Elias N Houstis, Naren Ramakrishnan, John R Rice, Commun. ACM. 42481999. 1999</p>
<p>SciAgents-an agent based environment for distributed, cooperative scientific computing. Anupam Tzvetan T Drashansky, John R Joshi, Rice, 7th IEEE International Conference on Tools with Artificial Intelligence. IEEE1995</p>
<p>Improving Factuality and Reasoning in Language Models through Multiagent Debate. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, Igor Mordatch, arXiv:2305.14325[cs.CL2023</p>
<p>The chemistry and applications of metal-organic frameworks. Hiroyasu Furukawa, Kyle E Cordova, Omar M Michael O'keeffe, Yaghi, Science. 34112304442013. 2013</p>
<p>Empowering biomedical discovery with AI agents. Shanghua Gao, Ada Fang, Yepeng Huang, Valentina Giunchiglia, Ayush Noori, Jonathan Richard Schwarz, Yasha Ektefaie, Jovana Kondic, Marinka Zitnik, 10.1016/j.cell.2024.09.022Cell. 1872024. 2024</p>
<p>Christian Garcia, Joe Stubbs, Julia Looney, Anagha Jamthe, Mike Packard, Abaco-A Modern Platform for High Throughput Parallel Scientific Computations. 2020</p>
<p>SciAgents: Automating Scientific Discovery Through Bioinspired Multi-Agent Intelligent Graph Reasoning. Alireza Ghafarollahi, Markus J Buehler, Advanced Materials. 24135232024. 2024</p>
<p>Formalizing properties of agents. Richard Goodwin, Journal of Logic and Computation. 51995. 1995</p>
<p>Agent2Agent Protocol (A2A). Google, 2025. Apr 2025</p>
<p>Accelerating scientific breakthroughs with an AI coscientist. 2025. Feb 2025Google Research</p>
<p>Gossip learning as a decentralized alternative to federated learning. István Hegedűs, Gábor Danner, Márk Jelasity, Distributed Applications and Interoperable Systems: 19th IFIP WG 6.1 International Conference, DAIS 2019, Held as Part of the 14th International Federated Conference on Distributed Computing Techniques. Kongens Lyngby, DenmarkSpringer2019. 2019. June 17-21, 201919</p>
<p>A universal modular ACTOR formalism for artificial intelligence. Carl Hewitt, Peter Bishop, Richard Steiger, Proceedings of the 3rd International Joint Conference on Artificial Intelligence. the 3rd International Joint Conference on Artificial IntelligenceStanford, USA; San Francisco, CA, USAMorgan Kaufmann Publishers Inc1973IJCAI'73)</p>
<p>Growing scale-free networks with tunable clustering. Petter Holme, Beom Jun, Kim , Physical review E. 65261072002. 2002</p>
<p>Autonomous LLM-Driven Research-from Data to Human-Verifiable Research Papers. Tal Ifargan, Lukas Hafner, Maor Kern, Ori Alcalay, Roy Kishony, NEJM AI. 212025. 2025. AIoa2400555</p>
<p>Anubhav Jain, Ping Shyue, Wei Ong, Bharat Chen, Xiaohui Medasani, Michael Qu, Miriam Kocher, Guido Brafman, Gian-Marco Petretto, Geoffroy Rignanese, Daniel Hautier, Kristin A Gunter, Persson, FireWorks: A dynamic workflow system designed for high-throughput applications. Concurrency and Computation: Practice and Experience. 2015. 201527</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, arXiv:2310.06825[cs.CLMistral 7B. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023</p>
<p>Lessons Learned from the Chameleon Testbed. Kate Keahey, Jason Anderson, Zhuo Zhen, Pierre Riteau, Paul Ruth, Dan Stanzione, Mert Cevik, Jacob Colleran, S Haryadi, Cody Gunawi, Joe Hammock, Alexander Mambretti, François Barnes, Alex Halbach, Joe Rocha, Stubbs, USENIX Annual Technical Conference. USENIX Association. 2020</p>
<p>PaperQA: Retrieval-Augmented Generative Agent for Scientific Research. Jakub Lála, O' Odhran, Aleksandar Donoghue, Sam Shtedritski, Samuel G Cox, Andrew D Rodriques, White, 2023</p>
<p>. Langchain, 2024. Jan 2025</p>
<p>Debugging parallel programs with instant replay. Thomas Leblanc, John Mellor-Crummey, IEEE Trans. Comput. 1001987. 1987</p>
<p>Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, Zhaopeng Tu, 10.18653/v1/2024.emnlp-main.992Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Mohit Al-Onaizan, Yun-Nung Bansal, Chen, the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USA2024Association for Computational Linguistics</p>
<p>Akka: the Actor Model on the JVM. Lightbend, 2009. Sep 2024</p>
<p>Yubo Ma, Zhibin Gou, Junheng Hao, Ruochen Xu, Shuohang Wang, Liangming Pan, Yujiu Yang, Yixin Cao, Aixin Sun, Hany Awadalla, arXiv:2402.11451SciAgent: Tool-augmented language models for scientific reasoning. 2024. 2024arXiv preprint</p>
<p>Azure: Service Fabric Reliable Actors. 2017. Sep 2024Microsoft</p>
<p>Office of Science. L William, Deborah Miller, Amber Bard, Kjiersten Boehnlein, Chin Fagnan, Eric Guok, Sreeranjani Lançon, Mallikarjun Ramprakash, Nicholas Shankar, Benjamin L Schwarz, Brown, 10.2172/1984466Integrated Research Infrastructure Architecture Blueprint Activity (Final Report. 2023. 2023US Department of Energy (USDOE) ; Lawrence Berkeley National Laboratory (LBNL)Technical ReportUnited States</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Nature. 5182015. 2015Daan Wierstra andShane Legg, and Demis Hassabis</p>
<p>Ray: A distributed framework for emerging AI applications. Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael I Jordan, Ion Stoica, Proceedings of the 13th USENIX Conference on Operating Systems Design and Implementation. the 13th USENIX Conference on Operating Systems Design and ImplementationCarlsbad, CA, USAUSENIX Association, USA2018</p>
<p>Software agents: An overview. Hyacinth S Nwana, 10.1017/S026988890000789XThe Knowledge Engineering Review. 111996. 1996</p>
<p>Cooperative multi-agent learning: The state of the art. Liviu Panait, Sean Luke, Autonomous agents and multi-agent systems. 112005. 2005</p>
<p>TaPS: A Performance Evaluation Suite for Task-based Execution Frameworks. J Gregory Pauloski, Valerie Hayot-Sasson, Maxime Gonthier, Nathaniel Hudson, Haochen Pan, Sicheng Zhou, Ian Foster, Kyle Chard, 10.1109/e-Science62913.2024.10678702IEEE 20th International Conference on e-Science. New York, NY, USAIEEE2024</p>
<p>Object Proxy Patterns for Accelerating Distributed Applications. J Gregory Pauloski, Valerie Hayot-Sasson, Logan Ward, Alexander Brace, André Bauer, Kyle Chard, Ian Foster, 10.1109/TPDS.2024.3511347IEEE Transactions on Parallel and Distributed Systems. 362025. 2025</p>
<p>Accelerating Communications in Federated Applications with Transparent Object Proxies. J Gregory Pauloski, Valerie Hayot-Sasson, Logan Ward, Nathaniel Hudson, Charlie Sabino, Matt Baughman, Kyle Chard, Ian Foster, 10.1145/3581784.3607047Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. the International Conference for High Performance Computing, Networking, Storage and AnalysisDenver, CO, USA; New York, NY, USA, ArticleACM202359SC '23)</p>
<p>. Pydantic, 2024. Jan 2025</p>
<p>Dask: Parallel Computation with Blocked algorithms and Task Scheduling. Matthew Rocklin, 10.25080/Majora-7b98e3ed-013Proceedings of the 14th Python in Science Conference. Kathryn Huff, James Bergstra, the 14th Python in Science ConferenceAustin, TX, USA2015SciPy</p>
<p>Mochi: Composing Data Services for High-Performance Computing Environments. Robert Ross, George Amvrosiadis, Philip Carns, Charles D Cranor, Matthieu Dorier, Kevin Harms, Greg Ganger, Garth Gibson, Samuel Gutierrez, Robert Latham, Bob Robey, Dana Robinson, Bradley Settlemyer, Galen Shipman, Shane Snyder, Jerome Soumagne, Zheng Qing, 10.1007/s11390-020-9802-0Journal of Computer Science and Technology. 3512020. Jan 2020</p>
<p>UCX: An open source framework for HPC network APIs and beyond. Pavel Shamis, Gorentla Manjunath, Graham Venkata, Matthew B Lopez, Oscar Baker, Yossi Hernandez, Mike Itigin, Gilad Dubman, Richard L Shainer, Liran Graham, Yiftah Liss, Sreeram Shahar, Davide Potluri, Donald Rossetti, Becker, IEEE 23rd Annual Symposium on High-Performance Interconnects. Duncan Poole, Christopher Lamb, Sameer Kumar, Craig StunkelIEEE2015</p>
<p>Accelerating science with humanaware artificial intelligence. Jamshid Sourati, James A Evans, Nature Human Behaviour. 72023. 2023</p>
<p>Towards a modular architecture for science factories. Rafael Vescovi, Tobias Ginsburg, Kyle Hippe, Doga Ozgulbas, Casey Stone, Abraham Stroka, Rory Butler, Ben Blaiszik, Tom Brettin, Kyle Chard, Digital Discovery. 22023. 2023</p>
<p>Scientific discovery in the age of artificial intelligence. Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Anima Anandkumar, Karianne J Bergen, Carla P Gomes, Shirley Ho, Pushmeet Kohli, Joan Lasenby, Jure Leskovec, Tie-Yan Liu, Arjun K Manrai, Debora S Marks, Bharath Ramsundar, Le Song, Jimeng Sun, Jian Tang, Petar Velickovic, Max Welling, Linfeng Zhang, Connor W Coley, Yoshua Bengio, and Marinka Zitnik. 2023. 2023620</p>
<p>Colmena: Scalable Machine-Learning-Based Steering of Ensemble Simulations for High Performance Computing. Logan Ward, J Gregory Ganesh Sivaraman, Yadu Pauloski, Ryan Babuji, Naveen Chard, Paul C Dandu, Rajeev S Redfern, Kyle Assary, Larry A Chard, Rajeev Curtiss, Ian Thakur, Foster, 10.1109/MLHPC54614.2021.00007IEEE/ACM Workshop on Machine Learning in High Performance Computing Environments. IEEE2021</p>
<p>Parallel scripting for applications at the petascale and beyond. Michael Wilde, Ian Foster, Kamil Iskra, Pete Beckman, Zhao Zhang, Allan Espinosa, Mihael Hategan, Ben Clifford, Ioan Raicu, Computer. 422009. 2009</p>
<p>Intelligent agents: Theory and practice. Michael Wooldridge, Nicholas R Jennings, The Knowledge Engineering Review. 102213429931995. 1995</p>
<p>Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang, arXiv:2308.08155[cs.AIAutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation. 2023</p>
<p>Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, Chi Wang, arXiv:2306.01337[cs.CLMathChat: Converse to Tackle Challenging Math Problems with LLM Agents. 2024</p>
<p>MOFA: Discovering Materials for Carbon Capture with a GenAI-and Simulation-Based Workflow. Xiaoli Yan, Nathaniel Hudson, Hyun Park, J Gregory Daniel Grzenda, Marcus Pauloski, Haochen Schwarting, Hassan Pan, Samuel Harb, Chris Foreman, Tom Knight, Kyle Gibbs, Santanu Chard, Emad Chaudhuri, Ian Tajkhorshid, Mohamad Foster, Logan Moosavi, E A Ward, Huerta, arXiv:2501.10651[cs.DC2025</p>
<p>GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics. Maxim Zvyagin, Alexander Brace, Kyle Hippe, Yuntian Deng, Bin Zhang, Cindy Orozco Bohorquez, Austin Clyde, Bharat Kale, Danilo Perez-Rivera, Heng Ma, The International Journal of High Performance Computing Applications. 372023. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
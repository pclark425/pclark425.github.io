<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8659 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8659</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8659</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-263831328</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.06083v1.pdf" target="_blank">Transformers and Large Language Models for Chemistry and Drug Discovery</a></p>
                <p><strong>Paper Abstract:</strong> Language modeling has seen impressive progress over the last years, mainly prompted by the invention of the Transformer architecture, sparking a revolution in many fields of machine learning, with breakthroughs in chemistry and biology. In this chapter, we explore how analogies between chemical and natural language have inspired the use of Transformers to tackle important bottlenecks in the drug discovery process, such as retrosynthetic planning and chemical space exploration. The revolution started with models able to perform particular tasks with a single type of data, like linearised molecular graphs, which then evolved to include other types of data, like spectra from analytical instruments, synthesis actions, and human language. A new trend leverages recent developments in large language models, giving rise to a wave of models capable of solving generic tasks in chemistry, all facilitated by the flexibility of natural language. As we continue to explore and harness these capabilities, we can look forward to a future where machine learning plays an even more integral role in accelerating scientific discovery.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8659.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8659.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (Jablonka et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3 (as applied in Jablonka et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3-style large pretrained decoder-only language models were fine-tuned and used via in‑context learning to perform low-data discovery tasks in chemistry and materials science, including inverse design and prediction of molecular/ materials properties.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Is GPT-3 all you need for low-data discovery in chemistry?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (decoder-only GPT family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only transformer (GPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on large-scale web and text corpora; fine-tuned or prompted with small, domain-specific datasets (molecular/property datasets) for downstream tasks (as reported by Jablonka et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery, materials science, molecular property prediction, inverse design</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Fine-tuning and in-context learning (prompting) to perform inverse design and low-data regression tasks; model outputs molecular representations (e.g., SMILES) or target property predictions</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not quantified in this review; Jablonka et al. report that GPT-3-style methods can produce useful candidate designs in low-data regimes but specific novelty metrics (e.g., fraction novel vs training set) are not reported here</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Tailoring achieved by fine-tuning on small labeled datasets or by providing in-context exemplars and task descriptions; reported to perform comparably to specialized algorithms on limited-data tasks</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Task-dependent: property prediction error (regression), success in inverse-design tasks relative to baselines; comparison to specialized low-data methods</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Jablonka et al. showed that GPT-3-style models, when fine-tuned or used with in-context learning, can solve a variety of chemistry/materials tasks in low-data regimes, sometimes matching or outperforming specialized methods; the chapter reports this as evidence of LLM flexibility but does not provide quantitative results itself.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Reported to be on-par with or sometimes superior to specialized, heavily engineered methods in low-data settings (per Jablonka et al.), but direct quantitative comparisons are in the cited work rather than this review.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Review highlights general LLM issues: potential hallucinations, need for careful prompting/fine-tuning, reliance on limited labeled data for domain adaptation; specifics (e.g., synthesizability of designs) not fully quantified in this review.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8659.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8659.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT + Bayesian Optimization (Ramos et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT models used with in-context learning for uncertainty-aware regression enabling Bayesian optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ramos et al. used GPT-family models with in-context learning to perform regression with uncertainty estimates, enabling Bayesian optimization workflows for catalyst and molecular optimization directly from synthesis-procedure text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bayesian Optimization of Catalysts With In-context Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-family models (unspecified version in review)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only transformer (GPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on large text corpora; method uses in-context examples consisting of synthesis procedures paired with measured properties (no heavy fine-tuning reported in the review)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Catalyst optimization, molecular optimization (property optimization tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>In-context learning to perform regression with uncertainty calibration; integrated into Bayesian Optimization to propose candidate synthesis/structures</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not explicitly quantified in this review; the approach enables proposing candidates grounded in synthesis procedure space, addressing synthesizability concerns typical of purely structure-based generators</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Specificity achieved by using the synthesis procedure as input and producing property predictions/uncertainties that feed BO; this grounds proposals toward synthesizable candidates</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BO performance metrics (e.g., ability to find improved catalysts/molecules with fewer experiments), uncertainty calibration for regression; exact metrics are provided in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Ramos et al. demonstrated that GPT-based in-context regression with uncertainty enables BO for catalyst and molecular optimization using synthesis-procedure text, representing a novel route that maps procedures to property space and helps address synthesizability limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Novel in that it avoids feature engineering and can enable BO without fine-tuned regressors; comparison details are in the original paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Review notes general LLM liabilities (hallucinations) and does not provide detailed failure modes for this method; practical BO success depends on quality of in-context examples and calibration of uncertainty estimates.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8659.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8659.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolGPT: Molecular Generation Using a Transformer-Decoder Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>MolGPT is a transformer-decoder generative model trained to produce molecular SMILES strings, used for de novo molecular generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MolGPT: Molecular Generation Using a Transformer-Decoder Model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer-decoder (GPT-style) trained on SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>SMILES strings from molecular databases (specific datasets not listed in this review)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>De novo molecular generation for drug discovery and cheminformatics</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct generation of SMILES strings via autoregressive sampling from a transformer-decoder model</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not quantified in the review; MolGPT is presented as capable of generating diverse molecules but exact novelty metrics were not reported here</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Can be conditioned or steered (in principle) via token-level conditioning or fine-tuning to favor properties, though specific conditioning strategies are in the original MolGPT work</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Standard generative-model metrics typically used: validity, uniqueness, novelty, drug-likeness, and property distributions (not enumerated quantitatively in this review)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MolGPT is cited as an example of transformer-based molecular generators that operate on SMILES and have been shown to produce chemically valid candidates; the review does not provide the original paper's quantitative outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Presented in the review alongside other SMILES-based generative approaches; transformer-based decoders are favored for training simplicity and generative flexibility, though potential SMILES robustness issues are discussed elsewhere in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>SMILES-based generation can produce invalid molecules (addressed by representations like SELFIES); also general challenges include ensuring synthesizability and optimization toward application-specific properties.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8659.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8659.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Regression Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Regression Transformer (concurrent sequence regression and generation for molecular language modelling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Regression Transformer is a model architecture that enables simultaneous sequence-to-sequence generation and regression, allowing the model to generate molecules while predicting associated continuous properties.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Regression Transformer enables concurrent sequence regression and generation for molecular language modelling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Regression Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer (sequence-to-sequence capable of joint regression and generation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Molecular sequence data (SMILES/SELFIES) paired with property labels for joint training (exact datasets not specified in this review)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecular generation with property conditioning (drug discovery, property-optimized design)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Concurrent generation/regression where generated sequences (molecules) are produced conditional on or alongside predicted target property values</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not quantified here; the architecture is intended to produce candidates with predicted target properties, improving application-aligned novelty implicitly by conditioning on properties</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Application-tailoring is achieved by jointly training on molecules and their properties so generation can be steered toward desired property values</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Property prediction accuracy, quality of generated molecules, and ability to generate molecules meeting target property values (specific metrics are in the cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as a promising approach for property-aware molecular generation that bridges regression and generation tasks in a single model, enabling more direct control over generated candidates' properties.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Offers a direct joint model alternative to two-step pipelines (separate property predictors and generators); review does not provide quantitative comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Joint training complexity and reliance on paired molecule-property datasets; ensuring real-world synthesizability and experimental validation remain open challenges.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8659.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8659.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>3D LM generation (Flam‑Shepherd & Aspuru‑Guzik)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language models that generate molecules, materials, and protein binding sites directly in three dimensions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Flam‑Shepherd and Aspuru‑Guzik showed that language-model architectures can directly generate 3D structural files (XYZ/CIF/PDB), enabling molecule and material generation in native spatial formats rather than via linearized strings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>3D structure-generating language models (Flam‑Shepherd & Aspuru‑Guzik)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-based language models adapted to output 3D coordinates / crystallographic formats</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>3D structural datasets for crystals, proteins, and molecules (CIF/ PDB/XYZ formats); specific datasets not detailed in the review</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials design, protein binding site design, molecular generation where 3D structure is required (beyond small organic molecules)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct autoregressive generation of 3D coordinate files and associated metadata (bypassing string/graph-to-3D conversion workflows)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Review states generated structures performed comparably to expert-designed state-of-the-art algorithms and overcome representation limitations, but specific novelty metrics are not provided here</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Generates structures in native 3D formats enabling direct evaluation of application-specific properties (e.g., crystal packing, binding site geometry); tailoring depends on training and conditioning signals</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Comparative performance to expert algorithms for structure generation, plausibility/validity of 3D geometries, and downstream property evaluations (details in original work)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>This approach demonstrates that language-model architectures can be extended to generate 3D representations for a wider class of chemical and biological systems, achieving performance comparable to established algorithms while addressing limitations of SMILES/graph-only generation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Reported comparable performance to state-of-the-art graph-based generation algorithms while offering more generality across materials/proteins; detailed benchmarks are in the cited paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Generation of physically realistic 3D structures requires careful encoding of invariances (rotational/translation) and may need substantial structural data; review does not list quantitative failure modes.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8659.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8659.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Molecular Transformer (Schwaller et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The Molecular Transformer treats reaction prediction and retrosynthesis as sequence translation problems using an encoder–decoder Transformer to translate reactants/reagents to products or vice versa, and has been extended to retrosynthetic planning and diversity steering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Molecular Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>encoder-decoder Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Reaction SMILES datasets extracted from patents and reaction databases (large reaction corpora referenced in the review)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Retrosynthesis planning, forward reaction outcome prediction, and chemical space exploration relevant to drug discovery and synthesis planning</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Sequence-to-sequence translation: product prediction from reactants (forward) and reactant prediction from product (retrosynthesis); used iteratively to generate retrosynthetic routes and candidate precursors</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Primarily generates candidate reactant sets and retrosynthetic disconnections rather than de novo scaffold generation; novelty depends on exploration strategies and dataset coverage—specific novelty metrics are not reported here</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Ensures application relevance by predicting synthetically plausible precursors and providing uncertainty estimates; extensions include prompting/steering to influence disconnections and improve diversity</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Top-k accuracy for reaction prediction, retrosynthesis route quality, uncertainty calibration, and diversity of proposed disconnections (as reported in the cited literature)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The Molecular Transformer achieved state-of-the-art results in reaction outcome prediction and has been effectively applied to retrosynthesis and route planning; it also supports diversity-enhancing and unbiasing strategies for proposed disconnections.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperforms many previous sequence- and template-based methods for reaction prediction and retrosynthesis in metrics such as top-k accuracy and calibrated confidence; detailed numbers are in the original papers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Depends on the coverage and quality of reaction datasets; may propose chemically valid but impractical transformations without additional grounding (e.g., experimental procedure details), and retrosynthesis still requires experimental realization steps.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8659.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8659.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemCrow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemCrow: Augmenting large language models with chemistry tools</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChemCrow is an LLM-powered agent framework that composes external computational chemistry tools and databases, enabling LLMs to plan and execute chemical tasks like molecular design and automated synthesis by grounding LLM outputs with tool outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ChemCrow: Augmenting largelanguage models with chemistry tools.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM (unspecified backbone in review) integrated with composable chemistry tools (ChemCrow system)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>LLM-based agent architecture (transformer backbone + tool-invocation interface)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not applicable in the usual sense: system augments pre-trained LLMs with curated computational chemistry tools and databases assembled by the authors; underlying LLM pretrained on standard corpora</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug and materials design, synthesis planning, automation of chemistry tasks, making computational tools accessible via natural language</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Agent-driven pipeline: natural-language prompts trigger LLM planning which invokes external tools (databases, cheminformatics toolkits, retrosynthesis engines) to iteratively generate, validate, and refine candidate molecules and syntheses</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>ChemCrow emphasizes grounded, tool-validated candidates rather than blind novelty; the review does not quantify novelty metrics but highlights improved grounding and reduced hallucination compared to LLM-only generation</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Specificity arises from composing domain tools (e.g., synthesizability checks, property predictors) into the agent pipeline so generated candidates are evaluated against application-relevant criteria</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Practical task success (ability to plan and execute chemistry tasks), grounding quality (reduction of hallucinations), and successful integration of tool outputs; precise quantitative metrics are given in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>ChemCrow demonstrates that augmenting LLMs with domain-specific tools substantially improves LLM performance on chemistry tasks, enabling more reliable molecule design and synthesis planning while mitigating hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Offers advantages over pure-LM generation by grounding outputs with computational tools; compared to standalone cheminformatics pipelines, ChemCrow provides flexible natural-language interfaces and tool composability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>System complexity, dependency on the correctness and availability of external tools and databases, and remaining risks of LM errors in planning steps; real-world experimental validation remains necessary.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8659.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8659.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer-based de novo generator (Wang et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A Transformer-based Generative Model for De Novo Molecular Design</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Wang et al. proposed a Transformer-based generative model specifically for de novo molecular design, demonstrating that transformer architectures can be effective generative models for molecule creation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Transformer-based Generative Model for De Novo Molecular Design.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer-based de novo generative model (Wang et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer (likely decoder/generative variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Molecular strings (e.g., SMILES) from molecular datasets; exact datasets not enumerated in the review</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>De novo molecular design (drug discovery, chemical space exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Autoregressive sequence generation of molecular string representations (SMILES) using a transformer generative model</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not quantified in the review; presented as another successful transformer-based approach for generating candidate molecules</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Can be steered via conditioning or fine-tuning to target properties (details are in the original paper)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Typical molecule-generation metrics: validity, uniqueness, novelty, property distributions; the review does not reproduce the original paper's quantitative results</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as an instance of transformer architectures applied successfully to de novo molecular design; specific performance summaries are in the source paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Placed among other transformer-based generative approaches; transformers offer training and generation simplicity compared to some specialized graph-based generative models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Dependence on linear string representations (SMILES) and their limitations; synthesizability and real-world validation of generated candidates remain outstanding concerns.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8659.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8659.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>C5T5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>C5T5: Controllable Generation of Organic Molecules with Transformers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>C5T5 is a transformer-based system for controllable generation of organic molecules, allowing conditioning of the generative process to bias outputs toward desired attributes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>C5T5: Controllable Generation of Organic Molecules with Transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>C5T5 (transformer generative model for controllable molecular generation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer (controllable generation architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>SMILES or other string-encoded molecular datasets (not explicitly detailed in the review)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery and cheminformatics where control over generated property distributions is required</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Conditional/controllable autoregressive generation using transformer architectures to bias generated molecules toward target attributes</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not reported in the review; intended to improve targeted generation rather than unconstrained novelty</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Control signals/conditioning enable generation tailored to property constraints or design objectives</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Property-target achievement, validity, uniqueness, and alignment with control inputs (exact metrics in the original paper)</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Presented as an example of transformer-based methods that allow controllable molecular generation, suitable for application-specific molecular design tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Offers controllability not always present in vanilla autoregressive generators; detailed empirical comparisons are in the source work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>As with other SMILES-based generators, ensuring robust validity and synthesizability of generated molecules is a concern; control fidelity depends on training signal quality.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Is GPT-3 all you need for low-data discovery in chemistry? <em>(Rating: 2)</em></li>
                <li>Bayesian Optimization of Catalysts With In-context Learning <em>(Rating: 2)</em></li>
                <li>MolGPT: Molecular Generation Using a Transformer-Decoder Model <em>(Rating: 2)</em></li>
                <li>Regression Transformer enables concurrent sequence regression and generation for molecular language modelling <em>(Rating: 2)</em></li>
                <li>Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files. <em>(Rating: 2)</em></li>
                <li>Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction. <em>(Rating: 2)</em></li>
                <li>ChemCrow: Augmenting largelanguage models with chemistry tools. <em>(Rating: 2)</em></li>
                <li>A Transformer-based Generative Model for De Novo Molecular Design. <em>(Rating: 2)</em></li>
                <li>C5T5: Controllable Generation of Organic Molecules with Transformers. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8659",
    "paper_id": "paper-263831328",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "GPT-3 (Jablonka et al.)",
            "name_full": "Generative Pre-trained Transformer 3 (as applied in Jablonka et al.)",
            "brief_description": "GPT-3-style large pretrained decoder-only language models were fine-tuned and used via in‑context learning to perform low-data discovery tasks in chemistry and materials science, including inverse design and prediction of molecular/ materials properties.",
            "citation_title": "Is GPT-3 all you need for low-data discovery in chemistry?",
            "mention_or_use": "mention",
            "model_name": "GPT-3 (decoder-only GPT family)",
            "model_type": "decoder-only transformer (GPT)",
            "model_size": null,
            "training_data": "Pretrained on large-scale web and text corpora; fine-tuned or prompted with small, domain-specific datasets (molecular/property datasets) for downstream tasks (as reported by Jablonka et al.).",
            "application_domain": "Drug discovery, materials science, molecular property prediction, inverse design",
            "generation_method": "Fine-tuning and in-context learning (prompting) to perform inverse design and low-data regression tasks; model outputs molecular representations (e.g., SMILES) or target property predictions",
            "novelty_of_chemicals": "Not quantified in this review; Jablonka et al. report that GPT-3-style methods can produce useful candidate designs in low-data regimes but specific novelty metrics (e.g., fraction novel vs training set) are not reported here",
            "application_specificity": "Tailoring achieved by fine-tuning on small labeled datasets or by providing in-context exemplars and task descriptions; reported to perform comparably to specialized algorithms on limited-data tasks",
            "evaluation_metrics": "Task-dependent: property prediction error (regression), success in inverse-design tasks relative to baselines; comparison to specialized low-data methods",
            "results_summary": "Jablonka et al. showed that GPT-3-style models, when fine-tuned or used with in-context learning, can solve a variety of chemistry/materials tasks in low-data regimes, sometimes matching or outperforming specialized methods; the chapter reports this as evidence of LLM flexibility but does not provide quantitative results itself.",
            "comparison_to_other_methods": "Reported to be on-par with or sometimes superior to specialized, heavily engineered methods in low-data settings (per Jablonka et al.), but direct quantitative comparisons are in the cited work rather than this review.",
            "limitations_and_challenges": "Review highlights general LLM issues: potential hallucinations, need for careful prompting/fine-tuning, reliance on limited labeled data for domain adaptation; specifics (e.g., synthesizability of designs) not fully quantified in this review.",
            "uuid": "e8659.0"
        },
        {
            "name_short": "GPT + Bayesian Optimization (Ramos et al.)",
            "name_full": "GPT models used with in-context learning for uncertainty-aware regression enabling Bayesian optimization",
            "brief_description": "Ramos et al. used GPT-family models with in-context learning to perform regression with uncertainty estimates, enabling Bayesian optimization workflows for catalyst and molecular optimization directly from synthesis-procedure text.",
            "citation_title": "Bayesian Optimization of Catalysts With In-context Learning",
            "mention_or_use": "mention",
            "model_name": "GPT-family models (unspecified version in review)",
            "model_type": "decoder-only transformer (GPT)",
            "model_size": null,
            "training_data": "Pretrained on large text corpora; method uses in-context examples consisting of synthesis procedures paired with measured properties (no heavy fine-tuning reported in the review)",
            "application_domain": "Catalyst optimization, molecular optimization (property optimization tasks)",
            "generation_method": "In-context learning to perform regression with uncertainty calibration; integrated into Bayesian Optimization to propose candidate synthesis/structures",
            "novelty_of_chemicals": "Not explicitly quantified in this review; the approach enables proposing candidates grounded in synthesis procedure space, addressing synthesizability concerns typical of purely structure-based generators",
            "application_specificity": "Specificity achieved by using the synthesis procedure as input and producing property predictions/uncertainties that feed BO; this grounds proposals toward synthesizable candidates",
            "evaluation_metrics": "BO performance metrics (e.g., ability to find improved catalysts/molecules with fewer experiments), uncertainty calibration for regression; exact metrics are provided in the cited work.",
            "results_summary": "Ramos et al. demonstrated that GPT-based in-context regression with uncertainty enables BO for catalyst and molecular optimization using synthesis-procedure text, representing a novel route that maps procedures to property space and helps address synthesizability limitations.",
            "comparison_to_other_methods": "Novel in that it avoids feature engineering and can enable BO without fine-tuned regressors; comparison details are in the original paper.",
            "limitations_and_challenges": "Review notes general LLM liabilities (hallucinations) and does not provide detailed failure modes for this method; practical BO success depends on quality of in-context examples and calibration of uncertainty estimates.",
            "uuid": "e8659.1"
        },
        {
            "name_short": "MolGPT",
            "name_full": "MolGPT: Molecular Generation Using a Transformer-Decoder Model",
            "brief_description": "MolGPT is a transformer-decoder generative model trained to produce molecular SMILES strings, used for de novo molecular generation tasks.",
            "citation_title": "MolGPT: Molecular Generation Using a Transformer-Decoder Model",
            "mention_or_use": "mention",
            "model_name": "MolGPT",
            "model_type": "transformer-decoder (GPT-style) trained on SMILES",
            "model_size": null,
            "training_data": "SMILES strings from molecular databases (specific datasets not listed in this review)",
            "application_domain": "De novo molecular generation for drug discovery and cheminformatics",
            "generation_method": "Direct generation of SMILES strings via autoregressive sampling from a transformer-decoder model",
            "novelty_of_chemicals": "Not quantified in the review; MolGPT is presented as capable of generating diverse molecules but exact novelty metrics were not reported here",
            "application_specificity": "Can be conditioned or steered (in principle) via token-level conditioning or fine-tuning to favor properties, though specific conditioning strategies are in the original MolGPT work",
            "evaluation_metrics": "Standard generative-model metrics typically used: validity, uniqueness, novelty, drug-likeness, and property distributions (not enumerated quantitatively in this review)",
            "results_summary": "MolGPT is cited as an example of transformer-based molecular generators that operate on SMILES and have been shown to produce chemically valid candidates; the review does not provide the original paper's quantitative outcomes.",
            "comparison_to_other_methods": "Presented in the review alongside other SMILES-based generative approaches; transformer-based decoders are favored for training simplicity and generative flexibility, though potential SMILES robustness issues are discussed elsewhere in the review.",
            "limitations_and_challenges": "SMILES-based generation can produce invalid molecules (addressed by representations like SELFIES); also general challenges include ensuring synthesizability and optimization toward application-specific properties.",
            "uuid": "e8659.2"
        },
        {
            "name_short": "Regression Transformer",
            "name_full": "Regression Transformer (concurrent sequence regression and generation for molecular language modelling)",
            "brief_description": "Regression Transformer is a model architecture that enables simultaneous sequence-to-sequence generation and regression, allowing the model to generate molecules while predicting associated continuous properties.",
            "citation_title": "Regression Transformer enables concurrent sequence regression and generation for molecular language modelling",
            "mention_or_use": "mention",
            "model_name": "Regression Transformer",
            "model_type": "Transformer (sequence-to-sequence capable of joint regression and generation)",
            "model_size": null,
            "training_data": "Molecular sequence data (SMILES/SELFIES) paired with property labels for joint training (exact datasets not specified in this review)",
            "application_domain": "Molecular generation with property conditioning (drug discovery, property-optimized design)",
            "generation_method": "Concurrent generation/regression where generated sequences (molecules) are produced conditional on or alongside predicted target property values",
            "novelty_of_chemicals": "Not quantified here; the architecture is intended to produce candidates with predicted target properties, improving application-aligned novelty implicitly by conditioning on properties",
            "application_specificity": "Application-tailoring is achieved by jointly training on molecules and their properties so generation can be steered toward desired property values",
            "evaluation_metrics": "Property prediction accuracy, quality of generated molecules, and ability to generate molecules meeting target property values (specific metrics are in the cited work)",
            "results_summary": "Cited as a promising approach for property-aware molecular generation that bridges regression and generation tasks in a single model, enabling more direct control over generated candidates' properties.",
            "comparison_to_other_methods": "Offers a direct joint model alternative to two-step pipelines (separate property predictors and generators); review does not provide quantitative comparisons.",
            "limitations_and_challenges": "Joint training complexity and reliance on paired molecule-property datasets; ensuring real-world synthesizability and experimental validation remain open challenges.",
            "uuid": "e8659.3"
        },
        {
            "name_short": "3D LM generation (Flam‑Shepherd & Aspuru‑Guzik)",
            "name_full": "Language models that generate molecules, materials, and protein binding sites directly in three dimensions",
            "brief_description": "Flam‑Shepherd and Aspuru‑Guzik showed that language-model architectures can directly generate 3D structural files (XYZ/CIF/PDB), enabling molecule and material generation in native spatial formats rather than via linearized strings.",
            "citation_title": "Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files.",
            "mention_or_use": "mention",
            "model_name": "3D structure-generating language models (Flam‑Shepherd & Aspuru‑Guzik)",
            "model_type": "Transformer-based language models adapted to output 3D coordinates / crystallographic formats",
            "model_size": null,
            "training_data": "3D structural datasets for crystals, proteins, and molecules (CIF/ PDB/XYZ formats); specific datasets not detailed in the review",
            "application_domain": "Materials design, protein binding site design, molecular generation where 3D structure is required (beyond small organic molecules)",
            "generation_method": "Direct autoregressive generation of 3D coordinate files and associated metadata (bypassing string/graph-to-3D conversion workflows)",
            "novelty_of_chemicals": "Review states generated structures performed comparably to expert-designed state-of-the-art algorithms and overcome representation limitations, but specific novelty metrics are not provided here",
            "application_specificity": "Generates structures in native 3D formats enabling direct evaluation of application-specific properties (e.g., crystal packing, binding site geometry); tailoring depends on training and conditioning signals",
            "evaluation_metrics": "Comparative performance to expert algorithms for structure generation, plausibility/validity of 3D geometries, and downstream property evaluations (details in original work)",
            "results_summary": "This approach demonstrates that language-model architectures can be extended to generate 3D representations for a wider class of chemical and biological systems, achieving performance comparable to established algorithms while addressing limitations of SMILES/graph-only generation.",
            "comparison_to_other_methods": "Reported comparable performance to state-of-the-art graph-based generation algorithms while offering more generality across materials/proteins; detailed benchmarks are in the cited paper.",
            "limitations_and_challenges": "Generation of physically realistic 3D structures requires careful encoding of invariances (rotational/translation) and may need substantial structural data; review does not list quantitative failure modes.",
            "uuid": "e8659.4"
        },
        {
            "name_short": "Molecular Transformer (Schwaller et al.)",
            "name_full": "Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction",
            "brief_description": "The Molecular Transformer treats reaction prediction and retrosynthesis as sequence translation problems using an encoder–decoder Transformer to translate reactants/reagents to products or vice versa, and has been extended to retrosynthetic planning and diversity steering.",
            "citation_title": "Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction.",
            "mention_or_use": "mention",
            "model_name": "Molecular Transformer",
            "model_type": "encoder-decoder Transformer",
            "model_size": null,
            "training_data": "Reaction SMILES datasets extracted from patents and reaction databases (large reaction corpora referenced in the review)",
            "application_domain": "Retrosynthesis planning, forward reaction outcome prediction, and chemical space exploration relevant to drug discovery and synthesis planning",
            "generation_method": "Sequence-to-sequence translation: product prediction from reactants (forward) and reactant prediction from product (retrosynthesis); used iteratively to generate retrosynthetic routes and candidate precursors",
            "novelty_of_chemicals": "Primarily generates candidate reactant sets and retrosynthetic disconnections rather than de novo scaffold generation; novelty depends on exploration strategies and dataset coverage—specific novelty metrics are not reported here",
            "application_specificity": "Ensures application relevance by predicting synthetically plausible precursors and providing uncertainty estimates; extensions include prompting/steering to influence disconnections and improve diversity",
            "evaluation_metrics": "Top-k accuracy for reaction prediction, retrosynthesis route quality, uncertainty calibration, and diversity of proposed disconnections (as reported in the cited literature)",
            "results_summary": "The Molecular Transformer achieved state-of-the-art results in reaction outcome prediction and has been effectively applied to retrosynthesis and route planning; it also supports diversity-enhancing and unbiasing strategies for proposed disconnections.",
            "comparison_to_other_methods": "Outperforms many previous sequence- and template-based methods for reaction prediction and retrosynthesis in metrics such as top-k accuracy and calibrated confidence; detailed numbers are in the original papers.",
            "limitations_and_challenges": "Depends on the coverage and quality of reaction datasets; may propose chemically valid but impractical transformations without additional grounding (e.g., experimental procedure details), and retrosynthesis still requires experimental realization steps.",
            "uuid": "e8659.5"
        },
        {
            "name_short": "ChemCrow",
            "name_full": "ChemCrow: Augmenting large language models with chemistry tools",
            "brief_description": "ChemCrow is an LLM-powered agent framework that composes external computational chemistry tools and databases, enabling LLMs to plan and execute chemical tasks like molecular design and automated synthesis by grounding LLM outputs with tool outputs.",
            "citation_title": "ChemCrow: Augmenting largelanguage models with chemistry tools.",
            "mention_or_use": "mention",
            "model_name": "LLM (unspecified backbone in review) integrated with composable chemistry tools (ChemCrow system)",
            "model_type": "LLM-based agent architecture (transformer backbone + tool-invocation interface)",
            "model_size": null,
            "training_data": "Not applicable in the usual sense: system augments pre-trained LLMs with curated computational chemistry tools and databases assembled by the authors; underlying LLM pretrained on standard corpora",
            "application_domain": "Drug and materials design, synthesis planning, automation of chemistry tasks, making computational tools accessible via natural language",
            "generation_method": "Agent-driven pipeline: natural-language prompts trigger LLM planning which invokes external tools (databases, cheminformatics toolkits, retrosynthesis engines) to iteratively generate, validate, and refine candidate molecules and syntheses",
            "novelty_of_chemicals": "ChemCrow emphasizes grounded, tool-validated candidates rather than blind novelty; the review does not quantify novelty metrics but highlights improved grounding and reduced hallucination compared to LLM-only generation",
            "application_specificity": "Specificity arises from composing domain tools (e.g., synthesizability checks, property predictors) into the agent pipeline so generated candidates are evaluated against application-relevant criteria",
            "evaluation_metrics": "Practical task success (ability to plan and execute chemistry tasks), grounding quality (reduction of hallucinations), and successful integration of tool outputs; precise quantitative metrics are given in the cited work.",
            "results_summary": "ChemCrow demonstrates that augmenting LLMs with domain-specific tools substantially improves LLM performance on chemistry tasks, enabling more reliable molecule design and synthesis planning while mitigating hallucinations.",
            "comparison_to_other_methods": "Offers advantages over pure-LM generation by grounding outputs with computational tools; compared to standalone cheminformatics pipelines, ChemCrow provides flexible natural-language interfaces and tool composability.",
            "limitations_and_challenges": "System complexity, dependency on the correctness and availability of external tools and databases, and remaining risks of LM errors in planning steps; real-world experimental validation remains necessary.",
            "uuid": "e8659.6"
        },
        {
            "name_short": "Transformer-based de novo generator (Wang et al.)",
            "name_full": "A Transformer-based Generative Model for De Novo Molecular Design",
            "brief_description": "Wang et al. proposed a Transformer-based generative model specifically for de novo molecular design, demonstrating that transformer architectures can be effective generative models for molecule creation.",
            "citation_title": "A Transformer-based Generative Model for De Novo Molecular Design.",
            "mention_or_use": "mention",
            "model_name": "Transformer-based de novo generative model (Wang et al.)",
            "model_type": "Transformer (likely decoder/generative variant)",
            "model_size": null,
            "training_data": "Molecular strings (e.g., SMILES) from molecular datasets; exact datasets not enumerated in the review",
            "application_domain": "De novo molecular design (drug discovery, chemical space exploration)",
            "generation_method": "Autoregressive sequence generation of molecular string representations (SMILES) using a transformer generative model",
            "novelty_of_chemicals": "Not quantified in the review; presented as another successful transformer-based approach for generating candidate molecules",
            "application_specificity": "Can be steered via conditioning or fine-tuning to target properties (details are in the original paper)",
            "evaluation_metrics": "Typical molecule-generation metrics: validity, uniqueness, novelty, property distributions; the review does not reproduce the original paper's quantitative results",
            "results_summary": "Cited as an instance of transformer architectures applied successfully to de novo molecular design; specific performance summaries are in the source paper.",
            "comparison_to_other_methods": "Placed among other transformer-based generative approaches; transformers offer training and generation simplicity compared to some specialized graph-based generative models.",
            "limitations_and_challenges": "Dependence on linear string representations (SMILES) and their limitations; synthesizability and real-world validation of generated candidates remain outstanding concerns.",
            "uuid": "e8659.7"
        },
        {
            "name_short": "C5T5",
            "name_full": "C5T5: Controllable Generation of Organic Molecules with Transformers",
            "brief_description": "C5T5 is a transformer-based system for controllable generation of organic molecules, allowing conditioning of the generative process to bias outputs toward desired attributes.",
            "citation_title": "C5T5: Controllable Generation of Organic Molecules with Transformers.",
            "mention_or_use": "mention",
            "model_name": "C5T5 (transformer generative model for controllable molecular generation)",
            "model_type": "Transformer (controllable generation architecture)",
            "model_size": null,
            "training_data": "SMILES or other string-encoded molecular datasets (not explicitly detailed in the review)",
            "application_domain": "Drug discovery and cheminformatics where control over generated property distributions is required",
            "generation_method": "Conditional/controllable autoregressive generation using transformer architectures to bias generated molecules toward target attributes",
            "novelty_of_chemicals": "Not reported in the review; intended to improve targeted generation rather than unconstrained novelty",
            "application_specificity": "Control signals/conditioning enable generation tailored to property constraints or design objectives",
            "evaluation_metrics": "Property-target achievement, validity, uniqueness, and alignment with control inputs (exact metrics in the original paper)",
            "results_summary": "Presented as an example of transformer-based methods that allow controllable molecular generation, suitable for application-specific molecular design tasks.",
            "comparison_to_other_methods": "Offers controllability not always present in vanilla autoregressive generators; detailed empirical comparisons are in the source work.",
            "limitations_and_challenges": "As with other SMILES-based generators, ensuring robust validity and synthesizability of generated molecules is a concern; control fidelity depends on training signal quality.",
            "uuid": "e8659.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Is GPT-3 all you need for low-data discovery in chemistry?",
            "rating": 2,
            "sanitized_title": "is_gpt3_all_you_need_for_lowdata_discovery_in_chemistry"
        },
        {
            "paper_title": "Bayesian Optimization of Catalysts With In-context Learning",
            "rating": 2,
            "sanitized_title": "bayesian_optimization_of_catalysts_with_incontext_learning"
        },
        {
            "paper_title": "MolGPT: Molecular Generation Using a Transformer-Decoder Model",
            "rating": 2,
            "sanitized_title": "molgpt_molecular_generation_using_a_transformerdecoder_model"
        },
        {
            "paper_title": "Regression Transformer enables concurrent sequence regression and generation for molecular language modelling",
            "rating": 2,
            "sanitized_title": "regression_transformer_enables_concurrent_sequence_regression_and_generation_for_molecular_language_modelling"
        },
        {
            "paper_title": "Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files.",
            "rating": 2,
            "sanitized_title": "language_models_can_generate_molecules_materials_and_protein_binding_sites_directly_in_three_dimensions_as_xyz_cif_and_pdb_files"
        },
        {
            "paper_title": "Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction.",
            "rating": 2,
            "sanitized_title": "molecular_transformer_a_model_for_uncertaintycalibrated_chemical_reaction_prediction"
        },
        {
            "paper_title": "ChemCrow: Augmenting largelanguage models with chemistry tools.",
            "rating": 2,
            "sanitized_title": "chemcrow_augmenting_largelanguage_models_with_chemistry_tools"
        },
        {
            "paper_title": "A Transformer-based Generative Model for De Novo Molecular Design.",
            "rating": 2,
            "sanitized_title": "a_transformerbased_generative_model_for_de_novo_molecular_design"
        },
        {
            "paper_title": "C5T5: Controllable Generation of Organic Molecules with Transformers.",
            "rating": 2,
            "sanitized_title": "c5t5_controllable_generation_of_organic_molecules_with_transformers"
        }
    ],
    "cost": 0.0168075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Transformers and Large Language Models for Chemistry and Drug Discovery
9 Oct 2023</p>
<p>Andres M Bran 
Laboratory of Artificial Chemical Intelligence (LIAC)
ISIC
EPFL
1050LausanneVDSwitzerland</p>
<p>EPFL
National Centre of Competence in Research (NCCR) Catalysis
1050LausanneVDSwitzerland</p>
<p>Philippe Schwaller philippe.schwaller@epfl.ch 
Laboratory of Artificial Chemical Intelligence (LIAC)
ISIC
EPFL
1050LausanneVDSwitzerland</p>
<p>EPFL
National Centre of Competence in Research (NCCR) Catalysis
1050LausanneVDSwitzerland</p>
<p>Transformers and Large Language Models for Chemistry and Drug Discovery
9 Oct 2023FCD2F3DED513E77F7E69995F08D6E6A5arXiv:2310.06083v1[cs.LG]TransformersAccelerated discoveryLanguage ModelsRetrosynthesisComputational tools
Language modeling has seen impressive progress over the last years, mainly prompted by the invention of the Transformer architecture, sparking a revolution in many fields of machine learning, with breakthroughs in chemistry and biology.In this chapter, we explore how analogies between chemical and natural language have inspired the use of Transformers to tackle important bottlenecks in the drug discovery process, such as retrosynthetic planning and chemical space exploration.The revolution started with models able to perform particular tasks with a single type of data, like linearised molecular graphs, which then evolved to include other types of data, like spectra from analytical instruments, synthesis actions, and human language.A new trend leverages recent developments in large language models, giving rise to a wave of models capable of solving generic tasks in chemistry, all facilitated by the flexibility of natural language.As we continue to explore and harness these capabilities, we can look forward to a future where machine learning plays an even more integral role in accelerating scientific discovery.</p>
<p>Introduction</p>
<p>The capacity to process and accurately model human language has been a persistent pursuit within the machine learning community [1][2][3][4][5].The belief is that language is intrinsic to human reasoning capabilities, thus successful language modeling could open the door to numerous applications, enhancing various information processing tasks with the potential to revolutionize several industries [6,7].The field of natural language processing has witnessed significant advancements in recent years, thanks to improved computing infrastructure, breakthroughs in algorithms, and the proliferation of abundant and accessible data [8].Language and technical terminology also play a pivotal role in the domain of chemistry, which serves as the fundamental basis for drug discovery and development.Analogous to human language, understanding and accurately modeling the language of chemistry is crucial for effective research and development in the pharmaceutical industry.By applying the advancements in language modeling and processing from the machine learning community to the domain of chemistry, it is possible to facilitate drug discovery by efficiently analyzing and interpreting vast amounts of chemical data and literature.</p>
<p>Introduced in 2017, the Transformer architecture revolutionized natural language processing [5].The Transformer is a type of neural network initially developed for neural machine translation.In its original architecture, the Transformer consists of an encoder, which encodes a sentence in the source language (e.g., French) and a decoder, which wordby-word decodes the translated sentence in the target language (e.g.English).The power of the Transformer model comes from its core building blocks, so-called attention layers [1].Those attention layers are excellent at capturing the meaning of words and subwords in their context.For this to work, text is split into subwords and inputted into the Transformer, which encodes the sequence of subwords and learns to use its attention layers to connect relevant pieces of information.The Transformer then generates another sequence of tokens as output, using its decoder part.</p>
<p>The original Transformer and variants of it, such as encoder-based and decoder-based architectures, have shown remarkable results in a range of language modeling tasks, including translation [9], sentiment classification [10,11], and summarization [12,13], to name a few.Over time, the architecture of these models has evolved, with the most popular variations being decoder-only structures, which make up the foundation of many contemporary, large language models (LLMs), such as ChatGPT and GPT-4 [14].</p>
<p>Similar to human language, where the tokens -the subunits in which the text is broken down -are words and subwords, we can imagine splitting proteins and peptides into sequences of amino acids, or molecules and chemical reactions into sequences of atoms (Figure 1).Such analogies between human language and (bio)chemistry extended the influence of Transformers in fields far beyond the boundaries of natural language processing impacting numerous scientific disciplines.</p>
<p>A prime example of Transformers' applications is their pivotal role in AlphaFold2, a leading system for predicting the 3D structure of proteins [15].The emergence of AlphaFold2 not only addressed a long-standing challenge in biochemistry but also catalyzed a surge of research, thereby becoming one of the most prevalent tools in the biochemistry community [16,17].</p>
<p>Chemistry has also benefited from these advancements.Inspired by developments in other areas, researchers have adapted a number of chemical tasks in the form of text sequences (Figure 1).This, together with the rise of open datasets and benchmarks [18][19][20][21], sparked a revolution that started with clearly defined chemical challenges fundamental to the drug development process, like reaction outcome prediction and retrosynthetic planning.Models at this stage are trained with only molecules as both input and output, making them single-modal models.</p>
<p>The revolution then continued in a new direction, with attempts to include additional types of data like spectra from analytical techniques, and human language as in synthesis procedures, giving rise to a wave of chemical multimodal models.The current stage aims to definitively bridge the gap between the modeling of chemical and natural language (Figure 1).With applications based on large language models, researchers have demonstrated how natural language can serve as interfaces for chemical reasoning and task solving across fields.</p>
<p>We are now in an era marked by significant progress in a host of tasks that are fundamental to the drug development process, with systems that go beyond single task solving and are capable of implementing complete pipelines in this process, accelerating chemical research.The influence of these Transformer models in the realm of chemistry is thus profound, highlighting their pivotal role in shaping the future of chemistry and drug discovery.The next sections will briefly introduce textual representations of molecules and reactions, and then continue to discuss task-specific Transformers for single-modality and multi-modality tasks.Lastly, we will address large language models and potential uses in chemistry and drug discovery.</p>
<p>Natural</p>
<p>2 Modeling the language of organic chemistry Chemistry, in many respects, resembles a language [22].Not only is most of the information in chemistry conveyed through human language, but the rules governing chemical transformations also form a distinct language themselves.Accurately modeling this language advances our understanding of its rules, opening up applications such as automatic retrosynthetic planning and efficient chemical space exploration [23], while also shedding light on the intricate grammar of organic chemistry [24].</p>
<p>However, the chemical language is not a conventional language like English or Mandarin, which operate on text.In organic chemistry, grammar operates on a spectrum of molecular graphs and reaction conditions, making the direct application of Transformers to chemistry a challenge.Overcoming this obstacle is thus vital and, as we will explore, can be accomplished using linear string molecular representations that have been in use for decades, with new revisions and proposals in recent years [25][26][27][28][29][30]</p>
<p>Text Representations of Molecules and Reactions</p>
<p>Chemistry has long been acknowledged as a fundamentally inventorial science, wherein new molecules and reactions are discovered, analyzed, and to a certain extent cataloged in databases [31].Researchers rely not only on scientific articles and patents but also on resources like handbooks and, more recently, computational databases, to advance their work.To streamline the storage and querying of these data, simplified molecular-input line-entry system (SMILES) strings have been proposed and utilized since the 1980s [25].</p>
<p>The process of linearizing molecular graphs involves choosing an atom and sequentially enumerating all other atoms in the molecule from this point (Figure 2a).Special characters are assigned to specify bond types, branches, rings, stereochemistry, and other pertinent information for molecular representation.SMILES leverage the fact that molecular graphs adhere to certain chemical rules, such as different types of atoms needing to respect their valence.This makes it feasible to represent a large portion of organic chemistry as sequences of characters.</p>
<p>As molecular machine learning applications have emerged, the limitations of these representations -such as their lack of robustness-have become increasingly apparent.In generative models, this flaw can lead to the production of entirely invalid molecules, becoming an additional hurdle in molecular discovery applications [32,33].To mitigate this issue, researchers have introduced Self-Referencing Embedded Strings (SELFIES), a novel string-based representation that effectively addresses the robustness concern [28].This new representation has found numerous applications, particularly in drug discovery and molecular generation, due to its unique formulation that guarantees the correct mapping of any such string to a valid molecule.While other representations have been proposed in the past to standardize the language (IUPAC names, InChI [29]), or to address the limitations of current representations for deep learning applications (DeepSMILES [26]), we will not delve further into these representations in this discussion [28,34].</p>
<p>With these molecular text representations in hand, chemical reactions can easily be encoded by leveraging the syntax of chemical equations.In that sense, sets of molecules -e.g. the set of reactants-are represented by listing all molecules with a dot "." between them.Reactants are separated from the products using the symbol "&gt;", representing the arrow, and other details like catalysts and reagents are placed in between the "&gt;&gt;" symbol.The resulting reaction representation has the form "A.B&gt;catatlyst.reagent&gt;C.D", where A and B are reactants and C and D products.This notation is commonly used in SMILES and called a "reaction SMILES".</p>
<p>Task-specific Transformers</p>
<p>The conversion of chemical problems into sequences of tokens -in the form of a languagehas unlocked the transformative potential of the Transformer architecture within the chemistry domain.This shift has led to remarkable model performances in prediction tasks in</p>
<p>CNC(C)CC1=CC=C(OCO2)C2=C1 SMILES SELFIES InChI [C][N][Branch1][C][C][C][C][=C][C] [=C][C][Branch1][Ring2][=C] [Ring1][=Branch1][O][=C][O] [Ring1][=Branch1]</p>
<p>IUPAC name Fig. 2 The language of organic chemistry operates on molecular graphs, which can easily be converted into multiple text formats (a), facilitating the use of Transformers in their multiple forms, for a variety of tasks in Chemistry (b).</p>
<p>chemistry, such as retro-and forward-synthesis [35][36][37], as well as molecular regression [38][39][40] and reaction classification [41].Moreover, it has set the stage for other, more diverse tasks, like inferring experimental procedures [42,43], that go beyond operations on molecular graphs and require a deeper understanding of experimental conditions and standard procedures, a feat only achievable through modeling human language.Achieving success in this wide range of tasks has been facilitated by different variations of the Transformer architecture.Depending on the application, these variations leverage different parts of the architecture (Figure 2b), leading to encoder-decoder models -useful for tasks involving conversion of a sequence into another, like translation-, encoder-only -to extract rich representations from data-, and decoder-only architectures, mostly used in generative applications (Figure 2b).</p>
<p>Chemical translation tasks</p>
<p>The primary motivation for the formulation of the Transformer architecture by Vaswani et al. [5] was translation.In this task, input text in one language is converted into corresponding text in another language, logically leading to an architecture with an encoder part linked to a decoder (Figure 2b).</p>
<p>Steps to exploit this design were taken by Schwaller et al. [36], they introduced the Molecular Transformer and treated the task of reaction prediction as a translation task.In this perspective, a Transformer model learns to "translate" from precursors to products SMILES.The approach proved highly successful, establishing itself as one of the state-ofthe-art methods for this task [44].Subsequent attempts applied a similar approach to other tasks, like retrosynthetic planning [37], where a model learns to predict the reactants (and reagents) necessary to produce a given product.The model is then successively applied to the predicted reactants iteratively, to construct a retrosynthetic tree that maps to commercially available building blocks.Recent work used similar retrosynthesis models to tackle the challenge of suggesting diverse candidate reactions [45], as well as prompting, steering and unbiasing the proposed disconnections [46].</p>
<p>The Molecular Transformer approach was further extended by Irwin et al. [47], who proposed the Chemformer, a model pre-trained on a variety of chemical tasks, that can then be specialized for specific applications.This methodology achieved even greater success while offering additional flexibility and transferability to new tasks.</p>
<p>An interesting variation of the architecture came from an attempt to directly encode molecules as molecular graphs, for translation tasks like reaction outcome prediction.Tu and Coley [48] used a custom-designed graph encoder, with a Transformer decoder, trained to translate molecular graphs to SMILES.This approach proved valuable in forwardand retrosynthesis tasks, improving over other Transformer-based baselines, due to the permutational invariance of the novel graph-encoder.</p>
<p>Unsupervised learning and feature extraction.</p>
<p>Transformers have also been applied in various fields to generate rich vectorial representations of text that encapsulate context and general features [11,49].Such methods have been used for sentiment analysis, where a given text is classified based on the sentiment it conveys, a useful tool for assessing customer satisfaction with a specific product.The process of encoding this text as vectors is known as representation learning.For this task, only the encoder part of the Transformer architecture is used, as there is no need for text generation.</p>
<p>In the realm of chemistry, representation learning is a critical issue [50].It enables the transformation of molecules and reactions into vector representations, thus enabling a multitude of downstream applications.These include similarity assessment for database lookups, as well as regression and classification for property prediction tasks, such as predicting reaction yields or identifying toxic compounds -both of which are crucial to the drug development process.</p>
<p>Adopting similar techniques, Wang et al. [51] trained a model to generate reaction representations, and compared them against commonly used hand-engineered molecular representations, showing gains in accuracy in a number of downstream regression tasks, demonstrating the power of Transformer encoders for tasks in chemistry.</p>
<p>In another study, the decoder side of the Transformer was replaced by a classification layer, and the model was trained to predict the class of chemical reactions [41].The resulting vector representations, in addition to achieving high classification accuracies, were used for the visualization and exploration of a database of chemical reactions from patents.This revealed patterns in the data that grouped reactions by class, but also by data source, and relevant product properties, like logP, number of H donors, and others [52].This study showcased how such numerical representations can encode intricate details of chemical reactions, thereby facilitating effective clustering and classification of reactions.Importantly, this was achieved without the need for hand-crafted molecular or reaction features, demonstrating the model's capacity to learn purely from extensive reaction databases.In a similar vein, a regression layer, instead of a decoder layer, was used for predicting reaction yields, achieving outstanding results on datasets from high-throughput-experimentation platforms [53,54].</p>
<p>Other works have further advanced the applications of encoder-only Transformers, with enhancements in training procedures [38][39][40], modifications in the architecture [55], and applications in larger machine learning systems towards the solution of more specific objectives [56].</p>
<p>Similar ideas have also been implemented, with a focus on biochemistry.Rives et al. [57] trained a transformer model on 250 million unlabeled protein sequences, with the goal of learning the "language of proteins", an approach known as unsupervised learning.The resulting representations not only enabled state-of-the-art property predictions for proteins, but also predictions of variant effects and protein folding [58].The model was further shown to generalize beyond naturally occurring proteins [59], paving the way for applications in de novo generation of proteins for specific applications.</p>
<p>Perhaps one of the most intriguing applications of Transformers in chemical representation learning, also leveraging unsupervised learning, emerged from an attempt to interpret their inner workings.Through a detailed examination of the attention weights in a Transformer model trained on unlabeled chemical reactions, Schwaller et al. [24] discovered that these models create internal representations that connect atoms from the reactants to other atoms in the products.This almost serendipitous discovery led to the creation of RXN-Mapper, an open-source algorithm that accurately calculates the atom mapping in a given chemical reaction.RXNMapper has been shown to outperform other approaches -many of which were under closed licenses-in terms of speed, parallelizability, and accuracy.A similar approach was recently investigated on enzymatic reactions, where the attention weights could be used to identify active sites in protein sequences [60].</p>
<p>Articulating chemical language and other modalities</p>
<p>Chemical transformations are not confined solely to the realm of chemical structures.They are enhanced and accessed through a variety of other data types, or modalities.These include human language used for describing molecules and experimental results, as well as the experimental results themselves, which may be presented in formats such as numerical arrays or images, among others.</p>
<p>Considering this, scientists have proposed tasks designed to bridge the divide between the language of molecules and human language.One such task is known as molecular captioning, which involves describing a specific molecule in natural language [61].The description can cover a wide range of features such as molecular scaffolds, the source of the substance, drug interactions, and more, all expressed in simple English.</p>
<p>Some researchers have expanded on this concept [37,62], enabling a smooth interconversion from molecules to natural language and vice versa.This advancement has given rise to versatile models capable of not just molecular captioning, but also generating molecules in response to text queries, or carrying out molecule-to-molecule conversion tasks, like predicting reaction outcomes and retrosynthesis [62].While these models are still in their early stages of development, they show immense potential for the future of drug discovery as they continue to be developed and refined.</p>
<p>Another task, highly relevant for the design of automatic robotic platforms for synthesis, is the prediction of experimental steps.A step missing from retrosynthetic planning is the experimental realization, which involves steps such as adding substances, stirring, and purifying, details not contained in predicted reactions.To bridge this gap, Vaucher et al. [42,43] compiled a database of reactions with associated action sequences, and trained models to generate actionable sequences of steps from a given reaction in SMILES format.Other tasks aim to link experimental results with molecular structures, effectively addressing the challenge of structural elucidation.A pioneering attempt using Transformers was recently made by Alberts et al. [63], who created a database of molecules and computationally generated IR spectra to train a Transformer model for this purpose.IR spectra, often overlooked for this task due to their complexity, were represented as a sequence of numerical values encoded in text alongside the chemical formula.These were then fed to the model, which was tasked with predicting the SMILES string of the underlying molecular structure.This method achieved a 45% accuracy rate in structure prediction, while also surpassing all previous efforts in predicting functional groups from IR spectra.This illustrates how underutilized data, such as IR spectra, can be harnessed for tasks as intricate as structural elucidation, despite their initial difficulty in interpretation.</p>
<p>These excellent applications of Transformers across a range of tasks in chemistry underscore just a fraction of the potential these models possess, as well as the wealth of latent knowledge embedded in chemical databases.So far, our discussion has focused on taskspecific applications, where a predefined task is established and models are specifically trained to solve that task.One of the advantages of language is its flexibility, enabling it to express various tasks in similar forms of the same language.The upcoming section will delve into the potential of more generalized language models for chemistry.</p>
<p>Advanced applications: Beyond task-specific models</p>
<p>The last few years have seen a remarkable rise in the power and popularity of foundation models: models pre-trained on vast amounts of data that can easily be adapted to other, more specific tasks [6,14].Such pre-training is conducted using rich databases of human text extracted from the internet, among others, to make the model learn the language along with general knowledge about the world.This rise has been achieved mainly through the escalation of Transformer architectures to large computational budgets and vast databases, leading to models capable of producing text that matches human-level quality in a wide range of scenarios [6,14].</p>
<p>The process of fine-tuning these models leverages their pre-learned language abilities while tailoring them toward a specific objective where less data might be available [64,65].A prime example of this is ChatGPT, a large language model fine-tuned for conversation.Its release has not only catapulted these models into popularity but has also reignited profound philosophical discussions about the nature of intelligence [7].However, it has also raised serious concerns about potential issues such as the spread of misinformation.The power and accessibility of ChatGPT have set the world on a new trajectory, prompting us to rethink how we produce and consume media, while also highlighting the need for careful consideration of the potential implications.The success and popularity of ChatGPT can be attributed to two key factors.Firstly, its freely available and highly user-friendly chat interface, which makes interaction with the model straightforward, and secondly, its impressive capabilities, which often extend beyond the tasks the model was initially trained for.These capabilities not only demonstrate the power of ChatGPT and models of its kind but also hint at their potential for innovative applications.</p>
<p>On the capabilities of Large Language Models</p>
<p>The rise of data-hungry machine learning algorithms, coupled with the increasing availability of data, has set a trend for scaling models to the maximum sizes that hardware constraints allow.As these models increase in size, they correspondingly improve in their capacity to perform the tasks they were originally trained for.This trend is particularly predictable in Their capabilities allow for a. general task solvers thanks to their flexibility and knowledge transferability [66,67] and b. agent architectures, capable of integrating virtually unlimited modalities, in the form of computational tools [68][69][70].</p>
<p>the realm of language models, and it manifests in the form of what researchers refer to as scaling laws [71].These scaling laws serve as a valuable tool for researchers, enabling them to identify performance trends and make accurate predictions about the capabilities of significantly larger models.However, the process of scaling does not simply enhance existing capabilities.As these models grow, they reach certain critical thresholds where not only their existing capabilities are enhanced, but entirely new capabilities are observed.</p>
<p>These new abilities are collectively referred to as emergent capabilities [72].They represent a fascinating aspect of model scaling, as they are not present or even predictable in smaller models, but suddenly appear as the models increase in size.These emergent capabilities offer exciting potential for the future of machine learning and its application in various fields, including chemistry.</p>
<p>Wei et al. [72] demonstrated how language models below certain computing budgets display somewhat random behavior across a range of tasks.However, once a certain model size is reached, sudden and significant improvements in performance are observed.Other capabilities are observed in the form of augmented prompting strategies, such as Chain of Thought (CoT) reasoning [73].In this approach, models are instructed to solve a task by following a step by step reasoning sequence.Another emergent capability is instruction following [74], where LMs are given a task in the form of a set of instructions to follow.</p>
<p>Interestingly, these techniques generally have a negative impact on the performance of smaller models [73].However a positive effect on performance is observed once models reach certain threshold sizes [72].</p>
<p>Emergent capabilities, therefore, enable language models to effectively tackle a variety of tasks that involve reasoning.This is achieved without explicit training and with the flexibility of a text query in natural language.Considering the remarkable capabilities and transformative potential of large language models in various fields, it naturally leads us to question what benefits these models could bring to the field of chemistry.This will be the focus of our exploration in the following sections.</p>
<p>Large Language Models in chemistry</p>
<p>Much of this chapter has been devoted to the application of the Transformer architecture to ingeniously-encoded specific chemical tasks.However, it is important to note that a significant portion of information in chemistry is conveyed through human language, which includes explanations of reaction mechanisms, descriptions of modes of action of drugs, to name a few.Reasoning in chemistry is thus fundamentally articulated in human language, even though it is complemented by other non-text objects like graphs and images.This observation raises the question of whether Large Language Models (LLMs) can replicate this level of chemical reasoning and, if so, to what extent.</p>
<p>Tailoring techniques: fine-tuning and in-context learning</p>
<p>One of the main goals of developing general, large pretrained language models, is being able to further tune them for specific applications.Techniques for this include fine-tuning, where a model's parameters are further optimized for a new task.This technique has been the dominating paradigm in machine learning for years, with excellent results in a number of applications [75][76][77].</p>
<p>A new paradigm has also become the target of investigations with the rise of LLMs, so-called in-context learning.This feature can be activated simply by providing the model with a task description, along with a small set of examples that serve as "training data".This method proves particularly useful when data is scarce or obtaining it is costly or time-consuming, as is often the case in chemistry laboratories.LLMs have demonstrated impressive performance across a range of tasks using this method, with their strong performance believed to stem from knowledge transfer across different pre-training sources.</p>
<p>In line with this, Jablonka et al. [66] demonstrated that LLMs like GPT-3 can effectively solve various tasks in chemistry and materials science by fine-tuning LLMs, to a good level of approximation.The authors chose a range of tasks for which datasets are typically limited, such as predicting the transition wavelength of molecules, the phase of solid solutions in high entropy alloys, or even inverse design.Given the limited datasets available for these tasks, machine learning solutions have traditionally relied on heavily engineered and specialized algorithms [50,78,79].These algorithms aim to directly incorporate chemical knowledge into the model architectures.In parallel, extensive research has been devoted to molecular and reaction representations, with some efforts also attempting to bias the representations using prior chemical knowledge to better encode relevant features in data [80].All these endeavors aim to make the most of existing knowledge, enabling models to learn as much as possible from the few available data points.</p>
<p>Interestingly, Jablonka et al. [66] demonstrated that fine-tuning and in-context learning can perform on par with, and in some instances even outperform, these specialized techniques, particularly when data is limited.The high performance of in-context learning, combined with its ease of use and flexibility, makes it one of the most impressive applications of LLMs in chemistry to date.This technique holds the potential to revolutionize the way machine learning is utilized in the scientific field, by rapidly highlighting complex correlations in data.</p>
<p>Another application of regression, which holds significant interest in chemistry and drug discovery cycles, is optimization.This process involves modifying an object until a property of interest reaches a desired value.Typically, this requires a large number of measurements of the desired property, which can be quite costly in chemistry use-cases.Applications of this include yield/selectivity optimization in chemical reactions and the generation of molecular candidates with target properties.Bayesian Optimization (BO) has recently been proposed as a solution to such problems in chemistry [81,82], particularly in situations where data is small.However, BO requires uncertainty-calibrated regression methods, which sets it apart from conventional regression.</p>
<p>In line with the concept of in-context learning, Ramos et al. [67] proposed a system that utilizes GPT models to perform regression while also incorporating uncertainty.This approach enables BO without the need for any feature engineering or fine-tuning.The flexibility of this method allowed the team to perform catalyst and molecular optimization using only the synthesis procedure of the catalyst as input.This work represents a paradigm shift in drug discovery and molecular design.For the first time, it showcases a direct map from synthesis procedure into property space, effectively overcoming issues like the synthesizability of proposed molecules, a key limitation of structure-based generative models.</p>
<p>Molecular generation</p>
<p>Another fascinating application of the generative capabilities of language models is molecular generation.This area, which is of significant importance in the drug discovery process, has been largely dominated by models that generate molecules in the form of linear string representations, such as SMILES or SELFIES [83][84][85].While this approach has proven successful due to the ease of training these models [66,86,87], its successful application is contingent on the ability to specify substances as graphs and their subsequent conversion to a linear string representation.However, this approach is only suitable for a subset of organic molecules.Other substances, such as macromolecules and materials, necessitate more comprehensive representations.A complete and accurate representation of these substances can only be achieved by specifying atomic positions, boundary conditions, and other factors.This requirement presents a significant challenge and limitation to the current methods of molecular generation.To address these limitations, Flam-Shepherd and Aspuru-Guzik [88] proposed using language models for structure generation, directly generating them with three-dimensional atomic positions.Besides being innovative and valid, the generated structures can be obtained by training models in a variety of formats used for crystals, proteins, and more.This work also demonstrates performance comparable to expert-designed, state-of-the-art algorithms for molecular generation based on graphs, while overcoming the limitations mentioned earlier.</p>
<p>Language Model-powered Agents</p>
<p>Among the most useful emergent abilities of language models are step-by-step reasoning, activated through chain-of-thought (CoT) prompting, and their capacity to effectively use tools [89].These capabilities have been the subject of extensive research in recent years, and their application has been shown to significantly enhance the performance of LLMs across a variety of tasks.CoT prompting is a technique where language models are instructed to solve a task by following a sequence of reasoning steps, rather than providing an answer in a single response [73].Instructing language models in this way effectively allows them to perform symbolic operations, much like humans perform arithmetic operations by keeping track of intermediate steps.</p>
<p>The ability to use tools is another significant capability of language models [89].This allows them to invoke external computational tools, thereby enriching their knowledge through querying search engines, accessing calculators, and so on [89].These capabilities have been demonstrated to enhance the performance of large language models in a range of tasks that were previously inaccessible.The recent advancements and results in revealing and exploiting the capabilities of LLMs suggest the potential for combining some of these capabilities to create more powerful and useful possibilities.This concept has been recently explored, leading to the development of the Modular Reasoning, Knowledge and Language (MRKL [90]) and Reason+Act (ReAct [91]) systems, which combine the CoT and tool-using capabilities of modern LLMs.By incorporating external tools into a CoT setting, agents of this type have recently been shown to outperform other methods based on large language models.</p>
<p>One direct benefit of effective tool usage is that it partially overcomes the unimodality issue of LLMs.Under this setting, they become capable of processing different types of input data, making real-time decisions in simulated environments, and even interacting with real-world robotic platforms.The solutions provided by LLMs to tasks also become more grounded in reality, as access to certain tools provides them with real, up-to-date information relevant to the task.This can, to some extent, limit the tendency of these models to generate unrealistic or "hallucinated" responses.</p>
<p>Agents in chemistry: Unleashing the power from tools</p>
<p>Despite their strengths as text generators and task solvers, and their remarkable few-shot and zero-shot performance, these models are also well known for their high propensity to generate false and inaccurate content, an issue that extends to easily verifiable matters such as basic arithmetic [89] and chemical operations [92].These limitations make the direct application of LLMs to chemistry a challenging matter.</p>
<p>The potential applications of large language models in chemistry were first explored in a large-scale collaboration involving researchers from around the world, an effort that resulted in the demonstration of 14 use-cases [68].The applications range from wrappers for computational tools, which enhance their accessibility by allowing natural language inputs to modify behaviors, to assistants for reaction optimization, and knowledge parsers and synthesizers for scientific question answering, among others.These are just a few of the possibilities that LLMs offer in chemistry, which, when combined with existing chemistry tools and databases, significantly increase the applicability and accessibility of computational applications.</p>
<p>More recently, Bran and Cox et al. [69] extended the concept of LLM-powered agents for chemistry by curating and compiling a set of computational chemistry tools.Their system, ChemCrow, has been shown to be capable of planning and executing tasks in chemistry, effectively streamlining the reasoning process for several common chemical tasks across areas such as drug and materials design and synthesis.The authors demonstrate that this approach has a highly positive effect on LLM's performance for tasks in chemistry, overcoming hallucinations and grounding their responses with data from reliable sources.Complementary approaches exist, with a sharper focus on cloud lab operability [70].</p>
<p>The power of platforms like ChemCrow extends beyond merely serving as independent task solvers.They can be viewed as general chemistry assistants with the ultimate goal of making computational tools more accessible to chemists, thereby accelerating discovery.An additional highlight is the seamless exploitation of tool composability that this allows.It makes it straightforward to enrich the results of one tool with another, or to construct custom tool pipelines, all through simple requests in natural language.</p>
<p>Outlook and final remarks</p>
<p>Advancements in neural translation models, and specially with the introduction of the Transformer architecture, has sparked a revolution in machine learning for applications in chemistry and drug development.Analogies between chemical and natural language, and the publication of open databases and benchmarks, have inspired the representation of chemical tasks in the form of text, allowing straightforward application of Transformers to problems in this field.This revolution has occurred in three stages, differentiated by the specificity of tasks.In a first stage, characterized by task-specificity and use of singlemodality models, applications spanned molecule-to-molecule conversion tasks, like reaction outcome prediction and retrosynthetic planning, along with representation learning and downstream tasks like regression and classification.Their excellent performance and relative simplicity made them de-facto models in an array of applications.</p>
<p>In a second stage, researchers attempted to connect multiple additional modalities relevant to chemistry, like spectra from experiments, sequences of experimental actions, and even natural language, opening the way for an expanded number of applications involving modalities of any sort, however still task-specific.More recently, powered by vertiginous advancements in training and tuning of large language models, a series of works have been published that leverage a number of capabilities from such models.Among others, these contributions showcase applications in regression, classification, molecular generation and reaction optimization, all with unprecedented flexibility and usually improved performance over other methods.Another direction explores the integration of virtually unlimited modalities -in the form of tools-into agents powered by LLMs.The power of these agents has been demonstrated through a number of diverse tasks, ranging from molecular generation to automated organic synthesis, in an open-ended, highly customizable fashion.</p>
<p>By leveraging the expressivity and flexibility of natural language, this last wave of applications aims to bridge the gap between the chemical and natural languages.As we continue to explore and harness these capabilities, we can look forward to a future where machine learning plays an even more integral role in accelerating scientific discovery.</p>
<p>Fig. 1
1
Fig.1Advances in Natural Language Processing have inspired applications in Chemistry.Over time, the gap between chemical Language and natural Language is being closed by including additional modalities.Most recent works present general task solvers for chemistry, capable of chemical reasoning and automatic synthesis, among others.</p>
<p>Fig. 3
3
Fig. 3 Recent advances in Large Language Models have launched a new era of Transformers in chemistry.Their capabilities allow for a. general task solvers thanks to their flexibility and knowledge transferability[66,67] and b. agent architectures, capable of integrating virtually unlimited modalities, in the form of computational tools[68][69][70].</p>
<p>AcknowledgementsThis work was created as part of NCCR Catalysis (grant number 180544), a National Centre of Competence in Research funded by the Swiss National Science Foundation.
D Bahdanau, K Cho, Y Bengio, 10.48550/arXiv.1409.0473arXiv.arXiv:1409.0473stat]Neural Machine Translation by Jointly Learning to Align and Translate. 2016</p>
<p>Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. K Cho, B Merrienboer, C Gulcehre, D Bahdanau, F Bougares, H Schwenk, Y Bengio, 10.48550/arXiv.1406.1078arXiv.arXiv:1406.10782014cs, stat</p>
<p>Long Short-Term Memory. S Hochreiter, J Schmidhuber, 10.1162/neco.1997.9.8.1735Neural Computation. 981997</p>
<p>I Sutskever, O Vinyals, Q V Le, 10.48550/arXiv.1409.3215arXiv.arXiv:1409.3215Sequence to Sequence Learning with Neural Networks. 2014</p>
<p>Attention Is All You Need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, 10.48550/arXiv.1706.03762arXiv.arXiv:1706.037622017</p>
<p>R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, E Brynjolfsson, S Buch, D Card, R Castellon, N Chatterji, A Chen, K Creel, J Q Davis, D Demszky, C Donahue, M Doumbouya, E Durmus, S Ermon, J Etchemendy, K Ethayarajh, L Fei-Fei, C Finn, T Gale, L Gillespie, K Goel, N Goodman, S Grossman, N Guha, T Hashimoto, P Henderson, J Hewitt, D E Ho, J Hong, K Hsu, J Huang, T Icard, S Jain, D Jurafsky, P Kalluri, S Karamcheti, G Keeling, F Khani, O Khattab, P W Koh, M Krass, R Krishna, R Kuditipudi, A Kumar, F Ladhak, M Lee, T Lee, J Leskovec, I Levent, X L Li, X Li, T Ma, A Malik, C D Manning, S Mirchandani, E Mitchell, Z Munyikwa, S Nair, A Narayan, D Narayanan, B Newman, A Nie, J C Niebles, H Nilforoshan, J Nyarko, G Ogut, L Orr, I Papadimitriou, J S Park, C Piech, E Portelance, C Potts, A Raghunathan, R Reich, H Ren, F Rong, Y Roohani, C Ruiz, J Ryan, C Ré, D Sadigh, S Sagawa, K Santhanam, A Shih, K Srinivasan, A Tamkin, R Taori, A W Thomas, F Tramèr, R E Wang, W Wang, B Wu, J Wu, Y Wu, S M Xie, M Yasunaga, J You, M Zaharia, M Zhang, T Zhang, X Zhang, Y Zhang, L Zheng, K Zhou, P Liang, arXiv.arXiv:2108.07258On the Opportunities and Risks of Foundation Models. 2022</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, H Nori, H Palangi, M T Ribeiro, Y Zhang, arXiv:2303.12712Sparks of Artificial General Intelligence: Early experiments with GPT-4. arXiv. 2023</p>
<p>L Gao, S Biderman, S Black, L Golding, T Hoppe, C Foster, J Phang, H He, A Thite, N Nabeshima, S Presser, C Leahy, 10.48550/arXiv.2101.00027arXiv.arXiv:2101.00027The Pile: An 800GB Dataset of Diverse Text for Language Modeling. 2020</p>
<p>Exploring the Limits of Transfer Learning with a Unified Text-to. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, 10.48550/arXiv.1910.10683arXiv:1910.106832020Text Transformer. arXiv.cs, stat</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, 10.48550/arXiv.1810.04805arXiv.arXiv:1810.048052019</p>
<p>N Reimers, I Gurevych, 10.48550/arXiv.1908.10084arXiv.arXiv:1908.10084Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. 2019</p>
<p>P J Liu, M Saleh, E Pot, B Goodrich, R Sepassi, L Kaiser, N Shazeer, arXiv.arXiv:1801.10198[cs]Generating Wikipedia by Summarizing Long Sequences. 2018</p>
<p>Neural Text Summarization: A Critical Evaluation. W Kryscinski, N S Keskar, B Mccann, C Xiong, R Socher, 10.18653/v1/D19-1051Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong; ChinaAssociation for Computational Linguistics2019</p>
<p>. 10.48550/arXiv.2303.08774arXiv.arXiv:2303.087742023Technical Report</p>
<p>Highly accurate protein structure prediction with AlphaFold. J Jumper, R Evans, A Pritzel, T Green, M Figurnov, O Ronneberger, K Tunyasuvunakool, R Bates, A Žídek, A Potapenko, A Bridgland, C Meyer, S A A Kohl, A J Ballard, A Cowie, B Romera-Paredes, S Nikolov, R Jain, J Adler, T Back, S Petersen, D Reiman, E Clancy, M Zielinski, M Steinegger, M Pacholska, T Berghammer, S Bodenstein, D Silver, O Vinyals, A W Senior, K Kavukcuoglu, P Kohli, D Hassabis, 10.1038/s41586-021-03819-22023-08-07Nature. 59678732021Nature Publishing Group</p>
<p>AlphaFold2 and its applications in the fields of biology and medicine. Z Yang, X Zeng, Y Zhao, R Chen, 10.1038/s41392-023-01381-zSignal Transduction and Targeted Therapy. 812023Nature Publishing Group</p>
<p>M Akdel, D E V Pires, E P Pardo, J Jänes, A O Zalevsky, B Mészáros, P Bryant, L L Good, R A Laskowski, G Pozzati, A Shenoy, W Zhu, P Kundrotas, V R Serra, C H M Rodrigues, A S Dunham, D Burke, N Borkakoti, S Velankar, A Frost, J Basquin, K Lindorff-Larsen, A Bateman, A V Kajava, A Valencia, S Ovchinnikov, J Durairaj, D B Ascher, J M Thornton, N E Davey, A Stein, A Elofsson, T I Croll, P Beltrao, 10.1038/s41594-022-00849-wA structural biology community assessment of AlphaFold2 applications. Nature Publishing Group202229</p>
<p>K Huang, T Fu, W Gao, Y Zhao, Y Roohani, J Leskovec, C Coley, C Xiao, J Sun, M Zitnik, Therapeutics Data Commons: Machine Learning Datasets and Tasks for Drug Discovery and Development. Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks. 2021</p>
<p>The Open Reaction Database. S M Kearnes, M R Maser, M Wleklinski, A Kast, A G Doyle, S D Dreher, J M Hawkins, K F Jensen, C W Coley, 10.1021/jacs.1c09820Journal of the American Chemical Society. 143452021American Chemical Society</p>
<p>Extraction of chemical structures and reactions from the literature. D Lowe, 10.17863/CAM.162932012University of CambridgePhD thesis</p>
<p>MoleculeNet: a benchmark for molecular machine learning. Z Wu, B Ramsundar, E N Feinberg, J Gomes, C Geniesse, A S Pappu, K Leswing, V Pande, 10.1039/C7SC02664AChemical Science. 922018The Royal Society of Chemistry</p>
<p>Organic Chemistry as a Language and the Implications of Chemical Linguistics for Structural and Retrosynthetic Analyses. A Cadeddu, E K Wylie, J Jurczak, M Wampler-Doty, B A Grzybowski, 10.1002/anie.201403708Angewandte Chemie International Edition. 53312014</p>
<p>Computer-designed repurposing of chemical wastes into drugs. A Wo Los, D Koszelewski, R Roszak, S Szymkuć, M Moskal, R Ostaszewski, B T Herrera, J M Maier, G Brezicki, J Samuel, J A M Lummiss, D T Mcquade, L Rogers, B A Grzybowski, 10.1038/s41586-022-04503-9Nature. 60479072022Nature Publishing Group</p>
<p>Extraction of organic chemistry grammar from unsupervised learning of chemical reactions. P Schwaller, B Hoover, J.-L Reymond, H Strobelt, T Laino, 10.1126/sciadv.abe41662023-02-07Science Advances. 71541662021American Association for the Advancement of Science</p>
<p>SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. D Weininger, 10.1021/ci00057a005Journal of Chemical Information and Computer Sciences. 2811988American Chemical Society</p>
<p>DeepSMILES: An Adaptation of SMILES for Use in Machine-Learning of Chemical Structures. N O'boyle, A Dalke, 10.26434/chemrxiv.7097960.v1ChemRxiv. 2018</p>
<p>Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation. M Krenn, F Häse, A Nigam, P Friederich, A Aspuru-Guzik, 10.1088/2632-2153/aba947Machine Learning: Science and Technology. 142020Publisher: IOP Publishing</p>
<p>M Krenn, Q Ai, S Barthel, N Carson, A Frei, N C Frey, P Friederich, T Gaudin, A A Gayle, K M Jablonka, R F Lameiro, D Lemm, A Lo, S M Moosavi, J M Nápoles-Duarte, A Nigam, R Pollice, K Rajan, U Schatzschneider, P Schwaller, M Skreta, B Smit, F Strieth-Kalthoff, C Sun, G Tom, G F Rudorff, A Wang, A White, A Young, R Yu, A Aspuru-Guzik, 10.1016/j.patter.2022.100588arXiv:2204.000562023-08-07SELFIES and the future of molecular string representations. 20223100588</p>
<p>InChI, the IUPAC International Chemical Identifier. S R Heller, A Mcnaught, I Pletnev, S Stein, D Tchekhovskoi, 10.1186/s13321-015-0068-4Journal of Cheminformatics. 712015</p>
<p>TUCAN: A molecular identifier and descriptor applicable to the whole periodic table from hydrogen to oganesson. J C Brammer, G Blanke, C Kellner, A Hoffmann, S Herres-Pawlis, U Schatzschneider, 10.1186/s13321-022-00640-5Journal of Cheminformatics. 1412022</p>
<p>Chemical space: limits, evolution and modelling of an object bigger than our universal library. G Restrepo, 10.1039/D2DD00030JDigital Discovery. 152022Royal Society of Chemistry</p>
<p>Grammar Variational Autoencoder. M J Kusner, B Paige, J M Hernández-Lobato, 10.48550/arXiv.1703.01925arXiv.arXiv:1703.019252017</p>
<p>Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules. R Gómez-Bombarelli, J N Wei, D Duvenaud, J M Hernández-Lobato, B Sánchez-Lengeling, D Sheberla, J Aguilera-Iparraguirre, T D Hirzel, R P Adams, A Aspuru-Guzik, 10.1021/acscentsci.7b00572ACS Central Science. 422018American Chemical Society</p>
<p>Exploring chemical space using natural language processing methodologies for drug discovery. H Öztürk, A Özgür, P Schwaller, T Laino, E Ozkirimli, 10.1016/j.drudis.2020.01.020Drug Discovery Today. 2542020</p>
<p>Transfer learning enables the molecular transformer to predict regio-and stereoselective reactions on carbohydrates. G Pesciullesi, P Schwaller, T Laino, J.-L Reymond, 10.1038/s41467-020-18671-7Nature Communications. 1112020Nature Publishing Group</p>
<p>Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction. P Schwaller, T Laino, T Gaudin, P Bolgar, C A Hunter, C Bekas, A A Lee, 10.1021/acscentsci.9b005762023-02-07ACS Central Science. 592019American Chemical Society</p>
<p>Predicting retrosynthetic pathways using transformer-based models and a hyper-graph exploration strategy. P Schwaller, R Petraglia, V Zullo, V H Nair, R A Haeuselmann, R Pisoni, C Bekas, A Iuliano, T Laino, 10.1039/C9SC05704HChemical Science. 11122020. 2023-07-20The Royal Society of Chemistry</p>
<p>Mol-BERT: An Effective Molecular Representation with BERT for Molecular Property Prediction. J Li, X Jiang, 10.1155/2021/7181815Wireless Communications and Mobile Computing 2021. 2021. 2023-07-277181815</p>
<p>ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction. S Chithrananda, G Grand, B Ramsundar, 10.48550/arXiv.2010.09885arXiv.arXiv:2010.098852020physics, q-bio</p>
<p>W Ahmad, E Simon, S Chithrananda, G Grand, B Ramsundar, 10.48550/arXiv.2209.01712arXiv.arXiv:2209.01712ChemBERTa-2: Towards Chemical Foundation Models. 2022cs, q-bio</p>
<p>Mapping the space of chemical reactions using attention-based neural networks. P Schwaller, D Probst, A C Vaucher, V H Nair, D Kreutter, T Laino, J.-L Reymond, 10.1038/s42256-020-00284-wNature Machine Intelligence. 322021. 2023-07-25Nature Publishing Group</p>
<p>Automated extraction of chemical synthesis actions from experimental procedures. A C Vaucher, F Zipoli, J Geluykens, V H Nair, P Schwaller, T Laino, 10.1038/s41467-020-17266-6Nature Communications. 1112020Nature Publishing Group</p>
<p>Inferring experimental procedures from text-based representations of chemical reactions. A C Vaucher, P Schwaller, J Geluykens, V H Nair, A Iuliano, T Laino, 10.1038/s41467-021-22951-1Nature Communications. 1212021Nature Publishing Group</p>
<p>State-of-the-art augmented NLP transformer models for direct and single-step retrosynthesis. I V Tetko, P Karpov, R Van Deursen, G Godin, 10.1038/s41467-020-19266-yNature Communications. 1112020Nature Publishing Group</p>
<p>Enhancing diversity in language based models for single-step retrosynthesis. A Toniato, C Vaucher, A Schwaller, P Laino, T , 10.1039/D2DD00110A2023-08-03Digital Discovery. 222023Royal Society of Chemistry</p>
<p>Unbiasing Retrosynthesis Language Models with Disconnection Prompts. A Thakkar, A C Vaucher, A Byekwaso, P Schwaller, A Toniato, T Laino, 10.1021/acscentsci.3c003722023-08-03ACS Central Science. 972023American Chemical Society</p>
<p>Chemformer: a pre-trained transformer for computational chemistry. R Irwin, S Dimitriadis, J He, E J Bjerrum, 10.1088/2632-2153/ac3ffb2023-02-15Machine Learning: Science and Technology. 31150222022Publisher: IOP Publishing</p>
<p>Permutation invariant graph-to-sequence model for template-free retrosynthesis and reaction prediction. Z Tu, C W Coley, 10.48550/arXiv.2110.09681arXiv.arXiv:2110.096812021</p>
<p>Distributed Representations of Words and Phrases and their Compositionality. T Mikolov, I Sutskever, K Chen, G Corrado, J Dean, ArXiv. 2013</p>
<p>D Duvenaud, D Maclaurin, J Aguilera-Iparraguirre, R Gómez-Bombarelli, T Hirzel, A Aspuru-Guzik, R P Adams, 10.48550/arXiv.1509.09292arXiv.arXiv:1509.09292Convolutional Networks on Graphs for Learning Molecular Fingerprints. 2015cs, stat</p>
<p>SMILES-BERT: Large Scale Unsupervised Pre-Training for Molecular Property Prediction. S Wang, Y Guo, Y Wang, H Sun, J Huang, 10.1145/3307339.3342186Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics. the 10th ACM International Conference on Bioinformatics, Computational Biology and Health InformaticsNiagara Falls NY USAACM2019</p>
<p>Machine intelligence for chemical reaction space. P Schwaller, A C Vaucher, R Laplaza, C Bunne, A Krause, C Corminboeuf, T Laino, 10.1002/wcms.1604WIREs Computational Molecular Science. 1252022</p>
<p>Prediction of chemical reaction yields using deep learning. P Schwaller, A C Vaucher, T Laino, J.-L Reymond, 10.1088/2632-2153/abc81dMachine Learning: Science and Technology. 212021Publisher: IOP Publishing</p>
<p>Global reactivity models are impactful in industrial synthesis applications. P Neves, K Mcclure, J Verhoeven, N Dyubankova, R Nugmanov, A Gedich, S Menon, Z Shi, J K Wegner, 10.1186/s13321-023-00685-0Journal of Cheminformatics. 1512023</p>
<p>J Ross, B Belgodere, V Chenthamarakshan, I Padhi, Y Mroueh, P Das, 10.48550/arXiv.2106.09553arXiv.arXiv:2106.09553Large-Scale Chemical Language Representations Capture Molecular Structure and Properties. 2022cs, q-bio</p>
<p>Molformer: Motif-based Transformer on 3D Heterogeneous Molecular Graphs. F Wu, D Radev, S Z Li, 10.48550/arXiv.2110.01191arXiv.arXiv:2110.011912023cs, q-bio</p>
<p>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. A Rives, J Meier, T Sercu, S Goyal, Z Lin, J Liu, D Guo, M Ott, C L Zitnick, J Ma, R Fergus, 10.1073/pnas.20162391182023-08-07Proceedings of the National Academy of Sciences. 118152016239118. 2021Proceedings of the National Academy of Sciences</p>
<p>Evolutionary-scale prediction of atomic-level protein structure with a language model. Z Lin, H Akin, R Rao, B Hie, Z Zhu, W Lu, N Smetanin, R Verkuil, O Kabeli, Y Shmueli, A Santos Costa, M Fazel-Zarandi, T Sercu, S Candido, 10.1126/science.ade25742023-08-07Science. 37966372023American Association for the Advancement of Science</p>
<p>Language models generalize beyond natural proteins. R Verkuil, O Kabeli, Y Du, B I M Wicky, L F Milles, J Dauparas, D Baker, S Ovchinnikov, T Sercu, A Rives, 10.1101/2022.12.21.521521v1bioRxiv. 2022Pages: 2022.12.21.521521 Section: New Results</p>
<p>Language models can identify enzymatic active sites in protein sequences. Y G N Teukam, L K Dassi, M Manica, D Probst, T Laino, </p>
<p>C Edwards, T Lai, K Ros, G Honke, K Cho, H Ji, 10.48550/arXiv.2204.11817arXiv.arXiv:2204.11817Translation between Molecules and Natural Language. 2022</p>
<p>Unifying Molecular and Textual Representations via Multi-task Language Modelling. D Christofidellis, G Giannone, J Born, O Winther, T Laino, M Manica, 10.48550/arXiv.2301.12586arXiv.arXiv:2301.125862023</p>
<p>Leveraging Infrared Spectroscopy for Automated Structure Elucidation. M Alberts, T Laino, A C Vaucher, 10.26434/chemrxiv-2023-5v27fChemistry. May 2023preprint</p>
<p>Finetuning Large Language Models. S Raschka, 2023</p>
<p>LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention. R Zhang, J Han, A Zhou, X Hu, S Yan, P Lu, H Li, P Gao, Y Qiao, arXiv.arXiv:2303.161992023</p>
<p>Is GPT-3 all you need for low-data discovery in chemistry?. K M Jablonka, P Schwaller, A Ortega-Guerrero, B Smit, 10.26434/chemrxiv-2023-fw8n4ChemRxiv. 2023</p>
<p>M C Ramos, S S Michtavy, M D Porosoff, A D White, 10.48550/arXiv.2304.05341arXiv.arXiv:2304.05341Bayesian Optimization of Catalysts With In-context Learning. 2023</p>
<p>K M Jablonka, Q Ai, A Al-Feghali, S Badhwar, J D Bocarsly, A M Bran, S Bringuier, L C Brinson, K Choudhary, D Circi, S Cox, W A Jong, M L Evans, N Gastellu, J Genzling, M V Gil, A K Gupta, Z Hong, A Imran, S Kruschwitz, A Labarre, J Lála, T Liu, S Ma, S Majumdar, G W Merz, N Moitessier, E Moubarak, B Mouriño, B Pelkie, M Pieler, M C Ramos, B Ranković, S G Rodriques, J N Sanders, P Schwaller, M Schwarting, J Shi, B Smit, B E Smith, J Van Herck, C Völker, L Ward, S Warren, B Weiser, S Zhang, X Zhang, G A Zia, A Scourtas, K J Schmidt, I Foster, A D White, B Blaiszik, 10.48550/arXiv.2306.06283arXiv.arXiv:2306.0628314 Examples of How LLMs Can Transform Materials Science and Chemistry: A Reflection on a Large Language Model Hackathon. 2023cond-mat, physics:physics</p>
<p>A M Bran, S Cox, A D White, P Schwaller, 10.48550/arXiv.2304.05376arXiv.arXiv:2304.05376ChemCrow: Augmenting largelanguage models with chemistry tools. 2023physics, stat</p>
<p>Emergent autonomous scientific research capabilities of large language models. D A Boiko, R Macknight, G Gomes, 10.48550/ARXIV.2304.053322023Publisher: arXiv Version Number: 1</p>
<p>J Hoffmann, S Borgeaud, A Mensch, E Buchatskaya, T Cai, E Rutherford, D D L Casas, L A Hendricks, J Welbl, A Clark, T Hennigan, E Noland, K Millican, G V D Driessche, B Damoc, A Guy, S Osindero, K Simonyan, E Elsen, J W Rae, O Vinyals, L Sifre, arXiv.arXiv:2203.15556Training Compute-Optimal Large Language Models. 2022</p>
<p>J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, D Yogatama, M Bosma, D Zhou, D Metzler, E H Chi, T Hashimoto, O Vinyals, P Liang, J Dean, W Fedus, 10.48550/ARXIV.2206.07682Emergent Abilities of Large Language Models. 2022Publisher: arXiv Version Number: 2</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q Le, D Zhou, 10.48550/arXiv.2201.11903arXiv.arXiv:2201.119032023</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C L Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, J Schulman, J Hilton, F Kelton, L Miller, M Simens, A Askell, P Welinder, P Christiano, J Leike, R Lowe, 10.48550/arXiv.2203.02155arXiv.arXiv:2203.021552022</p>
<p>Finetuned Language Models are Zero-Shot Learners. J Wei, M Bosma, V Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, 2021</p>
<p>Fine-tuning and visualization of convolutional neural networks. X Yin, W Chen, X Wu, H Yue, 10.1109/ICIEA.2017.828304112th IEEE Conference on Industrial Electronics and Applications (ICIEA). 2017. 2017</p>
<p>Universal Language Model Fine-tuning for Text Classification. J Howard, S Ruder, 10.18653/v1/P18-1031Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, Australia20181Association for Computational Linguistics</p>
<p>Retrosynthesis Prediction with Conditional Graph Logic Network. H Dai, C Li, C W Coley, B Dai, L Song, 10.48550/arXiv.2001.01408arXiv.arXiv:2001.014082020cs, stat</p>
<p>Chemistry-informed molecular graph as reaction descriptor for machine-learned retrosynthesis planning. B Zhang, X Zhang, W Du, Z Song, G Zhang, G Zhang, Y Wang, X Chen, J Jiang, Y Luo, 10.1073/pnas.22127111192023-08-07Company: National Academy of Sciences Distributor: National Academy of Sciences Institution. 1194122127111192022National Academy of Sciences Label: National Academy of Sciences Publisher: Proceedings of the National Academy of SciencesProceedings of the National Academy of Sciences</p>
<p>. K Jorner, L Turcani, 10.5281/zenodo.70175992022</p>
<p>Bayesian optimisation for additive screening and yield improvements in chemical reactions -beyond one-hot encoding. B Ranković, R.-R Griffiths, H B Moss, P Schwaller, 10.26434/chemrxiv-2022-nll2j-v3ChemRxiv. 2023</p>
<p>Bayesian reaction optimization as a tool for chemical synthesis. B J Shields, J Stevens, J Li, M Parasram, F Damani, J I M Alvarado, J M Janey, R P Adams, A G Doyle, 10.1038/s41586-021-03213-y2023-02-08Nature. 59078442021Nature Publishing Group</p>
<p>A Transformer-based Generative Model for De Novo Molecular Design. W Wang, Y Wang, H Zhao, S Sciabola, 10.48550/arXiv.2210.08749arXiv.arXiv:2210.087492022cs, q-bio</p>
<p>C5T5: Controllable Generation of Organic Molecules with Transformers. D Rothchild, A Tamkin, J Yu, U Misra, J Gonzalez, 10.48550/arXiv.2108.10307arXiv.arXiv:2108.103072021</p>
<p>MolGPT: Molecular Generation Using a Transformer-Decoder Model. V Bagal, R Aggarwal, P K Vinod, U D Priyakumar, 10.1021/acs.jcim.1c006002023-08-05Journal of Chemical Information and Modeling. 6292022American Chemical Society</p>
<p>Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation. E Bengio, M Jain, M Korablyov, D Precup, Y Bengio, 10.48550/arXiv.2106.04399arXiv.arXiv:2106.043992021</p>
<p>Regression Transformer enables concurrent sequence regression and generation for molecular language modelling. J Born, M Manica, 10.1038/s42256-023-00639-z2023-07-30Nature Machine Intelligence. 542023Nature Publishing Group</p>
<p>Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files. D Flam-Shepherd, A Aspuru-Guzik, 10.48550/arXiv.2305.05708arXiv.arXiv:2305.057082023cs, q-bio</p>
<p>T Schick, J Dwivedi-Yu, R Dessì, R Raileanu, M Lomeli, L Zettlemoyer, N Cancedda, T Scialom, 10.48550/arXiv.2302.04761arXiv:2302.04761Toolformer: Language Models Can Teach Themselves to Use Tools. arXiv. 2023</p>
<p>MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning. E Karpas, O Abend, Y Belinkov, B Lenz, O Lieber, N Ratner, Y Shoham, H Bata, Y Levine, K Leyton-Brown, D Muhlgay, N Rozen, E Schwartz, G Shachaf, S Shalev-Shwartz, A Shashua, M Tenenholtz, arXiv.arXiv:2205.004452022</p>
<p>S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, arXiv.arXiv:2210.03629ReAct: Synergizing Reasoning and Acting in Language Models. 2023</p>
<p>Assessment of chemistry knowledge in large language models that generate code. D White, A , M Hocky, G , A Gandhi, H Ansari, M Cox, S , P Wellawatte, G Sasmal, S Yang, Z Liu, K Singh, Y Ccoa, W J P , 10.1039/D2DD00087CDigital Discovery. 222023. 2023-07-25Royal Society of Chemistry</p>            </div>
        </div>

    </div>
</body>
</html>
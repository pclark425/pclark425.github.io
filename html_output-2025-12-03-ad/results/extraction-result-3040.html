<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3040 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3040</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3040</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-74.html">extraction-schema-74</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-269982440</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.13009v2.pdf" target="_blank">MetaReflection: Learning Instructions for Language Agents using Past Reflections</a></p>
                <p><strong>Paper Abstract:</strong> The popularity of Large Language Models (LLMs) have unleashed a new age of Language Agents for solving a diverse range of tasks. While contemporary frontier LLMs are capable enough to power reasonably good Language agents, the closed-API model makes it hard to improve in cases they perform sub-optimally. To address this, recent works have explored techniques to improve their performance using self reflection and prompt optimization techniques. While techniques like self reflection work well in an online setup, contemporary prompt optimization techniques are designed to work on simpler tasks. To address this, we introduce METAREFLECTION, a novel offline reinforcement learning technique that enhances the performance of Language Agents by augmenting a semantic memory based on experiential learnings from past trials. We demonstrate the efficacy of METAREFLECTION by evaluating across multiple domains, including complex logical reasoning, biomedical semantic similarity, open world question answering, and vulnerability threat detection, in Infrastructure-as-Code, with different agent design. METAREFLECTION boosts Language agents’ performance by 4 % to 16.82 % over the raw GPT-4 baseline and performs on par with existing state-of-the-art prompt optimization techniques while requiring fewer LLM calls.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3040.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3040.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>METAREFLECTION (single-step)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>METAREFLECTION applied to single-step LLM agents (semantic instruction memory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>METAREFLECTION is an offline reinforcement-style technique that learns a compact, rule-based semantic memory (a list of instructions) from self-reflections on failed trajectories and appends this memory to a single-step LLM prompt to improve performance on downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Single-Step Agent (vanilla zero-shot LLM; GPT-4 in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A single-call language agent characterized by a single zero-shot prompt that maps input to an answer in one step; in experiments the client agent is GPT-4 (and GPT-3.5-TURBO in some runs).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>semantic rule-based instruction list (task-specific semantic memory / instruction cache)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Self-reflection generates textual reflections for failed examples; a MetaReflect LLM prompt ingests prior instructions, reflections, training and small validation sets and outputs updated non-case-specific instructions (an ordered list). New instructions are validated on a random sample of the validation set; if performance degrades the method backtracks. Iterative refinement (multiple meta-reflection attempts per batch) is used to converge to better instructions. The resulting instruction list is prepended/appended to the agent prompt and conditions action/output selection.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple (IAC vulnerability detection, BIOSSES biomedical semantic similarity, BBH complex reasoning tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary/multi-class classification and reasoning tasks: domain-specific vulnerability detection in Terraform modules (IAC), biomedical sentence similarity (BIOSSES), and complex reasoning datasets (BBH subsets like Causal Judgement, Temporal Sequence, Epistemic Reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>classification / question answering / multi-step reasoning (single-shot in this config)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Across single-step benchmarks METAREFLECTION improves accuracy over a raw GPT-4 baseline by reported ranges of 4% to 16.82% (paper-level summary). The paper also reports consistent gains on IAC policies of about 12–17% accuracy improvement over baselines depending on batch size.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Raw GPT-4 baseline (no learned instruction memory). Exact baseline accuracies depend on dataset (e.g. paper reports GPT-4 baseline numbers per-table), but the paper summarizes the improvement range as above.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Appending a learned, compact set of semantic instructions (memory) to single-step LLM prompts yields consistent accuracy gains across domain-specific and reasoning benchmarks and matches or exceeds state-of-the-art prompt-optimization baselines while using fewer LLM calls.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Validation uses small held-out samples per batch which induces stochasticity; sparse 0-1 reward signals can produce poor self-reflections or over-generalized instructions; occasional drops in recall for some security policies were observed; the meta-reflection process can produce poorer instructions that require backtracking.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MetaReflection: Learning Instructions for Language Agents using Past Reflections', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3040.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3040.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>METAREFLECTION (COT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>METAREFLECTION applied to chain-of-thought (CoT) multi-step reasoning agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>METAREFLECTION augments chain-of-thought style agents by learning high-level and action-level instructions (semantic memory) that guide thought-generation and answer selection, improving CoT performance on HotpotQA variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>COT (Chain-of-Thought) agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A multi-step reasoning agent that generates an intermediate 'thought' step and then an 'answer' (CoT); in experiments CoT is implemented as a two-step agent and evaluated in GT (ground-truth) and Distractor settings on HOTPOTQA.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>semantic instruction list (task- and action-level guidance)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Meta-reflection produces instructions that change how the 'thought' step is produced (e.g., which contextual cues to prioritize); instructions are validated and used to condition the CoT prompt to influence subsequent intermediate reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HOTPOTQA (COT-GT and COT-Distractor)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-hop question answering over Wikipedia; GT setting provides supporting docs, Distractor setting includes extra distracting documents—challenge is retrieval/selection and multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / multi-hop reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported gains up to 32% for COT (GT) and 18% for COT (Distractor) over respective baselines; METAREFLECTION outperformed PROTEGI by 23.33% and PROMPTAGENT by 3.25% in the GT setting, and by 24% and 1.08% respectively in the Distractor setting (paper-reported comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baseline COT agent (no learned instruction memory); exact baseline percentages reported in paper tables, summarized above by improvement amounts.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Meta-learned instructions improve the quality of 'thought' steps and lead to large gains in CoT multi-hop QA (especially in GT setting), and outperform or match state-of-the-art prompt-optimization baselines while requiring fewer LLM calls.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Some instructions can be too abstract for weaker models; batching and instruction specificity interact with agent model capacity (GPT-4 follows abstract instructions better than GPT-3.5); validation/backtracking needed to avoid over-generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MetaReflection: Learning Instructions for Language Agents using Past Reflections', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3040.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3040.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>METAREFLECTION (REACT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>METAREFLECTION applied to REACT-style multi-step agent (search+lookup actions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>METAREFLECTION appends learned semantic memory to REACT agents so that action-selection strategies (search/lookup ordering, loop avoidance) are guided by learned rules, improving performance on retrieval+reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>REACT agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A multi-step agent framework that interleaves natural-language reasoning ('thoughts') with environment actions (Search, Lookup, etc.) before producing an Answer; trajectories can span multiple action steps conditioned on previous actions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>semantic instruction list for action-level guidance (procedural/trajectory memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Meta-reflection learns trajectory-guiding instructions (e.g., 'if stuck in a lookup loop, change keywords or look further down the page') that are appended to the agent's prompt and used at each decision point to influence action choices across multiple steps.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HOTPOTQA (Full-Wiki setting with REACT agent)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-domain retrieval + multi-step reasoning over the full wiki collection; the challenge includes retrieving relevant paragraphs and then reasoning correctly over them.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>retrieval + multi-step question answering</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Paper reports METAREFLECTION yields up to 15.5% improvement on REACT in one summary and elsewhere states METAREFLECTION 'nearly doubles' GPT-4 REACT baseline performance; exact per-table values are reported in the paper (e.g., gains up to 15.5% in one set of experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baseline REACT agent powered by GPT-4 without the learned instruction memory.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Learning procedural/action-level instructions (semantic memory) helps REACT agents escape failure modes like search/lookup loops and substantially improves retrieval+reasoning success rates on HOTPOTQA adversarial samples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>REACT's complexity (retrieval failures, larger failure surface) makes absolute performance lower than simpler CoT setups; learned instructions must capture granular action policies and can be sensitive to instruction generality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MetaReflection: Learning Instructions for Language Agents using Past Reflections', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3040.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3040.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>METAREFLECTION (small-client distillation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>METAREFLECTION-generated instructions (from GPT-4) applied to smaller client agents (GPT-3.5-TURBO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>METAREFLECTION uses a more powerful LLM (GPT-4) to generate specific instructions which are then provided as memory to smaller client agents (GPT-3.5-TURBO), improving their performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Client agent: GPT-3.5-TURBO (single-step / COT / REACT configurations)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Smaller capacity language model used as the client agent; in experiments it executes tasks conditioned on instructions produced by METAREFLECTION (which used GPT-4 to produce those instructions).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>instruction distillation / semantic instruction list</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>A powerful LLM (GPT-4) runs the MetaReflect procedure to produce compact, specific instructions; these instructions are then supplied to the smaller client LLM so it can follow higher-quality, task-specific heuristics during inference.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HOTPOTQA (COT and REACT settings with GPT-3.5-TURBO agents)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same multi-hop QA tasks but with smaller client LLMs that are easier to mislead; challenge is to improve smaller model behavior via distilled instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported gains when using METAREFLECTION-generated instructions with GPT-3.5-TURBO clients: up to 21% for COT (GT), 12% for COT (Distractor), and 18.33% for REACT (paper-reported numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>GPT-3.5-TURBO client agents without METAREFLECTION instructions (baseline performance reported in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Task-specific instructions produced by a stronger LLM can materially boost performance of smaller client LLMs, resembling task-specific distillation; specificity of instructions matters more for smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Smaller models benefit more from concrete, specific instructions; abstract/generalized instructions (that work for GPT-4) may not transfer well to GPT-3.5-TURBO.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MetaReflection: Learning Instructions for Language Agents using Past Reflections', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Promptagent: Strategic planning with language models enables expert-level prompt optimization <em>(Rating: 2)</em></li>
                <li>PROTEGI <em>(Rating: 1)</em></li>
                <li>REACT: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Semantic memory in reinforcement learning (Paischer et al., 2023) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3040",
    "paper_id": "paper-269982440",
    "extraction_schema_id": "extraction-schema-74",
    "extracted_data": [
        {
            "name_short": "METAREFLECTION (single-step)",
            "name_full": "METAREFLECTION applied to single-step LLM agents (semantic instruction memory)",
            "brief_description": "METAREFLECTION is an offline reinforcement-style technique that learns a compact, rule-based semantic memory (a list of instructions) from self-reflections on failed trajectories and appends this memory to a single-step LLM prompt to improve performance on downstream tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Single-Step Agent (vanilla zero-shot LLM; GPT-4 in experiments)",
            "agent_description": "A single-call language agent characterized by a single zero-shot prompt that maps input to an answer in one step; in experiments the client agent is GPT-4 (and GPT-3.5-TURBO in some runs).",
            "memory_used": true,
            "memory_type": "semantic rule-based instruction list (task-specific semantic memory / instruction cache)",
            "memory_mechanism_description": "Self-reflection generates textual reflections for failed examples; a MetaReflect LLM prompt ingests prior instructions, reflections, training and small validation sets and outputs updated non-case-specific instructions (an ordered list). New instructions are validated on a random sample of the validation set; if performance degrades the method backtracks. Iterative refinement (multiple meta-reflection attempts per batch) is used to converge to better instructions. The resulting instruction list is prepended/appended to the agent prompt and conditions action/output selection.",
            "task_name": "Multiple (IAC vulnerability detection, BIOSSES biomedical semantic similarity, BBH complex reasoning tasks)",
            "task_description": "Binary/multi-class classification and reasoning tasks: domain-specific vulnerability detection in Terraform modules (IAC), biomedical sentence similarity (BIOSSES), and complex reasoning datasets (BBH subsets like Causal Judgement, Temporal Sequence, Epistemic Reasoning).",
            "task_type": "classification / question answering / multi-step reasoning (single-shot in this config)",
            "performance_with_memory": "Across single-step benchmarks METAREFLECTION improves accuracy over a raw GPT-4 baseline by reported ranges of 4% to 16.82% (paper-level summary). The paper also reports consistent gains on IAC policies of about 12–17% accuracy improvement over baselines depending on batch size.",
            "performance_without_memory": "Raw GPT-4 baseline (no learned instruction memory). Exact baseline accuracies depend on dataset (e.g. paper reports GPT-4 baseline numbers per-table), but the paper summarizes the improvement range as above.",
            "has_performance_comparison": true,
            "key_findings": "Appending a learned, compact set of semantic instructions (memory) to single-step LLM prompts yields consistent accuracy gains across domain-specific and reasoning benchmarks and matches or exceeds state-of-the-art prompt-optimization baselines while using fewer LLM calls.",
            "limitations_or_challenges": "Validation uses small held-out samples per batch which induces stochasticity; sparse 0-1 reward signals can produce poor self-reflections or over-generalized instructions; occasional drops in recall for some security policies were observed; the meta-reflection process can produce poorer instructions that require backtracking.",
            "uuid": "e3040.0",
            "source_info": {
                "paper_title": "MetaReflection: Learning Instructions for Language Agents using Past Reflections",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "METAREFLECTION (COT)",
            "name_full": "METAREFLECTION applied to chain-of-thought (CoT) multi-step reasoning agents",
            "brief_description": "METAREFLECTION augments chain-of-thought style agents by learning high-level and action-level instructions (semantic memory) that guide thought-generation and answer selection, improving CoT performance on HotpotQA variants.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "COT (Chain-of-Thought) agent",
            "agent_description": "A multi-step reasoning agent that generates an intermediate 'thought' step and then an 'answer' (CoT); in experiments CoT is implemented as a two-step agent and evaluated in GT (ground-truth) and Distractor settings on HOTPOTQA.",
            "memory_used": true,
            "memory_type": "semantic instruction list (task- and action-level guidance)",
            "memory_mechanism_description": "Meta-reflection produces instructions that change how the 'thought' step is produced (e.g., which contextual cues to prioritize); instructions are validated and used to condition the CoT prompt to influence subsequent intermediate reasoning steps.",
            "task_name": "HOTPOTQA (COT-GT and COT-Distractor)",
            "task_description": "Multi-hop question answering over Wikipedia; GT setting provides supporting docs, Distractor setting includes extra distracting documents—challenge is retrieval/selection and multi-step reasoning.",
            "task_type": "question answering / multi-hop reasoning",
            "performance_with_memory": "Reported gains up to 32% for COT (GT) and 18% for COT (Distractor) over respective baselines; METAREFLECTION outperformed PROTEGI by 23.33% and PROMPTAGENT by 3.25% in the GT setting, and by 24% and 1.08% respectively in the Distractor setting (paper-reported comparisons).",
            "performance_without_memory": "Baseline COT agent (no learned instruction memory); exact baseline percentages reported in paper tables, summarized above by improvement amounts.",
            "has_performance_comparison": true,
            "key_findings": "Meta-learned instructions improve the quality of 'thought' steps and lead to large gains in CoT multi-hop QA (especially in GT setting), and outperform or match state-of-the-art prompt-optimization baselines while requiring fewer LLM calls.",
            "limitations_or_challenges": "Some instructions can be too abstract for weaker models; batching and instruction specificity interact with agent model capacity (GPT-4 follows abstract instructions better than GPT-3.5); validation/backtracking needed to avoid over-generalization.",
            "uuid": "e3040.1",
            "source_info": {
                "paper_title": "MetaReflection: Learning Instructions for Language Agents using Past Reflections",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "METAREFLECTION (REACT)",
            "name_full": "METAREFLECTION applied to REACT-style multi-step agent (search+lookup actions)",
            "brief_description": "METAREFLECTION appends learned semantic memory to REACT agents so that action-selection strategies (search/lookup ordering, loop avoidance) are guided by learned rules, improving performance on retrieval+reasoning tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "REACT agent",
            "agent_description": "A multi-step agent framework that interleaves natural-language reasoning ('thoughts') with environment actions (Search, Lookup, etc.) before producing an Answer; trajectories can span multiple action steps conditioned on previous actions.",
            "memory_used": true,
            "memory_type": "semantic instruction list for action-level guidance (procedural/trajectory memory)",
            "memory_mechanism_description": "Meta-reflection learns trajectory-guiding instructions (e.g., 'if stuck in a lookup loop, change keywords or look further down the page') that are appended to the agent's prompt and used at each decision point to influence action choices across multiple steps.",
            "task_name": "HOTPOTQA (Full-Wiki setting with REACT agent)",
            "task_description": "Open-domain retrieval + multi-step reasoning over the full wiki collection; the challenge includes retrieving relevant paragraphs and then reasoning correctly over them.",
            "task_type": "retrieval + multi-step question answering",
            "performance_with_memory": "Paper reports METAREFLECTION yields up to 15.5% improvement on REACT in one summary and elsewhere states METAREFLECTION 'nearly doubles' GPT-4 REACT baseline performance; exact per-table values are reported in the paper (e.g., gains up to 15.5% in one set of experiments).",
            "performance_without_memory": "Baseline REACT agent powered by GPT-4 without the learned instruction memory.",
            "has_performance_comparison": true,
            "key_findings": "Learning procedural/action-level instructions (semantic memory) helps REACT agents escape failure modes like search/lookup loops and substantially improves retrieval+reasoning success rates on HOTPOTQA adversarial samples.",
            "limitations_or_challenges": "REACT's complexity (retrieval failures, larger failure surface) makes absolute performance lower than simpler CoT setups; learned instructions must capture granular action policies and can be sensitive to instruction generality.",
            "uuid": "e3040.2",
            "source_info": {
                "paper_title": "MetaReflection: Learning Instructions for Language Agents using Past Reflections",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "METAREFLECTION (small-client distillation)",
            "name_full": "METAREFLECTION-generated instructions (from GPT-4) applied to smaller client agents (GPT-3.5-TURBO)",
            "brief_description": "METAREFLECTION uses a more powerful LLM (GPT-4) to generate specific instructions which are then provided as memory to smaller client agents (GPT-3.5-TURBO), improving their performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Client agent: GPT-3.5-TURBO (single-step / COT / REACT configurations)",
            "agent_description": "Smaller capacity language model used as the client agent; in experiments it executes tasks conditioned on instructions produced by METAREFLECTION (which used GPT-4 to produce those instructions).",
            "memory_used": true,
            "memory_type": "instruction distillation / semantic instruction list",
            "memory_mechanism_description": "A powerful LLM (GPT-4) runs the MetaReflect procedure to produce compact, specific instructions; these instructions are then supplied to the smaller client LLM so it can follow higher-quality, task-specific heuristics during inference.",
            "task_name": "HOTPOTQA (COT and REACT settings with GPT-3.5-TURBO agents)",
            "task_description": "Same multi-hop QA tasks but with smaller client LLMs that are easier to mislead; challenge is to improve smaller model behavior via distilled instructions.",
            "task_type": "question answering / multi-step reasoning",
            "performance_with_memory": "Reported gains when using METAREFLECTION-generated instructions with GPT-3.5-TURBO clients: up to 21% for COT (GT), 12% for COT (Distractor), and 18.33% for REACT (paper-reported numbers).",
            "performance_without_memory": "GPT-3.5-TURBO client agents without METAREFLECTION instructions (baseline performance reported in paper tables).",
            "has_performance_comparison": true,
            "key_findings": "Task-specific instructions produced by a stronger LLM can materially boost performance of smaller client LLMs, resembling task-specific distillation; specificity of instructions matters more for smaller models.",
            "limitations_or_challenges": "Smaller models benefit more from concrete, specific instructions; abstract/generalized instructions (that work for GPT-4) may not transfer well to GPT-3.5-TURBO.",
            "uuid": "e3040.3",
            "source_info": {
                "paper_title": "MetaReflection: Learning Instructions for Language Agents using Past Reflections",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Promptagent: Strategic planning with language models enables expert-level prompt optimization",
            "rating": 2,
            "sanitized_title": "promptagent_strategic_planning_with_language_models_enables_expertlevel_prompt_optimization"
        },
        {
            "paper_title": "PROTEGI",
            "rating": 1
        },
        {
            "paper_title": "REACT: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Semantic memory in reinforcement learning (Paischer et al., 2023)",
            "rating": 1,
            "sanitized_title": "semantic_memory_in_reinforcement_learning_paischer_et_al_2023"
        }
    ],
    "cost": 0.02152825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>METAREFLECTION: Learning Instructions for Language Agents using Past Reflections
10 Oct 2024</p>
<p>Priyanshu Gupta priyansgupta@microsoft.com 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Shashank Kirtania t-skirtania@microsoft.com 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Ananya Singha ananyasingha@microsoft.com 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Sumit Gulwani sumitg@microsoft.com 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Arjun Radhakrishna 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Sherry Shi 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Gustavo Soares gustavo.soares@microsoft.com 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Rohan Anil 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Andrew M Dai 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>MelvinOrhan Firat 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Dmitry Lepikhin 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Alexandre Passos 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Siamak Shakeri 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Emanuel Taropa 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Paige Bailey 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Zhifeng Chen 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Eric Chu 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Jonathan H Clark 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Laurent El 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Yanping Huang 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Kathy Meier-Hellstern 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Gau- Rav Mishra 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Erica Moreira 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Mark Omernick 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Kevin Robinson 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Sebastian Ruder 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Yi Tay 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Kefan Xiao 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Yuanzhong Xu 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Yujing Zhang 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Gustavo Hernandez Abrego 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Junwhan Ahn 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Jacob Austin 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Paul Barham 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Jan Botha 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>James Bradbury 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Siddhartha Brahma 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Kevin Brooks 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Michele Catasta 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Yong Cheng 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Colin Cherry 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Christopher A Choquette-Choo 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Aakanksha Chowdhery 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Clément Crepy 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Shachi Dave 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Mostafa Dehghani 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Sunipa Dev 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Jacob Devlin 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Mark Díaz 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Nan Du 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Ethan Dyer 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Vlad Feinberg 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Fangxiaoyu Feng 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Vlad Fienber 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Markus Freitag 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Xavier Gar- Cia 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Sebastian Gehrmann 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Lucas Gonzalez 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Guy Gur- Ari 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Steven Hand 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Hadi Hashemi 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Le Hou 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Joshua Howland 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Andrea Hu 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Jeffrey Hui 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Jeremy Hur- Witz 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Michael Isard 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Abe Ittycheriah 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Matthew Jagiel- Ski 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Wenhao Jia 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Kathleen Kenealy 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Maxim Krikun 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Sneha Kudugunta 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Chang Lan 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Katherine Lee 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Ben- Jamin Lee 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>MusicEric Li 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Wei Li 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Yaguang Li 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Jian Li 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Hyeontaek Li 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Hanzhao Lim 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Zhongtao Lin 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Frederick Liu 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Marcello Liu 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Aroma Maggioni 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Joshua Mahendru 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Vedant Maynez 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Maysam Misra 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Zachary Moussalem 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>John Nado 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Eric Nham 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Andrew Ni 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Alicia Nys- Trom 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Marie Parrish 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Martin Pellat 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Alex Polacek 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Reiner Polozov 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Siyuan Pope 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Emily Qiao 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Bryan Reif 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Parker Richter 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Alex Riley 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Ros Castro 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Au- Rko Roy 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Brennan Saeta 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Rajkumar Samuel 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Renee Shelby 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Ambrose Slone 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Daniel Smilkov 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>David R So 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Daniel Sohn 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Simon Tokumine 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Dasha Valter 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Yuntao Bai 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Saurav Kadavath 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Sandipan Kundu 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Amanda Askell 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Jackson Kernion 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Andy Jones 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Anna Chen 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Anna Goldie 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Azalia Mirhoseini 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Carol Mckinnon 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Catherine Chen 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Christo- Pher Olsson 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Danny Olah 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Dawn Hernandez 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Deep Drain 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Dustin Ganguli 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Eli Li 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Ethan Tran-Johnson 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Jamie Perez 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Jared Kerr 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Jeffrey Mueller 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Joshua Ladish 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Kamal Landau 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Kamile Ndousse 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Liane Lukosuite 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Michael Lovitt 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Nelson Sellitto 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Nicholas Elhage 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Noemi Schiefer 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Nova Mercado 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Robert Dassarma 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Robin Lasenby 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Sam Larson 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Scott Ringer 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Shauna John- Ston 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>SheerEl Kravec 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Stanislav Showk 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Tamera Fort 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Timothy Lanham 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Tom Telleen-Lawton 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Tom Henighan 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Tristan Hume 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Samuel R Bow- Man 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Zac Hatfield-Dodds 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Benjamin Mann 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Dario Amodei 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Tom B Brown 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Nick Ryder 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Melanie Subbiah 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Jared Kaplan 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Prafulla Dhariwal 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Arvind Neelakantan 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Pranav Shyam 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Girish Sastry 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Sandhini Agarwal 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Ariel Herbert-Voss 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Gretchen Krueger 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Rewon Child 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Aditya Ramesh 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Daniel M Ziegler 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Jeffrey Wu 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Clemens Winter 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Christopher Hesse 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Mark Chen 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Eric Sigler 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Mateusz Litwin 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Scott Gray 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Benjamin Chess 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Jack Clark 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Christopher Berner 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Sam Mc- Candlish 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Alec Radford 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Ilya Sutskever 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Aman Madaan 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Niket Tandon 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Prakhar Gupta 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Skyler Hallinan 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Luyu Gao 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Sarah Wiegreffe 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Uri Alon 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Nouha Dziri 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Shrimai Prabhumoye 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Yiming Yang 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Sean Welleck 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Bodhisattwa Prasad Majumder 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Shashank Gupta 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Amir Yazdanbakhsh 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Peter Clark 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Reiichiro Nakano 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Jacob Hilton 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Suchir Balaji 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Long Ouyang 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Christina Kim 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Shantanu Jain 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Vineet Kosaraju 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>William Saunders 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Xu Jiang 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Karl Cobbe 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Tyna Eloundou 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Kevin Button 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Matthew Knight 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Gabriel Poesia 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Alex Polozov 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Vu Le 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Ashish Tiwari 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Yujia Qin 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Shengding Hu 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Yankai Lin 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Weize Chen 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Ning Ding 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Ganqu Cui 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Zheni Zeng 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Yufei Huang 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Chaojun Xiao 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Chi Han 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Yi Ren Fung 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Yusheng Su 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Huadong Wang 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Cheng Qian 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Runchu Tian 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Kunlun Zhu 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Shihao Liang 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Xingyu Shen 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Bokai Xu 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Zhen Zhang 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Yining Ye 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Bowen Li 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Ziwei Tang 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Jing Yi 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Yuzhang Zhu 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Zhenning Dai 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Lan Yan 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Xin Cong 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Yaxi Lu 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Weilin Zhao 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Yuxiang Huang 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Junxi Yan 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Xu Han 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Xian Sun 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Dahai Li 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Jason Phang 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Cheng Yang 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Tongshuang Wu 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Heng Ji 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Zhiyuan Liu 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Timo Schick 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Jane Dwivedi-Yu 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Roberto Dessi 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Roberta Raileanu 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Maria Lomeli 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Eric Hambro 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Luke Zettle- Moyer 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Nicola Cancedda 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>Thomas 2023 Scialom 
Vijay Vasudevan
Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, John Wiet-ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Ce Zheng, Weikang ZhouTao Wang, Qiao Zhang, Steven Zheng, Denny Zhou, Slav</p>
<p>METAREFLECTION: Learning Instructions for Language Agents using Past Reflections
10 Oct 20247D82870BCF71FC6B331AC79378814D6CarXiv:2405.13009v2[cs.CL]
The popularity of Large Language Models (LLMs) have unleashed a new age of Language Agents for solving a diverse range of tasks.While contemporary frontier LLMs are capable enough to power reasonably good Language agents, the closed-API model makes it hard to improve in cases they perform suboptimally.To address this, recent works have explored ways to improve their performance using techniques like self-reflection and prompt optimization.Unfortunately, techniques like self-reflection can be used only in an online setup, while contemporary prompt optimization techniques are designed and tested to work on simple tasks.To this end, we introduce METAREFLECTION, a novel offline reinforcement learning technique that enhances the performance of Language Agents by augmenting a semantic memory based on experiential learnings from past trials.We demonstrate the efficacy of METAREFLECTION by evaluating across multiple domains, including complex logical reasoning, biomedical semantic similarity, open world question answering, and vulnerability threat detection, in Infrastructure-as-Code, spanning different agent designs.METAREFLECTION boosts Language agents' performance by 4 % to 16.82 % over the raw GPT-4 baseline and performs on par with existing state-of-the-art prompt optimization techniques while requiring fewer LLM calls.We release our experimental code at: aka.ms/metareflection-code.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs), such as GPT-4 (OpenAI, 2023), have gained significant popularity in recent years due to their ability to generate human-like text and solve complex tasks across various domains.To leverage these models, users typically craft prompts with instructions that are * Equal Contribution tailored to a specific task.Furthermore, many practical LLM applications setup complex multi-step systems with multiple LLM calls chained together (LangChain, 2023) or LLM calls with different prompts called in succession (Wu et al., 2023).Given their close resemblance to reinforcement learning agents, it has become useful to model these systems as Language Agents (Wu et al., 2023).</p>
<p>Building up on these insights, recent works (Shinn et al., 2023;Madaan et al., 2023) have showed that the performance of such language agents can be improved using verbal reinforcement learning across multiple conversational turns, by providing feedback at the end of a failing trajectory and asking the agent to reflect on its mistakes.The reflective text is then stored as episodic memory to improve future trajectories on the same task.</p>
<p>Parallel to this, there has been recent advancements in developing prompt optimization techniques (Pryzant et al., 2023;Wang et al., 2023).These techniques typically start with a seed human prompt and a train dataset of Input-Output pairs.Leveraging error feedback over failing cases, they deploy various search strategies to find prompt variants that can give the optimal results on train set.</p>
<p>While prompt optimization techniques exist for single LLM call setups, they are not designed for complex multi-step language agents.Here, techniques like self-reflection can help improve a language agent's performance significantly.However, these are online reinforcement processes that depend on the availability of performing multiple turns with a feedback mechanism on the same task instance and aren't applicable to similar new tasks.</p>
<p>Motivated by the semantic memory observed in humans (McRae and Jones, 2013;Weng, 2023;Paischer et al., 2023) we introduce METAREFLEC-TION, an offline reinforcement learning technique that works by augmenting a semantic memory to</p>
<p>Trajectory</p>
<p>Evaluation (feedback): You attempted to answer the above question and ran out of turns.Reflect on your mistakes . . .Self Reflection: In this trial, I failed to find the answer because I was stuck in a loop of looking up goal, but the goal scorer was not found in the first few results.This could be due to the fact that the information about the goal might be located further down the page or in a different section... Self-reflection . . .</p>
<p>Next trajectory</p>
<p>Your job is to learn from experience of another agent and help it perform better in the future.Use the agent's reflections about its failures or success and concretize them into instructions. . . .Self-reflection 1: In this trial, I was stuck in a loop... Self-reflection 2: . . .</p>
<p>Meta-Reflection</p>
<p>Instructions:</p>
<p>1.If you're not finding the desired information or stuck in a loop of looking up a keyword, consider changing the keyword and search strategy.The information might be located further down the page.2. . . .We evaluate the METAREFLECTION technique across multiple scenarios: 3 complex logical reasoning datasets (Suzgun et al., 2022), a biomedical semantic similarity dataset (Sogancıoglu et al., 2017), an open world question answering (Yang et al., 2018), and a new vulnerability threat detection in a new Infrastructure-as-Code (IAC) dataset.</p>
<p>Self-reflections from other tasks</p>
<p>The IAC vulnerability detection dataset, in particular, is a new work that we are introducing with this paper, entailing detection of 10 unique common security vulnerabilities found in Terraform (HashiCorp, 2023) cloud infrastructure configuration files.While static analysis tools like Terrascan (Tenable, 2023) exist, they are not able to detect semantic nuances beyond the standard hard-coded detection patterns.Being a low-resource language, vanilla GPT-4 struggles to effectively detect the vulnerabilities.This makes it a particularly interesting dataset for domain-specific reasoning.</p>
<p>METAREFLECTION achieves 4% to 16.82% higher accuracy over raw GPT-4 baseline.For setups, where they are directly comparable, METAREFLECTION performs on par or better than state of the art prompt optimization techniques, while requiring fewer LLM calls for learning.We also demonstrate how METAREFLECTION can be applied to multi-step agents for offline learning.</p>
<p>To summarize, we make the following contributions:</p>
<p>• We present METAREFLECTION, a technique for learning semantic memory for language agents using offline training simulations.To the best of our knowledge, this is the first approach towards learning semantic memory for generic language agents.</p>
<p>• We introduce a new dataset for vulnerability threat detection in IAC.The dataset comprises 416 challenging vulnerability detection tasks.</p>
<p>• We conducted an extensive evaluation of the METAREFLECTION technique across multiple distinct domains: vulnerability threat detection, causal reasoning, domain-specific semantic similarity, and open-domain question answering, demonstrating strong improvements in agent performances.(Section 3).</p>
<p>METAREFLECTION</p>
<p>We consider a setup of an LLM Agent A interacting with an environment Env to solve a task Agent A receives an observation o t at state S t and samples an action a t using a policy π(a t |T t , P t ) where T t = {o 0 , a 0 , o 1 . . ., a t , o t }, represents the trajectory of the agent and the prompt P t is the prompt that drives the action selection at time step t.Eventually, the agent reaches a state S n where it receives a reward from the agent r = R(S n ).This state marks the correct/incorrect completion of the task T by the language agent.METAREFLECTION aims to improve the performance of such language agents by augmenting them with a semantic memory (McRae and Jones, 2013) in the form of rule based instructions Instr to store the experiential learnings, replacing π(a t |T t , P) with π(a t |T t , P Instr).Algorithm 1 shows the outline of the METAREFLEC-TION process.At its core, the algorithm works by starting with an empty set of instructions and iteratively improving the instructions using small training batches.</p>
<p>For instance, if the agent A react is a REACTstyle agent working on a Question Answering Task over Wikipedia, at state S 0 , the agent will be provided by a question Q.Now, the agent can choose to take a search action, upon which it will receive an observation o.The agent can then either chose to pick further search or lookup actions, or pick an answer action to return an answer.This action will mark a transition to the state S n and a binary reward can be generated depending on the correctness of the answer.An even simpler example can be an agent A s which given a Multiple-Choice Question, picks an answer and receives a positive reward if the answer is correct.</p>
<p>Agent, Environment and Self-reflection method METAREFLECTION procedure uses the following components at its core:</p>
<p>(a) an agent A (i.e., an RL actor) that is based on a language model, (b) an environment Env that generates rewards for specific actions, given a agent's trajectory, and (c) A SelfReflect method on the lines of (Shinn et al., 2023) that produces a verbal reinforcement given an agent's trajectory (line 10, Algorithm 1) This is in contrast to prior state of the art prompt optimization techniques (Wang et al., 2023;Pryzant et al., 2023), where optimization relies solely on the final action of the agent and cannot be applied on the intermediate un-observed states of the actor.Additionally, we adapt the agent A to be parameterized by a set of instructions in addition to the standard task description and agent behaviour prompt.In our implementation and experiments, we utilize either a singlestep agent (a vanilla single-shot Language Model agent) or a multi-step agent based on REACT (Yao et al., 2023c) and COT (Wei et al., 2023).While METAREFLECTION makes no distinction between the two, we make this distinction to ease comparison with other prompt optimization techniques, which only work in the single-step agent configuration.More information on agent configurations can be found in Section 3.3.</p>
<p>The MetaReflect step The MetaReflect method is designed to take as input a prior set of instructions Instr, a set of self-reflections refl, the training data D T and a validation data D V and will produce an updated set of instructions Instr * as denoted in line 11 of Algorithm 1.</p>
<p>For the MetaReflect method, we use a standard language model with a prompt that instructs the LLM to observe the reflections, the training data, and produce a new non-case specific instruction style memory.Further, the prior memory is also passed as input so that the output is a generalization of the prior learnings.In our implementation, this meta-reflection and generalization are done in the same prompt for efficiency.Alternatively, new instructions can be generated first and then combined with existing ones.</p>
<p>We specify that the instructions need to take the form of a list.Hence, the meta-reflection step in line 11 typically either (a) updates the list by adding a new item, or (b) combines one or more previous items with learnings from the self-reflections to produce a shorter list.For example, one meta-reflection instruction learned during our HOT-POTQA experiments suggested including the profession when searching for a person to narrow down results.In a subsequent batch, the selfreflection step produces a reflection that mentions adding search terms like release date when searching for movies.The MetaReflect step may combine the previous instructions with the current selfreflections either by appending a new item to the list clarifying the strategy to search for movies, or may generalize the previous item to something like "When searching for specific entities, use additional contextual information to augment the primary search terms with secondary keywords corresponding to the characteristics of the entity".</p>
<p>Validation and Backtracking</p>
<p>In each iteration, after MetaReflect, we validate the quality of the new instructions.Due to the sparse reward signals leading to poor self-reflections or over-generalization of the meta-reflection instructions, we may end up with instructions that are of a poorer quality than the prior instructions.The poorer instructions may also be due to general capricious, unpredictable nature of large language models.Therefore, we validate the new instructions by testing them on training data and a random sample of the validation set to ensure that they perform better than the prior instructions as depicted in line 12 of Algorithm 1. Ideally, we would do this validation over the full validation dataset.However, in our case, we only validate on the current batch to balance quality of instructions and efficiency.In cases where the updated instructions perform poorly compared to the prior ones, we backtrack to prior instructions as depicted in line 13.</p>
<p>As an example, in the previous paragraph the meta-reflection step replaced the specific instruction on how to search for persons with a more general instruction on how to search for entities.However, it is possible that these general instructions are too vague (especially for smaller, less capable models) and the client agent is not able to apply them correctly to the case of searching for persons.</p>
<p>Iterative Refinement Motivated by the self refining behaviour of Language models as demonstrated in (Shinn et al., 2023), we use multiple attempts at meta-reflection for each batch (line 4 through line 13) until we see no failures in the current batch or exhaust a maximum number of trials (set to 3 for all our experiments).Similar to how self-reflections help optimize Language agents' tra-jectories towards achieving a task, the feedback on failed trajectories from the Instr * act as implicit verbal reinforcements in the meta-reflection process.These verbal reinforcements can then be used by the meta-reflection step to guide the instruction search.This trajectory driven iterative refinement strategy reduces the chances of repeating mistakes, such as proposing incorrect refinements that were already tried in the past, during the instruction refinement process.</p>
<p>In the single step agent setups where they are directly comparable, we observe that this iterative refinement strategy leads us to similar performance as state of the art prompt optimization techniques, with lesser number of LLM calls.</p>
<p>3 Experimental Setup</p>
<p>Datasets</p>
<p>We evaluate METAREFLECTION on datasets from different domains like vulnerability threat detection (IAC), question answering (HOTPOTQA), Complex Reasoning (BIGBENCH), Biomedical Semantic Similarity (BIOSSES).</p>
<p>Vulnerability</p>
<p>Threat Detection (IAC) Infrastructure-as-Code (IAC) is a popular method of configuring cloud infrastructures, on platforms such as Azure and AWS, using a configuration coding language.Here, we focus on Terraform, a leading IAC platform by Hashicorp (HashiCorp, 2023), as well as Azure, the cloud computing platform by Microsoft, which comes as a reusable configuration component.Such Cloud infrastructures are prone to security vulnerabilities such as open ports and exposed administrator accounts (Tenable, 2023).Vulnerability detection via static analysis such as Terrascan (Tenable, 2023) of IAC files is a hard problem due to the expressivity and complexity of the configuration language and the diversity of the resources being handled across multiple infrastructure providers (e.g., Amazon AWS and Microsoft Azure).</p>
<p>This opens up the possibility of using an LLM to perform vulnerability detection which entails checking if a given Terraform module violates a given Terrascan policy.To evaluate the efficacy of METAREFLECTION on vulnerability detection task we collected 202 Terraform modules by mining GitHub repositories and post processing it further to achieve 186 data points which is then split 40 : 60 for train and test respectively.You can find more  (Wang et al., 2023).Table 1 shows the test and the train distribution for the benchmarks.</p>
<p>Biomedical Semantic Similarity (BIOSSES) is a biomedical sentence similarly dataset (Sogancıoglu et al., 2017).Each instance in the dataset comprises of two sentences which are to be compared.As prior work (Wang et al., 2023), we model the problem as a classification task between labels 'similar', 'non-similar' or 'somewhat similar' and utilize the exact same test and train data splits.</p>
<p>Question Answering (HOTPOTQA) The dataset (Yang et al., 2018) is a Question-Answering dataset consisting of 113k question-and-answer pairs over Wikipedia.A typical system working over this dataset first performs a retrieval over wikipedia and the reasons over the retrievals to come up with an answer.Besides the retrieve and reason setup, the dataset also comes in 2 resoning only settings: 1. GT: where each Question is accompanied with the most relevant supporting ground truth documents and 2. Distractor where the Question is accompanied with the ground truth documents, alongside some distracting documents making context analysis and interpretation more challenging.Given the large scale of the dataset, we adversarially select (Appendix A.3) test samples from the test split of the dataset to ensure good sample diversity.To perform adversarial sampling, we identify samples where the Agent A consistently fails and conduct up to three self-reflection trials to correct the response.If the Agent A still fails, we discard these samples.This method ensures we gather challenging examples while filtering out noisy ones, and we sample 40 and 80 examples for the REACT train and test sets, and 50 and 80 for COT settings, respectively.</p>
<p>Baselines</p>
<p>In absence of techniques for direct comparison, we compare METAREFLECTION against strong contemporary prompt optimization baselines 1. PRO-TEGI and 2. PROMPTAGENT across multiple single step agent settings.</p>
<p>PROTEGI (Pryzant et al., 2023) performs prompt optimization by leveraging batch-wise error feedback as textual gradients and use them to generate multiple variations of the current prompt.Notably, these textual gradients are prompt update 'guidelines' generated by looking at the failing error output.They then deploy beam search to find the best prompt by iteratively evaluating and generating more prompt candidates.</p>
<p>PROMPTAGENT (Wang et al., 2023) Similar to PROTEGI, PROMPTAGENT also leverages error feedback to generate prompt candidates.They further optimize prompt search using a principled Monte-Carlo Tree Search and identify high-reward paths to find the best prompts.</p>
<p>Agent Configurations</p>
<p>The generic Language Agent A defined in Section 2 can operate upon multiple time steps before reaching a response state S n .While, the METAREFLEC-TION algorithm is broadly applicable to all such agents, for the purpose of simplification of comparison, we distinguish our agent setups in 2 categories:1.Single-Step and 2. Multi-Step Agents.</p>
<p>Single-Step Agent</p>
<p>We call an Agent A a Single-</p>
<p>Step Agent if it takes exactly 1 time step starting from the initial state S 0 to reach an answer state S 1 , without traversing intermediary belief states.</p>
<p>In practice, what it means is that the agent is characterized by a single zero-shot prompt that always elicits the agent to generate a response solving the input task.We define such single step agents for the IAC, BIOSSES and the BIGBENCH datasets.</p>
<p>Multi-Step Agent</p>
<p>We define Multi-Step Agents to be agents with atleast one intermediate belief state.In our evaluations we consider two well established multi-step agent style COT (Wei et al., 2022) and REACT (Yao et al., 2022).The COT setup entails exactly 2 time steps -the first transition generates the 'thought' and the next transition generates the 'answer'.The REACT setup, however, can entail multiple time steps with 'actions' for context gathering, before generating an 'answer' action.For the purpose of our evaluation, similar to (Shinn et al., 2023), we use the different HOTPOTQA datasets to setup COT-(Distractor) and COT-(GT) for the Distractor and GT settings respectively and a REACT agent with search and lookup actions on the wikipedia corpus for the retrieval and reasoning setting.</p>
<p>Other Experimental Configurations</p>
<p>We use GPT-4-32k chat model as an LLM throughout the experiments, with a temperature = 0.For METAREFLECTION, we set the batch size to be 4 and maxRetries to be 3, and the random sample size for the validation set to be 5.To account for statistical variations, all the experiments are run for 3 runs and we report the averages and standard deviation for each of them.</p>
<p>Results</p>
<p>Comparison with the baselines</p>
<p>Table 2 and Table 3 compares METAREFLECTION with baseline GPT-4 and various prompt optimization baselines over multiple datasets in a single-step agent setting.Further, Table 2 also presents the average number of LLM calls it took by different systems to generate the optimal prompt.We observe that METAREFLECTION fairs competitively with the other techniques, outperforming GPT-4 and PROTEGI over all the benchmarks.Compared to PROMPTAGENT we observe better performance in all the datasets except for EPIS-TEMIC REASONING where we see a slight regression.Notably, in all the configurations, METARE-FLECTION required the least number of LLM calls for training (include any intermediate inference calls), with PROTEGI and PROMPTAGENT requiring upto 17 and 1.5 times more LLM calls.In general, we observe that even with a lower number of required LLM calls, METAREFLECTION performs at par with PROMPTAGENT for generic causal reasoning tasks, while outperforming it on highly contextual domain-specific settings like IAC and BIOSSES.</p>
<p>Qualitative evaluations reveal that the instructions learned using METAREFLECTION capture Terraform Policy Description: { "name": "reme_containerRegistryResourceLock", "resource_type": "azurerm_container_registry", "description": "Ensure Container Registry has locks", "reference_id": "accurics.azure.AKS.3","category": "Resilience", "id": "AC_AZURE_0185" } ProTeGi optimized prompt:</p>
<p>Verify if module interacts with "azurerm_container_registry" type.Check for constructs in the code that ensure the Azure Container Registry has locks.In case there are constructs that provide these locks, classify the module as "Compliant" . . .</p>
<p>PromptAgent optimized prompt:</p>
<p>The policy is constructed around the 'Container Registry' resource type.Identify module as 'Compliant' if it meets these conditions: The resource type specified in the policy is not included in the module.And module does contain the resource type.Label module as 'Violating' if it falls short of complying with regulations . . .</p>
<p>MetaReflection learned instructions:</p>
<p>The policy in question is "reme_containerRegistryResourceLock" which pertains to the "azurerm_container_registry" resource type and requires locks be implemented for resilience.Look for the "azurerm_container_registry" resource.If the "azurerm_container_registry" resource is absent then policy is not violated.Check if "azurerm_container_registry" resource has implemented locks.This can be done by looking for the "azurerm_management_lock" resource in the Terraform code and checking if it is associated with the "azurerm_container_registry" resource.If the "azurerm_container_registry" resource has implemented locks, then policy is not violated else it is. . . .</p>
<p>METAREFLECTION for Multi-Step Agents</p>
<p>In the previous section we discussed how METARE-FLECTION fairs in settings with a single-step agent setup.In this, section we will evaluate METARE-FLECTION with multi-step agents.</p>
<p>Prompt optimization techniques like PROMPTA-GENT and PROTEGI are designed to model systems with a single LLM call akin to the single-step Table 3: Performance of METAREFLECTION and other baselines on IAC tasks using singe-step agent agent setup.Generalizing them to multi-step agent scenarios present the following challenges:</p>
<ol>
<li>
<p>How do we extend the notion of textual gradients to also include intermediate belief states in the absence of concrete feedback on them?</p>
</li>
<li>
<p>How do we ensure that the agent with the updated prompt continues to follow the agentic behaviour?</p>
</li>
</ol>
<p>Given these challenges, to draw a valid comparison of these baseline against METAREFLECTION we perform a single step adaption of the original two step COT.The adaption includes reducing the [Thought] and [Answer] steps within a single LLM call creating an analogue to single-step agentic flow and use this structure as a seed prompt.</p>
<p>Table 4 shows the performance of COT style of agent on HOTPOTQA benchmark.We observe that METAREFLECTION outperforms other promptoptimization baselines PROTEGI by 23.33% and PROMPTAGENT by 3.25% for the GT setting.Similarly, we see a similar trend in the Distractor setting where METAREFLECTION technique outperforms PROTEGI by 24% and PROMPTAGENT by 1.08%.Notably, compared to the best baseline PROMPTA-GENT, METAREFLECTION is capable of learning instructions to guide the Language Agent towards creating better thought actions instead of just optimizing it towards coming up with the right answer.</p>
<p>Towards truly multi-step agents While a COT agent can be represented as a single prompt with one call for optimization, similar adjustment is not possible for all multi-step agents.To this end we consider the REACT setup for HOTPOTQA unlike COT which entails a strict two step process with thought action followed by an answer, HOT-POTQA REACT setup can span over multiple steps of search and lookup actions.This leads to complex trajectories leading upto the final answer action for returning the answer.Besides, each action selection is conditioned on the previous action.Representing such a system with a single step prompt is non-trivial, making it hard to adapt existing prompt optimization techniques to work over such a set up.</p>
<p>METAREFLECTION procedure, on the other hand, allow us to seamlessly append semantic memory to such an agent and gather learning for each potential action states the agent can take.Table 5 shows the performance of multi-step RE-ACT style agent on HOTPOTQA.We observe that METAREFLECTION nearly doubles the performance of the baseline GPT4 agent.</p>
<p>For instance, in Figure 1, the updated semantic  memory contain clear instructions for the REACT agent to refine its search strategy if it gets stuck in loops.Note that this instruction guides the agent to better plan its trajectory rather than choosing the right answer.Later on, this learned rule aids the model in successfully concluding another trial where it was previously failing by explicitly guiding the action to look further down the context page to refine its lookup strategy, leading to the correct response, Bruce Dickinson.This is in contrast with the baseline attempt, where the agent ran out of trials by getting stuck in a loop.Notably, the HOTPOTQA REACT performance is lower than COT due to the nature of the REACT setup.The ReACT setting is based on the fullwiki setting where the task first involves "retrieving" relevant paragraphs from Wikipedia related to the Query and then "reasoning" over them.Being a more complex task, the ReACT setup exposes more failure surfaces (e.g., the system may not retrieve the correct information to begin with), which may lead to poorer performance compared to the simpler CoT setups.</p>
<p>Related Work</p>
<p>With the increasing ubiquity of black-box Large Language Models (OpenAI, 2023;Anil et al., 2023;Brown et al., 2020;Bai et al., 2022), there has been a growing interest in the community to develop strategies that can maximize the model's performance on a downstream task.These techniques may involve guiding an LLM to arrive at the correct answer (Wei et al., 2023;Zheng et al., 2023), output selection (Yao et al., 2023a;Poesia et al., 2022), picking up the right in-context examples (Khatry et al., 2023) or prompt optimization, etc. Being closely related to our work we dive deeper into contemporary Prompt optimization techniques in the literature.</p>
<p>Single Prompt Optimization</p>
<p>Given the potentially infinite space of instructions, recent works have studied the problem of 'guided' prompt search instead.To this end, OPRO (Yang et al., 2023)  The learned prompt can then be used during inference time in isolation.also leverage verbal feedback to generate and/or evolve task description prompts.In PE2 they additionally, maintain an optimization history to iteratively improve the prompt.PROMPTAGENT (Wang et al., 2023) also uses verbal feedback but views prompt optimization as a strategic planning problem and proposes a principled approach for prompt optimization.</p>
<p>Notably, these prompt optimization techniques are designed to work over single prompt single stepagents.The METAREFLECTION semantic memory learning can however be generalized for multi-step agents as well, as we demonstrated earlier.Even in the single-agent setup, METAREFLECTION differs from prompt optimization techniques in its focused objective-driven refinement of the memory instead of generating generic prompt candidates.This allows METAREFLECTION to perform especially good in knowledge intensive tasks.</p>
<p>LLMs as Agents</p>
<p>Recent works (Nakano et al., 2022;Schick et al., 2023) are leveraging these models to develop AI agents that act as a controller, extending their perceptual and action capabilities through tool utilization (Yao et al., 2023b;Qin et al., 2023).LLMbased agents can demonstrate reasoning and planning skills at par to symbolic agents by employing techniques like COT (Wei et al., 2023) and problem decomposition (Zhou et al., 2023a;Xi et al., 2023).Additionally, their ability to interact seamlessly using natural language comprehension allow them to operate in software development and research environment (Boiko et al., 2023;Qian et al., 2023).Furthermore, the interaction among multiple LLM-based agents can foster collaboration and competition, potentially leading to the emergence of complex social phenomena (Park et al., 2023).</p>
<p>Memory based methods in reinforcement learning</p>
<p>The concept of providing agents with controllable memory has a rich history.(Littman, 1993) discussed how hypothetical agents can utilize binary memory to store prior experiences, guiding their future action selection.In our context, we explore the interaction between language agents and their environment.</p>
<p>Previous research (Icarte et al., 2020) has demonstrated that memory-augmented agents can achieve globally optimal solutions.Inspired by the idea of rule-based semantic memory in humans (McRae and Jones, 2013), recent works have also investigated enabling reinforcement learning (RL) agents with semantic memory (Paischer et al., 2023).</p>
<p>Conclusion</p>
<p>In this work, we presented METAREFLECTION, a novel offline reinforcement learning technique that takes inspiration from how human brain store memories to enhance Language Agents by augmenting them with an experiential semantic memory.We further empirically demonstrated that the instructions learned using METAREFLECTION are more effective at capturing task-specific nuances.This behavior helps METAREFLECTION perform competitively with state of the art prompt optimization baseline for single-step agent scenarios.The objective driven iterative refinements also provide a significant advantage to capture the best learning at each iteration and save it to its semantic memory which helps the agent to perform better inference in next iteration.In the future, we would like to see how we can leverage our semantic memory in a multi-agent workflow.Sharing memories between agents can enhance synergy and improve task performance.</p>
<p>Limitation</p>
<p>METAREFLECTION currently relies on small scale held out validation for quantifying the efficacy of the batch.Such an approach leads to undesirable stochasticity in the results and cause instabilities in the learning.There is also scope of improving the quality of the reward signals that can potentially add to the stability of the learning.</p>
<p>A Appendix</p>
<p>A.1 METAREFLECTION optimized prompts</p>
<p>In this section, we show some input examples in several tasks for the base model .Specifically our tasks fall into 3 categories: binary classification, multiple choice selection and direct answer matching.As representative examples we take IAC vulnerability detection, casual judgement and HOT-POTQA.</p>
<p>In Figure 4, we illustrate that METAREFLEC-TION is capable of learning domain-specific instructions.Instructions 2 through 7 specifically address how 'azurerm_container_registry' influences the vulnerability within the code, thereby simplifying the task significantly.Conversely, in Figure 3, a detailed analysis of the prompt generated by PROMPTAGENT reveals an absence of domain-specific information relevant to the Terrascan policy it is intended to evaluate.</p>
<p>You are supplied with a terraform model and an Azure defense policy, both given in dictionary format.The policy is constructed around the 'Container Registry' resource type.Thoroughly examine each part of these inputs to understand and identify their characteristics.</p>
<p>Answer the question: "Does the terraform module comply with the given policy?" by considering the following instructions and applying them to your judgment.</p>
<p>Identify the terraform module as 'Compliant' if it meets either of these two conditions: 1.The resource type specified in the policy is not included in the terraform module.2. If the terraform module does contain the resource type, it must adhere to all the policy regulations.</p>
<p>Conversely, label the terraform module as 'Violating' if it includes the resource type in question as stated in the policy but falls short of complying with all the policy regulations.</p>
<p>Using your understanding of the context and the pattern of the defense policy dictionary, examine the terraform module and gauge whether it is 'Violating' or 'Compliant' regarding the presented policy.Your response should contain only 'Violating' or 'Compliant' with no further elaborations or explanations.</p>
<p>Reflect on the idea that merely listing the resource type within the terraform module isn't enough to fulfill compliance guidelines.It is imperative that to be flagged as 'Compliant', the module adheres to all policy regulations.If this is not the case, it should be labeled 'Violating'.</p>
<p>Notably, when the terraform modules do not outline the resource type specified in the policy, these modules are automatically determined as 'Compliant'.Use all the above information to formulate and provide your evaluation of the terraform model's alignment to the defense policy.</p>
<p>A.2 Vulnerability Detection in IAC Files</p>
<p>Infrastructure-as-Code (IAC) is a popular method of configuring cloud infrastructures, on platforms such as Azure and AWS, using a configuration coding language.These configuration files can declare resources such as virtual machines with specific capabilities, virtual networks and subnets, and data stores.IAC presents an alternative to the traditional ways of configuring cloud infrastructures, such as using a web-based graphical interface.There are numerous IAC platforms currently available for various cloud computing platforms.Here, we focus on Terraform, a leading IAC platform by Hashicorp (HashiCorp, 2023), as well as Azure, the cloud computing platform by Microsoft.Related Terraform resource declarations are grouped together into Terraform modules which act as a basic, reusable configuration component.1. Start by identifying the policy details.The policy in question is "reme_containerRegistryResourceLock" which pertains to the "azurerm_container_registry" resource type and requires that locks be implemented for resilience.2. Analyze the Terraform code to identify the resource types present.Look specifically for the "azurerm_container_registry" resource type as this is the resource type that the policy pertains to.3.If the "azurerm_container_registry" resource type is not present in the Terraform code, then the policy cannot be violated.In this case, you should predict False for policy violation.4. If the "azurerm_container_registry" resource type is present in the Terraform code, then proceed to the next step.5. Check if the "azurerm_container_registry" resource has implemented locks.This can be done by looking for the "azurerm_management_lock" resource in the Terraform code and checking if it is associated with the "azurerm_container_registry" resource.6.If the "azurerm_container_registry" resource has implemented locks, then the policy is not violated.In this case, you should predict False for policy violation.7. If the "azurerm_container_registry" resource has not implemented locks, then the policy is violated.In this case, you should predict True for policy violation.8. Remember to base your decisions on the evidence present in the Terraform code and avoid making assumptions about indirect compliance.9. Be adaptable and flexible in your approach, allowing for various scenarios and future updates to the policy or Terraform code.vn.type = "azurerm_virtual_network" object.get(vn.config,"subnet", \ "undefined") != "undefined" not sgExists(vn.config)traverse = "subnet[0].security_group"retVal := { "Id": vn.id, ... "Attribute": "subnet.security_group","Expected": "${<security_group_name>.id}", "Actual": "" } } sgExists(cfg) { ... } &lt;56 lines altogether&gt; language, the complexity of configurations and the diversity of the resources being handled across multiple infrastructure providers (e.g., Amazon AWS and Microsoft Azure).Further, Terraform uses a low-resource language -HashiCorp Configuration Language (HCL).</p>
<p>[Instructions End]</p>
<p>Task.Terrascan (Tenable, 2023) is a static analyzer for detecting security vulnerabilities in Terraform modules, and supports over 500 security policies, including 178 policies specific to Azure. Figure 5 shows the description and definition of a Terrascan policy that checks if every Azure virtual network subnet is configured with a corresponding network security policy.Note that the Terrascan policy is syntactic, i.e., it is checking for a declaration of an azurerm_virtual_network with a field named subnet, and so on.Hence, Terrascanlike static analysis based vulnerability detection is fragile and prone to both false positives and false negatives due to being sensitive to syntax.The task at hand is to check if a given Terraform module violates a given Terrascan policy.</p>
<p>Data collection.We collected 202 Terraform modules by mining GitHub repositories for IAC code written in HCL.These repositories corresponded to a diverse range of applications including load balancers, machine learning operations managers, and domain-specific data-stores.For policies, we selected the 10 most commonly violated Terrascan policies.Of the 2020 module-policy pairs, we eliminated a significant fraction of cases where the policies were not applicable to the module.For example, if the policy was for a specific resource type and the module did not contain declarations of that resource type, the pair was eliminated.After this process, we were left with 648 module-policy pairs, for which we manually annotated whether the module violated the policy (see Table 3 for the exact breakdown).Note that this ground-truth annotation was with respect to the description of Terrascan policy, not the definitionthat is, we use the intention behind the policy, not the letter of the definition.That is, we do not take the output of Terrascan as ground truth as it can be inaccurate, and instead manually examine if the policy (as per description) is violated.This data was then split into train and test sets in a 40 : 60 ratio per policy, taking care to balance the vulnerable and non-vulnerable classes.</p>
<p>ProTeGi optimized prompt</p>
<p>When answering, examine the question and context provided to offer an exact and accurate response without adding irrelevant details or making assumptions.Use the contextual details to answer specifically and pay attention to the language and form of the question.If a name is required in the answer, ensure to reproduce the exact name as it appears in the context without making alterations.Numeric or yes/no questions should be answered in a clear and straight-to-the-point manner, with no need for punctuation or capital letters.When the answer requires a term or phrase, select the most relevant and specific one as it appears in the context.Always maintain the form (singular or plural) used in the question when formulating answer.Answer must be based solely on the context and question provided, avoid straying from this by adding extra information unless it is specifically requested in the question.</p>
<p>PromptAgent optimized prompt</p>
<p>Your task is to provide an accurate answer to a question based on the given context.Be sure to consider all relevant information, not just those closest or directly related to the specifics of the question.Your task involves parsing both the local and larger context, and forming your response appropriately.The required answer may be indirectly linked to the focus of the question as justified by the overall context.It is important to process the complex or abstract links between the answer and the question within the context.Respond precisely, preferably with a single word or short phrase, but avoid forming full sentences or supplying extra information that doesn't serve the question.Remember, relevance is vital-your answer should reflect the content, essence, and nuance of the question while using the context as your guide.</p>
<p>MetaReflection learned instructions:</p>
<p>Be mindful of the exact phrasing in the question and the context.Even if your answer is technically correct, it may be marked as incorrect if it does not match the exact phrasing used in the context or the expected answer.Avoid adding extra information that is not directly asked in the question.Pay attention to singular and plural forms in both the question and the context.Be aware of implicit information in the context and make logical inferences when necessary.This will help you provide more accurate answers.When answering questions about historical figures or events, use the exact title or name mentioned in the context.When the question asks for numerical data, provide the number alone without additional context or explanation.When a question asks for a common characteristic or profession of multiple individuals, provide the answer in singular form even if it applies to multiple individuals.If the question asks for a specific individual or entity, provide only that individual or entity in your answer, even if the context mentions a connection to another individual or entity.Avoid including additional individuals or entities unless explicitly asked for in the question.When the question asks for a concept, provide the concept in the language used in the context, unless the question specifically asks for a translation or explanation.When answering questions about international competitions or events, include all relevant details from the context in your answer, especially when it comes to the country represented by the individual.When the context provides multiple names for the same entity, use the most detailed and specific name in your answer Experimental setup.As a baseline language agent, we use GPT-4 with an appropriate prompt that provides the code of the Terraform module and the description of the Terrascan policy, and asks if the module is vulnerable.While training, the agent is given a 0-1 feedback on whether its response is correct or not, and the model is asked to self-reflect if the response is incorrect.For each policy, we run the METAREFLECTION algorithm on the training // Method 1: For inline defintions resource "azurerm_virtual_network" "example" { ... subnet { ... security_group = ... } } // Method 2: Explicitly declared association resource "azurerm_subnet_nsg_association" { subnet_id = ... network_security_group_id = ... } (a) Associating a subnet with a NSG resource "azurerm_virtual_network" "vnet" { ... } resource "azurerm_subnet" "subnet" { name = "subnet1" ... } resource "azurerm_network_interface" "nic" { network_security_group_id = ... ip_configuration { subnet_id = "subnet1" } } resource "azurerm_virtual_machine" "..." { network_interface_ids = [ "nic" ] ... } (b) NSG associated with a VM's network interface instead of subnet.</p>
<p>4.</p>
<p>Remember that the association between "azurerm_virtual_network" and a NSG may not be direct.</p>
<p>It could be done through a separate resource block such as "azurerm_subnet_nsg_association" ... 7. Do not confuse NSG associations with network interfaces of VMs and the subnet of the Azure Virtual Network.The policy specifically requires the NSG be associated with the subnet.</p>
<p>(c) Instructions learned through meta-reflection  set and report the accuracy numbers for both the baseline agent and the agent with the instructions learned through METAREFLECTION.We also compare to LLMINSTRUCTION as another baselinehere the language model is asked to come up with instructions for a task given its description (Figure 7), and then these instructions are provided when the task is being performed.</p>
<p>Results.The results of the experiment are summarized in Tables 3 and 6 (last 2 cols).On the whole, across all policies, meta-reflection shows a 12−17% accuracy improvement over the baselines depending on the batch size.As Table 3 shows, meta-reflection provides consistent gains in accuracy for all policies over the GPT-4 baseline, with 32% in the best case.The precision with METARE-FLECTION is significantly better for all policies, while the recall decreases for some.</p>
<p>Exemplar case.We discuss the case of security policy reme_noSecurityGroupAssociated from Figure 5, i.e., that all Azure virtual network subnets are configured with a network security group (NSG).The main difficulty here is that HCL and Terraform offer many different ways of (a) associating a subnet with a virtual network, and (b) associating a NSG with a subnet.By default, the baseline GPT-4 agent fails to handle certain ways of specifying these associations, while spuriously assuming certain other associations.In Figure 8a, the baseline consistently failed to recognize a subnet-NSG association expressed using Method 2, i.e., using an explicitly declared association.On the other hand, it mis-identified declarations similar to the one in Figure 8b as valid subnet-NSG associations-here, the NSG is associated with a virtual machine's network interface (that is connected to the subnet) instead of the subnet itself.These limitations lead to both false positives and false negatives.With meta-reflection, we are able to learn the instructions in Figure 8c, using which the agent easily handles these kinds of cases.</p>
<p>Discussion.As the above exemplar case shows, METAREFLECTION is able to learn very domainspecific instructions to fix both false positives and false negatives.Other instructions include aspects like handling of wildcards for port numbers, stepby-step strategies for specific policies, etc.Note that these instructions not only include planning (or trajectory directing) instructions, but also grounding instructions-i.e., external facts that are not initially available.</p>
<p>In general, the experimental results show that meta-reflection is able to reduce the number of errors, i.e., improve the accuracy across a broad range of cases.However, one noticeable issue from the above results is the drop in recall for several policies.While the high recall in the baseline is artificial, coming at the cost of low precision, this is still an important issue to address.Our 0-1 feedback to the self-reflection agent does not state that false negatives are worse than false positives in the security domain.In the future, we plan to explore domain-specific feedback and self-reflection mechanisms that can account for the nature of errors, as well as better versions of the ShowsImprovement function that are aware of such domain-specific preferences.</p>
<p>A.3 HOTPOTQA HOTPOTQA (Yang et al., 2018) is an open-domain factual question answering dataset consisting of 113K question-answer pairs.The original paper proposes to use the data in 2 settings: (a) Distractor setting -where each question is to be answered using 10 wikipedia article excerpts; and (b) Full-Wiki setting which is a retrieval and reasoning task, where a given question is supposed to be answered after retrieving relevant context from wikipedia.</p>
<p>Notably, an answer is marked correct only if it matches exactly with the ground truth.Similar to Shinn et al. (Shinn et al., 2023), we design the following agents that operate over the dataset: (a) REACT-for the Full-Wiki setting (b) COT (Distractor) -for the Distractor Distractor setting (c) COT (GT) -a variant of COT (Distractor) with access to only ground truth articles.</p>
<p>Data Sampling.For each agent setting, we adversarialy sample subsets of the HOTPOTQA train split of 90K samples to create train and test sets.To perform adversarial sampling, we first identify samples where the base agent fails consistently in a given setting.On these failing examples we perform upto 3 self-reflection trials to get the model to the right response.If the agent is not able to get to the correct response even after self-reflection, we discard these samples.This strategy ensures that we get a set of hard examples in which the agents fail most of the times to get to the right answer in a single try, while also making sure that we filter examples that may be noisy due to missing context, incorrect questions etc.To account for randomness and given our computational budget, we sample 40 and 80 examples for the REACT train set and test set respectively.For COT settings, we pick 50 and 80 example respectively.</p>
<p>Experimental setup.We reuse the COT agent from (Wei et al., 2023) for the chain-of-thought experiments and use a re-implementation of (Yao et al., 2023c) for the REACT experiments.The RE-ACT agent is allowed at most 6 ACTION steps after which the trajectory is automatically determined to be a failure.Similar to Section A.2, we evaluate HOTPOTQA configurations for: (a) METAREFLEC-TION with batch sizes 1, 2, and 4; and (b) GPT-4 and LLMINSTRUCTION as baselines.In addition to this, we also evaluate variants of the agents powered by GPT-3.5-TURBOinstead of GPT-4, while using GPT-4 for METAREFLECTION.1</p>
<p>Results.We find that the generated METARE-FLECTION instruction consistently improved performance across different agent settings for HOT-POTQA.In Table 6, we present results using GPT-4 for both the agents and METAREFLECTION.We observe that METAREFLECTION help us achieve gains up to 32% for COT (GT), 18% for COT (Distractor), and 15.5% for REACT, over the respective test sets.Interestingly, higher batch sizes almost always help, reinforcing the importance of batching as observed in related works (Ouyang and Li, 2023;Ye et al., 2023).</p>
<p>In Table 7, we report results when using GPT-3.5-TURBO to power the client agents.We see gains of upto 21% gains for COT (GT), 12% for COT (Distractor) and 18.33% for the REACT case.Here, we observe that batching doesn't strictly improve the performance.Examining the data qualitatively, this difference can be attributed to the nature of the instructions generated in the two settings.In general we observe that with a small batch size, Solve a question answering task by having a Thought, then Finish with your answer.Thought can reason about the current situation.Finish[. . .] returns the answer and finishes the task.Question: What is one name for the Cassava after it is marketed as a paste and often served with palm nut soup?</p>
<p>Inference without meta-reflection</p>
<p>Thought: The question is asking for a name of cassava when it is marketed as a paste and often served with palm nut soup.From the context provided, I can see that cassava paste is referred to as "Akpu" in Nigeria and is often eaten with soups such as Egusi soup.Although palm nut soup is not explicitly mentioned in the context of Akpu, it is a type of soup that could be served with cassava paste, as indicated by the information about palm nut soup being used to accompany dishes like fufu, which is similar to Akpu in that they are both made from cassava.Action : Finish[Akpu] Answer a question interleaving Thought, Action, and Observation steps.Actions can be Search[. . .], Lookup[. . .], . . .Instructions: . . .If the context suggests multiple valid answers, choose the one that best matches the question's wording and the most direct information provided.In cases of ambiguity or multiple interpretations, provide an answer that adheres to the most common understanding of the question's terms and context.</p>
<p>. . .Question: What is one name for the Cassava after it is marketed as a paste and often served with palm nut soup?</p>
<p>Inference with meta-reflection . . .Thought: The question is asking for a name of cassava when it is marketed as a paste and often served with palm nut soup.From the context provided, I can see that cassava paste is referred to as "Akpu" in Nigeria and is often eaten with Egusi soup.However, since the question specifically mentions palm nut soup, I should look for a name that is associated with cassava paste served with palm nut soup.</p>
<p>The context also mentions "Plakali" as a staple food made of cassava dough and eaten with palm nut soup in Ghana.However, "Plakali" is described as cassava dough cooked in hot water, not as a marketed paste.The most direct match for cassava marketed as a paste and associated with palm nut soup is "Fufu," which is mentioned as a staple food often served with palm nut soup.Action: Finish[Fufu]</p>
<p>Figure 10: METAREFLECTION in the HOTPOTQA domain with COT style of reasoning METAREFLECTION produces a large amount of very specific instructions.On the contrary batching helps generalize these instructions into more widely applicable rules.GPT-4, being more powerful than GPT-3.5-TURBO, is able to better follow these abstract instructions, while specific instructions work better for GPT-3.5-TURBO.</p>
<p>// Chain-of-thought (A) Provide direct and concise responses to the question, using precise language that matches the specificity and terminology of the question, including singular or plural forms and definite articles as needed.</p>
<p>(B) If the context suggests multiple valid answers, choose the one that best matches the question's wording and the most direct information provided.</p>
<p>// React (C) When a question asks for a specific detail such as a 'full name', ensure to find and provide that exact information.Don't make assumptions based on limited or incomplete information.</p>
<p>(D) If you're not finding the desired information or stuck in a loop of looking up a keyword, consider changing the keyword and search strategy.The information might be located further down the page.</p>
<p>(E) When a question involves comparison, such as 'who received more rewards', ensure to search for each entity individually, gather all necessary information, and then make a comparison based on the data found.</p>
<p>(F) Be mindful of potential spelling errors or variations in the names of entities.If a search for a specific term doesn't yield results, consider possible alternative spellings or forms of the term.Exemplar case.Consider an example question from Figure 10.The question seeks information about the product made from Cassava and served with palm nut soup.The context presented within the COT (Distractor) setting includes articles about Akpu and Fufu, both of which are quite similar, being made from Cassava paste.However, the key distinction lies in Fufu being served with palm nut soup, while Akpu is served with Esupi soup.The baseline COT agent returns the incorrect response on this question: it is distracted by the related but incorrect articles, and makes an incorrect assumption and jumps to the wrong conclusion.</p>
<p>The METAREFLECTION technique learns an instruction that suggests looking for multiple valid answers and selecting the one most related to the question.When inferring with the meta-reflection instructions, it is clear from the thought that the agent did encounter the misleading answers, but was able to produce the right one by focusing on the specific key point "served with palm nut soup" mentioned in the question.</p>
<p>Similarly, in the REACT case (see Figure 1), we see the learned instruction enhancing search strategy by looking into the information further down the page rather looping around.This rule further aids the model in successfully concluding the trial where it was previously failing.The model uses the rule to explicitly guide the action space to look further down the context page and look up the right keyword, leading to the correct response, Bruce Dickinson.In contrast, in the baseline attempt, it ran out of trials by getting stuck in a loop.</p>
<p>Discussion.As we can see from the results, metareflection can produce significant improvements in accuracy in the question answer setting.This is especially promising given that the dataset was sampled using an adversarial sampling technique.The HOTPOTQA domain also shows the diversity of instructions learned by METAREFLECTION-a small selection of instructions learned in the COT and REACT settings are shown in Figure 9 We have instructions that are: i. specifically tuned to satisfy the overly strict rubric of the HOTPOTQA dataset (A); ii.domain-specific instructions for specific one-step actions in a RL trajectory (C); iii.directly the high-level strategy to be taken by the trajectory (D, E); and iv. for disambiguating answers (B) and questions (E).Further, the results on GPT-3.5-TURBOexperiments reveal that METAREFLEC-TION can be useful to enhance the performance</p>
<p>Answer a question interleaving Thought, Action, and Observation steps.Actions can be Search[. . .], Lookup[. . .], . . .Question: Which member of FC Barcelona scored the only goal in the 2010 FIFA World Cup final?Task Self-Reflection Thought 1: I need to search for the 2010 FIFA World Cup final and find out who scored the only goal in that match.Action 1: Search[2010 FIFA World Cup Final] Thought 2: The paragraph does not mention who scored the goal... Action 2: Lookup[goal] Obs.2: (Result 1/45) Sergio Ramos had a chance to score... Thought 3: The observation still does not contain...I need to continue looking for "goal" Action 3: Lookup[goal] Obs.3: (Result 2/45) Robben missed an opportunity... . . .</p>
<p>Figure 1: METAREFLECTION in the HOTPOTQA domain with a REACT agent</p>
<p>Figure 2 :
2
Figure 2: Snapshot of the optimized prompt learnt by the baselines and METAREFLECTION on IAC task</p>
<p>Figure 3 :
3
Figure 3: Prompt optimized by PROMPTAGENT</p>
<p>Figure 4 :
4
Figure 4: METAREFLECTION inference prompt for IAC</p>
<p>Figure 5 :
5
Figure 5: Rego code for the Terrascan policy reme_noSecurityGroupAssoc.</p>
<p>Figure 6 :
6
Figure 6: Snapshot of the optimized prompt learnt by the baselines and METAREFLECTION on HOTPOTQA task</p>
<p>Figure 8 :
8
Figure 8: reme_noSecurityGroupAssociated: Checking Subnet-NSG associations</p>
<p>Figure 7 :
7
Figure 7: Prompt for generating task-specific instructions in the LLMINSTRUCTION baseline</p>
<p>Figure 9 :
9
Figure 9: Instructions learned via the METAREFLEC-TION technique for the COT and REACT reasoning on HOTPOTQA.</p>
<p>Table 1 :
1
Count of train and test set distribution across benchmarks information on IAC, Terraform, Terrascan and data collection in Appendix A.2.
DatasetTrain set Test setBIOSSES6040CASUAL JUDGEMENT90100EPISTEMIC REASONING500500TEMPORAL SEQUENCE300500IAC Vulnerability Detection166250HOTPOTQA5080Complex Reasoning (BIGBENCH) Big-BenchHard (BBH) (Suzgun et al., 2022) consists of asubset of particularly challenging tasks from BIG-
(Srivastava et al., 2022)2022)that contains challenging reasoning questions.In this work, we pick 3 datasets from BBH -1.Causal Judgement 2. Temporal Sequence 3. Epistemic Reasoning and follow the exact same test and train distribution used in</p>
<p>Table 2 :
2
Comparison of METAREFLECTION with other baselines in a single-step agent set-up.#calls refers to average numbers of LLM calls required to generate optimal prompt
DatasetGPT-4PROTEGIPROMPTAGENTMETAREFLECTIONAcc σAcc σ# calls Acc σ# calls Acc σ# callsBIOSSES70.00 3.17 80.75 0.71 647880.00 1.34 50384.00 0.57 475Causal Judgement74.00 1.15 72.20 3.67 972573.40 3.19 35177.00 2.00 313Epistemic Reasoning70.80 1.03 85.40 2.40 16640 91.00 0.83 214388.40 1.02 1954Temporal Sequence98.00 0.57 97.50 0.27 13824 98.30 1.11 201799.10 0.63 1491IAC Vulnerability Detection 73.36 0.01 72.64 4.76 814884.17 6.91 35490.18 0.28 166PolicyData statistics Files Violating CompliantGPT-4 PROTEGI PROMPTAGENT METAREFLECTIONnetworkPolicyEnabled2310134862.1871.0683.34kubeDashboardDisabled293268986.2290.8094.63reme_keyVaultAuditLoggingEnabled4410345577.9782.7198.18reme_containerRegistryResourceLock359265368.3790.2296.55reme_appGatewayWAFEnabled5936237268.0688.7092.30networkPort22ExposedToInternetAz274239487.8489.0296.29networkPort22ExposedToPublicAz356299485.2280.6094.11reme_noSecurityGroupAssociated9060307067.2082.6087.58reme_checkStorageContainerAccess195149188.6288.05100.00reme_resourceGroupLock5543128180.3377.9882.90</p>
<p>Table 4 :
4
Comparison of METAREFLECTION with other baselines using COT style agent on prompt refinement.# calls refers to average numbers of LLM calls required to generate optimal prompt.
DatasetGPT-4PROTEGIPROMPTAGENTMETAREFLECTIONAccσAccσ# callsAccσ# callsAccσ# callsHOTPOTQA (GT)43.67 2.08 31.67 3.61540351.75 1.5238355.00 1.00303HOTPOTQA (Distractor) 37.33 2.08 23.33 0.72613246.25 2.1656847.33 1.15314Acc.σGPT-419.58 1.91METAREFLECTION35.00 1.25</p>
<p>Table 5 :
5
Comparison of METAREFLECTION and GPT-4 using REACT style agent on prompt refinement for HOTPOTQA.</p>
<p>You are an expert on Terraform and Azure.You are very good at detecting Terrascan rule violations in Terraform code.Answer as concisely as possible.Identify if the following policy is being violated by the Terraform code module below.Provide your response as'True' if you think the policy is violated, 'False' if you think the policy is not violated.
[Terraform Policy Begin]……………[Terraform Policy End][Terraform Module Begin]:```terraformFile Name: acr.tfCode:[Terraform Code Start]resource "random_id" "acr_suffix" {byte_length = 8} …………[Terraform Code End][Terraform Module End]Here are additional instructions that were added to avoidprevious mistakesyou made in the past. Follow these instructions as well.[Instructions Start]</p>
<p>Table 6 :
6</p>
<p>HOTPOTQAIAC vulnerabilityCOT (GT)COT (Distractor)REACTdetectionAcc.σAcc.σAcc.σAcc.σGPT-420.67 3.7932.673.2119.58 1.9173.360.012LLMINSTRUCTION20.67 4.5130.679.8127.52.573.310.43METAREFLECTION (batch size = 1)24.67 3.5138.002.6530.41 0.7287.170.63METAREFLECTION (batch size = 2)50.33 0.5848.672.5235.00 1.2585.090.4METAREFLECTION (batch size = 4)52.00 2.0050.671.5331.67 1.4490.180.28
A similar experiment on the IAC domain wasn't possible due to large context length of the Terraform modules
Table7: Results on HOTPOTQA with GPT-3.5-TURBOpowering the agents and GPT-4 for reflections and METAREFLECTION of smaller models by providing instructions rich in specific insights from a more powerful LLMs like GPT-4.This shows some resemblance to task-specific distillation and can be interesting to explore further in future works.
Reflexion: language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Shunyu Karthik R Narasimhan, Yao, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Biosses: a semantic sentence similarity estimation system for the biomedical domain. Gizem Sogancıoglu, Hakime Öztürk, Arzucan Özgür, Bioinformatics. 33142017</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.04615Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. 2022arXiv preprint</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Ed H Quoc V Le, Denny Chi, Zhou, arXiv:2210.092612022arXiv preprint</p>
<p>Terrascan sandbox | tenable. 2023</p>
<p>Promptagent: Strategic planning with language models enables expert-level prompt optimization. Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P Xing, Zhiting Hu, arXiv:2310.164272023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, Thirtysixth Conference on Neural Information Processing Systems. 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Autogen: Enabling next-gen llm applications via multi-agent conversation framework. Lilian Weng ; Qingyun, Gagan Wu, Jieyu Bansal, Yiran Zhang, Beibin Wu, Erkang Li, Li Zhu, Xiaoyun Jiang, Shaokun Zhang, Jiale Zhang, Ahmed Liu, Ryen W Hassan Awadallah, Doug White, Chi Burger, Wang, 2023. 2023Llm powered autonomous agents</p>
<p>Self-polish: Enhance reasoning in large language models via problem refinement. Z Xi, S Jin, Y Zhou, CoRR, abs/2305.144972023</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, Xinyun Chen, ; Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning, arXiv:2309.03409Conference on Empirical Methods in Natural Language Processing (EMNLP). 2023. 2018PreprintLarge language models as optimizers</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik R Narasimhan, Thirty-seventh Conference on Neural Information Processing Systems. 2023a</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022arXiv preprint</p>
<p>REACT: Synergizing Reasoning and Acting in Language Models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)Princeton, NJ, USA2023bICLR. *Equal contribution</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Yuan Karthik R Narasimhan, Cao, The Eleventh International Conference on Learning Representations. 2023c</p>
<p>Prompt engineering a prompt engineer. Qinyuan Ye, Maxamed Axmed, Reid Pryzant, Fereshte Khani, arXiv:2311.056612023Preprint</p>
<p>Take a step back: Evoking reasoning via abstraction in large language models. Swaroop Huaixiu Steven Zheng, Xinyun Mishra, Heng-Tze Chen, Ed H Cheng, Chi, Denny Quoc V Le, Zhou, arXiv:2310.061172023Preprint</p>
<p>Least-tomost prompting enables complex reasoning in large language models. D Zhou, N Schärli, L Hou, Proceedings of the Eleventh International Conference on Learning Representations (ICLR). the Eleventh International Conference on Learning Representations (ICLR)Kigali, Rwanda2023aOpenReview.net</p>
<p>Large language models are human-level prompt engineers. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba, The Eleventh International Conference on Learning Representations. 2023b</p>            </div>
        </div>

    </div>
</body>
</html>
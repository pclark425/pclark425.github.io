<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1812 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1812</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1812</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-950fdf14b46d5b00e8e854d9cf415804c883cdb1</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/950fdf14b46d5b00e8e854d9cf415804c883cdb1" target="_blank">RetinaGAN: An Object-aware Approach to Sim-to-Real Transfer</a></p>
                <p><strong>Paper Venue:</strong> IEEE International Conference on Robotics and Automation</p>
                <p><strong>Paper TL;DR:</strong> RetinaGAN, a generative adversarial network approach to adapt simulated images to realistic ones with object-detection consistency, is introduced and shows the method bridges the visual gap for a novel door opening task using imitation learning in a new visual domain.</p>
                <p><strong>Paper Abstract:</strong> The success of deep reinforcement learning (RL) and imitation learning (IL) in vision-based robotic manipulation typically hinges on the expense of large scale data collection. With simulation, data to train a policy can be collected efficiently at scale, but the visual gap between sim and real makes deployment in the real world difficult. We introduce RetinaGAN, a generative adversarial network (GAN) approach to adapt simulated images to realistic ones with object-detection consistency. RetinaGAN is trained in an unsupervised manner without task loss dependencies, and preserves general object structure and texture in adapted images. We evaluate our method on three real world tasks: grasping, pushing, and door opening. RetinaGAN improves upon the performance of prior sim-to-real methods for RL-based object instance grasping and continues to be effective even in the limited data regime. When applied to a pushing task in a similar visual domain, RetinaGAN demonstrates transfer with no additional real data requirements. We also show our method bridges the visual gap for a novel door opening task using imitation learning in a new visual domain. Visit the project website at retinagan.github.io</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1812.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1812.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RetinaGAN-grasping</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RetinaGAN applied to Q2-Opt instance grasping</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sim-to-real image-domain adaptation (RetinaGAN) was used to translate PyBullet simulation images to realistic-looking images with an object-detection consistency loss, and Q2-Opt policies trained on these adapted images achieved high real-world instance-grasping success without real-policy training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Q2-Opt instance grasping agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A vision-based robotic grasping policy trained with the Q2-Opt distributed RL framework (extension of QT-Opt) that takes RGB and a binary target mask and outputs grasping actions for a robot arm in a bin-sorting station.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation (vision-based robotic grasping)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Physics-based simulation (PyBullet) used to simulate object spawn, robot kinematics, contact interactions, and rendered RGB images; scenes include multiple spawned objects (9–18) in bin sorting setups.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>approximate physics simulation with non-photorealistic rendering (PyBullet); visual realism limited and augmented with photometric distortions</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>rigid-body physics and contact interactions (PyBullet), object spawning and pose variability, object geometry, camera viewpoint; simulation produced RGB renders and binary target masks; photometric distortions (brightness, saturation, hue, contrast, noise) applied as augmentations</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>image photorealism (native renders not photorealistic), fine-grained material appearance and lighting diversity, real-world texture and background variability, possibly unmodeled exact contact/frictional micro-physics and sensor calibration differences (paper identifies the visual gap as the primary bottleneck)</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical bin-based trash sorting stations with six robots across three bins; real images include cluttered objects (cups, cans, bottles and other discarded objects) and real camera sensors used during evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>instance grasping of cups, cans, and bottles (vision-based grasp selection and execution in cluttered bins)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>reinforcement learning (Q2-Opt) trained on sim-derived data; sim images were adapted via RetinaGAN and used as training input for the policy</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>task success rate (percentage of successful grasps) evaluated over 90 trials (30 attempts per class across 3 classes and multiple robots)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>92% instance grasping success in simulation (Q2-Opt)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>80.0% real-world instance grasping success when Q2-Opt trained on sim-to-real (RetinaGAN) translated data (no real RL episodes); other reported values: Sim-only 18.9%, Randomized Sim 41.1%, CycleGAN 67.8%, RL-CycleGAN 68.9%, RetinaGAN+Real (with mixed real episodes) up to 80.0%–65.6% depending on data mix</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Photometric distortions applied to simulated images (brightness, saturation, hue, contrast, noise). Baseline domain-randomized experiments varied textures and light positions; Automatic Domain Randomization cited for other tasks but not used here.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>primary factor: visual domain gap (appearance/texture/lighting/background); other contributing factors implicitly present: differences in contact/friction dynamics and sensor calibration, but authors state they observed the visual difference to be the bottleneck</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>object-aware pixel-level domain adaptation (RetinaGAN) with a frozen EfficientDet object detector enforcing perception consistency (box + class consistency with Focal Consistency Loss); sufficiently large simulated on-policy data (hundreds of thousands to millions of episodes) combined with RetinaGAN-adapted images; mixing of real images for some experiments further improved performance</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>paper emphasizes that photorealistic rendering is not strictly required if object-level visual semantics are preserved; enforcing object-detection consistency during image translation is critical to preserve task-relevant features — no numeric fidelity thresholds provided</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>No real RL fine-tuning required for the reported 80% result (Q2-Opt trained on RetinaGAN-translated simulated data only). The paper also reports mixed-data regimes (RetinaGAN+Real) where real episodes were included during Q2-Opt training (e.g., RetinaGAN+Real with 10K real episodes achieved 65.6%; with 135K+ real episodes, RetinaGAN+Real achieved 80%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Comparison across domain-adaptation / training-data regimes: Sim-only (18.9%); Randomized Sim (41.1%); CycleGAN (67.8%); RL-CycleGAN (68.9%); RetinaGAN (80.0%) for sim-only policy training on translated images. Mixing real data further changes numbers (e.g., RetinaGAN+Real up to 80%).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Object-aware GAN (RetinaGAN) that enforces detection-level consistency substantially reduces the visual sim-to-real gap for instance grasping; policies trained on RetinaGAN-translated sim images can achieve high real-world success (80%) without real-policy episodes, outperforming prior pixellevel adaptation baselines and domain-randomization baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RetinaGAN: An Object-aware Approach to Sim-to-Real Transfer', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1812.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1812.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RetinaGAN-pushing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RetinaGAN applied to 3D object pushing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Q2-Opt RL policy trained in PyBullet to push upright bottles was trained on RetinaGAN-adapted simulated images and achieved strong transfer (90% real-world success) with no additional real data or real policy training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Q2-Opt 3D pushing agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Vision-based RL controller (Q2-Opt) that issues pushing actions for a robot to move an upright object (tea bottle) to a goal marker within a bin while keeping it upright.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation (non-planar 3D object pushing)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Physics simulation (PyBullet) modeling object placement (single bottle), robot interactions, and rendered RGB images including a goal marker; used to generate on-policy training episodes for pushing</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>approximate physics simulation for pushing dynamics with non-photorealistic rendering; simulation produced successful pushing policies in sim but failed in reality without sim-to-real adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>rigid-body contact dynamics for pushing (PyBullet), object upright stability, robot kinematics, rendered RGB observations with stacked frames (initial with goal marker + current frame)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>visual realism and fine visual cues (lighting, textures, backgrounds) not photorealistic; possibly simplified friction and contact micro-physics relative to reality (paper does not claim high-fidelity contact beyond PyBullet's model)</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical bin with a single tea bottle (Ito En Green Tea Bottle, filled 25% water) which must be pushed to within 5 cm of a red goal marker without toppling</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>3D object pushing (push upright bottle to goal while keeping it upright)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>reinforcement learning (Q2-Opt) trained in simulation; training input images were adapted via RetinaGAN for the sim-to-real experiment</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>task success rate (percentage of successful pushes) measured over 10 attempts</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>90% success in simulation (Q2-Opt pushing task)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>RetinaGAN: 90.0% real-world push success; Sim-only: 0.0% real-world success</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Photometric distortions and baseline domain randomization experiments (textures, lighting) are described in the paper; RetinaGAN experiments used photometric distortions and GAN-based pixel adaptation instead of relying solely on domain randomization</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>visual appearance mismatch (rendered vs real RGB) produced a complete sim-to-real failure for sim-only policy; differences in visual cues and potentially unmodeled sensor/lighting differences hindered direct transfer</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Using RetinaGAN to adapt simulation images to the real visual domain preserved object structure via detection-consistency loss and enabled direct transfer of a simulation-trained pushing policy without real-policy episodes</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Visual-semantic consistency (object structure and position preservation) during image translation is critical; no quantitative physics-fidelity thresholds provided</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>No real-policy fine-tuning; RetinaGAN was reused from grasping experiments without additional real data and the pushing policy trained purely on adapted simulated images.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Direct sim-only transfer failed completely (0% real success). Applying RetinaGAN-adapted images for training restored sim performance in the real world (90% real success).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>When visual realism is the primary barrier, object-aware pixel-level adaptation (RetinaGAN) can enable near-perfect transfer for pushing tasks — policies trained on RetinaGAN-translated simulated images achieved 90% real success with no real policy data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RetinaGAN: An Object-aware Approach to Sim-to-Real Transfer', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1812.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1812.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RetinaGAN-door</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RetinaGAN (and Ensemble-RetinaGAN) applied to door opening imitation learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RetinaGAN (including an ensemble of GANs) was used to adapt simulated demonstration images for imitation learning of door-opening behaviors; an ensemble of RetinaGAN models enabled high success (up to ~96.6%) even with no real demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>ResNet-FiLM door-opening IL agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A supervised imitation-learning policy (ResNet-FiLM, ResNet-18 backbone) mapping adapted RGB observations to base-motion commands to approach and open conference-room doors and drive into rooms.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics imitation learning (navigation and door manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>PyBullet (simulated human demonstrations and environments)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulation of conference-room doors and robot base/navigation, with simulated human demonstration frames; renders provide RGB images of doors and rooms for training IL policies</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>approximate environment and robot rendering with limited photorealism; simulation captures door geometry and robot pose dynamics but visual domain differs substantially from real conference rooms</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>door geometry, robot base kinematics, demonstration sequences in simulation, RGB rendering of scenes (but not photorealistic), object poses and backgrounds</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>lighting, room background variability, fine-grained textures and environmental details are not photorealistic; detector trained on recycling objects produced only confident detections for the robot arm, indicating incomplete semantic coverage</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Three real conference-room doors (two swing rightwards, one leftwards); evaluation judged success if robot pushes door open and robot base enters the room</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>door opening and entry (navigation / interaction) learned by imitation from simulated demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>imitation learning / behavioral cloning (ResNet-FiLM) trained on a mixture of simulated demonstrations, RetinaGAN-adapted simulated demonstrations, and optionally real demonstrations; RetinaGAN (and ensembles) used to adapt images</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>task success rate (percentage of successful door-opening and entry episodes) averaged over 30 trials (three doors × ten trials each)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>98% success in simulation for IL policy trained on simulation demonstrations</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Ensemble-RetinaGAN (no real data): 96.6% real success; Ensemble-RetinaGAN+Real: 93.3%; RetinaGAN+Real (single GAN) 76.7%; Sim-only 0.0%; Real-only 36.6%; Sim+Real 75.0%</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Paper reports photometric distortions and baseline domain-randomization approaches; for the door task the authors used RetinaGAN (and ensembles of GANs) rather than heavy domain randomization to capture visual diversity</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>large visual-domain mismatch (lighting, room structure, background); low-probability or missing detector outputs for non-recycling objects in the door domain (detector trained on recycling objects) could limit semantic supervision, causing ambiguity in translations</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Perception-consistency loss using a pre-trained EfficientDet (even if trained on a different object set) helped preserve room structure and door locations in translated images; using an ensemble of RetinaGANs (diverse seeds/consistency weights) improved robustness by adding diversity to translated image set, enabling transfer without real data</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Preserving semantic structure (door frames, robot arm location) during image translation is essential; ensemble diversity of GAN outputs improves robustness when training data and detector are mismatched — no numerical fidelity thresholds specified</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Ensemble-RetinaGAN achieved 96.6% success when IL models were trained on adapted simulated demonstrations only (no real demonstrations). The paper also reports mixed-data experiments where small real demonstration sets were included (29,000 real demonstrations were collected for some experiments), but the high-performance ensemble result was achieved without real demonstration fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Sim-only IL failed in real (0%); adding real demonstrations or using single RetinaGAN with some real data yields ~75–77%; ensemble of multiple RetinaGANs yields ~93–97% success and can achieve high transfer even without real data.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Object-aware pixel-level adaptation with detection consistency can preserve critical semantic structure (door frames, robot pose) across domains; ensembling multiple adapted-image generators increases visual diversity and robustness, enabling near-complete transfer of imitation-learned door-opening policies without real demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RetinaGAN: An Object-aware Approach to Sim-to-Real Transfer', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Unpaired image-to-image translation using cycle-consistent adversarial networks <em>(Rating: 2)</em></li>
                <li>Rlcyclegan: Reinforcement learning aware simulation-to-real <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Unsupervised pixel-level domain adaptation with generative adversarial networks <em>(Rating: 2)</em></li>
                <li>Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1812",
    "paper_id": "paper-950fdf14b46d5b00e8e854d9cf415804c883cdb1",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "RetinaGAN-grasping",
            "name_full": "RetinaGAN applied to Q2-Opt instance grasping",
            "brief_description": "Sim-to-real image-domain adaptation (RetinaGAN) was used to translate PyBullet simulation images to realistic-looking images with an object-detection consistency loss, and Q2-Opt policies trained on these adapted images achieved high real-world instance-grasping success without real-policy training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Q2-Opt instance grasping agent",
            "agent_system_description": "A vision-based robotic grasping policy trained with the Q2-Opt distributed RL framework (extension of QT-Opt) that takes RGB and a binary target mask and outputs grasping actions for a robot arm in a bin-sorting station.",
            "domain": "general robotics manipulation (vision-based robotic grasping)",
            "virtual_environment_name": "PyBullet",
            "virtual_environment_description": "Physics-based simulation (PyBullet) used to simulate object spawn, robot kinematics, contact interactions, and rendered RGB images; scenes include multiple spawned objects (9–18) in bin sorting setups.",
            "simulation_fidelity_level": "approximate physics simulation with non-photorealistic rendering (PyBullet); visual realism limited and augmented with photometric distortions",
            "fidelity_aspects_modeled": "rigid-body physics and contact interactions (PyBullet), object spawning and pose variability, object geometry, camera viewpoint; simulation produced RGB renders and binary target masks; photometric distortions (brightness, saturation, hue, contrast, noise) applied as augmentations",
            "fidelity_aspects_simplified": "image photorealism (native renders not photorealistic), fine-grained material appearance and lighting diversity, real-world texture and background variability, possibly unmodeled exact contact/frictional micro-physics and sensor calibration differences (paper identifies the visual gap as the primary bottleneck)",
            "real_environment_description": "Physical bin-based trash sorting stations with six robots across three bins; real images include cluttered objects (cups, cans, bottles and other discarded objects) and real camera sensors used during evaluation",
            "task_or_skill_transferred": "instance grasping of cups, cans, and bottles (vision-based grasp selection and execution in cluttered bins)",
            "training_method": "reinforcement learning (Q2-Opt) trained on sim-derived data; sim images were adapted via RetinaGAN and used as training input for the policy",
            "transfer_success_metric": "task success rate (percentage of successful grasps) evaluated over 90 trials (30 attempts per class across 3 classes and multiple robots)",
            "transfer_performance_sim": "92% instance grasping success in simulation (Q2-Opt)",
            "transfer_performance_real": "80.0% real-world instance grasping success when Q2-Opt trained on sim-to-real (RetinaGAN) translated data (no real RL episodes); other reported values: Sim-only 18.9%, Randomized Sim 41.1%, CycleGAN 67.8%, RL-CycleGAN 68.9%, RetinaGAN+Real (with mixed real episodes) up to 80.0%–65.6% depending on data mix",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Photometric distortions applied to simulated images (brightness, saturation, hue, contrast, noise). Baseline domain-randomized experiments varied textures and light positions; Automatic Domain Randomization cited for other tasks but not used here.",
            "sim_to_real_gap_factors": "primary factor: visual domain gap (appearance/texture/lighting/background); other contributing factors implicitly present: differences in contact/friction dynamics and sensor calibration, but authors state they observed the visual difference to be the bottleneck",
            "transfer_enabling_conditions": "object-aware pixel-level domain adaptation (RetinaGAN) with a frozen EfficientDet object detector enforcing perception consistency (box + class consistency with Focal Consistency Loss); sufficiently large simulated on-policy data (hundreds of thousands to millions of episodes) combined with RetinaGAN-adapted images; mixing of real images for some experiments further improved performance",
            "fidelity_requirements_identified": "paper emphasizes that photorealistic rendering is not strictly required if object-level visual semantics are preserved; enforcing object-detection consistency during image translation is critical to preserve task-relevant features — no numeric fidelity thresholds provided",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": "No real RL fine-tuning required for the reported 80% result (Q2-Opt trained on RetinaGAN-translated simulated data only). The paper also reports mixed-data regimes (RetinaGAN+Real) where real episodes were included during Q2-Opt training (e.g., RetinaGAN+Real with 10K real episodes achieved 65.6%; with 135K+ real episodes, RetinaGAN+Real achieved 80%).",
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": "Comparison across domain-adaptation / training-data regimes: Sim-only (18.9%); Randomized Sim (41.1%); CycleGAN (67.8%); RL-CycleGAN (68.9%); RetinaGAN (80.0%) for sim-only policy training on translated images. Mixing real data further changes numbers (e.g., RetinaGAN+Real up to 80%).",
            "key_findings": "Object-aware GAN (RetinaGAN) that enforces detection-level consistency substantially reduces the visual sim-to-real gap for instance grasping; policies trained on RetinaGAN-translated sim images can achieve high real-world success (80%) without real-policy episodes, outperforming prior pixellevel adaptation baselines and domain-randomization baselines.",
            "uuid": "e1812.0",
            "source_info": {
                "paper_title": "RetinaGAN: An Object-aware Approach to Sim-to-Real Transfer",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "RetinaGAN-pushing",
            "name_full": "RetinaGAN applied to 3D object pushing",
            "brief_description": "A Q2-Opt RL policy trained in PyBullet to push upright bottles was trained on RetinaGAN-adapted simulated images and achieved strong transfer (90% real-world success) with no additional real data or real policy training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Q2-Opt 3D pushing agent",
            "agent_system_description": "Vision-based RL controller (Q2-Opt) that issues pushing actions for a robot to move an upright object (tea bottle) to a goal marker within a bin while keeping it upright.",
            "domain": "general robotics manipulation (non-planar 3D object pushing)",
            "virtual_environment_name": "PyBullet",
            "virtual_environment_description": "Physics simulation (PyBullet) modeling object placement (single bottle), robot interactions, and rendered RGB images including a goal marker; used to generate on-policy training episodes for pushing",
            "simulation_fidelity_level": "approximate physics simulation for pushing dynamics with non-photorealistic rendering; simulation produced successful pushing policies in sim but failed in reality without sim-to-real adaptation",
            "fidelity_aspects_modeled": "rigid-body contact dynamics for pushing (PyBullet), object upright stability, robot kinematics, rendered RGB observations with stacked frames (initial with goal marker + current frame)",
            "fidelity_aspects_simplified": "visual realism and fine visual cues (lighting, textures, backgrounds) not photorealistic; possibly simplified friction and contact micro-physics relative to reality (paper does not claim high-fidelity contact beyond PyBullet's model)",
            "real_environment_description": "Physical bin with a single tea bottle (Ito En Green Tea Bottle, filled 25% water) which must be pushed to within 5 cm of a red goal marker without toppling",
            "task_or_skill_transferred": "3D object pushing (push upright bottle to goal while keeping it upright)",
            "training_method": "reinforcement learning (Q2-Opt) trained in simulation; training input images were adapted via RetinaGAN for the sim-to-real experiment",
            "transfer_success_metric": "task success rate (percentage of successful pushes) measured over 10 attempts",
            "transfer_performance_sim": "90% success in simulation (Q2-Opt pushing task)",
            "transfer_performance_real": "RetinaGAN: 90.0% real-world push success; Sim-only: 0.0% real-world success",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Photometric distortions and baseline domain randomization experiments (textures, lighting) are described in the paper; RetinaGAN experiments used photometric distortions and GAN-based pixel adaptation instead of relying solely on domain randomization",
            "sim_to_real_gap_factors": "visual appearance mismatch (rendered vs real RGB) produced a complete sim-to-real failure for sim-only policy; differences in visual cues and potentially unmodeled sensor/lighting differences hindered direct transfer",
            "transfer_enabling_conditions": "Using RetinaGAN to adapt simulation images to the real visual domain preserved object structure via detection-consistency loss and enabled direct transfer of a simulation-trained pushing policy without real-policy episodes",
            "fidelity_requirements_identified": "Visual-semantic consistency (object structure and position preservation) during image translation is critical; no quantitative physics-fidelity thresholds provided",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": "No real-policy fine-tuning; RetinaGAN was reused from grasping experiments without additional real data and the pushing policy trained purely on adapted simulated images.",
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": "Direct sim-only transfer failed completely (0% real success). Applying RetinaGAN-adapted images for training restored sim performance in the real world (90% real success).",
            "key_findings": "When visual realism is the primary barrier, object-aware pixel-level adaptation (RetinaGAN) can enable near-perfect transfer for pushing tasks — policies trained on RetinaGAN-translated simulated images achieved 90% real success with no real policy data.",
            "uuid": "e1812.1",
            "source_info": {
                "paper_title": "RetinaGAN: An Object-aware Approach to Sim-to-Real Transfer",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "RetinaGAN-door",
            "name_full": "RetinaGAN (and Ensemble-RetinaGAN) applied to door opening imitation learning",
            "brief_description": "RetinaGAN (including an ensemble of GANs) was used to adapt simulated demonstration images for imitation learning of door-opening behaviors; an ensemble of RetinaGAN models enabled high success (up to ~96.6%) even with no real demonstrations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "ResNet-FiLM door-opening IL agent",
            "agent_system_description": "A supervised imitation-learning policy (ResNet-FiLM, ResNet-18 backbone) mapping adapted RGB observations to base-motion commands to approach and open conference-room doors and drive into rooms.",
            "domain": "robotics imitation learning (navigation and door manipulation)",
            "virtual_environment_name": "PyBullet (simulated human demonstrations and environments)",
            "virtual_environment_description": "Simulation of conference-room doors and robot base/navigation, with simulated human demonstration frames; renders provide RGB images of doors and rooms for training IL policies",
            "simulation_fidelity_level": "approximate environment and robot rendering with limited photorealism; simulation captures door geometry and robot pose dynamics but visual domain differs substantially from real conference rooms",
            "fidelity_aspects_modeled": "door geometry, robot base kinematics, demonstration sequences in simulation, RGB rendering of scenes (but not photorealistic), object poses and backgrounds",
            "fidelity_aspects_simplified": "lighting, room background variability, fine-grained textures and environmental details are not photorealistic; detector trained on recycling objects produced only confident detections for the robot arm, indicating incomplete semantic coverage",
            "real_environment_description": "Three real conference-room doors (two swing rightwards, one leftwards); evaluation judged success if robot pushes door open and robot base enters the room",
            "task_or_skill_transferred": "door opening and entry (navigation / interaction) learned by imitation from simulated demonstrations",
            "training_method": "imitation learning / behavioral cloning (ResNet-FiLM) trained on a mixture of simulated demonstrations, RetinaGAN-adapted simulated demonstrations, and optionally real demonstrations; RetinaGAN (and ensembles) used to adapt images",
            "transfer_success_metric": "task success rate (percentage of successful door-opening and entry episodes) averaged over 30 trials (three doors × ten trials each)",
            "transfer_performance_sim": "98% success in simulation for IL policy trained on simulation demonstrations",
            "transfer_performance_real": "Ensemble-RetinaGAN (no real data): 96.6% real success; Ensemble-RetinaGAN+Real: 93.3%; RetinaGAN+Real (single GAN) 76.7%; Sim-only 0.0%; Real-only 36.6%; Sim+Real 75.0%",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Paper reports photometric distortions and baseline domain-randomization approaches; for the door task the authors used RetinaGAN (and ensembles of GANs) rather than heavy domain randomization to capture visual diversity",
            "sim_to_real_gap_factors": "large visual-domain mismatch (lighting, room structure, background); low-probability or missing detector outputs for non-recycling objects in the door domain (detector trained on recycling objects) could limit semantic supervision, causing ambiguity in translations",
            "transfer_enabling_conditions": "Perception-consistency loss using a pre-trained EfficientDet (even if trained on a different object set) helped preserve room structure and door locations in translated images; using an ensemble of RetinaGANs (diverse seeds/consistency weights) improved robustness by adding diversity to translated image set, enabling transfer without real data",
            "fidelity_requirements_identified": "Preserving semantic structure (door frames, robot arm location) during image translation is essential; ensemble diversity of GAN outputs improves robustness when training data and detector are mismatched — no numerical fidelity thresholds specified",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": "Ensemble-RetinaGAN achieved 96.6% success when IL models were trained on adapted simulated demonstrations only (no real demonstrations). The paper also reports mixed-data experiments where small real demonstration sets were included (29,000 real demonstrations were collected for some experiments), but the high-performance ensemble result was achieved without real demonstration fine-tuning.",
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": "Sim-only IL failed in real (0%); adding real demonstrations or using single RetinaGAN with some real data yields ~75–77%; ensemble of multiple RetinaGANs yields ~93–97% success and can achieve high transfer even without real data.",
            "key_findings": "Object-aware pixel-level adaptation with detection consistency can preserve critical semantic structure (door frames, robot pose) across domains; ensembling multiple adapted-image generators increases visual diversity and robustness, enabling near-complete transfer of imitation-learned door-opening policies without real demonstrations.",
            "uuid": "e1812.2",
            "source_info": {
                "paper_title": "RetinaGAN: An Object-aware Approach to Sim-to-Real Transfer",
                "publication_date_yy_mm": "2020-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Unpaired image-to-image translation using cycle-consistent adversarial networks",
            "rating": 2
        },
        {
            "paper_title": "Rlcyclegan: Reinforcement learning aware simulation-to-real",
            "rating": 2
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2
        },
        {
            "paper_title": "Unsupervised pixel-level domain adaptation with generative adversarial networks",
            "rating": 2
        },
        {
            "paper_title": "Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks",
            "rating": 1
        }
    ],
    "cost": 0.014852749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>RetinaGAN: An Object-aware Approach to Sim-to-Real Transfer</h1>
<p>Daniel Ho ${ }^{1}$, Kanishka Rao ${ }^{2}$, Zhuo Xu ${ }^{3}$, Eric Jang ${ }^{2}$, Mohi Khansari ${ }^{1}$, Yunfei Bai ${ }^{1}$</p>
<h4>Abstract</h4>
<p>The success of deep reinforcement learning (RL) and imitation learning (IL) in vision-based robotic manipulation typically hinges on the expense of large scale data collection. With simulation, data to train a policy can be collected efficiently at scale, but the visual gap between sim and real makes deployment in the real world difficult. We introduce RetinaGAN, a generative adversarial network (GAN) approach to adapt simulated images to realistic ones with object-detection consistency. RetinaGAN is trained in an unsupervised manner without task loss dependencies, and preserves general object structure and texture in adapted images. We evaluate our method on three real world tasks: grasping, pushing, and door opening. RetinaGAN improves upon the performance of prior sim-to-real methods for RL-based object instance grasping and continues to be effective even in the limited data regime. When applied to a pushing task in a similar visual domain, RetinaGAN demonstrates transfer with no additional real data requirements. We also show our method bridges the visual gap for a novel door opening task using imitation learning in a new visual domain. Visit the project website at retinagan.github.io</p>
<h2>I. INTRODUCTION</h2>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. Overview of RetinaGAN pipeline. Left: Train RetinaGAN using pre-trained perception model to create a sim-to-real model. Right: Train the behavior policy model using the sim-to-real generated images. This policy can later be deployed in real.</p>
<p>Vision-based reinforcement learning and imitation learning methods incorporating deep neural network structure can express complex behaviors, and they solve robotics manipulation tasks in an end-to-end fashion [1], [2], [3]. These methods are able to generalize and scale on complicated robot manipulation tasks, though they require many hundreds of thousands of real world episodes which are costly to collect.</p>
<p>Some of this data collection effort can be mitigated by collecting these required episodes in simulation and applying sim-to-real transfer methods. Simulation provides a safe, controlled platform for policy training and development with known ground truth labels. Such simulated data can be cheaply scaled. However, directly executing such a policy in the real world typically performs poorly, even if the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>simulation configuration is carefully controlled, because of visual and physical differences between the domains known as the reality gap. In practice, we find the visual difference to be the bottleneck in our learning algorithms and focus further discussion solely on this.</p>
<p>One strategy to overcome the visual reality gap is pixellevel domain adaptation; such methods may employ generative adversarial networks to translate the synthetic images to the real world domain [4]. However, a GAN may arbitrarily change the image, including removing information necessary for a given task. More broadly for robotic manipulation, it is important to preserve scene features that directly interact with the robot, like object-level structure and textures.</p>
<p>To address this, we propose RetinaGAN, a domain adaptation technique which requires strong object semantic awareness through an object detection consistency loss. RetinaGAN involves a CycleGAN [5] that adapts simulated images to look more realistic while also resulting in consistent objects predictions. We leverage an object detector trained on both simulated and real domains to make predictions on original and translated images, and we enforce the invariant of the predictions with respect to the GAN translation.</p>
<p>RetinaGAN is a general approach to adaptation which provides reliable sim-to-real transfer for tasks in diverse visual environments (Fig. 1). In a specific scenario, we show how RetinaGAN may be reused for a novel pushing task. We evaluate the performance of our method on three real world robotics tasks and demonstrate the following:</p>
<p>1) RetinaGAN, when trained on robotic grasping data, allows for grasping RL task models that outperform prior sim-to-real methods on real world grasping by $12 \%$.
2) With limited (5-10\%) data, our method continues to work effectively for grasping, only suffering a $14 \%$ drop in performance.
3) The RetinaGAN trained with grasping data may be reused for another similar task, 3D object pushing, without any additional real data. It achieves $90 \%$ success.
4) We train RetinaGAN for a door opening imitation learning task in a drastically different environment, and we introduce an Ensemble-RetinaGAN method that adds more visual diversity to achieve $97 \%$ success rate.
5) We utilize the same pre-trained object detector in all experiments.</p>
<h2>II. Related Work</h2>
<p>To address the visual sim-to-reality gap, prior work commonly apply domain randomization and domain adaptation</p>
<p>techniques.</p>
<p>With domain randomization, a policy is trained with randomized simulation parameters and scene configurations which produce differences in visual appearance [6], [7], [8], [9], [10], [11]. The policy may learn to generalize across the parameter distribution and takes actions likely to work in all situations. Policy performance relies heavily on the kind of randomizations applied and whether they are close to or cover reality. The recently proposed method, Automatic Domain Randomization [12], automates the hyperparameter tuning process for Rubik's Cube manipulation. But, domain randomization still requires manual, task-specific selection of visual parameters like the scene, textures, and rendering.</p>
<p>Domain adaptation bridges the reality gap by directly resolving differences between the domains [13]. Images from a source domain can be modified at the pixel-level to resemble a target domain [4], [14]. Or, feature-level adaptation aligns intermediate network features between the domains [15], [16], [17]. GANs are a commonly applied method for pixellevel transfer which only require unpaired images from both domains [18], [19], [5], [20], [21]. Our method employs such pixel-level adaptation to address the sim-to-real gap.</p>
<p>Action Image [22] is another approach to bridge the sim-to-real gap through learning a domain invariant representation for the task of grasping. Our work is complementary to this work and can help to further reduce this gap.</p>
<p>Among prior work that apply semantic consistency to GAN training, CyCADA [23] introduces a pixel-level perception consistency loss (semantic segmentation) as a direct task loss, and applies the learned generator to other semantic segmentation and perception tasks. Comparatively, RetinaGAN uses object detection where labels on real data is much easier to obtain and demonstrates that feature understanding from object detection is sufficient to preserve object semantics for robotics applications.</p>
<p>Recently, RL-CycleGAN [24] extends vanilla CycleGAN [5] with an additional reinforcement learning task loss. RL-CycleGAN enforces consistency of task policy Q-values between the original and transferred images to preserve information important to a given task. RL-CycleGAN is trained jointly with the RL model and requires task-specific real world episodes collected via some preexisting policy. Comparatively, RetinaGAN works for supervised and imitation learning, as it uses object detection as a task-decoupled surrogate for object-level visual domain differences. This requires additional real-world bounding box labels, but the detector can be reused across robotics tasks. In practice, we find the RetinaGAN easier to train since the additional object detector is pre-trained and not jointly optimized.</p>
<h2>III. Preliminaries</h2>
<h3>A. Object Detection</h3>
<p>We leverage an object detection perception model to provide object awareness for the sim-to-real CycleGAN. We train the model by mixing simulated and real world datasets which contain ground-truth bounding box labels (illustrated in Fig. 2). The real world object detection dataset includes robot images collected in general robot operation; labeling granularity is based on general object type – all brands of soda will be part of the “can” class. Simulation data is generated with the PyBullet physics engine [25].</p>
<p>Object detection models are object-aware but task-agnostic, and thus, they do not require task-specific data. We use this single detection network as a multi-domain model for all tasks, and we suspect in-domain detection training data is not crucial to the success of our method. Notably, the door opening domain is very different from the perception training data domain, and we demonstrate successful transfer in Section V-C.</p>
<p>While the initial dataset required for object detection can be a significant expense, leveraging off-the-shelf models is a promising direction, especially given our experimental results with door opening. Furthermore, detection is a generally useful robot capability, so roboticists may create detection datasets for use cases beyond sim-to-real.</p>
<p>We select the EfficientDet-D1 [26] model architecture (using the same losses as RetinaNet [27]) for the object detector. EfficientDet passes an input RGB image through a backbone feedforward EfficientNet [28] architecture, and fuses features at multiple scales within a feature pyramid network. From the result, network heads predict class logit and bounding box regression targets.</p>
<h3>B. CycleGAN</h3>
<p>The RetinaGAN training process builds on top of CycleGAN [5]: an approach to learn a bidirectional mapping between unpaired datasets of images from two domains, $X$ and $Y$, with generators $G:X \rightarrow Y$ and $F: Y \rightarrow X$. These generators are trained alongside adversarial discriminators $D_{x}, D_{y}$, which classify images to the correct domain, and with the cycle consistency loss capturing $F(G(x)) \approx x, G(F(y) \approx y$ for $x \in X, y \in Y$. We can summarize the training process with the CycleGAN loss (described in detail in [5], [24]):</p>
<p>$$
\begin{aligned}
\mathscr{L}<em x="x">{\text {CycleGAN }}\left(G, F, D</em>}, D_{y}\right) &amp; =\mathscr{L<em Y="Y">{\text {GAN }}\left(G, D</em>, X, Y\right) \
&amp; +\mathscr{L}<em X="X">{\text {GAN }}\left(F, D</em>, Y, X\right) \
&amp; +\lambda_{\text {cycle }} \mathscr{L}_{\text {cycle }}(G, F)
\end{aligned}
$$</p>
<p>Algorithm 1 Summary of RetinaGAN training pipeline.
1: Given: EfficientDet, Det, trained with simulation and real robot data
2: Collect simulation $(X)$ and real $(Y)$ task episodes
3: while train $G: X \rightarrow Y$ and $F: Y \rightarrow X$ generators do
4: Iterate over batch of simulation $(x)$ and real $(y)$ data
5: Compute $G(x)=x^{\prime}, F\left(x^{\prime}\right)=x^{\prime \prime}, F(y)=y^{\prime}, G\left(y^{\prime}\right)=y^{\prime \prime}$
6: for pairs $p_{1}, p_{2}$ in $\left{\mathrm{x}, \mathrm{x}^{\prime}, \mathrm{x}^{\prime \prime}\right},\left{\mathrm{y}, \mathrm{y}^{\prime}, \mathrm{y}^{\prime \prime}\right}$ do
7: Compute $\operatorname{Det}\left(p_{1}\right) \approx \operatorname{Det}\left(p_{2}\right)$ loss, $\mathscr{L}<em 1="1">{\text {prep }}\left(p</em>\right)$
8: end for
9: Compute CycleGAN losses, $\mathscr{L}_{\text {CycleGAN }}$
10: Take optimization step using losses
11: end while}, p_{2</p>
<h2>IV. RETINAGAN</h2>
<p>RetinaGAN trains with a frozen object detector, EfficientDet, that provides object consistency loss. Once trained, the RetinaGAN model adapts simulated images for training the task policy model. Similarly to CycleGAN, we use unpaired data without labels. The overall framework is described in Algorithm 1 and illustrated in Fig. 3, and the details are described below.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 3. Diagram of RetinaGAN stages. The simulated image (top left) is transformed by the sim-to-real generator and subsequently by the real-tosim generator. The perception loss enforces consistency on object detections from each image. The same pipeline occurs for the real image branch at the bottom.</p>
<p>From CycleGAN, we have six images: sim, transferred sim, cycled sim, real, transferred real, and cycled real. Because of object invariance with respect to transfer, an oracle domain adapter would produce identical predictions between the former three images, as well as the latter three. To capture this invariance, we run inference using a pretrained and frozen EfficientDet model on each image; for
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 4. Diagram of perception consistency loss computation. An EfficientDet object detector predicts boxes and classes. Consistency of predictions between images is captured by losses similar to those in object detection training.
each of these pairs, we compute a perception consistency loss.</p>
<h2>A. Perception Consistency Loss</h2>
<p>The perception consistency loss penalizes the generator for discrepancies in object detections between translations. Given an image $I$, EfficientDet predicts a series of anchorbased bounding box regressions and class logits at several levels in its Feature Pyramid Network [29].</p>
<p>We compute the perception consistency loss $\left(\mathscr{L}_{\text {prep }}\right)$ given a pair of images similarly to the box and class losses in typical RetinaNet/EfficientDet training. However, because the Focal Loss [27], used as the class loss, assumes onehot vector ground truth labels, we propose a variation called Focal Consistency Loss (FCL) which is compatible with logit/probability labels (explained below in Section IV-B).</p>
<p>Without loss of generality, consider an image pair to be $x$ and $G(x)$. This loss can be computed with a pre-trained EfficientDet network as:</p>
<p>$$
\begin{aligned}
\operatorname{box}<em x="x">{x}, \operatorname{cls}</em>(x) \
\operatorname{box}} &amp; =\text { EfficientDet <em G_x_="G(x)">{G(x)}, \operatorname{cls}</em>(G(x)) \
\mathscr{L}} &amp; =\text { EfficientDet <em _Huber="{Huber" _text="\text">{\text {prep }}(x, G(x)) &amp; =\mathscr{L}</em>}}\left(\operatorname{box<em G_x_="G(x)">{x}, \operatorname{box}</em>\right) \
&amp; +\operatorname{FCL}\left(\operatorname{cls}<em G_x_="G(x)">{x}, \operatorname{cls}</em>\right)
\end{aligned}
$$</p>
<p>$\mathscr{L}_{\text {Huber }}$ is the Huber Loss [30] used as the box regression loss. This process is visualized in Fig. 4.</p>
<p>The Perception Consistency Loss on a batch of simulated images $x$ and real images $y$, using the sim-to-real generator $G$ and the real-to-sim generator $F$, is:</p>
<p>$$
\begin{aligned}
\mathscr{L}<em _prep="{prep" _text="\text">{\text {prep }}(x, y, F, G) &amp; =\mathscr{L}</em>}}(x, G(x))+\frac{1}{2} \mathscr{L<em _prep="{prep" _text="\text">{\text {prep }}(x, F(G(x))) \
&amp; +\frac{1}{2} \mathscr{L}</em>(G(x), F(G(x)) \
&amp; +\mathscr{L}}<em _prep="{prep" _text="\text">{\text {prep }}(y, F(y))+\frac{1}{2} \mathscr{L}</em>(y, G(F(y))) \
&amp; +\frac{1}{2} \mathscr{L}_{\text {prep }}(F(y), G(F(y))
\end{aligned}
$$}</p>
<p>We halve the losses involving the cycled $F(G(x))$ and $G(F(x))$ images because they are compared twice (against the orginal and transferred images), but find that this weight has little effect in practice.</p>
<p>We arrive at the overall RetinaGAN loss:</p>
<p>$$
\begin{aligned}
\mathscr{L}<em x="x">{\text {RetinaGAN }}\left(G, F, D</em>}, D_{y}\right) &amp; =\mathscr{L<em x="x">{\text {CycleGAN }}\left(G, F, D</em>\right) \
&amp; +\lambda_{\text {prep }} \mathscr{L}_{\text {prep }}(x, y, F, G)
\end{aligned}
$$}, D_{y</p>
<h2>B. Focal Consistency Loss (FCL)</h2>
<p>We introduce and derive a novel, interpolated version of the Focal Loss (FL) called Focal Consistency Loss (FCL), which extends support to a ground truth confidence probability $y \in[0,1]$ from a binary $y \in{0,1}$. Focal losses handle class imbalances in one-stage object detectors, improving upon Cross Entropy (CE) and Balanced Cross Entropy (BCE) losses (Section 3, [27]).</p>
<p>We begin from CE loss, which can be defined as:</p>
<p>$$
\mathrm{CE}(y, p)=y \log p-(1-y) \log (1-p)
$$</p>
<p>where $p$ is the predicted probability.
BCE loss handles class imbalance by including a weighting term $\alpha \in[0,1]$ if $y=1$ and $1-\alpha$ if $y=0$. Interpolation between these two terms yields:</p>
<p>$$
\mathrm{BCE}(y, p)=[(2 \alpha-1) p+(1-\alpha)] \mathrm{CE}(y, p)
$$</p>
<p>Focal Loss weights BCE by a focusing factor of $\left(1-p_{t}\right)^{\gamma}$, where $\gamma \geq 0$ and $p_{t}$ is $p$ if $y=0$ and $1-p$ if $y=1$ to addresses foreground-background imbalance. FCL is derived through interpolation between the binary cases of $p_{t}$ :</p>
<p>$$
\operatorname{FCL}(y, p)=|y-p|^{\gamma} \operatorname{BCE}(y, p)
$$</p>
<p>FCL is equivalent to FL when the class targets are onehot labels, but interpolates the loss for probability targets. Finally, FL is normalized by the number of anchors assigned to ground-truth boxes (Section 4, [27]). Instead, FCL is normalized by the total probability attributed to anchors in the class tensor. This weights each anchor by its inferred probability of being a ground-truth box.</p>
<h2>C. Hyperparameters</h2>
<p>We follow the hyperparameter selection of $\lambda_{\text {cycle }}=10$ from RL-CycleGAN without tuning. $\lambda_{\text {prep }}$ trades focus on object reconstruction quality with overall image realism. We find 0.1 to 1.0 to be stable, and selected 0.1 for all experiments, as objects were well-preserved at this value. We find relative weights between $\mathscr{L}_{\text {prep }}$ terms not important.</p>
<h2>V. Task Policy Models and Experiments</h2>
<p>We aim to understand the following scenarios: 1) the value of sim-to-real at various data sizes by comparing robotics models trained with RetinaGAN vs without RetinaGAN 2) with purely sim-to-real data, how models trained with various GANs perform 3) transfer to other tasks.</p>
<p>We begin with training and evaluating RetinaGAN for RL grasping. We then proceed by applying the same RetinaGAN model to RL pushing and finally re-train on an IL door opening task. See the Appendix for further details on training and model architecture.</p>
<h2>A. Reinforcement Learning: Grasping</h2>
<p>We use the distributed reinforcement learning method Q2Opt [31], an extension to QT-Opt [3], to train a vision based task model for instance grasping. In the grasping task, a robot is positioned in front of one of three bins within a trash sorting station and attempts to grasp targeted object
instances. The RGB image and a binary mask for the grasp target is input into the network. Real world object classes are focused on cups, cans, and bottles, although real training data is exposed to a long tail of discarded objects. Grasps in simulation are performed with the PyBullet [25] physics engine, with 9 to 18 spawned objects per scene. Example images are visualized in Fig. 5.</p>
<p>When using real data, we train RetinaGAN on 135,000 off-policy real grasping episodes and the Q2-Opt task model on 211,000 real episodes. We also run a low data experiment using 10,000 real episodes for training both RetinaGAN and Q2-Opt. We run distributed simulation to generate one-half to one million on-policy training episodes for RetinaGAN and one to two million for Q2-Opt.</p>
<p>We evaluate with six robots and sorting stations. Two robots are positioned in front of each of the three waste bins, and a human manually selects a cup, can, or bottle to grasp. Each evaluation includes thirty grasp attempts for each class, for ninety total. By assuming each success-failure experience is an independent Bernouili trial, we can estimate the sample standard deviation as $\sqrt{q(1-q) /(n-1)}$, where $q$ is the average failure rate and $n$ is the number of trials.</p>
<p>TABLE I
Instance grasping success mean and estimated standard deviation (est. std.) of Q2-Opt compared between different training data sources across 90 trials. Results are organized by the number of real grasping episodes used.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Grasp Success</th>
<th style="text-align: center;">Est. Std.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Sim-Only</td>
<td style="text-align: center;">$18.9 \%$</td>
<td style="text-align: center;">$4.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Randomized Sim</td>
<td style="text-align: center;">$41.1 \%$</td>
<td style="text-align: center;">$5.2 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAN: 10K Real, Q2-Opt: 10K Real</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Real</td>
<td style="text-align: center;">$22.2 \%$</td>
<td style="text-align: center;">$4.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">RetinaGAN</td>
<td style="text-align: center;">$47.4 \%$</td>
<td style="text-align: center;">$5.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">RetinaGAN+Real</td>
<td style="text-align: center;">$\mathbf{6 5 . 6 \%}$</td>
<td style="text-align: center;">$5.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAN: 135K Real, Q2-Opt: 211K Real</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Real</td>
<td style="text-align: center;">$30.0 \%$</td>
<td style="text-align: center;">$4.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Sim+Real</td>
<td style="text-align: center;">$54.4 \%$</td>
<td style="text-align: center;">$5.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">RetinaGAN+Real</td>
<td style="text-align: center;">$\mathbf{8 0 . 0 \%}$</td>
<td style="text-align: center;">$4.2 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GAN: 135K Real, Q2-Opt: 0 Real</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">CycleGAN [5]</td>
<td style="text-align: center;">$67.8 \%$</td>
<td style="text-align: center;">$5.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">RL-CycleGAN [24]</td>
<td style="text-align: center;">$68.9 \%$</td>
<td style="text-align: center;">$4.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">RetinaGAN</td>
<td style="text-align: center;">$\mathbf{8 0 . 0 \%}$</td>
<td style="text-align: center;">$4.2 \%$</td>
</tr>
</tbody>
</table>
<p>We use the RL grasping task to measure the sim-to-real gap and compare methods in the following scenarios, which are displayed in Table I:</p>
<ul>
<li>Train by mixing 10 K real episodes with simulation to gauge data efficiency in the limited data regime.</li>
<li>Train by mixing $135 \mathrm{~K}+$ real grasping episodes with simulation to investigate scalability with data, data efficiency, and performance against real data baselines.</li>
<li>Train Q2-Opt with only simulation to compare between RetinaGAN and other sim-to-real methods.
In the sim-only setup, we train with fixed light position and object textures, though we apply photometric distortions including brightness, saturation, hue, contrast, and noise.</li>
</ul>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 5. Sampled, unpaired images for the grasping task at various scales translated with either the sim-to-real (left) or real-to-sim (right) generator. Compared to other methods, the sim-to-real RetinaGAN consistently preserves object textures and better reconstructs real features. The real-to-sim RetinaGAN is able to preserve all object structure in cluttered scenes, and it correctly translates details of the robot gripper and floor.</p>
<p>In simulation evaluation, a Q2-Opt model achieves 92% instance grasping success on cups, cans, and bottles. A performance of 18.9% on the real object equivalents indicates a significant sim-to-real gap from training in simulation alone.</p>
<p>We compare against baselines in domain randomization and domain adaptation techniques. Domain randomization includes variations in texture and light positioning.</p>
<p>On the limited 10K episode dataset, RetinaGAN+Real achieves 65.6%, showing significant performance improvement compared to Real-only. When training on the large real dataset, RetinaGAN achieves 80%, demonstrating scalability with more data. Additionally, we find that RetinaGAN+Real with 10K examples outperforms Sim+Real with 135K+ episodes, showing more than 10X data efficiency.</p>
<p>We proceed to compare our method with other domain adaptation methods; here, we train Q2-Opt solely on sim-to-real translated data for a clear comparison. RL-CycleGAN is trained with the same indiscriminate grasping task loss as in [24], but used to adapt on instance grasping. This could explain its relatively lower improvement from results in [24]. RetinaGAN achieves 80%, outperforming other methods by over two standard deviations, and interestingly, is on par with RetinaGAN+Real. We hypothesize that the knowledge of the real data was largely captured during RetinaGAN training, and the near on-policy simulation data is enough to train a high performing model.</p>
<h3><em>B. Reinforcement Learning: 3D Object Pushing</em></h3>
<p>We investigate the transfer capability of RetinaGAN within the same sorting station environment by solving a 3D object pushing task. We test the same RetinaGAN model with this visually similar but distinct robotic pushing task and show that it may be reused without fine-tuning. No</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 6. Example unpaired images from the object pushing task, where the robot needs to push an upright object to the goal position, the red dot, without knocking it over.</p>
<p>additional real data is required for both the pushing task and RetinaGAN.</p>
<p>The pushing task trains purely in simulation, using a scene with a single bottle placed within the center bin of the sorting station and the same Q2-Opt RL framework (Fig. 6). Success is achieved when the object remains upright and is pushed to within 5 centimeters of the goal location indicated by a red marker. We stack the initial image (with the goal marker) and current RGB image as input. For both sim and real world evaluation, the robot needs to push a randomly placed tea bottle to a target location in the bin without knocking it over. Further details are described in [32], a concurrent submission. Evaluation results are displayed in Table II. We train a Q2-Opt policy to perform the pushing task in simulation only and achieve 90% sim success. When deploying the sim-only RL policy to real, we get 0% success, revealing a large sim-to-real gap. By applying RetinaGAN to the RL training data, we create a policy achieving 90% success, demonstrating strong transfer and understanding of the real domain.</p>
<p>TABLE II
Success rate mean and estimated standard deviation (est. std.) of pushing an upright tea bottle to goal position across 10 attempts.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Push Success</th>
<th style="text-align: center;">Est. Std.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Sim-Only</td>
<td style="text-align: center;">$0.0 \%$</td>
<td style="text-align: center;">$0.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">RetinaGAN</td>
<td style="text-align: center;">$\mathbf{9 0 . 0 \%}$</td>
<td style="text-align: center;">$10.0 \%$</td>
</tr>
</tbody>
</table>
<h2>C. Imitation Learning: Door Opening</h2>
<p>We investigate RetinaGAN with a mis-matched object detector (trained on recycling objects) on an door opening task using a supervised learning form of behavioral cloning and imitation learning (IL). This task is set in a dramatically different visual domain, policy learning framework and algorithm, and neural network architecture. It involves a fixed, extended robot arm with a policy controlling the wheels of the robot base to open the doors of, and enter, conference rooms (Fig. 7).</p>
<p>The supervised learning policy is represented by a ResNetFiLM architecture with 18 layers [33]. Both the RetinaGAN model and the supervised learning policy are trained on 1,500 human demonstrations in simulation and 29,000 human demonstrations on real conference doors. We evaluate on three conference rooms seen within the training demonstrations. We train and evaluate on three conference rooms with both left and right-swinging doors, for ten trials each and thirty total trials.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 7. Images sampled from the door opening task in simulation (red border) and real (blue border). Generated images from two separately trained RetinaGAN models highlight prediction diversity in features like lighting or background; this diversity is also present in the real world dataset.</p>
<p>With the door opening task, we explore how our domain adapation method performs in an entirely novel domain, training method, and action space, with a relatively low amount of real data. We train the RetinaGAN model using the same object detector trained on recycling objects. This demonstrates the capacity to re-use labeled robot bounding box data across environments, eliminating further human</p>
<p>TABLE III
Success rate mean and estimated standard deviation (est. std.) of door opening across 30 trials. RetinaGAN+Real result was selected from best of three models used in Multi-RetinaGAN+Real.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Seen Doors</th>
<th style="text-align: center;">Est. Std.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Sim-only</td>
<td style="text-align: center;">$0.0 \%$</td>
<td style="text-align: center;">$0.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Real</td>
<td style="text-align: center;">$36.6 \%$</td>
<td style="text-align: center;">$8.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Sim+Real</td>
<td style="text-align: center;">$75.0 \%$</td>
<td style="text-align: center;">$8.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">RetinaGAN+Real</td>
<td style="text-align: center;">$76.7 \%$</td>
<td style="text-align: center;">$7.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Ensemble-RetinaGAN+Real</td>
<td style="text-align: center;">$93.3 \%$</td>
<td style="text-align: center;">$4.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Ensemble-RetinaGAN</td>
<td style="text-align: center;">$\mathbf{9 6 . 6 \%}$</td>
<td style="text-align: center;">$3.4 \%$</td>
</tr>
</tbody>
</table>
<p>labeling effort. Within door opening images, the perception model produces confident detections only for the the robot arm, but we hypothesize that structures like door frames could be maintained by consistency in low-probability prediction regimes.</p>
<p>Compared to baselines without consistency loss, RetinaGAN strongly preserves room structures and door locations, while baseline methods lose this consistency (see Appendix). This semantic inconsistency in GAN baselines presents a safety risk in real world deployment, so we did not attempt evaluations with these models.</p>
<p>We then evaluate IL models trained with different data sources and domain adaptors, and displayed the results in Table III. An IL model trained on demonstrations in simulation and evaluated in simulation achieves $98 \%$ success. The same model fails in real with no success cases - showing a large sim-to-real gap.</p>
<p>By mixing real world demonstrations in IL model training, we achieve $75 \%$ success on conference room doors seen in training time. We achieve a comparable success rate, $76.7 \%$, when applying RetinaGAN.</p>
<p>By training on data from three separate RetinaGAN models with different random seeds and consistency loss weights (called Ensemble-RetinaGAN), we are able to achieve $93.3 \%$ success rate. In the low data regime, RetinaGAN can oscillate between various reconstructed semantics and ambiguity in lighting and colors as shown in Fig. 7. We hypothesize that mixing data from multiple GANs adds diversity and robustness, aiding in generalization. Finally, we attempt Ensemble-RetinaGAN without any real data for training the IL model. We achieve $96.6 \%$, within margin of error of the Ensemble-RetinaGAN+Real result.</p>
<h2>VI. CONCLUSIONS</h2>
<p>RetinaGAN is an object-aware sim-to-real adaptation technique which transfers robustly across environments and tasks, even with limited real data. We evaluate on three tasks and show $80 \%$ success on instance grasping, a 12 percentagepoint improvement upon baselines. Further extensions may look into pixel-level perception consistency or other modalities like depth. Another direction of work in task and domainagnostic transfer could extend RetinaGAN to perform well in a visual environment unseen at training time.</p>
<h2>APPENDIX</h2>
<h2>A. Alternative Perception Losses</h2>
<p>We note that it is also possible to train separate perception networks for each domain. However, this adds complexity and requires that the object sets between synthetic and real data be close to bijective, because both models would have to produce consistent predictions on perfectly paired images.</p>
<p>Providing perception consistency with off-the-shelf, pretrained models is a promising future direction that eliminates the costs of perception model creation. Future work may investigate whether such models can be successfully leveraged to train RetinaGAN. As they are likely trained solely on real data, the relatively unbalanced predictions between the sim and real domains may destablize training.</p>
<p>While segmentation models like Mask-RCNN [34] and ShapeMask [35] provide dense, pixel-level object supervision, it is practically easier and more efficient to label object detection data. However, it may provide a stronger supervision signal, and semantic segmentation models may provide stronger consistency for background objects and structures.</p>
<h2>B. Door Opening Figure</h2>
<p>See Fig. 8 for example of semantic structure distortions when training the door opening task with CycleGAN.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 8. CycleGAN can distort semantic structure when trained on door opening images, in the low data regime. Images on the right are transfered results of the simulated image on the left.</p>
<h2>C. Perception Model Training</h2>
<p>Hyperparameters used in object detection model training are listed in Table IV. We use default augmentation parameters from [27], including a scale range of 0.8 to 1.2. Among the 59 classes, the following are frequently used:
robot, bottle, bowl, can, cup, bag/wrapper, bowl, and plate. Other classes appear sparesely or not at all.</p>
<p>TABLE IV
Hyperparameters used for EfficientDet Training.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: right;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Training Hardware</td>
<td style="text-align: right;">4 x Google TPUv3 Pods</td>
</tr>
<tr>
<td style="text-align: left;">Network Architecture</td>
<td style="text-align: right;">EfficientNet-D1 [26]</td>
</tr>
<tr>
<td style="text-align: left;">Precision</td>
<td style="text-align: right;">bfloat16</td>
</tr>
<tr>
<td style="text-align: left;">Input Resolution</td>
<td style="text-align: right;">512x640 pixels</td>
</tr>
<tr>
<td style="text-align: left;">Preprocessing</td>
<td style="text-align: right;">Crop, scale, Horizontal flipping</td>
</tr>
<tr>
<td style="text-align: left;">Training Step Count</td>
<td style="text-align: right;">Pad to $640 \times 640$</td>
</tr>
<tr>
<td style="text-align: left;">Optimizer</td>
<td style="text-align: right;">90,000</td>
</tr>
<tr>
<td style="text-align: left;">Learning Rate</td>
<td style="text-align: right;">tf.train.MomentumOptimizier</td>
</tr>
<tr>
<td style="text-align: left;">Momentum</td>
<td style="text-align: right;">0.08 , stepped two times with $10 \%$ decay</td>
</tr>
<tr>
<td style="text-align: left;">Batch Size</td>
<td style="text-align: right;">0.08</td>
</tr>
<tr>
<td style="text-align: left;">Weight Decay</td>
<td style="text-align: right;">256</td>
</tr>
<tr>
<td style="text-align: left;">Classes</td>
<td style="text-align: right;">$1 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;">59</td>
</tr>
</tbody>
</table>
<h2>D. RetinaGAN Model Training</h2>
<p>We train RetinaGAN following the hyper-parameters described in Appendix A of [24]. We did not tune any CycleGAN hyper-parameters, and we primarily searched between 0.1 and 1 for $\mathscr{L}<em _prep="{prep" _text="\text">{\text {prep }}$. We did not run any hyper-parameter search on relative weights between $\mathscr{L}</em>$.}}$ terms. We generate simulation images with the following object set (and counts): paper bags (1), bottles (9), bowls (1), napkins (1), cans (12), cups (6), containers (2), plates (1), and wrappers (10). Each training batch includes 256 simulation and 256 real images. Photometric distortions are defined in the Tensor2Robot framework ${ }^{1</p>
<p>TABLE V
Hyperparameters used for GAN Training.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: right;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Training Hardware</td>
<td style="text-align: right;">4 x Google TPUv3 Pods</td>
</tr>
<tr>
<td style="text-align: left;">Network Architecture</td>
<td style="text-align: right;">U-Net [36], Fig. 5 in [24]</td>
</tr>
<tr>
<td style="text-align: left;">Precision</td>
<td style="text-align: right;">bfloat16</td>
</tr>
<tr>
<td style="text-align: left;">Input Resolution</td>
<td style="text-align: right;">512x640 pixels</td>
</tr>
<tr>
<td style="text-align: left;">Preprocessing</td>
<td style="text-align: right;">Crop to 472x472 pixels</td>
</tr>
<tr>
<td style="text-align: left;">Training Step Count</td>
<td style="text-align: right;">Apply photometric distortions</td>
</tr>
<tr>
<td style="text-align: left;">Optimizer</td>
<td style="text-align: right;">$50,000-100,000$</td>
</tr>
<tr>
<td style="text-align: left;">Learning Rate</td>
<td style="text-align: right;">tf.train.AdamOptimizer</td>
</tr>
<tr>
<td style="text-align: left;">Batch Size</td>
<td style="text-align: right;">$\beta_{1}=0.1, \beta_{2}=0.999$</td>
</tr>
<tr>
<td style="text-align: left;">Weight Decay</td>
<td style="text-align: right;">0.0001</td>
</tr>
<tr>
<td style="text-align: left;">Additional Normalization</td>
<td style="text-align: right;">512</td>
</tr>
<tr>
<td style="text-align: left;">$\mathscr{L}_{\text {GAN }}$</td>
<td style="text-align: right;">$7 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: left;">$\mathscr{L}_{\text {cycle }}$</td>
<td style="text-align: right;">Spectral Normalization [37]</td>
</tr>
<tr>
<td style="text-align: left;">$\mathscr{L}_{\text {prep }}$</td>
<td style="text-align: right;">1 weight $\left(\lambda_{\text {GAN }}\right)$, updates $G, F, D_{v}, D_{y}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;">10 weight $\left(\lambda_{\text {cycle }}\right)$, updates $G, F$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;">0.1 weight $\left(\lambda_{\text {prep }}\right)$, updates $G, F$</td>
</tr>
</tbody>
</table>
<h2>E. Q2-Opt RL Model Training</h2>
<p>We use the Q2R-Opt [31] model and training pipeline for both the grasping and pushing tasks. We use the same hyperparameters as in this prior work, without any tuning. We train on the same simulated object set as in the RetinaGAN setup.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>When using the full real dataset, we sample each minibatch from simulation episodes with a 50% weight and real episodes with a 50% weight. With the restricted 10K episode dataset, we sample from simulation with 20% weight and real with 80% weight, as to not overfit on the smaller real dataset. We did not tune these ratios, as in prior experiments, we found that careful tuning was not required.</p>
<h2>F. ResNet-FiLM IL Model Training</h2>
<p>We train IL with the ResNet-FiLM [33] model with a ResNet-18 architecture defined in the Tensor2Robot framework${ }^{2}$. For training RetinaGAN and Multi-RetinaGAN, we mix real demonstrations, simulated demonstrations, and RetinaGAN-adapted simulated demonstrations. We use a lower 20% weight for real data (because of the small dataset size) and evenly weight simulated and adapted demonstrations. The action space is the 2D movement of the robot base. Additional details will be provided in an as-yet unreleased paper; this work focuses on the benefits of CycleGANadapted data independently of whether policies are trained with IL or RL. We used the same hyper-parameters for all experiments.</p>
<h2>G. Evaluation</h2>
<p>For grasping, we evaluate with the station setup in Fig. 9. Each setup is replicated three times (with potentially different object brands/instances, but the same classes), and one robot positioned in front of each bin. We target the robot to only grasp the cup, can, and bottle, for a total of eighteen grasps. This is repeated five times for ninety total grasps.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 9. The two evaluation station setups displaying the object classes present in each bin.</p>
<p>For pushing, we evaluate with a single Ito En Green Tea Bottle filled 25\% full of water.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>For door opening, we evaluate on three real world conference room doors. Two doors swing rightwards and one door swings leftwards. The episode is judged as successful if the robot autonomously pushes the door open and the robot base enters the room.</p>
<h2>ACKNOWLEDGEMENTS</h2>
<p>We thank Noah Brown, Christopher Paguyo, Armando Fuentes, and Sphurti More for overseeing robot operations, and Daniel Kappler, Paul Wohlhart, and Alexander Herzog for helpful discussions. We thank Chris Harris and Alex Irpan for comments on the manuscript.</p>
<h2>REFERENCES</h2>
<p>[1] S. Levine, P. Pastor, A. Krizhevsky, J. Ibarz, and D. Quillen, "Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection," The International Journal of Robotics Research, vol. 37, no. 4-5, pp. 421-436, 2018.
[2] L. Pinto and A. Gupta, "Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours," in 2016 IEEE international conference on robotics and automation (ICRA). IEEE, 2016, pp. 3406-3413.
[3] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly, M. Kalakrishnan, V. Vanhoucke, and S. Levine, "Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation," 2018.
[4] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Krishnan, "Unsupervised pixel-level domain adaptation with generative adversarial networks," 2017.
[5] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, "Unpaired image-to-image translation using cycle-consistent adversarial networks," 2020.
[6] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, "Domain randomization for transferring deep neural networks from simulation to the real world," in 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2017, pp. 23-30.
[7] J. Matas, S. James, and A. J. Davison, "Sim-to-real reinforcement learning for deformable object manipulation," 2018.
[8] S. James, A. Davison, and E. Johns, "Transferring end-to-end visuomotor control from simulation to real world for a multi-stage task," in CoRL, 2017.
[9] F. Sadeghi, A. Toshev, E. Jang, and S. Levine, "Sim2real viewpoint invariant visual servoing by recurrent control," in 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018, pp. 4691-4699.
[10] X. Yan, M. Khansari, J. Hsu, Y. Gong, Y. Bai, S. Pirk, and H. Lee, "Data-efficient learning for sim-to-real robotic grasping using deep point cloud prediction networks," arXiv preprint arXiv:1906.08989, 2019.
[11] X. Yan, J. Hsu, M. Khansari, Y. Bai, A. Pathak, A. Gupta, J. Davidson, and H. Lee, "Learning 6-dof grasping interaction via deep 3d geometry-aware representations," 2018. [Online]. Available: https://arxiv.org/pdf/1708.07303.pdf
[12] I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A. Paino, M. Plappert, G. Powell, R. Ribas et al., "Solving rubik's cube with a robot hand," arXiv preprint arXiv:1910.07113, 2019.
[13] V. M. Patel, R. Gopalan, R. Li, and R. Chellappa, "Visual domain adaptation: A survey of recent advances," IEEE Signal Processing Magazine, vol. 32, no. 3, pp. 53-69, 2015.
[14] D. Yoo, N. Kim, S. Park, A. S. Paek, and I. S. Kweon, "Pixel-level domain transfer," 2016.
[15] R. Gopalan, Ruonan Li, and R. Chellappa, "Domain adaptation for object recognition: An unsupervised approach," in 2011 International Conference on Computer Vision, 2011, pp. 999-1006.
[16] M. Long, Y. Cao, J. Wang, and M. Jordan, "Learning transferable features with deep adaptation networks," ser. Proceedings of Machine Learning Research, F. Bach and D. Blei, Eds., vol. 37. Lille, France: PMLR, 07-09 Jul 2015, pp. 97-105. [Online]. Available: http://proceedings.mlr.press/v37/long15.html
[17] K. Fang, Y. Bai, S. Hinterstoisser, S. Savarese, and M. Kalakrishnan, "Multi-task domain adaptation for deep learning of instance grasping from simulation," in 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2018, pp. 3516-3523.
[18] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. WardeFarley, S. Ozair, A. Courville, and Y. Bengio, "Generative adversarial nets," in Advances in Neural Information Processing Systems 27, Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, Eds. Curran Associates, Inc., 2014, pp. 2672-2680. [Online]. Available: http://papers.nips.cc/paper/ 5423-generative-adversarial-nets.pdf
[19] A. Brock, J. Donahue, and K. Simonyan, "Large scale gan training for high fidelity natural image synthesis," 2019.
[20] K. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kelcey, M. Kalakrishnan, L. Downs, J. Ibarz, P. P. Sampedro, K. Konolige, S. Levine, and V. Vanhoucke, "Using simulation and domain adaptation to improve efficiency of deep robotic grasping," 2018. [Online]. Available: https://arxiv.org/abs/1709.07857
[21] S. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz, S. Levine, R. Hadsell, and K. Bousmalis, "Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks," in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 12 619-12 629.
[22] M. Khansari, D. Kappler, J. Luo, J. Bingham, and M. Kalakrishnan, "Action image representation: Learning scalable deep grasping policies with zero real world data," 2020.
[23] J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko, A. A. Efros, and T. Darrell, "Cycada: Cycle-consistent adversarial domain adaptation," 2017.
[24] K. Rao, C. Harris, A. Irpan, S. Levine, J. Ibarz, and M. Khansari, "Rlcyclegan: Reinforcement learning aware simulation-to-real," 2020.
[25] E. Coumans and Y. Bai, "Pybullet, a python module for physics simulation in robotics, games and machine learning," 2017.
[26] M. Tan, R. Pang, and Q. V. Le, "Efficientdet: Scalable and efficient object detection," in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 10 778-10 787.
[27] T. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, "Focal loss for dense object detection," in 2017 IEEE International Conference on Computer Vision (ICCV), 2017, pp. 2999-3007.
[28] M. Tan and Q. V. Le, "Efficientnet: Rethinking model scaling for convolutional neural networks," 2020.
[29] T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie, "Feature pyramid networks for object detection," 2017.
[30] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning, ser. Springer Series in Statistics. New York, NY, USA: Springer New York Inc., 2001.
[31] C. Bodnar, A. Li, K. Hausman, P. Pastor, and M. Kalakrishnan, "Quantile qt-opt for risk-aware vision-based robotic grasping," ArXiv, vol. abs/1910.02787, 2019.
[32] Z. Xu, W. Yu, A. Herzog, W. Lu, C. Fu, M. Tomizuka, Y. Bai, C. K. Liu, and D. Ho, "Cocoi: Contact-aware online context inference for generalizable non-planar pushing," 2020.
[33] E. Perez, F. Strub, H. de Vries, V. Dumoulin, and A. Courville, "Film: Visual reasoning with a general conditioning layer," 2017.
[34] K. He, G. Gkioxari, P. Dollár, and R. Girshick, "Mask r-cnn," 2018.
[35] W. Kuo, A. Angelova, J. Malik, and T.-Y. Lin, "Shapemask: Learning to segment novel objects by refining shape priors," in Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 9207-9210.
[36] O. Ronneberger, P. Fischer, and T. Brox, "U-net: Convolutional networks for biomedical image segmentation," 2015.
[37] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena, "Self-attention generative adversarial networks," 2019.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://github.com/google-research/tensor2robot/ blob/master/layers/film_resnet_model.py&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
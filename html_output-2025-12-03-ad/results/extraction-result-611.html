<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-611 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-611</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-611</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-79baf8cf6be6510f69be8c515516136138678cf5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/79baf8cf6be6510f69be8c515516136138678cf5" target="_blank">The More You Know: Using Knowledge Graphs for Image Classification</a></p>
                <p><strong>Paper Venue:</strong> Computer Vision and Pattern Recognition</p>
                <p><strong>Paper TL;DR:</strong> This paper investigates the use of structured prior knowledge in the form of knowledge graphs and shows that using this knowledge improves performance on image classification, and introduces the Graph Search Neural Network as a way of efficiently incorporating large knowledge graphs into a vision classification pipeline.</p>
                <p><strong>Paper Abstract:</strong> One characteristic that sets humans apart from modern learning-based computer vision algorithms is the ability to acquire knowledge about the world and use that knowledge to reason about the visual world. Humans can learn about the characteristics of objects and the relationships that occur between them to learn a large variety of visual concepts, often with few examples. This paper investigates the use of structured prior knowledge in the form of knowledge graphs and shows that using this knowledge improves performance on image classification. We build on recent work on end-to-end learning on graphs, introducing the Graph Search Neural Network as a way of efficiently incorporating large knowledge graphs into a vision classification pipeline. We show in a number of experiments that our method outperforms standard neural network baselines for multi-label classification.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e611.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e611.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GSNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Search Neural Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scalable hybrid system that integrates large symbolic knowledge graphs with neural graph propagation and image feature networks to improve multi-label image classification by selectively expanding a relevant subset of the graph and learning propagation, importance scoring, and output prediction end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Graph Search Neural Network (GSNN)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>GSNN is an end-to-end hybrid architecture for vision that combines a declarative knowledge graph (nodes = visual concepts; edges = object-object and object-attribute relations from Visual Genome and optionally WordNet) with imperative neural modules. The system (1) initializes a small active subgraph from detector/classifier confidences (Faster R-CNN over COCO categories), (2) runs a gated propagation network (GGNN-style gated recurrent updates) on the active nodes, (3) uses a learned per-node importance network to iteratively expand the active subgraph by adding top-scoring neighbors, and (4) produces per-node outputs that are concatenated with VGG fc7 features and detector scores to form the final classification input. The propagation net, importance net, and output net are trained jointly (except the pre-trained detectors). Node-specific bias vectors are learned to capture node identity.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Knowledge graphs representing visual concepts and relations: Visual Genome scene graphs (object-object and object-attribute relations) and optionally fused WordNet semantic edges; represented as nodes and typed/directed edges and encoded via adjacency matrices (A) used to retrieve neighboring node states.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural networks: (a) a GGNN-style propagation network with gated recurrent updates (GRU-like z, r gates, tanh updates) operating over node hidden states; (b) a learned importance network g_i(h_v, x_v) that scores nodes for expansion; (c) a per-node output network g(h_v(T), x_v, n_v) that produces node outputs; (d) image feature networks: Faster R-CNN detectors (pretrained) for initial annotations and a VGG-16 fc7 branch whose features are concatenated into final classifier. Training uses gradient-based optimization (ADAM/SGD) end-to-end for GSNN modules.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular, differentiable integration with end-to-end training: the symbolic knowledge graph provides structure (adjacency matrices and node identities); neural modules propagate and transform annotations over that structure via differentiable gated updates; the importance network selects graph expansion steps during forward passes (the selection is learned via predicted importance scores and top-P expansion policy); outputs from graph modules are concatenated with standard CNN features and backpropagated through both output and importance losses (importance targets are assigned based on graph distance to ground-truth nodes). Node-bias tables tie learned parameters to specific symbolic node identities. The system uses selective, learned graph expansion to keep computation tractable.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>1) Improved multi-label classification accuracy over neural-only baselines (better use of relational context); 2) Data-efficiency gains in low-data regimes (improvements observed down to ~2,000 training examples); 3) Multi-hop relational inference: the system can reinforce weak detections via multi-step graph connections (e.g., reasoning that 'jeans' are likely because 'person' with wearing edges); 4) Explainability: ability to trace sensitivities through hidden states and detections to identify which nodes/edges influenced a prediction; 5) Scalability via learned selective expansion (reduces computational cost relative to full-graph GGNN). These behaviors arise from combining explicit symbolic relations with learned propagation and selection mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Multi-label image classification on (1) Visual Genome Multi-Label subset (VGML) with 316 visual concepts and (2) MS COCO multi-label (minival).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Visual Genome (VGML) mAP: VGG baseline 30.57; VGG+Det 31.4; GSNN using Visual Genome graph 32.83; GSNN using combined Visual Genome+WordNet graph 33.00 (reported as 33). COCO mAP: VGG baseline 69.86; VGG+Det 73.93; GSNN-VG 77.57; GSNN-VG+WN 75.73. (metric: mean Average Precision (mAP), percentage points).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td>Visual Genome (VGML): VGG baseline mAP 30.57; VGG+Det mAP 31.4. COCO: VGG baseline mAP 69.86; VGG+Det mAP 73.93.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>GSNN shows improved generalization over purely neural baselines in the tested settings: (a) helps in low-data regimes down to ~2,000 examples (performance advantage diminishes below ~1,000 examples), (b) transfers relational knowledge to improve performance on COCO even when the graph is built from Visual Genome, and (c) is robust to partial detector sets (ablation with only even/odd COCO detectors still showed GSNN improvements). No formal OOD or compositional generalization proofs are provided; observed gains are empirical and depend on graph relevance and detector coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High: GSNN supports sensitivity analysis by backpropagating partial derivatives of outputs w.r.t. hidden states and detections, enabling identification of which nodes and propagation steps contributed to a decision. The learned importance scores and the explicit expansion sequence provide a traceable explanation path through symbolic nodes and edges.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Dependence on graph quality and coverage (missing edges or relations prevent useful reasoning, e.g., missing 'bus-windshield' relation), dependence on initial detector confidences (though some robustness shown), hyperparameters for expansion (thresholds, P, T) matter, fails when training data is too small (<~1,000 examples) because many classes have too few examples, and GSNN still requires pre-trained detectors (not fully end-to-end from pixels). Computational complexity is reduced but still depends on expansion policy; the paper notes GGNN is infeasible on very large graphs without GSNN's selection.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>No formal theoretical proof; authors provide an intuitive complementary-strengths argument: declarative graphs encode structured priors/relations while neural propagation learns how to propagate and combine evidence over that structure; training jointly allows the system to learn propagation and importance policies specific to visual tasks. The framework is pragmatic/architectural rather than a formal theory.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The More You Know: Using Knowledge Graphs for Image Classification', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e611.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e611.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GGNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gated Graph Sequence Neural Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural architecture that performs learned recurrent gated propagation over arbitrary input graphs, producing per-node or global outputs by iteratively updating node hidden states using information from adjacent nodes via learned gated update rules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gated graph sequence neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Gated Graph Sequence Neural Network (GGNN)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>GGNN is a neural message-passing model that represents each node with a hidden state and updates node states over T time-steps using neighbor aggregation via an adjacency-encoded linear combination followed by gated recurrent (GRU-like) updates (reset and update gates). It takes node annotations as initialization and outputs per-node or graph-level predictions via learned output networks.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Graph structure (nodes, typed/directed edges) encoded into adjacency matrices A (retrieved per-node) representing symbolic relational structure provided as input (e.g., program graphs, molecule graphs).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural gated recurrent propagation using learned weight matrices (W, U) and gating functions (z, r), with nonlinearities (tanh, sigmoid); trained via gradient descent in an end-to-end fashion.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>The graph structure is used as an input wiring for a fully differentiable neural propagation model. The model conditions on symbolic adjacency and node annotations but learns propagation and output mappings by gradient-based training.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Ability to learn algorithmic-like relational reasoning over graph structures (e.g., reachability, program verification tasks) and produce task-specific per-node or sequence outputs through learned message passing instead of hand-crafted rules.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Described in the paper as applicable to logical tasks (reachability) and program verification; in this paper GGNN is cited as prior art but not re-evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>GGNNs have been shown (in the original work) to generalize to some graph reasoning tasks (e.g., reachability) by learning propagation rules, but no formal guarantees are provided in this paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Limited: GGNN is a learned neural propagator over a symbolic graph; interpretation requires analysis (e.g., hidden state inspection) but no native symbolic explanations. The current paper leverages GGNN-like structure within GSNN and augments interpretability via sensitivity analyses and explicit importance scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Scalability: naive GGNN forward/backward passes scale poorly with large numbers of nodes (O(N^2) forward, O(N^T) backward according to the paper), making direct use on knowledge graphs with thousands/millions of nodes infeasible without selection/pruning strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>GGNN is framed as a learned message-passing recurrent neural network over graphs; theoretical discussion centers on analogy to recurrent networks and learned propagation rather than formal symbolic-theorem guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The More You Know: Using Knowledge Graphs for Image Classification', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e611.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e611.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NEIL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NEIL (Extracting visual knowledge from web data)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system for extracting a large set of visual concepts and relations from web-scale data to construct a visual knowledge base/graph (cited as an example large, noisy knowledge source used in vision tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neil: Extracting visual knowledge from web data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>NEIL</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>NEIL is a pipeline for mining visual concepts and relationships from web images and text to build a large-scale visual knowledge base containing many concepts (~2000 cited) and their relationships; it is presented in this paper as an example of a large knowledge graph resource that motivates the need for scalable graph reasoning techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>A large-scale visual knowledge graph (concept nodes and relation edges) learned/collected from web data; stored as symbolic beliefs/facts.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Computer vision classifiers/detectors and extraction algorithms used to ground visual concepts into the knowledge base; NEIL also uses iterative learning/pruning heuristics (procedural components) to improve the KB.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Implicit integration: NEIL couples symbolic fact collection with procedural vision models and heuristics to construct the graph; the paper cites NEIL to motivate GSNN's need to handle large, noisy graphs, but does not detail end-to-end differentiable integration.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Provides broad coverage of visual concepts and relations to support downstream reasoning tasks, but the paper cites NEIL mainly to highlight scale/noise challenges rather than specific emergent behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Knowledge extraction from web data / building visual knowledge bases; used as a knowledge source for downstream vision tasks in literature.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>NEIL aims to generalize by harvesting diverse web data, but no specific generalization metrics are reported within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>As a KB, NEIL's contents are symbolic and inspectable; the paper does not report interpretability analyses for NEIL itself.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>NEIL/large KBs are noisy and large; the paper emphasizes that direct use of very large knowledge graphs (e.g., NEIL, NELL) is computationally challenging for naive neural graph methods and motivates GSNN's selective expansion.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>No formal theoretical framework included here; NEIL is presented as an empirical, large-scale KB construction system.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The More You Know: Using Knowledge Graphs for Image Classification', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e611.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e611.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NELL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Never-Ending Language Learning (NELL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A long-running system that continuously reads the web to populate and expand a knowledge base of beliefs (cited here as an example of very large knowledge sources that pose scalability challenges).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Toward an architecture for neverending language learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Never-Ending Language Learning (NELL)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>NELL is an architecture for continuously extracting and consolidating beliefs from web text into a large knowledge base; cited in this paper as an example knowledge source (millions of beliefs) whose scale motivates efficient selective graph-based reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>A large symbolic knowledge base containing millions of beliefs/facts collected over continuous web reading.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Procedural extraction, pattern learning, and iterative learning heuristics to expand and refine the KB; not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>NELL couples extraction algorithms with symbolic belief storage; this paper cites it to illustrate scale/noise rather than as an integrated neural-symbolic model.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Long-term accumulation of beliefs and continual improvement of KB coverage; not analyzed in detail here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Large-scale knowledge extraction and KB construction.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Designed for continual learning across diverse web data; specifics not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>KB contents are symbolic and inspectable, but paper does not discuss interpretability for NELL.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Scale and noise in large KBs are noted as challenges that make naive neural propagation over entire KBs infeasible; motivates selective expansion in GSNN.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>NELL is an engineering system with iterative learning heuristics; no unified formal theory is provided in this paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The More You Know: Using Knowledge Graphs for Image Classification', 'publication_date_yy_mm': '2016-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gated graph sequence neural networks <em>(Rating: 2)</em></li>
                <li>Neil: Extracting visual knowledge from web data <em>(Rating: 2)</em></li>
                <li>Toward an architecture for neverending language learning <em>(Rating: 2)</em></li>
                <li>Building a large-scale multimodal knowledge base system for answering visual queries <em>(Rating: 1)</em></li>
                <li>Reasoning about Object Affordances in a Knowledge Base Representation <em>(Rating: 1)</em></li>
                <li>The graph neural network model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-611",
    "paper_id": "paper-79baf8cf6be6510f69be8c515516136138678cf5",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "GSNN",
            "name_full": "Graph Search Neural Network",
            "brief_description": "A scalable hybrid system that integrates large symbolic knowledge graphs with neural graph propagation and image feature networks to improve multi-label image classification by selectively expanding a relevant subset of the graph and learning propagation, importance scoring, and output prediction end-to-end.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Graph Search Neural Network (GSNN)",
            "system_description": "GSNN is an end-to-end hybrid architecture for vision that combines a declarative knowledge graph (nodes = visual concepts; edges = object-object and object-attribute relations from Visual Genome and optionally WordNet) with imperative neural modules. The system (1) initializes a small active subgraph from detector/classifier confidences (Faster R-CNN over COCO categories), (2) runs a gated propagation network (GGNN-style gated recurrent updates) on the active nodes, (3) uses a learned per-node importance network to iteratively expand the active subgraph by adding top-scoring neighbors, and (4) produces per-node outputs that are concatenated with VGG fc7 features and detector scores to form the final classification input. The propagation net, importance net, and output net are trained jointly (except the pre-trained detectors). Node-specific bias vectors are learned to capture node identity.",
            "declarative_component": "Knowledge graphs representing visual concepts and relations: Visual Genome scene graphs (object-object and object-attribute relations) and optionally fused WordNet semantic edges; represented as nodes and typed/directed edges and encoded via adjacency matrices (A) used to retrieve neighboring node states.",
            "imperative_component": "Neural networks: (a) a GGNN-style propagation network with gated recurrent updates (GRU-like z, r gates, tanh updates) operating over node hidden states; (b) a learned importance network g_i(h_v, x_v) that scores nodes for expansion; (c) a per-node output network g(h_v(T), x_v, n_v) that produces node outputs; (d) image feature networks: Faster R-CNN detectors (pretrained) for initial annotations and a VGG-16 fc7 branch whose features are concatenated into final classifier. Training uses gradient-based optimization (ADAM/SGD) end-to-end for GSNN modules.",
            "integration_method": "Modular, differentiable integration with end-to-end training: the symbolic knowledge graph provides structure (adjacency matrices and node identities); neural modules propagate and transform annotations over that structure via differentiable gated updates; the importance network selects graph expansion steps during forward passes (the selection is learned via predicted importance scores and top-P expansion policy); outputs from graph modules are concatenated with standard CNN features and backpropagated through both output and importance losses (importance targets are assigned based on graph distance to ground-truth nodes). Node-bias tables tie learned parameters to specific symbolic node identities. The system uses selective, learned graph expansion to keep computation tractable.",
            "emergent_properties": "1) Improved multi-label classification accuracy over neural-only baselines (better use of relational context); 2) Data-efficiency gains in low-data regimes (improvements observed down to ~2,000 training examples); 3) Multi-hop relational inference: the system can reinforce weak detections via multi-step graph connections (e.g., reasoning that 'jeans' are likely because 'person' with wearing edges); 4) Explainability: ability to trace sensitivities through hidden states and detections to identify which nodes/edges influenced a prediction; 5) Scalability via learned selective expansion (reduces computational cost relative to full-graph GGNN). These behaviors arise from combining explicit symbolic relations with learned propagation and selection mechanisms.",
            "task_or_benchmark": "Multi-label image classification on (1) Visual Genome Multi-Label subset (VGML) with 316 visual concepts and (2) MS COCO multi-label (minival).",
            "hybrid_performance": "Visual Genome (VGML) mAP: VGG baseline 30.57; VGG+Det 31.4; GSNN using Visual Genome graph 32.83; GSNN using combined Visual Genome+WordNet graph 33.00 (reported as 33). COCO mAP: VGG baseline 69.86; VGG+Det 73.93; GSNN-VG 77.57; GSNN-VG+WN 75.73. (metric: mean Average Precision (mAP), percentage points).",
            "declarative_only_performance": null,
            "imperative_only_performance": "Visual Genome (VGML): VGG baseline mAP 30.57; VGG+Det mAP 31.4. COCO: VGG baseline mAP 69.86; VGG+Det mAP 73.93.",
            "has_comparative_results": true,
            "generalization_properties": "GSNN shows improved generalization over purely neural baselines in the tested settings: (a) helps in low-data regimes down to ~2,000 examples (performance advantage diminishes below ~1,000 examples), (b) transfers relational knowledge to improve performance on COCO even when the graph is built from Visual Genome, and (c) is robust to partial detector sets (ablation with only even/odd COCO detectors still showed GSNN improvements). No formal OOD or compositional generalization proofs are provided; observed gains are empirical and depend on graph relevance and detector coverage.",
            "interpretability_properties": "High: GSNN supports sensitivity analysis by backpropagating partial derivatives of outputs w.r.t. hidden states and detections, enabling identification of which nodes and propagation steps contributed to a decision. The learned importance scores and the explicit expansion sequence provide a traceable explanation path through symbolic nodes and edges.",
            "limitations_or_failures": "Dependence on graph quality and coverage (missing edges or relations prevent useful reasoning, e.g., missing 'bus-windshield' relation), dependence on initial detector confidences (though some robustness shown), hyperparameters for expansion (thresholds, P, T) matter, fails when training data is too small (&lt;~1,000 examples) because many classes have too few examples, and GSNN still requires pre-trained detectors (not fully end-to-end from pixels). Computational complexity is reduced but still depends on expansion policy; the paper notes GGNN is infeasible on very large graphs without GSNN's selection.",
            "theoretical_framework": "No formal theoretical proof; authors provide an intuitive complementary-strengths argument: declarative graphs encode structured priors/relations while neural propagation learns how to propagate and combine evidence over that structure; training jointly allows the system to learn propagation and importance policies specific to visual tasks. The framework is pragmatic/architectural rather than a formal theory.",
            "uuid": "e611.0",
            "source_info": {
                "paper_title": "The More You Know: Using Knowledge Graphs for Image Classification",
                "publication_date_yy_mm": "2016-12"
            }
        },
        {
            "name_short": "GGNN",
            "name_full": "Gated Graph Sequence Neural Network",
            "brief_description": "A neural architecture that performs learned recurrent gated propagation over arbitrary input graphs, producing per-node or global outputs by iteratively updating node hidden states using information from adjacent nodes via learned gated update rules.",
            "citation_title": "Gated graph sequence neural networks",
            "mention_or_use": "mention",
            "system_name": "Gated Graph Sequence Neural Network (GGNN)",
            "system_description": "GGNN is a neural message-passing model that represents each node with a hidden state and updates node states over T time-steps using neighbor aggregation via an adjacency-encoded linear combination followed by gated recurrent (GRU-like) updates (reset and update gates). It takes node annotations as initialization and outputs per-node or graph-level predictions via learned output networks.",
            "declarative_component": "Graph structure (nodes, typed/directed edges) encoded into adjacency matrices A (retrieved per-node) representing symbolic relational structure provided as input (e.g., program graphs, molecule graphs).",
            "imperative_component": "Neural gated recurrent propagation using learned weight matrices (W, U) and gating functions (z, r), with nonlinearities (tanh, sigmoid); trained via gradient descent in an end-to-end fashion.",
            "integration_method": "The graph structure is used as an input wiring for a fully differentiable neural propagation model. The model conditions on symbolic adjacency and node annotations but learns propagation and output mappings by gradient-based training.",
            "emergent_properties": "Ability to learn algorithmic-like relational reasoning over graph structures (e.g., reachability, program verification tasks) and produce task-specific per-node or sequence outputs through learned message passing instead of hand-crafted rules.",
            "task_or_benchmark": "Described in the paper as applicable to logical tasks (reachability) and program verification; in this paper GGNN is cited as prior art but not re-evaluated.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": null,
            "generalization_properties": "GGNNs have been shown (in the original work) to generalize to some graph reasoning tasks (e.g., reachability) by learning propagation rules, but no formal guarantees are provided in this paper's discussion.",
            "interpretability_properties": "Limited: GGNN is a learned neural propagator over a symbolic graph; interpretation requires analysis (e.g., hidden state inspection) but no native symbolic explanations. The current paper leverages GGNN-like structure within GSNN and augments interpretability via sensitivity analyses and explicit importance scoring.",
            "limitations_or_failures": "Scalability: naive GGNN forward/backward passes scale poorly with large numbers of nodes (O(N^2) forward, O(N^T) backward according to the paper), making direct use on knowledge graphs with thousands/millions of nodes infeasible without selection/pruning strategies.",
            "theoretical_framework": "GGNN is framed as a learned message-passing recurrent neural network over graphs; theoretical discussion centers on analogy to recurrent networks and learned propagation rather than formal symbolic-theorem guarantees.",
            "uuid": "e611.1",
            "source_info": {
                "paper_title": "The More You Know: Using Knowledge Graphs for Image Classification",
                "publication_date_yy_mm": "2016-12"
            }
        },
        {
            "name_short": "NEIL",
            "name_full": "NEIL (Extracting visual knowledge from web data)",
            "brief_description": "A system for extracting a large set of visual concepts and relations from web-scale data to construct a visual knowledge base/graph (cited as an example large, noisy knowledge source used in vision tasks).",
            "citation_title": "Neil: Extracting visual knowledge from web data",
            "mention_or_use": "mention",
            "system_name": "NEIL",
            "system_description": "NEIL is a pipeline for mining visual concepts and relationships from web images and text to build a large-scale visual knowledge base containing many concepts (~2000 cited) and their relationships; it is presented in this paper as an example of a large knowledge graph resource that motivates the need for scalable graph reasoning techniques.",
            "declarative_component": "A large-scale visual knowledge graph (concept nodes and relation edges) learned/collected from web data; stored as symbolic beliefs/facts.",
            "imperative_component": "Computer vision classifiers/detectors and extraction algorithms used to ground visual concepts into the knowledge base; NEIL also uses iterative learning/pruning heuristics (procedural components) to improve the KB.",
            "integration_method": "Implicit integration: NEIL couples symbolic fact collection with procedural vision models and heuristics to construct the graph; the paper cites NEIL to motivate GSNN's need to handle large, noisy graphs, but does not detail end-to-end differentiable integration.",
            "emergent_properties": "Provides broad coverage of visual concepts and relations to support downstream reasoning tasks, but the paper cites NEIL mainly to highlight scale/noise challenges rather than specific emergent behaviors.",
            "task_or_benchmark": "Knowledge extraction from web data / building visual knowledge bases; used as a knowledge source for downstream vision tasks in literature.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": null,
            "generalization_properties": "NEIL aims to generalize by harvesting diverse web data, but no specific generalization metrics are reported within this paper.",
            "interpretability_properties": "As a KB, NEIL's contents are symbolic and inspectable; the paper does not report interpretability analyses for NEIL itself.",
            "limitations_or_failures": "NEIL/large KBs are noisy and large; the paper emphasizes that direct use of very large knowledge graphs (e.g., NEIL, NELL) is computationally challenging for naive neural graph methods and motivates GSNN's selective expansion.",
            "theoretical_framework": "No formal theoretical framework included here; NEIL is presented as an empirical, large-scale KB construction system.",
            "uuid": "e611.2",
            "source_info": {
                "paper_title": "The More You Know: Using Knowledge Graphs for Image Classification",
                "publication_date_yy_mm": "2016-12"
            }
        },
        {
            "name_short": "NELL",
            "name_full": "Never-Ending Language Learning (NELL)",
            "brief_description": "A long-running system that continuously reads the web to populate and expand a knowledge base of beliefs (cited here as an example of very large knowledge sources that pose scalability challenges).",
            "citation_title": "Toward an architecture for neverending language learning",
            "mention_or_use": "mention",
            "system_name": "Never-Ending Language Learning (NELL)",
            "system_description": "NELL is an architecture for continuously extracting and consolidating beliefs from web text into a large knowledge base; cited in this paper as an example knowledge source (millions of beliefs) whose scale motivates efficient selective graph-based reasoning.",
            "declarative_component": "A large symbolic knowledge base containing millions of beliefs/facts collected over continuous web reading.",
            "imperative_component": "Procedural extraction, pattern learning, and iterative learning heuristics to expand and refine the KB; not detailed in this paper.",
            "integration_method": "NELL couples extraction algorithms with symbolic belief storage; this paper cites it to illustrate scale/noise rather than as an integrated neural-symbolic model.",
            "emergent_properties": "Long-term accumulation of beliefs and continual improvement of KB coverage; not analyzed in detail here.",
            "task_or_benchmark": "Large-scale knowledge extraction and KB construction.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": null,
            "generalization_properties": "Designed for continual learning across diverse web data; specifics not discussed in this paper.",
            "interpretability_properties": "KB contents are symbolic and inspectable, but paper does not discuss interpretability for NELL.",
            "limitations_or_failures": "Scale and noise in large KBs are noted as challenges that make naive neural propagation over entire KBs infeasible; motivates selective expansion in GSNN.",
            "theoretical_framework": "NELL is an engineering system with iterative learning heuristics; no unified formal theory is provided in this paper's discussion.",
            "uuid": "e611.3",
            "source_info": {
                "paper_title": "The More You Know: Using Knowledge Graphs for Image Classification",
                "publication_date_yy_mm": "2016-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gated graph sequence neural networks",
            "rating": 2
        },
        {
            "paper_title": "Neil: Extracting visual knowledge from web data",
            "rating": 2
        },
        {
            "paper_title": "Toward an architecture for neverending language learning",
            "rating": 2
        },
        {
            "paper_title": "Building a large-scale multimodal knowledge base system for answering visual queries",
            "rating": 1
        },
        {
            "paper_title": "Reasoning about Object Affordances in a Knowledge Base Representation",
            "rating": 1
        },
        {
            "paper_title": "The graph neural network model",
            "rating": 1
        }
    ],
    "cost": 0.01357775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The More You Know: Using Knowledge Graphs for Image Classification</h1>
<p>Kenneth Marino, Ruslan Salakhutdinov, Abhinav Gupta<br>Carnegie Mellon University<br>5000 Forbes Ave, Pittsburgh, PA 15213<br>{kdmarino, rsalakhu, abhinavg}@cs.cmu.edu</p>
<h4>Abstract</h4>
<p>One characteristic that sets humans apart from modern learning-based computer vision algorithms is the ability to acquire knowledge about the world and use that knowledge to reason about the visual world. Humans can learn about the characteristics of objects and the relationships that occur between them to learn a large variety of visual concepts, often with few examples. This paper investigates the use of structured prior knowledge in the form of knowledge graphs and shows that using this knowledge improves performance on image classification. We build on recent work on end-to-end learning on graphs, introducing the Graph Search Neural Network as a way of efficiently incorporating large knowledge graphs into a vision classification pipeline. We show in a number of experiments that our method outperforms standard neural network baselines for multi-label classification.</p>
<h2>1. Introduction</h2>
<p>Our world contains millions of visual concepts understood by humans. These often are ambiguous (tomatoes can be red or green), overlap (vehicles includes both cars and planes) and have dozens or hundreds of subcategories (thousands of specific kinds of insects). While some visual concepts are very common such as person or car, most categories have many fewer examples, forming a long-tail distribution [37]. And yet, even when only shown a few or even one example, humans have the remarkable ability to recognize these categories with high accuracy. In contrast, while modern learning-based approaches can recognize some categories with high accuracy, it usually requires thousands of labeled examples for each of these categories. Given how large, complex and dynamic the space of visual concepts is, this approach of building large datasets for every concept is unscalable. Therefore, we need to ask what humans have that current approaches do not.</p>
<p>One possible answer to this is structured knowledge and reasoning. Humans are not merely appearance-based classifiers; we gain knowledge of the world from experience and
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Example of how semantic knowledge about the world aids classification. Here we see an elephant shrew. Humans are able to make the correct classification based on what we know about the elephant shrew and other similar animals.
language. We use this knowledge in our everyday lives to recognize objects. For instance, we might have read in a book about the "elephant shrew" (maybe even seen an example) and will have gained knowledge that is useful for recognizing one. Figure 1 illustrates how we might use our knowledge about the world in this problem. We might know that an elephant shrew looks like a mouse, has a trunk and a tail, is native to Africa, and is often found in bushes. With this information, we could probably identify the elephant shrew if we saw one in the wild. We do this by first recognizing (we see a small mouse-like object with a trunk in a bush), recalling knowledge (we think of animals we have heard of and their parts, habitat, and characteristics) and then reasoning (it is an elephant shrew because it has a trunk and a tail, and looks like a mouse while mice and elephants do not have all these characteristics). With this information, even if we have only seen one or two pictures of this animal, we would be able to classify it.</p>
<p>There has been a lot of work in end-to-end learning on graphs or neural network trained on graphs [31, 2, 6, 11, $25,22,9,21]$. Most of these approaches either extract features from the graph or they learn a propagation model that transfers evidence between nodes conditional on the type of edge. An example of this is the Gated Graph Neural Network [18] which takes an arbitrary graph as input. Given</p>
<p>some initialization specific to the task, it learns how to propagate information and predict the output for every node in the graph. This approach has been shown to solve basic logical tasks as well as program verification.</p>
<p>Our work improves on this model and adapts end-to-end graph neural networks to multi-label image classification. We introduce the Graph Search Neural Network (GSNN) which uses features from the image to efficiently annotate the graph, select a relevant subset of the input graph and predict outputs on nodes representing visual concepts. These output states are then used to classify the objects in the image. GSNN learns a propagation model which reasons about different types of relationships and concepts to produce outputs on the nodes which are then used for image classification. Our new architecture mitigates the computational issues with the Gated Graph Neural Networks for large graphs which allows our model to be efficiently trained for image tasks using large knowledge graphs. We show how our model is effective at reasoning about concepts to improve image classification tasks. Importantly, our GSNN model is also able to provide explanations on classifications by following how the information is propagated in the graph.</p>
<p>The major contributions of this work are (a) the introduction of the GSNN as a way of incorporating potentially large knowledge graphs into an end-to-end learning system that is computationally feasible for large graphs; (b) a framework for using noisy knowledge graphs for image classification; and (c) the ability to explain our image classifications by using the propagation model. Our method significantly outperforms baselines for multi-label classification.</p>
<h2>2. Related Work</h2>
<p>Learning knowledge graphs [4, 3, 30] and using graphs for visual reasoning [37, 20] has recently been of interest to the vision community. For reasoning on graphs, several approaches have been studied. For example, [38] collects a knowledge base and then queries this knowledge base to do first-order probabilistic reasoning to predict affordances. [20] builds a graph of exemplars for different categories and uses the spatial relationships to perform contextual reasoning. Approaches such as [17] use random walks on the graphs to learn patterns of edges while performing the walk and predict new edges in the knowledge graph. There has also been some work using a knowledge base for image retrieval [12] or answering visual queries [39], but these works are focused on building and then querying knowledge bases rather than using existing knowledge bases as side information for some vision task.</p>
<p>However, none of these approaches have been learned in an end-to-end manner and the propagation model on the graph is mostly hand-crafted. More recently, learning from knowledge graphs using neural networks and other end-to-
end learning systems to perform reasoning has become an active area of research. Several works treat graphs as a special case of a convolutional input where, instead of pixel inputs connected to pixels in a grid, we define the inputs as connected by an input graph, relying on either some global graph structure or doing some sort of pre-processing on graph edges [2, 6, 11, 25]. However, most of these approaches have been tried on smaller, cleaner graphs such as molecular datasets. In vision problems, these graphs encode contextual and common-sense relationships and are significantly larger and noisier.</p>
<p>Li and Zemel present Graph Gated Neural Networks (GGNN) [18] which uses neural networks on graph structured data. This paper (an extension of Graph Neural Networks [31]) serves as the foundation for our Graph Search Neural Network (GSNN). Several papers have found success using variants of Graph Neural Networks applied to various simple domains such as quantitative structureproperty relationship (QSPR) analysis in chemistry [22] and subgraph matching and other graph problems on toy datasets [9]. GGNN is a fully end-to-end network that takes as input a directed graph and outputs either a classification over the entire graph or an output for each node. For instance, for the problem of graph reachability, GGNN is given a graph, a start node and end node, and the GGNN will have to output whether the end node is reachable from the start node. They show results for logical tasks on graphs and more complex tasks such as program verification.</p>
<p>There is also a substantial amount of work on various types of kernels defined for graphs [36] such as diffusion kernels [14], graphlet kernels [33], Weisfeiler-Lehman graph kernels [32], deep graph kernels [27], graph invariant kernels [26] and shortest-path kernels [1]. The methods have various ways of exploiting common graph structures, however, these approaches are only helpful for kernel-based approaches such as SVMs which do not compare well with neural network architectures in vision.</p>
<p>Our work is also related to attribute approaches [8] to vision such as [16] which uses a fixed set of binary attributes to do zero-shot prediction, [34] which uses attributes shared across categories to prevent semantic drift in semi-supervised learning and [5] which automatically discovers attributes and uses them for fine-grained classification. Our work also uses attribute relationships that appear in our knowledge graphs, but also uses relationships between objects and reasons directly on graphs rather than using object-attribute pairs directly.</p>
<h2>3. Methodology</h2>
<h3>3.1. Graph Gated Neural Network</h3>
<p>The idea of GGNN is that given a graph with $N$ nodes, we want to produce some output which can either be an output for every graph node $o_{1}, o_{2}, \ldots o_{N}$ or a global output</p>
<p>$o_{G}$. This is done by learning a propagation model similar to an LSTM. For each node in the graph $v$, we have a hidden state representation $h_{v}^{(t)}$ at every time step $t$. We start at $t=0$ with initial hidden states $x_{v}$ that depends on the problem. For instance, for learning graph reachability, this might be a two bit vector that indicates whether a node is the source or destination node. In case of visual knowledge graph reasoning, $x_{v}$ can be a one bit activation representing the confidence of a category being present based on an object detector or classifier.</p>
<p>Next, we use the structure of our graph, encoded in a matrix $A$ which serves to retrieve the hidden states of adjacent nodes based on the edge types between them. The hidden states are then updated by a gated update module similar to an LSTM. The basic recurrence for this propagation network is</p>
<p>$$
\begin{gathered}
h_{v}^{(1)}=\left[x_{v}^{T}, 0\right]^{T} \
a_{v}^{(t)}=A_{v}^{T}\left[h_{1}^{(t-1)} \ldots h_{N}^{(t-1)}\right]^{T}+b \
z_{v}^{t}=\sigma\left(W^{z} a_{v}^{(t)}+U^{z} h_{v}^{(t-1)}\right) \
r_{v}^{t}=\sigma\left(W^{r} a_{v}^{(t)}+U^{r} h_{v}^{(t-1)}\right) \
\overrightarrow{h_{v}^{t}}=\operatorname{tanh}\left(W a_{v}^{(t)}+U\left(r_{v}^{t} \odot h_{v}^{(t-1)}\right)\right) \
h_{v}^{(t)}=\left(1-z_{v}^{t}\right) \odot h_{v}^{(t-1)}+z_{v}^{t} \odot \overrightarrow{h_{v}^{t}}
\end{gathered}
$$</p>
<p>where $h_{v}^{(t)}$ is the hidden state for node $v$ at time step $t, x_{v}$ is the problem specific annotation, $A_{v}$ is the adjacency matrix of the graph for node $v$, and $W$ and $U$ are learned parameters. Eq 1 is the initialization of the hidden state with $x_{v}$ and empty dimensions. Eq 2 shows the propagation updates from adjacent nodes. Eq (3-6) combine the information from adjacent nodes and current hidden state of the nodes to compute the next hidden state.</p>
<p>After $T$ time steps, we have our final hidden states. The node level outputs can then just be computed as</p>
<p>$$
o_{v}=g\left(h_{v}^{(T)}, x_{v}\right)
$$</p>
<p>where $g$ is a fully connected network, the output network, and $x_{v}$ is the original annotation for the node.</p>
<h3>3.2. Graph Search Neural Network</h3>
<p>The biggest problem in adapting GGNN for image tasks is computational scalability. NEIL [4] for example has over 2000 concepts, and NELL [3] has over 2M confident beliefs. Even after pruning to our task, these graphs would still be huge. Forward propagation on the standard GGNN is $O\left(N^{2}\right)$ to the number of nodes $N$ and backward propagation is $O\left(N^{T}\right)$ where $T$ is the number of propagation steps. We perform simple experiments on GGNNs on synthetic graphs and find that after more than about 500 nodes, a forward and backward pass takes over 1 second on a single instance, even when making generous parameter assumptions. On 2,000 nodes, it takes well over a minute for a single image. Using GGNN out of the box is infeasible.</p>
<p>Our solution to this problem is the Graph Search Neural Network (GSNN). As the name might imply, the idea is
that rather than performing our recurrent update over all of the nodes of the graph at once, we start with some initial nodes based on our input and only choose to expand nodes which are useful for the final output. Thus, we only compute the update steps over a subset of the graph. So how do we select which subset of nodes to initialize the graph with? During training and testing, we determine initial nodes in the graph based on likelihood of the concept being present as determined by an object detector or classifier. For our experiments, we use Faster R-CNN [28] for each of the 80 COCO categories. For scores over some chosen threshold, we choose the corresponding nodes in the graph as our initial set of active nodes.</p>
<p>Once we have initial nodes, we also add the nodes adjacent to the initial nodes to the active set. Given our initial nodes, we want to first propagate the beliefs about our initial nodes to all of the adjacent nodes. After the first time step, however, we need a way of deciding which nodes to expand next. We therefore learn a per-node scoring function that estimates how "important" that node is. After each propagation step, for every node in our current graph, we predict an importance score</p>
<p>$$
i_{v}^{(t)}=g_{i}\left(h_{v}, x_{v}\right)
$$</p>
<p>where $g_{i}$ is a learned network, the importance network.
Once we have values of $i_{v}$, we take the top $P$ scoring nodes that have never been expanded and add them to our expanded set, and add all nodes adjacent to those nodes to our active set. Figure 2 illustrates this expansion. At $t=1$ only the detected nodes are expanded. At $t=2$ we expand chosen nodes based on importance values and add their neighbors to the graph. At the final time step $T$ we compute the per-node-output and re-order and zero-pad the outputs into the final classification net.</p>
<p>To train the importance net, we assign target importance value to each node in the graph for a given image. Nodes corresponding to ground-truth concepts in an image are assigned an importance value of 1 . The neighbors of these nodes are assigned a value of $\gamma$. Nodes which are two-hop away have value $\gamma^{2}$ and so on. The idea is that nodes closest to the final output are the most important to expand.</p>
<p>We now have an end-to-end network which takes as input a set of initial nodes and annotations and outputs a per-node output for each of the active nodes in the graph. It consists of three sets of networks: the propagation net, the importance net, and the output net. The final loss from the image problem can be backpropagated from the final output of the pipeline back through the output net and the importance loss is backpropagated through each of the importance outputs. See Figure 3 to see the GSNN architecture. First $x_{\text {init }}$, the detection confidences initialize $h_{\text {init }}^{(1)}$, the hidden states of the initially detected nodes. We then initialize $h_{\text {adj1 }}^{(1)}$, the</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Graph Search Neural Network expansion. Starts with detected nodes and expands neighbors. Adds nodes adjacent to expand nodes predicted by importance net.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Graph Search Neural Network diagram. Shows initialization of hidden states, addition of new nodes as graph is expanded and the flow of losses through the output, propagation and importance nets.</p>
<p>hidden states of the adjacent nodes, with 0. We then update the hidden states using the propagation net. The values of h(2) are then used to predict the importance scores i(1), which are used to pick the next nodes to add adj2. These nodes are then initialized with h(2)adj2 = 0 and the hidden states are updated again through the propagation net. After T steps, we then take all of the accumulated hidden states hT to predict the GSNN outputs for all the active nodes. During backpropagation, the binary cross entropy (BCE) loss is fed backward through the output layer, and the importance losses are fed through the importance networks to update the network parameters.</p>
<p>One final detail is the addition of a "node bias" into GSNN. In GGNN, the per-node output function g(hv(T), xv) takes in the hidden state and initial annotation of the node v to compute its output. In a certain sense it is agnostic to the meaning of the node. That is, at train or test time, GSNN takes in a graph it has perhaps never seen before, and some initial annotations xv for each node. It then uses the structure of the graph to propagate those annotations through the network and then compute an output. The nodes of the graph could have represented anything from human relationships to a computer program. However, in our graph network, the fact that a particular node represents "horse" or "cat" will probably be relevant, and we can also constrain ourselves to a static graph over image concepts. Hence we introduce node bias terms that, for every node in our graph, has some learned values. Our output equations are now g(hv(T), xv, nv) where nv is a bias term that is tied to a particular node v in the overall graph. This value is stored in a table and its value are updated by backpropagation.</p>
<h3>3.3. Image pipeline and baselines</h3>
<p>Another problem we face adapting graph networks for vision problems is how to incorporate the graph network into an image pipeline. For classification, this is fairly straightforward. We take the output of the graph network, reorder it so that nodes always appear in the same order into the final network, and zero pad any nodes that were not expanded. Therefore, if we have a graph with 316 node outputs, and each node predicts a 5-dim hidden variable, we create a 1580-dim feature vector from the graph. We also concatenate this feature vector with fc7 layer (4096-dim) of a fine-tuned VGG-16 network [35] and top-score for each COCO category predicted by Faster R-CNN (80-dim). This 5756-dim feature vector is then fed into 1-layer final classification network trained with dropout.</p>
<p>For baselines, we compare to: (1) VGG Baseline - feed just fc7 into final classification net; (2) Detection Baseline - feed fc7 and top COCO scores into final classification net.</p>
<h2>4. Results</h2>
<h3>4.1. Datasets</h3>
<p>For our experiments, we wanted to test on a dataset that represents the complex, noisy visual world with its many different kinds of objects, where labels are potentially ambiguous and overlapping, and categories fall into a long-tail distribution [37]. Humans do well in this setting, but vision algorithms still struggle with it. To this end, we chose the Visual Genome dataset [15] v1.0.</p>
<p>Visual Genome contains over 100,000 natural images from the Internet. Each image is labeled with objects, attributes and relationships between objects entered by human annotators. Annotators could enter any object in the image rather than from a predefined list, so as a result there are thousands of object labels with some being more common</p>
<p>and most having many fewer examples. There are on average 21 labeled objects in an image, so compared to datasets such as ImageNet [29] or PASCAL [7], the scenes we are considering are far more complex. Visual Genome is also labeled with object-object relationships and object-attribute relationships which we use for GSNN.</p>
<p>In our experiments, we create a subset from Visual Genome which we call Visual Genome multi-label dataset or VGML. In VGML, we take the 200 most common objects in the dataset and the 100 most common attributes and also add any COCO categories not in those 300 for a total of 316 visual concepts. Our task is then multi-label classification: for each image predict which subset of the 316 total categories appear in the scene. We randomly split the images into a roughly 80-20 train/test split. Since we used pre-trained detectors from COCO, we ensure none of our test images overlap with our detector's training images.</p>
<p>We also evaluate out method on the more standard COCO dataset [19] to show that our approach is useful on multiple datasets and that our method does not rely on graphs built specifically for our datasets. We train and test in the multi-label setting [24], and evaluate on the minival set [28].</p>
<h3>4.2. Building the Knowledge Graph</h3>
<p>We also use Visual Genome as a source for our knowledge graph. Using only the train split, we build a knowledge graph connecting the concepts using the most common object-attribute and object-object relationships in the dataset. Specifically, we counted how often an object/object relationship or object/attribute pair occurred in the training set, and pruned any edges that had fewer than 200 instances. This leaves us with a graph over all of the images with each edge being a common relationship. The idea is that we would get very common relationships (such as grass is green or person wears clothes) but not relationships that are rare and only occur in single images (such as person rides zebra).</p>
<p>The Visual Genome graphs are useful for our problem because they contain scene-level relationships between objects, e.g. person wears pants or fire hydrant is red and thus allow the graph network to reason about what is in a scene. However, it does not contain useful semantic relationships. For instance, it might be helpful to know that dog is an animal if our visual system sees a dog and one of our labels is animal. To address this, we also create a version of graph by fusing the Visual Genome Graphs with WordNet [23]. Using the subset of WordNet from [10], we first collect new nodes in WordNet not in our output label by including those which directly connect to our output labels and thus likely to be relevant and add them to a combined graph. We then take all of the WordNet edges between these nodes and add them to our combined graph.</p>
<h3>4.3. Training details</h3>
<p>We jointly train all parts of the pipeline (except for the detectors). All models are trained with Stochastic Gradient Descent, except GSNN which is trained using ADAM [13]. We use an initial learning rate of $0.05,0.005$ for the VGG net before $f c 7$, decreasing by a factor of 0.1 every 10 epochs, an L2 penalty of $1 e^{-6}$ and a momentum of 0.5 . We set our GSNN hidden state size to 10 , importance discount factor $\gamma$ to 0.3 , number of time steps $T$ to 3 , initial confidence threshold to 0.5 and our expand number $P$ to 5 . Our GSNN importance and output networks are single layer networks with sigmoid activations. All networks were trained for 20 epochs with a batch size of 16.</p>
<h3>4.4. Quantitative Evaluation</h3>
<p>Table 1 shows the result of our method on Visual Genome multi-label classification. In this experiment, the combined Visual Genome, WordNet graph outperforms the Visual Genome graph. This suggests that including the outside semantic knowledge from WordNet and performing explicit reasoning on a knowledge graph allows our model to learn better representations compared to the other models.</p>
<p>We also perform experiments to test the effect of limiting the size of the training dataset has on performance. Figure 4 shows the results of this experiment on Visual Genome, varying the training set size from the entire training set (approximately 80,000), all the way down to 500 examples. Choosing the subsets of examples for these experiments is done randomly, but each training set is a subset of the larger ones-e.g. all of the examples in the 1,000 set are also in the 2,000 set. We see that, until the 1,000 sample set, the GSNN-based methods all outperform baselines. At 1,000 and 500 examples, all of the methods perform equally. Given the long-tail nature of Visual Genome, it is likely that for fewer than 2,000 samples, many categories do not have enough examples for any method to learn well. This experiment indicates that our method is able to improve even in the low-data case up to a point.</p>
<p>In Table 2, we show results on the COCO multi-label dataset. We can see that the boost from using graph knowledge is more significant than it was on Visual Genome. One possible explanation is that the Visual Genome knowledge graph provides significant information which helps improve the performance on the COCO dataset itself. In the previous Visual Genome experiment, much of the graph information is contained in the labels and images themselves. One other interesting result is that the Visual Genome graph outperforms the combined graph for COCO, though both outperform baselines. One possible reason is that the original VGML graph is smaller, cleaner, and contains more relevant information than the combined graph. Furthermore, in the VGML experiment, WordNet is new outside information for the algorithm helping boost the performance.</p>
<p>Table 1. Mean Average Precision for multi-label classification on Visual Genome Multi-Label dataset. Numbers for VGG baseline, VGG baseline with detections, GSNN using Visual Genome graph and GSNN using a combined Visual Genome and WordNet graph.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>mAP</th>
</tr>
</thead>
<tbody>
<tr>
<td>VGG</td>
<td>30.57</td>
</tr>
<tr>
<td>VGG+Det</td>
<td>31.4</td>
</tr>
<tr>
<td>GSNN-VG</td>
<td>32.83</td>
</tr>
<tr>
<td>GSNN-VG+WN</td>
<td>$\mathbf{3 3}$</td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Mean Average Precision on Visual Genome in the low data setting. Shows performance for all methods for the full dataset, 40,000, 20,000, 10,000, 5,000, 2,000, 1,000, and 500 training examples.</p>
<p>Table 2. Mean Average Precision for multi-label classification on COCO. Numbers for VGG baseline, VGG baseline with detections, GSNN using Visual Genome graph and GSNN using combined Visual Genome and WordNet graph.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>mAP</th>
</tr>
</thead>
<tbody>
<tr>
<td>VGG</td>
<td>69.86</td>
</tr>
<tr>
<td>VGG+Det</td>
<td>73.93</td>
</tr>
<tr>
<td>GSNN-VG</td>
<td>$\mathbf{7 7 . 5 7}$</td>
</tr>
<tr>
<td>GSNN-VG+WN</td>
<td>75.73</td>
</tr>
</tbody>
</table>
<p>Table 3. Mean Average Precision for multi-label classification on COCO, using only odd and even detectors.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>even mAP</th>
<th>odd mAP</th>
</tr>
</thead>
<tbody>
<tr>
<td>VGG+Det</td>
<td>71.87</td>
<td>71.73</td>
</tr>
<tr>
<td>GSNN-VG</td>
<td>73</td>
<td>73.43</td>
</tr>
<tr>
<td>GSNN-VG+WN</td>
<td>$\mathbf{7 3 . 5 9}$</td>
<td>$\mathbf{7 3 . 9 7}$</td>
</tr>
</tbody>
</table>
<p>One possible concern is the over dependence of the graph reasoning on the set of 80 COCO detectors and initial detections. Therefore, we performed an ablation experiment to see how sensitive our method is to having all of the initial detections. We reran the COCO experiments with both graphs using two different subsets of COCO detectors. The first subset is just the even COCO categories and the second subset is just the odd categories. We see from Table 3 that GSNN methods again outperform the baselines.</p>
<p>As one might suspect, our method does not perform uni- formly on all categories, but rather does better on some categories and worse on others. Figure 5 shows the differences in average precision for each category between our GSNN model with the combined graph and the detection baseline for the VGML experiment. Figure 6 shows the same for our COCO experiment. Performance on some classes improves greatly, such as "fork" in our VGML experiment and "scissors" in our COCO experiment. These and other good results on "knife" and "toothbrush" seem to indicate that the graph reasoning helps especially with small objects in the image. In the next section, we analyze our GSNN models on several examples to try to gain a better intuition as to what the GSNN model is doing and why it does well or poorly on certain examples.</p>
<h1>4.5. Qualitative Evaluation</h1>
<p>One way to analyse the GSNN is to look at the sensitivities of parameters in our model with respect to a particular output. Given a single image $I$, and a single label of interest $y_{i}$ that appears in the image, we would like to know how information travels through the GSNN and what nodes and edges it uses. We examined the sensitivity of the output to hidden states and detections by computing the partial derivatives $\frac{\partial y_{i}}{\partial h^{(1)}} \frac{\partial y_{i}}{\partial h^{(2)}} \frac{\partial y_{i}}{\partial x_{d i}}$ with respect to the category of interest. These values tell us how a small change in the hidden state of a particular node affects a particular output. We would expect to see, for instance, that for labeling elephant, we see a high sensitivity for the hidden states corresponding to grey and trunk.</p>
<p>In this section, we show the sensitivity analysis for the GSNN combined graph model on the VGML experiment and the Visual Genome graph on the COCO experiments. In particular, we examine some classes that performed well under GSNN compared to the detection baseline and a few that performed poorly to try to get a better intuition into why some categories improve more.</p>
<p>Figure 7 shows the graph sensitivity analysis for the experiments with VGML on the left and COCO on the right, showing four examples where GSNN does better and two where it does worse. Each example shows the image, the ground truth output we are analyzing and the sensitivities of the concept of interest with respect to the hidden states of the graph or detections. For convenience, we display the names of the top detections or hidden states. We also show part of the graph that was expanded, to see what relationships GSNN was using.</p>
<p>For the VGML experiment, the top left of Figure 7 shows that using the detection for person, GSNN is able to reason that jeans are more likely since jeans are usually on people in images using the "wearing" edge. It is also sensitive to skateboard and horse, and each of these has a second order connection to jeans through person, so it is likely able to capture the fact that people tend to wear jeans while on</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Difference in Average Precision for each of the 316 labels in VGML between our GSNN combined graph model and detection baseline for the Visual Genome experiment. Top categories: scissors, donut, frisbee, microwave, fork. Bottom categories: stacked, tiled, light brown, ocean, grassy.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Difference in Average Precision for each of the 80 labels in COCO between our GSNN VG graph model and detection baseline for the COCO experiment. Top categories: fork, donut, cup, apple, microwave. Bottom categories: hairdryer, parking meter, bear, kite, and giraffe.</p>
<p>horses and skateboards. Note that the sensitivities are not the same as the actual detections, so it is not contradictory that horse has high sensitivity. The second row on the left shows a successful example for bicycle, using detections from person and skateboard and the fact that people tend to be "on" bicycles and skateboards. The last row shows a failure case for windshield. It correctly correlates with bus, but because the knowledge graph lacks a connection between bus and windshield, the graph network is unable to do better than the detection baseline. On the right, for the COCO experiment, the top example shows that fork is highly correlated with the detection for fork, which should not be surprising. However, it is able to reinforce this detection with the connections between broccoli and dining table, which are both two step connections to fork on the graph. Similarly, the middle example shows that the graph connections for pizza, bowl, and bottle being "on" dining table reinforce the detection of dining table. The bottom right shows another failure case. It is able to get the connection between the detection for toilet and hair dryer (both found in the bathroom), but the lack of good connections in the graph prevent the GSNN from improving over the baseline.</p>
<h2>5. Conclusion</h2>
<p>In this paper, we present the Graph Search Neural Network (GSNN) as a way of efficiently using knowledge graphs as extra information to improve image classification. We provide analysis that examines the flow of information through the GSNN and provides insights into why our model improves performance. We hope that this work provides a step towards bringing symbolic reasoning into traditional feed-forward computer vision frameworks.</p>
<p>The GSNN and the framework we use for vision problems is completely general. Our next steps will be to apply the GSNN to other vision tasks, such as detection, Visual Question Answering, and image captioning. Another interesting direction would be to combine the procedure of this work with a system such as NEIL [4] to create a system which builds knowledge graphs and then prunes them to get a more accurate, useful graph for image tasks.</p>
<p>Acknowledgements: We would like to thank everyone who took time to review this work and provide helpful comments. This research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA). The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied of ODNI, IARPA, or the US government. The US Government is authorized to reproduce and distribute the reprints for governmental purposes notwithstanding any copyright annotation therein. This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE-1252522 and ONR MURI N000141612007.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Sensitivity analysis of GSNN in VGML experiment (left) and COCO experiment (right) with the combined graph and Visual Genome graphs respectively. Each example shows the image, part of the knowledge graph expanded during the classification, and the sensitivity values of the initial detections, and the hidden states at time steps 2 and 3 with respect to the output class listed. The top detections and hidden state nodes are printed for convenience since the x -axis is too large to list every class. The top and middle rows show the results for images and classes where the GSNN significantly outperforms the detection baseline to get an intuition for when our method is working. The bottom row shows images and classes where GSNN does worse than the detection baseline to get an idea of when our method fails and why.</p>
<h2>References</h2>
<p>[1] K. M. Borgwardt and H.-P. Kriegel. Shortest-path kernels on graphs. ICDM, 2005.
[2] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral networks and locally connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.
[3] A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R. Hruschka, and T. M. Mitchell. Toward an architecture for neverending language learning. AAAI, 2010.
[4] X. Chen, A. Shrivastava, and A. Gupta. Neil: Extracting visual knowledge from web data. CVPR, 2013.
[5] K. Duan, D. Parikh, D. Crandall, and K. Grauman. Discovering localized attributes for fine-grained recognition. CVPR, 2012.
[6] D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel, A. Aspuru-Guzik, and R. P. Adams. Convolutional networks on graphs for learning molecular fingerprints. NIPS, 2015.
[7] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results. http://www.pascalnetwork.org/challenges/VOC/voc2012/workshop/index.html.
[8] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. Describing objects by their attributes. CVPR, 2009.
[9] M. Gori, G. Monfardini, and F. Scarselli. A new model for learning in graph domains. IEEE International Joint Conference on Neural Networks, 2, 2005.
[10] K. Guu, J. Miller, and P. Liang. Traversing knowledge graphs in vector space. In Empirical Methods in Natural Language Processing (EMNLP), 2015.
[11] M. Henaff, J. Bruna, and Y. LeCun. Deep convolutional networks on graph-structured data. arXiv preprint arXiv:1506.05163, 2015.
[12] J. Johnson, R. Krishna, M. Stark, L.-J. Li, D. A. Shamma, M. S. Bernstein, and L. Fei-Fei. Image retrieval using scene graphs. CVPR, 2015.
[13] D. P. Kingma and J. L. Ba. Adam: A method for stochastic optimization. $I C L R, 2015$.
[14] R. I. Kondor and J. Lafferty. Diffusion kernels on graphs and other discrete input spaces. ICML, 2, 2002.
[15] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma, M. Bernstein, and L. Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense image annotations. 2016.
[16] C. H. Lampert, H. Nickisch, and S. Harmeling. Attributebased classification for zero-shot visual object categorization. TPAMI, 2014.
[17] N. Lao, T. Mitchell, and W. W. Cohen. Random walk inference and learning in a large scale knowledge base. NIPS, 2011.
[18] Y. Li and R. Zemel. Gated graph sequence neural networks. $I C L R, 2016$.
[19] T. Lin, M. Maire, S. J. Belongie, R. B. Girshick, J. Hays, P. Perona, D. Ramanan, P. Dollr, and C. L. Zitnick. Microsoft COCO: common objects in context. ECCV, 2014.
[20] T. Malisiewicz and A. Efros. Beyond categories: The visual memex model for reasoning about object relationships. NIPS, 2009.
[21] V. D. Massa, G. Monfardini, L. Sarti, F. Scarselli, M. Maggini, and M. Gori. A comparison between recursive neural networks and graph neural networks. IEEE International Joint Conference on Neural Network Proceedings, 2006.
[22] A. Micheli. Neural network for graphs: A contextual constructive approach. IEEE Transactions on Neural Networks, 2009.
[23] G. A. Miller. Wordnet: A lexical database for english. $A C M$, 38, 1995.
[24] I. Misra, C. L. Zitnick, M. Mitchell, and R. Girshick. Seeing through the Human Reporting Bias: Visual Classifiers from Noisy Human-Centric Labels. In CVPR, 2016.
[25] M. Niepert, M. Ahmed, and K. Kutzkov. Learning convolutional neural networks for graphs. arXiv preprint arXiv:1605.05273, 2016.
[26] F. Orsini, P. Frasconi, and L. D. Raedt. Graph invariant kernels. IJCAI, 2015.
[27] Pinar, Yanardag, and S. V. N. Vishwanathan. Deep graph kernels. KDDM, 2015.
[28] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. NIPS, 2015.
[29] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. IJCV, 115(3):211-252, 2015.
[30] F. Sadeghi, S. K. Divvala, and A. Farhadi. Viske: Visual knowledge extraction and question answering by visual verification of relation phrases. CVPR, 2015.
[31] F. Scarselli, M. Gori, A. C. Tsoi, and G. Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 2009.
[32] N. Shervashidze, P. Schweitzer, E. J. van Leeuwen, K. Mehlhorn, and K. M. Borgwardt. Weisfeiler-lehman graph kernels. JMLR, 2011.
[33] N. Shervashidze, S. V. N. Vishwanathan, T. H. Petri, K. Mehlhorn, and K. M. Borgwardt. Efficient graphlet kernels for large graph comparison. AISTATS, 5, 2009.
[34] A. Shrivastava, S. Singh, and A. Gupta. Constrained semisupervised learning using attributes and comparative attributes. ECCV, 2012.
[35] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.
[36] S. V. N. Vishwanathan, N. N. Schraudolph, R. Kondor, and K. M. Borgwardt. Graph kernels. JMLR, 2010.
[37] X. Zhu, D. Anguelov, and D. Ramanan. Capturing long-tail distributions of object subcategories. CVPR, 2014.
[38] Y. Zhu, A. Fathi, and L. Fei-Fei. Reasoning about Object Affordances in a Knowledge Base Representation. In European Conference on Computer Vision, 2014.
[39] Y. Zhu, C. Zhang, C. R, and L. Fei-Fei. Building a largescale multimodal knowledge base system for answering visual queries. arXiv preprint arXiv:1507.05670, 2015.</p>            </div>
        </div>

    </div>
</body>
</html>
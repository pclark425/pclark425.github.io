<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5366 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5366</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5366</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-7eda139d737eea10fc1d95364327a41ec0cee4a4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7eda139d737eea10fc1d95364327a41ec0cee4a4" target="_blank">CoLAKE: Contextualized Language and Knowledge Embedding</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> The Contextualized Language and Knowledge Embedding (CoLAKE) is proposed, which jointly learns contextualized representation for both language and knowledge with the extended MLM objective, and achieves surprisingly high performance on a synthetic task called word-knowledge graph completion, which shows the superiority of simultaneously contextualizing language andknowledge representation.</p>
                <p><strong>Paper Abstract:</strong> With the emerging branch of incorporating factual knowledge into pre-trained language models such as BERT, most existing models consider shallow, static, and separately pre-trained entity embeddings, which limits the performance gains of these models. Few works explore the potential of deep contextualized knowledge representation when injecting knowledge. In this paper, we propose the Contextualized Language and Knowledge Embedding (CoLAKE), which jointly learns contextualized representation for both language and knowledge with the extended MLM objective. Instead of injecting only entity embeddings, CoLAKE extracts the knowledge context of an entity from large-scale knowledge bases. To handle the heterogeneity of knowledge context and language context, we integrate them in a unified data structure, word-knowledge graph (WK graph). CoLAKE is pre-trained on large-scale WK graphs with the modified Transformer encoder. We conduct experiments on knowledge-driven tasks, knowledge probing tasks, and language understanding tasks. Experimental results show that CoLAKE outperforms previous counterparts on most of the tasks. Besides, CoLAKE achieves surprisingly high performance on our synthetic task called word-knowledge graph completion, which shows the superiority of simultaneously contextualizing language and knowledge representation.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5366.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5366.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoLAKE WK graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Word-Knowledge (WK) Graph used in CoLAKE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unified heterogeneous graph representation that merges the fully-connected word graph of a sentence with extracted knowledge sub-graphs (entities and relation nodes) centered on linked entities; fed into a modified Transformer with adjacency-masked attention and trained with an extended masked language model objective over nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Word-Knowledge (WK) graph / graph-to-sequence node serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Constructed by (1) tokenizing a sentence into subword tokens and fully connecting them (word graph); (2) recognizing mentions and replacing mention nodes by their linked KG entities (anchor nodes); (3) extracting for each anchor node a sub-graph of up to 15 neighboring triplets (only triplets where the anchor is the head), converting relations into relation nodes and including neighboring entity nodes; (4) concatenating the word graph and these KG sub-graphs (with entities unique but relations allowed to repeat) into a single sequence of graph nodes for input. Each node carries a token embedding (separate embedding tables for words, entities, relations), a type embedding (word/entity/relation), and a soft position index (allows repeated indices and keeps tokens in the same triplet continuous). The graph structure is provided via an adjacency mask matrix M used in the masked multi-head self-attention so that attention is allowed only between connected nodes (1-hop per layer).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph subgraphs combined with sentence word graph (heterogeneous graph: words, entities, relations)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Heterogeneous (word/entity/relation nodes); structure-aware (adjacency mask enforces graph connectivity in attention); contextualized entity and relation embeddings (entity/relation representation depends on injected subgraph + sentence context); inductive capability (can infer embeddings for unseen entities via neighbor aggregation); preserves local (1-hop) graph structure per Transformer layer; uses soft position indexes to preserve continuity of triplet tokens; scalable concerns handled via CPU-stored entity embeddings and negative sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Fine-tuning/evaluation on knowledge-driven tasks (entity typing on Open Entity; relation extraction on FewRel), factual probing (LAMA / LAMA-UHN), general NLU (GLUE), and a synthetic word-knowledge graph completion task (transductive and inductive splits derived from FewRel).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Entity typing (Open Entity) CoLAKE: P=77.0%, R=75.7%, F1=76.4%. Relation extraction (FewRel) CoLAKE: P=90.6%, R=90.6%, F1=90.5%. LAMA-Google-RE P@1 = 9.5; LAMA-T-REx P@1 = 28.8. GLUE average (dev) = 86.3 (vs RoBERTa BASE 86.4). WK graph completion (transductive): MR=2.03, MRR=82.48, HITS@1=72.14, HITS@3=92.19, HITS@10=98.58. WK graph completion (inductive): MR=31.01, MRR=28.10, HITS@1=15.69, HITS@3=30.28, HITS@10=58.05.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared against standard PLMs (BERT, RoBERTa) and joint models (ERNIE, KnowBERT, KEPLER, E-BERT, K-Adapter) on knowledge tasks: CoLAKE outperforms these counterparts on most knowledge-driven benchmarks (e.g., FewRel F1 90.5% vs RoBERTa 85.3%, ERNIE 88.3%). On LAMA, CoLAKE improves over RoBERTa BASE (e.g., LAMA-Google-RE 9.5 vs RoBERTa 5.3) and is competitive with/close to larger or adapter-based models (K-Adapter). On WK graph completion, CoLAKE substantially outperforms classic KE models (TransE, DistMult, ComplEx, RotatE) in the transductive setting (e.g., MRR 82.48 vs best baseline RotatE MRR 70.90) and shows much stronger inductive generalization than DKRL (inductive MRR 28.10 vs DKRL 8.18).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires an entity linker for mention-to-entity alignment; only extracts neighbor triplets where anchor is head (design choice may omit other context); pretraining scale challenges due to millions of entities (addressed via mixed CPU-GPU embedding storage and negative sampling); predicting masked anchor nodes can be trivial if model relies solely on knowledge context (mitigated by discarding neighbors of anchor nodes 50% of the time); slight degradation on general NLU (GLUE) relative to RoBERTa; training resource intensive (832 V100 GPUs × 38 hours reported).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CoLAKE: Contextualized Language and Knowledge Embedding', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5366.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5366.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mention+Entity concat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Concatenate textual mention and linked entity token (Pörner et al. style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple serialization strategy where, instead of replacing a textual mention with the symbolic entity token, both the mention text and the canonical entity token are concatenated in the input (e.g., 'Jean Mara ##is Jean_Marais'), encouraging alignment between textual and entity embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BERT is not a knowledge base (yet): Factual knowledge vs. name-based reasoning in unsupervised QA</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Mention+Entity token concatenation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>During preprocessing/fine-tuning, when a mention is linked to a KG entity, the pipeline appends/injects the canonical entity token next to the original mention tokens (rather than replacing them). Implemented in CoLAKE following Pörner et al. (2019) to encourage mapping between textual forms and entity embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Mention annotations linked to knowledge graph entities (sentence-level with entity tokens injected)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Simple, low-overhead, preserves original textual context while providing a direct symbolic entity anchor; helps align word and entity embedding spaces; does not require deep graph serialization logic.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used in CoLAKE fine-tuning/evaluation on entity typing, relation extraction, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Applied in CoLAKE experiments; performance reported under CoLAKE results (see CoLAKE metrics). The paper does not separately ablate this concatenation method's isolated effect with independent numeric metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Presented as the chosen practical approach (citing Pörner et al.) instead of full replacement; no direct numeric comparison to replacement-in-place in this paper, but used consistently in reported CoLAKE fine-tuning experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No detailed ablation in the paper; potential token-length increase and ambiguity if not disambiguated; relies on quality of entity linker.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CoLAKE: Contextualized Language and Knowledge Embedding', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5366.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5366.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>K-BERT style injection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>K-BERT triplet injection (as described in Liu et al., 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A related prior approach that injects KG triplets into the input text (typically at fine-tuning time) by inserting triples as additional tokens into the sentence, allowing the LM to attend to explicit knowledge; CoLAKE is compared to and distinguished from this practice.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>K-BERT: enabling language representation with knowledge graph</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Triplet injection into input sequence (fine-tuning-time insertion)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Inject KG triplets directly into the textual input by adding nodes/tokens derived from triplets into the sentence (commonly during fine-tuning rather than pre-training), creating an augmented input sequence that the Transformer processes.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph triplets inserted into sentence sequences</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Enables explicit KG facts to be present in the model context; typically applied at fine-tuning (not pre-training), so model may not learn joint contextualized KG embeddings during base pretraining; simpler to implement but less integrated than joint pretraining approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Discussed conceptually; K-BERT evaluated in its own work on downstream tasks (not detailed numerically in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>CoLAKE differs by jointly pretraining language and knowledge (MLM extended over graph nodes) and learning contextualized entity/relation embeddings, whereas K-BERT injects triplets at fine-tuning; CoLAKE argues joint pretraining is more concise and gives contextualized knowledge representations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>According to the paper, triplet injection at fine-tuning does not yield joint contextualized KG embeddings and may be less effective than joint pretraining; detailed limitations of K-BERT are not experimentally enumerated here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CoLAKE: Contextualized Language and Knowledge Embedding', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5366.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5366.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-to-text templates (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-to-text template methods (conceptual mention)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Graph-to-text template methods convert KG facts or graph structures into textual templates (sentences) to be used as training data or prompts for language models; CoLAKE suggests it can be used to measure the quality of such templates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph-to-text templates (template-based textualization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Use templates to linearize graph facts/triplets into natural-language-like sentences (e.g., 'Harry Potter is the enemy of Lord Voldemort'), which can then be used to train or probe language models. The paper mentions these templates as a potential application area for CoLAKE to assess template quality, but does not describe specific template rules or algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graph triplets / KG subgraphs converted into templated sentences</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Produces human-readable text preserving fact triples; easy to feed into standard language models; template choice affects faithfulness and variety; may lose structured multi-hop graph information unless templates encode it.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Suggested use: measuring quality of templates by checking whether CoLAKE preserves original graph structure when encoding such template-derived text; not empirically evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>No direct comparisons or experimental results provided in this paper; CoLAKE authors propose using their WK-graph-aware model to assess such template quality in future work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Templates can be brittle, may introduce linguistic bias, and may fail to express multi-fact/complex graph structure; paper only mentions templates as an application idea and does not empirically analyze these limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CoLAKE: Contextualized Language and Knowledge Embedding', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>BERT is not a knowledge base (yet): Factual knowledge vs. name-based reasoning in unsupervised QA <em>(Rating: 2)</em></li>
                <li>K-BERT: enabling language representation with knowledge graph <em>(Rating: 2)</em></li>
                <li>Integrating graph contextualized knowledge into pre-trained language models <em>(Rating: 1)</em></li>
                <li>KEPLER: A unified model for knowledge embedding and pre-trained language representation <em>(Rating: 2)</em></li>
                <li>Commonsense knowledge mining from pretrained models <em>(Rating: 2)</em></li>
                <li>Inducing relational knowledge from BERT <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5366",
    "paper_id": "paper-7eda139d737eea10fc1d95364327a41ec0cee4a4",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "CoLAKE WK graph",
            "name_full": "Word-Knowledge (WK) Graph used in CoLAKE",
            "brief_description": "A unified heterogeneous graph representation that merges the fully-connected word graph of a sentence with extracted knowledge sub-graphs (entities and relation nodes) centered on linked entities; fed into a modified Transformer with adjacency-masked attention and trained with an extended masked language model objective over nodes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Word-Knowledge (WK) graph / graph-to-sequence node serialization",
            "representation_description": "Constructed by (1) tokenizing a sentence into subword tokens and fully connecting them (word graph); (2) recognizing mentions and replacing mention nodes by their linked KG entities (anchor nodes); (3) extracting for each anchor node a sub-graph of up to 15 neighboring triplets (only triplets where the anchor is the head), converting relations into relation nodes and including neighboring entity nodes; (4) concatenating the word graph and these KG sub-graphs (with entities unique but relations allowed to repeat) into a single sequence of graph nodes for input. Each node carries a token embedding (separate embedding tables for words, entities, relations), a type embedding (word/entity/relation), and a soft position index (allows repeated indices and keeps tokens in the same triplet continuous). The graph structure is provided via an adjacency mask matrix M used in the masked multi-head self-attention so that attention is allowed only between connected nodes (1-hop per layer).",
            "graph_type": "Knowledge graph subgraphs combined with sentence word graph (heterogeneous graph: words, entities, relations)",
            "representation_properties": "Heterogeneous (word/entity/relation nodes); structure-aware (adjacency mask enforces graph connectivity in attention); contextualized entity and relation embeddings (entity/relation representation depends on injected subgraph + sentence context); inductive capability (can infer embeddings for unseen entities via neighbor aggregation); preserves local (1-hop) graph structure per Transformer layer; uses soft position indexes to preserve continuity of triplet tokens; scalable concerns handled via CPU-stored entity embeddings and negative sampling.",
            "evaluation_task": "Fine-tuning/evaluation on knowledge-driven tasks (entity typing on Open Entity; relation extraction on FewRel), factual probing (LAMA / LAMA-UHN), general NLU (GLUE), and a synthetic word-knowledge graph completion task (transductive and inductive splits derived from FewRel).",
            "performance_metrics": "Entity typing (Open Entity) CoLAKE: P=77.0%, R=75.7%, F1=76.4%. Relation extraction (FewRel) CoLAKE: P=90.6%, R=90.6%, F1=90.5%. LAMA-Google-RE P@1 = 9.5; LAMA-T-REx P@1 = 28.8. GLUE average (dev) = 86.3 (vs RoBERTa BASE 86.4). WK graph completion (transductive): MR=2.03, MRR=82.48, HITS@1=72.14, HITS@3=92.19, HITS@10=98.58. WK graph completion (inductive): MR=31.01, MRR=28.10, HITS@1=15.69, HITS@3=30.28, HITS@10=58.05.",
            "comparison_to_other_representations": "Compared against standard PLMs (BERT, RoBERTa) and joint models (ERNIE, KnowBERT, KEPLER, E-BERT, K-Adapter) on knowledge tasks: CoLAKE outperforms these counterparts on most knowledge-driven benchmarks (e.g., FewRel F1 90.5% vs RoBERTa 85.3%, ERNIE 88.3%). On LAMA, CoLAKE improves over RoBERTa BASE (e.g., LAMA-Google-RE 9.5 vs RoBERTa 5.3) and is competitive with/close to larger or adapter-based models (K-Adapter). On WK graph completion, CoLAKE substantially outperforms classic KE models (TransE, DistMult, ComplEx, RotatE) in the transductive setting (e.g., MRR 82.48 vs best baseline RotatE MRR 70.90) and shows much stronger inductive generalization than DKRL (inductive MRR 28.10 vs DKRL 8.18).",
            "limitations_or_challenges": "Requires an entity linker for mention-to-entity alignment; only extracts neighbor triplets where anchor is head (design choice may omit other context); pretraining scale challenges due to millions of entities (addressed via mixed CPU-GPU embedding storage and negative sampling); predicting masked anchor nodes can be trivial if model relies solely on knowledge context (mitigated by discarding neighbors of anchor nodes 50% of the time); slight degradation on general NLU (GLUE) relative to RoBERTa; training resource intensive (832 V100 GPUs × 38 hours reported).",
            "uuid": "e5366.0",
            "source_info": {
                "paper_title": "CoLAKE: Contextualized Language and Knowledge Embedding",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Mention+Entity concat",
            "name_full": "Concatenate textual mention and linked entity token (Pörner et al. style)",
            "brief_description": "A simple serialization strategy where, instead of replacing a textual mention with the symbolic entity token, both the mention text and the canonical entity token are concatenated in the input (e.g., 'Jean Mara ##is Jean_Marais'), encouraging alignment between textual and entity embeddings.",
            "citation_title": "BERT is not a knowledge base (yet): Factual knowledge vs. name-based reasoning in unsupervised QA",
            "mention_or_use": "use",
            "representation_name": "Mention+Entity token concatenation",
            "representation_description": "During preprocessing/fine-tuning, when a mention is linked to a KG entity, the pipeline appends/injects the canonical entity token next to the original mention tokens (rather than replacing them). Implemented in CoLAKE following Pörner et al. (2019) to encourage mapping between textual forms and entity embeddings.",
            "graph_type": "Mention annotations linked to knowledge graph entities (sentence-level with entity tokens injected)",
            "representation_properties": "Simple, low-overhead, preserves original textual context while providing a direct symbolic entity anchor; helps align word and entity embedding spaces; does not require deep graph serialization logic.",
            "evaluation_task": "Used in CoLAKE fine-tuning/evaluation on entity typing, relation extraction, etc.",
            "performance_metrics": "Applied in CoLAKE experiments; performance reported under CoLAKE results (see CoLAKE metrics). The paper does not separately ablate this concatenation method's isolated effect with independent numeric metrics.",
            "comparison_to_other_representations": "Presented as the chosen practical approach (citing Pörner et al.) instead of full replacement; no direct numeric comparison to replacement-in-place in this paper, but used consistently in reported CoLAKE fine-tuning experiments.",
            "limitations_or_challenges": "No detailed ablation in the paper; potential token-length increase and ambiguity if not disambiguated; relies on quality of entity linker.",
            "uuid": "e5366.1",
            "source_info": {
                "paper_title": "CoLAKE: Contextualized Language and Knowledge Embedding",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "K-BERT style injection",
            "name_full": "K-BERT triplet injection (as described in Liu et al., 2020)",
            "brief_description": "A related prior approach that injects KG triplets into the input text (typically at fine-tuning time) by inserting triples as additional tokens into the sentence, allowing the LM to attend to explicit knowledge; CoLAKE is compared to and distinguished from this practice.",
            "citation_title": "K-BERT: enabling language representation with knowledge graph",
            "mention_or_use": "mention",
            "representation_name": "Triplet injection into input sequence (fine-tuning-time insertion)",
            "representation_description": "Inject KG triplets directly into the textual input by adding nodes/tokens derived from triplets into the sentence (commonly during fine-tuning rather than pre-training), creating an augmented input sequence that the Transformer processes.",
            "graph_type": "Knowledge graph triplets inserted into sentence sequences",
            "representation_properties": "Enables explicit KG facts to be present in the model context; typically applied at fine-tuning (not pre-training), so model may not learn joint contextualized KG embeddings during base pretraining; simpler to implement but less integrated than joint pretraining approaches.",
            "evaluation_task": "Discussed conceptually; K-BERT evaluated in its own work on downstream tasks (not detailed numerically in this paper).",
            "performance_metrics": null,
            "comparison_to_other_representations": "CoLAKE differs by jointly pretraining language and knowledge (MLM extended over graph nodes) and learning contextualized entity/relation embeddings, whereas K-BERT injects triplets at fine-tuning; CoLAKE argues joint pretraining is more concise and gives contextualized knowledge representations.",
            "limitations_or_challenges": "According to the paper, triplet injection at fine-tuning does not yield joint contextualized KG embeddings and may be less effective than joint pretraining; detailed limitations of K-BERT are not experimentally enumerated here.",
            "uuid": "e5366.2",
            "source_info": {
                "paper_title": "CoLAKE: Contextualized Language and Knowledge Embedding",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Graph-to-text templates (mentioned)",
            "name_full": "Graph-to-text template methods (conceptual mention)",
            "brief_description": "Graph-to-text template methods convert KG facts or graph structures into textual templates (sentences) to be used as training data or prompts for language models; CoLAKE suggests it can be used to measure the quality of such templates.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Graph-to-text templates (template-based textualization)",
            "representation_description": "Use templates to linearize graph facts/triplets into natural-language-like sentences (e.g., 'Harry Potter is the enemy of Lord Voldemort'), which can then be used to train or probe language models. The paper mentions these templates as a potential application area for CoLAKE to assess template quality, but does not describe specific template rules or algorithms.",
            "graph_type": "Knowledge graph triplets / KG subgraphs converted into templated sentences",
            "representation_properties": "Produces human-readable text preserving fact triples; easy to feed into standard language models; template choice affects faithfulness and variety; may lose structured multi-hop graph information unless templates encode it.",
            "evaluation_task": "Suggested use: measuring quality of templates by checking whether CoLAKE preserves original graph structure when encoding such template-derived text; not empirically evaluated in this paper.",
            "performance_metrics": null,
            "comparison_to_other_representations": "No direct comparisons or experimental results provided in this paper; CoLAKE authors propose using their WK-graph-aware model to assess such template quality in future work.",
            "limitations_or_challenges": "Templates can be brittle, may introduce linguistic bias, and may fail to express multi-fact/complex graph structure; paper only mentions templates as an application idea and does not empirically analyze these limitations.",
            "uuid": "e5366.3",
            "source_info": {
                "paper_title": "CoLAKE: Contextualized Language and Knowledge Embedding",
                "publication_date_yy_mm": "2020-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "BERT is not a knowledge base (yet): Factual knowledge vs. name-based reasoning in unsupervised QA",
            "rating": 2
        },
        {
            "paper_title": "K-BERT: enabling language representation with knowledge graph",
            "rating": 2
        },
        {
            "paper_title": "Integrating graph contextualized knowledge into pre-trained language models",
            "rating": 1
        },
        {
            "paper_title": "KEPLER: A unified model for knowledge embedding and pre-trained language representation",
            "rating": 2
        },
        {
            "paper_title": "Commonsense knowledge mining from pretrained models",
            "rating": 2
        },
        {
            "paper_title": "Inducing relational knowledge from BERT",
            "rating": 2
        }
    ],
    "cost": 0.013786499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>CoLAKE: Contextualized Language and Knowledge Embedding</h1>
<p>Tianxiang Sun ${ }^{1, *}$, Yunfan Shao ${ }^{1}$, Xipeng Qiu ${ }^{1, \dagger}$ Qipeng Guo ${ }^{1}$, Yaru Hu ${ }^{1}$, Xuanjing Huang ${ }^{1}$, Zheng Zhang ${ }^{2}$<br>${ }^{1}$ Shanghai Key Laboratory of Intelligent Information Processing, Fudan University<br>${ }^{1}$ School of Computer Science, Fudan University<br>${ }^{2}$ Amazon Shanghai AI Lab<br>{txsun19,yfshao19,xpqiu,qpguo16,xjhuang}@fudan.edu.cn<br>yrhu112358@outlook.com zhaz@amazon.com</p>
<h4>Abstract</h4>
<p>With the emerging branch of incorporating factual knowledge into pre-trained language models such as BERT, most existing models consider shallow, static, and separately pre-trained entity embeddings, which limits the performance gains of these models. Few works explore the potential of deep contextualized knowledge representation when injecting knowledge. In this paper, we propose the Contextualized Language and Knowledge Embedding (CoLAKE), which jointly learns contextualized representation for both language and knowledge with the extended MLM objective. Instead of injecting only entity embeddings, CoLAKE extracts the knowledge context of an entity from large-scale knowledge bases. To handle the heterogeneity of knowledge context and language context, we integrate them in a unified data structure, word-knowledge graph (WK graph). CoLAKE is pre-trained on large-scale WK graphs with the modified Transformer encoder. We conduct experiments on knowledge-driven tasks, knowledge probing tasks, and language understanding tasks. Experimental results show that CoLAKE outperforms previous counterparts on most of the tasks. Besides, CoLAKE achieves surprisingly high performance on our synthetic task called word-knowledge graph completion, which shows the superiority of simultaneously contextualizing language and knowledge representation. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Deep contextualized language models pre-trained on large-scale unlabeled corpora have achieved significant improvement on a wide range of NLP tasks (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). However, they are shown to have difficulty capturing factual knowledge (Logan et al., 2019).</p>
<p>Recently, there is a growing interest in combining pre-trained language models (PLMs) with structured knowledge. A popular approach is to inject pre-trained entity embeddings into PLMs to better capture factual knowledge, such as ERNIE (Zhang et al., 2019) and KnowBERT (Peters et al., 2019). The shortcomings of these models can be summarized as follows: (1) The entity embeddings are separately pre-trained with some knowledge embedding (KE) models (e.g., TransE (Bordes et al., 2013)), and fixed during training PLMs. Thus they are not real joint models to learn the knowledge embedding and language embedding simultaneously. (2) The previous models only take entity embeddings to enhance PLMs, which are hard to fully capture the rich contextual information of an entity in the knowledge graph (KG). Thus their performance gains are limited by the quality of pre-trained entity embeddings. (3) The pre-trained entity embeddings are static and need to be re-trained when the KG is slightly changed.</p>
<p>In this paper, we propose the Contextualized Language And Knowledge Embedding (CoLAKE), which jointly learns language representation and knowledge representation in a common representation space. Different from the previous models, CoLAKE dynamically represents an entity according to its knowledge context and language context. For each entity, CoLAKE considers a sub-graph surrounding</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (a) When injecting knowledge, CoLAKE considers the <em>knowledge context</em> about the entity while previous semi-contextualized joint models only consider a single entity embedding. By contextualizing the entity, CoLAKE is able to directly access (Harry_Potter, <em>enemy of</em>, Lord_Voldemort) to help understand sentence (1) and access (Harry_Potter, <em>mother</em>, Lily_Poter) to help understand sentence (2). (b) The word-knowledge graph (WK graph) is a unified structure to represent both language context and knowledge context, which is composed of two parts: the fully-connected word graph and knowledge sub-graphs extracted from large KGs.</p>
<p>it as its knowledge context that contains the facts (triplets) about the entity. In this way, CoLAKE can dynamically access different facts as background knowledge to help understand the current text. As shown in Figure 1(a), to understand different sentences, CoLAKE can utilize different facts about the linked entity Harry_Potter. The <em>knowledge context</em> of Harry_Potter is a sub-graph containing the triplets about it. According to whether or not to utilize entities' knowledge context, our proposed CoLAKE can be distinguished from previous models, which we call <em>semi-contextualized joint models</em> since they only contextualize language representation.</p>
<p>To deal with the heterogeneous structure of language and KG, we build a graph to integrate them into a unified data structure, called <em>word-knowledge graph (WK graph)</em>. Most recent successful PLMs use Transformer architecture (Vaswani et al., 2017), which treats input sequences as fully-connected word graphs. WK graph is knowledge-augmented word graph. Using entities mentioned in the sentence, we extract sub-graphs centered on those mentioned entities from KGs. Then we mosaic such sub-graphs and the word graph in a unified heterogeneous graph, i.e. WK graph. An instance of the WK graph can be found in Figure 1(b). The constructed WK graph is fed into CoLAKE along with its adjacency matrix to control the information flow to reflect the graph structure. CoLAKE is based on the Transformer encoder, with the embedding layer and the encoder layers slightly modified to adapt to input in the form of WK graph. Besides, we extend the masked language model (MLM) objective (Devlin et al., 2019) to the whole input graph. That is, apply the same masking strategy to word, entity, and relation nodes and training the model to predict the masked nodes based on the rest of the graph.</p>
<p>We evaluate CoLAKE on several knowledge-required tasks and GLUE (Wang et al., 2019a). Experimental results demonstrate that CoLAKE outperforms previous semi-contextualized counterparts on most of the tasks. To explore potential applications of CoLAKE, we design a synthetic task called word-knowledge graph completion. Our evaluation on this task shows that CoLAKE outperforms several KE models by a large margin, in transductive setting and inductive setting.</p>
<p>In summary, CoLAKE can be characterized in three-fold: (1) CoLAKE learns contextualized language representation and contextualized knowledge representation simultaneously with the extended MLM objective. (2) CoLAKE adopts the WK graph to integrate the heterogeneous input for language and knowledge. (3) CoLAKE is essentially a pre-trained graph neural network (GNN), thereby being structure-aware and easy to extend.</p>
<h1>2 Related Work</h1>
<p>Language Representation Learning. The past decade has witnessed the great success of pre-trained language representation. Initially, word representation pre-trained using multi-task objectives (Collobert and Weston, 2008) or co-occurrence statistics (Mikolov et al., 2013; Pennington et al., 2014) are static and non-contextual. Recently, contextualized word representation pre-trained on large-scale unlabeled corpora with deep neural networks has dominated across a wide range of NLP tasks (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019; Qiu et al., 2020).</p>
<p>Knowledge Representation Learning. Knowledge Representation Learning (KRL) is also termed as Knowledge Embedding (KE), which is to map entities and relations into low-dimensional continuous vectors. Most existing methods use triplets as training samples to learn static, non-contextual embeddings for entities and relations (Bordes et al., 2013; Yang et al., 2015; Lin et al., 2015). Recent advances focusing on contextualized representation, which use subgraphs or paths as training samples, have achieved new state-of-the-art results on KG tasks (Wang et al., 2019b; Wang et al., 2020a).</p>
<p>Joint Language and Knowledge Models. Due to the mutual information existing in language and KGs, joint models often benefit both sides. Besides, tasks such as entity linking also require entity embeddings that are compatible with word embeddings. Combining the success of Mikolov et al. (2013) and Bordes et al. (2013), Wang et al. (2014) jointly learn embeddings for language and KG. Targeting mention-entity matching in entity linking, Yamada et al. (2016), Ganea and Hofmann (2017) also proposed joint methods to map entities and words into the same vector space. Inspired by the recent success of contextualized language representation, much effort has been devoted to injecting entity embeddings into PLMs (Zhang et al., 2019; Peters et al., 2019). Despite their success, the knowledge gains are limited by the expressivity of their used pre-trained entity embeddings, which is static and inflexible. In contrast, KEPLER (Wang et al., 2019c) aims to benefit both sides so jointly learn language model and knowledge embedding. However, KEPLER does not directly learn embeddings for each entity but learns to generate entity embeddings with PLMs from entity descriptions. Besides, none of these work exploits the potential of contextualized knowledge representation, which makes them different from our proposed CoLAKE. A brief comparison can be found in Table 1. CoLAKE is conceptually similar to K-BERT (Liu et al., 2020) and BERT-MK (He et al., 2019). CoLAKE differs from K-BERT in that, instead of injecting triplets during fine-tuning, CoLAKE jointly learns embeddings for entities and relations during pretraining LMs. Besides, CoLAKE places language and knowledge representation learning into a unified pre-training task, masked language model, which makes it more concise than BERT-MK. In addition, CoLAKE is a general-purpose joint model while BERT-MK mainly focuses on medical domain.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Joint Models</th>
<th style="text-align: center;">Language <br> Objective</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Knowledge <br> Contextualized?</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Non-contextual</td>
<td style="text-align: left;">Wang et al. (2014)</td>
<td style="text-align: center;">Skip-Gram</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">TransE</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Yamada et al. (2016)</td>
<td style="text-align: center;">Skip-Gram</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">Skip-Gram</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">ERNIE (Zhang et al., 2019)</td>
<td style="text-align: center;">MLM</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">TransE*</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">Semi-contextualized</td>
<td style="text-align: left;">KnowBERT (Peters et al., 2019)</td>
<td style="text-align: center;">MLM</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">KEPLER (Wang et al., 2019c)</td>
<td style="text-align: center;">MLM</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">TransE</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">Contextualized</td>
<td style="text-align: left;">CoLAKE (Ours)</td>
<td style="text-align: center;">MLM</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">MLM</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparison of several joint models based on whether the representation is contextualized. *The entity embeddings are fixed during pre-training ERNIE. KnowBERT does not have restrictions on the entity embeddings. For ERNIE and KnowBERT, we omit the next sentence prediction (NSP) objective.</p>
<h2>3 CoLAKE</h2>
<p>CoLAKE jointly learns contextualized representation for language and knowledge by pre-training on structured, unlabeled word-knowledge graphs (WK graphs). We first introduce how to construct such WK graphs, then we describe the model architecture and the implementation details.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of WK graph construction. The WK graph is an undirected heterogeneous graph. The numbers marked on graph nodes indicate the position index introduced in Section 3.2.</p>
<h1>3.1 Graph Construction</h1>
<p>Typically, language embedding models take sequences as input while knowledge embedding (KE) models take triplets or knowledge sub-graphs as input. Recent successful PLMs take Transformer (Vaswani et al., 2017) as their backbone architecture, which actually processes sequences as fully-connected word graphs. Thus, graph is a common data structure to represent language and knowledge. In this section, we show how to integrate word graph and knowledge sub-graphs into the unified WK graph.</p>
<p>We first tokenize a sentence into a sequence of tokens and fully connect them as a word graph. Then we recognize the mentions in the sentence and use an entity linker to find the corresponding entities in a certain KG. The mention nodes are then replaced by their linked entities, which are called anchor nodes. By this replacement, the model is encouraged to map the injected entities and mention words near one another in the vector space. Centered on the anchor nodes $\left{e_{i}\right}<em i="i">{i}$, we can extract their knowledge contexts $\left{\left{e</em>\right}}, r_{i j}, e_{i j<em i="i">{j}\right}</em>$ to form sub-graphs, in which relations are also transformed into graph nodes. The extracted sub-graphs and the word graph are then concatenated with anchor nodes to obtain the WK graph. Figure 2 shows the process of constructing a WK graph. In practice, for each anchor node we randomly select up to 15 neighboring relations and entities to construct a sub-graph to be injected into the WK graph. We only consider triplets in which anchor node is head (subject) instead of tail (object). In the WK graph, entities are unique but relations are allowed to repeat.</p>
<h3>3.2 Model Architecture</h3>
<p>The constructed WK graphs are then fed into the Transformer (Vaswani et al., 2017) encoder. We modify the embedding and encoder layers of vanilla Transformer to adapt to input in the form of WK graph.</p>
<p>Embedding Layer. The input embedding is the sum of token embedding, type embedding, and position embedding. For token embedding, we maintain three lookup tables for words, entities, and relations respectively. For word embedding, we follow RoBERTa (Liu et al., 2019) which uses Byte-Pair Encoding (BPE) (Sennrich et al., 2016) to transform sequence into subwords units to handle the large vocabulary. In contrast, we directly learn embeddings for each unique entity and relation as common knowledge embedding methods do. The token embeddings are obtained by concatenating word, entity, and relation embeddings, which are of the same dimensionality. There are different types of nodes so the WK graph is heterogeneous. To handle this, we simply use type embedding to indicate the node types, i.e. word, entity, and relation. For position embedding, we need to assign each injected entity and relation a position index. Inspired by Liu et al. (2020), we adopt the soft-position index which allows repeated position indices and keeps tokens in the same triplet continuous. Figure 2 shows an intuitive example of how to assign position index to graph nodes.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Overall architecture of CoLAKE. In this case, three triplets, (Harry_Potter, mother, Lily_Potter), (Harry_Potter, spouse, Ginny_Weasley), and (Harry_Potter, enemy of, Lord_Voldemort) are injected into the raw sequence. The model is asked to predict the masked word wand, the masked entity Lily_Potter, and the masked relation enemy of.</p>
<p>Masked Transformer Encoder. We use masked multi-head self-attention to control the information flow to reflect the structure of WK graph. Given the representation of graph nodes $\mathbf{X} \in \mathbb{R}^{n \times d}$, where $n$ is the number of nodes and $d$ is the dimension for each node, the representation after masked self-attention is obtained by</p>
<p>$$
\begin{aligned}
\mathbf{Q}, \mathbf{K}, \mathbf{V} &amp; =\mathbf{X} \mathbf{W}^{Q}, \mathbf{X} \mathbf{W}^{K}, \mathbf{X} \mathbf{W}^{V} \
\mathbf{A} &amp; =\frac{\mathbf{Q K}^{\top}}{\sqrt{d_{k}}} \
\operatorname{Attn}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) &amp; =\operatorname{Softmax}(\mathbf{A}+\mathbf{M}) \mathbf{V}
\end{aligned}
$$</p>
<p>where $\mathbf{W}^{Q}, \mathbf{W}^{K}, \mathbf{W}^{V} \in \mathbb{R}^{d \times d_{k}}$ are learnable parameters. $\mathbf{M} \in \mathbb{R}^{n \times n}$ is the mask matrix given by</p>
<p>$$
\mathbf{M}<em i="i">{i j}= \begin{cases}0 &amp; \text { if } x</em>
$$} \text { and } x_{j} \text { are connected } \ -\inf &amp; \text { if } x_{i} \text { and } x_{j} \text { are disconnected }\end{cases</p>
<p>With the masked Transformer encoder, each node can only gather information from its 1-hop neighbor at each layer. Masked Transformer encoder works similar to GAT (Velickovic et al., 2018).</p>
<h1>3.3 Pre-Training Objective</h1>
<p>The Masked Language Model (MLM) objective is to randomly mask some of tokens from the input and train the model to predict the original vocabulary id of the masked tokens based on their contexts. In this section, we extend the MLM from word sequences to WK graphs.</p>
<p>In particular, we mask $15 \%$ of graph nodes at random. When a node is masked, we replace it with (1) the [MASK] token $80 \%$ of time, (2) a randomly sampled node with the same type as the original node $10 \%$ of time, (3) the unchanged node $10 \%$ of time. As different types of nodes are masked, we encourage CoLAKE to learn different aspects of capabilities:</p>
<ul>
<li>
<p>Masking word nodes. When words are masked, the objective is similar to traditional MLM. The difference is, CoLAKE can predict masked words based on not only the context words but also the entities and relations in the WK graph. Masking words helps CoLAKE learn linguistic knowledge.</p>
</li>
<li>
<p>Masking entity nodes. If the masked entity is an anchor node, the objective, which is to predict the anchor node based on its context, helps to align the representation spaces of language and knowledge. Take the instance in Figure 1(b), the embedding of entity Harry_Potter will be learned to be similar to its textual form, Harry Potter. If the masked entity is not an anchor node, the MLM objective is similar to that used in semantic matching-based KE methods such as ConvE (Dettmers et al., 2018) and CoKE (Wang et al., 2019b), which enables CoLAKE to learn a large number of entity embeddings. Masking entity nodes helps CoLAKE (a) map words and entities into a common representation space, and (b) learn contextualized representation for entities.</p>
</li>
<li>Masking relation nodes. If the masked relation is between two unique anchor nodes, the objective is similar to distantly supervised relation extraction (Craven and Kumlien, 1999), which requires the model to classify the relationship between two entities mentioned in the text. Otherwise, the objective is to predict the relationship between its two neighboring entities, which is similar to traditional KE methods. Masking relation nodes helps CoLAKE (a) learn to do relation extraction, and (b) learn contextualized representation for relations.</li>
</ul>
<p>However, the pre-training task of predicting masked anchor nodes could be trivial because the model is easy to accomplish this task only based on the knowledge context instead of the language context, which is more varied than knowledge context. To mitigate this, we discard neighbors of anchor nodes in $50 \%$ of time during pre-training.</p>
<h1>3.4 Model Training</h1>
<p>CoLAKE is trained with cross-entropy loss. We use three classification heads to predict three types of nodes. In practice, however, the large number of entities brings challenges to training and predicting.</p>
<p>Mixed CPU-GPU Training. Due to the large number of entities in KG, training the whole model in GPU is intractable. To handle this, we asynchronously update entity embeddings in CPU memory while keeping the rest of our model updated in GPU. In particular, we store and update entity embeddings in CPU memory which is shared among multiple trainer processes. During pre-training, the trainer processes read the entity embeddings from the shared CPU memory and write the gradients back to CPU. Our implementation is based on the distributed key-value store (KVStore) from Zheng et al. (2020).</p>
<p>Negative Sampling. Applying the Softmax function to the huge number of entities is very timeconsuming. CoLAKE uses negative sampling to conduct prediction for each entity over one positive entity and $k(k \ll n)$ negative entities instead of all $n$ entities in KG. Following Mikolov et al. (2013), we sample negative entities from the $3 / 4$ powered entity frequency distribution.</p>
<h2>4 Experiments</h2>
<p>In this section, we present the details of pre-training and fine-tuning CoLAKE, and its experimental results on knowledge-driven tasks, knowledge probing tasks, and language understanding tasks.</p>
<h3>4.1 Pre-Training Data and Implementation Details</h3>
<p>CoLAKE uses English Wikipedia (2020/03/01) ${ }^{2}$ as pre-training data and uses WikiExtractor ${ }^{3}$ to process the downloaded Wikipedia dump. We use Wikipedia anchors to align text to Wikidata5M (Wang et al., 2019c), which is a newly proposed large-scale KG containing 21M fact triplets. We construct WK graphs as training samples and filter out graph samples without entity nodes and relation nodes. Finally, CoLAKE pre-trained the Transformer encoder along with 3,085,345 entity embeddings and 822 relation embeddings on 26 M training samples.</p>
<p>The Transformer encoder of CoLAKE is initialized with RoBERTa ${ }<em _BASE="{BASE" _text="\text">{\text {BASE }}$ (Liu et al., 2019). We use the implementation from HuggingFace's Transformer (Wolf et al., 2019). The entity embeddings and relation embeddings are initialized with the average of the RoBERTa ${ }</em>$ BPE embeddings of entity}</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>and relation aliases provided by Wang et al. (2019c). AdamW with $\beta_{1}=0.9, \beta_{2}=0.98$ is used in pre-training. We train CoLAKE with the batch size of 2048 and the learning rate of 1e-4 for 1 epoch. For each anchor node, we sample $k=200$ negative entities. CoLAKE is trained on 832 G NVIDIA V100 GPUs for 38 hours.</p>
<h1>4.2 Knowledge-Driven Tasks</h1>
<p>We first fine-tune and evaluate CoLAKE on knowledge-driven tasks. To annotate entities in the sentence, we use TAGME (Ferragina and Scaiella, 2010) to link mentions to entities in KGs. Instead of replacing the textual mention with its symbolic entity, we follow Pörner et al. (2019) and concatenate the two forms of tokens, e.g. Jean Mara ##is Jean_Marais. Concretely, we conduct experiments on two knowledge-driven tasks: entity typing and relation extraction.</p>
<p>Entity Typing. The entity typing task is to classify the semantic type of a given entity mention based on its surface form and context. We add two special tokens, [ENT] and [/ENT], before and after the entity mentions to be classified and use the final representation of the [CLS] token as the feature to conduct classification ${ }^{4}$. We evaluate CoLAKE on Open Entity (Choi et al., 2018). To compare with ERNIE, KnowBERT, and KEPLER, we adopt the same experiment setting which considers nine general types. To be consistent with previous work, we adopt micro precision, recall, and F1 score as evaluation metrics. The experimental results are shown in Table 2.</p>
<p>Relation Extraction. The relation extraction task is to classify the relationship between two entities mentioned in a given sentence. During fine-tuning, we add four special tokens, [HD], [/HD], [TL] and [/TL] to identify the head entity and the tail entity. Also, we use the final representation of the [CLS] token as the feature to be fed into the classifier. We evaluate CoLAKE on FewRel (Han et al., 2018) that is rearranged by Zhang et al. (2019). Since FewRel is built with Wikidata, we discard triplets in the FewRel test set from pre-training data to avoid information leakage. Following previous work, we report macro precision, recall and F1 score on FewRel. The experimental results can be found in Table 2.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Open Entity</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">FewRel</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F</td>
</tr>
<tr>
<td style="text-align: left;">BERT (Devlin et al., 2019)</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">84.9</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa (Liu et al., 2019)</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">85.3</td>
</tr>
<tr>
<td style="text-align: left;">ERNIE (Zhang et al., 2019)</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">88.3</td>
</tr>
<tr>
<td style="text-align: left;">KnowBERT (Peters et al., 2019)</td>
<td style="text-align: center;">$\mathbf{7 8 . 6}$</td>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER (Wang et al., 2019c)</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">E-BERT (Pörner et al., 2019)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">88.5</td>
</tr>
<tr>
<td style="text-align: left;">CoLAKE (Ours)</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">$\mathbf{7 5 . 7}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 4}$</td>
<td style="text-align: center;">$\mathbf{9 0 . 6}$</td>
<td style="text-align: center;">$\mathbf{9 0 . 6}$</td>
<td style="text-align: center;">$\mathbf{9 0 . 5}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Experimental results on Open Entity and FewRel.</p>
<h3>4.3 Knowledge Probing</h3>
<p>LAMA (LAnguage Model Analysis) probe (Petroni et al., 2019) aims to measure factual knowledge stored in language models via cloze-style statement like: Dante was born in [MASK]. Subsequently, a more "factual" subset of LAMA, LAMA-UHN (Pörner et al., 2019), is constructed by filtering out easy-to-answer samples. We evaluate CoLAKE on these two probes and report the mean precision at one (P@1) macro-averaged over relations.</p>
<p>For fair comparision, we use the intersection of the vocabularies for all considered models and construct a common vocabulary of $\sim 18 \mathrm{~K}$ case-sensitive tokens. In this experiment, considered models include ELMo (Peters et al., 2018), ELMo5.5B (Peters et al., 2018), BERT ${ }<em _BASE="{BASE" _text="\text">{\text {BASE }}$ (Devlin et al., 2019), RoBERTa ${ }</em>$ (Liu et al., 2019), and K-Adapter (Wang et al., 2020b).}</p>
<p>The results of LAMA and LAMA-UHN are shown in Table 3. It is worth noticing that BERT outperforms RoBERTa by a large margin. Wang et al. (2020b) reported the same phenomenon in their</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Corpus</th>
<th style="text-align: center;">Pre-trained Models</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">ELMo</td>
<td style="text-align: center;">ELMo5.5B</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">CoLAKE</td>
<td style="text-align: center;">K-Adapter*</td>
</tr>
<tr>
<td style="text-align: left;">LAMA-Google-RE</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">11.4</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">9.5</td>
<td style="text-align: center;">7.0</td>
</tr>
<tr>
<td style="text-align: left;">LAMA-UHN-Google-RE</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">5.7</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">3.7</td>
</tr>
<tr>
<td style="text-align: left;">LAMA-T-REx</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">29.1</td>
</tr>
<tr>
<td style="text-align: left;">LAMA-UHN-T-REx</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">23.0</td>
</tr>
</tbody>
</table>
<p>Table 3: P@1 on LAMA and LAMA-UHN. *K-Adapter is based on RoBERTa ${ }<em _BASE="{BASE" _text="\text">{\text {LARGE }}$ while other Transformer-based LMs are of BASE size. Besides, K-Adapter uses a subset of T-REx as its training data, which may contribute to its superiority over CoLAKE on LAMA-T-REx and LAMA-UHN-T-REx.
paper. We conjecture that the main reason behind this is the larger and byte-level BPE vocabulary used by RoBERTa. Though, CoLAKE outperforms its baseline, RoBERTa ${ }</em>$, by a significant margin. Besides, CoLAKE even improves over the LARGE size of model, K-Adapter, by $2.5 \%$ and $1.2 \%$ on LAMA-Google-RE and LAMA-UHN-Google-RE respectively.}</p>
<h1>4.4 Language Understanding Tasks</h1>
<p>We also evaluate CoLAKE on the General Language Understanding Evaluation (GLUE) (Wang et al., 2019a), which provides a collection of diverse NLU tasks. Since these tasks require little factual knowledge, we attempt to explore whether CoLAKE degenerates the performance on these NLU tasks.</p>
<p>The experimental results on GLUE dev set are shown in Table 4. CoLAKE is slightly degraded from RoBERTa but improves over KEPLER by $1.4 \%$ on average. We conclude that CoLAKE is able to simultaneously model text and knowledge via the heterogeneous WK graph. In summary, our experiments demonstrate that CoLAKE significantly improves the performance on knowledge-required tasks and at the same time achieves comparable results on language understanding tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">MNLI (m/mm)</th>
<th style="text-align: center;">QQP</th>
<th style="text-align: center;">QNLI</th>
<th style="text-align: center;">SST-2</th>
<th style="text-align: center;">CoLA</th>
<th style="text-align: center;">STS-B</th>
<th style="text-align: center;">MRPC</th>
<th style="text-align: center;">RTE</th>
<th style="text-align: center;">AVG.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RoBERTa</td>
<td style="text-align: center;">$87.5 / 87.3$</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">92.8</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">91.2</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">86.4</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER</td>
<td style="text-align: center;">$87.2 / 86.5$</td>
<td style="text-align: center;">91.5</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">94.4</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">89.4</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">84.9</td>
</tr>
<tr>
<td style="text-align: left;">CoLAKE</td>
<td style="text-align: center;">$87.4 / 87.2$</td>
<td style="text-align: center;">92.0</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">90.8</td>
<td style="text-align: center;">90.9</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">86.3</td>
</tr>
</tbody>
</table>
<p>Table 4: GLUE results on dev set. Both of KEPLER and CoLAKE are initialized with RoBERTa ${ }_{\text {BASE }}$.</p>
<h3>4.5 Word-Knowledge Graph Completion</h3>
<p>Note that CoLAKE is essentially a GNN pre-trained on large-scale WK graphs, which makes it structureaware and easy to generalize to unseen entities.</p>
<p>To probe CoLAKE's capability of modeling both structural and semantic features, we design a task named word-knowledge graph completion. In particular, we use the FewRel test set to construct two experimental settings: transductive setting and inductive setting. In both settings, each sample provides a triplet $(h, r, t)$ and a sentence that expresses the triplet. The considered models are required to predict the relation $r$. For each sample in transductive setting, the two entities, $h$ and $t$, and their relation $r$ are seen in training phase. But the triplet $(h, r, t)$ has not appeared in the training data. We collect 10 K samples from the FewRel test set to construct the transductive setting. For each sample in inductive setting, at least one entity is unseen during training. This setting requires the model to be inductive so that it can generalize to unseen entities. We collect 1 K samples from the FewRel test set to construct the inductive setting. We directly evaluate CoLAKE in the two settings without further training on the FewRel training set. The forms of word-knowledge graph are depicted in Figure 4. For inductive setting, we encourage CoLAKE to infer the unseen entity by aggregating messages from its neighbors.</p>
<p>We take several well-known models for link prediction as our baselines ${ }^{5}$. For transductive setting, we compare CoLAKE with four widely-used models, i.e. TransE (Bordes et al., 2013), DistMult (Yang et</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Illustration of the input word-knowledge graph for transductive setting and inductive setting. In transductive setting, Harry_Potter and Lord_Voldemort are seen during training. In inductive setting, Harry_Potter is unknown but its neighboring entities are seen in training data.
al., 2015), ComplEx (Trouillon et al., 2016), and RotatE (Sun et al., 2019). We use DGL-KE (Zheng et al., 2020) to train the four baseline models on Wikidata $5 \mathrm{M}^{6}$. For inductive setting, we take DKRL (Xie et al., 2016) as our baseline. As shown in Table 5, CoLAKE outperforms other models by a large margin thanks to its capability of simultaneously utilizing structural knowledge and rich text semantics while traditional KE models can only handle structural knowledge. Besides, the inductive ability of CoLAKE is more realistic. Unlike DKRL and KEPLER, which generate entity embeddings from descriptions, CoLAKE generates entity embeddings based on their neighbors.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">MR $\downarrow$</th>
<th style="text-align: center;">MRR</th>
<th style="text-align: center;">HITS@1</th>
<th style="text-align: center;">HITS@3</th>
<th style="text-align: center;">HITS@10</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Transductive setting</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">TransE (Bordes et al., 2013)</td>
<td style="text-align: center;">15.97</td>
<td style="text-align: center;">67.30</td>
<td style="text-align: center;">60.28</td>
<td style="text-align: center;">70.96</td>
<td style="text-align: center;">79.75</td>
</tr>
<tr>
<td style="text-align: left;">DistMult (Yang et al., 2015)</td>
<td style="text-align: center;">27.09</td>
<td style="text-align: center;">60.56</td>
<td style="text-align: center;">48.66</td>
<td style="text-align: center;">69.69</td>
<td style="text-align: center;">79.61</td>
</tr>
<tr>
<td style="text-align: left;">ComplEx (Trouillon et al., 2016)</td>
<td style="text-align: center;">26.73</td>
<td style="text-align: center;">61.09</td>
<td style="text-align: center;">49.80</td>
<td style="text-align: center;">70.64</td>
<td style="text-align: center;">79.78</td>
</tr>
<tr>
<td style="text-align: left;">RotatE (Sun et al., 2019)</td>
<td style="text-align: center;">30.36</td>
<td style="text-align: center;">70.90</td>
<td style="text-align: center;">64.74</td>
<td style="text-align: center;">74.89</td>
<td style="text-align: center;">81.05</td>
</tr>
<tr>
<td style="text-align: left;">CoLAKE</td>
<td style="text-align: center;">2.03</td>
<td style="text-align: center;">82.48</td>
<td style="text-align: center;">72.14</td>
<td style="text-align: center;">92.19</td>
<td style="text-align: center;">98.58</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Inductive setting</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">DKRL (Xie et al., 2016)</td>
<td style="text-align: center;">168.21</td>
<td style="text-align: center;">8.18</td>
<td style="text-align: center;">5.03</td>
<td style="text-align: center;">7.28</td>
<td style="text-align: center;">14.13</td>
</tr>
<tr>
<td style="text-align: left;">CoLAKE</td>
<td style="text-align: center;">31.01</td>
<td style="text-align: center;">28.10</td>
<td style="text-align: center;">15.69</td>
<td style="text-align: center;">30.28</td>
<td style="text-align: center;">58.05</td>
</tr>
</tbody>
</table>
<p>Table 5: The experimental results on word-knowledge graph completion task.</p>
<h1>5 Conclusion</h1>
<p>In this paper, we propose CoLAKE to jointly learn contextualized representation for language and knowledge. We integrate the language context and knowledge context in a unified data structure, wordknowledge graph. The experimental results show the effectiveness of CoLAKE on knowledge-required tasks. Besides, to explore the potential application of the WK graph, we design a task named WK graph completion, which shows that CoLAKE is essentially a powerful GNN that is structure-aware and inductive. The surprisingly high performance on WK graph completion inspires the potential applications of WK graph, for example, (a) CoLAKE may help to denoise distantly annotated samples of relation extraction (Craven and Kumlien, 1999; Mintz et al., 2009), (b) CoLAKE can be used to measure the quality of graph-to-text templates (Davison et al., 2019; Bouraoui et al., 2020) due to its capability of preserving the original graph structure. We leave these applications as future work.</p>
<h2>Acknowledgements</h2>
<p>This work was supported by the National Key Research and Development Program of China (No. 2020AAA0106700) and National Natural Science Foundation of China (No. 62022027 and 61976056).</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>References</h1>
<p>Antoine Bordes, Nicolas Usunier, Alberto García-Durán, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi-relational data. In NIPS.</p>
<p>Zied Bouraoui, José Camacho-Collados, and Steven Schockaert. 2020. Inducing relational knowledge from BERT. In AAAI, pages 7456-7463.</p>
<p>Eunsol Choi, Omer Levy, Yejin Choi, and Luke Zettlemoyer. 2018. Ultra-fine entity typing. In ACL, pages 87-96.
Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: deep neural networks with multitask learning. In ICML, pages 160-167.</p>
<p>Mark Craven and Johan Kumlien. 1999. Constructing biological knowledge bases by extracting information from text sources. In ISMB, pages 77-86.</p>
<p>Joe Davison, Joshua Feldman, and Alexander M. Rush. 2019. Commonsense knowledge mining from pretrained models. In EMNLP-IJCNLP, pages 1173-1178.</p>
<p>Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. 2018. Convolutional 2d knowledge graph embeddings. In AAAI, pages 1811-1818.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, pages 4171-4186.</p>
<p>Paolo Ferragina and Ugo Scaiella. 2010. TAGME: on-the-fly annotation of short text fragments (by wikipedia entities). In CIKM, pages 1625-1628.</p>
<p>Octavian-Eugen Ganea and Thomas Hofmann. 2017. Deep joint entity disambiguation with local neural attention. In EMNLP, pages 2619-2629.</p>
<p>Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, and Maosong Sun. 2018. Fewrel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation. In EMNLP, pages $4803-4809$.</p>
<p>Bin He, Di Zhou, Jinghui Xiao, Xin Jiang, Qun Liu, Nicholas Jing Yuan, and Tong Xu. 2019. Integrating graph contextualized knowledge into pre-trained language models. CoRR, abs/1912.00147.</p>
<p>Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning entity and relation embeddings for knowledge graph completion. In AAAI, pages 2181-2187.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692.</p>
<p>Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, and Ping Wang. 2020. K-BERT: enabling language representation with knowledge graph.</p>
<p>Robert L. Logan, Nelson F. Liu, Matthew E. Peters, Matt Gardner, and Sameer Singh. 2019. Barack’s wife hillary: Using knowledge graphs for fact-aware language modeling. In ACL, pages 5962-5971.</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111-3119.</p>
<p>Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In $A C L$, pages 1003-1011.</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In EMNLP, pages 1532-1543.</p>
<p>Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In NAACL-HLT, pages 2227-2237.</p>
<p>Matthew E. Peters, Mark Neumann, Robert L. Logan IV, Roy Schwartz, Vidur Joshi, Sameer Singh, and Noah A. Smith. 2019. Knowledge enhanced contextual word representations. In EMNLP-IJCNLP, pages 43-54.</p>
<p>Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander H. Miller. 2019. Language models as knowledge bases? In EMNLP-IJCNLP, pages 2463-2473.</p>
<p>Nina Pörner, Ulli Waltinger, and Hinrich Schütze. 2019. BERT is not a knowledge base (yet): Factual knowledge vs. name-based reasoning in unsupervised QA. CoRR, abs/1911.03681.</p>
<p>Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang. 2020. Pre-trained models for natural language processing: A survey. CoRR, abs/2003.08271.</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In $A C L$.</p>
<p>Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. 2019. Rotate: Knowledge graph embedding by relational rotation in complex space. In $I C L R$.</p>
<p>Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, and Guillaume Bouchard. 2016. Complex embeddings for simple link prediction. In ICML.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NIPS, pages 5998-6008.</p>
<p>Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2018. Graph attention networks. In $I C L R$.</p>
<p>Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge graph and text jointly embedding. In EMNLP, pages 1591-1601.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019a. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In $I C L R$.</p>
<p>Quan Wang, Pingping Huang, Haifeng Wang, Songtai Dai, Wenbin Jiang, Jing Liu, Yajuan Lyu, Yong Zhu, and Hua Wu. 2019b. Coke: Contextualized knowledge graph embedding. CoRR, abs/1911.02168.</p>
<p>Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2019c. KEPLER: A unified model for knowledge embedding and pre-trained language representation. CoRR, abs/1911.06136.</p>
<p>Hongwei Wang, Hongyu Ren, and Jure Leskovec. 2020a. Entity context and relational paths for knowledge graph completion. CoRR, abs/2002.06757.</p>
<p>Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Jianshu Ji, Guihong Cao, Daxin Jiang, and Ming Zhou. 2020b. K-adapter: Infusing knowledge into pre-trained models with adapters. CoRR, abs/2002.01808.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R'emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface's transformers: State-of-theart natural language processing. ArXiv, abs/1910.03771.</p>
<p>Ruobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan, and Maosong Sun. 2016. Representation learning of knowledge graphs with entity descriptions. In $A A A I$.</p>
<p>Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda, and Yoshiyasu Takefuji. 2016. Joint learning of the embedding of words and entities for named entity disambiguation. In CoNLL, pages 250-259.</p>
<p>Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2015. Embedding entities and relations for learning and inference in knowledge bases. In $I C L R$.</p>
<p>Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In NeurIPS, pages 5754-5764.</p>
<p>Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019. ERNIE: enhanced language representation with informative entities. In $A C L$, pages 1441-1451.</p>
<p>Da Zheng, Xiang Song, Chao Ma, Zeyuan Tan, Zihao Ye, Jin Dong, Hao Xiong, Zheng Zhang, and George Karypis. 2020. DGL-KE: training knowledge graph embeddings at scale. CoRR, abs/2004.08532.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ The triplets in FewRel test set are removed from Wikidata5M to avoid information leakage.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6381 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6381</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6381</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-273403906</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.12853v1.pdf" target="_blank">Diversity of Thought Elicits Stronger Reasoning Capabilities in Multi-Agent Debate Frameworks</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) excel in natural language generation but often confidently produce incorrect responses, especially in tasks like mathematical reasoning. Chain-of-thought prompting, self-verification, and multi-agent debate are among the strategies proposed to improve the reasoning and factual accuracy of LLMs. Building on Du et al.'s multi-agent debate framework, we find that multi-agent debate helps at any model scale, and that diversity of thought elicits stronger reasoning in debating LLMs. Across various model sizes, performance on mathematical reasoning tasks benefits most when diverse trained models are used. Remarkably, after 4 rounds of debate, a diverse set of medium-capacity models (Gemini-Pro, Mixtral 7BX8, and PaLM 2-M) outperforms GPT-4 on the GSM-8K benchmark, scoring 91% accuracy. By comparison, when 3 instances of Gemini-Pro are used, performance only reaches 82%. Finally, this diverse set of medium-capacity models sets a new state-of-the-art performance on the ASDiv benchmark (94%). These results underscore the idea that the future of AI is agentic, with diverse cooperating agents yielding emergent capabilities beyond even the most powerful individual models.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6381.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6381.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diverse-medium GSM-8K (debate)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-agent debate with a diverse medium-capacity ensemble on GSM-8K</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multi-agent debate using three diverse medium-capacity LLMs (Gemini-Pro, Mixtral 7B×8, PaLM 2-M) with a Gemini‑Pro summarizer and 4 iterative debate rounds produced large gains on GSM-8K, reportedly reaching 91% accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-Pro; Mixtral 7B×8; PaLM 2-M (ensemble, multi-agent debate)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer-based LLMs (various families combined)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>medium-capacity (Mixtral 7B×8, PaLM 2-M, Gemini-Pro described as medium-capacity)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM-8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step math word problems (grade-school arithmetic / reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems; model prompts configured to provide a boxed answer at the end of each response; iterative summarized response fed back between rounds</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school multi-step problems (GSM-8K standard difficulty)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>multi-agent iterative debate with a dedicated response summarizer (Gemini-Pro); baseline round0 direct generation; experiments run with and without chain-of-thought prompting (COT) across different setups</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>91% accuracy after 4 rounds of debate (reported improvement from 78% baseline for the lead model in that setup)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Only qualitative internal analysis reported: models changed answers across rounds (e.g., Mixtral shifted its stance by round 3 and provided self-explanations). No low-level mechanistic probes (attention, activation or logit‑lens) were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>No systematic token-level numeric failure modes analyzed; observed failure pattern: homogeneous influence and teacher/student interference can reduce a strong model's performance (see teacher-student experiments). Homogeneous ensembles produced smaller gains.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Gains from multi-agent debate observed at medium scale; the authors emphasize that diversity (different architectures) rather than sheer size produced the strongest gains in this configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Diversity of Thought Elicits Stronger Reasoning Capabilities in Multi-Agent Debate Frameworks', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6381.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6381.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Homogeneous Gemini-Pro GSM-8K</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-agent debate with three Gemini-Pro instances on GSM-8K</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Homogeneous multi-agent debate using three instances of Gemini‑Pro produced smaller improvements compared to a diverse ensemble; zero-shot chain-of-thought gave an additional boost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>3 × Gemini‑Pro (homogeneous ensemble)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer-based LLM (Gemini family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>medium-capacity (Gemini‑Pro instances)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM-8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language questions with boxed answer formatting</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school multi-step</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>multi-agent debate; comparison of with/without chain-of-thought (zero-shot COT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Improved from 78% to 80% without COT and to 82% with zero-shot COT after debate rounds (as reported)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No mechanistic probes; results contrasted qualitatively with diverse-ensemble case to show smaller gains when models are homogeneous.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Smaller improvements relative to diverse setups; homogeneous agents tended to converge on similar (sometimes incorrect) reasoning paths limiting error correction.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Homogeneous ensembles showed limited additional benefit from debate relative to diverse ensembles; chain-of-thought provided modest extra gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Diversity of Thought Elicits Stronger Reasoning Capabilities in Multi-Agent Debate Frameworks', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6381.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6381.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diverse ASDiv (debate)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-agent debate with Gemini Flash 1.5, Gemini Pro, and GPT-3.5 on ASDiv</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diverse three-model debate (Gemini Flash 1.5, Gemini Pro, GPT‑3.5) with 4 rounds and Gemini‑Pro summarizer achieved a state-of-the-art reported accuracy on ASDiv.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini Flash 1.5; Gemini Pro; GPT-3.5 (ensemble)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer-based LLMs (multiple families)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Gemini Flash 1.5 (medium/flash variant), Gemini Pro (medium), GPT-3.5 (medium-ish pre-trained model)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ASDiv</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>diverse elementary school math word problems (multi-step)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems; boxed answer formatting; iterative summarization between rounds</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>elementary-school problems (ASDiv annotation includes problem type and grade level)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>multi-agent debate with a response summarizer (4 rounds)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>94% accuracy after 4 rounds of debate (reported), compared to individual model accuracies of 89% (Gemini Flash 1.5), 86% (Gemini Pro), and 81% (GPT-3.5).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Qualitative observation of collaborative improvement; no detailed internal mechanistic analysis provided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not explicitly characterized; implicit failure mode when ensemble members are homogeneous or lack diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Diversity across architectures produced large gains even when individual models were already strong on the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Diversity of Thought Elicits Stronger Reasoning Capabilities in Multi-Agent Debate Frameworks', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6381.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6381.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diverse MATH (debate)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-agent debate with Gemini Flash 1.5, Gemini Pro, and GPT-3.5 on the MATH benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The same diverse ensemble applied to the MATH benchmark showed large relative improvements and reportedly outperformed GPT‑4 and Gemini Ultra by reported margins at round 4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini Flash 1.5; Gemini Pro; GPT-3.5 (ensemble)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer-based LLMs (various families)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Gemini Flash 1.5 (medium), Gemini Pro (medium), GPT-3.5 (medium-ish)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>competition-level mathematics problems with step-by-step solutions (algebra, number theory, geometry, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language presentation of competition math problems with expected stepwise answers; boxed answer evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>hard / competition-level</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>multi-agent debate with iterative summarization (4 rounds)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (and relative improvement vs baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Individual accuracies reported as 55% (Gemini Flash 1.5), 32% (Gemini Pro), 33% (GPT-3.5); by round 4 the multi-agent debate framework reportedly outperformed GPT-4 by 24% and Gemini Ultra by 14% (absolute final accuracy value not explicitly provided in the text).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No mechanistic probes; authors report only aggregate performance improvements and qualitative behavior across rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not analyzed in token-level detail; heterogeneous behavior across rounds and datasets suggests some configurations can destabilize stronger models when paired with much weaker ones.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Debate yields large relative improvements on a hard benchmark; diversity yields gains beyond what single larger models achieved in reported comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Diversity of Thought Elicits Stronger Reasoning Capabilities in Multi-Agent Debate Frameworks', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6381.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6381.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scaling sweep (0.75B–100B+)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scaling experiment across model sizes on GSM-8K (multiple sizes from ~0.75B to 100B+)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sweep across model scales showed that multi-agent debate improves reasoning performance across all model sizes tested, suggesting the debate benefit is not solely an emergent property of very large models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple models across sizes (0.75B to 100B+)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer-based LLMs (various families)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>range: 0.75B to 100B+ (as reported in scaling experiment figure)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM-8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems with boxed answer prompts</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school multi-step</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>multi-agent debate (with and without chain-of-thought variants); baseline round0 direct generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Authors report 'similar performance gains across all model scales' (no single-size emergent threshold reported); specific small-scale examples: 7B diverse ensemble +17% by round 4, 2B diverse ensemble +10% by round 4.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No mechanistic analysis; results emphasize that debate benefits are robust across scales and that architectural diversity is a key driver.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>None mechanistically decomposed; smaller models still improved but absolute performance remains lower than larger-capacity settings.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Improvements from debate are present at small (2B), medium (7B), and large (>70B) scales; authors conclude debate efficacy is not strictly tied to model size and that diversity matters more than raw scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Diversity of Thought Elicits Stronger Reasoning Capabilities in Multi-Agent Debate Frameworks', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6381.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6381.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Teacher–Student behaviour</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Teacher–student dynamics in heterogeneous-capacity multi-agent debate</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When mixing very strong and much weaker models, the stronger 'teacher' model's accuracy can drop during debate while weaker 'student' models improve, occasionally reducing ensemble performance during active debate.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Examples: Mixtral 7Bx8 (strong) with Gemma 7B and Qwen1.5 7B; also Gemma7B (teacher) with Gemma2B and TinyLlama 1.1B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer-based LLMs (various)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>mixed: strong model Mixtral 7Bx8; students 7B and smaller (2B, 1.1B)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM-8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems; boxed-answer prompts; summary aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school multi-step</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>multi-agent debate with iterative summarization; also evaluated the 'summary-only' (no debate) aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example reported behavior: summary-only aggregation (no rounds) achieved 69% accuracy; after debate begins, ensemble accuracy dropped to 66% in that heterogeneous mixture; strong model Mixtral's individual performance dropped >20% upon debate start then partially recovered by round 2 or 3.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Qualitative interpretability: authors hypothesize a 'teaching' dynamic where weaker models learn from the strong model but the strong model is influenced (pulled) by weaker models' incorrect reasoning; no neuron-level probes reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>When model capacities are mismatched, debate can degrade the strong model's performance (information contagion of errors), producing net performance decrease versus simple aggregation; instability in early rounds for heterogeneous groups.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Effect is tied to heterogeneity of capacities: strong-single + weaker partners can cause temporary degradation in strong model performance while improving weaker ones; authors recommend similar-size diversity for best debate effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Diversity of Thought Elicits Stronger Reasoning Capabilities in Multi-Agent Debate Frameworks', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Improving factuality and reasoning in language models through multiagent debate <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Large language models are better reasoners with self-verification <em>(Rating: 1)</em></li>
                <li>Sparks of artificial general intelligence: Early experiments with gpt-4 <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6381",
    "paper_id": "paper-273403906",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "Diverse-medium GSM-8K (debate)",
            "name_full": "Multi-agent debate with a diverse medium-capacity ensemble on GSM-8K",
            "brief_description": "Multi-agent debate using three diverse medium-capacity LLMs (Gemini-Pro, Mixtral 7B×8, PaLM 2-M) with a Gemini‑Pro summarizer and 4 iterative debate rounds produced large gains on GSM-8K, reportedly reaching 91% accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemini-Pro; Mixtral 7B×8; PaLM 2-M (ensemble, multi-agent debate)",
            "model_family": "transformer-based LLMs (various families combined)",
            "model_size": "medium-capacity (Mixtral 7B×8, PaLM 2-M, Gemini-Pro described as medium-capacity)",
            "training_data_description": null,
            "benchmark_name": "GSM-8K",
            "task_type": "multi-step math word problems (grade-school arithmetic / reasoning)",
            "problem_format": "natural-language word problems; model prompts configured to provide a boxed answer at the end of each response; iterative summarized response fed back between rounds",
            "difficulty_level": "grade-school multi-step problems (GSM-8K standard difficulty)",
            "prompting_method": "multi-agent iterative debate with a dedicated response summarizer (Gemini-Pro); baseline round0 direct generation; experiments run with and without chain-of-thought prompting (COT) across different setups",
            "performance_metric": "accuracy",
            "performance_value": "91% accuracy after 4 rounds of debate (reported improvement from 78% baseline for the lead model in that setup)",
            "internal_analysis": "Only qualitative internal analysis reported: models changed answers across rounds (e.g., Mixtral shifted its stance by round 3 and provided self-explanations). No low-level mechanistic probes (attention, activation or logit‑lens) were reported.",
            "failure_modes": "No systematic token-level numeric failure modes analyzed; observed failure pattern: homogeneous influence and teacher/student interference can reduce a strong model's performance (see teacher-student experiments). Homogeneous ensembles produced smaller gains.",
            "scaling_trend": "Gains from multi-agent debate observed at medium scale; the authors emphasize that diversity (different architectures) rather than sheer size produced the strongest gains in this configuration.",
            "uuid": "e6381.0",
            "source_info": {
                "paper_title": "Diversity of Thought Elicits Stronger Reasoning Capabilities in Multi-Agent Debate Frameworks",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Homogeneous Gemini-Pro GSM-8K",
            "name_full": "Multi-agent debate with three Gemini-Pro instances on GSM-8K",
            "brief_description": "Homogeneous multi-agent debate using three instances of Gemini‑Pro produced smaller improvements compared to a diverse ensemble; zero-shot chain-of-thought gave an additional boost.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "3 × Gemini‑Pro (homogeneous ensemble)",
            "model_family": "transformer-based LLM (Gemini family)",
            "model_size": "medium-capacity (Gemini‑Pro instances)",
            "training_data_description": null,
            "benchmark_name": "GSM-8K",
            "task_type": "multi-step math word problems",
            "problem_format": "natural-language questions with boxed answer formatting",
            "difficulty_level": "grade-school multi-step",
            "prompting_method": "multi-agent debate; comparison of with/without chain-of-thought (zero-shot COT) prompting",
            "performance_metric": "accuracy",
            "performance_value": "Improved from 78% to 80% without COT and to 82% with zero-shot COT after debate rounds (as reported)",
            "internal_analysis": "No mechanistic probes; results contrasted qualitatively with diverse-ensemble case to show smaller gains when models are homogeneous.",
            "failure_modes": "Smaller improvements relative to diverse setups; homogeneous agents tended to converge on similar (sometimes incorrect) reasoning paths limiting error correction.",
            "scaling_trend": "Homogeneous ensembles showed limited additional benefit from debate relative to diverse ensembles; chain-of-thought provided modest extra gains.",
            "uuid": "e6381.1",
            "source_info": {
                "paper_title": "Diversity of Thought Elicits Stronger Reasoning Capabilities in Multi-Agent Debate Frameworks",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Diverse ASDiv (debate)",
            "name_full": "Multi-agent debate with Gemini Flash 1.5, Gemini Pro, and GPT-3.5 on ASDiv",
            "brief_description": "A diverse three-model debate (Gemini Flash 1.5, Gemini Pro, GPT‑3.5) with 4 rounds and Gemini‑Pro summarizer achieved a state-of-the-art reported accuracy on ASDiv.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemini Flash 1.5; Gemini Pro; GPT-3.5 (ensemble)",
            "model_family": "transformer-based LLMs (multiple families)",
            "model_size": "Gemini Flash 1.5 (medium/flash variant), Gemini Pro (medium), GPT-3.5 (medium-ish pre-trained model)",
            "training_data_description": null,
            "benchmark_name": "ASDiv",
            "task_type": "diverse elementary school math word problems (multi-step)",
            "problem_format": "natural-language word problems; boxed answer formatting; iterative summarization between rounds",
            "difficulty_level": "elementary-school problems (ASDiv annotation includes problem type and grade level)",
            "prompting_method": "multi-agent debate with a response summarizer (4 rounds)",
            "performance_metric": "accuracy",
            "performance_value": "94% accuracy after 4 rounds of debate (reported), compared to individual model accuracies of 89% (Gemini Flash 1.5), 86% (Gemini Pro), and 81% (GPT-3.5).",
            "internal_analysis": "Qualitative observation of collaborative improvement; no detailed internal mechanistic analysis provided.",
            "failure_modes": "Not explicitly characterized; implicit failure mode when ensemble members are homogeneous or lack diversity.",
            "scaling_trend": "Diversity across architectures produced large gains even when individual models were already strong on the benchmark.",
            "uuid": "e6381.2",
            "source_info": {
                "paper_title": "Diversity of Thought Elicits Stronger Reasoning Capabilities in Multi-Agent Debate Frameworks",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Diverse MATH (debate)",
            "name_full": "Multi-agent debate with Gemini Flash 1.5, Gemini Pro, and GPT-3.5 on the MATH benchmark",
            "brief_description": "The same diverse ensemble applied to the MATH benchmark showed large relative improvements and reportedly outperformed GPT‑4 and Gemini Ultra by reported margins at round 4.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemini Flash 1.5; Gemini Pro; GPT-3.5 (ensemble)",
            "model_family": "transformer-based LLMs (various families)",
            "model_size": "Gemini Flash 1.5 (medium), Gemini Pro (medium), GPT-3.5 (medium-ish)",
            "training_data_description": null,
            "benchmark_name": "MATH",
            "task_type": "competition-level mathematics problems with step-by-step solutions (algebra, number theory, geometry, etc.)",
            "problem_format": "natural-language presentation of competition math problems with expected stepwise answers; boxed answer evaluation",
            "difficulty_level": "hard / competition-level",
            "prompting_method": "multi-agent debate with iterative summarization (4 rounds)",
            "performance_metric": "accuracy (and relative improvement vs baselines)",
            "performance_value": "Individual accuracies reported as 55% (Gemini Flash 1.5), 32% (Gemini Pro), 33% (GPT-3.5); by round 4 the multi-agent debate framework reportedly outperformed GPT-4 by 24% and Gemini Ultra by 14% (absolute final accuracy value not explicitly provided in the text).",
            "internal_analysis": "No mechanistic probes; authors report only aggregate performance improvements and qualitative behavior across rounds.",
            "failure_modes": "Not analyzed in token-level detail; heterogeneous behavior across rounds and datasets suggests some configurations can destabilize stronger models when paired with much weaker ones.",
            "scaling_trend": "Debate yields large relative improvements on a hard benchmark; diversity yields gains beyond what single larger models achieved in reported comparisons.",
            "uuid": "e6381.3",
            "source_info": {
                "paper_title": "Diversity of Thought Elicits Stronger Reasoning Capabilities in Multi-Agent Debate Frameworks",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Scaling sweep (0.75B–100B+)",
            "name_full": "Scaling experiment across model sizes on GSM-8K (multiple sizes from ~0.75B to 100B+)",
            "brief_description": "A sweep across model scales showed that multi-agent debate improves reasoning performance across all model sizes tested, suggesting the debate benefit is not solely an emergent property of very large models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple models across sizes (0.75B to 100B+)",
            "model_family": "transformer-based LLMs (various families)",
            "model_size": "range: 0.75B to 100B+ (as reported in scaling experiment figure)",
            "training_data_description": null,
            "benchmark_name": "GSM-8K",
            "task_type": "multi-step math word problems",
            "problem_format": "natural-language word problems with boxed answer prompts",
            "difficulty_level": "grade-school multi-step",
            "prompting_method": "multi-agent debate (with and without chain-of-thought variants); baseline round0 direct generation",
            "performance_metric": "accuracy",
            "performance_value": "Authors report 'similar performance gains across all model scales' (no single-size emergent threshold reported); specific small-scale examples: 7B diverse ensemble +17% by round 4, 2B diverse ensemble +10% by round 4.",
            "internal_analysis": "No mechanistic analysis; results emphasize that debate benefits are robust across scales and that architectural diversity is a key driver.",
            "failure_modes": "None mechanistically decomposed; smaller models still improved but absolute performance remains lower than larger-capacity settings.",
            "scaling_trend": "Improvements from debate are present at small (2B), medium (7B), and large (&gt;70B) scales; authors conclude debate efficacy is not strictly tied to model size and that diversity matters more than raw scale.",
            "uuid": "e6381.4",
            "source_info": {
                "paper_title": "Diversity of Thought Elicits Stronger Reasoning Capabilities in Multi-Agent Debate Frameworks",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Teacher–Student behaviour",
            "name_full": "Teacher–student dynamics in heterogeneous-capacity multi-agent debate",
            "brief_description": "When mixing very strong and much weaker models, the stronger 'teacher' model's accuracy can drop during debate while weaker 'student' models improve, occasionally reducing ensemble performance during active debate.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Examples: Mixtral 7Bx8 (strong) with Gemma 7B and Qwen1.5 7B; also Gemma7B (teacher) with Gemma2B and TinyLlama 1.1B",
            "model_family": "transformer-based LLMs (various)",
            "model_size": "mixed: strong model Mixtral 7Bx8; students 7B and smaller (2B, 1.1B)",
            "training_data_description": null,
            "benchmark_name": "GSM-8K",
            "task_type": "multi-step math word problems",
            "problem_format": "natural-language word problems; boxed-answer prompts; summary aggregation",
            "difficulty_level": "grade-school multi-step",
            "prompting_method": "multi-agent debate with iterative summarization; also evaluated the 'summary-only' (no debate) aggregation",
            "performance_metric": "accuracy",
            "performance_value": "Example reported behavior: summary-only aggregation (no rounds) achieved 69% accuracy; after debate begins, ensemble accuracy dropped to 66% in that heterogeneous mixture; strong model Mixtral's individual performance dropped &gt;20% upon debate start then partially recovered by round 2 or 3.",
            "internal_analysis": "Qualitative interpretability: authors hypothesize a 'teaching' dynamic where weaker models learn from the strong model but the strong model is influenced (pulled) by weaker models' incorrect reasoning; no neuron-level probes reported.",
            "failure_modes": "When model capacities are mismatched, debate can degrade the strong model's performance (information contagion of errors), producing net performance decrease versus simple aggregation; instability in early rounds for heterogeneous groups.",
            "scaling_trend": "Effect is tied to heterogeneity of capacities: strong-single + weaker partners can cause temporary degradation in strong model performance while improving weaker ones; authors recommend similar-size diversity for best debate effectiveness.",
            "uuid": "e6381.5",
            "source_info": {
                "paper_title": "Diversity of Thought Elicits Stronger Reasoning Capabilities in Multi-Agent Debate Frameworks",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Improving factuality and reasoning in language models through multiagent debate",
            "rating": 2,
            "sanitized_title": "improving_factuality_and_reasoning_in_language_models_through_multiagent_debate"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Large language models are better reasoners with self-verification",
            "rating": 1,
            "sanitized_title": "large_language_models_are_better_reasoners_with_selfverification"
        },
        {
            "paper_title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "rating": 1,
            "sanitized_title": "sparks_of_artificial_general_intelligence_early_experiments_with_gpt4"
        }
    ],
    "cost": 0.012224249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Diversity of Thought Elicits Stronger Reasoning Capabilities in Multi-Agent Debate Frameworks
10 Oct 2024</p>
<p>Mahmood Hegazy mahmood.hegazy@umontreal.ca 
University of Montreal Mila -Quebec AI Institute</p>
<p>Diversity of Thought Elicits Stronger Reasoning Capabilities in Multi-Agent Debate Frameworks
10 Oct 202459F449F0BB0DA3917ED891C26C89F99FarXiv:2410.12853v1[cs.CL]
Large language models (LLMs) excel in natural language generation but often confidently produce incorrect responses, especially in tasks like mathematical reasoning.Chain-of-thought prompting, self-verification, and multi-agent debate are among the strategies proposed to improve the reasoning and factual accuracy of LLMs.Building on Du et al.'s multi-agent debate framework, we find that multi-agent debate helps at any model scale, and that diversity of thought elicits stronger reasoning in debating LLMs.Across various model sizes, performance on mathematical reasoning tasks benefits most when diverse trained models are used.Remarkably, after 4 rounds of debate, a diverse set of medium-capacity models (Gemini-Pro, Mixtral 7B×8, and PaLM 2-M) outperforms GPT-4 on the GSM-8K benchmark, scoring 91% accuracy.By comparison, when 3 instances of Gemini-Pro are used, performance only reaches 82%.Finally, this diverse set of medium-capacity models sets a new state-of-the-art performance on the ASDiv benchmark (94%).These results underscore the idea that the future of AI is agentic, with diverse cooperating agents yielding emergent capabilities beyond even the most powerful individual models.Preprint.Under review.</p>
<p>Introduction</p>
<p>In the dynamic realm of artificial intelligence, enhancing the reasoning abilities and factual accuracy of large language models (LLMs) stands as a pivotal challenge.Central to this pursuit is the exploration of innovative methodologies that address existing shortcomings and chart new pathways for advancement.Our research aims to fortify the foundations of LLMs through the lens of multi-agent debate.</p>
<p>The key motivation behind this project is solving the issue of "hallucination" within language models, where plausible yet erroneous information is generated, undermining their reliability and trustworthiness.Inspired by the collaborative nature of human intellectual discourse, the methodology of multi-agent debate emerges as a promising solution to this problem.By harnessing the collective insights of multiple AI agents engaged in structured debate, we seek to not only mitigate hallucinations but also elevate the precision and reliability of LLM responses.</p>
<p>A glance at the landscape of current research reveals a series of endeavors aimed at fortifying the reasoning capabilities of LLMs.While recent iterations of LLMs, such as GPT-4, represent significant strides forward, concerns persist regarding their reasoning capabilities.In response to these limitations, Du et al. [2023] advocates the transformative potential of multi-agent debate.Furthermore, advancements in agentic approaches such as MetaGPT, as proposed by Hong et al.</p>
<p>[2023], and Agentverse, as proposed by Chen et al. [2023], offer diverse perspectives, delving into collaborative problem-solving and simulation of human behavior, thus broadening the horizons of LLM research.Our work aims to mitigate serious LLM deficiencies in reasoning by offering a nuanced analysis of existing multi-agent debate frameworks and their effectiveness in enhancing reasoning in LLMs.Building upon the insight from previous studies, our approach emphasizes the importance of diversity in models and debate strategies (diversity of thought).By synthesizing findings from diverse benchmarks and methodologies, we provide a comprehensive understanding of the strengths and limitations of current frameworks.</p>
<p>To empirically validate the effectiveness of our approach, we performed comprehensive experiments utilizing both diverse and homogeneous sets of language models with varying capacities.These experiments were conducted on multiple mathematical reasoning benchmarks, including the challenging GSM-8K dataset and the recently introduced ASDiv benchmark, which assess the models' ability to generate accurate and well-reasoned solutions to complex problems.Our results demonstrate that leveraging diversity of thought in multi-agent debate significantly enhances the reasoning capabilities of LLMs, outperforming even state-of-the-art models like GPT-4.These findings underscore the potential of diverse, cooperating agents in achieving emergent capabilities beyond individual models.</p>
<p>Related work</p>
<p>The landscape of large language models (LLMs) has witnessed remarkable advancements in recent years, exemplified by innovations such as GPT-4 (OpenAI [2023]), Llama (Touvron et al. [2023]), and PaLM (Chowdhery et al. [2023]).However, despite these breakthroughs, a critical examination reveals significant concerns regarding the reasoning capabilities of LLMs, as highlighted by Arkoudas [2023].This recognition has motivated a series of research efforts aimed at enhancing the reasoning and problem-solving abilities of LLMs through various methodologies.</p>
<p>Approaches like chain-of-thought prompting, self-verification, and multi-agent debate, as introduced by Wei et al. [2022], Weng et al. [2023] and Du et al. [2023] respectively, have been proposed to address this challenge.The multi-agent debate approach, inspired by Minsky [1988]'s "Society of Mind" theory, posits that diverse agents approaching a problem with different methods, purposes, knowledge representations, and result-production techniques can enhance factual accuracy through debate and communication.Similarly, Hong et al. [2023] introduced MetaGPT, a meta-programming framework designed to tackle logic inconsistencies and hallucination by incorporating Standardized Operating Procedures (SOPs) and structured communication within LLM-based multi-agent systems.</p>
<p>In parallel, efforts have been directed towards enhancing the generative capabilities of LLMs to simulate believable human behavior, as studied by Park et al. [2023]; Wang et al. [2023]; Yao et al. [2022].These endeavors, while successful in creating realistic simulations, have also prompted further exploration into refining retrieval modules and real-time interactivity to mitigate instances of hallucination.</p>
<p>Furthermore, frameworks such as Agentverse, as proposed by Chen et al. [2023], prioritize collaborative problem-solving among autonomous agents, emulating human group dynamics to achieve superior performance across diverse tasks.This emphasis on collaborative reasoning sets a precedent Collectively, these studies underscore the imperative to address the reasoning deficits of LLMs while also exploring avenues for enhancing their generative capabilities and facilitating collaborative problem-solving.Our work builds upon these foundations, leveraging the power of multi-agent debate and diversity of thought to push the boundaries of LLM reasoning and pave the way for more reliable and capable language models.</p>
<p>Methodology</p>
<p>Our multi-agent debate framework for enhancing the mathematical reasoning capabilities of LLMs broadened the scope of the framework, as introduced by Du et al. [2023], to ensure it was compatible with diverse models architectures.As illustrated in Figure 3, it consists of the following key components:</p>
<ol>
<li>
<p>Question Encoding: The mathematical question or problem is provided as input to the system.This question serves as the starting point for the debate among the participating models.</p>
</li>
<li>
<p>Debating Models: Three diverse language models -Model 1, Model 2, and Model 3 -are employed as the debating agents.These models can be chosen to have different architectures.</p>
</li>
</ol>
<p>We utilized this architecture to run experiments with and without diverse models.</p>
<ol>
<li>
<p>Debate Rounds: The debating models engage in multiple rounds of debate, where each model generates a response to the question based on its own reasoning capabilities.In each round, the models take turns providing their responses.</p>
</li>
<li>
<p>Response Summarization: After each round, the responses from all three debating models are passed through a fourth model -Model 4 (Response Summarizer).This model's role is to analyze and summarize the key arguments, reasoning steps, and conclusions presented by the debating models.The summarized response captures the most salient and convincing points from the debate round.Our model of choice for response summarization was Gemini-Pro for all experiments.</p>
</li>
<li>
<p>Iterative Refinement: The summarized response from Model 4 is then fed back as input to the debating models for the next round of debate.This iterative process allows the models to build upon each other's reasoning, identify and correct errors, and refine their arguments based on the collective insights generated in the previous rounds.</p>
</li>
<li>
<p>Final Summary: After a predefined number of debate rounds (n), the final summarized response from the summarizer model (Model 4) is considered as the output of the multi-agent debate framework.This final summary represents the consolidated reasoning and conclusion arrived at through the iterative debate process.Here we can extract what the mode of the answers of the 3 models was.The debate framework leverages the diversity of the participating models to explore different reasoning paths, challenge assumptions, and arrive at more robust and accurate conclusions.By encouraging the models to critically examine and build upon each other's arguments, the framework aims to mitigate the limitations of individual models and enhance the overall mathematical reasoning capabilities of the system.</p>
</li>
</ol>
<p>Datasets</p>
<p>To empirically evaluate the effectiveness of our multi-agent debate framework, we employ a diverse set of benchmarks that assess various aspects of language understanding and mathematical reasoning.</p>
<p>• GSM-8K (Cobbe et al. [2021]): This benchmark comprises 8.5K linguistically diverse grade school math word problems, making it ideal for evaluating multi-step mathematical reasoning.</p>
<p>• Academia Sinica Diverse MWP Dataset (ASDiv) (Miao et al. [2021]): ASDiv features 2,306 diverse math word problems covering various language patterns and problem types encountered in elementary school.It includes annotations for problem type and grade level.</p>
<p>• MATH (Enderton [2001]): This historically challenging dataset provides 12,500 competition mathematics problems, each accompanied by step-by-step solutions.It facilitates the teaching of answer derivations and explanations.</p>
<p>Through extensive experiments, we analyze the impact of model diversity, debate round count, and model size on reasoning performance.The results provide insights into the optimal configuration of the multi-agent debate framework for achieving superior mathematical reasoning capabilities compared to individual models.</p>
<p>By leveraging these diverse datasets, we aim to comprehensively assess the effectiveness of our approach in enhancing the reasoning capabilities of LLMs across a wide range of problem types, difficulty levels, and language patterns.This rigorous evaluation enables us to draw meaningful conclusions about the potential of multi-agent debate in advancing the state-of-the-art in language understanding and mathematical reasoning.</p>
<p>Experiments</p>
<p>To validate the effectiveness of our multi-agent debate framework, we conducted a series of experiments using diverse and homogeneous sets of language models with varying capacities.These experiments were performed on multiple mathematical reasoning benchmarks, as outlined in sec- tion 3.1, to assess the models' ability to generate accurate and well-reasoned solutions to complex problems.Our two main goals of this study were as follows:</p>
<ol>
<li>
<p>Explore the relationship between model capacity and reasoning performance in the context of multi-agent debate.</p>
</li>
<li>
<p>Investigate the impact of model diversity on the reasoning performance of the multi-agent debate framework.</p>
</li>
</ol>
<p>Baseline To establish a fair and consistent baseline for comparison, we begin by asking each agent to directly generate responses to the given prompts without engaging in debate.This initial response generation serves as round 0 of the debate process and allows us to analyze the performance of the individual models before the collaborative reasoning begins.</p>
<p>By evaluating the models' standalone performance in round 0, we can effectively measure the impact of the subsequent debate rounds on the overall reasoning capabilities of the system.This baseline assessment is crucial for understanding the extent to which the multi-agent debate framework enhances the models' ability to generate accurate and well-reasoned solutions.</p>
<p>To ensure the validity and reliability of our comparisons, we maintain a consistent experimental setup across all evaluations.We use identical starting prompts and language models for both the baseline and the multi-agent debate framework approaches.This consistency eliminates potential confounding factors and enables us to attribute any observed improvements in performance to the effectiveness of the debate process itself.</p>
<p>Evaluation methods To facilitate a comprehensive evaluation of our multi-agent debate framework, we designed a systematic approach to assess the model outputs at each round of the debate.We configured the model prompts to consistently provide a boxed answer at the conclusion of each response, ensuring a standardized format for evaluation.</p>
<p>Following each experiment, we generate a JSON file that encapsulates the model outputs at each round of the debate for every question in the dataset.This structured data allows for a granular analysis of the framework's performance throughout the debate process.Our evaluation script iterates through the debate rounds, assessing the accuracy and quality of the framework's responses at Furthermore, we visualize the performance of the framework across rounds, as exemplified in Figure 2.This visual representation provides valuable insights into the progression of reasoning quality as the debate unfolds, allowing us to identify patterns, convergence points, and potential limitations of the approach.</p>
<p>Effect of model capacity on performance</p>
<p>While previous work by Du et al. [2023] explored the effectiveness of multi-agent debate by varying the number of agents and the number of debate rounds, they did not investigate the effect of model scale on the framework's performance.Considering the findings of Wei et al. [2022], which demonstrated that chain-of-thought (COT) reasoning is an emergent ability that arises with increased model size, we recognized the importance of conducting a similar experiment for multi-agent debate.</p>
<p>To address this gap in the literature, we performed a scaling experiment on the GSM-8K dataset to examine the relationship between model capacity and the performance of the multi-agent debate framework.We evaluated the framework's performance with and without COT reasoning across a range of model sizes, from small-scale models to large-scale ones.The results of our experiment, as presented in Figure 5, revealed a surprising finding.Contrary to the expectation that multi-agent debate performance would emerge as a direct consequence of increasing model size, we observed similar performance gains across all model scales.This result suggests that the effectiveness of multi-agent debate in enhancing reasoning capabilities is not solely dependent on the model's capacity.</p>
<p>Our findings have significant implications for the development and deployment of multi-agent debate systems.They indicate that even smaller-scale models can benefit from the collaborative reasoning process facilitated by the debate framework, without the need for resource-intensive large-scale models.This insight opens up new possibilities for implementing multi-agent debate in resourceconstrained environments and facilitates the widespread adoption of this approach.</p>
<p>Furthermore, our experiment highlights the importance of considering factors beyond model size when designing and optimizing multi-agent debate systems.It suggests that the effectiveness of the debate process may be influenced by other aspects, such as the diversity of the participating models, the structure of the debate, and the quality of the response summarization.</p>
<p>Diversity of thought</p>
<p>Building upon the previous experiment, we aimed to investigate the impact of model diversity on the performance of the multi-agent debate framework.To introduce diversity, we replicated the study using three models of similar capacity but featuring diverse model families.).When tested individually, these models achieved accuracy rates of 78%, 64%, and 70%, respectively, on the benchmark.Remarkably, the accuracy of the framework improved significantly, rising from 78% to an impressive 91% after 4 rounds of debate, as shown in Figure 2.This substantial improvement outperforms GPT-4 (Bubeck et al. [2023]), and highlights the power of collaborative reasoning and the synergistic effect of diverse perspectives.</p>
<p>To further emphasize the importance of diversity, we compared these results to a homogeneous setup consisting of 3 Gemini Pro models.In the homogeneous case, the performance improved from 78% to 80% without chain-of-thought (COT) reasoning and to 82% with zero-shot COT, as depicted in Figure 2.While still an improvement, the gains in the homogeneous setup were notably lower than those observed in the diverse model configuration.</p>
<p>These findings strongly suggest that the models in the diverse setup greatly benefited from the debate process, leveraging the unique reasoning approaches of their counterparts to refine and enhance their responses.The synergistic effect of combining models with different architectures and capabilities underscores the crucial role of collective insight in boosting overall performance.</p>
<p>Our study thus highlights a key insight: within the multi-agent debate framework, diversity is a critical driver of success.By fostering collaboration among models with complementary strengths, the framework enables the emergence of novel reasoning patterns and more accurate solutions to complex problems.</p>
<p>Qualitative Result: Figure 4 provides an illustrative example of the dynamics that unfolded during the multi-agent debate experiment.In this particular instance, we observe that Mixtral, one of the participating models, initially maintained its original answer for the first two rounds of the debate.However, by the third round, Mixtral's stance began to shift as it carefully considered the reasoning put forth by the other two models.This pivotal moment in the debate showcases the power of diverse perspectives in challenging and refining the models' understanding of the problem at hand.Mixtral, before fully adapting its reasoning to align with the collective insights, took a step back to articulate the rationale behind its initial divergent answer.This act of self-explanation not only adds transparency to the debate process but also highlights the model's ability to engage in metacognitive reflection.By acknowledging its initial reasoning and subsequently integrating the persuasive arguments presented by its counterparts, Mixtral demonstrates the capacity for growth and learning within the multi-agent debate framework.This qualitative analysis offers a glimpse into the rich interplay of ideas and the collaborative knowledge construction that emerges when diverse models engage in structured debate.More of this can be found in the appendix.</p>
<p>To further validate the effectiveness of the multi-agent debate framework with diverse models, we conducted additional experiments on the ASDiv and MATH benchmarks.For these experiments, we employed three diverse models: Gemini Flash 1.5, Gemini Pro, and GPT 3.5.On the ASDiv benchmark, the individual performance of these models was already impressive, with Gemini Flash 1.5, Gemini Pro, and GPT 3.5 achieving accuracy rates of 89%, 86%, and 81%, respectively.However, when these models were combined in our multi-agent debate framework, the results were even more remarkable.After 4 rounds of debate, the framework reached an accuracy of 94%, setting a new Similarly, on the challenging MATH benchmark, the individual models achieved accuracy rates of 55%, 32%, and 33% for Gemini Flash 1.5, Gemini Pro, and GPT 3.5, respectively.When engaged in multi-agent debate, the framework's performance significantly improved.By the 4th round of debate, the framework outperformed both GPT-4 and Gemini Ultra by substantial margins of 24% and 14%, respectively, as observed in Figure 8.</p>
<p>These results further underscore the power of diverse models in the multi-agent debate framework.By leveraging the unique strengths and perspectives of each model, the framework is able to achieve remarkable performance gains, pushing the boundaries of what is possible on these challenging benchmarks.</p>
<p>Moreover, to investigate whether diversity of thought is an emergent ability that arises with model scale, we conducted the same diversity experiment on GSM-8K using smaller models.The results were quite notable.Whether using 7B models (Gemma 7B, Mistral 7B, and Llama 2 7B) or 2B models (Gemma 2B, Qwen 2B, and Rho 1B), diversity of thought consistently elicited enhanced reasoning capabilities among these smaller models.As shown in Figures 6 and 7, the 7B model framework achieved a significant performance increase of 17% by the 4th round, while the 2B model framework saw a 10% improvement.These findings challenge our initial hypothesis that the framework's initial competency in the specific benchmark would be crucial for facilitating a productive debate.Instead, our results suggest that the critical requirement for an effective debate is the presence of diverse model architectures of similar capacity, which induces learning and enhances reasoning capabilities.</p>
<p>The consistent performance improvements observed across different model scales highlight the robustness and generalizability of the multi-agent debate framework.The effectiveness of the approach is not limited to large-scale models but extends to smaller models as well, provided that architectural diversity is maintained.This finding has significant implications for the development and deployment of multi-agent debate systems, as it demonstrates the potential for achieving enhanced reasoning capabilities even with resource-constrained models.</p>
<p>Conclusion</p>
<p>In this paper, we have presented a comprehensive investigation into the effectiveness of multi-agent debate in enhancing the reasoning capabilities and factual accuracy of large language models (LLMs).By building upon the foundational work of Du et al. [2023], we have developed an advanced framework that leverages the power of diverse models and iterative refinement to push the boundaries of what is possible in collaborative reasoning and problem-solving.</p>
<p>Our experiments on a range of challenging benchmarks, including GSM-8K, ASDiv, and MATH, have consistently demonstrated the remarkable performance gains achieved by the multi-agent debate framework.The diversity of thought inherent in the framework, brought about by the inclusion of Through the iterative process of debate, the models are able to learn from each other, refine their reasoning, and converge on more accurate and robust solutions.</p>
<p>The framework's ability to outperform even state-of-the-art models like GPT-4 and set new records on established benchmarks underscores its potential to revolutionize the field of AI.By harnessing the collective intelligence of diverse models, we have shown that it is possible to achieve emergent capabilities that surpass those of individual models, even the most advanced ones.</p>
<p>Moreover, the consistent effectiveness of the multi-agent debate framework across different model scales and datasets highlights its robustness and generalizability.This versatility opens up exciting possibilities for the application of the framework in various domains, from education and research to industry and beyond.</p>
<p>As we look to the future, our findings suggest that the key to unlocking the full potential of AI lies in fostering collaboration and diversity.By embracing the power of multi-agent systems and encouraging the development of models with complementary strengths, we can continue to push the boundaries of what is possible and address increasingly complex challenges.</p>
<p>In conclusion, our work represents a significant step forward in the quest to enhance the reasoning capabilities and factual accuracy of LLMs.The multi-agent debate framework, with its emphasis on diversity and iterative refinement, offers a promising path towards the development of more reliable, trustworthy, and capable AI systems.As we continue to explore and refine this approach, we invite the research community to build upon our findings and join us in shaping the future of collaborative agenitc AI.This appendix presents a deeper dive into the debates, providing supplementary analysis and visual representations to enhance understanding.</p>
<p>A.1 Supplementary analysis.Teacher -Student behaviour appearing in framework with diverse capacities</p>
<p>Given the previous result of diversity of thought, we sought to better understand what is truly happening in the mutliagent debate framework.We tested the framework while allowing one of the 3 models to be of higher capacity to observe if the smaller models can be taught at faster pace.The first experiment was done by using 2 small models (Gemma 7B and Qwen1.5 7B) and a third of higher capacity; Mixtral 7Bx8.The final outcomes are presented in Figure 9a, where the effectiveness of the multiagent debate framework is showcased.Remarkably, without any rounds of debate and simply by adopting the summary answer of the three models, we achieve the best performance compared to the individual models at 69% accuracy.However, surprisingly, the framework's performance does degrade to 66% as the models debate.Looking at the performance of individual models in a vacuum we can start to understand why.The 2 models that start at sub-30% accuracy, Gemma7B and Qwen1.5 7B, mark some impressive performance gains, with Gemma7B in particular punching above it's weight and getting more than half the questions correctly by the 3rd round.However, Mixtral, which notably was performing very well in this benchmark as expected et al. [2024], decreased it's performance by more than 20% as soon as debate started but started to regain performance on the 2nd round.One hypothesis for the observed behavior suggests that the superior model, Mixtral, functions akin to a teacher for the two lesser models.These models learn and adapt throughout the debate process.Meanwhile, the "teacher" model's performance diminishes as it becomes influenced by the lesser models.</p>
<p>To confirm this hypothesis, a similar experiment was made where Gemma7B was made to be the "teacher" as it was coupled with smaller inferior models; Gemma2B and TinyLlama (1.1B parameter Llama-based architecture model introduced by Zhang et al. [2024]).The final outcomes are presented in Figure 9b.Here we observe similar patterns to what we observed in Figure 9a and confirm our hypothesis.While the, now "teacher", Gemma7B decreases its performance as it is influenced by the 2 less capable models, the other models increase their performance as debate continues.With this finding we can adjust our previous theory to be that for a debate to be effective, the crucial requirement is that the participating models must have diverse architectures of similar sizes.</p>
<p>Figure 1 :
1
Figure 1: Diverse Model Debate Performance Across 4 rounds on the ASDiv benchmark</p>
<p>Figure 2 :
2
Figure 2: Diverse Model Debate Performance Across 4 rounds on the GSM-8K benchmark</p>
<p>Figure 3 :
3
Figure 3: Multi Agent Debate Framework Architecture</p>
<p>Figure 4 :
4
Figure 4: Illustration of the debate procedure.</p>
<p>Figure 5 :
5
Figure 5: Debate Framework Performance across Model Scales 0.75B to 100B+ on GSM8K</p>
<p>Figure 6 :
6
Figure 6: 7B Diverse Models Debate Performance Across 4 rounds on GSM8K Dataset</p>
<p>Figure 7 :
7
Figure 7: 2B Diverse Models Debate Performance Across 4 rounds on GSM8K Dataset</p>
<p>Figure 8 :
8
Figure 8: Diverse Model Debate Performance Across 4 rounds on the MATH benchmark</p>
<p>Figure 9 :
9
Figure 9: Illustration of Teacher -Student behaviour in debate framework on the GSM8K Benchmark</p>
<p>Konstantine Arkoudas, arXiv:2308.03762Gpt-4 can't reason. 2023arXiv preprint</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, Sparks of artificial general intelligence: Early experiments with gpt-4. 2023</p>
<p>Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, arXiv:2308.10848Facilitating multi-agent collaboration and exploring emergent behaviors in agents. 2023arXiv preprint</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Journal of Machine Learning Research. 242402023</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, Igor Mordatch, arXiv:2305.14325Rohan Anil et al. Palm 2 technical report. Academic Press2023. 2001. 2023arXiv preprintHerbert B. Enderton. A Mathematical Introduction to Logic</p>
<p>Albert Q Jiang, arXiv:2308.00352Meta programming for multi-agent collaborative framework. 2023arXiv preprint</p>
<p>Mathematical reasoning with thought expansion. Jb, Hazel Kim, Joonghyuk Kim, Yo-Sub Hahn, Han, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingAthenaAssociation for Computational Linguistics2023</p>
<p>A diverse corpus for evaluating and developing english math word problem solvers. Shen-Yun, Chao-Chun Miao, Keh-Yih Liang, Su, 2021</p>
<p>Society of mind. Marvin Minsky, 1988Simon and Schuster</p>
<p>Openai, arxiv 2303.08774Gpt-4 technical report. 20232</p>
<p>Generative agents: Interactive simulacra of human behavior. Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. the 36th Annual ACM Symposium on User Interface Software and Technology2023</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv:2305.162912023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Large language models are better reasoners with self-verification. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, Jun Zhao, 2023</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022arXiv preprint</p>
<p>Tinyllama: An open-source small language model. Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, Wei Lu, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2342 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2342</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2342</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-63.html">extraction-schema-63</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <p><strong>Paper ID:</strong> paper-270738175</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.17812v1.pdf" target="_blank">Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars</a></p>
                <p><strong>Paper Abstract:</strong> In a post-ChatGPT world, this paper explores the potential of leveraging scalable artificial intelligence for scientific discovery. We propose that scaling up artificial intelligence on high-performance computing platforms is essential to address such complex problems. This perspective focuses on scientific use cases like cognitive simulations, large language models for scientific inquiry, medical image analysis, and physics-informed approaches. The study outlines the methodologies needed to address such challenges at scale on supercomputers or the cloud and provides exemplars of such approaches applied to solve a variety of scientific problems.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2342.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2342.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PINNs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Physics-Informed Neural Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neural networks trained with loss terms that penalize PDE residuals and enforce boundary/initial conditions to produce solutions of differential equations consistent with known physics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Partial differential equations / physics-based simulation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Solve forward and inverse problems governed by PDEs where data are sparse or where solutions must satisfy conservation laws and boundary/initial conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Often limited; scientific PDE problems may have sparse or expensive-to-obtain labeled data from experiments or high-fidelity simulations; PINNs leverage physics to compensate for limited labeled data.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Functional data over space-time (continuous fields); input often coordinates (x,y,z,t) and outputs state variables u(x,t).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: PDEs involve infinite-dimensional function spaces, strong nonlinearity in many applications, high spatial/temporal dimensionality and boundary condition constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established methodology with growing adoption; grounded in classical PDE theory but still active research on stability, scalability, and training practices.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High - models are built to respect known mechanistic laws and interpretability of learned solutions relative to physical equations is required.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Physics-Informed Neural Networks (PINNs)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Feed-forward neural networks take coordinates as input and output field values; the loss combines data-fitting terms and a PDE residual norm ∥R(u)∥ along with boundary/initial condition penalties; training minimizes combined loss to fit both data and physics.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>physics-informed ML / hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable and appropriate for PDE-governed scientific problems where physics is known and data are scarce; must be adapted for high-dimensional/complex geometries and can struggle with stiff dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Qualitatively effective for small-to-moderate problems and inverse problems; demonstrated on 1D and 2D geometries in cited works, but scaling and training stability remain challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High potential to reduce reliance on labeled data and produce solutions consistent with conservation laws, enabling inverse modeling and surrogate modeling in science.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared conceptually to pure data-driven surrogates (which ignore physics); PINNs incorporate physics to improve generalization, especially with limited data. Specific numerical comparisons not provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of accurate PDE formulations to include in loss, appropriate network architecture, and loss weighting between data and physics residues; capacity to represent solution smoothness and boundary behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Incorporating PDE residuals into training makes neural networks capable of solving and inverting PDE problems even with limited labeled data, but scaling and numerical stability are major practical constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2342.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2342.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural ODEs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Ordinary Differential Equations (NODE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Models that parameterize time derivatives with neural networks and integrate them with ODE solvers, enabling continuous-time modeling and good extrapolation in temporal forecasting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Temporal forecasting in dynamical systems (e.g., turbulence forecasting, climate, fluid dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Learn continuous-time dynamics from time series or simulation data to forecast temporal evolution and enable extrapolation beyond training windows.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Variable; temporal data may be available from simulations or sensors but can be costly to obtain at high fidelity; NODEs are useful when temporal resolution is moderate and extrapolation is required.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Time series (possibly high-dimensional state vectors), trajectories from dynamical systems.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: stiff or chaotic dynamics, high-dimensional state spaces, need for long-term stable integration; backpropagation through ODE solvers introduces computational complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging research area with multiple variants (ANODE, SDE-NODE, DUNODE, HONODE, CNODE, PNODE).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium to high - often used when mechanistic insight into dynamics is desired; NODEs provide an interpretable continuous-time representation but are still black-box in parameterization.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Neural ODEs and variants (ANODE, SDE-NODE, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>A neural network models the time derivative f(u,t;θ); an ODE solver integrates this to produce trajectories; training uses adjoint sensitivity methods to backpropagate through the solver; variants add augmentation, stochasticity, discrete updates or control inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>hybrid / deep learning for dynamical systems</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Well-suited for problems requiring continuous-time modeling and good extrapolation; practical applicability depends on solver efficiency and ability to handle stiffness/chaos.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Reported to perform well in extrapolation modes and in turbulence forecasting; many variants address different shortcomings (e.g., stiffness, stochasticity).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Enables data-driven continuous-time models that can replace or augment time-stepping simulators, with potential for efficient long-term forecasts and model discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared favorably to discrete-time RNNs in extrapolation contexts; specific numeric comparisons not given in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Appropriate choice of ODE solver, adjoint method for gradients, architectural variants to handle stochasticity or augmentation, and sufficient trajectory data for training.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Parameterizing time derivatives with neural networks plus ODE solvers yields models that extrapolate better in time than many discrete-time models, useful for forecasting complex transient scientific phenomena.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2342.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2342.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UDE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Universal Differential Equations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Hybrid differential equation models where some components are specified by known operators and others are modeled by neural networks, enabling blending of mechanistic terms and data-driven submodels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Scientific modeling and equation discovery (PDE/ODE systems), e.g., reaction-diffusion, Navier–Stokes sub-models</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Model systems where some physics is known (operators) but other terms are uncertain or poorly modeled and can be learned from data.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Moderate; UDEs are useful when partial knowledge exists and data are sufficient to learn missing components; data may be simulation or experimental.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Time-series or spatio-temporal fields; neural networks represent components within ODE/PDE solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: mixed operator types (differential, algebraic, stochastic), potential for stiff systems, and the need to jointly train neural components with solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging with available toolkits (Julia-based) and demonstrated on several canonical problems.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High - users require mechanisms to be interpretable and separated from learned components; UDEs facilitate interpretability of known vs learned terms.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Universal Differential Equations (UDEs)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Embed neural network modules inside differential equation formulations; train network components together with differential equation solver workflows; can combine with sparse regression for equation discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>hybrid physics-ML / equation discovery</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable when some mechanistic operators are known and remaining terms can be learned; advantageous for improving model accuracy and enabling interpretability of learned sub-models.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Reported to improve equation discovery relative to applying sparse regression to full equations; demonstrated on reaction–diffusion and Navier–Stokes sub-model learning.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High potential to improve model fidelity by fusing known physics with learned components, enabling discovery of missing terms and better surrogate sub-models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Shown to outperform direct sparse regression (e.g., SINDy) for certain problems by training networks only for unknown components; quantitative specifics not provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Accurate specification of known operators, adequate data to train unknown components, and use of sparsity/regularization to aid interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Embedding neural sub-models within differential equations allows leveraging prior mechanistic knowledge while learning unknown components, improving discovery and accuracy compared to purely data-driven equation discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2342.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2342.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural Operators</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Operators (DeepONet, Fourier Neural Operator, Graph Kernel Networks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Architectures that learn mappings between function spaces (operators), enabling models that take input functions (e.g., spatially varying coefficients) and output solution functions of PDEs for entire domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>PDE solution operators and parametric PDE problems (climate, fluid flow, heat conduction, materials)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Learn a map from an input function (e.g., spatially varying coefficient ν(x) ) to the PDE solution u(x,t) across the domain, allowing fast evaluation for many parameter instances.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires datasets of input functions and corresponding solutions, typically generated via simulation (can be large but costly to produce at high fidelity); some studies trained at large scale (thousands of GPUs).</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Functions over space-time; can be represented on grids, graphs, or via spectral coefficients (non-uniform material properties, fields).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Very high: infinite-dimensional inputs/outputs, high-resolution domains, nonlinear PDE behavior, and need for generalization across function instances.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Rapidly maturing with several architectures (DeepONet, FNO, graph kernel networks) and large-scale demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium to high - methods can incorporate known physics and aim to provide operators applicable across problem instances; interpretability relative to operators is desired.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Neural Operators (DeepONet, Fourier Neural Operator, Graph Kernel Networks)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>DeepONet uses branch and trunk networks to represent operator action; FNO uses Fourier transforms to parameterize global convolutional kernels; graph kernel networks deploy GNNs to learn kernels for elliptic PDEs; models are trained on pairs of input functions and solution functions to learn operator mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>physics-informed / operator learning</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable for parametric PDE problems where fast repeated evaluations across varying input functions are needed; architectural choice depends on geometry and data representation.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Reported to achieve superior performance to conventional neural networks for operator tasks; FNO achieves scalable performance though with geometric limitations; large-scale training (thousands of GPUs) demonstrated for high-resolution tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — enables sampling-independent solvers that, once trained, can rapidly evaluate entire classes of PDE problems, improving simulation throughput and enabling inverse design and uncertainty quantification.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Neural operators outperform coordinate-based PINNs for operator mapping tasks and standard CNNs when input is function-valued; FNO has advantages in scalability but may have geometric limitations relative to other methods.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Sufficient coverage of input-function space in training data, architecture matched to domain geometry (spectral vs spatial), and large-scale compute for high-resolution training.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Learning mappings between function spaces lets a single trained model solve whole classes of PDE instances rapidly, trading expensive offline training for fast online evaluation across parameter variations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2342.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2342.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Surrogate Turbulence Models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Machine-learned Surrogate Turbulence Models (hybrid RANS/ML surrogates)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Data-driven turbulence closures or corrective modules that replace or augment traditional turbulence models (e.g., k-ω) within CFD simulations to improve accuracy or reduce computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Computational fluid dynamics — turbulence modeling for engineering and geophysical flows</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Model turbulent flows (wall-bounded or wall-free) where direct numerical simulation is infeasible; learn closures or correction terms from DNS/LES data to improve predictive accuracy of RANS or other reduced models.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Training data often derived from high-fidelity simulations (DNS/LES) which are expensive; for some wall-bounded problems DNS datasets exist but are limited in parameter coverage, making generalization challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Spatio-temporal fields representing velocity, pressure, turbulent quantities; high-resolution 3D grids; possibly multi-scale and multi-modal.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Extremely high: multiscale nonlinear chaotic dynamics, high-dimensional state, extrapolation beyond training regimes is common and hard, and boundary interactions add complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Active research area with many studies showing promising results but with open challenges in generalization and trustworthiness.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High - physics constraints (conservation laws, boundary behavior) and interpretability are critical for deployment in engineering contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Surrogate ML turbulence closures (data-driven turbulence models, NODE-based models)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Approaches include directly modeling turbulence production terms, predictor-corrector schemes where an ML model corrects traditional model outputs, and NODE-based temporal models for atmospheric turbulence; deployment strategies include in-memory or remote inference and integration into simulation codes via frameworks like SmartSim.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>hybrid / surrogate modeling / physics-informed ML</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable for accelerating and improving reduced-order turbulence models; suitability depends on representativeness of training data and incorporation of physical constraints to ensure stable deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Many studies report improved accuracy over empirical closures in certain regimes, but concerns remain about extrapolation and robustness; some ML closures can outperform empirical models when trained on representative DNS data.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High impact for engineering design (aircraft, rotorcraft) and climate modeling if robust generalization can be achieved, with potential for reduced compute cost and improved fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared against empirical turbulence models (e.g., k-ω); ML-based models can outperform in trained regimes but may fail in extrapolation without physical constraints; hybrid predictor-corrector strategies provide pragmatic compromise.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>High-quality DNS/LES training data, incorporation of physical constraints, careful deployment/inference integration in HPC simulations, and methods to detect out-of-distribution regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Machine-learned surrogate turbulence models can improve predictions over empirical closures when trained on high-fidelity data and integrated with physics-aware deployment, but robust generalization and trust require physics constraints and careful validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2342.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2342.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WSI Cancer Detection (MIL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Whole-Slide Image Cancer Detection using Multiple-Instance Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Weakly-supervised approaches for gigapixel histopathology images that divide whole-slide images into patches and aggregate patch-level features via multiple-instance learning to predict slide-level labels with reduced annotation needs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Computational pathology / cancer detection</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Detect cancerous regions and classify pathological outcomes from extremely high-resolution whole-slide images (WSIs) where pixel-level annotations are costly and rare.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>WSIs are available but often with limited pixel-level labels; slide-level labels are more common; data are large (gigapixel), requiring HPC resources for processing.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Very high-resolution images (gigapixel), handled by patching into many smaller tiles; structured as sets/bags of patches per slide.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: enormous input size, class imbalance, high intra-class variability, multiple scales of morphology, and need to aggregate information across many patches.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Mature research area with many methods (patching, MIL, self-supervised) and active improvements; state-of-the-art is evolving.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High for clinical deployment - interpretability and localization of predictions are needed for acceptance and to reduce false positives.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Multiple-Instance Learning (MIL), patch-based CNNs, self-supervised learning</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Divide WSIs into patches, extract features via CNNs, and use MIL frameworks to aggregate patch features into slide-level predictions (weakly-supervised). Self-supervised pretraining can help when labels are scarce; patching strategies and postprocessing are required to manage false positives.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning / weakly-supervised / self-supervised</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Well-suited given scarcity of pixel-level labels; approaches are computationally intensive and benefit from HPC for training and inference on gigapixel images.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Patch-based MIL works well in many cases but can produce many false positives and requires careful postprocessing; self-supervised methods are promising but less mature in generality.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Significant — can automate screening, reduce expert workload, and enable large-scale pathology studies, but clinical deployment needs robust validation and interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Patch-level supervised training requires pixel labels and is less scalable; MIL trades localization detail for scalable slide-level learning and reduced labeling cost.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Access to large slide-level labeled datasets, careful patch selection and augmentation, scalable HPC pipelines, and interpretability techniques to highlight diagnostic regions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Weakly-supervised MIL on patchwise representations enables classification of gigapixel pathology slides with limited annotations, but managing false positives and ensuring interpretability are essential for clinical impact.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2342.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2342.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BigBird Pathology Model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sparse Transformer (BigBird) for Pathology Report Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sparse-attention transformer architecture adapted for long clinical documents, trained on millions of pathology reports to extract structured cancer information from narrative reports.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Medical document analysis / oncology informatics</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Extract structured cancer characteristics (site, subsite, laterality, histology, behavior) from long pathology reports to populate cancer registries.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Large dataset available: 2.7 million pathology reports from six SEER registries used for training; labels exist for registry coding but may require mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Unstructured clinical text (long documents), variable-length and domain-specific language.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: very long documents exceeding standard transformer context windows, domain-specific vocabulary, class imbalance across extracted fields.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Applied research maturity in clinical NLP; transformer models adapted for long sequences are mature and being actively applied.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium - downstream use requires reliable extraction and traceability but clinical interpretability of predictions is important.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>BigBird sparse-attention transformer (pathology BigBird)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>A sparse-attention transformer variant designed for long sequences; trained on 2.7M SEER pathology reports to perform information extraction tasks; sparse attention reduces memory and compute for long documents compared to dense transformers like BERT.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning / foundation models / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Highly applicable to long-document clinical NLP where document lengths exceed dense-transformer limits; sparse attention enables training on large corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Demonstrated suitability for pathology information extraction at scale; choice of sparse transformer allowed handling of very long reports that dense models cannot process efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — automates registry coding, accelerates data curation for epidemiology and research, and scales to national-scale clinical corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Sparse transformers (BigBird) are more suitable than dense attention models (BERT) for very long documents; specific performance metrics not provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Availability of very large labeled/unlabeled report corpora, architecture that accommodates long contexts (sparse attention), and domain adaptation to clinical language.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Sparse-attention transformers trained on millions of pathology reports enable scalable extraction of structured cancer information from long clinical texts that dense models cannot handle efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2342.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2342.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Computational Steering (Neutron Scattering)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI-enabled Computational Steering for Neutron Scattering Experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Edge-deployed AI workflows that analyze initial experimental scans in near-real-time to steer subsequent measurements, reducing expert decision time and improving experiment efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Experimental physical sciences — neutron scattering</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Automate decision-making in multi-stage neutron scattering experiments to identify regions of interest and determine refined scan parameters in near-real-time.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Experimental scan images and telemetry are generated during beamline operations; data are available in real-time but may be limited per experiment and require fast processing at the edge.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Image-like scan outputs, time-ordered experimental telemetry streams; data are streaming and potentially multimodal.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Medium to high: real-time constraints, heterogeneity of experimental setups, and need for robust detection/localization under variable conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging — prototypes and workflows demonstrated (ORNL example) but not yet widespread.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium - decisions must be explainable to beamline scientists and respect experimental constraints; trustworthiness is important.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Edge-deployed computer vision models and autonomous workflows (AI steering)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Computer vision models infer features from initial scans; a workflow deployed at the edge assimilates telemetry and model outputs to recommend next experimental parameters; asynchronous HPC/edge integration supports low-latency steering.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning / edge AI / autonomous experimentation</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Appropriate for experiments with iterative scanning and clear decision rules informed by imaging; reduces latency of human decision-making and increases throughput.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Described as having potential to drastically reduce time for decision-making from days to near-real-time; specific deployment (Yin et al.) demonstrates feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — enables autonomous or semi-autonomous experiments, increases facility throughput, and accelerates discovery cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Manual expert steering is slower and less scalable; AI steering offers speed and potential consistency benefits but requires validation and expert oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Low-latency edge compute, reliable computer vision models tuned to instrument data, integration with experimental control systems, and human-in-the-loop oversight for safety.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Edge-deployed AI can meaningfully accelerate experimental workflows by enabling near-real-time decision-making, but requires integration with instrument control and rigorous validation for trust.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2342.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2342.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Uncertainty Quantification (UQ)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Uncertainty Quantification Methods for Scientific ML (GPR, PI-GAN, Monte Carlo Dropout, PI3NN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of probabilistic and ensemble-based approaches to estimate predictive uncertainty in scientific ML models, including Gaussian processes, physics-informed GANs, Monte Carlo dropout, and prediction-interval neural nets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>General scientific modeling requiring predictive confidence (multiple domains)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Provide reliable uncertainty estimates for ML predictions to support scientific decision-making, detect OOD samples, and quantify confidence for high-consequence applications.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Varies by domain; uncertainty methods can operate with limited data but quality of uncertainty estimates depends on data representativeness and noise characteristics.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Applies across data types: tabular, structured, spatio-temporal, and functional data.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Medium to high: requires modeling both epistemic and aleatoric uncertainty, scalable inference for large datasets, and OOD detection.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Maturing — many established methods (GPR) and modern neural approaches (MC dropout, PI3NN) with active research on calibration and scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High for scientific applications — uncertainty estimates must be trustworthy and interpretable to inform experiments and safety-critical decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Gaussian Process Regression (GPR), Physics-Informed GAN (PI-GAN), Monte Carlo Dropout, PI3NN prediction-interval networks</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>GPR provides Bayesian predictive distributions but scales poorly; PI-GANs incorporate physics into generative models to produce distributions; Monte Carlo Dropout uses dropout at inference to approximate Bayesian model uncertainty; PI3NN uses three networks to produce prediction intervals and OOD-aware bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>probabilistic ML / uncertainty quantification</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Essential for scientific ML where decision-making demands confidence estimates; choice of method depends on data scale and required fidelity of UQ.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Methods like PI3NN are noted as state-of-the-art for out-of-distribution-aware prediction intervals; GPR is principled but needs variational approximations for scale.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — UQ methods are critical to adoption of ML in science, enabling risk-aware decisions, model validation, and detection of OOD regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>GPR offers strong theoretical guarantees but poor scalability; MC Dropout is scalable but approximate; PI-GANs and PI3NN offer physics-aware or interval-focused alternatives with better OOD behavior in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Incorporation of physics constraints, scalable approximations for large data, and careful calibration and validation on representative datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Reliable uncertainty quantification is indispensable for scientific ML; hybrid and interval-based neural approaches offer scalable, OOD-aware uncertainty estimates suited for high-consequence scientific applications.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2342.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2342.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Symbolic Regression / Equation Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Symbolic Regression and Sparse Identification (e.g., SINDy, neural symbolic methods)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods to infer explicit mathematical expressions or governing equations from data using techniques like sparse regression, genetic programming, and neural-symbolic approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Equation discovery / model discovery in dynamical systems and physics</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Discover interpretable closed-form equations that describe observed system dynamics from data, enabling mechanistic understanding and reduced-order models.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Often limited; high-quality time-series or spatio-temporal data are required; sparse regression methods prefer noise-free or denoised data.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Time-series and state trajectories; can be high-dimensional but methods often seek low-dimensional governing expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: combinatorial search over symbolic expressions, sensitivity to noise, and curse of dimensionality in candidate function spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Established research area with classical genetic programming and modern ML-enhanced methods (Cranmer et al., Biggio et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Very high - primary aim is interpretable mechanistic equations rather than black-box prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Symbolic regression (genetic programming), sparse regression (SINDy), neural symbolic regression</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Methods range from genetic programming to neural-network-guided symbolic discovery; sparse regression methods select parsimonious terms from a library of candidate functions to identify governing equations.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>symbolic / interpretable ML / equation discovery</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable when interpretability is primary and data quality allows identification of parsimonious models; less applicable in noisy, high-dimensional cases without preprocessing.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>UDEs combined with sparse regression can outperform direct sparse regression on whole equations; scalable neural symbolic methods can handle larger problems than classic GP but still have limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for scientific insight — direct extraction of governing equations can advance theory and enable simpler predictive models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Symbolic methods provide interpretability compared to black-box ML; UDE+sparse regression can be more effective than SINDy alone by restricting learned components.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>High-quality, appropriately sampled data, domain knowledge to construct candidate libraries or constrain search, and methods to mitigate noise sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Symbolic and sparse-regression approaches can recover interpretable governing equations, especially when combined with hybrid modeling that limits search to unknown components, improving discovery in scientific contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2342.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2342.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Temporal Forecasting Architectures</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Temporal CNNs, RNNs (LSTM), Transformers, and NODEs for Time-series Forecasting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of neural architectures effective for transient scientific forecasting: dilated causal CNNs (TCNN), recurrent models (LSTM), attention-based transformers, and Neural ODEs for continuous-time dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Time-series forecasting in sciences (fluid dynamics, turbulence, weather/climate)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict future states of transient processes given historical sequence windows, often for systems with multi-scale temporal dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Depends on domain; time series may be plentiful (e.g., weather reanalyses) or limited (experimental runs); preprocessing into time windows required.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Time series and sequences, possibly multivariate and high-dimensional (fields over space and time).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: long-range dependencies, multi-scale dynamics, non-stationarity, and high-dimensional outputs (spatio-temporal fields).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Mature toolkit from ML community with specific adaptations for scientific forecasting; NODEs add continuous-time modeling benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium - forecasting often values predictive accuracy, but interpretability and physical consistency remain important in science.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Temporal CNNs (dilated causal), RNNs (LSTM), Transformers, Neural ODE variants</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>TCNNs use dilated causal convolutions for long-range temporal dependencies; LSTMs capture sequential dependencies via gated RNNs; transformers use attention for sequence modeling; NODEs model derivatives and integrate continuously for improved extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning / sequence modeling / hybrid (NODE)</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Widely applicable; TCNNs and transformers excel with large datasets and long contexts, NODEs useful for continuous-time extrapolation and physical-consistency needs.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Each architecture has tradeoffs: transformers excel with long-context data but are costly; TCNNs and LSTMs are effective for certain scales; NODEs perform well in extrapolation for turbulence forecasting.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High for improved forecasting in climate, turbulence, and other transient scientific systems, enabling better predictions and decision support.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Transformers vs RNNs: transformers scale better for long-context but with quadratic cost; NODEs can outperform discrete-time models in extrapolation contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Proper input sequence construction (windowing), architecture selection for context length and compute constraints, and integration of physical constraints where possible.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>No single sequence architecture dominates; choice depends on context length, compute cost, and whether continuous-time extrapolation (NODE) or long-range attention (transformer) is most valuable for the scientific forecasting task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2342.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e2342.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Coarse-graining / Data-driven Discretization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Data-Driven Discretization for Coarse-Grained PDE Solutions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Learned discretization operators that enable high-fidelity solutions of PDEs on much coarser grids by training data-driven numerical stencils or operators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Numerical simulation / PDE solvers (fluid dynamics, thermodynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Reduce computational cost of high-fidelity PDE simulation by learning discretizations that approximate fine-grid behavior on coarser meshes.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires high-fidelity fine-grid simulation data to train coarse-grained operators; such data can be expensive to generate but are used to produce training labels.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Spatially discretized field data (grids) with corresponding fine-grid solutions used as supervision for coarse-grid operator learning.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: capturing multiscale interactions on coarse grids is challenging; learned discretizations must preserve stability and conservation properties.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging approach inspired by image super-resolution and GANs; promising early results but ongoing research on stability and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>High - numerical stability and adherence to conservation laws are important; interpretability of learned stencils may be desired.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Data-driven discretization / coarse-graining using ML (GAN-inspired super-resolution techniques)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Train networks (often convolutional architectures) to map coarse-grid representations to high-fidelity approximations or to learn discretization operators; methods adapt concepts from image super-resolution (e.g., GANs) to PDE contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>supervised learning / surrogate modeling</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable when fine-grid simulation data are available to supervise learning and when reducing runtime for many-query settings is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Reported to allow accurate solutions on coarser grids in some PDE problems; success depends on representativeness of training data and ability to enforce stability.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>Potentially large computational savings for simulation-driven science and engineering, enabling faster design loops and multiscale coupling.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Traditional coarse-graining uses analytically derived closures; data-driven methods can capture finer features but risk instability and lack of theoretical guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Access to diverse high-fidelity simulation datasets, incorporation of conservation or stability constraints, and careful architecture/training choices to avoid spurious artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Learning discretizations from data can deliver high-fidelity coarse-grid solutions, trading offline training cost for significant online simulation speedups if stability and generalization are achieved.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2342.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e2342.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs for Drug Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models and Foundation Models for Drug Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large pretrained language or sequence models adapted to molecular sequences and chemistry tasks to accelerate candidate generation, property prediction, and virtual screening.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>Drug discovery / molecular design</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Predict molecular properties, propose candidate molecules, and assist virtual screening by leveraging learned sequence/structure representations.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Molecular datasets can be large (public chemical libraries) but high-quality labeled assay data may be limited; models often pretrain on large unlabeled corpora and fine-tune on task-specific datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Sequential representations (SMILES, protein sequences), graph-structured molecular data, and multimodal (sequence+structure) data.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: combinatorial chemical space is enormous, properties result from complex physics/chemistry, and experimental validation is expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Rapidly emerging with growing adoption of foundation models adapted to chemistry and biology; research is active and methods evolving.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium to high - drug development benefits from interpretable predictions and mechanistic hypotheses, though generative proposals may be acceptable for exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Large Language Models / foundation models adapted for molecular tasks (sequence models, LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Pretrain large transformer-based models on large corpora of chemical/molecular sequences or text and fine-tune for property prediction, inhibitor identification, or generative design; may be integrated into high-throughput virtual screening pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>foundation models / supervised & unsupervised learning / generative modeling</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Applicable for representation learning, candidate generation, and accelerating early-stage discovery; dependent on fine-tuning data and validation pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Promising for improving virtual screening and representation of molecules; specific effectiveness varies by task and data availability.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — can accelerate lead identification and reduce experimental screening burden if integrated into HTVS pipelines and validated experimentally.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Classical cheminformatics and QSAR rely on handcrafted features; LLMs offer richer learned representations and generative capabilities but require large compute and careful validation.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Large pretraining corpora, task-relevant fine-tuning data, integration with physics-aware filters and experimental validation, and scalable HTVS workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Foundation models adapted to molecular data can provide powerful representations and generative capabilities for drug discovery, but utility depends on high-quality finetuning and integration into screening pipelines with experimental feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2342.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e2342.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or machine learning methodologies being applied to scientific problems, including details about the problem characteristics (data availability, data structure, complexity, domain maturity, mechanistic understanding requirements) and the outcomes (applicability, effectiveness, impact).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SmartSim / Petascale Inference</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SmartSim and Scalable Inference Frameworks for HPC-integrated Surrogates</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Middleware and frameworks (e.g., SmartSim, RedisAI, TensorFlow Serving) and studies showing petascale inference deployments to integrate ML surrogates into large-scale simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_problem_domain</strong></td>
                            <td>HPC-integrated surrogate deployment (rotorcraft aerodynamics, ocean/climate modeling, CFD)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Deploy ML surrogates at scale alongside traditional simulations to perform inference within simulation loops, enabling cognitive simulations and hybrid AI/simulation workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Surrogate models trained offline require high-fidelity training datasets; inference uses runtime simulation data which is abundant in large-scale simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>data_structure</strong></td>
                            <td>Model input/output tensors of simulation states (high-dimensional arrays), streaming telemetry for decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>High: needs low-latency, high-throughput inference integrated with MPI simulations; scaling inference across thousands of GPUs demands sophisticated orchestration.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Developing maturity with frameworks and exemplar studies demonstrating petascale inference (Brewer et al., Partee et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_understanding_requirements</strong></td>
                            <td>Medium - surrogates must respect simulation stability and physical constraints for safe integration; interpretability may be required for debugging.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_name</strong></td>
                            <td>Inference-serving frameworks (SmartSim, RedisAI, TensorFlow Serving) and remote/in-memory inference strategies</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_description</strong></td>
                            <td>Use of in-memory or RPC-based remote inference servers scaled asynchronously (load balancing or MPI messaging) to serve ML surrogate models to simulation codes; frameworks handle data movement and integration at HPC scale.</td>
                        </tr>
                        <tr>
                            <td><strong>ai_methodology_category</strong></td>
                            <td>systems / deployment / ML ops for science</td>
                        </tr>
                        <tr>
                            <td><strong>applicability</strong></td>
                            <td>Essential for integrating ML surrogates into production-scale simulations; selection of deployment strategy depends on latency, fault tolerance, and communication constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_quantitative</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>effectiveness_qualitative</strong></td>
                            <td>Studies show feasibility of petascale inference for rotorcraft aerodynamics and ocean climate modeling; effective integration requires careful orchestration and choice of serving strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_potential</strong></td>
                            <td>High — enables real-time surrogate-assisted simulation and reduced time-to-solution for multi-query experiments, facilitating cognitive simulations and digital twins.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>In-memory inference reduces latency but may be less fault-tolerant than RPC/load-balanced approaches; MPI-based message passing can be more performant but complex.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Efficient model serving stacks, scalable orchestration, low-latency data paths, fault-tolerant strategies, and co-design with simulation code (C++/MPI integration).</td>
                        </tr>
                        <tr>
                            <td><strong>key_insight</strong></td>
                            <td>Scalable inference frameworks are critical to realize hybrid AI/simulation workflows at HPC scale, and deployment strategy must balance latency, performance, and fault tolerance to integrate ML surrogates effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations <em>(Rating: 2)</em></li>
                <li>Neural ordinary differential equations <em>(Rating: 2)</em></li>
                <li>Universal differential equations for scientific machine learning <em>(Rating: 2)</em></li>
                <li>Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators <em>(Rating: 2)</em></li>
                <li>Fourier Neural Operator for Parametric Partial Differential Equations <em>(Rating: 2)</em></li>
                <li>Learning data-driven discretizations for partial differential equations <em>(Rating: 1)</em></li>
                <li>Time-series forecasting with deep learning: a survey <em>(Rating: 1)</em></li>
                <li>Multiple instance learning of deep convolutional neural networks for breast histopathology whole slide classification <em>(Rating: 1)</em></li>
                <li>Big bird: Transformers for longer sequences <em>(Rating: 2)</em></li>
                <li>Using machine learning at scale in numerical simulations with smartsim: An application to ocean climate modeling <em>(Rating: 2)</em></li>
                <li>Production deployment of machine-learned rotorcraft surrogate models on HPC <em>(Rating: 2)</em></li>
                <li>Pi3nn: Out-of-distribution-aware prediction intervals from three neural networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2342",
    "paper_id": "paper-270738175",
    "extraction_schema_id": "extraction-schema-63",
    "extracted_data": [
        {
            "name_short": "PINNs",
            "name_full": "Physics-Informed Neural Networks",
            "brief_description": "Neural networks trained with loss terms that penalize PDE residuals and enforce boundary/initial conditions to produce solutions of differential equations consistent with known physics.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Partial differential equations / physics-based simulation",
            "problem_description": "Solve forward and inverse problems governed by PDEs where data are sparse or where solutions must satisfy conservation laws and boundary/initial conditions.",
            "data_availability": "Often limited; scientific PDE problems may have sparse or expensive-to-obtain labeled data from experiments or high-fidelity simulations; PINNs leverage physics to compensate for limited labeled data.",
            "data_structure": "Functional data over space-time (continuous fields); input often coordinates (x,y,z,t) and outputs state variables u(x,t).",
            "problem_complexity": "High: PDEs involve infinite-dimensional function spaces, strong nonlinearity in many applications, high spatial/temporal dimensionality and boundary condition constraints.",
            "domain_maturity": "Established methodology with growing adoption; grounded in classical PDE theory but still active research on stability, scalability, and training practices.",
            "mechanistic_understanding_requirements": "High - models are built to respect known mechanistic laws and interpretability of learned solutions relative to physical equations is required.",
            "ai_methodology_name": "Physics-Informed Neural Networks (PINNs)",
            "ai_methodology_description": "Feed-forward neural networks take coordinates as input and output field values; the loss combines data-fitting terms and a PDE residual norm ∥R(u)∥ along with boundary/initial condition penalties; training minimizes combined loss to fit both data and physics.",
            "ai_methodology_category": "physics-informed ML / hybrid",
            "applicability": "Applicable and appropriate for PDE-governed scientific problems where physics is known and data are scarce; must be adapted for high-dimensional/complex geometries and can struggle with stiff dynamics.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Qualitatively effective for small-to-moderate problems and inverse problems; demonstrated on 1D and 2D geometries in cited works, but scaling and training stability remain challenges.",
            "impact_potential": "High potential to reduce reliance on labeled data and produce solutions consistent with conservation laws, enabling inverse modeling and surrogate modeling in science.",
            "comparison_to_alternatives": "Compared conceptually to pure data-driven surrogates (which ignore physics); PINNs incorporate physics to improve generalization, especially with limited data. Specific numerical comparisons not provided in paper.",
            "success_factors": "Availability of accurate PDE formulations to include in loss, appropriate network architecture, and loss weighting between data and physics residues; capacity to represent solution smoothness and boundary behavior.",
            "key_insight": "Incorporating PDE residuals into training makes neural networks capable of solving and inverting PDE problems even with limited labeled data, but scaling and numerical stability are major practical constraints.",
            "uuid": "e2342.0",
            "source_info": {
                "paper_title": "Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Neural ODEs",
            "name_full": "Neural Ordinary Differential Equations (NODE)",
            "brief_description": "Models that parameterize time derivatives with neural networks and integrate them with ODE solvers, enabling continuous-time modeling and good extrapolation in temporal forecasting.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Temporal forecasting in dynamical systems (e.g., turbulence forecasting, climate, fluid dynamics)",
            "problem_description": "Learn continuous-time dynamics from time series or simulation data to forecast temporal evolution and enable extrapolation beyond training windows.",
            "data_availability": "Variable; temporal data may be available from simulations or sensors but can be costly to obtain at high fidelity; NODEs are useful when temporal resolution is moderate and extrapolation is required.",
            "data_structure": "Time series (possibly high-dimensional state vectors), trajectories from dynamical systems.",
            "problem_complexity": "High: stiff or chaotic dynamics, high-dimensional state spaces, need for long-term stable integration; backpropagation through ODE solvers introduces computational complexity.",
            "domain_maturity": "Emerging research area with multiple variants (ANODE, SDE-NODE, DUNODE, HONODE, CNODE, PNODE).",
            "mechanistic_understanding_requirements": "Medium to high - often used when mechanistic insight into dynamics is desired; NODEs provide an interpretable continuous-time representation but are still black-box in parameterization.",
            "ai_methodology_name": "Neural ODEs and variants (ANODE, SDE-NODE, etc.)",
            "ai_methodology_description": "A neural network models the time derivative f(u,t;θ); an ODE solver integrates this to produce trajectories; training uses adjoint sensitivity methods to backpropagate through the solver; variants add augmentation, stochasticity, discrete updates or control inputs.",
            "ai_methodology_category": "hybrid / deep learning for dynamical systems",
            "applicability": "Well-suited for problems requiring continuous-time modeling and good extrapolation; practical applicability depends on solver efficiency and ability to handle stiffness/chaos.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Reported to perform well in extrapolation modes and in turbulence forecasting; many variants address different shortcomings (e.g., stiffness, stochasticity).",
            "impact_potential": "Enables data-driven continuous-time models that can replace or augment time-stepping simulators, with potential for efficient long-term forecasts and model discovery.",
            "comparison_to_alternatives": "Compared favorably to discrete-time RNNs in extrapolation contexts; specific numeric comparisons not given in paper.",
            "success_factors": "Appropriate choice of ODE solver, adjoint method for gradients, architectural variants to handle stochasticity or augmentation, and sufficient trajectory data for training.",
            "key_insight": "Parameterizing time derivatives with neural networks plus ODE solvers yields models that extrapolate better in time than many discrete-time models, useful for forecasting complex transient scientific phenomena.",
            "uuid": "e2342.1",
            "source_info": {
                "paper_title": "Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "UDE",
            "name_full": "Universal Differential Equations",
            "brief_description": "Hybrid differential equation models where some components are specified by known operators and others are modeled by neural networks, enabling blending of mechanistic terms and data-driven submodels.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Scientific modeling and equation discovery (PDE/ODE systems), e.g., reaction-diffusion, Navier–Stokes sub-models",
            "problem_description": "Model systems where some physics is known (operators) but other terms are uncertain or poorly modeled and can be learned from data.",
            "data_availability": "Moderate; UDEs are useful when partial knowledge exists and data are sufficient to learn missing components; data may be simulation or experimental.",
            "data_structure": "Time-series or spatio-temporal fields; neural networks represent components within ODE/PDE solvers.",
            "problem_complexity": "High: mixed operator types (differential, algebraic, stochastic), potential for stiff systems, and the need to jointly train neural components with solvers.",
            "domain_maturity": "Emerging with available toolkits (Julia-based) and demonstrated on several canonical problems.",
            "mechanistic_understanding_requirements": "High - users require mechanisms to be interpretable and separated from learned components; UDEs facilitate interpretability of known vs learned terms.",
            "ai_methodology_name": "Universal Differential Equations (UDEs)",
            "ai_methodology_description": "Embed neural network modules inside differential equation formulations; train network components together with differential equation solver workflows; can combine with sparse regression for equation discovery.",
            "ai_methodology_category": "hybrid physics-ML / equation discovery",
            "applicability": "Applicable when some mechanistic operators are known and remaining terms can be learned; advantageous for improving model accuracy and enabling interpretability of learned sub-models.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Reported to improve equation discovery relative to applying sparse regression to full equations; demonstrated on reaction–diffusion and Navier–Stokes sub-model learning.",
            "impact_potential": "High potential to improve model fidelity by fusing known physics with learned components, enabling discovery of missing terms and better surrogate sub-models.",
            "comparison_to_alternatives": "Shown to outperform direct sparse regression (e.g., SINDy) for certain problems by training networks only for unknown components; quantitative specifics not provided in paper.",
            "success_factors": "Accurate specification of known operators, adequate data to train unknown components, and use of sparsity/regularization to aid interpretability.",
            "key_insight": "Embedding neural sub-models within differential equations allows leveraging prior mechanistic knowledge while learning unknown components, improving discovery and accuracy compared to purely data-driven equation discovery.",
            "uuid": "e2342.2",
            "source_info": {
                "paper_title": "Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Neural Operators",
            "name_full": "Neural Operators (DeepONet, Fourier Neural Operator, Graph Kernel Networks)",
            "brief_description": "Architectures that learn mappings between function spaces (operators), enabling models that take input functions (e.g., spatially varying coefficients) and output solution functions of PDEs for entire domains.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "scientific_problem_domain": "PDE solution operators and parametric PDE problems (climate, fluid flow, heat conduction, materials)",
            "problem_description": "Learn a map from an input function (e.g., spatially varying coefficient ν(x) ) to the PDE solution u(x,t) across the domain, allowing fast evaluation for many parameter instances.",
            "data_availability": "Requires datasets of input functions and corresponding solutions, typically generated via simulation (can be large but costly to produce at high fidelity); some studies trained at large scale (thousands of GPUs).",
            "data_structure": "Functions over space-time; can be represented on grids, graphs, or via spectral coefficients (non-uniform material properties, fields).",
            "problem_complexity": "Very high: infinite-dimensional inputs/outputs, high-resolution domains, nonlinear PDE behavior, and need for generalization across function instances.",
            "domain_maturity": "Rapidly maturing with several architectures (DeepONet, FNO, graph kernel networks) and large-scale demonstrations.",
            "mechanistic_understanding_requirements": "Medium to high - methods can incorporate known physics and aim to provide operators applicable across problem instances; interpretability relative to operators is desired.",
            "ai_methodology_name": "Neural Operators (DeepONet, Fourier Neural Operator, Graph Kernel Networks)",
            "ai_methodology_description": "DeepONet uses branch and trunk networks to represent operator action; FNO uses Fourier transforms to parameterize global convolutional kernels; graph kernel networks deploy GNNs to learn kernels for elliptic PDEs; models are trained on pairs of input functions and solution functions to learn operator mapping.",
            "ai_methodology_category": "physics-informed / operator learning",
            "applicability": "Highly applicable for parametric PDE problems where fast repeated evaluations across varying input functions are needed; architectural choice depends on geometry and data representation.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Reported to achieve superior performance to conventional neural networks for operator tasks; FNO achieves scalable performance though with geometric limitations; large-scale training (thousands of GPUs) demonstrated for high-resolution tasks.",
            "impact_potential": "High — enables sampling-independent solvers that, once trained, can rapidly evaluate entire classes of PDE problems, improving simulation throughput and enabling inverse design and uncertainty quantification.",
            "comparison_to_alternatives": "Neural operators outperform coordinate-based PINNs for operator mapping tasks and standard CNNs when input is function-valued; FNO has advantages in scalability but may have geometric limitations relative to other methods.",
            "success_factors": "Sufficient coverage of input-function space in training data, architecture matched to domain geometry (spectral vs spatial), and large-scale compute for high-resolution training.",
            "key_insight": "Learning mappings between function spaces lets a single trained model solve whole classes of PDE instances rapidly, trading expensive offline training for fast online evaluation across parameter variations.",
            "uuid": "e2342.3",
            "source_info": {
                "paper_title": "Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Surrogate Turbulence Models",
            "name_full": "Machine-learned Surrogate Turbulence Models (hybrid RANS/ML surrogates)",
            "brief_description": "Data-driven turbulence closures or corrective modules that replace or augment traditional turbulence models (e.g., k-ω) within CFD simulations to improve accuracy or reduce computational cost.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Computational fluid dynamics — turbulence modeling for engineering and geophysical flows",
            "problem_description": "Model turbulent flows (wall-bounded or wall-free) where direct numerical simulation is infeasible; learn closures or correction terms from DNS/LES data to improve predictive accuracy of RANS or other reduced models.",
            "data_availability": "Training data often derived from high-fidelity simulations (DNS/LES) which are expensive; for some wall-bounded problems DNS datasets exist but are limited in parameter coverage, making generalization challenging.",
            "data_structure": "Spatio-temporal fields representing velocity, pressure, turbulent quantities; high-resolution 3D grids; possibly multi-scale and multi-modal.",
            "problem_complexity": "Extremely high: multiscale nonlinear chaotic dynamics, high-dimensional state, extrapolation beyond training regimes is common and hard, and boundary interactions add complexity.",
            "domain_maturity": "Active research area with many studies showing promising results but with open challenges in generalization and trustworthiness.",
            "mechanistic_understanding_requirements": "High - physics constraints (conservation laws, boundary behavior) and interpretability are critical for deployment in engineering contexts.",
            "ai_methodology_name": "Surrogate ML turbulence closures (data-driven turbulence models, NODE-based models)",
            "ai_methodology_description": "Approaches include directly modeling turbulence production terms, predictor-corrector schemes where an ML model corrects traditional model outputs, and NODE-based temporal models for atmospheric turbulence; deployment strategies include in-memory or remote inference and integration into simulation codes via frameworks like SmartSim.",
            "ai_methodology_category": "hybrid / surrogate modeling / physics-informed ML",
            "applicability": "Applicable for accelerating and improving reduced-order turbulence models; suitability depends on representativeness of training data and incorporation of physical constraints to ensure stable deployment.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Many studies report improved accuracy over empirical closures in certain regimes, but concerns remain about extrapolation and robustness; some ML closures can outperform empirical models when trained on representative DNS data.",
            "impact_potential": "High impact for engineering design (aircraft, rotorcraft) and climate modeling if robust generalization can be achieved, with potential for reduced compute cost and improved fidelity.",
            "comparison_to_alternatives": "Compared against empirical turbulence models (e.g., k-ω); ML-based models can outperform in trained regimes but may fail in extrapolation without physical constraints; hybrid predictor-corrector strategies provide pragmatic compromise.",
            "success_factors": "High-quality DNS/LES training data, incorporation of physical constraints, careful deployment/inference integration in HPC simulations, and methods to detect out-of-distribution regimes.",
            "key_insight": "Machine-learned surrogate turbulence models can improve predictions over empirical closures when trained on high-fidelity data and integrated with physics-aware deployment, but robust generalization and trust require physics constraints and careful validation.",
            "uuid": "e2342.4",
            "source_info": {
                "paper_title": "Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "WSI Cancer Detection (MIL)",
            "name_full": "Whole-Slide Image Cancer Detection using Multiple-Instance Learning",
            "brief_description": "Weakly-supervised approaches for gigapixel histopathology images that divide whole-slide images into patches and aggregate patch-level features via multiple-instance learning to predict slide-level labels with reduced annotation needs.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Computational pathology / cancer detection",
            "problem_description": "Detect cancerous regions and classify pathological outcomes from extremely high-resolution whole-slide images (WSIs) where pixel-level annotations are costly and rare.",
            "data_availability": "WSIs are available but often with limited pixel-level labels; slide-level labels are more common; data are large (gigapixel), requiring HPC resources for processing.",
            "data_structure": "Very high-resolution images (gigapixel), handled by patching into many smaller tiles; structured as sets/bags of patches per slide.",
            "problem_complexity": "High: enormous input size, class imbalance, high intra-class variability, multiple scales of morphology, and need to aggregate information across many patches.",
            "domain_maturity": "Mature research area with many methods (patching, MIL, self-supervised) and active improvements; state-of-the-art is evolving.",
            "mechanistic_understanding_requirements": "High for clinical deployment - interpretability and localization of predictions are needed for acceptance and to reduce false positives.",
            "ai_methodology_name": "Multiple-Instance Learning (MIL), patch-based CNNs, self-supervised learning",
            "ai_methodology_description": "Divide WSIs into patches, extract features via CNNs, and use MIL frameworks to aggregate patch features into slide-level predictions (weakly-supervised). Self-supervised pretraining can help when labels are scarce; patching strategies and postprocessing are required to manage false positives.",
            "ai_methodology_category": "supervised learning / weakly-supervised / self-supervised",
            "applicability": "Well-suited given scarcity of pixel-level labels; approaches are computationally intensive and benefit from HPC for training and inference on gigapixel images.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Patch-based MIL works well in many cases but can produce many false positives and requires careful postprocessing; self-supervised methods are promising but less mature in generality.",
            "impact_potential": "Significant — can automate screening, reduce expert workload, and enable large-scale pathology studies, but clinical deployment needs robust validation and interpretability.",
            "comparison_to_alternatives": "Patch-level supervised training requires pixel labels and is less scalable; MIL trades localization detail for scalable slide-level learning and reduced labeling cost.",
            "success_factors": "Access to large slide-level labeled datasets, careful patch selection and augmentation, scalable HPC pipelines, and interpretability techniques to highlight diagnostic regions.",
            "key_insight": "Weakly-supervised MIL on patchwise representations enables classification of gigapixel pathology slides with limited annotations, but managing false positives and ensuring interpretability are essential for clinical impact.",
            "uuid": "e2342.5",
            "source_info": {
                "paper_title": "Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "BigBird Pathology Model",
            "name_full": "Sparse Transformer (BigBird) for Pathology Report Analysis",
            "brief_description": "A sparse-attention transformer architecture adapted for long clinical documents, trained on millions of pathology reports to extract structured cancer information from narrative reports.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Medical document analysis / oncology informatics",
            "problem_description": "Extract structured cancer characteristics (site, subsite, laterality, histology, behavior) from long pathology reports to populate cancer registries.",
            "data_availability": "Large dataset available: 2.7 million pathology reports from six SEER registries used for training; labels exist for registry coding but may require mapping.",
            "data_structure": "Unstructured clinical text (long documents), variable-length and domain-specific language.",
            "problem_complexity": "High: very long documents exceeding standard transformer context windows, domain-specific vocabulary, class imbalance across extracted fields.",
            "domain_maturity": "Applied research maturity in clinical NLP; transformer models adapted for long sequences are mature and being actively applied.",
            "mechanistic_understanding_requirements": "Medium - downstream use requires reliable extraction and traceability but clinical interpretability of predictions is important.",
            "ai_methodology_name": "BigBird sparse-attention transformer (pathology BigBird)",
            "ai_methodology_description": "A sparse-attention transformer variant designed for long sequences; trained on 2.7M SEER pathology reports to perform information extraction tasks; sparse attention reduces memory and compute for long documents compared to dense transformers like BERT.",
            "ai_methodology_category": "supervised learning / foundation models / NLP",
            "applicability": "Highly applicable to long-document clinical NLP where document lengths exceed dense-transformer limits; sparse attention enables training on large corpora.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Demonstrated suitability for pathology information extraction at scale; choice of sparse transformer allowed handling of very long reports that dense models cannot process efficiently.",
            "impact_potential": "High — automates registry coding, accelerates data curation for epidemiology and research, and scales to national-scale clinical corpora.",
            "comparison_to_alternatives": "Sparse transformers (BigBird) are more suitable than dense attention models (BERT) for very long documents; specific performance metrics not provided in paper.",
            "success_factors": "Availability of very large labeled/unlabeled report corpora, architecture that accommodates long contexts (sparse attention), and domain adaptation to clinical language.",
            "key_insight": "Sparse-attention transformers trained on millions of pathology reports enable scalable extraction of structured cancer information from long clinical texts that dense models cannot handle efficiently.",
            "uuid": "e2342.6",
            "source_info": {
                "paper_title": "Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Computational Steering (Neutron Scattering)",
            "name_full": "AI-enabled Computational Steering for Neutron Scattering Experiments",
            "brief_description": "Edge-deployed AI workflows that analyze initial experimental scans in near-real-time to steer subsequent measurements, reducing expert decision time and improving experiment efficiency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "Experimental physical sciences — neutron scattering",
            "problem_description": "Automate decision-making in multi-stage neutron scattering experiments to identify regions of interest and determine refined scan parameters in near-real-time.",
            "data_availability": "Experimental scan images and telemetry are generated during beamline operations; data are available in real-time but may be limited per experiment and require fast processing at the edge.",
            "data_structure": "Image-like scan outputs, time-ordered experimental telemetry streams; data are streaming and potentially multimodal.",
            "problem_complexity": "Medium to high: real-time constraints, heterogeneity of experimental setups, and need for robust detection/localization under variable conditions.",
            "domain_maturity": "Emerging — prototypes and workflows demonstrated (ORNL example) but not yet widespread.",
            "mechanistic_understanding_requirements": "Medium - decisions must be explainable to beamline scientists and respect experimental constraints; trustworthiness is important.",
            "ai_methodology_name": "Edge-deployed computer vision models and autonomous workflows (AI steering)",
            "ai_methodology_description": "Computer vision models infer features from initial scans; a workflow deployed at the edge assimilates telemetry and model outputs to recommend next experimental parameters; asynchronous HPC/edge integration supports low-latency steering.",
            "ai_methodology_category": "supervised learning / edge AI / autonomous experimentation",
            "applicability": "Appropriate for experiments with iterative scanning and clear decision rules informed by imaging; reduces latency of human decision-making and increases throughput.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Described as having potential to drastically reduce time for decision-making from days to near-real-time; specific deployment (Yin et al.) demonstrates feasibility.",
            "impact_potential": "High — enables autonomous or semi-autonomous experiments, increases facility throughput, and accelerates discovery cycles.",
            "comparison_to_alternatives": "Manual expert steering is slower and less scalable; AI steering offers speed and potential consistency benefits but requires validation and expert oversight.",
            "success_factors": "Low-latency edge compute, reliable computer vision models tuned to instrument data, integration with experimental control systems, and human-in-the-loop oversight for safety.",
            "key_insight": "Edge-deployed AI can meaningfully accelerate experimental workflows by enabling near-real-time decision-making, but requires integration with instrument control and rigorous validation for trust.",
            "uuid": "e2342.7",
            "source_info": {
                "paper_title": "Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Uncertainty Quantification (UQ)",
            "name_full": "Uncertainty Quantification Methods for Scientific ML (GPR, PI-GAN, Monte Carlo Dropout, PI3NN)",
            "brief_description": "A set of probabilistic and ensemble-based approaches to estimate predictive uncertainty in scientific ML models, including Gaussian processes, physics-informed GANs, Monte Carlo dropout, and prediction-interval neural nets.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "scientific_problem_domain": "General scientific modeling requiring predictive confidence (multiple domains)",
            "problem_description": "Provide reliable uncertainty estimates for ML predictions to support scientific decision-making, detect OOD samples, and quantify confidence for high-consequence applications.",
            "data_availability": "Varies by domain; uncertainty methods can operate with limited data but quality of uncertainty estimates depends on data representativeness and noise characteristics.",
            "data_structure": "Applies across data types: tabular, structured, spatio-temporal, and functional data.",
            "problem_complexity": "Medium to high: requires modeling both epistemic and aleatoric uncertainty, scalable inference for large datasets, and OOD detection.",
            "domain_maturity": "Maturing — many established methods (GPR) and modern neural approaches (MC dropout, PI3NN) with active research on calibration and scalability.",
            "mechanistic_understanding_requirements": "High for scientific applications — uncertainty estimates must be trustworthy and interpretable to inform experiments and safety-critical decisions.",
            "ai_methodology_name": "Gaussian Process Regression (GPR), Physics-Informed GAN (PI-GAN), Monte Carlo Dropout, PI3NN prediction-interval networks",
            "ai_methodology_description": "GPR provides Bayesian predictive distributions but scales poorly; PI-GANs incorporate physics into generative models to produce distributions; Monte Carlo Dropout uses dropout at inference to approximate Bayesian model uncertainty; PI3NN uses three networks to produce prediction intervals and OOD-aware bounds.",
            "ai_methodology_category": "probabilistic ML / uncertainty quantification",
            "applicability": "Essential for scientific ML where decision-making demands confidence estimates; choice of method depends on data scale and required fidelity of UQ.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Methods like PI3NN are noted as state-of-the-art for out-of-distribution-aware prediction intervals; GPR is principled but needs variational approximations for scale.",
            "impact_potential": "High — UQ methods are critical to adoption of ML in science, enabling risk-aware decisions, model validation, and detection of OOD regimes.",
            "comparison_to_alternatives": "GPR offers strong theoretical guarantees but poor scalability; MC Dropout is scalable but approximate; PI-GANs and PI3NN offer physics-aware or interval-focused alternatives with better OOD behavior in some cases.",
            "success_factors": "Incorporation of physics constraints, scalable approximations for large data, and careful calibration and validation on representative datasets.",
            "key_insight": "Reliable uncertainty quantification is indispensable for scientific ML; hybrid and interval-based neural approaches offer scalable, OOD-aware uncertainty estimates suited for high-consequence scientific applications.",
            "uuid": "e2342.8",
            "source_info": {
                "paper_title": "Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Symbolic Regression / Equation Discovery",
            "name_full": "Symbolic Regression and Sparse Identification (e.g., SINDy, neural symbolic methods)",
            "brief_description": "Methods to infer explicit mathematical expressions or governing equations from data using techniques like sparse regression, genetic programming, and neural-symbolic approaches.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Equation discovery / model discovery in dynamical systems and physics",
            "problem_description": "Discover interpretable closed-form equations that describe observed system dynamics from data, enabling mechanistic understanding and reduced-order models.",
            "data_availability": "Often limited; high-quality time-series or spatio-temporal data are required; sparse regression methods prefer noise-free or denoised data.",
            "data_structure": "Time-series and state trajectories; can be high-dimensional but methods often seek low-dimensional governing expressions.",
            "problem_complexity": "High: combinatorial search over symbolic expressions, sensitivity to noise, and curse of dimensionality in candidate function spaces.",
            "domain_maturity": "Established research area with classical genetic programming and modern ML-enhanced methods (Cranmer et al., Biggio et al.).",
            "mechanistic_understanding_requirements": "Very high - primary aim is interpretable mechanistic equations rather than black-box prediction.",
            "ai_methodology_name": "Symbolic regression (genetic programming), sparse regression (SINDy), neural symbolic regression",
            "ai_methodology_description": "Methods range from genetic programming to neural-network-guided symbolic discovery; sparse regression methods select parsimonious terms from a library of candidate functions to identify governing equations.",
            "ai_methodology_category": "symbolic / interpretable ML / equation discovery",
            "applicability": "Applicable when interpretability is primary and data quality allows identification of parsimonious models; less applicable in noisy, high-dimensional cases without preprocessing.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "UDEs combined with sparse regression can outperform direct sparse regression on whole equations; scalable neural symbolic methods can handle larger problems than classic GP but still have limitations.",
            "impact_potential": "High for scientific insight — direct extraction of governing equations can advance theory and enable simpler predictive models.",
            "comparison_to_alternatives": "Symbolic methods provide interpretability compared to black-box ML; UDE+sparse regression can be more effective than SINDy alone by restricting learned components.",
            "success_factors": "High-quality, appropriately sampled data, domain knowledge to construct candidate libraries or constrain search, and methods to mitigate noise sensitivity.",
            "key_insight": "Symbolic and sparse-regression approaches can recover interpretable governing equations, especially when combined with hybrid modeling that limits search to unknown components, improving discovery in scientific contexts.",
            "uuid": "e2342.9",
            "source_info": {
                "paper_title": "Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Temporal Forecasting Architectures",
            "name_full": "Temporal CNNs, RNNs (LSTM), Transformers, and NODEs for Time-series Forecasting",
            "brief_description": "A set of neural architectures effective for transient scientific forecasting: dilated causal CNNs (TCNN), recurrent models (LSTM), attention-based transformers, and Neural ODEs for continuous-time dynamics.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Time-series forecasting in sciences (fluid dynamics, turbulence, weather/climate)",
            "problem_description": "Predict future states of transient processes given historical sequence windows, often for systems with multi-scale temporal dynamics.",
            "data_availability": "Depends on domain; time series may be plentiful (e.g., weather reanalyses) or limited (experimental runs); preprocessing into time windows required.",
            "data_structure": "Time series and sequences, possibly multivariate and high-dimensional (fields over space and time).",
            "problem_complexity": "High: long-range dependencies, multi-scale dynamics, non-stationarity, and high-dimensional outputs (spatio-temporal fields).",
            "domain_maturity": "Mature toolkit from ML community with specific adaptations for scientific forecasting; NODEs add continuous-time modeling benefits.",
            "mechanistic_understanding_requirements": "Medium - forecasting often values predictive accuracy, but interpretability and physical consistency remain important in science.",
            "ai_methodology_name": "Temporal CNNs (dilated causal), RNNs (LSTM), Transformers, Neural ODE variants",
            "ai_methodology_description": "TCNNs use dilated causal convolutions for long-range temporal dependencies; LSTMs capture sequential dependencies via gated RNNs; transformers use attention for sequence modeling; NODEs model derivatives and integrate continuously for improved extrapolation.",
            "ai_methodology_category": "supervised learning / sequence modeling / hybrid (NODE)",
            "applicability": "Widely applicable; TCNNs and transformers excel with large datasets and long contexts, NODEs useful for continuous-time extrapolation and physical-consistency needs.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Each architecture has tradeoffs: transformers excel with long-context data but are costly; TCNNs and LSTMs are effective for certain scales; NODEs perform well in extrapolation for turbulence forecasting.",
            "impact_potential": "High for improved forecasting in climate, turbulence, and other transient scientific systems, enabling better predictions and decision support.",
            "comparison_to_alternatives": "Transformers vs RNNs: transformers scale better for long-context but with quadratic cost; NODEs can outperform discrete-time models in extrapolation contexts.",
            "success_factors": "Proper input sequence construction (windowing), architecture selection for context length and compute constraints, and integration of physical constraints where possible.",
            "key_insight": "No single sequence architecture dominates; choice depends on context length, compute cost, and whether continuous-time extrapolation (NODE) or long-range attention (transformer) is most valuable for the scientific forecasting task.",
            "uuid": "e2342.10",
            "source_info": {
                "paper_title": "Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Coarse-graining / Data-driven Discretization",
            "name_full": "Data-Driven Discretization for Coarse-Grained PDE Solutions",
            "brief_description": "Learned discretization operators that enable high-fidelity solutions of PDEs on much coarser grids by training data-driven numerical stencils or operators.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Numerical simulation / PDE solvers (fluid dynamics, thermodynamics)",
            "problem_description": "Reduce computational cost of high-fidelity PDE simulation by learning discretizations that approximate fine-grid behavior on coarser meshes.",
            "data_availability": "Requires high-fidelity fine-grid simulation data to train coarse-grained operators; such data can be expensive to generate but are used to produce training labels.",
            "data_structure": "Spatially discretized field data (grids) with corresponding fine-grid solutions used as supervision for coarse-grid operator learning.",
            "problem_complexity": "High: capturing multiscale interactions on coarse grids is challenging; learned discretizations must preserve stability and conservation properties.",
            "domain_maturity": "Emerging approach inspired by image super-resolution and GANs; promising early results but ongoing research on stability and generalization.",
            "mechanistic_understanding_requirements": "High - numerical stability and adherence to conservation laws are important; interpretability of learned stencils may be desired.",
            "ai_methodology_name": "Data-driven discretization / coarse-graining using ML (GAN-inspired super-resolution techniques)",
            "ai_methodology_description": "Train networks (often convolutional architectures) to map coarse-grid representations to high-fidelity approximations or to learn discretization operators; methods adapt concepts from image super-resolution (e.g., GANs) to PDE contexts.",
            "ai_methodology_category": "supervised learning / surrogate modeling",
            "applicability": "Applicable when fine-grid simulation data are available to supervise learning and when reducing runtime for many-query settings is critical.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Reported to allow accurate solutions on coarser grids in some PDE problems; success depends on representativeness of training data and ability to enforce stability.",
            "impact_potential": "Potentially large computational savings for simulation-driven science and engineering, enabling faster design loops and multiscale coupling.",
            "comparison_to_alternatives": "Traditional coarse-graining uses analytically derived closures; data-driven methods can capture finer features but risk instability and lack of theoretical guarantees.",
            "success_factors": "Access to diverse high-fidelity simulation datasets, incorporation of conservation or stability constraints, and careful architecture/training choices to avoid spurious artifacts.",
            "key_insight": "Learning discretizations from data can deliver high-fidelity coarse-grid solutions, trading offline training cost for significant online simulation speedups if stability and generalization are achieved.",
            "uuid": "e2342.11",
            "source_info": {
                "paper_title": "Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLMs for Drug Discovery",
            "name_full": "Large Language Models and Foundation Models for Drug Discovery",
            "brief_description": "Large pretrained language or sequence models adapted to molecular sequences and chemistry tasks to accelerate candidate generation, property prediction, and virtual screening.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "scientific_problem_domain": "Drug discovery / molecular design",
            "problem_description": "Predict molecular properties, propose candidate molecules, and assist virtual screening by leveraging learned sequence/structure representations.",
            "data_availability": "Molecular datasets can be large (public chemical libraries) but high-quality labeled assay data may be limited; models often pretrain on large unlabeled corpora and fine-tune on task-specific datasets.",
            "data_structure": "Sequential representations (SMILES, protein sequences), graph-structured molecular data, and multimodal (sequence+structure) data.",
            "problem_complexity": "High: combinatorial chemical space is enormous, properties result from complex physics/chemistry, and experimental validation is expensive.",
            "domain_maturity": "Rapidly emerging with growing adoption of foundation models adapted to chemistry and biology; research is active and methods evolving.",
            "mechanistic_understanding_requirements": "Medium to high - drug development benefits from interpretable predictions and mechanistic hypotheses, though generative proposals may be acceptable for exploration.",
            "ai_methodology_name": "Large Language Models / foundation models adapted for molecular tasks (sequence models, LLMs)",
            "ai_methodology_description": "Pretrain large transformer-based models on large corpora of chemical/molecular sequences or text and fine-tune for property prediction, inhibitor identification, or generative design; may be integrated into high-throughput virtual screening pipelines.",
            "ai_methodology_category": "foundation models / supervised & unsupervised learning / generative modeling",
            "applicability": "Applicable for representation learning, candidate generation, and accelerating early-stage discovery; dependent on fine-tuning data and validation pipelines.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Promising for improving virtual screening and representation of molecules; specific effectiveness varies by task and data availability.",
            "impact_potential": "High — can accelerate lead identification and reduce experimental screening burden if integrated into HTVS pipelines and validated experimentally.",
            "comparison_to_alternatives": "Classical cheminformatics and QSAR rely on handcrafted features; LLMs offer richer learned representations and generative capabilities but require large compute and careful validation.",
            "success_factors": "Large pretraining corpora, task-relevant fine-tuning data, integration with physics-aware filters and experimental validation, and scalable HTVS workflows.",
            "key_insight": "Foundation models adapted to molecular data can provide powerful representations and generative capabilities for drug discovery, but utility depends on high-quality finetuning and integration into screening pipelines with experimental feedback.",
            "uuid": "e2342.12",
            "source_info": {
                "paper_title": "Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "SmartSim / Petascale Inference",
            "name_full": "SmartSim and Scalable Inference Frameworks for HPC-integrated Surrogates",
            "brief_description": "Middleware and frameworks (e.g., SmartSim, RedisAI, TensorFlow Serving) and studies showing petascale inference deployments to integrate ML surrogates into large-scale simulations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "scientific_problem_domain": "HPC-integrated surrogate deployment (rotorcraft aerodynamics, ocean/climate modeling, CFD)",
            "problem_description": "Deploy ML surrogates at scale alongside traditional simulations to perform inference within simulation loops, enabling cognitive simulations and hybrid AI/simulation workflows.",
            "data_availability": "Surrogate models trained offline require high-fidelity training datasets; inference uses runtime simulation data which is abundant in large-scale simulations.",
            "data_structure": "Model input/output tensors of simulation states (high-dimensional arrays), streaming telemetry for decision-making.",
            "problem_complexity": "High: needs low-latency, high-throughput inference integrated with MPI simulations; scaling inference across thousands of GPUs demands sophisticated orchestration.",
            "domain_maturity": "Developing maturity with frameworks and exemplar studies demonstrating petascale inference (Brewer et al., Partee et al.).",
            "mechanistic_understanding_requirements": "Medium - surrogates must respect simulation stability and physical constraints for safe integration; interpretability may be required for debugging.",
            "ai_methodology_name": "Inference-serving frameworks (SmartSim, RedisAI, TensorFlow Serving) and remote/in-memory inference strategies",
            "ai_methodology_description": "Use of in-memory or RPC-based remote inference servers scaled asynchronously (load balancing or MPI messaging) to serve ML surrogate models to simulation codes; frameworks handle data movement and integration at HPC scale.",
            "ai_methodology_category": "systems / deployment / ML ops for science",
            "applicability": "Essential for integrating ML surrogates into production-scale simulations; selection of deployment strategy depends on latency, fault tolerance, and communication constraints.",
            "effectiveness_quantitative": null,
            "effectiveness_qualitative": "Studies show feasibility of petascale inference for rotorcraft aerodynamics and ocean climate modeling; effective integration requires careful orchestration and choice of serving strategy.",
            "impact_potential": "High — enables real-time surrogate-assisted simulation and reduced time-to-solution for multi-query experiments, facilitating cognitive simulations and digital twins.",
            "comparison_to_alternatives": "In-memory inference reduces latency but may be less fault-tolerant than RPC/load-balanced approaches; MPI-based message passing can be more performant but complex.",
            "success_factors": "Efficient model serving stacks, scalable orchestration, low-latency data paths, fault-tolerant strategies, and co-design with simulation code (C++/MPI integration).",
            "key_insight": "Scalable inference frameworks are critical to realize hybrid AI/simulation workflows at HPC scale, and deployment strategy must balance latency, performance, and fault tolerance to integrate ML surrogates effectively.",
            "uuid": "e2342.13",
            "source_info": {
                "paper_title": "Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
            "rating": 2,
            "sanitized_title": "physicsinformed_neural_networks_a_deep_learning_framework_for_solving_forward_and_inverse_problems_involving_nonlinear_partial_differential_equations"
        },
        {
            "paper_title": "Neural ordinary differential equations",
            "rating": 2,
            "sanitized_title": "neural_ordinary_differential_equations"
        },
        {
            "paper_title": "Universal differential equations for scientific machine learning",
            "rating": 2,
            "sanitized_title": "universal_differential_equations_for_scientific_machine_learning"
        },
        {
            "paper_title": "Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators",
            "rating": 2,
            "sanitized_title": "learning_nonlinear_operators_via_deeponet_based_on_the_universal_approximation_theorem_of_operators"
        },
        {
            "paper_title": "Fourier Neural Operator for Parametric Partial Differential Equations",
            "rating": 2,
            "sanitized_title": "fourier_neural_operator_for_parametric_partial_differential_equations"
        },
        {
            "paper_title": "Learning data-driven discretizations for partial differential equations",
            "rating": 1,
            "sanitized_title": "learning_datadriven_discretizations_for_partial_differential_equations"
        },
        {
            "paper_title": "Time-series forecasting with deep learning: a survey",
            "rating": 1,
            "sanitized_title": "timeseries_forecasting_with_deep_learning_a_survey"
        },
        {
            "paper_title": "Multiple instance learning of deep convolutional neural networks for breast histopathology whole slide classification",
            "rating": 1,
            "sanitized_title": "multiple_instance_learning_of_deep_convolutional_neural_networks_for_breast_histopathology_whole_slide_classification"
        },
        {
            "paper_title": "Big bird: Transformers for longer sequences",
            "rating": 2,
            "sanitized_title": "big_bird_transformers_for_longer_sequences"
        },
        {
            "paper_title": "Using machine learning at scale in numerical simulations with smartsim: An application to ocean climate modeling",
            "rating": 2,
            "sanitized_title": "using_machine_learning_at_scale_in_numerical_simulations_with_smartsim_an_application_to_ocean_climate_modeling"
        },
        {
            "paper_title": "Production deployment of machine-learned rotorcraft surrogate models on HPC",
            "rating": 2,
            "sanitized_title": "production_deployment_of_machinelearned_rotorcraft_surrogate_models_on_hpc"
        },
        {
            "paper_title": "Pi3nn: Out-of-distribution-aware prediction intervals from three neural networks",
            "rating": 1,
            "sanitized_title": "pi3nn_outofdistributionaware_prediction_intervals_from_three_neural_networks"
        }
    ],
    "cost": 0.0244865,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SCALABLE ARTIFICIAL INTELLIGENCE FOR SCIENCE: PERSPECTIVES, METHODS AND EXEMPLARS
24 Jun 2024</p>
<p>Wesley Brewer brewerwh@ornl.gov 
National Center for Computational Sciences Oak Ridge National Laboratory Oak Ridge
37380TNUSA</p>
<p>Aditya Kashi kashia@ornl.gov 
National Center for Computational Sciences Oak Ridge National Laboratory Oak Ridge
37380TNUSA</p>
<p>Sajal Dash dashs@ornl.gov 
National Center for Computational Sciences Oak Ridge National Laboratory Oak Ridge
37380TNUSA</p>
<p>Aristeidis Tsaris tsarisa@ornl.gov 
National Center for Computational Sciences Oak Ridge National Laboratory Oak Ridge
37380TNUSA</p>
<p>Junqi Yin yinj@ornl.gov 
National Center for Computational Sciences Oak Ridge National Laboratory Oak Ridge
37380TNUSA</p>
<p>Mallikarjun Shankar shankarm@ornl.gov 
National Center for Computational Sciences Oak Ridge National Laboratory Oak Ridge
37380TNUSA</p>
<p>Feiyi Wang fwang2@ornl.gov 
National Center for Computational Sciences Oak Ridge National Laboratory Oak Ridge
37380TNUSA</p>
<p>SCALABLE ARTIFICIAL INTELLIGENCE FOR SCIENCE: PERSPECTIVES, METHODS AND EXEMPLARS
24 Jun 202463EB1DB823150EF7A53E741628F91514arXiv:2406.17812v1[cs.LG]scalingmachine learningscienceneural networks
In a post-ChatGPT world, this paper explores the potential of leveraging scalable artificial intelligence for scientific discovery.We propose that scaling up artificial intelligence on high-performance computing platforms is essential to address such complex problems.This perspective focuses on scientific use cases like cognitive simulations, large language models for scientific inquiry, medical image analysis, and physics-informed approaches.The study outlines the methodologies needed to address such challenges at scale on supercomputers or the cloud and provides exemplars of such approaches applied to solve a variety of scientific problems.</p>
<ol>
<li>
<p>The distinct requirements that differentiate AI for Science (AI4S) from consumer-centric AI.</p>
</li>
<li>
<p>The need for specific methods and architectures to accommodate scientific data as well as enforce laws of science.</p>
</li>
<li>
<p>The strategic shift from a singular, large monolithic model towards a synergistic mixture of experts (MoE) models.</p>
</li>
<li>
<p>The imperative of scaling not merely in computational terms, but also across infrastructural resources and integrated research infrastructures (IRI).</p>
</li>
</ol>
<p>Our paper is laid out as follows: Section 2 discusses lessons that were learned from scaling GPT-3; Section 3 contrasts how scientific AI differs from consumer-grade AI; Section 4 introduces specific types of techniques that are unique to scientific AI, such as soft-penalty constraints and neural operators; Section 5 discusses how to train and deploy neural networks at scale on supercomputers; Section 6 outlines the different types of AI-HPC workflows that are typically used in practice; and Section 7 concludes with a summary as well as numerous perspectives on where we believe the field is headed.Along the way, we disperse various exemplars of using such methods, touching on various topics that employ different methods and data modalities, such as: LLMs for drug discovery; physics-informed approaches for modeling turbulence; medical document analysis for critical insights into cancer diagnosis and management; whole slide image analysis for cancer detection; and computational steering for neutron scattering experiments.</p>
<p>2 Lessons from GPT-3</p>
<p>Large models and data, whether from neural architecture or training size, require distribution across multiple GPUs for scalability, necessitating parallel scaling.Consider that it would take 288 years to train the GPT-3 large language model on a single NVIDIA V100 GPU [14]; however, using parallel scaling techniques, this time drops to 36 years on eight GPUs or seven months on 512 GPUs [15], and just 34 days using 1024 A100 GPUs [15].Besides faster training, scaling enhances model performance [16].</p>
<p>Parallel scaling has two main approaches: model-based and data-based parallelism.Model-based parallelism is needed when models exceed GPU memory capacity, like GPT-3's 800GB size versus an NVIDIA A100's 80GB memory [16].</p>
<p>Data-based parallelism arises from the large amounts of data that are required to train such models, e.g., GPT-3 requires 45TB of text data for training [17].We explore this subject in more detail in Section 4.</p>
<p>How AI for Science is Different</p>
<p>Given that "data is the lifeblood of AI", it is important to understand how scientific data in AI4S differs from consumer or commercial data and the techniques to handle it.Scientific data is often more sparse and less accessible than commercial data, typically provided via expensive experiments or simulations.This data might have fewer labels or show asymmetry, with some samples labeled and others not.</p>
<p>For scientific model validation, high-precision floating-point numbers are common, while consumer models often inference as 8-bit integers.The requirements for AI in engineering, like self-driving cars or virtual sensors in helicopters, are much stricter than for photo generation or modification.Trustworthiness is paramount for AI4S applications [20].</p>
<p>Moreover, neural networks in physics-based applications might need to impose boundary conditions or conservation laws.This is particularly true for surrogate models, which replace parts of larger simulations, like machine-learned turbulence models.</p>
<p>Methods for Scientific Machine Learning</p>
<p>Unlike conventional machine learning in consumer IT and commerce, natural sciences present unique requirements and opportunities.Three other pillars of investigation-theory, experiment, and numerical computation-have long provided valuable insights.Exploring the dynamic interplay between these pillars and machine learning is of great interest.Certain natural science fields demand greater accuracy and reliability than typical machine learning.Aerospace engineering, nuclear physics and engineering, and medical research are prime examples.Furthermore, limited data availability poses challenges, particularly in fields such as medical research with privacy concerns and costly experiments.</p>
<p>This section describes methods of scientific machine learning, which broadly cater to the following needs of scientific computing:</p>
<p>• Utilizing known domain knowledge, particularly physics; this is commonly in the form of partial differential equations (PDEs)</p>
<p>• Dealing with multiple inputs and outputs which are not independent numbers but represent functions over space and time</p>
<p>• Identifying patterns and dynamics in terms of mathematical formulae</p>
<p>• Error estimates and uncertainty quantification</p>
<p>These factors have led to the emergence of methods that we collectively refer to as "scientific machine learning".</p>
<p>Many of these methods deal with differential equations, particularly PDEs.The PDE stands as a far-reaching, expressive, and ubiquitous modeling tool in mathematics and the sciences.A PDE model includes the equation, spatial domain, boundary conditions and initial conditions.It involves sets of functions (the 'state') defined in the domain of interest in space-time.Solving PDEs is crucial in prediction and analysis (forward problem) and design, control, and decisionmaking (backward problem).While PDEs are potent tools, relying solely on them may lead to challenges when dealing with complex phenomena.In computational fluid dynamics, for example, obtaining highly accurate solutions using direct numerical simulation can be prohibitively expensive for realistic turbulent flows, while approximating them with turbulence models can lead to incorrect predictions.Augmenting PDEs with data-driven models has been explored to address these issues and improve model accuracy and cost.Efforts to integrate physics-based and machine learning models have yielded promising results.</p>
<p>Soft Penalty Constraints</p>
<p>Physics-informed neural networks (PINNs) are an increasingly common approach for modeling PDEs [21].These networks take spatial coordinates (x, y, z, t) as input and output an approximate solution u(x, y, z, t) to the PDE R(u) = 0 at that location and time (see figure 1).Incorporating the PDE residual norm ∥R(u)∥ in the loss function ensures that the model optimizer minimizes both the data loss and the PDE residual, leading to a satisfying approximation of the physics.Raissi et al. [21] use simple feed-forward neural networks with a tanh activation function for a 1D geometry, while Chen et al. [22] apply PINNs to solve 2D inverse problems in nano-optics, demonstrating the promising role of AI in solving such challenges.</p>
<p>Neural Ordinary Differential Equations (NODE)</p>
<p>The idea behind the original NODE [23] landmark paper is to use a neural network to learn the time derivative of a dataset, and then use an ODE solver to time integrate the neural network.The difficulty of such approaches lies in how to perform backpropagation through the ODE solver, which is handled using the adjoint sensitivity approach.Such methods are useful to solve temporal forecasting problems, due to the fact that they perform well in extrapolation mode.There have been a numerous variants of NODE since the original paper, including: Augmented (ANODE) [24], Stochastic (SDE-NODE) [25], Discretely updated (DUNODE) [26], Higher-order (HONODE) [27], Controlled (CNODE) [28], and Piecewise (PNODE) [29] variants.</p>
<p>Universal Differential Equations</p>
<p>Rackauckas et al. [30] introduced 'universal differential equations' (UDE) where neural networks model components of the equations.UDEs are particularly useful when certain physics terms are well-defined using operators, while others require data-driven modeling.These equations can be stochastic, and some components may be algebraic, leading to differential algebraic equation (DAE) systems.The authors provide Julia-based software toolkits for working with UDEs.They demonstrate that training neural network components of UDEs and employing sparse regression leads to improved equation discovery compared to direct sparse regression on the entire differential equation (e.g., SINDy [31]).UDEs can also encompass PDEs.For example, they demonstrate learning one-dimensional reaction-diffusion equations by using a convolutional neural network (CNN) for the diffusion operator and a simple feedforward network for the nonlinear reaction operator, trained for specific geometries and fixed boundary conditions.Moreover, the authors showcase learning a sub-model for incompressible Navier-Stokes equations, the averaged temperature flux model, employing the Boussinesq approximation for gravity.</p>
<p>Neural Operators</p>
<p>An interesting and important area of research involves 'neural operators' -maps between function spaces using neural networks.These operators are especially useful for PDEs and scenarios dealing with functions or signals, like non-uniform material properties.For instance, thermal conductivity ν may vary in space in the heat equation
∂u ∂t − ν(x)∇ 2 u = 0.(1)
The objective is now to map any given input function ν to the solution u of the PDE, unlike regular models (like most PINNs) that take the coordinates (x, t) as input and output the value of u at that point.</p>
<p>Lu et al. [32] introduced 'DeepONet', a deep operator network, that learns linear or nonlinear operators.They utilize branch and trunk networks (figure 2), demonstrating superior performance over other neural networks.Li et al. [33] introduced 'graph kernel networks' for solving elliptic PDEs, using graph neural networks to learn kernel functions.An innovative new technique that leverages classical Fourier analysis is the Fourier neural operator [34,35], though it also has geometric limitations.Kurth et al. [35] trained their neural operator model on up to 3808 GPUs on some of the world's largest supercomputing systems.Their adaptive Fourier neural operator architecture obtains reasonably good results with high, scalable performance.Neural operators can also incorporate known physics [36,37].</p>
<p>Coarse-graining</p>
<p>Machine learning can be used to improve simulations by coarse-graining using data-driven discretization [38].Datadriven discretization allows for the high fidelity solution of PDEs on much coarser grids.Such methods were inspired by the concept of super-resolution of images using a generative adversarial network (GAN) [39].</p>
<p>Temporal forecasting</p>
<p>Many scientific problems are transient in nature, such as fluid dynamics in a pumping heart.Lim and Zohren [40] suggest that three particular neural network architectures are effective for temporal forecasting type problems: (1) CNN with dilated causal convolutional layers -sometimes referred to as Temporal CNNs (TCNN), ( 2) recurrent neural networks (RNN) such as long short-term memory (LSTM), and (3) attention-based models such as the transformer architecture [18].One point to note for such problems is that the training data must be first converted to time sequences, given a window size.</p>
<p>One novel technique for modeling temporal forecasting is neural ordinary differential equations (NODE) [23], where a neural network is used to learn the time derivative of a dataset, and then an ODE solver is used to integrate the neural network in time.The difficulty of such approaches lies in how to perform backpropagation through the ODE solver, which is handled using the adjoint sensitivity approach.Such methods are useful to solve temporal forecasting problems, e.g., turbulence forecasting [41], especially due to the fact that they perform well in extrapolation mode.There have been a numerous variants of NODE since the original paper, which are covered in [42].</p>
<p>Symbolic Regression</p>
<p>Symbolic regression is a method for learning explicit mathematical representations from data.One popular method for this can be traced back to genetic programming by [43].Recent methods have leveraged machine learning techniques, e.g., Cranmer et al. [44], and developed scalable implementations of such methods, e.g., Biggio et al. [45].There are both linear [31] and nonlinear methods [46], though both classes of methods can learn nonlinear functional forms.</p>
<p>Uncertainty Quantification</p>
<p>A challenge in using AI for science is the lack of reliability estimates for predictions.Scientific models should not only predict outcomes but also quantify uncertainty [47].Methods such as Gaussian Process Regression (GPR) are in essence neural networks that predict probability distributions.GPFlow [48] is a framework built on TensorFlow Probability that is able to solve such problems at scale.Physics-informed Generative Adversarial Networks (PI-GAN) have also been able to predict variables along with uncertainty [49].MonteCarlo Dropout [50] is another technique which uses the idea of dropout -a method often used for regularization in the training of neural networks [51] -but instead of using it during training, it is used during inference.The state-of-the-art for uncertainty prediction are prediction interval methods, such as PI3NN [52], which uses a linear combination of three neural networks to predict confidence levels.The PI3NN method is also able to predict when unseen samples lie outside the bounds of the training data, i.e., out-of-distribution (OOD).</p>
<p>Surrogate Models</p>
<p>Beyond efforts to replace traditional simulations with AI/ML methods like NVIDIA Modulus [56], there are increasing efforts on hybrid AI-simulations, also known as cognitive simulations (CogSim).A common example of this type of workflow is machine-learned surrogate modeling, in which case only a portion of the overall workflow is replaced with a machine-learned model.For example, rather than use a traditional turbulence model, such as k-ω, data-driven</p>
<p>Example: Modeling Turbulence Richard Feynman once described turbulence as "the most important unsolved problem of classical physics".There have been numerous research efforts towards the efforts of using AI/ML to understand and model turbulence.These generally fall into either wall-bounded turbulence, such as found in engineering problems of airplanes or ships, or wall-free turbulence, which is found in geophysical flows, such as found in atmospheric and oceanic flows.Wall-bounded flows generally train on Direct Numerical Simulation data to train a model which is deployed using strategies shown in Fig. 3 -a recent survey of such methods may be found in [53].On the other hand, stateof-the-art approaches for modeling atmospheric turbulence are using NODE-type techniques, such as described by Shankar et al. [54].Finally, there is also ongoing work which uses unsupervised learning to analyze the structure of stratified turbulent flows, such as the work by Couchman et al. [55].</p>
<p>Figure 3: Different strategies for deploying machine-learned surrogate models on HPC [64].</p>
<p>machine-learned turbulence models can be learned, which can often outperform empirically based models.In this case either the turbulence production term can be modeled directly, or a predictor-corrector type implementation can be employed, where the machine-learned model corrects the prediction of the traditional model [57,53].Such methods may be deployed using either in-memory or remote inference strategies via remote procedure calls (RPC) as shown in Fig. 3. Partee et al. [58] present SmartSim, a framework for augmenting simulations to inference machine-learned surrogate models at scale.Brewer et al. [59] study inferencing at petascale for surrogate models used in rotorcraft aerodynamics with both a RedisAI-based framework as well as TensorFlow Serving [60].Boyer et al. [61] extend this study to fully integrate inference serving techniques in to C++ computational physics simulations in a scalable way using MPI.</p>
<p>Finally, one of the more interesting implementations of blending simulations and machine-learned models is the work by Vlachas et al. [62], in which they define "algorithmic alloys" to meld machine learning approaches with more traditional approaches to learn effective dynamics of complex systems.</p>
<p>A new area of implementation that also blends simulation and machine-learned models is the area of digital twins [63], where there is not only simulation and machine-learned models intercommunicating, but also real-time telemetry data from the physical twin being assimilated into the simulation.</p>
<p>Deployment and Distributed Inference</p>
<p>Inference may be performed using either an in-memory approach or an remote approach (client-server architecture) as shown in Fig. 3.In the case of remote inference using remote procedure calls (RPC), it is typically scaled asynchronously, where multiple inference servers run in parallel, and inference requests are sent in an embarrassingly parallel fashion.There are generally two ways scaling is achieved: (a) using a load balancer (e.g., haproxy), or (b) message-passing approach using the message passing interface (MPI).While the latter method can be more performant than the former, the former approach may be better for handling node faults, and thus may be more appropriate for high availability.</p>
<p>Yin et al. [64]</p>
<p>Methods of Parallel Scaling</p>
<p>Many frameworks have been engineered to accommodate the increasing demand for highly scalable AI systems and complex workflows.Examples of such frameworks include: Horovod [67], Megatron-LM [15], DeepSpeed [68], and Fully Sharded Data Parallel (FSDP) [69].Each of these frameworks employs different strategies to enable scalability, primarily falling into two categories of parallelism techniques: Data Parallelism and Model Parallelism, with Pipeline Parallelism being the most prominent implementation of the latter.Intriguingly, most of these approaches are orthogonal to each other, as illustrated in Fig. 4, meaning that they address unique challenges and bottlenecks in system performance.By understanding these nuances, researchers can more effectively leverage the right frameworks and parallelism strategies to optimize their AI deployments.For example, recent studies have investigated such techniques for deploying large language models that scale efficiently on leadership-class supercomputers [70,71].</p>
<p>Data-Parallel Training</p>
<p>In data-parallel training, the model is replicated across n compute devices.A "mini-batch" of data is divided into n parts, known as "micro-batches".Each copy of the model is trained on one micro-batch during a forward pass; the individual losses are then aggregated (typically via a ring-allreduce as shown in Fig. 5), and the gradient of the aggregated loss is back-propagated to update the model parameters in parallel.</p>
<p>To optimize GPU utilization, we use a large micro-batch size per device, resulting in a larger mini-batch.While large-scale high-performance computing (HPC) systems enabled data-parallel training in unprecedented scale, model convergence suffers with large mini-batches.Several approaches have been developed to mitigate these large-batch related issues: gradual warm-up, layer-wise adaptive learning rate scaling (LARS), batch-normalization, mixed-precision training, and utilization of second derivatives of loss.</p>
<p>Model-Parallel Training</p>
<p>Model parallelism is typically employed when the model size or data sample size is too large to fit on a single GPU, or to improve the strong scaling of the application.The core idea of model parallelism is to place different sub-networks of a model on different devices.Compared to data-parallelism, where the model is duplicated across devices, model parallel implementations typically do not alter the parameter space of the problem, so the same hyper-parameters can work at different scales.Also, in the data-parallel method communication is limited to the backward pass; however, in most model parallel methods, communication is denser in the backward pass and is also present in the forward pass.The most straightforward method involves distributing different layers of the model among various devices: the parameters are spread across these devices, and the backward pass requires all-to-all communication.The most commonly used method is pipeline parallelism, where the layers of the model are distributed across devices without being cut, resulting in sparse communication.Pipeline parallelism is favored due to its efficient communication and is predominantly used across nodes.</p>
<p>In many scientific contexts, data samples are too large to fit on a single device.Spatial decomposition can be applied [73], wherein data is tiled across devices alongside parts of the model.This method has been successfully employed in [74] for large 3D computerized tomography images.The approach can also enhance strong scaling, or time-to-solution, by distributing the compute load across devices.Efficient compute and communication overlap is essential, and the LBANN framework addresses this [75,76].Most deep learning frameworks have integrated several model parallel implementations.For instance, TensorFlow provides Mesh TensorFlow [77], Pytorch supports intranode pipeline parallelism with GPipe [78], and internode parallelism via PyRPC [79].A comprehensive comparison is provided by [80], and efficient implementations of model parallelism across various architectures can be found in [78,15].The study in [81] simulated the communication costs for all three types of parallelism and their combinations: the parallel strategies mentioned are largely orthogonal to each other.When model parallelism is implemented, a combination of different model methods and data-parallel is often employed, depending on the communication bottlenecks of the application [15].</p>
<p>During the process of training a neural network, hyperparameter optimization (HPO) -the process of adjusting training-related hyperparameters (e.g., batch size, learning rate) and neural architectures (e.g., number of units per layer, number of layers, dropout rate) -is essential.There are different types of approaches for HPO, primarily either random search, Bayesian, or genetic algorithms.KerasTuner has recently become a popular approach for tuning TensorFlow models, which can be distributed across multiple nodes using a chief-worker strategy to allow for hundreds or thousands of differently configured neural networks to be trained in parallel.One of the novel algorithms that KerasTuner implements is HyperBand [88], which achieves speedup by trying many different combinations, but using a "principled early stopping" mechanism to discern which hyperparameters are optimal, which means the training events do not have to be run for hundreds of epochs to full completion.Ray Tune [89] is another widely used framework for distributed hyperparameter optimization that supports Hyperband, grid search, Bayesian optimization, and population-based methods.</p>
<p>Here, we list several examples which uses genetic algorithms for performing neural architecture search (NAS) on supercomputers.Genetic algorithms mimic the mutation selection process found in nature.By representing the neural network architecture as an array of bit strings (i.e., its genome), where each entry represents for example, the number of units or filters in a layer, or the dropout rate, a new candidate population can be formed by the reproduction operations (pairwise combination of two networks), then stochastically adding mutations, and then using selection to select the N fittest candidate architectures, where the fitness is typically defined according as the most accurate models.Multi-node Evolutionary Neural Networks for Deep Learning (MENNDL) uses such an approach to optimally design an convolutional neural network, utilizing a large number of compute nodes [90].DeepHyper [91] is an open source framework for performing NAS on HPC, which use the Balsam [92] framework to hide the complexities of scaling up the workflow on HPC.Such methods are able to speed up the development of designing optimal neural network architectures significantly.</p>
<p>AI-HPC Workflows</p>
<p>Over the years, several design patterns or execution motifs have emerged in solving AI problems on HPC.Brewer et al. [93] identify six execution motifs depicting how these techniques are deployed on HPC for scientific problems.The steering motif utilizes a trained ML model to steer an ensemble of simulations, such as [94].Multistage pipelining is an abstraction typically instantiated as high-throughput virtual screening (HTVS) pipelines that are typically used in drug discovery and the design of materials [95].Inverse design methods use machine learning methods to compute optimal solutions, e.g., in the search for new materials, without necessarily having to invert differential equations [96].Model duality refers to hybrid AI/ML simulations [58,64,61], also referred to as cognitive simulations (CogSim) [97], and such as used in digital twins [35].Distributed models and dynamic data are used in federated workflows, which span across an edge-to-cloud continuum [98,99].Finally, the adaptive execution motif describes the training of large scale AI/ML models, such as large language models (LLM), using techniques such as hyperparameter optimization or neural architecture search [90].</p>
<p>Conclusion</p>
<p>In this paper, we explored using AI for large-scale science on supercomputers.We introduced AI4S, emphasizing its significance, distinctive data modality, methods, application domains, and workflows.We delved into the various computational methods that are used for scaling such workflows, as well as the numerical methods which may be used to solve specific types of scientific problems.We covered a wide range of examples in various areas of science, and also covered specific use cases in more detail.Looking into the future of scalable AI, we outline these considerations:</p>
<ol>
<li>Increasingly hybrid.While there has been a considerable amount of research in developing neural networks, which may be used in lieu of traditional simulations [56], we do not foresee traditional simulations completely Figure 4: Showing the orthogonal nature of scalable AI workflows.</li>
</ol>
<p>Figure 5: Ring all-reduce approach to distributed training [72].</p>
<p>Example: Cancer Detection Much of the cancer detection research focuses on Whole Slide Images (WSI).WSIs are digital microscopy images acquired at very high pixel-level resolution.For example, a standard 4 x 6 cm glass slide at 40x magnification, after digitization turns into 200,000 x 300,000 pixels.A plethora of deep learning research has been performed around WSIs, and due to the extreme resolution of those images, the developed workloads are very well suited for HPC environments.The most common method is by patching the image online, and then training the model on a few selected patches.This method works very well in some cases; however, it has some major limitations.For example, pixel-level information is required, and careful post-processing is usually necessary due to the large number of false positives it creates [82].The state-of-the-art method on WSIs uses multiple-instance learning (MIL) algorithms [83], in which a large-size image is divided into multiple smaller images [84].The final decision is made by a weakly-supervised training model on the extracted features from the image patches [85].There are more recent self-supervised approaches [86,87], although the performance and the generality of those are not as mature.</p>
<p>Example: Computational Steering Scientific discoveries are often complex and involve multiple stages of decision-making, which have traditionally been carried out by human experts.Neutron scattering is one such experimental technique that involves initial scans to identify regions of interest and refined scans to take accurate measurements of atomic and magnetic structures and dynamics of matter.This process requires a beamline scientist to review the scan images and make decisions regarding the next set of experiment parameters, which can take several days to complete.However, with the advent of AI, particularly in computer vision, there is now an opportunity to develop AI-enabled methods to steer experiments.This has the potential to significantly reduce the time required for the decision-making process and improve the accuracy of experimental results.With the increasing computing power available today, AI-driven discoveries can be deployed at the edge through an autonomous workflow.Yin et al. [100] present an autonomous edge workflow for a neutron scattering experiment at Oak Ridge National Laboratory (ORNL), which utilizes AI to steer the experiment.This workflow has the potential to revolutionize the field of experimental science by enabling faster and more accurate decision-making, ultimately leading to more efficient and effective scientific discoveries.</p>
<p>being replaced by AI anytime soon.Rather, we see more hybrid HPC-AI applications, in the form of Cognitive Simulations "CogSim" or Digital Twins.2. Mixture-of-experts over monoliths.While large monolithic models have generally shown better performance with the downstream scientific tasks, their training cost is prohibitively expensive.An alternative is sparsely connected mixture of experts (MoE) where it is possible to scale the number of parameters many-fold at a fractional cost of a monolithic counterpart.GPT-4 is rumored to be a mixture of eight 220B parameter models.However, this may lead to complex inference pipelines.On the HPC front, model inter-communication can become a challenge.3. Complex inferencing pipelines.The introduction such methods as MoE and Hybrid AI/Sim will require more complex training/inferencing pipelines.For such systems to run efficiently on HPC, new innovative hardware that can handle the significant amounts of data movement will be important.4. AI for Autonomous Lab.With the Department of Energy (DOE) prioritizing integrated research infrastructure (IRI), we foresee a pivotal role for AI, particularly foundation models, in the realization of autonomous, self-driving laboratories.These cutting-edge facilities can harness the power of computational resources to enable real-time decision-making within the experimental environment, ultimately expediting scientific discovery.5. Resurrection of Linear RNNs.Because transformer-based LLMs are limited by context length and computationally expensive, i.e., training speed is quadratic in length and attention requires full lookback for inference, there have been significant efforts recently looking into attention alternatives, specifically in the form of Linear RNNs [101,102,103,104].While linear RNNs typically do not learn as effectively as attention-based models, their main advantage lies in their greater computational efficiency, especially for long token lengths.6. Operator-based models for solving PDEs.The beginning of the move to operator-based models that use the technology of neural networks will make AI much more useful for simulation of PDEs.The ability to infer functions from input functions in a sampling-independent way will make models, once trained, useful to solve an entire class of related problems rather than just one.7. The crucial role of multi-modal AI for general-purpose foundation models.There has been extensive research in vision-language (VL) models [105,106], but the scale in which those models are trained is still far behind LLMs.Also, most of the largest multi-modal models target VL image-level tasks rather than VL region-level localization tasks.As large-scale multi-modal datasets become available, and unified model architecture approaches are widely adopted [107,108], we might see at the same level of scaling multimodal models as LLMs, unlocking a much wider application potential.8. Importance of interpretability/explainability.Many scientists are skeptical of AI/ML methods for science.</p>
<p>To address such concerns, researchers have been developing tools to explain the rationale behind inference results.Class Activation Mapping (CAM) [109] and GRADient-weighted Class Activation Mapping (Grad-CAM) [110] can highlight important regions of an image while using a CNN model.Attention map visualization is a related concept applicable to transformer based models.Bringing the interpretation techniques to modern AI models is a timely endeavour.9. Emergence of science-inspired and science-informed neural network architectures.While transformer-based language models are becoming ubiquitous in various scientific applications, a new direction that maps underlying physical, chemical, and biological processes through the attention mechanism has come to the fore.</p>
<p>Problems in biochemistry and structural molecular biology, such as protein folding (AlphaFold [111]) and molecular docking (TankBind [112]) have benefited from such novel architectures.</p>
<ol>
<li>Development of AI4S Benchmarks.Finally, continuing to develop higher-level workflows which make training and deploying such systems will be important, as well as benchmarks that are able to assess AI4S workflow performance, as opposed to simple throughput of training or inference performance [113].</li>
</ol>
<p>As we look to the future in a post-ChatGPT world, where AI's outperform humans in many tasks, it is clear that using such techniques will be critical and essential to continue to push the boundaries of science forward.</p>
<p>Figure 1 :
1
Figure 1: PINN architecture from Chen et al.[22]</p>
<p>Figure 2 :
2
Figure 2: DeepONet architecture [32].</p>
<p>[66]ied such techniques for thermodynamics surrogate models, and studied different types of couplingfrom strong to weak -and demonstrate scaling up to 1000 GPUs.LLM on the other hand must be deployed with synchronous inference, where the model must be distributed across multiple accelerators.Pope et al.[65]studied inference performance of Transformer models with 500B+ parameters distributed across 64 TPU v4 chips.Example: Medical Document Analysis Analysis of pathology reports provides critical insight into cancer diagnosis and management.A number of cancer characteristics are coded manually from the cancer pathology report as part of Surveillance, Epidemiology, and End Results (SEER) database.A recent collaborative effort between DOE and NCI has tried to build AI-based models for pathology information extraction tasks namely: site, subsite, laterality, histology, and behavior.In that work, we build a transformer model that can effectively accommodate the length of typical cancer pathology reports.We use 2.7 million pathology reports from six SEER cancer registries to train purpose-built sparse transformer models such as pathology BigBird model[66].BigBird model is a sparse attention-based transformer model built for long documents compared to popular dense attention-based models such as BERT.</p>
<p>AcknowledgementsThis research was sponsored by and used resources of the Oak Ridge Leadership Computing Facility (OLCF), which is a DOE Office of Science User Facility at the Oak Ridge National Laboratory supported by the U.S. Department of Energy under Contract No. DE-AC05-00OR22725.Declaration of generative AI and AI-assisted technologies in the writing processDuring the preparation of this work the authors used GPT-4 in order to improve grammar structures in certain places.After using this tool/service, the authors reviewed and edited the content as needed and take full responsibility for the content of the publication.
This is how AI will transform the way science gets done. Eric Schmidt, MIT Technology Review. 2023</p>
<p>Could AI transform science itself?. Anonymous, 2023</p>
<p>Scientific discovery in the age of artificial intelligence. Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Anima Anandkumar, Karianne Bergen, Carla P Gomes, Shirley Ho, Pushmeet Kohli, Joan Lasenby, Jure Leskovec, Tie-Yan Liu, Arjun Manrai, Debora Marks, Bharath Ramsundar, Le Song, Jimeng Sun, Jian Tang, Petar Veličković, Max Welling, Linfeng Zhang, Connor W Coley, Yoshua Bengio, Marinka Zitnik, Nature. 62079722023</p>
<p>Ai and extreme scale computing to learn and infer the physics of higher order gravitational wave modes of quasi-circular, spinning, non-precessing black hole mergers. Asad Khan, Prayush Huerta, Kumar, Physics Letters B. 8351375052022</p>
<p>The effect of prandtl number on decaying stratified turbulence. Miles James J Riley, Stephen M Mp Couchman, De Bruyn Kops, Journal of Turbulence. 2023</p>
<p>Mixing across stable density interfaces in forced stratified turbulence. Stephen M Miles Mp Couchman, De Bruyn Kops, Caulfield Colm-Cille, Journal of Fluid Mechanics. 961A202023</p>
<p>Unsupervised machine learning to teach fluid dynamicists to think in 15 dimensions. Sm De Bruyn Kops, Saunders, Ea Rietman, Portwood, arXiv-19072019arXiv e-prints</p>
<p>The impact of AlphaFold2 one year on. D T Jones, J M Thornton, Nat Methods. 192022</p>
<p>Highly accurate protein structure prediction with alphafold. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, Nature. 59678732021</p>
<p>Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics. Alexander Max T Zvyagin, Kyle Brace, Yuntian Hippe, Bin Deng, Cindy Orozco Zhang, Austin Bohorquez, Bharat Clyde, Danilo Kale, Heng Perez-Rivera, Ma, bioRxiv. 2022</p>
<p>Pushing the limit of molecular dynamics with ab initio accuracy to 100 million atoms with machine learning. Weile Jia, Han Wang, Mohan Chen, Denghui Lu, Lin Lin, Roberto Car, Linfeng Weinan, Zhang, SC20: International conference for high performance computing, networking, storage and analysis. IEEE2020</p>
<p>Ai-driven multiscale simulations illuminate mechanisms of sars-cov-2 spike dynamics. Lorenzo Casalino, Abigail C Dommer, Zied Gaieb, Emilia P Barros, Terra Sztain, Anda Surl-Hee Ahn, Alexander Trifan, Anthony T Brace, Austin Bogetti, Clyde, The International Journal of High Performance Computing Applications. 3552021</p>
<p>Exascale deep learning for climate analytics. Thorsten Kurth, Sean Treichler, Joshua Romero, Mayur Mudigonda, Nathan Luehr, Everett Phillips, Ankur Mahesh, Michael Matheson, Jack Deslippe, Massimiliano Fatica, SC18: International conference for high performance computing, networking, storage and analysis. IEEE2018</p>
<p>Openai's gpt-3 language model: A technical overview. Chuan Li, june 2020</p>
<p>Efficient large-scale language model training on GPU clusters using Megatron-LM. Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick Legresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. the International Conference for High Performance Computing, Networking, Storage and Analysis2021</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Parallel I/O evaluation techniques and emerging HPC workloads: A perspective. Sarah Neuwirth, Arnab K Paul, 2021 IEEE International Conference on Cluster Computing (CLUSTER). IEEE2021</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730</p>
<p>Language models for the prediction of SARS-CoV-2 inhibitors. Andrew E Blanchard, John Gounley, Debsindhu Bhowmik, Chandra Mayanka, Isaac Shekar, Shang Lyngaas, Junqi Gao, Aristeidis Yin, Feiyi Tsaris, Jens Wang, Glaser, The International Journal of High Performance Computing Applications. 365-62022</p>
<p>Deployment of hybrid HPC, quantum, and AI infrastructure in Riken R-CCS. Satoshi Matsuoka, 20th Smoky Mountain Computational Sciences and Engineering Conference (SMC2023). 2023. August</p>
<p>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. M Raissi, P Perdikaris, G E Karniadakis, Journal of Computational Physics. 378February 2019</p>
<p>Physics-informed neural networks for inverse problems in nano-optics and metamaterials. Yuyao Chen, Lu Lu, George Em Karniadakis, Luca Dal, Negro , Optics Express. 288April 2020Optica Publishing Group</p>
<p>Neural ordinary differential equations. T Q Ricky, Yulia Chen, Jesse Rubanova, David Bettencourt, Duvenaud, arXiv:1806.0736632nd Conference on Neural Information Processing Systems. Montréal, CanadaNeurIPS 2018. 2018Preprint</p>
<p>Augmented neural odes. Emilien Dupont, Arnaud Doucet, Yee Whye Teh, Advances in Neural Information Processing Systems. 201932</p>
<p>Neural stochastic differential equations: Deep latent gaussian models in the diffusion limit. Belinda Tzen, Maxim Raginsky, Advances in Neural Information Processing Systems. 201932</p>
<p>Ziyang Li, Tzu-Mao Chen, Jun-Yan Zhu, arXiv:2102.06465Discretely updated neural odes: Lambdas all the way down. 2021arXiv preprint</p>
<p>Stefano Massaroli, Michael Poli, Mattia Atzori, Jinkyoo Kim, Attilio Park, Atsushi Yamashita, Hajime Asama, arXiv:2002.08071Dissecting neural odes. 2020arXiv preprint</p>
<p>Michael Poli, Stefano Massaroli, Atsushi Yamashita, Hajime Asama, arXiv:2005.08926Controlled neural odes. 2020arXiv preprint</p>
<p>T Q Ricky, Yulia Chen, Jesse Rubanova, David K Bettencourt, Duvenaud, arXiv:2103.10950Piecewise neural ODEs. 2021arXiv preprint</p>
<p>Universal differential equations for scientific machine learning. Christopher Rackauckas, Yingbo Ma, Julius Martensen, Collin Warner, Kirill Zubov, Rohit Supekar, Dominic Skinner, Ali Ramadhan, Adam Edelman, arXiv:2001.04385November 2021</p>
<p>Discovering governing equations from data by sparse identification of nonlinear dynamical systems. L Steven, Joshua L Brunton, J Nathan Proctor, Kutz, PNAS. 11315</p>
<p>Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators. Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, George Em Karniadakis, Nature Machine Intelligence. 33mar 2021</p>
<p>Neural operator: Graph kernel network for partial differential equations. Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, Anima Anandkumar, arXiv:2003.03485March 2020cs, math, stat</p>
<p>Fourier Neural Operator for Parametric Partial Differential Equations. Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, Anima Anandkumar, arXiv:2010.08895May 2021cs, math</p>
<p>Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay, Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, arXiv:2202.11214A global datadriven high-resolution weather model using adaptive fourier neural operators. 2022arXiv preprint</p>
<p>. Somdatta Goswami, Aniruddha Bora, Yue Yu, George Em Karniadakis, arXiv:2207.05748Physics-Informed Deep Neural Operator Networks. July 2022cs, math</p>
<p>Zongyi Li, Hongkai Zheng, Nikola Kovachki, David Jin, Haoxuan Chen, Burigede Liu, Kamyar Azizzadenesheli, Anima Anandkumar, arXiv:2111.03794Physics-Informed Neural Operator for Learning Partial Differential Equations. November 2022cs, math</p>
<p>Learning data-driven discretizations for partial differential equations. Yohai Bar-Sinai, Stephan Hoyer, Jason Hickey, Michael P Brenner, 2019Proceedings of the National Academy of Sciences116</p>
<p>Photo-realistic single image super-resolution using a generative adversarial network. Christian Ledig, Lucas Theis, Ferenc Huszár, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Time-series forecasting with deep learning: a survey. Bryan Lim, Stefan Zohren, Philosophical Transactions of the Royal Society A. 379202002092194. 2021</p>
<p>Gavin D Portwood, Mateus Peetak P Mitra, Tan Dias Ribeiro, Minh Nguyen, T Balasubramanya, Juan A Nadiga, Michael Saenz, Animesh Chertkov, Garg, arXiv:1911.05180Anima Anandkumar, Andreas Dengel, et al. Turbulence forecasting via neural ode. 2019arXiv preprint</p>
<p>Pnode: A memory-efficient neural ode framework based on high-level adjoint differentiation. Hong Zhang, Wenjun Zhao, arXiv:2206.012982022arXiv preprint</p>
<p>Genetic programming: On the programming of computers by means of natural selection. John R Koza, 1992MIT press</p>
<p>Discovering symbolic models from deep learning with inductive biases. Miles Cranmer, Alvaro Sanchez Gonzalez, Peter Battaglia, Rui Xu, Kyle Cranmer, David Spergel, Shirley Ho, Advances in Neural Information Processing Systems. 202033</p>
<p>Neural symbolic regression that scales. Luca Biggio, Tommaso Bendinelli, Alexander Neitz, Aurelien Lucchi, Giambattista Parascandolo, International Conference on Machine Learning. PMLR2021</p>
<p>A unified framework for deep symbolic regression. Mikel Landajuela, Chak Shing Lee, Jiachen Yang, Ruben Glatt, Claudio P Santiago, Ignacio Aravena, Terrell Mundhenk, Garrett Mulcahy, Brenden K Petersen, Advances in Neural Information Processing Systems. 202235</p>
<p>Uncertainty quantification in scientific machine learning: Methods, metrics, and comparisons. Apostolos F Psaros, Xuhui Meng, Zongren Zou, Ling Guo, George Em Karniadakis, Journal of Computational Physics. 1119022023</p>
<p>Scalable Gaussian process inference using variational methods. Alexander Graeme De Garis Matthews, 2017University of CambridgePhD thesis</p>
<p>Adversarial uncertainty quantification in physics-informed neural networks. Yibo Yang, Paris Perdikaris, Journal of Computational Physics. 3942019</p>
<p>Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. Yarin Gal, Zoubin Ghahramani, international conference on machine learning. PMLR2016</p>
<p>Dropout: a simple way to prevent neural networks from overfitting. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, The journal of machine learning research. 1512014</p>
<p>Pi3nn: Out-of-distribution-aware prediction intervals from three neural networks. Siyan Liu, Pei Zhang, Dan Lu, Guannan Zhang, arXiv:2108.023272021arXiv preprint</p>
<p>Shanti Bhushan, Greg W Burgreen, Wesley Brewer, Ian D Dettwiller, arXiv:2303.11447Assessment of neural network augmented reynolds averaged navier stokes turbulence model in extrapolation modes. 2023arXiv preprint</p>
<p>Validation and parameterization of a novel physics-constrained neural dynamics model applied to turbulent fluid flow. Varun Shankar, Gavin D Portwood, Arvind T Mohan, Peetak P Mitra, Dilip Krishnamurthy, Christopher Rackauckas, Lucas A Wilson, David P Schmidt, Venkatasubramanian Viswanathan, Physics of Fluids. 34111151102022</p>
<p>Routes to stratified turbulence revealed by unsupervised classification of experimental data. Adrien Lefauve, M P Miles, Couchman, arXiv:2305.040512023arXiv preprint</p>
<p>NVIDIA SimNet™: An AI-accelerated multi-physics simulation framework. Oliver Hennigh, Susheela Narasimhan, Mohammad Amin Nabian, Akshay Subramaniam, Kaustubh Tangsali, Zhiwei Fang, Max Rietmann, Wonmin Byeon, Sanjay Choudhry, Computational Science-ICCS 2021: 21st International Conference. Krakow, PolandSpringerJune 16-18, 2021. 2021Proceedings, Part V</p>
<p>A predictor-corrector deep learning algorithm for high dimensional stochastic partial differential equations. He Zhang, Ran Zhang, Tao Zhou, arXiv:2208.098832022arXiv preprint</p>
<p>Using machine learning at scale in numerical simulations with smartsim: An application to ocean climate modeling. Sam Partee, Matthew Ellis, Alessandro Rigazzi, Andrew E Shao, Scott Bachman, Gustavo Marques, Benjamin Robbins, Journal of Computational Science. 1017072022</p>
<p>Production deployment of machine-learned rotorcraft surrogate models on HPC. Wesley Brewer, Daniel Martinez, Mathew Boyer, Dylan Jude, Andy Wissink, Ben Parsons, Junqi Yin, Valentine Anantharaj, 2021 IEEE/ACM Workshop on Machine Learning in High Performance Computing Environments (MLHPC). IEEE2021</p>
<p>Christopher Olston, Noah Fiedel, Kiril Gorovoy, Jeremiah Harmsen, Li Lao, Fangwei Li, Vinu Rajashekhar, Ramesh Sukriti, Jordan Soyke, arXiv:1712.06139Tensorflow-Serving: Flexible, high-performance ML serving. 2017arXiv preprint</p>
<p>Scalable integration of computational physics simulations with machine learning. Mathew Boyer, Wesley Brewer, Dylan Jude, Ian Dettwiller, IEEE/ACM Workshop on Artificial Intelligence and Machine Learning for Scientific Applications. AI4S2022. 2022IEEE</p>
<p>Multiscale simulations of complex systems by learning their effective dynamics. Georgios Pantelis R Vlachas, Caroline Arampatzis, Petros Uhler, Koumoutsakos, Nature Machine Intelligence. 442022</p>
<p>Mathematical and Computational Foundations to Enable Predictive Digital Twins at Scale. Michael G Kapteyn, 2021Massachusetts Institute of TechnologyPhD thesis</p>
<p>Strategies for integrating deep learning surrogate models with HPC simulation applications. Junqi Yin, Feiyi Wang, Mallikarjun Shankar, 2022 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW). IEEE2022</p>
<p>Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, Jeff Dean, arXiv:2211.05102Efficiently scaling transformer inference. 2022arXiv preprint</p>
<p>Big bird: Transformers for longer sequences. Manzil Zaheer, Guru Guruganesh, Avinava Kumar, Joshua Dubey, Chris Ainslie, Santiago Alberti, Philip Ontanon, Anirudh Pham, Qifan Ravula, Li Wang, Yang, Advances in neural information processing systems. 202033</p>
<p>Horovod: fast and easy distributed deep learning in tensorflow. Alexander Sergeev, Mike Del Balso, arXiv:1802.057992018arXiv preprint</p>
<p>Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, Yuxiong He, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2020</p>
<p>Pytorch fsdp: experiences on scaling fully sharded data parallel. Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, arXiv:2304.112772023arXiv preprint</p>
<p>Evaluation of pre-training large language models on leadership-class supercomputers. Junqi Yin, Sajal Dash, John Gounley, Feiyi Wang, Georgia Tourassi, The Journal of Supercomputing. 79182023</p>
<p>Optimizing distributed training on frontier for large language models. Sajal Dash, Isaac R Lyngaas, Junqi Yin, Xiao Wang, Romain Egele, J Austin Ellis, Matthias Maiterth, Guojing Cong, Feiyi Wang, Prasanna Balaprakash, Proceedings of the International Supercomputing Conference (ISC). the International Supercomputing Conference (ISC)Hamburg, GermanyMay 12-16 2024Oak Rid National Laboratory</p>
<p>Bringing HPC techniques to deep learning. Andrew Gibiansky, 2017</p>
<p>Peter Jin, Boris Ginsburg, Kurt Keutzer, Spatially parallel convolutions. 2018</p>
<p>High resolution medical image analysis with spatial partitioning. Le Hou, Youlong Cheng, Noam Shazeer, Niki Parmar, Yeqing Li, Panagiotis Korfiatis, Travis M Drucker, Daniel J Blezek, Xiaodan Song, 2019</p>
<p>Improving strong-scaling of cnn training by exploiting finer-grained parallelism. Naoya Nikoli Dryden, Tom Maruyama, Tim Benson, Marc Moon, Brian Snir, Van Essen, 2019</p>
<p>The case for strong scaling in deep learning: Training large 3d cnns with hybrid parallelism. Yosuke Oyama, Naoya Maruyama, Nikoli Dryden, Erin Mccarthy, Peter Harrington, Jan Balewski, Satoshi Matsuoka, Peter Nugent, Brian Van Essen, CoRR, abs/2007.128562020</p>
<p>Mesh-TensorFlow: Deep learning for supercomputers. Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, Hyoukjoong Lee, Mingsheng Hong, Cliff Young, Advances in neural information processing systems. 201831</p>
<p>Gpipe: Efficient training of giant neural networks using pipeline parallelism. Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, Hyoukjoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, Zhifeng Chen, 2019</p>
<p>. Anonymous, 2019PyTorch distributed RPC framework</p>
<p>Demystifying parallel and distributed deep learning: An in-depth concurrency analysis. Tal Ben-Nun, Torsten Hoefler, 2018</p>
<p>Integrated model, batch and domain parallelism in training neural networks. Amir Gholami, Ariful Azad, Peter Jin, Kurt Keutzer, Aydin Buluc, 2017</p>
<p>TEAM HMS-MGH-CCDS METHOD1. Aoxiao Zhong, Quanzheng Li, 2018</p>
<p>A framework for multiple-instance learning. Oded Maron, Tomás Lozano-Pérez, Advances in neural information processing systems. 101997</p>
<p>Data-efficient and weakly supervised computational pathology on whole-slide images. Ming Y Lu, Tiffany Y Drew Fk Williamson, Richard J Chen, Matteo Chen, Faisal Barbieri, Mahmood, Nature Biomedical Engineering. 562021</p>
<p>Multiple instance learning of deep convolutional neural networks for breast histopathology whole slide classification. Kausik Das, Sailesh Conjeti, Guha Abhijit, Jyotirmoy Roy, Debdoot Chatterjee, Sheet, 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018). IEEE2018</p>
<p>Scaling vision transformers to gigapixel images via hierarchical self-supervised learning. Chengkuan Richard J Chen, Yicong Chen, Tiffany Y Li, Andrew D Chen, Rahul G Trister, Faisal Krishnan, Mahmood, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Scaling resolution of gigapixel whole slide images using spatial decomposition on convolutional neural networks. Aristeidis Tsaris, Josh Romero, Thorsten Kurth, Jacob Hinkle, Hong-Jun Yoon, Feiyi Wang, Sajal Dash, Georgia Tourassi, Proceedings of the Platform for Advanced Scientific Computing Conference. the Platform for Advanced Scientific Computing Conference2023</p>
<p>Hyperband: A novel bandit-based approach to hyperparameter optimization. Lisha Li, Kevin Jamieson, Giulia Desalvo, Afshin Rostamizadeh, Ameet Talwalkar, The Journal of Machine Learning Research. 1812017</p>
<p>Tune: A research platform for distributed model selection and training. Richard Liaw, Eric Liang, Robert Nishihara, Philipp Moritz, Joseph E Gonzalez, Ion Stoica, arXiv:1807.051182018arXiv preprint</p>
<p>Optimizing deep learning hyper-parameters through an evolutionary algorithm. Derek C Steven R Young, Thomas P Rose, Seung-Hwan Karnowski, Robert M Lim, Patton, Proceedings of the workshop on machine learning in high-performance computing environments. the workshop on machine learning in high-performance computing environments2015</p>
<p>Deephyper: Asynchronous hyperparameter search for deep neural networks. Prasanna Balaprakash, Michael Salim, Thomas D Uram, Venkat Vishwanath, Stefan M Wild, IEEE 25th international conference on high performance computing (HiPC). IEEE2018. 2018</p>
<p>Balsam: A distributed task execution system for scientific workflows. Babak Behzad, Taylor J Childers, Nathan A Lemons, David E Bernholdt, Tom Papatheodore, John E Pickett, R Wael, Nirmal K Elwasif, Gupta, Proceedings of the 2020 International Conference for High Performance Computing, Networking, Storage and Analysis. the 2020 International Conference for High Performance Computing, Networking, Storage and AnalysisIEEE Press2020</p>
<p>AI-coupled HPC workflow applications, middleware and performance. Wesley Brewer, Ana Gainaru, Frédéric Suter, Feiyi Wang, Murali Emani, Shantenu Jha, 2024ACM Computing Surveyssubmitted</p>
<p>Colmena: Scalable machine-learning-based steering of ensemble simulations for high performance computing. Logan Ward, Ganesh Sivaraman, Gregory Pauloski, Yadu Babuji, Ryan Chard, Naveen Dandu, Rajeev S Paul C Redfern, Kyle Assary, Larry A Chard, Curtiss, 2021 IEEE/ACM Workshop on Machine Learning in High Performance Computing Environments (MLHPC). IEEE2021</p>
<p>Optimal decision making in high-throughput virtual screening pipelines. Hyun-Myung Woo, Xiaoning Qian, Li Tan, Shantenu Jha, Francis J Alexander, Edward R Dougherty, Byung-Jun Yoon, arXiv:2109.116832021arXiv preprint</p>
<p>Inverse design of materials by machine learning. Jia Wang, Yingxue Wang, Yanan Chen, Materials. 15518112022</p>
<p>Is disaggregation possible for hpc cognitive simulation. Valen Michael R Wyatt, Zoë Yamamoto, Ian Tosi, Brian Karlin, Van Essen, 2021 IEEE/ACM Workshop on Machine Learning in High Performance Computing Environments (MLHPC). IEEE2021</p>
<p>Distributed intelligence on the edge-to-cloud continuum: A systematic literature review. Daniel Rosendo, Alexandru Costan, Patrick Valduriez, Gabriel Antoniu, Journal of Parallel and Distributed Computing. 2022</p>
<p>Supporting efficient workflow deployment of federated learning systems across the computing continuum. Cédric Prigent, Gabriel Antoniu, Alexandru Costan, Loïc Cudennec, SC 2022-SuperComputing. 2022</p>
<p>Toward an autonomous workflow for single crystal neutron diffraction. Junqi Yin, Guannan Zhang, Huibo Cao, Sajal Dash, Bryan C Chakoumakos, Feiyi Wang, Accelerating Science and Engineering Discoveries Through Integrated Research Infrastructure for Experiment, Big Data, Modeling and Simulation. Kothe Doug, Geist Al, Swaroop Pophale, Hong Liu, Suzanne Parete-Koon, ChamSpringer Nature Switzerland2022</p>
<p>Do we need attention? Keynote talk at the sixth conference on Machine Learning and Systems (MLSys23). Sasha Rush, May 2023</p>
<p>Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran, G V , arXiv:2305.13048Reinventing rnns for the transformer era. 2023arXiv preprint</p>
<p>Retentive network: A successor to transformer for large language models. Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, Furu Wei, arXiv:2307.086212023arXiv preprint</p>
<p>Mamba: Linear-time sequence modeling with selective state spaces. Albert Gu, Tri Dao, arXiv:2312.007522023arXiv preprint</p>
<p>Xi Chen, Xiao Wang, Soravit Changpinyo, Piotr Piergiovanni, Daniel Padlewski, Sebastian Salz, Adam Goodman, Basil Grycner, Lucas Mustafa, Alexander Beyer, Joan Kolesnikov, Nan Puigcerver, Keran Ding, Hassan Rong, Gaurav Akbari, Linting Mishra, Ashish Xue, James Thapliyal, Weicheng Bradbury, Mojtaba Kuo, Chao Seyedhosseini, Jia, A jointly-scaled multilingual language-image model. Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut, Pali2023</p>
<p>Flamingo: a visual language model for few-shot learning. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, Karen Simonyan, 2022</p>
<p>Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent. Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gómez Colmenarejo, Alexander Novikov, Gabriel Barthmaron, Mai Giménez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Transactions on Machine Learning Research. 2022Featured Certification, Outstanding Certification</p>
<p>Meta-transformer: A unified framework for multimodal learning. Yiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hongsheng Li, Yu Qiao, Wanli Ouyang, Xiangyu Yue, 2023</p>
<p>Is object localization for free?-weakly-supervised learning with convolutional neural networks. Maxime Oquab, Léon Bottou, Ivan Laptev, Josef Sivic, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2015</p>
<p>Grad-cam: Visual explanations from deep networks via gradient-based localization. Michael Ramprasaath R Selvaraju, Abhishek Cogswell, Ramakrishna Das, Devi Vedantam, Dhruv Parikh, Batra, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2017</p>
<p>Alphafold at casp13. Mohammed Alquraishi, Bioinformatics. 35222019</p>
<p>TankBind: Trigonometryaware neural networks for drug-protein binding structure prediction. Wei Lu, Qifeng Wu, Jixian Zhang, Jiahua Rao, Chengtao Li, Shuangjia Zheng, Advances in neural information processing systems. 202235</p>
<p>Scientific machine learning benchmarks. Jeyan Thiyagalingam, Mallikarjun Shankar, Geoffrey Fox, Tony Hey, Nature Reviews Physics. 462022</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2925 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2925</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2925</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-72.html">extraction-schema-72</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-272828154</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.14908v2.pdf" target="_blank">KARMA: Augmenting Embodied AI Agents with Long-and-Short Term Memory Systems</a></p>
                <p><strong>Paper Abstract:</strong> Embodied AI agents responsible for executing interconnected, long-sequence household tasks often face difficulties with in-context memory, leading to inefficiencies and errors in task execution. To address this issue, we introduce KARMA, an innovative memory system that integrates longterm and short-term memory modules, enhancing large language models (LLMs) for planning in embodied agents through memory-augmented prompting. Karma distinguishes between long-term and short-term memory, with long-term memory capturing comprehensive 3D scene graphs as representations of the environment, while short-term memory dynamically records changes in objects' positions and states. This dualmemory structure allows agents to retrieve relevant past scene experiences, thereby improving the accuracy and efficiency of task planning. Short-term memory employs strategies for effective and adaptive memory replacement, ensuring the retention of critical information while discarding less pertinent data. Compared to state-of-the-art embodied agents enhanced with memory, our memory-augmented embodied AI agent improves success rates by $1.3 \times$ and $2.3 \times$ in Composite Tasks and Complex Tasks within the AI2-THOR simulator, respectively, and enhances task execution efficiency by $3.4 \times$ and $62.7 \times$. Furthermore, we demonstrate that KARMA's plug-and-play capability allows for seamless deployment on real-world robotic systems, such as mobile manipulation platforms. Through this plug-and-play memory system, KARMA significantly enhances the ability of embodied agents to generate coherent and contextually appropriate plans, making the execution of complex household tasks more efficient. Our code is available at https://github.com/WZX0Swarm0Robotics/KARMA/tree/master.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2925.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2925.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KARMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KARMA: Augmenting Embodied AI Agents with Long-and-short Term Memory Systems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A plug-and-play memory system that augments an LLM planner for embodied agents with a dual memory: a persistent long-term 3D scene-graph and a volatile short-term object cache (multimodal). Uses embedding-based recall and advanced cache replacement (W-TinyLFU) to improve planning accuracy and efficiency in long-horizon household tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KARMA-augmented LLM planner</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-based task planner (GPT-4o in experiments) that is memory-augmented via two modules: (1) long-term memory implemented as a non-volatile hierarchical 3D scene graph (3DSG) serialized into text and appended to prompts; (2) short-term memory containing multimodal per-object memory units (image, VLM-derived state, world coordinates, objectId) embedded into vectors for retrieval. Memory replacement for short-term memory uses configurable policies (FIFO with merging, W-TinyLFU) and retrieval uses vector-similarity top-K.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>ALFRED-L (derived from ALFRED) evaluated in AI2-THOR; additional experiments on ALFWorld-R-style long-sequence tasks</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>ALFRED-L: long-sequence indoor household instruction-following tasks (simple, composite, complex) adapted from the ALFRED benchmark and executed in the AI2-THOR simulator; tasks require navigation and manipulation, multi-step dependency, and object-state goals (e.g., wash/cut/place). ALFWorld-R/ALFRED-R used to stress replacement policies with long sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>dual: long-term semantic 3D scene-graph (persistent) + short-term volatile object cache (episodic/working memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Long-term memory: hierarchical 3D scene graph (3DSG) with area nodes, object nodes, topological edges, object attributes (3D position, volume, type); stored non-volatile and serialized into text prompts for the LLM. Short-term memory: fixed-capacity list of memory units; each unit stores objectType, objectId, world coordinates, raw imagePath, and VLM-extracted object state; units are embedded by a multimodal embedding model (OpenAI text-embedding-3-large used for experiments) into vectors and indexed for similarity retrieval. Replacement policies implemented: FIFO with merging, W-TinyLFU (two-segment LRU-style main/protection+window, uses counting Bloom filters for frequency stats and periodic halving).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Short-term retrieval: embedding-based cosine similarity (OpenAI text-embedding-3-large) between instruction embedding and memory-unit embeddings; top-K most similar units are returned and their text content appended to LLM prompt. Long-term retrieval: entire serialized 3DSG is appended to the prompt (no embedding-based selection).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>Configurable fixed-size short-term cache; experiments used sizes such as 10 memory units (detailed W-TinyLFU [9,1] etc.) and larger sizes (up to 25 units in analyses); long-term memory treated as non-volatile (no strict capacity limit described in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Long-term: area nodes, adjacency/topology, immovable object nodes with attributes (3D positions, types, volumes). Short-term: per-object records containing objectType, objectId, world coordinates, image path / raw image, VLM-derived object state (e.g., cleaned, sliced), plus embedded vector representation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>On ALFRED-L in AI2-THOR: reported relative improvements over best baselines — composite tasks: 1.3x success rate improvement (reported as +43 percentage points absolute SR improvement in paper's summary) and 3.4x task execution efficiency improvement (68.7% reduction in time); complex tasks: 2.3x success rate improvement (reported as +21 percentage points absolute SR improvement) and 62.7x task execution efficiency improvement (69% reduction in time reported as absolute reduction). Exact SR numbers per category appear in Table 1 of the paper (KARMA composite SR ~0.43, complex SR improvements reported in text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Ablation: removing short-term memory caused large drops in success rate (SR dropped by factors of ~1.9x for complex tasks and ~4.2x for composite tasks). Removing long-term memory had a larger impact on execution efficiency (reported RT decrease by factor ~2.7) and only modest impact on SR (~1.2x drop). Baseline (LoTa-Bench modified / other baselines) performance is reported in Table 1 (KARMA outperforms LoTa-Bench / HELPER / CAPEAM).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td>Relative improvements: +1.3x SR (composite) and +2.3x SR (complex) vs best baselines; absolute reported gains cited in paper: +43 percentage points SR for composite tasks and +21 percentage points SR for complex tasks; efficiency improvements: 3.4x faster (composite) and 62.7x faster (complex) in task execution vs best baselines. Ablation: removal of short-term memory reduces SR by ~1.9x–4.2x depending on task class.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Short-term (object-centric) memory is critical for increasing task success rates on related multi-step tasks (composite/complex); long-term 3DSG memory primarily improves planning efficiency by reducing repeated exploration and action code generated by the LLM; higher short-term memory hit rates (measured by embedding similarity recall) linearly correlate with reduced exploration and faster task execution; W-TinyLFU replacement outperforms FIFO variants in hit rate and downstream efficiency when properly sized (large window favored).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Evaluated in ideal simulation (no adversarial/environmental disturbances) so scalability to many real-world objects untested; semantic matching/VLM limitations cause degraded Memory Retrieval Accuracy for ambiguous instructions; fixed-size short-term cache can suffer from capacity constraints—requires careful replacement policy; current design is open-loop (no feedback-driven memory correction) and lacks biologically grounded theory.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Paper compares short-term replacement strategies (FIFO with merging vs W-TinyLFU variants) and reports W-TinyLFU (large window) achieves highest memory hit rates and better reuse; ablation compares presence/absence of short-term vs long-term memory showing complementary roles (short-term -> SR, long-term -> efficiency). No direct comparison to other high-level memory paradigms (e.g., rehearsal, reflections) beyond cache policies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KARMA: Augmenting Embodied AI Agents with Long-and-Short Term Memory Systems', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2925.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2925.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HELPER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open-ended instructable embodied agents with memory-augmented large language models (HELPER)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior memory-augmented LLM-based embodied agent referenced as a state-of-the-art baseline; designed to store and recall past experiences to support long-horizon instruction following in embodied environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Open-ended instructable embodied agents with memory-augmented large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>HELPER (as cited baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Cited as a memory-augmented LLM agent that stores past experiences to inform planning for embodied tasks; used in the paper as a strong baseline for complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>ALFRED-L / ALFRED tasks in AI2-THOR (used as baseline comparisons in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Same family of long-horizon indoor household tasks (multi-step instruction following) executed in AI2-THOR; used to evaluate long-term planning with memory augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>memory-augmented (experience/past-experiences / episodic-like memory) as described in the original HELPER work; paper refers to it as a memory-augmented baseline</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Not specified in detail in this paper; referenced as an approach that stores past experiences and uses them to augment LLM planning. (Original paper should be consulted for architecture specifics.)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Described generally as storing past experiences; specifics not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Used as best-performing baseline for complex tasks; KARMA reported a relative improvement over HELPER of 2.3x in success rate for complex tasks and a 62.7x improvement in task execution efficiency (paper reports KARMA vs HELPER numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Referenced as an example of a memory-augmented embodied agent; used to show KARMA's improvements in complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Not discussed in detail in this paper (referenced baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Not compared in detail here; used as a baseline for KARMA's improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KARMA: Augmenting Embodied AI Agents with Long-and-Short Term Memory Systems', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2925.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2925.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CAPEAM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CAPEAM (Context-aware planning and environment-aware memory for instruction following embodied agents)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based embodied agent (Kim et al., 2023) that uses short-term memory to track object positions via semantic labels; used as a baseline in KARMA experiments and cited for its short-term memory approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Context-aware planning and environment-aware memory for instruction following embodied agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CAPEAM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-planner augmented with environment-aware memory; specifically referenced for using short-term memory to continuously track object positions with semantic labels to assist planning in embodied instruction following.</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>ALFRED-L / ALFRED tasks in AI2-THOR (used as baseline comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Long-horizon indoor manipulation and navigation tasks with subtask dependencies (ALFRED-derived).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>short-term/working memory for tracking object positions (semantic-label based), per the citation referenced in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Not fully detailed in this paper; the referenced research (Kim et al., 2023) is noted for maintaining short-term memory that continuously tracks object positions via semantic labels.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Not specified in this paper (refer to original CAPEAM paper).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td>Reportedly object positions and semantic labels for recently observed objects (short-term tracking).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Used as a baseline; KARMA reports a relative improvement over CAPEAM of 1.3x in success rate for composite tasks and a 3.4x improvement in task execution efficiency (paper reports KARMA vs CAPEAM numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Cited as prior work demonstrating benefit of short-term object-tracking memory; used to contextualize KARMA's design and improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Not discussed in detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Not directly compared here beyond KARMA's reported comparisons to CAPEAM as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KARMA: Augmenting Embodied AI Agents with Long-and-Short Term Memory Systems', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2925.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2925.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents that use memory mechanisms to solve text games, including details about the memory architecture, the text games being solved, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LoTa-Bench (Modified)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LoTa-Bench: Benchmarking language-oriented task planners for embodied agents (modified baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-based baseline for LLM task planning used as a primary baseline in experiments; provides prefix + in-context examples and selects skills from a skill set by scoring via the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Lota-bench: Benchmarking language-oriented task planners for embodied agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LoTa-Bench (modified prompt-based planner)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A language-oriented task planner baseline that constructs prompts with a prefix and in-context examples; the LLM computes probabilities over executable skills and selects next skill to execute. The paper uses a modified LoTa-Bench as a baseline (non-memory or limited memory baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>base_llm_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_game_name</strong></td>
                            <td>ALFRED-L / ALFRED tasks in AI2-THOR (used as the baseline benchmark environment)</td>
                        </tr>
                        <tr>
                            <td><strong>text_game_description</strong></td>
                            <td>Long-horizon indoor instruction-following tasks (ALFRED-derived) executed in AI2-THOR.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>what_is_stored_in_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Used as a non-memory (or minimally in-context-only) baseline; Table 1 reports its success rates and efficiency on ALFRED-L (modified LoTa-Bench results are presented but specific numbers are in Table 1 of the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_improvement_magnitude</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_about_memory</strong></td>
                            <td>Serves as a reference baseline demonstrating that simple in-context prompting without explicit external memory is less effective and less efficient than memory-augmented approaches like KARMA.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>As an in-context-only baseline, susceptible to context-window and in-context forgetting for long-horizon tasks (discussed as motivation in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>Not applicable (baseline without explicit external memory).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KARMA: Augmenting Embodied AI Agents with Long-and-Short Term Memory Systems', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Open-ended instructable embodied agents with memory-augmented large language models <em>(Rating: 2)</em></li>
                <li>Context-aware planning and environment-aware memory for instruction following embodied agents <em>(Rating: 2)</em></li>
                <li>Lota-bench: Benchmarking language-oriented task planners for embodied agents <em>(Rating: 2)</em></li>
                <li>Rap: Retrieval-augmented planning with contextual memory for multimodal LLM agents <em>(Rating: 2)</em></li>
                <li>ALFWorld: Aligning Text and Embodied Environments for Interactive Learning <em>(Rating: 2)</em></li>
                <li>Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2925",
    "paper_id": "paper-272828154",
    "extraction_schema_id": "extraction-schema-72",
    "extracted_data": [
        {
            "name_short": "KARMA",
            "name_full": "KARMA: Augmenting Embodied AI Agents with Long-and-short Term Memory Systems",
            "brief_description": "A plug-and-play memory system that augments an LLM planner for embodied agents with a dual memory: a persistent long-term 3D scene-graph and a volatile short-term object cache (multimodal). Uses embedding-based recall and advanced cache replacement (W-TinyLFU) to improve planning accuracy and efficiency in long-horizon household tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "KARMA-augmented LLM planner",
            "agent_description": "An LLM-based task planner (GPT-4o in experiments) that is memory-augmented via two modules: (1) long-term memory implemented as a non-volatile hierarchical 3D scene graph (3DSG) serialized into text and appended to prompts; (2) short-term memory containing multimodal per-object memory units (image, VLM-derived state, world coordinates, objectId) embedded into vectors for retrieval. Memory replacement for short-term memory uses configurable policies (FIFO with merging, W-TinyLFU) and retrieval uses vector-similarity top-K.",
            "base_llm_model": "GPT-4o",
            "base_llm_size": null,
            "text_game_name": "ALFRED-L (derived from ALFRED) evaluated in AI2-THOR; additional experiments on ALFWorld-R-style long-sequence tasks",
            "text_game_description": "ALFRED-L: long-sequence indoor household instruction-following tasks (simple, composite, complex) adapted from the ALFRED benchmark and executed in the AI2-THOR simulator; tasks require navigation and manipulation, multi-step dependency, and object-state goals (e.g., wash/cut/place). ALFWorld-R/ALFRED-R used to stress replacement policies with long sequences.",
            "uses_memory": true,
            "memory_type": "dual: long-term semantic 3D scene-graph (persistent) + short-term volatile object cache (episodic/working memory)",
            "memory_architecture": "Long-term memory: hierarchical 3D scene graph (3DSG) with area nodes, object nodes, topological edges, object attributes (3D position, volume, type); stored non-volatile and serialized into text prompts for the LLM. Short-term memory: fixed-capacity list of memory units; each unit stores objectType, objectId, world coordinates, raw imagePath, and VLM-extracted object state; units are embedded by a multimodal embedding model (OpenAI text-embedding-3-large used for experiments) into vectors and indexed for similarity retrieval. Replacement policies implemented: FIFO with merging, W-TinyLFU (two-segment LRU-style main/protection+window, uses counting Bloom filters for frequency stats and periodic halving).",
            "memory_retrieval_mechanism": "Short-term retrieval: embedding-based cosine similarity (OpenAI text-embedding-3-large) between instruction embedding and memory-unit embeddings; top-K most similar units are returned and their text content appended to LLM prompt. Long-term retrieval: entire serialized 3DSG is appended to the prompt (no embedding-based selection).",
            "memory_capacity": "Configurable fixed-size short-term cache; experiments used sizes such as 10 memory units (detailed W-TinyLFU [9,1] etc.) and larger sizes (up to 25 units in analyses); long-term memory treated as non-volatile (no strict capacity limit described in experiments).",
            "what_is_stored_in_memory": "Long-term: area nodes, adjacency/topology, immovable object nodes with attributes (3D positions, types, volumes). Short-term: per-object records containing objectType, objectId, world coordinates, image path / raw image, VLM-derived object state (e.g., cleaned, sliced), plus embedded vector representation.",
            "performance_with_memory": "On ALFRED-L in AI2-THOR: reported relative improvements over best baselines — composite tasks: 1.3x success rate improvement (reported as +43 percentage points absolute SR improvement in paper's summary) and 3.4x task execution efficiency improvement (68.7% reduction in time); complex tasks: 2.3x success rate improvement (reported as +21 percentage points absolute SR improvement) and 62.7x task execution efficiency improvement (69% reduction in time reported as absolute reduction). Exact SR numbers per category appear in Table 1 of the paper (KARMA composite SR ~0.43, complex SR improvements reported in text).",
            "performance_without_memory": "Ablation: removing short-term memory caused large drops in success rate (SR dropped by factors of ~1.9x for complex tasks and ~4.2x for composite tasks). Removing long-term memory had a larger impact on execution efficiency (reported RT decrease by factor ~2.7) and only modest impact on SR (~1.2x drop). Baseline (LoTa-Bench modified / other baselines) performance is reported in Table 1 (KARMA outperforms LoTa-Bench / HELPER / CAPEAM).",
            "has_ablation_study": true,
            "memory_improvement_magnitude": "Relative improvements: +1.3x SR (composite) and +2.3x SR (complex) vs best baselines; absolute reported gains cited in paper: +43 percentage points SR for composite tasks and +21 percentage points SR for complex tasks; efficiency improvements: 3.4x faster (composite) and 62.7x faster (complex) in task execution vs best baselines. Ablation: removal of short-term memory reduces SR by ~1.9x–4.2x depending on task class.",
            "key_findings_about_memory": "Short-term (object-centric) memory is critical for increasing task success rates on related multi-step tasks (composite/complex); long-term 3DSG memory primarily improves planning efficiency by reducing repeated exploration and action code generated by the LLM; higher short-term memory hit rates (measured by embedding similarity recall) linearly correlate with reduced exploration and faster task execution; W-TinyLFU replacement outperforms FIFO variants in hit rate and downstream efficiency when properly sized (large window favored).",
            "memory_limitations": "Evaluated in ideal simulation (no adversarial/environmental disturbances) so scalability to many real-world objects untested; semantic matching/VLM limitations cause degraded Memory Retrieval Accuracy for ambiguous instructions; fixed-size short-term cache can suffer from capacity constraints—requires careful replacement policy; current design is open-loop (no feedback-driven memory correction) and lacks biologically grounded theory.",
            "comparison_with_other_memory_types": "Paper compares short-term replacement strategies (FIFO with merging vs W-TinyLFU variants) and reports W-TinyLFU (large window) achieves highest memory hit rates and better reuse; ablation compares presence/absence of short-term vs long-term memory showing complementary roles (short-term -&gt; SR, long-term -&gt; efficiency). No direct comparison to other high-level memory paradigms (e.g., rehearsal, reflections) beyond cache policies.",
            "uuid": "e2925.0",
            "source_info": {
                "paper_title": "KARMA: Augmenting Embodied AI Agents with Long-and-Short Term Memory Systems",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "HELPER",
            "name_full": "Open-ended instructable embodied agents with memory-augmented large language models (HELPER)",
            "brief_description": "A prior memory-augmented LLM-based embodied agent referenced as a state-of-the-art baseline; designed to store and recall past experiences to support long-horizon instruction following in embodied environments.",
            "citation_title": "Open-ended instructable embodied agents with memory-augmented large language models",
            "mention_or_use": "use",
            "agent_name": "HELPER (as cited baseline)",
            "agent_description": "Cited as a memory-augmented LLM agent that stores past experiences to inform planning for embodied tasks; used in the paper as a strong baseline for complex tasks.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": "ALFRED-L / ALFRED tasks in AI2-THOR (used as baseline comparisons in this paper)",
            "text_game_description": "Same family of long-horizon indoor household tasks (multi-step instruction following) executed in AI2-THOR; used to evaluate long-term planning with memory augmentation.",
            "uses_memory": true,
            "memory_type": "memory-augmented (experience/past-experiences / episodic-like memory) as described in the original HELPER work; paper refers to it as a memory-augmented baseline",
            "memory_architecture": "Not specified in detail in this paper; referenced as an approach that stores past experiences and uses them to augment LLM planning. (Original paper should be consulted for architecture specifics.)",
            "memory_retrieval_mechanism": "Not specified in this paper.",
            "memory_capacity": null,
            "what_is_stored_in_memory": "Described generally as storing past experiences; specifics not provided here.",
            "performance_with_memory": "Used as best-performing baseline for complex tasks; KARMA reported a relative improvement over HELPER of 2.3x in success rate for complex tasks and a 62.7x improvement in task execution efficiency (paper reports KARMA vs HELPER numbers).",
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Referenced as an example of a memory-augmented embodied agent; used to show KARMA's improvements in complex tasks.",
            "memory_limitations": "Not discussed in detail in this paper (referenced baseline).",
            "comparison_with_other_memory_types": "Not compared in detail here; used as a baseline for KARMA's improvements.",
            "uuid": "e2925.1",
            "source_info": {
                "paper_title": "KARMA: Augmenting Embodied AI Agents with Long-and-Short Term Memory Systems",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "CAPEAM",
            "name_full": "CAPEAM (Context-aware planning and environment-aware memory for instruction following embodied agents)",
            "brief_description": "An LLM-based embodied agent (Kim et al., 2023) that uses short-term memory to track object positions via semantic labels; used as a baseline in KARMA experiments and cited for its short-term memory approach.",
            "citation_title": "Context-aware planning and environment-aware memory for instruction following embodied agents",
            "mention_or_use": "use",
            "agent_name": "CAPEAM",
            "agent_description": "An LLM-planner augmented with environment-aware memory; specifically referenced for using short-term memory to continuously track object positions with semantic labels to assist planning in embodied instruction following.",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": "ALFRED-L / ALFRED tasks in AI2-THOR (used as baseline comparisons)",
            "text_game_description": "Long-horizon indoor manipulation and navigation tasks with subtask dependencies (ALFRED-derived).",
            "uses_memory": true,
            "memory_type": "short-term/working memory for tracking object positions (semantic-label based), per the citation referenced in the paper",
            "memory_architecture": "Not fully detailed in this paper; the referenced research (Kim et al., 2023) is noted for maintaining short-term memory that continuously tracks object positions via semantic labels.",
            "memory_retrieval_mechanism": "Not specified in this paper (refer to original CAPEAM paper).",
            "memory_capacity": null,
            "what_is_stored_in_memory": "Reportedly object positions and semantic labels for recently observed objects (short-term tracking).",
            "performance_with_memory": "Used as a baseline; KARMA reports a relative improvement over CAPEAM of 1.3x in success rate for composite tasks and a 3.4x improvement in task execution efficiency (paper reports KARMA vs CAPEAM numbers).",
            "performance_without_memory": null,
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Cited as prior work demonstrating benefit of short-term object-tracking memory; used to contextualize KARMA's design and improvements.",
            "memory_limitations": "Not discussed in detail in this paper.",
            "comparison_with_other_memory_types": "Not directly compared here beyond KARMA's reported comparisons to CAPEAM as a baseline.",
            "uuid": "e2925.2",
            "source_info": {
                "paper_title": "KARMA: Augmenting Embodied AI Agents with Long-and-Short Term Memory Systems",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "LoTa-Bench (Modified)",
            "name_full": "LoTa-Bench: Benchmarking language-oriented task planners for embodied agents (modified baseline)",
            "brief_description": "A prompt-based baseline for LLM task planning used as a primary baseline in experiments; provides prefix + in-context examples and selects skills from a skill set by scoring via the LLM.",
            "citation_title": "Lota-bench: Benchmarking language-oriented task planners for embodied agents",
            "mention_or_use": "use",
            "agent_name": "LoTa-Bench (modified prompt-based planner)",
            "agent_description": "A language-oriented task planner baseline that constructs prompts with a prefix and in-context examples; the LLM computes probabilities over executable skills and selects next skill to execute. The paper uses a modified LoTa-Bench as a baseline (non-memory or limited memory baseline).",
            "base_llm_model": null,
            "base_llm_size": null,
            "text_game_name": "ALFRED-L / ALFRED tasks in AI2-THOR (used as the baseline benchmark environment)",
            "text_game_description": "Long-horizon indoor instruction-following tasks (ALFRED-derived) executed in AI2-THOR.",
            "uses_memory": false,
            "memory_type": null,
            "memory_architecture": null,
            "memory_retrieval_mechanism": null,
            "memory_capacity": null,
            "what_is_stored_in_memory": null,
            "performance_with_memory": null,
            "performance_without_memory": "Used as a non-memory (or minimally in-context-only) baseline; Table 1 reports its success rates and efficiency on ALFRED-L (modified LoTa-Bench results are presented but specific numbers are in Table 1 of the paper).",
            "has_ablation_study": null,
            "memory_improvement_magnitude": null,
            "key_findings_about_memory": "Serves as a reference baseline demonstrating that simple in-context prompting without explicit external memory is less effective and less efficient than memory-augmented approaches like KARMA.",
            "memory_limitations": "As an in-context-only baseline, susceptible to context-window and in-context forgetting for long-horizon tasks (discussed as motivation in the paper).",
            "comparison_with_other_memory_types": "Not applicable (baseline without explicit external memory).",
            "uuid": "e2925.3",
            "source_info": {
                "paper_title": "KARMA: Augmenting Embodied AI Agents with Long-and-Short Term Memory Systems",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Open-ended instructable embodied agents with memory-augmented large language models",
            "rating": 2,
            "sanitized_title": "openended_instructable_embodied_agents_with_memoryaugmented_large_language_models"
        },
        {
            "paper_title": "Context-aware planning and environment-aware memory for instruction following embodied agents",
            "rating": 2,
            "sanitized_title": "contextaware_planning_and_environmentaware_memory_for_instruction_following_embodied_agents"
        },
        {
            "paper_title": "Lota-bench: Benchmarking language-oriented task planners for embodied agents",
            "rating": 2,
            "sanitized_title": "lotabench_benchmarking_languageoriented_task_planners_for_embodied_agents"
        },
        {
            "paper_title": "Rap: Retrieval-augmented planning with contextual memory for multimodal LLM agents",
            "rating": 2,
            "sanitized_title": "rap_retrievalaugmented_planning_with_contextual_memory_for_multimodal_llm_agents"
        },
        {
            "paper_title": "ALFWorld: Aligning Text and Embodied Environments for Interactive Learning",
            "rating": 2,
            "sanitized_title": "alfworld_aligning_text_and_embodied_environments_for_interactive_learning"
        },
        {
            "paper_title": "Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models",
            "rating": 1,
            "sanitized_title": "jarvis1_openworld_multitask_agents_with_memoryaugmented_multimodal_language_models"
        }
    ],
    "cost": 0.017751,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>KARMA: Augmenting Embodied AI Agents with Long-and-short Term Memory Systems</p>
<p>Zixuan Wang 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Bo Yu2 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Junzhe Zhao 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Wenhao Sun 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Sai Hou® 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Shuai Liang' 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Xing Hu' 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Yinhe Han 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Yiming Gan 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Joon Sung Park 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Joseph O'brien 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Carrie Jun Cai 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Mered- Ith Ringel Morris 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Percy Liang 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Chen Qian 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Xin Cong 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Cheng Yang 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Weize Chen 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Yusheng Su 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Juyuan Xu 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Zhiyuan Liu 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Krishan Rana 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Jesse Haviland 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Sourav Garg 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Jad Abou- Chakra 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Ian Reid 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Niko 2023 Suenderhauf 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Say 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Gabriel Sarch 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Yue Wu 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>Michael J Tarr 
Institute of Computing Technology
Chinese Academy of Sciences (ICT, CAS) * Beijing Institute of Technology *University of Chinese Academy of Sciences</p>
<p>KARMA: Augmenting Embodied AI Agents with Long-and-short Term Memory Systems
4C73EFA125FF85EDFA6D18FB8B141051unified instructable embodied agent to tackle four interactive vision-language domains with memoryaugmented language models. arXiv preprint arXiv: 2404.19065.
Embodied AI agents responsible for executing interconnected, long-sequence household tasks often face difficulties with in-context memory, leading to inefficiencies and errors in task execution.To address this issue, we introduce KARMA, an innovative memory system that integrates long-term and short-term memory modules, enhancing large language models (LLMs) for planning in embodied agents through memory-augmented prompting.KARMA distinguishes between long-term and short-term memory, with long-term memory capturing comprehensive 3D scene graphs as representations of the environment, while shortterm memory dynamically records changes in objects' positions and states.This dualmemory structure allows agents to retrieve relevant past scene experiences, thereby improving the accuracy and efficiency of task planning.Short-term memory employs strategies for effective and adaptive memory replacement, ensuring the retention of critical information while discarding less pertinent data.Compared to state-of-the-art embodied agents enhanced with memory, our memory-augmented embodied AI agent improves success rates by 1.3x and 2.3x in Composite Tasks and Complex Tasks within the AI2-THOR simulator, respectively, and enhances task execution efficiency by 3.4x and 62.7x.Furthermore, we demonstrate that KARMA's plug-and-play capability allows for seamless deployment on real-world robotic systems, such as mobile manipulation platforms.Through this plug-andplay memory system, KARMA significantly enhances the ability of embodied agents to generate coherent and contextually appropriate plans, making the execution of complex household tasks more efficient.The experimental videos from the work can be found at https://youtu.be/4BT7fnw9ehs.Our code is available at https://github.com/WZX®@Swarm@Robotics/KARMA/tree/master.</p>
<p>Introduction</p>
<p>Robotic applications are evolving towards longer and more complex tasks.Using an LLM as its core planning module can effectively decompose long and complex tasks into multiple short and fixed movements (Choi et al., 2024;Sarch et al., 2024;Chen et al., 2023b;Vemprala et al., 2023;Rana et al., 2023;Brohan et al., 2022Brohan et al., , 2023;;Belkhale et al., 2024), increasing the success rate.</p>
<p>Yet, simply equipping an embodied agent or a robot with an LLM is not enough.Take indoor household tasks as an example, they usually require a sequence of interrelated instructions where later ones have strong or weak dependencies on previous ones.When the amount of in-context examples and task descriptions necessary to cover the task constraints increases, even advanced models like GPT-40 can blur critical details, such as the location of a previously used object.Thus, there is a growing need to enhance the power of LLMs with "memory-augmented prompting" (Sarch et al., 2023a;Lewis et al., 2020;Mao et al., 2020).</p>
<p>We introduce KARMA, a plug and play memory system tailored for indoor embodied agents.The memory system comprises both long-term memory, 'Institute of Automation, Chinese Academy of Science -represented as a non-volatile 3D scene graph, and for Society ate information about objects encountered during instruction execution.The memory system allows agents to accurately recall the positions and states of objects during complex household tasks, reducing task redundancy and enhancing execution efficiency and success rates.</p>
<p>On top of the memory system design, we propose to effectively maintain the contents of the memory given the capacity constraints.Specifically, we use the metric hit rate that measures how often a memory recall requirement is satisfied.We demonstrate that a higher hit rate indicates an improved replacement policy and enhanced system performance.Using this metric, we propose replacing the least recently used (LRU) unit whenever a new unit needs to be incorporated into a full memory.Our findings show that this approach achieves a higher hit rate compared to a naive first-in-firstout policy.</p>
<p>In summary, the paper makes following contributions to the community: Large language models have been widely used in robotic applications (Huang et al., 2022;Ahn et al., 2022) due to their impressive generalization abilities and common-sense reasoning capabilities (Brown et al., 2020;Madaan et al., 2022;Achiam et al., 2023).In most cases, LLMs replace the task planning and decision making modules in traditional robotic computing pipeline.Most robotic applications now encode sensor inputs into the format of LLM-accepted tokens and use LLMs to generate the next instructions, which further connect to robots through predefined skills or basic movements across different degrees of freedom (Ahn et al., 2022;Jin et al., 2023;Wu et al.. 2023a,c).</p>
<p>2.2</p>
<p>Memory-Augmented Prompting of LLM-Based Agent Using LLMs as task planner for robots face the challenge of accurately retaining information across multiple interdependent tasks.Thus, augmenting LLM-based agents with different forms of memory is a common approach in role-playing games (Shao et al., 2023;Li et al., 2023a;Wang et al., 2023e;Zhou et al., 2023;Zhao et al., 2023), social simulations (Kaiya et al., 2023;Park et al., 2023;Gao et al., 2023;Li et al., 2023b;Hua et al., 2023), personal assistants (Zhong et al., 2024;Modarressi et al., 2023;Lu et al., 2023;Packer et al., 2023;Lee et al., 2023;Wu et al., 2023b;Hu et al., 2023;Liu et al., 2023;Liang et al., 2023), open-world games (Wang et al., 2023a;Zhu et al., 2023;Wang et al., 2023f;Yan et al., 2023), code generation (Tsai et al., 2023;Chen et al., 2023a;Qian et al., 2023;Li et al., 2023b;Zhang et al., 2024b), recommendations (Wang et al., 2023d,c; Zhang et al., 2024a), and domain-specific expert systems (Wang et al., 2023b;Yang et al., 2023;Zhao et al., 2024b).</p>
<p>The definition and formats of the memory is distinctive in different works.Historical actions (Park et al., 2023), thoughts (Liu et al., 2023), contexts (Liang et al., 2023;Packer et al., 2023) are explored.Different memory management mechanisms are also designed and evaluated.For example, agents can simply use text indexing to match relevant memory; the memory recall and management can also be much more complicated, involving text embedding, semantic retrieval (Zhao et al., 2024a) and Graph RAG (Edge et al., 2024).</p>
<p>In the field of embodied agents, much of the research (Kagaya et al., 2024;Zhang et al., 2023;Sarch et al., 2023a;Wang et al., 2023f) focuses on storing and recalling past experiences, allowing agents to learn from previous interactions and make more informed decisions.Research (Kim et al., 2023) uses short-term memory to maintain and continuously track the positions of objects through semantic labels.Additionally, other studies utilizes structured maps as long-term memory, enabling agents to more efficiently locate places or objects in vision-language navigation tasks (Zhan et al., 2024;Chiang et al., 2024).However, these approaches fail to address challenges such as hallucinations or memory inconsistencies that often arise long-sequence task planning with LLMs.Furthermore, integrating memory mechanisms into LLMs remains at a preliminary stage, particularly regarding memory saving and updating mechanisms.For example, saving everything permanently can result in unaffordable storage requirements, while refreshing the memory every time agents restart will lose any long-term capability.Additionally, the decision of which memory unit to replace remains unsolved.Most approaches use either a forgetting curve (Zhong et al., 2024) or the simple first-infirst-out principle (Packer et al., 2023) without detailed discussions on context-specific updates.</p>
<p>Our work addresses these limitations by incorporating a tailored memory framework for embodied Al agents.This system includes long-term memory in the form of a 3D scene graph representing static objects and short-term memory for instant information about recent activities.This long-short memory approach helps the agent better understand its environment and recent actions.Various exit and update mechanisms are discussed to maintain effectiveness even under fixed memory capacity, providing a comprehensive solution for long sequential tasks in household environments.</p>
<p>Method</p>
<p>We describe the methodology in this section, with start on elaborating the problem setup (Sec.3.1), Sec.3.2 gives an overview of the framework and Sec.3.3 and Sec.3.4 reveals the long-term and short-term memory design.We wrap Sec.3.6 with the novel memory exit and replacement mechanism.</p>
<p>Problem setup</p>
<p>Although generalizable, our work focuses on indoor environment where users send instructions to an agent to perform a series of tasks, H = Lig, Lt, ,.-., Tt), These tasks are typically related in terms of both time and order of completion.For instance, if the agent is asked to prepare a salad, it must first wash an apple (/;,) and cut it (J;,), then repeat the process with a tomato (J;,,J¢,), and finally place the ingredients into a bowl and mix them.During this process, an large volume of highdimensional data is incorporated through various sensors, such as the agent's location and the po-sition and status of different objects.Even when equipped with a large language model as its planner, the agent may lose track of its tasks and need to re-explore the environment, which motivates our work to customize a memory system to augment the agent.</p>
<p>In this paper, we use S € {Smanipulation U Snavigation } to represent the set of skills that the agent can perform, which should be executed by a LLM through pre-defined APIs.The instruction I can be further decomposed into an ordered set of K sub-tasks, T = {T), T2,...,Tk}, where Kk represents the sequence of sub-tasks over time.</p>
<p>Overview</p>
<p>KARMA is a memory system tailored for embodied Al agents, incorporating memory design, recall using context embedding with a pre-trained LLM and an accurate replacement policy.Specifically, we design two memory modules: long-term memory and short-term memory.The long-term memory comprises a 3D scene graph (3DSG) representing static objects in the environment, while the shortterm memory stores instant information about used or witnessed objects.The long-term memory aids the agent in better understanding the environment, and the short-term memory helps the agent understand its recent activities.Due to fixed memory capacity, we also discuss various exit and update mechanisms.Fig. 1 provides an overview of our work.</p>
<p>Long-Term Memory Design</p>
<p>Long-term memory is large in size, non-volatile, and task-irrelevant.It should be built incrementally and updated infrequently.This type of memory is designed to store static information that remains constant over extended periods, such as the layout of the environment and the positions of immovable objects.In the context of an indoor agent, semantic maps serve as an appropriate carrier for it.In many forms of semantic maps, KARMA uses a 3D scene graph to represent the environment.The main reason we choose a 3DSG instead of 2D semantic maps or voxel grids is that 3DSG offers a more accurate and comprehensive representation of the environment and features a topological structure, which is essential for tasks that require precise navigation and manipulation.Also, even a state-ofthe-art multi-modality LLM has difficulties understanding the geographic relationships from a 2D semantic map, while a 3DSG display it explicitly.The 3DSG utilizes a hierarchical structure encompassing floors, areas, and objects, not only capturing the spatial relationships and attributes of objects but also leveraging the benefits of a topological graph.This structure is particularly advantageous when expanding the map to represent the environment, as its sparse topological nature effectively mitigates the impact of accumulated drifts compared to dense semantic maps.Thus, 3DSG is better suited to meet the navigation needs in unknown environments.The construction process of the 3DSG is similar to existing works (Rosinol et al., 2021;Armeni et al., 2019;Rana et al., 2023), as illustrated in Figure 2. We establish and manage a hierarchical topological graph G = (V,E), where the set of vertices V is composed of Vj U... UV, with &amp; = 3, Each V; represents the set of vertices at a particular level of the hierarchy.The area nodes, V2 = {V3}, V?,..., ViV}, are evenly distributed across the reachable regions in the indoor environment, with their world coordinates acquired through a simulator.If two area nodes are navigable to each other, an edge is established between them.For each area node, we detect the types and additional information of ob-jects within a certain radius, using data acquired through a simulator.In real-world applications,this object detection can be performed using methods such as Faster R-CNN.The detected immovable entities are then assigned as object nodes to their respective area nodes.These object nodes encode detailed attributes such as volume and 3D position.</p>
<p>In our framework, the agent gradually builds and maintains a 3DSG as it explores the indoor environment.The graph remains unchanged unless the indoor environment change.When being used by the planner, we transform the 3DSG into a topological graph and serialized it into a text data format that can be directly parsed by a pretrained LLM.An example of a single area node from the 3DSG is as follows: {name: node_1l, type: Area, contains: [bed, table, window, ...], adjacent nodes: [node_2, node_8], position: [2.34, 0.00, 2.23]} with edges between nodes captured as {node_1 ++ node_2, node_1 &lt;&gt; node_8}.</p>
<p>Our design and use of long-term memory aim to provide accurate geometric relationships within the indoor environment.With this information, the agent is able to reduce the cost for repetitive environment exploration by allowing the addition or deletion of nodes through topological relationships, thus updating the environment representation seamlessly.This approach effectively avoids the drift errors typically caused by loop closure detection in traditional SLAM methods, and it minimizes the need for extensive place recognition processes, saving computational resources, storage, and time.</p>
<p>Moreover, long-term memory enhances the agent's ability to make informed decisions based on a comprehensive understanding of the environment.This capability is particularly useful for planning complex, multi-step tasks.By accessing detailed and persistent environmental data, the agent can predict potential obstacles and plan its actions more effectively, thereby improving both task completion success rates and execution efficiency.Also, the 3DSG is updated when the indoor environment changes, capturing the up-to-date information.</p>
<p>Short-Term Memory</p>
<p>Short-term memory is small, volatile, and frequently updated.It is refreshed every time the agent starts and provides instant memorization of recently used objects and their status during task execution.This ensures that the same objects or relevant information are readily available for subsequent tasks.</p>
<p>Among all the information the agent captures during tasks, vision data is relied upon, as it provides the highest information density compared to other sensor inputs.After capturing an image, we use a vision language model (VLM) to analyze the image and extract the state of the object of interest (OOT).This process is task-specific, meaning the VLM is fed both the task and the image to handle multiple objects in the image.Subsequently, the world coordinates (acquired through a simulator), the state (generated by the VLM), and the raw image form a memory unit in the short-term memory, akin to a line of data in a cache.Finally, a multimodality embedding model converts the memory unit into a vector for later recall.</p>
<p>We use an example to illustrate the design of KARMA's short-term memory.Given a task asking the agent to 'wash an apple and place it in a bowl, the agent will memorize the coordinates of the apple and its state (cleaned) at the end.If a subsequent task asks the agent to "bring an apple,' KARMA will retrieve the apple's memory from short-term memory, include it in the prompt, and query the LLM to generate a more efficient task plan.This saves the agent from exploring the kitchen to find the apple, reduces interactions with the LLM, and speeds up the process.KARMA's planner uses both long-term and shortterm memory when interacting with the LLM.As mentioned earlier, the entire long-term memory is directly serialized into the prompt, while only one unit of the short-term memory can be selected.KARMA uses vector similarity to select from the entire short-term memory.Each short-term memory is embedded into a set of vectors using a pre-trained embedding model.For the current instruction J, KARMA retrieves the top-K most similar memories-those with the smallest cosine distance to the embedding of the input instruction J.The corresponding text content of these memories is then added as context to the LLM prompt.</p>
<p>We show an example prompt in Apdx. A. It includes the action code for the basic skills S (parameterized as Python functions), examples of task decomposition, the input instruction J, and the retrieves short-term memory and long-term memory.The LLM is tasked with generating action code based on the parameterized basic skills S.</p>
<p>Memory Replacement</p>
<p>Unlike long-term memory that can be stored in nonvolatile storage, short-term memory has a fixed capacity and can easily become full.An effective short-term replacement policy ensures it remains highly relevant to subsequent tasks.</p>
<p>Hit rate.We use memory hit rate to evaluate the effectiveness of memory replacement policies.This metric is defined as the ratio of the number of times the required memory units are found in short-term memory to the total number of queries.It is widely used in evaluating cache replacement policies (Einziger and Friedman, 2014), with higher values indicating better performance.</p>
<p>First-In-First-Out (FIFO).The FIFO replacement policy is the most straightforward.It manages memory units as a queue.When the queue is full and a new memory unit needs to be added, the earliest entry will be removed from the queue.</p>
<p>We improve the FIFO policy to better suit our application by adding a merging option.When a new memory unit needs to join the queue and the queue is full, we first check the object's ID in all memory units in the queue.If the same ID exists, the new unit will replace the old one with the same object's ID, instead of replacing the oldest unit.</p>
<p>Least Frequently Used.A more complex yet accurate replacement policy is Least Frequently Used (LFU).The design principle of LFU is based on the usage frequency of each memory unit.Whenever a new memory unit needs to join, the existing unit with the lowest usage frequency is replaced.This results in a high hit rate, as the memory retains frequently-used units.Since perfect LFU is not feasible, we use an approximate method called W-TinyLFU.</p>
<p>W-TinyLFU maintains two segments of memory: a main segment and a window segment.The main segment is organized in a two-segment Least Recently Used (LRU) manner, containing a protection segment and an elimination segment.Units in the protection segment are the safest; even if they are picked for replacement, they first move to the elimination segment.</p>
<p>Every time a unit needs to join the memory, it enters the window segment first.When the memory is full and a unit needs to be evicted, a comparison occurs among all units in the window segment and the elimination segment.The memory then selects the unit whose eviction would minimally impact the overall usage frequency and evicts it.W-TinyLFU uses counting Bloom filters (Luo et al., 2018) as the basic data structure to count the usage of memory units.To keep frequency statistics fresh, W-TinyLFU applies a reset method.Each time a memory unit is added, a global counter is incremented.When the counter reaches a threshold W, all counters are halved:c; &lt;-3.</p>
<p>Experiments</p>
<p>We discuss the setup Sec.4.1 and metrics Sec.4.2 first, followed by extensive experiments.This includes success rate and efficiency (Sec.4.3), different replacement policies (Sec.4.4), ablation study (Sec.4.5) and real-world deployment(Sec.4.6).</p>
<p>Experimental Setup and Metrics</p>
<p>Experimental Settings.We use the widelyadopted AI2-THOR simulator (Kolve et al., 2017) for evaluation.The simulator's built-in object detection algorithm provided the label of objects and their relevant information for both long-term and short-term memory.Additionally, we employ Ope-nAI's text-embedding-3-large model as the embedding model for memory recall.</p>
<p>Baseline.To our best knowledge, most current methods using LLMs for task planning are very similar with LoTa-Bench (Choi et al., 2024).It provides a prompt that includes a prefix, in-context examples to the LLM, and then the LLM calculates the probabilities of all executable skills based on this prompt and selects the skill from skill sets most likely to complete the task.We also use it as our baseline.Additionally, we optimize the efficiency and success rate of planning and executing tasks in LoTa-Bench by referring to the skill sets configurations and selection described in SMART-LLM (Kannan et al., 2023).</p>
<p>Dataset.</p>
<p>The dataset construction utilizes tasks from the ALFRED benchmark (Shridhar et al., 2021).By extracting its typical tasks and reorganizing them into long sequence tasks that align with everyday human needs, we ensured a more accurate assessment.More details of the dataset are provided in supplementary material.</p>
<p>This new dataset, ALFRED-L, includes 48 highlevel instructions that detail the length, relevance, and complexity of sequential tasks.Additionally, it provides corresponding AI2-THOR floor plans to offer spatial context for task execution.We also include the ground truth states and corresponding location of objects after the completion of each subtask.This ground truth is used as symbolic goal conditions to determine whether the tasks are successfully completed.For example, conditions such as heated, cooked, sliced, or cleaned are specified.Our dataset comprises three task categories: Simple Tasks have multiple unrelated tasks.The agent is assumed to perform sequential tasks with a length of less than five, without requiring specific memory to assist in task completion.</p>
<p>Composite Tasks include highly related tasks.These tasks involve multiple objects, and the agent needs to utilize memories generated from previous related tasks to execute subsequent subtasks.</p>
<p>Complex Tasks consist of multiple loosely related tasks.Some of these tasks involve specific objects, while others involve vague object concepts.For example, the agent be instructed to wash an apple(J;,) and cut it(/;, ), then to place a red food on the plate(J;,).ALFRED-L comprises 15 tasks categorized as simple tasks, 15 tasks as composite tasks, 18 tasks as complex tasks.</p>
<p>Additionally, we use another dataset to better assess the performance of the memory replacement mechanism.</p>
<p>The new dataset, ALFWorld-R, consists of long-sequence tasks A = {Ii,,1h,,..-, Jey}.with each task [;,,7 € {0, 1,2, ..., N} in the sequence randomly selected from tasks in ALFRED.</p>
<p>4.2</p>
<p>Evaluation Metrics.</p>
<p>Success Rate (SR) is the percentage of tasks fully completed by the agent.A task is considered complete only when all subtasks are achieved.</p>
<p>Memory Retrieval Accuracy (MRA) is a binary variable determines if related memory can be successfully retrieved.</p>
<p>Memory Hit Rate (MHR).The definition is the same as the hit rate described in Sec.3.6.</p>
<p>Reduced Exploration (RE).This metric measures the effectiveness of the system in reducing unnecessary exploration attempts.RE = Fegieel where Fotai is the total number of exploration attempts, Freduced 1s the number of exploration attempts that were reduced.</p>
<p>Reduced Time (RT).This metric measures the proportion of time saved by reducing unnecessary actions during task execution.RT' = Tlic where Tiotal 18 the total time taken for the task, Theducea 18 the time that was reduced.</p>
<p>Success Rate and Efficiency Evaluation</p>
<p>Success Rate &amp; Task Efficiency.In Tbl. 1, we present the quantitative results of KARMA and the baselines on the sequence tasks dataset ALFRED-L.For complex tasks, KARMA achieves a 21% task success rate improvement and a 69% reduction in time, which represent a relative improvement of 2.3x and 62.7x, respectively, compared to HELPER (the best-performing baseline in this setting).For composite tasks, KARMA achieves a 43% task success rate improvement and a 68.7% reduction in time, which represent a relative improvement of 1.3x and 3.4x, respectively, compared to CAPEAM (the best-performing baseline in this setting).For simple tasks, KARMA achieves a 42% task success rate improvement and a 61.2% reduction in time, which represent a relative improvement of 1.1x and 2.3x, respectively, compared to HELPER (the best-performing baseline in this setting).It is worth noting that since simple tasks do not require the use of short-term memory, KARMA does not show a significant improvement in task success rate over other baselines.</p>
<p>Memory Retrieval Accuracy.We show the accuracy of memory recall in the MRA column of Tbl. 1.Our memory system achieves a recall accuracy that is 2.2x higher for composite tasks compared to complex tasks, as the recall method has certain limitations when instructions contain ambiguous information.We believe this is due to the inherent performance limitations of the commonly used models for semantic matching.For complex tasks, instructions may contain particularly ambiguous semantics, such as "get me a highcalorie food," where even the most advanced semantic matching models perform poorly.</p>
<p>Replacement Policy Evaluation</p>
<p>Fig. 4 illustrates the efficiency of the FIFO policy compared to the W-TinyLFU policy under various configurations of window segment size and main segment size, with a total of 10 memory units.We show the number of consecutive tasks performed by the agent on the x-axis.The y-axis shows the memory hit rate for each memory replacement policy, representing the effectiveness of each policy.Vertical lines of different colors indicate whether the corresponding policy has undergone a warm-up phase.We consider memory to be warmed up when the occupancy rate of the memory units exceeds  [10] means the memory size of FIFO is 10, [9,1] means the memory size of W-TinyLFU is also 10, the main segment is 1, window segment is 9.
@-W-TinyLFU [9,1] W-TinyLFU [7,3] W-TinyLFU [4,6] -@-W-TinyLFU [1,9] -@-FIFO [10]
95%.After all replacement policies have undergone their warm-up phases, the W-TinyLFU policy with a window segment size of 9 achieves the highest memory hit rate.This indicates that, on the ALFRED-R dataset, a larger window segment size in the W-TinyLFU policy allows for more effective utilization of memory units.For W-TinyLFU, a larger window size typically covers a broader time range, capturing more memory units that are likely to be frequently recalled.These memory units have a high probability of being reused in the task sequence, thereby increasing the memory hit rate.</p>
<p>-e@-FIFO [5] -@-FIFO [10] FIFO the memory is with size equals to 10. Fig. 5 illustrates the memory hit rate of FIFO pol-icy with different numbers of memory units, with X-axis represents the number of tasks.As expected, larger memory size brings higher hit rate, the memory hit rate with 25 memory units is 4.6 higher than with only 5 memory units.Similar results can be extracted through Fig. 6, where memory hit rate with 25 memory units is 3.9 higher than with only 5 memory units.</p>
<p>-e@-W-TinyLFU means the memory size of wlinyLFU is 10, the main segment is 1, window segment is 9.In Fig. 7, we illustrate the impact of memory hit rate on the efficiency of task execution.The x-axis shows the memory hit rate of the W-TinyLFU policy with a window segment size of 9 and a main segment size of 1.The y-axis displays the proportion of reduced exploration.We demonstrate that the memory hit rate and the proportion of reduced exploration are linearly correlated.This means that increasing the memory hit rate enhances the agent's task execution efficiency.A higher memory hit rate signifies more efficient use of memory units.This enhances the agent's ability to recall relevant information, reducing the amount of action code needed for task execution, and ultimately improving overall task performance.
[4,1] -@-W-TinyLFU [9,1] W-TinyLFU [14,1] W-TinyLFU [19,1] W-TinyLFU [24,1] 0.4 | 0.3+ | oO I © 0.2- = oat - 0.0 1 1 L 1 L rT 0 10</p>
<p>Ablation Study</p>
<p>In Tbl. 2, we evaluate the performance of KARMA after removing short-term memory or long-term memory.The removal of short-term memory significantly affected the agent's ability to handle com- plex and composite tasks, with the success rate dropping by 1.9x and 4.2x, respectively.However, this did not greatly impact the agent's task execution efficiency, which decreased only by 1.2 and 1.1.On the other hand, removing long-term memory had a notable impact on task execution efficiency, with the RT decreasing by 2.7, but its effect on success rate was less pronounced, with SR only dropping by 1.2x.</p>
<p>In summary, short-term memory plays a key role in improving task success rates, while long-term memory has a greater impact on task efficiency.</p>
<p>Long-term memory retains 3D scene maps representing the environment, helping to reduce the action code generated by the LLM during task planning, thereby enhancing task execution efficiency.Meanwhile, short-term memory stores information about recently used objects, ensuring that these objects or relevant details are readily accessible for future tasks.</p>
<p>4.6</p>
<p>Real-world deployment</p>
<p>We deploy KARMA ona mobile manipulation robot consisting of a UR3 robotic arm and a six-wheeled chassis to demonstrate KARMA's ability to store and retrieve memory, enhancing the LLM's capability for planning long-sequence tasks in real-world environments.For the robot's navigation and obstacle avoidance, we utilize Google Cartographer for simultaneous localization and mapping (SLAM).</p>
<p>The camera mounted on the robotic arm's gripper ing embodied AI agents by integrating external long-and-short term memory systems.Through the implementation of a customized memory system, recall mechanism, and replacement policy, we demonstrate significant improvements over stateof-the-art embodied agents that also utilize memory.Specifically, our memory-augmented AI agent achieves success rates that are 1.3x higher in composite tasks and 2.3 higher in complex tasks.Additionally, task execution efficiency is improved by 3.4x in composite tasks and an impressive 62.7 x in complex tasks.This memory system streamlines the transition from simulation to real-world robotic applications, allowing long-and short-term memory storage and recall methods to be seamlessly integrated into task planning for real robotic systems (Sun et al., 2024).</p>
<p>Limitations</p>
<p>Ideal Simulation Environments.</p>
<p>In this work, all evaluations are performed under ideal simulation environments, free from interruptions by other agents or humans.However, this ideal situation is not reflective of real life.Although this paper includes extensive experiments, it lacks evaluation of how the memory system will behave in real-world scenarios.Specifically, the number of objects in the real world will significantly increase compared to a simulation environment, making the effectiveness of recall and replacement mechanisms crucial to final performance.Additionally, we have not tested the system's response to intentional disturbances by humans.These factors constitute the primary limitation of this paper.</p>
<p>Lack of Biological Theory.Although effective, the current design of the memory system is analogous to the memory systems of existing computing platforms.For instance, the concept of short-term memory and its replacement can be found in cache design.However, human memory may not function in this manner.This work borrows terminology from human memory yet lacks theoretical support from a biological perspective, which constitutes its second limitation.</p>
<p>Open-loop Planning.In this work, all memory operations and planning are open-loop, meaning there is no feedback.However, in most robot system designs, feedback is necessary.For example, if the memory is incorrect, there is no mechanism designed for eviction or updating.The lack of feedback constitutes the third limitation of this paper.We provide detailed skill APIs and their corresponding action codes in the Listing?2.</p>
<p>E LANGUAGE MODELS</p>
<p>Tbl. 4 lists the language models used in experiments and outlines their core functions.</p>
<p>F Details of image analysis in short-term memory</p>
<p>In Fig. 10, we present the prompt used to analyze images stored in short-term memory by the Vision-Language Model (VLM).The text highlighted in blue, [Image], represents the placeholder that will be filled with an image, while [task] will be replaced with the actual instruction.We employed a step-by-step Chain of Thought approach to guide the VLM in identifying the relevant objects and their corresponding states.</p>
<p>G Anexample result of KARMA on the</p>
<p>ALFRED-L dataset</p>
<p>In Fig. 11, we present images of the agent performing tasks in the AI2-THOR simulator.</p>
<p>Listing 2: Full Skill API and Action CODE used in the prompts.</p>
<p>Fig. 2 :
2
Fig. 2: Transforming 3D scene graphs into prompts.</p>
<p>Fig. 3 :
3
Fig. 3: Recalling long-term and short-term memory</p>
<p>Fig. 4 :
4
Fig. 4: The memory hit rate of FIFO and W-TinyLFU.</p>
<p>Fig. 5 :
5
Fig. 5: Evaluation on different FIFO sizes.[10] means</p>
<p>Fig. 6 :
6
Fig.6: Evaluation on W-TinyLFU configurations.[9,1]means the memory size of wlinyLFU is 10, the main segment is 1, window segment is 9.In Fig.7, we illustrate the impact of memory hit rate on the efficiency of task execution.The x-axis shows the memory hit rate of the W-TinyLFU policy with a window segment size of 9 and a main segment size of 1.The y-axis displays the proportion of reduced exploration.We demonstrate that the memory hit rate and the proportion of reduced exploration are linearly correlated.This means that increasing the memory hit rate enhances the agent's task execution efficiency.A higher memory hit rate signifies more efficient use of memory units.This enhances the agent's ability to recall relevant information, reducing the amount of action code needed for task execution, and ultimately improving overall task performance.</p>
<p>Fig.9: " /short_term/images/Bread.jpq" tured after the task of putting bread on the countertop was executed.stores at was cap-</p>
<p>Table 1 :
1
Evaluation of KARMA and baseline for different categories of tasks in ALFRED-L.
MethodsSimple TasksComposite TasksComplex TasksSRMRARERTSRMRARERTSRMRARERTLoTa-Bench(Modified) HELPER(Sarch et al.,2023b)0.41 0.40-0.251-0.2630.23 = 0.21---0.243-0.1780.04 0.09-0.018-0.011CAPEAM(Kim et al., 2023) KARMA0.35 0.42-0.054 0.582-0.002 0.6120.33 0.43-0.930.293 0.9020.201 0.6870.07 0.21-0420.012 0.8670.008 0.690</p>
<p>Table 2 :
2
Ablation Study.
MethodsSimple TasksComposite TasksComplex TasksSR. 041 KARMA(w/o long term memory) -<em> 0.40 LoTa-Bench(Modified) KaRMA(w/o short term memory) 0.44 KARMA 0.42MRA ----RE ; O01 0573 0582 0.612 RT -0.002 0.605SR 0.23 0.35 0.22 043MRA 5 1 -0.93RE 5 0.329 «0.210 RI -0.774 0.624 0.902 0687SR. 0.04 «0.12 0.05 0.21MRA -043 -042RE _ RT ; 0.021</em>-(0.013 0.784 (0.654 0.867 0.690© 046012was used to detect objects, feeding the input intooceLangSAM(Kirillov et al., 2023) for segmentation© oral = 0 .79.10 Sand semantic matching to locate the object to be2 oo &amp; 0.08no &amp;ad eo?70.08 a =! 710.06 3grasped. And then AnyGrasp(Fang et al., 2023) . . tos generates the most suitable grasping position anda fii o.o4to °70.04 B. plans the arm's motion path.oe Q = 0.008Rg a-40.02 &gt; 0.005 Conclusionwy90.000.050.100.150.20Hit RateIn this paper, we explore the potential of enhanc-Fig. 7: The impact of memory hit rate on the agent'stask execution efficiency.
Alibaba Group, Hangzhou, China
Supplementary MaterialA PromptsIn Fig.8, we provide a prompt template that integrates both long-term and short-term memory, specifically designed to enhance the capabilities of LLMs in planning long-sequence tasks.We present the contents stored in short-term (Listingl) during task execution .In Listing], we present the text and image stored in short-term memory after executing the sequential tasks of washing a potato and placing it on the countertop, washing a tomato and placing it on the countertop, putting bread on the countertop, and throwing the knife in the trash.In short-term memory, the "objectId" is a unique identifier for each object that remains constant over time.This identifier is used to determine if the object is the same before and after memory updates.The "position" records the current location of the object after the agent's interaction or the location of objects the agent has encountered during task execution.The "imagePath" stores images of objects captured by the agent, which are used for subsequent analysis by the Vision-Language Model (VLM).In Fig.9, we present the image of bread captured by the agent after executing the task of putting bread on the countertop.This image is stored at " [short_term/images/ Bread.jpq' .Listing 1: The detailed content of short-term memory during task execution.short_term_memory=[ { "objectType":"Tomato", "position": { "x": @.9792354106903076, "vy":1.7150063514709473, "2":-2.606173276901245 },"objectId": "Tomato | -@0.39|]+01.14]-00.81" "imagePath": "/short_term/images /Tomato.jpg" }, { "objectType":"Apple", "position": { "x":1.0981664657592773 , "vy": @.9569252133369446, "2":-2.4071836471557617 }, "objectId": "Apple | -@0.47|+01.15|+00.48""imagePath": "/short_term/images /Apple.jpg" Be { "objectType": "DishSponge", "position": { "x":-1,.8567615747451782, "y": @.14490127563476562, "Zz":-1.619217514991 7603 }, "objectId": "DishSponge | -@1.94|+@0.75|-@1.71""imagePath": ""/short_term/ images /DishSponge.jpg" be { "objectType": "Potato", "position": { "x":1.098166584968567, "vy": @.9390283823013306, "2"s -2.2535505294799805 Be "objectId": "Potato |-@1.66|+00.93]-@2.15""imagePath": "/short_term/images /Potato.jpg" }, { "objectType": "Book", "position": { "x": -1.35060715675354, "y": 1.1669094562530518, "Zz":1.970085859298706 }, "objectId": "Book |+0@.15/+01.10]+00.62""imagePath": "/short_term/ images /Book.jpg" }; { "objectType": "Bread", "position": { "x": @.9692967534065247 , "y": @.9761490225791931, "2": -2.330367088317871Io "objectId": "Bread |-@0.52]+01.17|-00.03""imagePath": "/short_term/ images /Bread.jpg" }, { "objectType":"Knife", "position": { "x": -2.0168256759643555, "y": @.24547088146209717, "2": 2.1725265979766846 }, "objectId": "Knife | -@1.70|+00.79|]-00,.22""imagePath": "/short_term/images /Knife.jpg" }, { "objectType":"Lettuce", "position": { "x":-1.6119909286499023, "y": @.9801480174064636, "Zz":-@.6989647150039673 }, "objectId": "Lettuce |-@1.81]+00.97|-00.94""imagePath": "/short_term/ images /Lettuce.jpg" }CMore Details on ALFRED-L ALFRED-L includes three types of tasks: simple tasks, composite tasks, and complex tasks.These tasks are adapted from the original ALFRED dataset.In ALFRED-L, placing an object inside the fridge was deemed successful when the object is in the fridge.We enhanced this by adding a subgoal "INSIDE(Fridge): 1" to ensure the object is correctly placed inside fridge.For tasks like "wash an apple" in ALFRED-L, the goal conditions involve the apple being rinsed in the sink.The Table3: Task types and samples for each type in the ALFRED-L dataset.place potato on the plate -wash an apple &gt; get a frying pan.Composite Tasks wash a tomato -wash a potato -slice a bread -put the bread in the fridge -place the clean, red food on the plate.Table4: List of language models used in the experiments and their respective roles.Language Model Role FunctionOpenAlI GPT-40 VLM Analyzes the state of objects within the image of short-term memory.OpenAI GPT-4o0 LLM as Planner Task decomposition.OpenAI text-embedding-3-large Embedding Model Recalls memory units.<System Rolo> As an image analysis expert, your task is to infer the state of objects in the image through step-by-step reasoning.<User Role> 1.Provide a detailed description of this image|!mage}.2.From the given task{ Task], extract the relevant content from the first step's image description that pertains to the mentioned objects.3.Based on the object descriptions extracted in the second step, match each object to one of the following states: heated, cooked, sliced, cleaned, dirty, filled, used up, off, on, opened, closed, none.4,Summarize the results from step three in the following format: object: state.Fig.10: The prompt template for GPT-4, utilizing a step-by-step approach to guide VLM in identifying the relevant objects and their corresponding states.Instruction: wash an tomato and place it on the countertop &gt;&gt; find an apple and place it on the countertop =&gt; slice the clean tomato
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Do As I Can. Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Chuyuan Finn, Keerthana Fu, Karol Gopalakrishnan, Hausman, arXiv:2204.01691Grounding Language in Robotic Affordances. Not As I Say2022arXiv preprint</p>
<p>3d scene graph: A structure for unified semantics, 3d space, and camera. Iro Armeni, Zhi-Yang He, Amir Zamir, Junyoung Gwak, Jitendra Malik, Martin Fischer, Silvio Savarese, 2019 IEEE/CVF International Conference on Computer Vision (ICCV). 2019</p>
<p>Tianli Suneel Belkhale, Ted Ding, Pierre Xiao, Quon Sermanet, Jonathan Vuong, Yevgen Tompson, Chebotar, arXiv:2403.01823Debidatta Dwibedi, and Dorsa Sadigh. 2024. Rt-h: Action hierarchies using language. arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Rt-1: Robotics transformer for real-world control at scale. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, arXiv:22122022arXiv preprint</p>
<p>Language Models are Few-shot Learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 2020</p>
<p>Dake Chen, Hanbin Wang, Yunhao Huo, Yuzhao Li, Haoyang Zhang, arXiv:23 10.08067Gamegpt: Multi-agent collaborative framework for game development. 2023aarXiv preprint</p>
<p>Siwei Chen, Anxing Xiao, David Hsu, arXiv:23 11.17406Llm-state: Expandable state representation for longhorizon task planning in the open world. 2023barXiv preprint</p>
<p>Lewis Hao-Tien, Zhuo Chiang, Zipeng Xu, Mithun Fu, George Jacob, Tingnan Zhang, Tsang-Wei Edward Lee, Wenhao Yu, Connor Schenck, David Rendleman, Dhruv Shah, arXiv:2407.07775Mobility vla: Multimodal instruction navigation with long-context vims and topological graphs. 2024arXiv preprint</p>
<p>Jae-Woo Choi, Youngwoo Yoon, Hyobin Ong, Jaehong Kim, Minsu Jang, arXiv:2402.08178Lota-bench: Benchmarking language-oriented task planners for embodied agents. 2024arXiv preprint</p>
<p>From local to global: A graph rag approach to query-focused summarization. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Jonathan Larson, arXiv:2404.161302024arXiv preprint</p>
<p>Tinylfu: A highly efficient cache admission policy. Gil Einziger, Roy Friedman, 201420</p>
<p>22nd Euromicro International Conference on Parallel, Distributed, and Network-Based Processing. </p>
<p>Anygrasp: Robust and efficient grasp perception in spatial and temporal domains. Chenxi Hao-Shu Fang, Hongjie Wang, Minghao Fang, Jirong Gou, Hengxu Liu, Wenhai Yan, Yichen Liu, Cewu Xie, Lu, EEE Transactions on Robotics. 2023</p>
<p>Chen Gao, Xiaochong Lan, Zhihong Lu, Jinzhu Mao, Jinghua Piao, Huandong Wang, Depeng Jin, Yong Li, arXiv:2307.149842023. s*: Social-network simulation system with large language model-empowered agents. arXiv preprint</p>
<p>War and peace (waragent): Large language model-based multi-agent simulation of world wars. Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, Hang Zhao ; Wenyue, Lizhou Hua, Lingyao Fan, Kai Li, Jianchao Mei, Yingqiang Ji, Libby Ge, Yongfeng Hemphill, Zhang, arXiv:2306.03901arX1v:2311.17227Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022. Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents. 2023. 2023arXiv preprintInternational Conference on Machine Learning</p>
<p>Adapt: Action-aware driving caption transformer. Bu Jin, Xinyu Liu, Yupeng Zheng, Pengfei Li, Hao Zhao, Tong Zhang, Yuhang Zheng, Guyue Zhou, Jingjing Liu, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Rap: Retrieval-augmented planning with contextual memory for multimodal Ilm agents. Tomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou, Jayashree Karlekar, Sugiri Pranata, Akira Kinose, Koki Oguri, Felix Wick, Yang You, arXiv:2402.036102024arXiv preprint</p>
<p>Zhao Kaiya, Michelangelo Naim, Jovana Kondic, Manuel Cortes, Jiaxin Ge, Shuying Luo, Guangyu Robert Yang, Andrew Ahn, arXiv:2310.02172Lyfe agents: Generative agents for low-cost real-time social interactions. 2023arXiv preprint</p>
<p>Shyam Sundar Kannan, L N Vishnunandan, Byung-Cheol Venkatesh, Min, arXiv:2309.10062Smart-llm: Smart multi-agent robot task planning using large language models. 2023arXiv preprint</p>
<p>Context-aware planning and environment-aware memory for instruction following embodied agents. Byeonghwi Kim, Jinyeon Kim, Yuyeong Kim, Cheolhong Min, Jonghyun Choi, Proceedings of the LEEE/CVF International Conference on Computer Vision. the LEEE/CVF International Conference on Computer Vision2023</p>
<p>Segment anything. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Van-Derbilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, Ali Farhadi, arXivAI2-THOR: An Interactive 3D Environment for Visual AI. 2017</p>
<p>Prompted Ilms as chatbot modules for long open-domain conversation. Gibbeum Lee, Jongho Volker Hartmann, Dimitris Park, Kangwook Papailiopoulos, Lee, arXiv:2305.045332023arXiv preprint</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kiittler, Mike Lewis, Wen-Tau Yih, Tim Rocktaschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Cheng Li, Ziang Leng, Chenxi Yan, Junyi Shen, Hao Wang, Weishi Mi, Yaying Fei, Xiaoyang Feng, Song Yan, Haosheng Wang, arXiv:2308.09597Chatharuhi: Reviving anime character in reality via large language model. 2023aarXiv preprint</p>
<p>Yuan Li, Yixuan Zhang, Lichao Sun, arXiv:2310.06500Metaagents: Simulating interactions of human behavyiors for llm-based task-oriented coordination via collaborative generative agents. 2023barXiv preprint</p>
<p>Unleashing infinite-length input capacity for largescale language models with self-controlled memory system. Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, Zhoujun Li, arXiv:2304.133432023arXiv preprint</p>
<p>Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhigiang Zhang, Jinjie Gu, Guannan Zhang, arXiv:2311.08719Thinkin-memory: Recalling and post-thinking enable ms with long-term memory. 2023arXiv preprint</p>
<p>Memochat: Tuning Ilms to use memos for consistent long-range open-domain conversation. Junru Lu, Siyu An, Mingbao Lin, Gabriele Pergola, Yulan He, Di Yin, Xing Sun, Yunsheng Wu, arXiv:2308.082392023arXiv preprint</p>
<p>Optimizing bloom filter: Challenges, solutions, and comparisons. Lailong Luo, Deke Guo, Richard Ma, Ori Rottenstreich, Xueshan Luo, 2018JEEE Communications Surveys &amp; Tutorials</p>
<p>Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, Graham Neubig, arXtv:2210.07128Language Models of Code are Few-shot Commonsense Learners. 2022arXiv preprint</p>
<p>Generation-augmented retrieval for open-domain question answering. Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, Weizhu Chen, arXiv:2009.085532020arXiv preprint</p>
<p>Ret-IIm: Towards a general read-write memory for large language models. Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, Hinrich Schiitze, arXiv:2305.143222023arXiv preprint</p>
<p>Charles Packer, Vivian Fang, G Shishir, Kevin Patil, Sarah Lin, Joseph E Wooders, Gonzalez, arXtv:23 10.08560Memegpt: Towards Ilms as operating systems. 2023arXiv preprint</p>
<p>Open-ended instructable embodied agents with memory-augmented large language models. Fragkiadaki, arXiv:2310.151272023aarXiv preprint</p>
<p>Open-ended instructable embodied agents with memory-augmented large language models. Gabriel Sarch, Yue Wu, Michael J Tarr, Katerina Fragkiadaki, arXiv:2310.151272023barXiv preprint</p>
<p>Yunfan Shao, Linyang Li, Junqi Dai, Xipeng Qiu, arXiv:2310.10158Character-llm: A trainable agent for roleplaying. 2023arXiv preprint</p>
<p>ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Coté, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021</p>
<p>Wenhao Sun, Sai Hou, Zixuan Wang, Bo Yu, Shaoshan Liu, Xu Yang, Shuai Liang, Yiming Gan, Yinhe Han, arXiv:2412.01663Dadu-e: Rethinking the role of large language model in robotic computing pipeline. 2024arXiv preprint</p>
<p>Rtlfixer: Automatically fixing rtl syntax errors with large language models. Yunda Tsai, Mingjie Liu, Haoxing Ren, arXiv:2311.165432023arXiv preprint</p>
<p>Sai Vemprala, Rogerio Bonatti, Arthur Bucker, Ashish Kapoor, ChatGPT for Robotics: Design Principles and Model Abilities. 2023</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, arXiv:2305.16291arXiv:2304.06975Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting Liu. 2023b. Huatuo: Tuning llama model with chinese medical knowledge. 2023aarXiv preprintLinxi Fan, and Anima Anandkumar</p>
<p>Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai Tang, Zeyu Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, ArXiv:2306.02552c. when large language model based agent meets user behavior analysis: A novel user simulation paradigm. 2023carXiv preprint</p>
<p>Recmind: Large language model powered agent for recommendation. Yancheng Wang, Ziyan Jiang, Zheng Chen, Fan Yang, Yingxue Zhou, Eunah Cho, Xing Fan, Xiaojiang Huang, Yanbin Lu, Yingzhen Yang, arXiv:2308.142962023darXiv preprint</p>
<p>Zekun Moore, Wang , Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, arXiv:2310.00746arXiv:23 11.05997Zhaofeng He, Zilong Zheng, Yaodong Yang, et al. 2023f. Jarvis-1: Open-world multi-task agents with memoryaugmented multimodal language models. 2023earXiv preprintRolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models</p>
<p>TidyBot: Personalized Robot Assistance with Large Language Models. Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, Thomas Funkhouser, Autonomous Robots. 2023a</p>
<p>Autogen: Enabling next-gen Ilm applications via multiagent conversation framework. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, Chi Wang, arXiv: 2308.08 1552023barXiv preprint</p>
<p>Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, Haibin Yan, arXiv:2307.01848Embodied task planning with large language models. 2023carXiv preprint</p>
<p>Ming Yan, Ruihao Li, Hao Zhang, Hao Wang, Zhilan Yang, Ji Yan, arXiv:2312.17653Larp: Language-agent role play for open-world games. 2023arXiv preprint</p>
<p>Yi Yang, Yixuan Tang, Kar Yan, Tam , arXiv:2309.13064vestlm: A large language model for investment using financial domain instruction tuning. 2023arXiv preprint</p>
<p>Mc-gpt: Empowering vision-and-language navigation with memory map and reasoning chains. Zhaohuan Zhan, Lisha Yu, Sijie Yu, Guang Tan, arXiv:2405.106202024arXiv preprint</p>
<p>Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B Tenenbaum, Tianmin Shu, Chuang Gan, arXiv:2307.02485Building cooperative embodied agents modularly with large language models. 2023arXiv preprint</p>
<p>Agentcf: Collaborative learning with autonomous language agents for recommender systems. Junjie Zhang, Yupeng Hou, Ruobing Xie, Wenqi Sun, Julian Mcauley, Wayne Xin Zhao, Leyu Lin, Ji-Rong Wen, Proceedings of the ACM on Web Conference 2024. the ACM on Web Conference 20242024a</p>
<p>Codeagent: Enhancing code generation with tool-integrated agent systems for realworld repo-level coding challenges. Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, Zhi Jin, arXiv:2401.073392024barXiv preprint</p>
<p>Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, arXiv:2402.19473Wentao Zhang, and Bin Cui. 2024a._ Retrievalaugmented generation for ai-generated content: A survey. arXiv preprint</p>
<p>Runcong Zhao, Wenjia Zhang, Jiazheng Li, Lixing Zhu, Yanran Li, Yulan He, Lin Gui, arXiv:2310.01459Narrativeplay: Interactive narrative understanding. 2023arXiv preprint</p>
<p>Zihan Zhao, Da Ma, Lu Chen, Liangtai Sun, Zihao Li, Hongshen Xu, Zichen Zhu, Su Zhu, Shuai Fan, Guodong Shen, arXiv:2401.14818Chemdfm: Dialogue foundation model for chemistry. 2024barXiv preprint</p>
<p>Memorybank: Enhancing large language models with long-term memory. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, Yanlin Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Characterglm: Customizing chinese conversational ai characters with large language models. Jinfeng Zhou, Zhuang Chen, Dazhen Wan, Bosi Wen, Yi Song, Jifan Yu, Yongkang Huang, Libiao Peng, Jiaming Yang, Xiyao Xiao, arXiv:2311.168322023arXiv preprint</p>
<p>Xizhou Zhu, Yuntao Chen, Chenxin Hao Tian, Weijie Tao, Chenyu Su, Gao Yang, Bin Huang, Lewei Li, Xiaogang Lu, Wang, arXiv:2305.17144Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6460 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6460</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6460</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-277150776</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.16416v1.pdf" target="_blank">Survey on Evaluation of LLM-based Agents</a></p>
                <p><strong>Paper Abstract:</strong> The emergence of LLM-based agents represents a paradigm shift in AI, enabling autonomous systems to plan, reason, use tools, and maintain memory while interacting with dynamic environments. This paper provides the first comprehensive survey of evaluation methodologies for these increasingly capable agents. We systematically analyze evaluation benchmarks and frameworks across four critical dimensions: (1) fundamental agent capabilities, including planning, tool use, self-reflection, and memory; (2) application-specific benchmarks for web, software engineering, scientific, and conversational agents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating agents. Our analysis reveals emerging trends, including a shift toward more realistic, challenging evaluations with continuously updated benchmarks. We also identify critical gaps that future research must address-particularly in assessing cost-efficiency, safety, and robustness, and in developing fine-grained, and scalable evaluation methods. This survey maps the rapidly evolving landscape of agent evaluation, reveals the emerging trends in the field, identifies current limitations, and proposes directions for future research.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6460.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6460.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReadAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReadAgent (human-inspired reading agent with gist memory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reading agent that groups long documents into episodes, condenses episodes into gist memories, and retrieves relevant passages to answer long-context queries and summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A human-inspired reading agent with gist memory of very long contexts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReadAgent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Structures reading by grouping content into episodes, condensing each episode into a compact 'gist' memory, and retrieving passages from those memories to support long-context question answering and summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic / gist memory (condensed episodic summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>condensed textual episode summaries (gist), and retrieved passages</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>passage retrieval from condensed episode memories (retrieval over stored episode summaries / passages)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>QUALITY; NarrativeQA; QMSum</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>long-context reading / document question answering / summarization</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Designed to improve long-context handling; specific trade-offs (latency, storage) are not quantified in the survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No detailed failure modes or numeric degradation reported in the survey; evaluation primarily demonstrates improved handling of very long inputs but no quantitative comparisons provided in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Lee et al., 2024. A human-inspired reading agent with gist memory of very long contexts. arXiv preprint (as cited in this survey).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Survey on Evaluation of LLM-based Agents', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6460.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6460.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent architecture that manages a tiered memory system to support multi-session interactions and open-domain question answering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MemGPT: Towards LLMs as operating systems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Manages a tiered memory hierarchy (multiple memory tiers) to store and retrieve information across sessions, enabling multi-session chat and open-domain QA by reading/writing to different memory tiers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>tiered memory system (multi-tier long/short caches)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>tiered memories containing session records and retrieved textual passages; likely embeddings + raw text (survey summary)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>managed tiered read/write (explicit memory management across tiers with selective retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NaturalQuestions-Open; multi-session chat datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>open-domain QA; multi-session conversational memory</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Survey notes MemGPT uses tiered memory to handle multi-session state; specific trade-offs (latency, storage, cost) are not enumerated in the survey excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No specific failure cases reported in the survey; detailed empirical numbers not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Packer et al., 2024. MemGPT: Towards LLMs as operating systems. arXiv preprint (as cited in this survey).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Survey on Evaluation of LLM-based Agents', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6460.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6460.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>A-MEM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A-MEM (Agentic Memory for LLM agents)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An advanced agentic memory architecture intended to support persistent memory for LLM-based agents and evaluated on long-term memory benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A-mem: Agentic memory for llm agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>A-MEM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Introduces an advanced memory architecture for agents (agentic memory) aimed at persistent storage and retrieval to support reasoning across long interactions; evaluated using LoCoMo/related long-memory benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>agentic long-term memory (architectural long-term memory module)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>not specified in survey (likely structured episodic/semantic memories or embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>not specified in survey (presumably retrieval-based access with controllers to write/read episodes)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Lo-CoMo (LoCoMo) benchmark / long-term conversational memory evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>long-term conversational memory / retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Survey notes A-MEM is an advanced memory architecture but does not report quantitative trade-offs in this summary.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Survey does not include detailed failure cases for A-MEM; full details likely in original paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Xu et al., 2025. A-mem: Agentic memory for llm agents. arXiv preprint (as cited in this survey).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Survey on Evaluation of LLM-based Agents', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6460.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6460.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAISE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RAISE (ReAct + memory enhancement)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of the ReAct framework that augments agent reasoning with a two-part memory system to improve quality and efficiency as judged by human evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAISE</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Enhances the ReAct agent framework with a two-part memory system (presumably short- and long-term components) to store intermediate reasoning traces and past interactions for reuse.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>two-part memory (short-term + long-term hybrid)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>not specified in the survey summary (likely reasoning traces / episodic entries)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>not specified in the survey summary (described as memory-enhanced ReAct with retrieval and storage components)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Human-judged quality and efficiency evaluations (no single named benchmark specified in survey excerpt)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>multi-step reasoning / ReAct-style interactive reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>human judgment (quality) and efficiency metrics (as reported by authors)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Survey indicates evaluation used human judgments and efficiency metrics; explicit trade-offs (latency, memory footprint) are not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Survey does not enumerate detailed failure cases; original paper likely contains empirical results and limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Survey on Evaluation of LLM-based Agents', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6460.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6460.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion (Language agents with verbal reinforcement learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agentic approach where the agent reflects on past attempts (verbal reinforcement) to improve future actions; used to track success rate on tasks over time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses verbal self-reflection (recording outcomes and lessons) to adapt behavior across trials; can act as a form of memory by keeping and using past attempt summaries to guide future actions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>reflective episodic memory (history of attempts, lessons learned)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>textual summaries of past attempts and outcomes (verbal reflections)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>explicit recall of past reflections (agent reads its previous reflections and uses them as context for subsequent attempts)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotPotQA; ALFWorld (examples of tasks tracked for success rate)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>multi-hop QA; interactive embodied/text game tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate (as tracked by authors on given tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Survey notes Reflexion tracks success rate; no explicit trade-offs (latency/memory cost) are given in the summary.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Survey references broader literature noting that LLMs cannot fully self-correct reasoning yet; Reflexion's improvements are task-dependent and not universally quantified in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Shinn et al., 2023. Reflexion: language agents with verbal reinforcement learning. NeurIPS 2023 (as cited in this survey).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Survey on Evaluation of LLM-based Agents', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6460.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6460.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KARMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KARMA (Augmenting embodied AI agents with long-and-short term memory systems)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embodied-agent memory system that augments agents with explicit long- and short-term memories to improve task success in household / embodied environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KARMA</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Augments embodied agents with a combined long-term and short-term memory subsystem; evaluates memory impact on success rate, retrieval accuracy, and memory hit rate in household tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>hybrid long-term + short-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>not fully specified in survey (likely state/action traces, observations, and episodic records)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>retrieval-based access with metrics tracking memory hit rate and retrieval accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Household task suite (embodied tasks) as used in KARMA evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>embodied agent tasks / action optimization</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate; retrieval accuracy; memory hit rate</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Survey notes KARMA reports improved performance with memory; specific trade-offs (compute, latency) are not enumerated in this summary.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Survey does not detail failure cases; original paper likely discusses scenarios where retrieval or memory mistakes reduce performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Wang et al., 2024b. KARMA: Augmenting embodied AI agents with long-and-short term memory systems. arXiv preprint (as cited in this survey).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Survey on Evaluation of LLM-based Agents', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6460.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6460.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LTMbenchmark</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LTMbenchmark (Long-Term Memory benchmark for conversational agents)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark for evaluating conversational agents' long-term memory, multitask interactions and context switching capabilities, testing integration of long-term memories with smaller-context models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LTMbenchmark (as evaluation benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Evaluates conversational agents on extended, multitask conversations with frequent context switching to test long-term memory and information integration; highlights comparisons between long-term memory systems and large-context LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>long-term memory systems integrated with conversational agent</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>not specified in detail in survey (likely episodic/semantic memory stores or vector indices)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>retrieval-based long-term memory access; tested in combination with short-context LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Extended multi-session conversational tasks with frequent context switching (LTMbenchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>conversational long-term memory / multitask dialogue</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Survey reports that short-context LLMs equipped with long-term memory systems can match or exceed the performance of models with larger context windows (qualitative claim, no numeric values given).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Survey highlights that integrating long-term memory can allow smaller-context models to close the gap vs large-context models; explicit runtime/memory costs are not quantified in the summary.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Survey notes models still struggle with interleaved tasks; while long-term memory helps, interleaved/multi-task scenarios remain challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Castillo-Bolado et al., 2024a. LTMbenchmark (as cited in this survey).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Survey on Evaluation of LLM-based Agents', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6460.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6460.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StreamBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>StreamBench (benchmark for continuous improvement using external memory components)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark that evaluates agents' ability to continuously improve by leveraging external memory of previous interactions and external feedback across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>StreamBench (benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Evaluates agents that use external memory components (memory of previous interactions and feedback) to continuously improve performance over time across diverse datasets including text-to-SQL and multi-hop QA.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external memory components (interaction history + feedback stores)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>stored previous interactions, feedback entries, and retrieved past states (format not further specified in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>retrieval of past interactions/feedback to inform future behavior; continuous update as new interactions occur</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Text-to-SQL (e.g., Spider); ToolBench; HotpotQA (examples listed in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>continuous learning / retrieval-enhanced multi-turn tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>quality and efficiency (benchmarks assess continuous improvement and efficiency)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>StreamBench evaluates quality and efficiency trade-offs for continuous memory usage; survey does not provide quantified trade-offs here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Survey does not list explicit failure cases; continuous improvement is a challenging setting and may reveal issues with robustness and drift (implied).</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Wu et al., 2024a. StreamBench (as cited in this survey).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Survey on Evaluation of LLM-based Agents', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6460.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6460.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LoCoMo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LoCoMo (Lo‑CoMo / Long‑term Conversational Memory benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark for evaluating very long-term conversational memory of LLM agents across extended dialogues and sessions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating very long-term conversational memory of llm agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LoCoMo</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Benchmark focusing on very long-term conversational memory; used to evaluate agents' ability to remember and integrate information across long multi-session dialogues.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>long-term conversational memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>episodic conversational records / memory entries (not fully specified in the survey)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>retrieval from stored conversational memory across sessions</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Lo-CoMo benchmark (evaluating very long-term conversational memory)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>conversational long-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Survey mentions LoCoMo is used to evaluate memory architectures (A-MEM uses LoCoMo) but provides no numeric trade-offs here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Survey does not provide detailed failure cases; long-horizon conversational memory remains an open challenge according to the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Maharana et al., 2024. Evaluating very long-term conversational memory of LLM agents. arXiv preprint (as cited in this survey).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Survey on Evaluation of LLM-based Agents', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A human-inspired reading agent with gist memory of very long contexts <em>(Rating: 2)</em></li>
                <li>MemGPT: Towards LLMs as operating systems <em>(Rating: 2)</em></li>
                <li>A-mem: Agentic memory for llm agents <em>(Rating: 2)</em></li>
                <li>Evaluating very long-term conversational memory of llm agents <em>(Rating: 2)</em></li>
                <li>RAISE <em>(Rating: 1)</em></li>
                <li>Reflexion: language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>KARMA: Augmenting embodied AI agents with long-and-short term memory systems <em>(Rating: 2)</em></li>
                <li>LTMbenchmark <em>(Rating: 2)</em></li>
                <li>StreamBench <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6460",
    "paper_id": "paper-277150776",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "ReadAgent",
            "name_full": "ReadAgent (human-inspired reading agent with gist memory)",
            "brief_description": "A reading agent that groups long documents into episodes, condenses episodes into gist memories, and retrieves relevant passages to answer long-context queries and summaries.",
            "citation_title": "A human-inspired reading agent with gist memory of very long contexts",
            "mention_or_use": "mention",
            "agent_name": "ReadAgent",
            "agent_description": "Structures reading by grouping content into episodes, condensing each episode into a compact 'gist' memory, and retrieving passages from those memories to support long-context question answering and summarization.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "episodic / gist memory (condensed episodic summaries)",
            "memory_representation": "condensed textual episode summaries (gist), and retrieved passages",
            "memory_access_mechanism": "passage retrieval from condensed episode memories (retrieval over stored episode summaries / passages)",
            "task_name": "QUALITY; NarrativeQA; QMSum",
            "task_category": "long-context reading / document question answering / summarization",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Designed to improve long-context handling; specific trade-offs (latency, storage) are not quantified in the survey summary.",
            "limitations_or_failure_cases": "No detailed failure modes or numeric degradation reported in the survey; evaluation primarily demonstrates improved handling of very long inputs but no quantitative comparisons provided in this survey.",
            "citation": "Lee et al., 2024. A human-inspired reading agent with gist memory of very long contexts. arXiv preprint (as cited in this survey).",
            "uuid": "e6460.0",
            "source_info": {
                "paper_title": "Survey on Evaluation of LLM-based Agents",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "MemGPT",
            "name_full": "MemGPT",
            "brief_description": "An agent architecture that manages a tiered memory system to support multi-session interactions and open-domain question answering.",
            "citation_title": "MemGPT: Towards LLMs as operating systems",
            "mention_or_use": "mention",
            "agent_name": "MemGPT",
            "agent_description": "Manages a tiered memory hierarchy (multiple memory tiers) to store and retrieve information across sessions, enabling multi-session chat and open-domain QA by reading/writing to different memory tiers.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "tiered memory system (multi-tier long/short caches)",
            "memory_representation": "tiered memories containing session records and retrieved textual passages; likely embeddings + raw text (survey summary)",
            "memory_access_mechanism": "managed tiered read/write (explicit memory management across tiers with selective retrieval)",
            "task_name": "NaturalQuestions-Open; multi-session chat datasets",
            "task_category": "open-domain QA; multi-session conversational memory",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Survey notes MemGPT uses tiered memory to handle multi-session state; specific trade-offs (latency, storage, cost) are not enumerated in the survey excerpt.",
            "limitations_or_failure_cases": "No specific failure cases reported in the survey; detailed empirical numbers not provided here.",
            "citation": "Packer et al., 2024. MemGPT: Towards LLMs as operating systems. arXiv preprint (as cited in this survey).",
            "uuid": "e6460.1",
            "source_info": {
                "paper_title": "Survey on Evaluation of LLM-based Agents",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "A-MEM",
            "name_full": "A-MEM (Agentic Memory for LLM agents)",
            "brief_description": "An advanced agentic memory architecture intended to support persistent memory for LLM-based agents and evaluated on long-term memory benchmarks.",
            "citation_title": "A-mem: Agentic memory for llm agents",
            "mention_or_use": "mention",
            "agent_name": "A-MEM",
            "agent_description": "Introduces an advanced memory architecture for agents (agentic memory) aimed at persistent storage and retrieval to support reasoning across long interactions; evaluated using LoCoMo/related long-memory benchmarks.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "agentic long-term memory (architectural long-term memory module)",
            "memory_representation": "not specified in survey (likely structured episodic/semantic memories or embeddings)",
            "memory_access_mechanism": "not specified in survey (presumably retrieval-based access with controllers to write/read episodes)",
            "task_name": "Lo-CoMo (LoCoMo) benchmark / long-term conversational memory evaluations",
            "task_category": "long-term conversational memory / retrieval",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Survey notes A-MEM is an advanced memory architecture but does not report quantitative trade-offs in this summary.",
            "limitations_or_failure_cases": "Survey does not include detailed failure cases for A-MEM; full details likely in original paper.",
            "citation": "Xu et al., 2025. A-mem: Agentic memory for llm agents. arXiv preprint (as cited in this survey).",
            "uuid": "e6460.2",
            "source_info": {
                "paper_title": "Survey on Evaluation of LLM-based Agents",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "RAISE",
            "name_full": "RAISE (ReAct + memory enhancement)",
            "brief_description": "An extension of the ReAct framework that augments agent reasoning with a two-part memory system to improve quality and efficiency as judged by human evaluators.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "RAISE",
            "agent_description": "Enhances the ReAct agent framework with a two-part memory system (presumably short- and long-term components) to store intermediate reasoning traces and past interactions for reuse.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "two-part memory (short-term + long-term hybrid)",
            "memory_representation": "not specified in the survey summary (likely reasoning traces / episodic entries)",
            "memory_access_mechanism": "not specified in the survey summary (described as memory-enhanced ReAct with retrieval and storage components)",
            "task_name": "Human-judged quality and efficiency evaluations (no single named benchmark specified in survey excerpt)",
            "task_category": "multi-step reasoning / ReAct-style interactive reasoning",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": true,
            "performance_metric": "human judgment (quality) and efficiency metrics (as reported by authors)",
            "tradeoffs_reported": "Survey indicates evaluation used human judgments and efficiency metrics; explicit trade-offs (latency, memory footprint) are not detailed here.",
            "limitations_or_failure_cases": "Survey does not enumerate detailed failure cases; original paper likely contains empirical results and limitations.",
            "citation": "",
            "uuid": "e6460.3",
            "source_info": {
                "paper_title": "Survey on Evaluation of LLM-based Agents",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion (Language agents with verbal reinforcement learning)",
            "brief_description": "An agentic approach where the agent reflects on past attempts (verbal reinforcement) to improve future actions; used to track success rate on tasks over time.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "agent_name": "Reflexion",
            "agent_description": "Uses verbal self-reflection (recording outcomes and lessons) to adapt behavior across trials; can act as a form of memory by keeping and using past attempt summaries to guide future actions.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "reflective episodic memory (history of attempts, lessons learned)",
            "memory_representation": "textual summaries of past attempts and outcomes (verbal reflections)",
            "memory_access_mechanism": "explicit recall of past reflections (agent reads its previous reflections and uses them as context for subsequent attempts)",
            "task_name": "HotPotQA; ALFWorld (examples of tasks tracked for success rate)",
            "task_category": "multi-hop QA; interactive embodied/text game tasks",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "success rate (as tracked by authors on given tasks)",
            "tradeoffs_reported": "Survey notes Reflexion tracks success rate; no explicit trade-offs (latency/memory cost) are given in the summary.",
            "limitations_or_failure_cases": "Survey references broader literature noting that LLMs cannot fully self-correct reasoning yet; Reflexion's improvements are task-dependent and not universally quantified in this survey.",
            "citation": "Shinn et al., 2023. Reflexion: language agents with verbal reinforcement learning. NeurIPS 2023 (as cited in this survey).",
            "uuid": "e6460.4",
            "source_info": {
                "paper_title": "Survey on Evaluation of LLM-based Agents",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "KARMA",
            "name_full": "KARMA (Augmenting embodied AI agents with long-and-short term memory systems)",
            "brief_description": "An embodied-agent memory system that augments agents with explicit long- and short-term memories to improve task success in household / embodied environments.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "KARMA",
            "agent_description": "Augments embodied agents with a combined long-term and short-term memory subsystem; evaluates memory impact on success rate, retrieval accuracy, and memory hit rate in household tasks.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "hybrid long-term + short-term memory",
            "memory_representation": "not fully specified in survey (likely state/action traces, observations, and episodic records)",
            "memory_access_mechanism": "retrieval-based access with metrics tracking memory hit rate and retrieval accuracy",
            "task_name": "Household task suite (embodied tasks) as used in KARMA evaluations",
            "task_category": "embodied agent tasks / action optimization",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": true,
            "performance_metric": "success rate; retrieval accuracy; memory hit rate",
            "tradeoffs_reported": "Survey notes KARMA reports improved performance with memory; specific trade-offs (compute, latency) are not enumerated in this summary.",
            "limitations_or_failure_cases": "Survey does not detail failure cases; original paper likely discusses scenarios where retrieval or memory mistakes reduce performance.",
            "citation": "Wang et al., 2024b. KARMA: Augmenting embodied AI agents with long-and-short term memory systems. arXiv preprint (as cited in this survey).",
            "uuid": "e6460.5",
            "source_info": {
                "paper_title": "Survey on Evaluation of LLM-based Agents",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "LTMbenchmark",
            "name_full": "LTMbenchmark (Long-Term Memory benchmark for conversational agents)",
            "brief_description": "A benchmark for evaluating conversational agents' long-term memory, multitask interactions and context switching capabilities, testing integration of long-term memories with smaller-context models.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "LTMbenchmark (as evaluation benchmark)",
            "agent_description": "Evaluates conversational agents on extended, multitask conversations with frequent context switching to test long-term memory and information integration; highlights comparisons between long-term memory systems and large-context LLMs.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "long-term memory systems integrated with conversational agent",
            "memory_representation": "not specified in detail in survey (likely episodic/semantic memory stores or vector indices)",
            "memory_access_mechanism": "retrieval-based long-term memory access; tested in combination with short-context LLMs",
            "task_name": "Extended multi-session conversational tasks with frequent context switching (LTMbenchmark)",
            "task_category": "conversational long-term memory / multitask dialogue",
            "performance_with_memory": "Survey reports that short-context LLMs equipped with long-term memory systems can match or exceed the performance of models with larger context windows (qualitative claim, no numeric values given).",
            "performance_without_memory": null,
            "has_comparative_results": true,
            "performance_metric": null,
            "tradeoffs_reported": "Survey highlights that integrating long-term memory can allow smaller-context models to close the gap vs large-context models; explicit runtime/memory costs are not quantified in the summary.",
            "limitations_or_failure_cases": "Survey notes models still struggle with interleaved tasks; while long-term memory helps, interleaved/multi-task scenarios remain challenging.",
            "citation": "Castillo-Bolado et al., 2024a. LTMbenchmark (as cited in this survey).",
            "uuid": "e6460.6",
            "source_info": {
                "paper_title": "Survey on Evaluation of LLM-based Agents",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "StreamBench",
            "name_full": "StreamBench (benchmark for continuous improvement using external memory components)",
            "brief_description": "A benchmark that evaluates agents' ability to continuously improve by leveraging external memory of previous interactions and external feedback across tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "StreamBench (benchmark)",
            "agent_description": "Evaluates agents that use external memory components (memory of previous interactions and feedback) to continuously improve performance over time across diverse datasets including text-to-SQL and multi-hop QA.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "external memory components (interaction history + feedback stores)",
            "memory_representation": "stored previous interactions, feedback entries, and retrieved past states (format not further specified in survey)",
            "memory_access_mechanism": "retrieval of past interactions/feedback to inform future behavior; continuous update as new interactions occur",
            "task_name": "Text-to-SQL (e.g., Spider); ToolBench; HotpotQA (examples listed in survey)",
            "task_category": "continuous learning / retrieval-enhanced multi-turn tasks",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "quality and efficiency (benchmarks assess continuous improvement and efficiency)",
            "tradeoffs_reported": "StreamBench evaluates quality and efficiency trade-offs for continuous memory usage; survey does not provide quantified trade-offs here.",
            "limitations_or_failure_cases": "Survey does not list explicit failure cases; continuous improvement is a challenging setting and may reveal issues with robustness and drift (implied).",
            "citation": "Wu et al., 2024a. StreamBench (as cited in this survey).",
            "uuid": "e6460.7",
            "source_info": {
                "paper_title": "Survey on Evaluation of LLM-based Agents",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "LoCoMo",
            "name_full": "LoCoMo (Lo‑CoMo / Long‑term Conversational Memory benchmark)",
            "brief_description": "A benchmark for evaluating very long-term conversational memory of LLM agents across extended dialogues and sessions.",
            "citation_title": "Evaluating very long-term conversational memory of llm agents",
            "mention_or_use": "mention",
            "agent_name": "LoCoMo",
            "agent_description": "Benchmark focusing on very long-term conversational memory; used to evaluate agents' ability to remember and integrate information across long multi-session dialogues.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "long-term conversational memory",
            "memory_representation": "episodic conversational records / memory entries (not fully specified in the survey)",
            "memory_access_mechanism": "retrieval from stored conversational memory across sessions",
            "task_name": "Lo-CoMo benchmark (evaluating very long-term conversational memory)",
            "task_category": "conversational long-term memory",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Survey mentions LoCoMo is used to evaluate memory architectures (A-MEM uses LoCoMo) but provides no numeric trade-offs here.",
            "limitations_or_failure_cases": "Survey does not provide detailed failure cases; long-horizon conversational memory remains an open challenge according to the survey.",
            "citation": "Maharana et al., 2024. Evaluating very long-term conversational memory of LLM agents. arXiv preprint (as cited in this survey).",
            "uuid": "e6460.8",
            "source_info": {
                "paper_title": "Survey on Evaluation of LLM-based Agents",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A human-inspired reading agent with gist memory of very long contexts",
            "rating": 2,
            "sanitized_title": "a_humaninspired_reading_agent_with_gist_memory_of_very_long_contexts"
        },
        {
            "paper_title": "MemGPT: Towards LLMs as operating systems",
            "rating": 2,
            "sanitized_title": "memgpt_towards_llms_as_operating_systems"
        },
        {
            "paper_title": "A-mem: Agentic memory for llm agents",
            "rating": 2,
            "sanitized_title": "amem_agentic_memory_for_llm_agents"
        },
        {
            "paper_title": "Evaluating very long-term conversational memory of llm agents",
            "rating": 2,
            "sanitized_title": "evaluating_very_longterm_conversational_memory_of_llm_agents"
        },
        {
            "paper_title": "RAISE",
            "rating": 1
        },
        {
            "paper_title": "Reflexion: language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "KARMA: Augmenting embodied AI agents with long-and-short term memory systems",
            "rating": 2,
            "sanitized_title": "karma_augmenting_embodied_ai_agents_with_longandshort_term_memory_systems"
        },
        {
            "paper_title": "LTMbenchmark",
            "rating": 2,
            "sanitized_title": "ltmbenchmark"
        },
        {
            "paper_title": "StreamBench",
            "rating": 1,
            "sanitized_title": "streambench"
        }
    ],
    "cost": 0.0200165,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Survey on Evaluation of LLM-based Agents
20 Mar 2025</p>
<p>Asaf Yehudai asaf.yehudai@ibm.com 
The Hebrew University of Jerusalem</p>
<p>IBM Research</p>
<p>Alan Li 
IBM Research</p>
<p>Guy Uziel guy.uziel1@ibm.com 
IBM Research</p>
<p>Yale University</p>
<p>Yilun Zhao yilun.zhao@yale.edu 
Yale University</p>
<p>Roy Bar-Haim 
IBM Research</p>
<p>Arman Cohan arman.cohan@yale.edu 
Yale University</p>
<p>Michal Shmueli-Scheuer 
IBM Research</p>
<p>Survey on Evaluation of LLM-based Agents
20 Mar 20252BC3E9CF94CCB2E3F754A11551B38112arXiv:2503.16416v1[cs.AI]
The emergence of LLM-based agents represents a paradigm shift in AI, enabling autonomous systems to plan, reason, use tools, and maintain memory while interacting with dynamic environments.This paper provides the first comprehensive survey of evaluation methodologies for these increasingly capable agents.We systematically analyze evaluation benchmarks and frameworks across four critical dimensions: (1) fundamental agent capabilities, including planning, tool use, self-reflection, and memory; (2) applicationspecific benchmarks for web, software engineering, scientific, and conversational agents;(3) benchmarks for generalist agents; and (4) frameworks for evaluating agents.Our analysis reveals emerging trends, including a shift toward more realistic, challenging evaluations with continuously updated benchmarks.We also identify critical gaps that future research must address-particularly in assessing costefficiency, safety, and robustness, and in developing fine-grained, and scalable evaluation methods.This survey maps the rapidly evolving landscape of agent evaluation, reveals the emerging trends in the field, identifies current limitations, and proposes directions for future research.</p>
<p>Introduction</p>
<p>Recent years saw a huge leap in the ability of Large Language Models (LLMs) to address a wide range of challenging tasks.Yet, LLMs are static models that are restricted to single-turn, text-to-text interactions.LLM-based agents, hereinafter also referred to as LLM agents, take the power of LLMs a step further by integrating them into a multi-step flow, while maintaining a state that is shared by multiple LLM calls, providing context and consistency.They also utilize external tools to perform computations, access external knowledge and interact with their environment.Agents are able to autonomously conceive, execute and adapt complex plans in real-world environments.This newfound agency empowers them to address problems previously beyond the reach of AI, paving the way for innovative applications across a wide spectrum of domains.</p>
<p>Reliable evaluation of agents is critical to ensure their efficacy in real-world applications, and to guide further progress in this rapidly evolving field.Since agents are sometimes applied to "classical" text-to-text AI tasks, there is some overlap between their evaluation and standard LLM benchmarking.However, as their applicability is much broader, they require new types of evaluation methodologies, benchmarks, environments and metrics.The very characteristics that define LLM-based agents -their reliance on specific LLM abilities, their sequential operation within dynamic environments, and their capacity to undertake diverse, intricate tasks -introduce novel challenges for their evaluation. 1his survey provides the first comprehensive mapping of LLM-based agent evaluation, serving four key audiences: (1) LLM agent developers assessing the capabilities of their systems, (2) practitioners deploying agents in domain-specific applications, (3) benchmark developers addressing evaluation challenges, (4) AI researchers broadly studying current capabilities, risks and limitations of agents.</p>
<p>We begin by discussing the evaluation of fundamental agentic capabilities ( §2), which are commonly employed by agents across different domains and applications.These include planning and multi-step reasoning, tool use, self-reflection, Agent Evaluation Agent Capabilities Evaluation ( §2)</p>
<p>Planning and Multi-Step Reasoning ( §2.1) AQUA-RAT (Ling et al., 2017); HotpotQA (Yang et al., 2018); ARC (Clark et al., 2018a); StrategyQA (Geva et al., 2021); GSM8K (Cobbe et al., 2021); MATH (Hendrycks et al., 2021b); Game of 24 (Yao et al., 2023); MINT (Wang et al., 2023); PlanBench (Valmeekam et al., 2023); FlowBench (Xiao et al., 2024); FOLIO (Han et al., 2022); P-FOLIO (Han et al., 2024); MultiRC (Khashabi et al., 2018); MUSR (Sprague et al., 2023); BBH (Suzgun et al., 2022); ToolEmu (Ruan et al., 2023); MINT (Wang et al., 2023); AutoPlanBench (Stein et al., 2023); ACPBench (Kokel et al., 2024); Natural Plan (Zheng et al., 2024) Function Calling &amp; Tool Use ( §2.2) BFCL (Yan et al., 2024); ToolBench (Qin et al., 2023); ToolAlpaca (Tang et al., 2023); APIBench (Patil et al., 2025); API-Bank (Li et al., 2023); NexusRaven (team, 2023); Seal-Tools (Wu et al., 2024b); ComplexFuncBench (Zhong et al., 2025); ToolSandbox (Lu et al., 2024); RestBench (Song et al., 2023); APIGen (Liu et al., 2024c); StableToolBench (Guo et al., 2024); NESTFUL (Basu et al., 2024b) Self-Reflection ( §2.3) LLF-Bench (Cheng et al., 2023); LLM-Evolve (You et al., 2024); Reflection-Bench (Li et al., 2024) Memory ( §2.4)</p>
<p>NarrativeQA (Kočiskỳ et al., 2018); QMSum (Zhong et al., 2021); QUALITY (Pang et al., 2021); RAISE (Liu et al., 2024a); ReadAgent (Lee et al., 2024); MemGPT (Packer et al., 2024); LoCoMo (Maharana et al., 2024); A-MEM (Xu et al., 2025); StreamBench (Wu et al., 2024a); LTMbenchmark (Castillo-Bolado et al., 2024a) Application-Specific Agent Evaluation ( §3)</p>
<p>Web Agents ( §3.1)</p>
<p>MiniWob (Shi et al., 2017); MiniWoB++ (Liu et al., 2018); Web-Shop (Yao et al., 2022); Mind2web (Deng et al., 2023); WebVoyager (He et al., 2024); WebLinX (Lù et al., 2024); WebArena (Zhou et al., 2023); VisualWebArena (Koh et al., 2024); MMInA (Zhang et al., 2024); AssistantBench (Yoran et al., 2024); Web-Canvas (Pan et al., 2024b); ST-WebAgentBench (Levy et al., 2024); WorkArena (Drouin et al., 2024); WorkArena++ (Boisvert et al., 2025) Software Engineering Agents ( §3.2)</p>
<p>HumanEval (Chen et al., 2021b); SWE-bench (Jimenez et al., 2023); SWE-bench Verified (OpenAI, 2024); SWE-bench Lite (SWEbench Lite, 2024); SWE-bench+ (Aleithan et al., 2024); SWEbench Multimodal (Yang et al., 2024); TDD-Bench Verified (Ahmed et al., 2024); SWT-Bench (Mündler et al., 2024); IT-Bench (Jha et al., 2025); SWELancer (Miserendino et al., 2025) Scientific Agents ( §3.3) ScienceQA (Lu et al., 2022); QASPER (Dasigi et al., 2021); MS 2 (DeYoung et al., 2021); ScienceWorld (Wang et al., 2022a); SUPER (Bogin et al., 2024); Ideation (Si et al., 2025); AAAR-1.0 (Lou et al., 2025); ScienceAgentBench (Chen et al., 2024); CORE-Bench (Siegel et al., 2024); SciCode (Tian et al., 2024b); MLGym-Bench (Nathani et al., 2025);</p>
<p>DiscoveryWorld (Jansen et al., 2024); LAB-Bench (Laurent et al., 2024) Conversational Agents ( §3.4)</p>
<p>ABCD (Chen et al., 2021a); MultiWOZ (Budzianowski et al., 2018); SMCalFlow (Andreas et al., 2020); ALMITA (Arcadinho et al., 2024); τ -Bench (Yao et al., 2024); IntellAgent (Levi and Kadar, 2025a); LTM (Castillo-Bolado et al., 2024b) Generalist Agents Evaluation ( §4)</p>
<p>GAIA (Mialon et al., 2023); AgentBench (Liu et al., 2023b); Galileo's Agent Leaderboard (Bhavsar, 2025); OSWorld (Xie et al., 2024); AppWorld (Trivedi et al., 2024); OmniACT (Kapoor et al., 2024a); TheAgentCompany (Xu et al., 2024); CR-MArena (Huang et al., 2025); HAL (Stroebl et al., 2025) Frameworks for Agent Evaluation ( §5)</p>
<p>Development Frameworks</p>
<p>Databricks Mosaic AI (Databricks, 2023); Galileo Agentic (Galileo, 2025); Vertex AI Gen AI (Google Cloud, 2025); LangSmith (LangChain, 2023); Langfuse (Langfuse, 2023)  and memory.We then review benchmarks and evaluation strategies for prominent types of agentic applications: web agents, software engineering agents, scientific agents and conversational agents ( §3).Next, we describe benchmarks and leaderboards for evaluating general-purpose agents ( §4), which assess the agent's ability to perform different tasks that require diverse skills.The next section ( §5) reviews current evaluation frameworks for agent developers.These frameworks integrate with the agent's development environment, and support its evaluation throughout the entire development cycle.We conclude with a discussion ( §6) of current trends and emerging research directions in agent evaluation.Figure 1 provides a visual overview of the structure of our survey.Our survey is intended to offer researchers and practitioners a comprehensive understanding of the current state of agent evaluation and to highlight key areas for future innovation.</p>
<p>Scope In this survey, we specifically focus on evaluation methodologies for LLM-based agents.Consequently, widely-used single-call LLM benchmarks, such as MMLU, AlpacaEval, GSM8K, or similar standardized evaluation datasets, won't be extensively discussed.Additionally, detailed introduction to LLM-based agents, modeling choices and architectures, and design considerations are outside our focus, given their comprehensive treatment in existing surveys (e.g., Wang et al. (2024a)).Similarly, while we mention topics such as interactions between multi-agent systems, game agents, and embodied agents in some sections, they are not the main focus of this survey.Instead, our objective is to provide a comprehensive overview of evaluation methods for LLM-based agents.</p>
<p>Agent Capabilities Evaluation</p>
<p>LLM-based agents have evolved to rely on specific design patterns that encapsulate a core suite of LLM abilities.Evaluating these capabilities is paramount to understanding the potential and limitations of LLM-based agents.Here we focus on four foundational capabilities of LLM-based agents.</p>
<p>Planning and Multi-Step Reasoning</p>
<p>Planning and multi-step reasoning form the foundation of an LLM agent's ability to tackle complex tasks effectively which enables agents to decompose problems into smaller, more manageable subtasks and create strategic execution paths toward solutions (Gao et al., 2023a).</p>
<p>Multi-step reasoning in LLMs typically involves executing sequential logical operations-typically requiring 3-10 intermediate steps-to arrive at solutions that cannot be derived through single-step inference (Cobbe et al., 2021;Yang et al., 2018;Suzgun et al., 2022).This foundational need for multi-step planning has led to the development of specialized benchmarks and evaluation frameworks that systematically assess these capabilities across diverse domains, including: mathematical reasoning (GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), AQUA-RAT (Ling et al., 2017)), multi-hop question answering (HotpotQA (Yang et al., 2018), StrategyQA (Geva et al., 2021), MultiRC (Khashabi et al., 2018)), scientific reasoning (ARC (Clark et al., 2018a)), logical reasoning (FOLIO, P-FOLIO (Han et al., 2024(Han et al., , 2022))) constraint satisfaction puzzles (Game of 24 (Yao et al., 2023)), everyday common sense (MUSR (Sprague et al., 2023)), and challenging reasoning tasks (BBH (Suzgun et al., 2022)).Several of these benchmarks, particularly HotpotQA, ALF-Worlds, and Game of 24, have been specifically adapted for evaluating agent-based approaches like ReAct, where planning and calling the tools proposed by the agent are interleaved in interactive problem-solving settings.</p>
<p>Recent work has developed more specialized frameworks targeting LLM planning capabilities.ToolEmu (Ruan et al., 2023) introduces a simulator-based approach for evaluating tool-using agents, revealing that successful planning requires explicit state tracking and the ability to recover from errors.The MINT benchmark (Wang et al., 2023) evaluates planning in interactive environments, showing that even advanced LLMs struggle with long-horizon tasks requiring multiple steps.</p>
<p>PlanBench (Valmeekam et al., 2023) provides a comprehensive evaluation framework specifically designed to assess planning capabilities in LLM agents across diverse domains, revealing that current models excel at short-term tactical planning but struggle with strategic long-horizon planning.Complementing this, AutoPlanBench (Stein et al., 2023) focuses on evaluating planning in everyday scenarios, demonstrating that even SoTA LLM agents lag behind classical symbolic planners.</p>
<p>FlowBench (Xiao et al., 2024) evaluates workflow planning abilities, focusing on expertiseintensive tasks.ACPBench (Kokel et al., 2024) focuses on evaluating LLMs on core reasoning skills.The Natural Plan benchmark (Zheng et al., 2024) is designed to evaluate how LLMs handle real-world planning tasks presented in natural language.SoTA LLM agents perform poorly on this benchmark, particularly as complexity increases.</p>
<p>These benchmarks highlight key abilities essential for effective agent planning: (1) task decomposition for breaking down complex problems, (2) state tracking and belief maintenance for accurate multi-step reasoning, (3) self-correction to detect and recover from errors, (4) causal understanding to predict action outcomes, and (5) meta-planning to refine planning strategies.</p>
<p>Function Calling &amp; Tool Use</p>
<p>The ability of LLMs to interact with external tools through function calling is fundamental for building intelligent agents capable of delivering realtime, contextually accurate responses (Qin et al., 2023;Tang et al., 2023).Early works utilized targeted tools, such as retrieval in approaches by augmented language models with retrieval capabilities (Lewis et al., 2020;Gao et al., 2023b;Nakano et al., 2021).Later developments included more generalpurpose tools, exemplified by ToolFormer (Schick et al., 2023), Chameleon (Lu et al., 2023), and MRKL (Karpas et al., 2022).</p>
<p>Function calling involves several sub-tasks that work together seamlessly.Intent recognition identifies when a function is needed based on user requests.Function selection determines the most appropriate tool for the task.Parameter-value-pair mapping extracts relevant arguments from the conversation and assigns them to function parameters.Function execution invokes the selected function with those parameters to interact with external systems.Finally, response generation processes the function output and incorporates it into the LLM's reply to the user.This integrated process ensures accurate and efficient function calling within the LLM's workflow.</p>
<p>Early evaluation efforts offered approaches to evaluate the above sub-tasks while focusing on relatively simple, one-step interactions with explicitly provided parameters.Benchmarks such as ToolAlpaca (Tang et al., 2023), APIBench (Patil et al., 2025), ToolBench (Qin et al., 2023), and the Berkeley Function Calling Leaderboard v1 (BFCL) (Yan et al., 2024) exemplify this phase, employing synthetic datasets and rule-based matching (e.g., via Abstract Syntax Trees) to establish baseline metrics like pass rates and structural accuracy.However, these methods were limited in capturing the complexities of real-world scenarios, which might include multistep conversations, parameters that are not explicitly mentioned in the conversation, and tools with complex input structures and long, intricate outputs.</p>
<p>The live nature of BFCL aimed to bridge some of these gaps by introducing BFCL v2, which includes organizational tools, and BFCL v3, which incorporates integrated multi-turn and multi-step evaluation logic.These enhancements provide a closer approximation of real-world complexity and emphasize the importance of continuous state management.</p>
<p>Complementing this evolution, several benchmarks have broadened the evaluation landscape.For example, ToolSandbox (Lu et al., 2024) differs from previous benchmarks by incorporating stateful tool execution, implicit state dependencies, on-policy conversational evaluation with a built-in user simulator, and dynamic evaluation strategies for intermediate and final milestones across arbitrary trajectories.Seal-Tools (Wu et al., 2024b) adopts a self-instruct (Wang et al., 2022b) methodology to generate nested tool calls, effectively modeling layered and interdependent interactions.In parallel, API-Bank (Li et al., 2023) emphasizes realistic API engagements by utilizing dialoguebased evaluations and extensive training datasets.Frameworks like NexusRaven (team, 2023) further enrich this landscape by focusing on generalized tool-use scenarios that mirror the diverse challenges encountered in practice.API-Blend (Basu et al., 2024a) suggested a comprehensive approach focusing on identifying, curating, and transforming existing datasets into a large corpus for training and systematic testing of tool-augmented LLMs.API-Blend mimics real-world scenarios involving API tasks such as API/tool detection, slot filling, and sequencing of detected APIs, providing utility for both training and benchmarking purposes.Rest-Bench (Song et al., 2023) facilitates exploration of utilizing multiple APIs to address complex realworld user instructions.APIGen (Liu et al., 2024c) provides a comprehensive automated data generation pipeline that synthesizes high-quality functioncalling datasets verified through hierarchical stages.StableToolBench (Guo et al., 2024) addresses the challenges of function-calling evaluation by introducing a virtual API server with caching and simulators to alleviate API status changes.</p>
<p>Addressing the inherent complexity of multistep interactions, ComplexFuncBench (Zhong et al., 2025) was specifically designed to assess scenarios requiring implicit parameter inference, adherence to user-defined constraints, and efficient long-context processing.NESTFUL (Basu et al., 2024b) focuses on adding complexity by evaluating LLMs on nested sequences of API calls where outputs from one call serve as inputs to subsequent calls.</p>
<p>Self-Reflection</p>
<p>An emerging line of research focuses on whether agents can self-reflect and improve their reasoning through interactive feedback, thereby reducing errors in multi-step interactions.This requires the model to understand the feedback and dynamically update its beliefs to carry out adjusted actions or reasoning steps over extensive trajectories.</p>
<p>Early efforts to gauge LLM agent self-reflection were often indirect, repurposing existing reasoning or planning tasks, such as AGIEval (Zhong et al., 2023), MedMCQA (Pal et al., 2022), ALF-World (Shridhar et al., 2021), MiniWoB++ (Liu et al., 2018), etc., into multi-turn feedback loops, to see if models could recognize or correct their own errors given external feedback in confined settings (Renze and Guven, 2024;Huang et al., 2024;Shinn et al., 2023;You et al., 2024;Sun et al., 2023;Liu et al., 2025).Improvement was typically measured by determining if the final answer was corrected, providing only a coarse evaluation and potentially ill-defined measurement, as observed improvements may depend on specific prompting techniques lacking proper standardization (Huang et al., 2024;Liu et al., 2025).</p>
<p>As a dedicated effort to establish a standardized benchmark for interactive self-reflection, LLF-Bench (Cheng et al., 2023) was proposed.This benchmark extends diverse decision-making tasks and incorporates task instructions as part of the environment rather than as part of the agent.To mitigate overfitting to specific environments, LLF-Bench offers options to randomize textual descriptions of task instructions and feedback received by agents.</p>
<p>Similarly, LLM-Evolve (You et al., 2024) was introduced to evaluate LLM agents' self-reflection capabilities on standard benchmarks such as MMLU (Hendrycks et al., 2020).This approach evaluates agents based on past experiences by collecting previous queries with feedback and extracting them as in-context demonstrations.To provide more granular insights into different feedback types, (Pan et al., 2025) focused specifically on coding agents, extending existing coding benchmarks like APPS (Hendrycks et al., 2021a) and LiveCodeBench (Jain et al., 2024) to interactive settings.</p>
<p>From a cognitive science perspective, Reflection-Bench (Li et al., 2024) was designed to assess LLMs' cognitive reflection capabilities, breaking down reflection into components like perception of new information, memory usage, belief updating following surprise, decision-making adjustments, counterfactual reasoning, and meta-reflection.</p>
<p>Memory</p>
<p>Memory mechanisms in LLM-based agents improve their handling of long contexts and information retrieval, overcoming static knowledge limits and supporting reasoning and planning in dynamic scenarios (Park et al., 2023).Unlike tool use, which connects agents to external resources, memory ensures context retention for extended interactions like processing documents or maintaining conversations.Agents rely on short-term memory for realtime responses and long-term memory for deeper understanding and applying knowledge over time.Together, these memory systems allow LLM-based agents to adapt, learn, and make well-informed decisions in tasks requiring persistent information access.</p>
<p>One prominent line of research focuses on addressing the challenge of limited context lengths in LLMs by incorporating memory mechanisms to enhance reasoning and retrieval across extended contexts and conversations.Recent works, such as ReadAgent (Lee et al., 2024), MemGPT (Packer et al., 2024), and A-MEM (Xu et al., 2025), investigate these methods and evaluate their efficacy through reasoning and retrieval metrics.</p>
<p>Specifically, ReadAgent structures reading by grouping content, condensing episodes into memories, and retrieving passages, with effectiveness shown on datasets like QUALITY (Pang et al., 2021), NarrativeQA (Kočiskỳ et al., 2018), and QMSum (Zhong et al., 2021).</p>
<p>Similarly, A-MEM introduces an advanced memory architecture evaluated using the Lo-CoMo benchmark (Maharana et al., 2024), while MemGPT manages a tiered memory system tested on NaturalQuestions-Open (Liu et al., 2024b) and multi-session chat datasets (Xu et al., 2021).</p>
<p>For episodic memory evaluation, (Huet et al., 2025) proposes a specialized benchmark to assess how LLMs generate and manage memories that capture specific events with contextual details.This benchmark utilizes synthetically created book chapters and events with LLMs-based judge evaluation metrics to measure accuracy and relevance.StreamBench (Wu et al., 2024a) represents a more challenging setting, evaluating how agents leverage external memory components-including the memory of previous interactions and external feed-back-to continuously improve performance over time, with quality and efficiency assessed across diverse datasets including text-to-SQL tasks (e.g., Spider (Yu et al., 2018)), ToolBench (Xu et al., 2023), and HotpotQA (Yang et al., 2018).</p>
<p>Beyond context length optimization, memory mechanisms also enhance real-time decisionmaking and learning in agent settings, focusing on action optimization (Liu et al., 2024a;Shinn et al., 2023;Wang et al., 2024b).For example, Reflexion (Shinn et al., 2023) tracks success rate on tasks like HotPotQA (Yang et al., 2018) and ALFWorld (Shridhar et al., 2021), while RAISE (Liu et al., 2024a) enhances the ReAct framework with a two-part memory system evaluated through human judgment on quality metrics and efficiency.Similarly, KARMA (Wang et al., 2024b) tests memory in household tasks using metrics such as success rate, retrieval accuracy, and memory hit rate, demonstrating how memory mechanisms significantly improve agent performance across diverse domains requiring complex reasoning and persistent information retention.LTMbenchmark (Castillo-Bolado et al., 2024a) evaluates conversational agents through extended, multitask interactions with frequent context switching to test long-term memory and information integration capabilities.The results demonstrate that while LLMs generally perform well in single-task scenarios, they struggle with interleaved tasks, and interestingly, short-context LLMs equipped with long-term memory systems can match or exceed the performance of models with larger context windows.</p>
<p>Application-Specific Agents Evaluation</p>
<p>The landscape of application-specific agents is rapidly expanding, with an increasing number of specialized agents emerging across popular categories such as tools, web, software, game, embodied, and scientific agents (Wang et al., 2024a).In this section, we focus on four prominent categories that exemplify the diversity and potential of these agents, offering insights into their evaluation frameworks and performance metrics tailored to their unique applications.</p>
<p>Agent benchmarks offer a systematic framework for assessing the diverse capabilities of LLMbased agents by integrating three key elements.First, they utilize a dataset of clearly defined tasks-ranging from website navigation to com-plex scientific problem-solving-that outline what agents are expected to achieve.Second, they establish the operating environment, which may be simulated (whether static or dynamic) or real-world, and can incorporate user simulations, a variety of tools, and specific policies to adhere to.Third, they apply evaluation metrics, such as success rate, efficiency, and accuracy, to measure performance.These metrics can be applied with varying degrees of granularity, from tracking individual actions and milestones to assessing overall, end-to-end task completion.</p>
<p>Web Agents</p>
<p>Web agents are AI systems designed to interact with websites to perform tasks such as booking flights or shopping.Their evaluation involves testing how effectively they complete tasks, navigate web environments, and adhere to safety and compliance rules.As these agents have evolved, so too have the benchmarks used to assess them, with recent developments capturing an increasingly complex range of real-world interactions.</p>
<p>Initial efforts in web-agent evaluation focused on basic simulation environments.Early benchmarks such as MiniWob (Shi et al., 2017) and MiniWoB++ (Liu et al., 2018) provided fundamental frameworks for assessing navigation and task automation capabilities.These pioneering studies established essential evaluation protocols and highlighted key challenges in executing web-based tasks, laying the groundwork for more sophisticated assessments.</p>
<p>Building on these early efforts, subsequent research introduced static datasets that enable offline, reproducible evaluation.For example, WebShop (Yao et al., 2022) simulates online shopping scenarios, requiring agents to perform tasks ranging from product search to checkout processes.Similarly, Mind2Web (Deng et al., 2023) and WebVoyager (He et al., 2024) extend this paradigm by incorporating a broader spectrum of web interactions, thereby allowing for comprehensive assessments of an agent's ability to navigate complex website structures and achieve intermediate goals.These benchmarks have been instrumental in standardizing the evaluation process, and facilitating direct comparisons across different methodologies.</p>
<p>More recent efforts have shifted toward dynamic, online benchmarks that more closely mimic realworld conditions.WebLinX (Lù et al., 2024) introduces a dynamic interaction model in which agents must adapt to continuous changes in the web interface, testing the robustness of their decisionmaking processes.WebArena (Zhou et al., 2023) and its visual variant, Visual-WebArena (Koh et al., 2024), incorporate realistic user interface elements and visual cues, requiring agents to not only follow predefined workflows but also interpret and respond to visual information.In addition, WorkArena (Drouin et al., 2024) and WorkArena++ (Boisvert et al., 2025) simulate complex, multistep tasks typical of office or enterprise environments, where coordinating several actions is necessary to achieve long-term objectives.MMInA (Zhang et al., 2024) provides multimodal, Multihop, holistic evaluation.AssistantBench (Yoran et al., 2024) focuses on realistic multi-site timeconsuming tasks.Notably, WebCanvas (Pan et al., 2024b) refines the dynamic evaluation framework by specifically measuring the completion rates of key navigational nodes, thereby offering a more granular analysis of agent performance.</p>
<p>Recent advances have further broadened the evaluation landscape.The introduction of ST-WebAgentBench (Levy et al., 2024) represents an effort to assess web agents in settings that integrate both static and dynamic elements, providing insights into agents' performance under varied conditions.While these benchmarks have significantly advanced our understanding of web-agent capabilities, most of them continue to focus primarily on task completion and navigational efficiency.Critical aspects such as policy compliance, risk mitigation, and adherence to organizational safety protocols remain underexplored.As web agents move closer to real-world deployment, addressing these gaps will be essential for ensuring both their practical utility and safe operation.</p>
<p>Software Engineering Agents</p>
<p>The evaluation of software engineering (SWE) agents began with benchmarks that measured fundamental coding capabilities, such as HumanEval (Chen et al., 2021b) and MBPP (Austin et al., 2021).These early benchmarks focused on short, selfcontained, algorithm-specific tasks, offering an initial glimpse into the potential of LLMs for code generation.More recently, open-domain coding benchmarks (Wang et al., 2022c;Lai et al., 2022) have emerged as a notable step forward by incorporating application-specific scenarios and interactions with diverse libraries and tools.However, while these benchmarks mark clear progress, they generally evaluate simpler intents and limited tool usage, thus falling short of addressing the full complexity of real-world SWE tasks.SWE-bench (Jimenez et al., 2023) was introduced to address the above shortcomings.It is constructed from real-world GitHub issues and offers an end-to-end evaluation framework, including detailed issue descriptions, complete code repositories, execution environments (e.g., Docker), and validation tests.To enhance evaluation reliability, several variants have been proposed.SWE-bench Lite (SWE-bench Lite, 2024) focuses on a subset of 300 issues involving bug fixing, filtering out tasks requiring complicated multi-file edits or extraneous elements.SWE-bench LiteS (Xia et al., 2024) further refines the dataset by removing tasks with exact patches or insufficient descriptive information, while SWE-bench Verified (OpenAI, 2024) includes only those issues with clear, informative descriptions and robust test cases.More recently, SWE-bench+ (Aleithan et al., 2024) has been introduced to mitigate critical evaluation flaws such as solution leakage and weak test cases, thereby providing a more robust benchmark for assessing SWE agents.Additionally, a Java version of SWE-bench was introduced in (Zan et al., 2024).SWE-bench Multimodal (Yang et al., 2024) evaluates agents in visual software domains, targeting JavaScript-based applications with visual elements in problems and tests.It highlights challenges in visual problem-solving and cross-language generalization, with top systems showing lower resolution rates.TDD-Bench Verified (Ahmed et al., 2024) and SWT-Bench (Mündler et al., 2024) evaluate the agent's ability to generate tests from user issues in real-world Github repositories.ITBench (Jha et al., 2025) offers a benchmark for evaluating challenging real-world IT automation tasks.</p>
<p>Complementing these developments, Agent-Bench (Liu et al., 2023b) has emerged as a dedicated framework for evaluating the interactive capabilities of SWE agents.AgentBench provides insights into agent performance in dynamic settings through real-time interaction and environment manipulation.Finally, the introduction of SWE-Lancer (Miserendino et al., 2025) represents the latest trend in benchmark development.By targeting freelance coding tasks, SWELancer links agent performance to monetary value, underscoring the challenges in achieving long-term reasoning and decision-making in complex, real-world scenarios.</p>
<p>Scientific Agents</p>
<p>The evaluation of scientific agents has evolved from early benchmarks assessing basic reasoning to comprehensive frameworks evaluating diverse scientific research capabilities.Initial benchmarks emphasized scientific knowledge recall and reasoning-examples include ARC (Clark et al., 2018b), ScienceQA (Lu et al., 2022), and Science-World (Wang et al., 2022a).Others focused on the synthesis and contextualization of scientific literature, such as QASPER (Dasigi et al., 2021), QASA (Lee et al., 2023), and MS 2 (DeYoung et al., 2021).More recent benchmarks, like SciRiff (Wadden et al., 2024), have expanded to evaluate a broader range of tasks, emphasizing the ability to follow user instructions across scientific domains.</p>
<p>Recent advancements have shifted the focus toward developing and assessing scientific agents in accelerating scientific research.Emerging benchmarks now cover various stages of the scientific research process: (1) Scientific Ideation: This stage explores whether scientific agents can autonomously generate novel, expert-level research ideas that are comparable to those proposed by human experts (Si et al., 2025).The emphasis is on creativity, relevance, and feasibility in scientific thinking.(2) Experiment Design: Benchmarks like the AAAR-1.0dataset (Lou et al., 2025) assess an agent's ability to systematically plan experiments.This includes formulating hypotheses, selecting appropriate methodologies, and outlining experimental procedures that adhere to scientific rigor.(3) Code Generation for Experiment Execution: Benchmarks such as SciCode (Tian et al., 2024a), ScienceAgentBench (Chen et al., 2025), SUPER (Bogin et al., 2024), and CORE-Bench (Siegel et al., 2024) are pivotal in verifying whether agents can produce accurate, executable scientific code.These benchmarks ensure the code aligns with the specific demands of scientific protocols and maintains computational accuracy.(4) Peer-Review Generation: It examines whether agents can provide comprehensive, substantive feedback that matches or surpasses the quality of human peer reviewers (Chamoun et al., 2024).</p>
<p>Beyond the individual task-specific benchmarks described above, there is a growing interest in unified frameworks that integrate multiple, interrelated scientific tasks into single platforms.AAAR-1.0 (Lou et al., 2025) evaluates scientific agents across four core research tasks, i.e., equation inference, experiment design, paper weakness identification, and review critique, focusing on tasks that require deep domain expertise.MLGym (Nathani et al., 2025) introduces a gym-like environment for AI research tasks, covering 13 diverse challenges that simulate real-world research workflows, from hypothesis generation to experimentation and analysis.DiscoveryWorld (Jansen et al., 2024) offers a virtual, text-based environment for simulating complete scientific discovery cycles across 120 diverse tasks, emphasizing hypothesis formation, experimentation, and result interpretation.LAB-Bench (Laurent et al., 2024) offers a domainspecific evaluation tailored to biological research.It challenges agents with tasks ranging from experimental design to the interpretation of texts, images, and tables.</p>
<p>Conversational Agents</p>
<p>Customer-facing agents are required to handle user requests, while adhering to the company's policies and procedures.Successful completion of such tasks requires the agent to engage with a user in a multi-turn, task-oriented dialogue, while performing a sequence of actions that involve various function calls.</p>
<p>A common benchmarking approach for these agents is to collect ground truth trajectories with user and agent messages, and function calls.Given a prefix of such a trajectory, the agent is evaluated on predicting the next step.A more flexible approach simulates both the environment and the user.The agent is assessed on its ability to bring the environment to the desired state and communicate the right answer to the user.</p>
<p>The Action-Based Conversations Dataset (ABCD) (Chen et al., 2021a) includes over 10K customer-agent conversations, collected via crowdsourcing.These dialogues contain 55 distinct user intents, each requiring a unique sequence of actions defined by the corresponding policy.Additional examples of crowdsourced task-oriented dialogue benchmarks are MultiWOZ (Budzianowski et al., 2018) and SMCalFlow (Andreas et al., 2020).</p>
<p>A fully automated pipeline for generating tests for conversational AI agents in the customer service domain is described in (Arcadinho et al., 2024).Utilizing an LLM as a generator at each step, they create a set of intents, a procedure defining how each intent should be handled by the agent, tool APIs to be called by the agent, a flow graph and a conversation graph, from which conversation paths are sampled.Finally, prefixes of the conversation path are extracted as tests.Their manually-filtered ALMITA benchmark includes 192 conversations for 14 intents, resulting in 1420 tests.</p>
<p>The τ -Bench benchmark (Yao et al., 2024) emulates dynamic conversations between an agent and an LLM-simulated user in two customer service domains, airline and retail.The benchmark was constructed manually, with some LM assistance.Each domain includes several databases, associated APIs, and a domain policy provided to the agent as a system prompt.Task instances include an instruction for the user simulation and ground truth annotation for the expected database write operation and the required output for the user's question.The dataset includes 115 retail tasks and 50 airline tasks.</p>
<p>IntellAgent (Levi and Kadar, 2025a) provides an open-source framework for automatic benchmarking of conversational agents, taking a schema of the system database and a company policies document as input.It constructs a policy graph, from which a list of policies is sampled.It then creates an event addressing these policies, and simulates a dialogue between the tested agent and a user agent, based on the event information.Finally, a critique agent analyzes the dialogue and provides detailed feedback on the tested policies.</p>
<p>Generalist Agents Evaluation</p>
<p>Building on the evaluation of basic agentic capabilities and application-specific ones, we now turn to examine benchmarks for general agents.As LLMs evolved from task-specific to general-purpose, agents are now transitioning from applicationspecific to more general-purpose ones.These agents integrate core LLM abilities with skills like web navigation, information retrieval, and code execution to tackle complex challenges.This shift necessitates broader evaluation methods, leading to the development of benchmarks that assess their diverse capabilities.</p>
<p>A primary category of these benchmarks focuses on evaluating general capabilities that emphasize multi-step reasoning, interactive problem-solving, and proficient tool use.The GAIA benchmark (Mialon et al., 2023) includes 466 human-crafted, realworld questions that test an agent's reasoning, multimodal understanding, web navigation, and gen-eral tool-use abilities.Similarly, Galileo's Agent Leaderboard (Galileo, 2025) emphasizes the evaluation of agents' abilities to perform function calls and API invocations in real-world applications such as database queries, online calculators, and web services.AgentBench (Liu et al., 2023a) introduces a suite of interactive environments that include operating system commands, SQL databases, digital games, and household tasks.These benchmarks collectively highlight the core competencies required for general agents-flexibility, multi-step reasoning, and adaptive tool use.</p>
<p>Beyond general reasoning and tool use, another crucial dimension in evaluating general agents lies in their performance within full-scale computer operating environments.Benchmarks like OS-World (Xie et al., 2024), OmniACT (Kapoor et al., 2024a), and AppWorld (Trivedi et al., 2024) test whether agents can navigate real-world computer systems, execute complex tasks, and coordinate actions across multiple applications.In these settings, agents must write and modify interactive code, handle complex control flows, and ensure robust execution without causing unintended system changes.</p>
<p>Motivated by the need to assess how general agents perform in realistic professional settings, recent benchmarks extend evaluation into digital work environments, where agents must manage tasks akin to those of human employees.TheAgent-Company (Xu et al., 2024) creates an extensible environment resembling a small software company, in which agents browse internal websites, write code, run programs, and communicate with coworkers.CRMArena (Huang et al., 2025) focuses on customer relationship management (CRM), simulating a large-scale CRM environment filled with interconnected data about accounts, orders, knowledge articles, and cases.It examines whether agents can perform multi-step operations using both UI and API access, adhere to domain-specific policies, and integrate various pieces of information to complete complex enterprise tasks.</p>
<p>As benchmarks diversify, there is a growing need for unified platforms that consolidate testing criteria.Holistic Agent Leaderboard (HAL) (Stroebl et al., 2025) serves as a standardized evaluation platform that aggregates multiple benchmarks, covering coding, interactive applications, and safety assessments.</p>
<p>Frameworks for Agent Evaluation</p>
<p>In response to the growing need for systematic assessment of LLM agents, several frameworks have recently emerged, providing developers with essential tools to evaluate, refine, and improve their performance, quality, and efficiency.Unlike the benchmarks discussed in the preceding section, which compare the performance of fully developed systems across predefined scenarios, these frameworks serve as integral components of the development ecosystem, enabling continuous monitoring and indepth error analysis across both development and deployment.Rather than supplying standardized test data, they allow developers to design and evaluate their own scenarios, offering greater flexibility.Moreover, these frameworks are designed to be highly general, supporting a wide range of development use cases rather than focusing on specific tasks, making them versatile tools for AI research and application.</p>
<p>While earlier evaluation frameworks for LLMbased applications primarily focused on assessing a model's task completion ability through single-call interactions (e.g.OpenAI Evals (OpenAI, 2023)), the rise of agentic workflows has created a need for more advanced evaluation frameworks capable of assessing multi-step reasoning, trajectory analysis, and specific agent capabilities such as tool usage.</p>
<p>There are many frameworks supporting the evaluation of a wide range of agent types, including LangSmith (LangChain, 2023), Langfuse (Langfuse, 2023), Google Vertex AI evaluation service (Google Cloud, 2025), Arize AI's Evaluation Framework (Arize AI, Inc, 2025), Galileo Agentic Evaluation (Galileo, 2025), Patronus AI (Patronus AI, Inc., 2023), LangChains' AgentEvals (LangChain, 2025); Databricks Mosaic AI Agent Evaluation (Databricks, 2023) which is mostly designed for RAG like tasks, Botpress Multi-Agent Evaluation System (Kargwal, 2025) and AutoGen (Dibia et al., 2024) for multi-agent systems, and more.</p>
<p>All evaluation platforms provide continuous monitoring of agent trajectories, assessing key performance metrics such as task completion rates, latency, execution speed, and, in some cases, throughput and memory usage (LangChain, 2023).Some frameworks utilize the OpenTelemetry (Blanco, 2023) observability framework and their infrastructure, including Langfuse (Langfuse, 2023) and Google Vertex AI (Google Cloud, 2025).Beyond observability and monitoring, each framework incorporates unique methodologies for quality assessment, providing additional layers of evaluation.</p>
<p>Evaluating agentic workflows occurs at multiple levels of granularity, each focusing on different aspects of the agent's dynamics.</p>
<p>Final Response Evaluation.Frameworks often incorporate LLM-based judges to evaluate agent responses against predefined criteria, with some offering proprietary judge models (e.g.Databricks Mosaic (Databricks, 2023), and PatronusAI (Patronus AI, Inc., 2023)).Additionally, most platforms allow for customizable assessment metrics, enabling domain-specific evaluation of output quality and relevance.</p>
<p>Stepwise Evaluation.Most evaluation frameworks support granular assessments of individual agent actions or LLM calls, facilitating root cause analysis of errors.This includes assessing textual output with predefined judges, and assessing tool selection and execution by either comparing the selected tool against an expected tool for a given step, or using an automated judge to verify the tool choice, its parameters, and the correctness of the execution output.Furthermore, Galileo Agentic Evaluation (Galileo, 2025) introduces an action advancement metric, which measures whether each step successfully contributes to or advances toward a user-defined goal.This approach refines stepwise evaluation by assessing progress rather than solely relying on binary success/failure outcomes.</p>
<p>A key challenge in the current stepwise evaluation schemes lies in the scope and reliability of the automated judges.Many judges are task-specific, making them well-suited for particular evaluations but difficult to generalize across complex workflows.Conversely, more general-purpose judges may offer broad applicability but lack clear quality guarantees.</p>
<p>Trajectory-Based Assessment.In addition to stepwise evaluation, some platforms-such as Google Vertex AI (Google Cloud, 2025) and Lang-Smith (LangChain, 2023) support trajectory-based assessments, which analyze the sequence of steps taken by an agent in relation to an expected optimal path.This method evaluates the agent's decisionmaking process, particularly regarding tool selection and sequencing.AgentEvals (LangChain, 2025) also enables LLM-as-judge evaluation of agent trajectories, with or without a reference trajectory.Additionally, it supports graph evaluation for frameworks like LangGraph, which model
(LangChain) ✓ ✓ ✓ ✓ × ✓ Langfuse (Langfuse) ✓ ✓ × ✓ × ✓ Google Vertex AI evaluation (Google Cloud) ✓ ✓ ✓ × × ✓ Arize AI's Evaluation (Arize AI, Inc) ✓ ✓ × ✓ ✓ ✓ Galileo Agentic Evaluation (Galileo) ✓ ✓ × ✓ × ✓ Patronus AI (Patronus AI, Inc.) ✓ ✓ × ✓ ✓ ✓ AgentsEval (LangChain) × × ✓ × × × Mosaic AI (Databricks) ✓ ✓ × ✓ ✓ ✓
Table 1: Supported evaluation capabilities of major agent frameworks.Note that some of these capabilities are still in initial phases of development, as discussed further in the text.</p>
<p>agents as graphs, by assessing whether an agent follows the expected workflow and correctly invokes the appropriate nodes and transitions.However, the reliance on reference sequences, combined with the non-deterministic nature of agentic workflows and the existence of multiple valid solutions, introduces significant challenges in defining and benchmarking optimal trajectories.Datasets.A critical aspect of agent evaluation is input data curation and annotation.Most frameworks provide integrated annotation tools, support human-in-the-loop evaluation, where human feedback is collected from production runs to refine model configurations, and enable the extraction of evaluation datasets from production logs, leveraging real-world interactions to enhance assessment quality.Additionally, platforms such as Patronus AI (Patronus AI, Inc., 2023), and Databricks Mosaic (Databricks, 2023) facilitate synthetic data generation using proprietary seed data.</p>
<p>A/B comparisons.Current evaluation frameworks support A/B comparisons, enabling the sideby-side analysis of inputs, outputs, and metrics from at least two test runs.In some cases-such as Patronus AI (Patronus AI, Inc., 2023)-these frameworks also facilitate the comparison of aggregated results across multiple runs from distinct experimental setups.Additionally, these frameworks provide the capability to drill down into individual trajectories, identifying specific failure points.However, obtaining large-scale insights at the trajectory or stepwise level remains a significant challenge.</p>
<p>Table 1 presents key frameworks for agent evaluation along with their support for the evaluation features discussed in this section.</p>
<p>Gym-like Environments.The frameworks discussed primarily monitor and assess agent behavior passively in real-world scenarios.However, evaluation often requires a controlled, interactive setting with a simulated environ-ment, provided by Gym-like frameworks.Inspired by OpenAI Gym (Brockman et al., 2016), which was originally designed for training and evaluating Reinforcement Learning algorithms, these frameworks have been adapted to the training and evaluation of LLM agents using realistic task simulations, allowing LLM agents to interact with dynamic environments.Moreover, these frameworks enable standardized evaluation across various benchmarks, with proposals made for web agents (Chezelles et al., 2024), AI research agents (Nathani et al., 2025) and SWE agents (Pan et al., 2024a).</p>
<p>Discussion</p>
<p>Current Trends</p>
<p>Our review of the evolution of agent benchmarking highlights several converging trends shaping the field.We identify two primary motives exhibited in the development of new evaluation methodologies, which we outline in the subsequent discussion.Realistic and Challenging Evaluation.Early agent evaluations often relied on simplified, static environments.However, there is a clear shift toward benchmarks that more accurately reflect realworld complexities.In web agent evaluation, for example, we have moved from basic simulations like MiniWob to dynamic online environments like WebArena and VisualWebArena, and from LAB-Bench which is static and narrow to Discovery-World for scientific agents.In software engineering, SWE-bench utilizes real-world GitHub issues, moving beyond synthetic coding problems.This shift toward realism is key to evaluating agents in real-world scenarios, capturing interaction nuances missed by simpler benchmarks.Benchmarks like Natural Plan, which incorporates simulated API results from real-world tools like Google Calendar and Maps, further exemplify this drive for realistic task settings.Concurrently, to keep pace with increasingly capable agents and ensure benchmarks remain challenging, there is a distinct trend toward greater task complexity and difficulty.This is evident from benchmarks like SWE-bench and SWE-Lancer targeting complex coding tasks, CORE-Bench for scientific computational reproducibility, and intricate general agent benchmarks like GAIA and TheAgentCompany.A key indicator of their difficulty is the low score of the best-performing agents in their papers, sometimes as low as 2%.This increased challenge is crucial for stress-testing agents, revealing limitations, and driving advances in long-horizon planning, robust reasoning, and tool use.Live Benchmarks.The rapid pace of LLM and agent development necessitates evaluation methodologies that are adaptive and continuously updated.Static benchmarks can quickly become outdated as models improve, potentially leading to benchmark saturation and a reduced ability to differentiate between systems.We observe this dynamic approach in the evolution of BFCL, which has progressed through multiple versions, incorporating live datasets, organizational tools, and multiturn evaluation logic to remain relevant.Similarly, the continuous refinement and variant creation within the SWE-bench family (SWE-bench Lite, SWE-bench Verified, SWE-bench+) along with the development of IntellAgent based on τ -Bench , demonstrates an ongoing effort to enhance and adapt agent benchmarks to meet evolving evaluation needs.This dynamic approach is essential for maintaining the relevance of benchmarks in this rapidly advancing field.</p>
<p>Emergent Directions</p>
<p>Observed but not yet fully established trends point to promising future research opportunities for advancing agent evaluation.Advancing Granular Evaluation.Many current benchmarks rely on coarse-grained, end-to-end success metrics that, while useful for gauging overall performance, fall short in diagnosing specific agent failures.This lack of granularity obscures insights into intermediate decision processes such as tool selection and reasoning quality.Addressing this limitation calls for the development of standardized, fine-grained evaluation metrics that capture the trajectory of an agent's task execution.Future work should explore detailed, step-by-step assessments-similar to those emerging in benchmarks like WebCanvas (Pan et al., 2024b) and frameworks like LangSmith and Galileo Agentic Evaluations (Galileo, 2025) -to provide richer feedback and guide targeted improvements.</p>
<p>Cost and Efficiency Metrics.As observed by Kapoor et al. (2024b), current evaluations often prioritize accuracy while overlooking cost and efficiency measurements.This emphasis can inadvertently drive the development of highly capable but resource-intensive agents, limiting their practical deployment.Future evaluation frameworks should integrate cost efficiency as a core metric, tracking factors such as token usage, API expenses, inference time, and overall resource consumption.Establishing standardized cost metrics will help guide the development of agents that balance performance with operational viability.</p>
<p>Scaling &amp; Automating.The reliance on static human annotated evaluation poses significant scalability challenges, as these methods can be resourceintensive and quickly outdated in a rapidly evolving field.This shortcoming underscores the need for scalable, automated evaluation approaches.Future directions include leveraging synthetic data generation techniques to create diverse and realistic task scenarios-imitated by efforts such as IntellAgent (Levi and Kadar, 2025b) and Mosaic AI Agent Evaluation (Databricks, 2023).Another avenue is automating evaluation by employing LLM-based agents as evaluators, termed Agent-as-a-Judge.As highlighted by Zhuge et al. (2024), this approach not only reduces the reliance on resource-intensive human annotation but also holds the potential to capture more nuanced aspects of agent performance through agentic evaluation processes.By harnessing these approaches, the community can achieve continuous, fine-grained, and cost-effective assessment of agent performance.</p>
<p>Safety and Compliance.</p>
<p>A notable shortcoming in current benchmarks is the limited focus on safety, trustworthiness, and policy compliance.While early efforts (e.g., AgentHarm (Andriushchenko et al., 2024) and ST-WebAgentBench) have begun to address these dimensions, evaluations still lack comprehensive tests for robustness against adversarial inputs, bias mitigation, and organizational and societal policy compliance.Future research should prioritize developing multi-dimensional safety benchmarks that simulate real-world scenarios, particularly in multi-agent scenarios where emergent risks may arise (Hammond et al., 2025).This can ensure that agents are not only effective but also safe and secure.</p>
<p>Conclusion</p>
<p>The field of LLM-based agent evaluation is rapidly advancing, driven by the need to assess increasingly complex and autonomous systems.While significant progress has been made in creating more realistic, dynamic, and challenging benchmarks, critical gaps remain, particularly in the areas of safety, fine-grained evaluation, and cost-efficiency.Addressing these shortcomings and pursuing the outlined future directions will be crucial for ensuring the responsible development and effective deployment of LLM-based agents in real-world applications.</p>
<p>Figure 1 :
1
Figure 1: Overview of the paper.</p>
<p>Framework</p>
<p>Stepwise Assessment Monitoring Trajectory Assessment Human in the Loop Synthetic Data Generation A/B Comparisons LangSmith</p>
<p>;</p>
<p>(LangChain, 2025)onus AI, Inc., 2023); LangChain AgentEvals(LangChain, 2025)
Gym-like EnvironmentsMLGym(Nathani et al., 2025); BrowserGym(Chezelles et al., 2024); SWE-Gym(Pan et al., 2024a)Discussion ( §6)Current Trends ( §6.1)Realistic and Challenging Evaluation; Live BenchmarksEmergent Di-Advancing Granular Evaluation; Cost and Efficiency Met-rections ( §6.2)rics; Scaling &amp; Automating; Safety and Compliance
The terminology for more complex systems that utilize LLMs as core components varies across the literature. Common terms include "LLM-based Agents", 'LLM Agents", "Language Agents", "AI Agents", "Agents," and "Agentic Systems". We adopt the terms "LLM-based agents" and "LLM agents" to emphasize both the foundational technology (LLMs) and the agentic capabilities that extend beyond single model inference.</p>
<p>Tdd-bench verified: Can llms generate tests for issues before they get resolved?. Toufique Ahmed, Martin Hirzel, Rangeet Pan, Avraham Shinnar, Saurabh Sinha, arXiv:2412.028832024arXiv preprint</p>
<p>Swe-bench+: Enhanced coding benchmark for llms. Haoran Reem Aleithan, Mohammad Xue, Elijah Mahdi Mohajer, Gias Nnorom, Song Uddin, Wang, ArXiv, abs/2410.069922024</p>
<p>Task-oriented dialogue as dataflow synthesis. Jacob Andreas, John Bufe, David Burkett, Charles Chen, Josh Clausman, Jean Crawford, Kate Crim, Jordan Deloach, Leah Dorner, Jason Eisner, Hao Fang, Alan Guo, David Hall, Kristin Hayes, Kellie Hill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan Klein, Jayant Krishnamurthy, Theo Lanman, Percy Liang, Christopher H Lin, Ilya Lintsbakh, Andy Mc-Govern, Aleksandr Nisnevich ; Subhro, Jesse Roy, Beth Rusak, Div Short, Ben Slomin, Stephon Snyder, Yu Striplin, Zachary Su, Sam Tellman, Andrei Thomson, Izabela Vorobev, Jason Witoszko, Abby Wolfe, Yuchen Wray, Alexander Zhang, Zotov, 10.1162/tacl_a_0033320208Adam Pauls, Dmitrij Petters, Brent Read, Dan Roth,Transactions of the Association for Computational Linguistics</p>
<p>Agentharm: A benchmark for measuring harmfulness of llm agents. Maksym Andriushchenko, Alexandra Souly, Mateusz Dziemian, Derek Duenas, Maxwell Lin, Justin Wang, Dan Hendrycks, Andy Zou, Zico Kolter, Matt Fredrikson, Eric Winsor, Jerome Wynne, Yarin Gal, Xander Davies, arXiv:2410.090242024Preprint</p>
<p>Automated test generation to evaluate tool-augmented LLMs as conversational AI agents. Samuel Arcadinho, David Oliveira Aparicio, Mariana S C Almeida, 10.18653/v1/2024.genbench-1.4Proceedings of the 2nd GenBench Workshop on Generalisation (Benchmarking) in NLP. the 2nd GenBench Workshop on Generalisation (Benchmarking) in NLPMiami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Program synthesis with large language models. A I Arize, Inc Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J Cai, Michael Terry, Quoc V Le, Charles Sutton, ArXiv, abs/2108.077322025. 2021Agent evaluation</p>
<p>Kinjal Basu, Ibrahim Abdelaziz, Subhajit Chaudhury, Soham Dan, Maxwell Crouse, Asim Munawar, Sadhana Kumaravel, Vinod Muthusamy, Pavan Kapanipathi, Luis A Lastras, arXiv:2402.15491Api-blend: A comprehensive corpora for training and benchmarking api llms. 2024aarXiv preprint</p>
<p>Nestful: A benchmark for evaluating llms on nested sequences of api calls. Kinjal Basu, Ibrahim Abdelaziz, Kiran Kate, Mayank Agarwal, Maxwell Crouse, Yara Rizk, Kelsey Bradford, Asim Munawar, Sadhana Kumaravel, Saurabh Goyal, arXiv:2409.037972024barXiv preprint</p>
<p>. Pratik Bhavsar, 2025Agent leaderboard</p>
<p>Practical OpenTelemetry. Daniel Gomez, Blanco , 2023Springer</p>
<p>Super: Evaluating agents on setting up and executing tasks from research repositories. Ben Bogin, Kejuan Yang, Shashank Gupta, Kyle Richardson, Erin Bransom, Peter Clark, Ashish Sabharwal, Tushar Khot, arXiv:2409.074402024Preprint</p>
<p>Workarena++: Towards compositional planning and reasoning-based common knowledge work tasks. Léo Boisvert, Megh Thakkar, Maxime Gasse, Massimo Caccia, Quentin Thibault De Chezelles, Nicolas Cappart, Alexandre Chapados, Alexandre Lacoste, Drouin, Advances in Neural Information Processing Systems. 202537</p>
<p>. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba, arXiv:1606.015402016Openai gym. arXiv preprint</p>
<p>MultiWOZ -a largescale multi-domain Wizard-of-Oz dataset for taskoriented dialogue modelling. Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Iñigo Casanueva, Stefan Ultes, Milica Osman Ramadan, Gašić, 10.18653/v1/D18-1547Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>David Castillo-Bolado, Joseph Davidson, Finlay Gray, Marek Rosa, arXiv:2409.20222Beyond prompts: Dynamic conversational benchmarking of large language models. 2024aarXiv preprint</p>
<p>Beyond prompts: Dynamic conversational benchmarking of large language models. David Castillo-Bolado, Joseph Davidson, Finlay Gray, Marek Rosa, The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024b</p>
<p>Automated focused feedback generation for scientific writing assistance. Eric Chamoun, Michael Schlichtkrull, Andreas , 10.18653/v1/2024.findings-acl.580Findings of the Association for Computational Linguistics: ACL 2024. Bangkok, ThailandAssociation for Computational LinguisticsVlachos. 2024</p>
<p>Action-based conversations dataset: A corpus for building more in-depth taskoriented dialogue systems. Derek Chen, Howard Chen, Yi Yang, Alexander Lin, Zhou Yu, 10.18653/v1/2021.naacl-main.239Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics2021a</p>
<p>Fotios Chantzis. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mo Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, David W Cummings, Matthias Plappert, ArXiv, abs/2107.03374Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter WelinderJan LeikeElizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, Suchir Balaji, Shantanu Jain, Andrew Carr; Bob McGrew, Dario Amodei, Sam McCandlishIlya Sutskever, and Wojciech Zaremba. 2021b. Evaluating large language models trained on code</p>
<p>Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery. Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, Huan Sun, arXiv:2410.050802024Preprint</p>
<p>Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery. Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, Huan Sun, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Llf-bench: Benchmark for interactive learning from language feedback. Ching-An Cheng, Andrey Kolobov, Dipendra Misra, Allen Nie, Adith Swaminathan, arXiv:2312.068532023Preprint</p>
<p>Rim Assouel, et al. 2024. The browsergym ecosystem for web agent research. De Chezelles, Le Thibault, Maxime Sellier, Alexandre Gasse, Alexandre Lacoste, Massimo Drouin, Léo Caccia, Megh Boisvert, Tom Thakkar, Marty, arXiv:2412arXiv preprint</p>
<p>Think you have solved question answering? try arc, the ai2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, arXiv:1803.054572018aarXiv preprint</p>
<p>Think you have solved question answering? try arc, the ai2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, arXiv:1803.054572018bPreprint</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>A dataset of information-seeking questions and answers anchored in research papers. Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith, Matt Gardner, arXiv:2105.030112021Preprint</p>
<p>Mosaic ai agent evaluation: Assessing ai application performance. Databricks, 2023</p>
<p>Mind2web: Towards a generalist agent for the web. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, Yu Su, Advances in Neural Information Processing Systems. 202336</p>
<p>Ms2: Multi-document summarization of medical studies. Jay Deyoung, Iz Beltagy, Madeleine Van Zuylen, Bailey Kuehl, Lucy Lu, Wang , arXiv:2104.064862021Preprint</p>
<p>Autogen studio: A no-code developer tool for building and debugging multi-agent systems. Victor Dibia, Jingya Chen, Gagan Bansal, Suff Syed, Adam Fourney, Erkang Zhu, Chi Wang, Saleema Amershi, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations2024</p>
<p>Alexandre Drouin, Maxime Gasse, Massimo Caccia, H Issam, Manuel Laradji, Tom Del Verme, Léo Marty, Megh Boisvert, Quentin Thakkar, David Cappart, Vazquez, arXiv:2403.07718Workarena: How capable are web agents at solving common knowledge work tasks?. 2024arXiv preprint</p>
<p>Introducing agentic evaluations. Galileo, 2025</p>
<p>Large language models empowered agent-based modeling and simulation: A survey and perspectives. Chen Gao, Xiaochong Lan, Nian Li, Yuan Yuan, Jingtao Ding, Zhilun Zhou, Fengli Xu, Yong Li, arXiv:2312.119702023aPreprint</p>
<p>Retrieval-augmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Haofen Wang, Haofen Wang, arXiv:2312.109972023barXiv preprint</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, Transactions of the Association for Computational Linguistics. 92021</p>
<p>Evaluate your ai agents with vertex gen ai evaluation service. Google Cloud, 2025</p>
<p>Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun, Yang Liu, arXiv:2403.07714Stabletoolbench: Towards stable large-scale benchmarking on tool learning of large language models. 2024arXiv preprint</p>
<p>Lewis Hammond, Alan Chan, Jesse Clifton, Jason Hoelscher-Obermaier, Akbir Khan, Euan Mclean, Chandler Smith, Wolfram Barfuss, Jakob Foerster, Tomáš Gavenčiak, arXiv:2502.14143Multi-agent risks from advanced ai. 2025arXiv preprint</p>
<p>Folio: Natural language reasoning with first-order logic. Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, E Zubova, Yujie Qiao, Matthew Burtell, David Peng, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Shafiq R Joty, Alexander R Fabbri, Wojciech Kryscinski, Xi Victoria Lin, Caiming Xiong, Dragomir R Radev, 2022. 2024</p>
<p>P-folio: Evaluating and improving logical reasoning with abundant human-written reasoning chains. Simeng Han, Aaron Yu, Rui Shen, Zhenting Qi ; Wenfei, Yujie Zhou, Yilun Qiao, Semih Zhao, Ye Yavuz, Shafiq Liu, Yingbo Joty, Caiming Zhou, Dragomir R Xiong, Rex Radev, Arman Ying, Cohan, Martin Riddell,. 2024EMNLP 2024 Findings</p>
<p>Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, Dong Yu, arXiv:2401.13919Webvoyager: Building an end-toend web agent with large multimodal models. 2024arXiv preprint</p>
<p>Measuring coding challenge competence with apps. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, Jacob Steinhardt, arXiv:2105.099382021aPreprint</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.03874arXiv:2310.01798Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou2021b. 2024PreprintLarge language models cannot self-correct reasoning yet</p>
<p>Crmarena: Understanding the capacity of llm agents to perform professional crm tasks in realistic environments. Kung-Hsiang Huang, Akshara Prabhakar, Sidharth Dhawan, Yixin Mao, Huan Wang, Silvio Savarese, Caiming Xiong, Philippe Laban, Chien-Sheng Wu, arXiv:2411.023052025Preprint</p>
<p>Episodic memories generation and evaluation benchmark for large language models. Alexis Huet, Zied Ben Houidi, Dario Rossi, arXiv:2501.131212025Preprint</p>
<p>Livecodebench: Holistic and contamination free evaluation of large language models for code. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, arXiv:2403.079742024PreprintArmando Solar-Lezama, Koushik Sen, and Ion Stoica</p>
<p>Discoveryworld: A virtual environment for developing and evaluating automated scientific discovery agents. Peter Jansen, Marc-Alexandre Côté, Tushar Khot, Erin Bransom, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Oyvind Tafjord, Peter Clark, arXiv:2406.067692024Preprint</p>
<p>Saurabh Jha, Rohan Arora, Yuji Watanabe, Takumi Yanagawa, Yinfang Chen, Jackson Clark, Bhavya Bhavya, Mudit Verma, Harshit Kumar, Hirokuni Kitahara, arXiv:2502.05352Itbench: Evaluating ai agents across diverse real-world it automation tasks. 2025arXiv preprint</p>
<p>Swe-bench: Can language models resolve real-world github issues?. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, ArXiv, abs/2310.067702023Ofir Press, and Karthik Narasimhan</p>
<p>Omniact: A dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web. Raghav Kapoor, Parag Yash, Melisa Butala, Jing Russak, Kiran Yu Koh, Waseem Kamble, Ruslan Alshikh, Salakhutdinov, 10.1007/978-3-031-73113-6_10Computer Vision -ECCV 2024: 18th European Conference. Milan, Italy; BerlinSpringer-Verlag2024a. September 29-October 4, 2024Heidelberg</p>
<p>Sayash Kapoor, Benedikt Stroebl, Zachary S Siegel, Nitya Nadgir, Arvind Narayanan, arXiv:2407.01502Ai agents that matter. 2024barXiv preprint</p>
<p>Mastering multi-agent evaluation systems in 2025. Aryan Kargwal, 2025Botpress Blog</p>
<p>Mrkl systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning. Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, arXiv:2205.004452022arXiv preprint</p>
<p>Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, Dan Roth, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesLong Papers20181</p>
<p>The narrativeqa reading comprehension challenge. Tomáš Kočiskỳ, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, Edward Grefenstette, Transactions of the Association for Computational Linguistics. 62018</p>
<p>Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, Daniel Fried, arXiv:2401.13649Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. 2024arXiv preprint</p>
<p>Acpbench: Reasoning about action, change, and planning. Harsha Kokel, Michael Katz, Kavitha Srinivas, Shirin Sohrabi, arXiv:2410.056692024arXiv preprint</p>
<p>Ds-1000: A natural and reliable benchmark for data science code generation. Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott Yih, Daniel Fried, Si Yi Wang, Tao Yu, International Conference on Machine Learning. 2022</p>
<p>Agentevals: Evaluating agent trajectories. Langchain, 2025</p>
<p>Langsmith: Evaluation framework for ai applications. Inc Langchain, 2023</p>
<p>Langfuse: Observability for ai applications. Langfuse, 2023</p>
<p>Lab-bench: Measuring capabilities of language models for biology research. Jon M Laurent, Joseph D Janizek, Michael Ruzo, Michaela M Hinks, Michael J Hammerling, Siddharth Narayanan, Manvitha Ponnapati, Andrew D White, Samuel G Rodriques, arXiv:2407.103622024Preprint</p>
<p>A human-inspired reading agent with gist memory of very long contexts. Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John Canny, Ian Fischer, arXiv:2402.097272024Preprint</p>
<p>Qasa: advanced question answering on scientific articles. Yoonjoo Lee, Kyungjae Lee, Sunghyun Park, Dasol Hwang, Jaehyeon Kim, arXiv:2501.11067Proceedings of the 40th International Conference on Machine Learning, ICML'23. JMLR.org. Elad Levi and Ilan Kadar. 2025a. Intellagent: A multiagent framework for evaluating conversational ai systems. the 40th International Conference on Machine Learning, ICML'23. JMLR.org. Elad Levi and Ilan Kadar. 2025a. Intellagent: A multiagent framework for evaluating conversational ai systems2023PreprintHong-in Lee, and Moontae Lee</p>
<p>Intellagent: A multiagent framework for evaluating conversational ai systems. Elad Levi, Ilan Kadar, arXiv:2501.110672025barXiv preprint</p>
<p>Ido Levy, Ben Wiesel, Sami Marreed, Alon Oved, Avi Yaeli, Segev Shlomov, arXiv:2410.06703Stwebagentbench: A benchmark for evaluating safety and trustworthiness in web agents. 2024arXiv preprint</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in neural information processing systems. 202033</p>
<p>Reflection-bench: probing ai intelligence with reflection. Lingyu Li, Yixu Wang, Haiquan Zhao, Shuqi Kong, Yan Teng, Chunbo Li, Yingchun Wang, arXiv:2410.162702024Preprint</p>
<p>Api-bank: A comprehensive benchmark for tool-augmented llms. Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, Yongbin Li, arXiv:2304.082442023Preprint</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, 10.18653/v1/P17-1015Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20171</p>
<p>Reinforcement learning on web interfaces using workflow-guided exploration. Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, Percy Liang, arXiv:1802.088022018Preprint</p>
<p>Self-reflection makes large language models safer, less biased, and ideologically neutral. Fengyuan Liu, Nouar Aldahoul, Gregory Eady, Yasir Zaki, Talal Rahwan, arXiv:2406.104002025Preprint</p>
<p>From llm to conversational agent: A memory enhanced architecture with fine-tuning of large language models. Na Liu, Liangyu Chen, Xiaoyu Tian, Wei Zou, Kaijiang Chen, Ming Cui, ArXiv, abs/2401.027772024a</p>
<p>Lost in the middle: How language models use long contexts. Kevin Nelson F Liu, John Lin, Ashwin Hewitt, Michele Paranjape, Fabio Bevilacqua, Percy Petroni, Liang, Transactions of the Association for Computational Linguistics. 122024b</p>
<p>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, arXiv:2308.03688Agentbench: Evaluating llms as agents. 2023aarXiv preprint</p>
<p>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Yuxian Gu, Hangliang Ding, Kai Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Shengqi Shen, Tianjun Zhang, Sheng Shen, Yu Su, Huan Sun, Minlie Huang, ArXiv, abs/2308.03688Yuxiao Dong, and Jie Tang. 2023b. Agentbench: Evaluating llms as agents. </p>
<p>Apigen: Automated pipeline for generating verifiable and diverse function-calling datasets. Zuxin Liu, Thai Hoang, Jianguo Zhang, Ming Zhu, Tian Lan, Juntao Tan, Weiran Yao, Zhiwei Liu, Yihao Feng, R N Rithesh, Advances in Neural Information Processing Systems. 2024c37</p>
<p>Renze Lou, Hanzi Xu, Sijia Wang, Jiangshu Du, Ryo Kamoi, Xiaoxin Lu, Jian Xie, Yuxuan Sun, Yusen Zhang, Jihyun Janice Ahn, Hongchao Fang, Zhuoyang Zou, Wenchao Ma, Xi Li, Kai Zhang, arXiv:2410.22394Congying Xia, Lifu Huang, and Wenpeng Yin. 2025. Aaar-1.0: Assessing ai's potential to assist research. Preprint</p>
<p>Toolsandbox: A stateful, conversational, interactive evaluation benchmark for llm tool use capabilities. Jiarui Lu, Thomas Holleis, Yizhe Zhang, Bernhard Aumayer, Feng Nan, Felix Bai, Shuang Ma, Shen Ma, Mengyu Li, Guoli Yin, arXiv:2408.046822024arXiv preprint</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan, arXiv:2209.095132022Preprint</p>
<p>Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Jianfeng Gao, Chameleon: Plug-and-play compositional reasoning with large language models. Advances in Neural Information Processing Systems. 202336</p>
<p>Xing Han Lù, Zdeněk Kasner, Siva Reddy, arXiv:2402.05930Weblinx: Real-world website navigation with multiturn dialogue. 2024arXiv preprint</p>
<p>Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, Yuwei Fang, arXiv:2402.17753Evaluating very long-term conversational memory of llm agents. 2024arXiv preprint</p>
<p>Gaia: a benchmark for general ai assistants. Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann Lecun, Thomas Scialom, arXiv:2311.129832023Preprint</p>
<p>Swe-lancer: Can frontier llms earn $1 million from real-world freelance software engineering?. Samuel Miserendino, Michele Wang, Tejal Patwardhan, Johannes Heidecke, arXiv:2502.121152025arXiv preprint</p>
<p>Swt-bench: Testing and validating real-world bug-fixes with code agents. Niels Mündler, Mark Müller, Jingxuan He, Martin Vechev, Advances in Neural Information Processing Systems. 202437</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, arXiv:2112.09332Webgpt: Browser-assisted questionanswering with human feedback. 2021arXiv preprint</p>
<p>Deepak Nathani, Lovish Madaan, Nicholas Roberts, Nikolay Bashlykov, Ajay Menon, Vincent Moens, Amar Budhiraja, Despoina Magka, Vladislav Vorotilov, Gaurav Chaurasia, arXiv:2502.14499Mlgym: A new framework and benchmark for advancing ai research agents. 2025arXiv preprint</p>
<p>Openai evals: A framework for evaluating large language models. 2023OpenAI</p>
<p>Introducing swe-bench verified. 2024OpenAI</p>
<p>Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, G Shishir, Ion Patil, Joseph E Stoica, Gonzalez, arXiv:2310.08560Memgpt: Towards llms as operating systems. 2024Preprint</p>
<p>Medmcqa : A large-scale multisubject multi-choice dataset for medical domain question answering. Ankit Pal, Logesh Kumar Umapathi, Malaikannan Sankarasubbu, arXiv:2203.143712022Preprint</p>
<p>Ameet Talwalkar, He He, and Valerie Chen. 2025. When benchmarks talk: Re-evaluating code llms with interactive feedback. Jane Pan, Ryan Shar, Jacob Pfau, arXiv:2502.18413Preprint</p>
<p>Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, Yizhe Zhang, arXiv:2412.21139Training software engineering agents and verifiers with swe-gym. 2024aarXiv preprint</p>
<p>Yichen Pan, Dehan Kong, Sida Zhou, Cheng Cui, Yifei Leng, Bing Jiang, Hangyu Liu, Yanyi Shang, Shuyan Zhou, Tongshuang Wu, arXiv:2406.12373Webcanvas: Benchmarking web agents in online environments. 2024barXiv preprint</p>
<p>Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, arXiv:2112.08608He He, et al. 2021. Quality: Question answering with long input texts, yes! arXiv preprint. </p>
<p>Generative agents: Interactive simulacra of human behavior. Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 36th annual acm symposium on user interface software and technology. the 36th annual acm symposium on user interface software and technology2023</p>
<p>Gorilla: Large language model connected with massive apis. Tianjun Shishir G Patil, Xin Zhang, Joseph E Wang, Gonzalez, Advances in Neural Information Processing Systems. 202537</p>
<p>Patronus ai: Automated testing and evaluation platform for generative ai applications. A I Patronus, Inc, 2023</p>
<p>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, arXiv:2307.16789Toolllm: Facilitating large language models to master 16000+ real-world apis. 2023arXiv preprint</p>
<p>Self-reflection in llm agents: Effects on problem-solving performance. Matthew Renze, Erhan Guven, arXiv:2405.066822024Preprint</p>
<p>Identifying the risks of lm agents with an lmemulated sandbox. Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao Zhou, Jimmy Ba, Yann Dubois, Chris J Maddison, Tatsunori Hashimoto, ArXiv, abs/2309.158172023</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, Advances in Neural Information Processing Systems. 202336</p>
<p>World of bits: An open-domain platform for web-based agents. Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, Percy Liang, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningPMLR201770</p>
<p>Reflexion: language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Neural Information Processing Systems. 2023</p>
<p>Alfworld: Aligning text and embodied environments for interactive learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, arXiv:2010.037682021Preprint</p>
<p>Can LLMs generate novel research ideas? a largescale human study with 100+ NLP researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Core-bench: Fostering the credibility of published research through a computational reproducibility agent benchmark. Zachary S Siegel, Sayash Kapoor, Nitya Nagdir, Benedikt Stroebl, Arvind Narayanan, arXiv:2409.113632024Preprint</p>
<p>Yifan Song, Weimin Xiong, Dawei Zhu, Wenhao Wu, Han Qian, Mingbo Song, Hailiang Huang, Cheng Li, Ke Wang, Rong Yao, arXiv:2306.06624Restgpt: Connecting large language models with real-world restful apis. 2023arXiv preprint</p>
<p>Musr: Testing the limits of chain-of-thought with multistep soft reasoning. Zayne Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, Greg Durrett, arXiv:2310.160492023arXiv preprint</p>
<p>Autoplanbench: Automatically generating benchmarks for llm planners from pddl. Katharina Stein, Daniel Fišer, Jörg Hoffmann, Alexander Koller, arXiv:2311.098302023arXiv preprint</p>
<p>Hal: A holistic agent leaderboard for centralized and reproducible agent evaluation. Benedikt Stroebl, Sayash Kapoor, Arvind Narayanan, 2025</p>
<p>Adaplanner: Adaptive planning from feedback with language models. Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, Chao Zhang, arXiv:2305.166532023Preprint</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Ed H Quoc V Le, Denny Chi, Zhou, arXiv:2210.092612022arXiv preprint</p>
<p>Swe-Bench Lite, Swe-bench lite. 2024</p>
<p>Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, Le Sun, arXiv:2306.053012023arXiv preprint</p>
<p>Nexusraven-v2: Surpassing gpt-4 for zero-shot function calling. Nexusflow, Team, 2023</p>
<p>Scicode: A research coding benchmark curated by scientists. Minyang Tian, Luyu Gao, Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji, Kittithat Krongchon, Yao Li, Shengyan Liu, Di Luo, Yutao Ma, Hao Tong, Kha Trinh, Chenyu Tian, Zihan Wang, Bohao Wu, Shengzhu Yin, Minhui Zhu, Kilian Lieret, Yanxin Lu, Genglin Liu, Yufeng Du, Tianhua Tao, The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Jamie CallanEliu A Huerta, and Hao Peng2024a</p>
<p>Minyang Tian, Luyu Gao, Dylan Shizhuo, Xinan Zhang, Cunwei Chen, Xuefei Fan, Roland Guo, Pan Haas, Kittithat Ji, Yao Krongchon, Shengyan Li, Di Liu, Yutao Luo, Hao Ma, Kha Tong, Chenyu Trinh, Zihan Tian, Bohao Wang, Yanyu Wu, Shengzhu Xiong, Minhui Yin, Kilian Zhu, Yanxin Lieret, Genglin Lu, Yufeng Liu, Tianhua Du, Tao, arXiv:2407.13168Eliu Huerta, and Hao Peng. 2024b. Scicode: A research coding benchmark curated by scientists. Jamie CallanOfir PressPreprint</p>
<p>AppWorld: A controllable world of apps and people for benchmarking interactive coding agents. Harsh Trivedi, Tushar Khot, Mareike Hartmann, Ruskin Manku, Vinty Dong, Edward Li, Shashank Gupta, Ashish Sabharwal, Niranjan Balasubramanian, 10.18653/v1/2024.acl-long.850Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change. Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati, Advances in Neural Information Processing Systems. 202336</p>
<p>Sciriff: A resource to enhance language model instruction-following over scientific literature. David Wadden, Kejian Shi, Jacob Morrison, Aakanksha Naik, Shruti Singh, Nitzan Barzilay, Kyle Lo, Tom Hope, Luca Soldaini, Shannon Zejiang Shen, Doug Downey, Hannaneh Hajishirzi, Arman Cohan, arXiv:2406.078352024Preprint</p>
<p>A survey on large language model based autonomous agents. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Frontiers of Computer Science. 1861863452024a</p>
<p>ScienceWorld: Is your agent smarter than a 5th grader?. Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, Prithviraj Ammanabrolu, 10.18653/v1/2022.emnlp-main.775Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022a</p>
<p>Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, Heng Ji, arXiv:2309.10691Mint: Evaluating llms in multi-turn interaction with tools and language feedback. 2023arXiv preprint</p>
<p>Self-instruct: Aligning language models with self-generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, arXiv:2212.105602022barXiv preprint</p>
<p>Execution-based evaluation for opendomain code generation. Zhiruo Wang, Shuyan Zhou, Daniel Fried, Graham Neubig, Conference on Empirical Methods in Natural Language Processing. 2022c</p>
<p>Karma: Augmenting embodied ai agents with long-and-short term memory systems. Zixuan Wang, Bo Yu, Junzhe Zhao, Wenhao Sun, Sai Hou, Shuai Liang, Xing Hu, Yinhe Han, Yiming Gan, arXiv:2409.149082024bPreprint</p>
<p>Seal-tools: Self-instruct tool learning dataset for agent tuning and detailed benchmark. Cheng-Kuang Wu, Zhi Rui Tam, Chieh-Yen Lin, Yun-Nung Chen, Hung-Yi Lee ; Mengsong Wu, Tong Zhu, Han Han, Chuanyuan Tan, Xiang Zhang, Wenliang Chen, arXiv:2406.08747CCF International Conference on Natural Language Processing and Chinese Computing. Springer2024a. 2024barXiv preprintStreambench: Towards benchmarking continuous improvement of language agents</p>
<p>Agentless: Demystifying llm-based software engineering agents. Chun Xia, Yinlin Deng, Soren Dunn, Lingming Zhang, ArXiv, abs/2407.014892024</p>
<p>Ruixuan Xiao, Wentao Ma, Ke Wang, Yuchuan Wu, Junbo Zhao, Haobo Wang, Fei Huang, Yongbin Li, arXiv:2406.14884Flowbench: Revisiting and benchmarking workflow-guided planning for llm-based agents. 2024arXiv preprint</p>
<p>OSWorld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Jing Toh, Zhoujun Hua, Dongchan Cheng, Fangyu Shin, Yitao Lei, Yiheng Liu, Shuyan Xu, Silvio Zhou, Caiming Savarese, Victor Xiong, Tao Zhong, Yu, The Thirtyeight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024</p>
<p>Shuyan Zhou, and Graham Neubig. F Frank, Yufan Xu, Boxuan Song, Yuxuan Li, Kritanjali Tang, Mengxue Jain, Zora Z Bao, Xuhui Wang, Zhitong Zhou, Murong Guo, Mingyang Cao, Yang, Yang Hao, Amaad Lu, Zhe Martin, Leander Su, Raj Maben, Wayne Mehta, Lawrence Chi, Yiqing Jang, Xie, arXiv:2412.14161Theagentcompany: Benchmarking llm agents on consequential real world tasks. 2024Preprint</p>
<p>Beyond goldfish memory: Long-term open-domain conversation. Jing Xu, Arthur Szlam, Jason Weston, arXiv:2107.075672021arXiv preprint</p>
<p>Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, Jian Zhang, arXiv:2305.16504On the tool manipulation capability of open-source large language models. 2023arXiv preprint</p>
<p>A-mem: Agentic memory for llm agents. Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, Yongfeng Zhang, arXiv:2502.121102025Preprint</p>
<p>Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie, Tianjun Ji, Zhang, G Shishir, Ion Patil, Joseph E Stoica, Gonzalez, Berkeley function calling leaderboard. 2024</p>
<p>Swe-bench multimodal: Do ai systems generalize to visual software domains?. John Yang, Carlos E Jimenez, Alex L Zhang, Kilian Lieret, Joyce Yang, Xindi Wu ; Muennighoff, Gabriele Synnaeve, R Karthik, Diyi Narasimhan, Yang, ArXiv, abs/2410.038592024Sida I. Wang, and Ofir PressNiklas</p>
<p>Hotpotqa: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning, arXiv:1809.096002018Preprint</p>
<p>Webshop: Towards scalable realworld web interaction with grounded language agents. Shunyu Yao, Howard Chen, John Yang, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202235</p>
<ol>
<li>τ -bench: A benchmark for tool-agent-user interaction in real-world domains. Shunyu Yao, Noah Shinn, Pedram Razavi, Karthik Narasimhan, arXiv:2406.12045Preprint</li>
</ol>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in neural information processing systems. 202336</p>
<p>Ori Yoran, Samuel Joseph Amouyal, Chaitanya Malaviya, Ben Bogin, arXiv:2407.15711Ofir Press, and Jonathan Berant. 2024. Assistantbench: Can web agents solve realistic and time-consuming tasks?. arXiv preprint</p>
<p>LLM-evolve: Evaluation for LLM's evolving capability on benchmarks. Jiaxuan You, Mingjie Liu, Shrimai Prabhumoye, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, 10.18653/v1/2024.emnlp-main.940Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, arXiv:1809.088872018arXiv preprint</p>
<p>Daoguang Zan, Zhirong Huang, Ailun Yu, Shaoxin Lin, Yifan Shi, Wei Liu, Dong Chen, Zongshuai Qi, Hao Yu, Lei Yu, arXiv:2408.14354Swe-bench-java: A github issue resolving benchmark for java. 2024arXiv preprint</p>
<p>Ziniu Zhang, Shulin Tian, Liangyu Chen, Ziwei Liu, arXiv:2404.09992Mmina: Benchmarking multihop multimodal internet agents. 2024arXiv preprint</p>
<p>Swaroop Huaixiu Steven Zheng, Hugh Mishra, Xinyun Zhang, Minmin Chen, Azade Chen, Le Nova, Hou, Heng-Tze, Quoc V Cheng, Ed H Le, Chi, arXiv:2406.04520Natural plan: Benchmarking llms on natural language planning. 2024arXiv preprint</p>
<p>Complexfuncbench: Exploring multi-step and constrained function calling under long-context scenario. Lucen Zhong, Zhengxiao Du, Xiaohan Zhang, Haiyi Hu, Jie Tang, arXiv:2501.101322025arXiv preprint</p>
<p>Qmsum: A new benchmark for query-based multidomain meeting summarization. Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, arXiv:2104.059382021arXiv preprint</p>
<p>Agieval: A humancentric benchmark for evaluating foundation models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, arXiv:2304.063642023Preprint</p>
<p>Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, arXiv:2307.13854Webarena: A realistic web environment for building autonomous agents. 2023arXiv preprint</p>
<p>Mingchen Zhuge, Changsheng Zhao, Dylan Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong, Zechun Liu, Ernie Chang, Raghuraman Krishnamoorthi, Yuandong Tian, arXiv:2410.10934Agent-as-ajudge: Evaluate agents with agents. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
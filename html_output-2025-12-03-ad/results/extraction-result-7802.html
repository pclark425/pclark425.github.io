<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7802 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7802</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7802</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-143.html">extraction-schema-143</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fineâ€‘tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <p><strong>Paper ID:</strong> paper-7e17ef56273063dfa838de30b7cc0546b2e5ee10</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7e17ef56273063dfa838de30b7cc0546b2e5ee10" target="_blank">Jellyfish: A Large Language Model for Data Preprocessing</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper considers instruction-tuning local LLMs (7 -- 13B models) as universal DP task solvers that operate on a local, single, and low-priced GPU, ensuring data security and enabling further customization.</p>
                <p><strong>Paper Abstract:</strong> This paper explores the utilization of LLMs for data preprocessing (DP), a crucial step in the data mining pipeline that transforms raw data into a clean format conducive to easy processing. Whereas the use of LLMs has sparked interest in devising universal solutions to DP, recent initiatives in this domain typically rely on GPT APIs, raising inevitable data breach concerns. Unlike these approaches, we consider instruction-tuning local LLMs (7 -- 13B models) as universal DP task solvers that operate on a local, single, and low-priced GPU, ensuring data security and enabling further customization. We select a collection of datasets across four representative DP tasks and construct instruction tuning data using data configuration, knowledge injection, and reasoning data distillation techniques tailored to DP. By tuning Mistral-7B, Llama 3-8B, and OpenOrca-Platypus2-13B, our models, namely, Jellyfish-7B/8B/13B, deliver competitiveness compared to GPT-3.5/4 models and strong generalizability to unseen tasks while barely compromising the base models' abilities in NLP tasks. Meanwhile, Jellyfish offers enhanced reasoning capabilities compared to GPT-3.5. Our models are available at: https://huggingface.co/NECOUDBFM/Jellyfish . Our instruction dataset is available at: https://huggingface.co/datasets/NECOUDBFM/Jellyfish-Instruct .</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7802",
    "paper_id": "paper-7e17ef56273063dfa838de30b7cc0546b2e5ee10",
    "extraction_schema_id": "extraction-schema-143",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.005301999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Jellyfish: A Large Language Model for Data Preprocessing <br> ${ }^{1}$ Haochen Zhang, ${ }^{2}$ Yuyang Dong, ${ }^{1,3}$ Chuan Xiao, ${ }^{2}$ Masafumi Oyamada <br> ${ }^{1}$ Osaka University, ${ }^{2}$ NEC Corporation, ${ }^{3}$ Nagoya University <br> {chou.koushin, chuanx}@ist.osaka-u.ac.jp, {dongyuyang, oyamada}@nec.com</h1>
<h4>Abstract</h4>
<p>This paper explores the utilization of LLMs for data preprocessing (DP), a crucial step in the data mining pipeline that transforms raw data into a clean format conducive to easy processing. Whereas the use of LLMs has sparked interest in devising universal solutions to DP, recent initiatives in this domain typically rely on GPT APIs, raising inevitable data breach concerns. Unlike these approaches, we consider instruction-tuning local LLMs (7 - 13B models) as universal DP task solvers that operate on a local, single, and low-priced GPU, ensuring data security and enabling further customization. We select a collection of datasets across four representative DP tasks and construct instruction tuning data using data configuration, knowledge injection, and reasoning data distillation techniques tailored to DP. By tuning Mistral-7B, Llama 3-8B, and OpenOrca-Platypus2-13B, our models, namely, Jellyfish-7B/8B/13B, deliver competitiveness compared to GPT-3.5/4 models and strong generalizability to unseen tasks while barely compromising the base models' abilities in NLP tasks. Meanwhile, Jellyfish offers enhanced reasoning capabilities compared to GPT-3.5.</p>
<h2>1 Introduction</h2>
<p>The proliferation of large language models (LLMs) has catalyzed a diverse array of applications, extending beyond the domain of NLP to encompass a wide range of fields that require the processing of natural language data. Notably, LLMs have been applied in areas such as software engineering [77, 90], computer simulation [104, 23], data analytics [8, 85], and tabular data processing [52, 62, 112].</p>
<p>This paper focuses on the utilization of LLMs for data preprocessing (DP), a critical step in the data mining pipeline that involves transforming raw data into a manageable and processable format ready for use. Over the past decades, significant strides have been made in various DP tasks. Until 2021, most efforts were concentrated on one or two specific tasks such as error detection (ED) [27, 65], data imputation (DI) [80, 64, 66], schema matching (SM) [110], and entity matching (EM) [42, 55]. A key challenge in developing generic solutions to DP is that these tasks differ in nature: they deal with errors, anomalies, matches, etc. and require different actions such as detection, repairing, and alignment.</p>
<p>With the advent of LLMs like GPT-3 and subsequent versions, researchers have found a key to address this challenge, spurring the development of generic solutions for a wider array of DP tasks [70, 109]. The application of LLMs in DP has the following strengths: (1) The primary strengths of using LLMs in DP lie in their ability to process natural language. Most LLMs provide a prompting interface with which users can interact and assign tasks in natural language, contrasting with existing DP solutions that require computer programming or specific tools (e.g., HoloClean [80] and Magellan [42]). (2) With the knowledge acquired through training on vast amounts of data, LLMs are universal problem</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of instruction tuning for data preprocessing.</p>
<p>Solvers capable of identifying errors, anomalies, and matches in data (and particularly unseen datasets in unseen tasks), aligning with the aims of DP tasks without needing human-engineered rules [79]. (3) LLMs are excellent reasoners [41], enabling them to not only return DP results but also provide the reasons for these results. In this sense, their answers are more interpretable than those of other deep learning approaches. (4) LLMs can be conditioned by few- [5] or zero-shot [41] prompting. As such, we can condition the criteria for DP tasks (e.g., the degree of matching) using few-shot examples or zero-shot prompts, contrasting with traditional solutions based on a threshold [83, 42] or a time-consuming training process to fit to the data [66].</p>
<p>Despite these strengths, existing LLM-based solutions to DP [70, 109, 43], with reliance on GPT APIs, have raised concerns about data breaches, as evidenced by OpenAI's first confirmed data breach involving ChatGPT [73]. Another limitation is the difficulty in domain specification [70]. When dealing with data from highly specialized domains, training the LLMs used in these solutions can be costly (e.g., GPT-3.5) and even unavailable due to frozen parameters (e.g., GPT-4), posing difficulty in customizing the model.</p>
<p>In response to these challenges, we propose to construct instruction data and tune LLMs for various DP tasks. The tuned model, namely Jellyfish, distinguishes itself with several key features:</p>
<ul>
<li><strong>Versatility:</strong> Jellyfish is a universal DP task solver tuned to the following tasks: ED and DI for data cleaning, and SM and EM for data integration.</li>
<li><strong>Cost-Efficiency and Security:</strong> Varying from 7B to 13B, Jellyfish can operate on a local, single, and low-priced GPU, ensuring data security and allowing further tuning.</li>
<li><strong>Customizability:</strong> Capable of understanding natural language, Jellyfish allows users to manually craft instructions for DP tasks (or simply use our prompts in this paper) and apply prompt engineering techniques to tailor it to specific tasks and datasets.</li>
<li><strong>Domain Knowledge:</strong> Unlike many existing methods that rely heavily on handcrafted knowledge during inference [80, 78], Jellyfish features domain knowledge in its instruction tuning and enables optional knowledge injection during inference.</li>
<li><strong>Interpretability:</strong> By employing reasoning data in its instruction tuning, Jellyfish's interpretation ability provides natural language explanations of its outputs.</li>
</ul>
<p>Whereas instruction tuning of LLMs has been largely used for unstructured text [111], the construction of Jellyfish is non-trivial in the sense that (1) it tunes for structured data, (2) it finds a good data configuration for various DP tasks, and (3) it specifies domain knowledge that can be applied to unseen datasets. Besides, it is expected that the model's performance in NLP tasks can be preserved for generalizability and further customization. To the best of our knowledge, this is the first study that investigates instruction tuning for DP with LLMs as universal solutions.</p>
<p>As depicted in Figure 1, Jellyfish is constructed by carefully selecting data from several public datasets widely used for DP evaluation, considering their impacts on the overall performance.</p>
<p>instance serialization, raw data is serialized into instruction tuning prompts. By knowledge injection, task- and dataset-specific knowledge - particularly domain knowledge that can be extended to unseen datasets - is infused to the prompts. Moreover, we resort to Mixtral-8x7B-Instruct-v0.1 to generate reasoning data. As such, Jellyfish distills Mixtral's knowledge in reasoning DP results.</p>
<p>Our evaluation focuses on tuning a set of prevalent open LLMs, including Mistral-7B-Instruct-v0.2 (as Jellyfish-7B), Llama 3-8B (as Jellyfish-8B), and OpenOrca-Platypus2-13B (as Jellyfish-13B). The results show that our instruction data applies to all these base models, substantially improving the DP performance. Compared to two categories of baseline methods, (1) non-LLM methods - typically solutions based on machine learning (ML) or pre-trained language models (PLMs) - and (2) LLM methods - typically GPT series methods, Jellyfish-13B consistently outperforms non-LLM methods on its seen datasets, and its effectiveness on unseen datasets even surpasses non-LLM methods on their respective seen datasets. Meanwhile, Jellyfish-7B/8B also exhibit competitiveness, especially on DI and EM tasks. For unseen tasks, Jellyfish models also deliver strong performance, rivaling GPT-3.5/4 models and showcasing generalizability to a wider range of DP tasks beyond the four tasks used for tuning. Our evaluation reveals the impacts of data configuration and the use of reasoning data in building Jellyfish, and discovers that Jellyfish barely compromises the base model's NLP performance. Furthermore, experiments demonstrate the advantage of Jellyfish's interpretation over GPT-3.5 in reasoning capabilities as well as the effectiveness of knowledge injection.</p>
<p>Our contributions are summarized as follows.</p>
<ul>
<li>We develop Jellyfish, an instruction-tuned LLM as a universal DP task solver.</li>
<li>Jellyfish showcases several notable features: universal model design, moderate model size, assurance of data security, feasibility for further tuning, natural language instruction handling, optional specification of prior knowledge, and model interpretability.</li>
<li>Our experiments demonstrate Jellyfish-7B, 8B, and 13B models' effectiveness in DP task solving, generalizability to new tasks beyond what they are tuned for, and superior reasoning abilities.</li>
</ul>
<p>The rest of the paper is organized as follows: Section 2 introduces the DP tasks targeted by our model and briefly reviews LLMs. Section 3 describes the instruction data for tuning Jellyfish. Section 4 introduces how to use Jellyfish for solving DP tasks. Section 5 discusses the extensions to unseen tasks. Section 6 reports experimental results and analysis. Section 7 reviews related works on DP. Section 8 concludes this paper.</p>
<h1>2 Preliminaries</h1>
<h3>2.1 Data Preprocessing</h3>
<p>In data mining, DP is a crucial step that deals with noise, missing values, inconsistencies, and heterogeneity in data. Major DP procedures include data cleaning, data integration, data transformation, and data reduction [25]. In this initial exploration of LLMs for DP, we concentrate on tabular data, one of the most common data types.</p>
<p>Our data model operates on relational tables specified by schemas. We assume all attributes are either numerical (including binary) or textual (including categorical) values. Diverging from the traditional definition that presents the entire dataset and finds or fixes all the errors (or matches, etc.) within, we define the problem by handling one record (or a pair, depending on the task) at a time, so the prompt can be easily written and its length is within LLMs' token limitation. Next, we outline the DP tasks involved in this study:</p>
<ul>
<li>
<p>Error Detection (ED): Given a record (i.e., a tuple in a relational table) and an attribute, our task is to detect whether there is an error in the cell value of this attribute.</p>
</li>
<li>
<p>Data Imputation (DI): Given a record and an attribute such that cell value for this attribute is missing, our task is to infer its correct value.</p>
</li>
<li>Schema Matching (SM): Given a pair of attributes represented in the form of (name, description), our task is to find whether they refer to the same attribute.</li>
<li>Entity Matching (EM): Given a pair of records, our task is to infer whether they refer to the same entity.</li>
</ul>
<p>These four tasks form the most critical part of DP [70, 109] and are extensively discussed in the context of data mining [25]. We use them for instruction tuning. Besides, we consider two unseen tasks:</p>
<ul>
<li>Column Type Annotation (CTA): Given a table with no header, our task is to infer the type of each column from a set of predefined types (e.g., name, time, location).</li>
<li>Attribute Value Extraction (AVE): Given a text description of an entity and a set of predefined attributes, the task is to extract attribute values from the text description.</li>
</ul>
<p>We term each input object an instance, i.e., a record for ED and DI, a pair of attributes for SM, a pair of records for EM, a table or a column for CTA, and a text description for AVE.</p>
<h1>2.2 Large Language Models</h1>
<p>With advancements in the field of natural language processing (NLP), LLMs have become one of the hottest topics in the AI research community. Representative LLMs include OpenAI's GPT series (in particular, GPT-3, 3.5, and 4), Anthropic's Claude, Google's Gemini, Mistral AI's Mistral [36], Meta's Llama [93] series, as well as their variants that can be found at Hugging Face [34]. Due to their superb ability to process natural language, LLMs have not only been used in NLP applications (e.g., ChatGPT and Claude), but also catalyzed the rise of LLM-powered autonomous agents [97] as AI assistants (e.g., by GPTs) or tools for engineering [77, 30] or simulation [105, 104] purposes. Another popular LLM-centric research direction is retrieval-augmented generation (RAG) [50, 51], which gives LLMs access to external information to improve generation performance. We refer readers to [115] for a survey on LLMs. Some LLMs are open-source (e.g., Llama and Llama 2), and they can be fine-tuned with additional tasks to improve their abilities in logical reasoning, question answering, and so on. Among these fine-tuning approaches, instruction tuning [111] has become a prevalent one which further trains LLMs on a dataset consisting of (instruction, output) pairs in a supervised fashion, hence bridging the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. For efficiency of fine-tuning, parameter-efficient fine-tuning (PEFT) approaches enable adaptation of LLMs to downstream applications without fine-tuning all the parameters. Notable methods are adapter tuning [31], prefix-tuning [54], and low-rank adaptation (LoRA) [32]. In particular, LoRA achieves significantly fewer trainable parameters and no additional inference latency, and has become a prevalent PEFT approach.</p>
<p>In addition to the strengths outlined in Section 1, we discuss the limitations of LLMs in the context of DP: (1) LLMs often require substantial computational resources, thereby increasing the cost of use and compromising the efficiency and scalability of DP on large-scale data. (2) Due to token limitation (the maximum input length, e.g., 4 k tokens for GPT-3.5) and lack of memory for keeping historical information, the input to the LLM is often instance-by-instance, and the DP results may exhibit inconsistency across different instances. Simply raising the token limitation (e.g., 128 k tokens for GPT-4-turbo) does not solve the problem, because performance may degrade due to increased lengths of input [58]. (3) LLMs sometimes exhibit hallucination [114], i.e., they generate text that is plausible-sounding but factually incorrect or non-sensical, as they lack a fundamental understanding of the world and rely solely on the patterns they learned during training.</p>
<p>Table 1: DP task data statistics. #Positives denotes the number of instances having an error (for ED) or matching objects (for SM and EM). $\times 2$ denotes duplication of instances for treating missing values as errors or not.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">#Instances</th>
<th style="text-align: center;">#Positives</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ED</td>
<td style="text-align: center;">Adult</td>
<td style="text-align: center;">$550 \times 2$</td>
<td style="text-align: center;">$35 \times 2$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Hospital</td>
<td style="text-align: center;">$1710 \times 2$</td>
<td style="text-align: center;">$44 \times 2$</td>
</tr>
<tr>
<td style="text-align: center;">DI</td>
<td style="text-align: center;">Buy</td>
<td style="text-align: center;">586</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Restaurant</td>
<td style="text-align: center;">778</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;">SM</td>
<td style="text-align: center;">MIMIC-III</td>
<td style="text-align: center;">7000</td>
<td style="text-align: center;">11</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Synthea</td>
<td style="text-align: center;">5000</td>
<td style="text-align: center;">18</td>
</tr>
<tr>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">Amazon-Google</td>
<td style="text-align: center;">6874</td>
<td style="text-align: center;">699</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Beer</td>
<td style="text-align: center;">359</td>
<td style="text-align: center;">54</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DBLP-ACM</td>
<td style="text-align: center;">5000</td>
<td style="text-align: center;">885</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DBLP-GoogleScholar</td>
<td style="text-align: center;">5000</td>
<td style="text-align: center;">924</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fodors-Zagats</td>
<td style="text-align: center;">757</td>
<td style="text-align: center;">88</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">iTunes-Amazon</td>
<td style="text-align: center;">430</td>
<td style="text-align: center;">105</td>
</tr>
</tbody>
</table>
<h1>3 Instruction Tuning of Jellyfish</h1>
<h3>3.1 Dataset Preparation</h3>
<p>For the four seen tasks, we choose a series of datasets that have been widely used in previous studies and cover a variety of application domains. (1) ED: Adult and Hospital [27]; (2) DI: Buy and Restaurant [66]; (3) SM: MIMIC-III and Synthea [110]; (4) EM: Amamzon-Google, Beer, DBLP-ACM, DBLP-GoogleScholar, Fodors-Zagats, and iTunes-Amazon from the Magellan data repository [15]. We use the publicly available version of these datasets [70], where errors and missing values are already injected to the datasets of ED and DI, respectively.</p>
<p>To determine the data size for each task, we first consider a constraint that for fair comparison with non-LLM methods [66, 110, 55], the training data in building Jellyfish does not exceed those used for building these methods, which serve as a pool of 115 k instances. Then, we have the following observations (Section 6.6): (1) The performance of DI can benefit from the other three tasks, but increasing DI data is relatively negative to them. (2) Increasing ED and SM data is generally beneficial to other tasks. (3) Increasing SM data is beneficial to the overall DP performance. (4) Increasing EM data compromises the performance of other tasks, but keeping its size is the key to the EM performance.</p>
<p>Based on these observations, we use all the ED and DI data in the 115 k pool as their sizes are small, and then choose a large data size for SM and a moderate data size for EM. Specifically, we control the data used in large EM datasets (e.g., for DBLP-GoogleScholar, $1 / 3$ is chosen from the pool). As such, we determine the data size for the four tasks, as shown in Table 1.</p>
<p>In addition, we undertake the following efforts to prepare data: (1) Given the disproportionately small number of positive instances compared to negative ones, we incorporate all positive instances available in the datasets. (2) For ED, since missing values can be interpreted as either errors or nonerrors, depending on the context, we create two versions of each instance: one treating missing values as errors and the other as non-errors. The duplication is guided by knowledge injection, which is to be detailed in Section 3.2.</p>
<p>Next, we transform raw data to (1) DP task data, for DP task-solving ability, and (2) DP task with reasoning data, for interpretation ability. They can be jointly used for tuning a Jellyfish model.</p>
<h3>3.2 DP Task Data</h3>
<p>To prepare the DP task data for an LLM, we need to serialize (a.k.a. contextualize) each instance in the raw data to a prompt. The prompt contains the task description, the instance content, and any</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">DP Task Data</th>
<th style="text-align: center;">DP Task with Reasoning Data</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">system <br> message</td>
<td style="text-align: center;">You are an AI assistant that follows instruction extremely well. User will give you a question. Your task is to answer as faithfully as you can.</td>
<td style="text-align: center;">You are an AI assistant that follows instruction extremely well. User will give you a question. Your task is to answer as faithfully as you can. While answering, provide detailed explanation and justify your answer.</td>
</tr>
<tr>
<td style="text-align: center;">task <br> description</td>
<td style="text-align: center;">You are tasked with determining whether two Products listed below are the same based on the information provided. Carefully compare all the attributes before making your decision.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">injected <br> knowledge</td>
<td style="text-align: center;">Note that missing values (N/A or "nan") should not be used as a basis for your decision.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">instance <br> content</td>
<td style="text-align: center;">Product A: [name: "Sequoia American Amber Ale", factory: "Wig And Pen"] Product B: [name: "Aarhus Cains Triple A American Amber Ale", factory: "Aarhus Bryghus"]</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">question</td>
<td style="text-align: center;">Are Product A and Product B the same Product?</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">output <br> format</td>
<td style="text-align: center;">Choose your answer from: [Yes, No]</td>
<td style="text-align: center;">After your reasoning, finish your response in a separate line with and ONLY with your final answer. Choose your final answer from [Yes, No].</td>
</tr>
<tr>
<td style="text-align: center;">answer</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">... the names are not identical, with Product B having ... These are different factories ... they are not the same product. Final answer: No</td>
</tr>
</tbody>
</table>
<p>Figure 2: Example prompt in instruction data. The leftmost column is description and not prompted to the model. Response indicates the answer to the prompt. Detailed prompts are provided in Appendix B.
injected knowledge. To describe our techniques for constructing the DP task data for training, we use an example for an instance in the Beer dataset used for EM, as shown in Figure 2.</p>
<p>At the beginning, there is a system message guiding the model behavior. Here, we instruct the model to act as an AI assistant to answer the user's question, and its response should always respect this constraint. Then, we describe the DP task. The following part refers to injected knowledge. There are two types of injected knowledge: (1) general knowledge that applies to many datasets, and (2) specific knowledge that only applies to the given dataset. In this example, the knowledge belongs to general knowledge and concerns with missing values. Such knowledge injection may prevent the model incorrectly handling certain values in the dataset, especially when training data is noisy. The following part pertain to the instance content. Finally, there is a question presented to the model, and the output format is specified afterwards.</p>
<p>Whereas in the above example we specify knowledge on missing values, there are other forms of general knowledge used in tuning, including error types and terminology. For example, for ED, we inform the model of the fact that errors can include, but are not limited to, spelling errors, inconsistencies, or values that do not make sense for that attribute; for EM, we instruct the model to consider the full name of an attribute and its acronym to determine if the two values are the same. Specific knowledge highly depends on the application domain, mainly including constraints or rules that pertain to the dataset. For example, in publication datasets, authors' names may occur in different forms and different orders even for the same article. Additionally, the model can be configured to assign greater importance to certain attributes. In the context of product data, for example, the model is directed to prioritize the comparison of product numbers. Specific knowledge can be applicable to datasets within the same domain, thereby enhancing the model's performance on unseen datasets, particularly in scenarios where prior knowledge about these datasets is absent. Overall, the knowledge injected through tuning becomes the built-in knowledge of the model and can be used even without user-specification during inference.</p>
<h1>3.3 DP Task with Reasoning Data</h1>
<p>(DP task with) reasoning data, not only empowers the model to interpret the DP results, but also has the potential in enhancing the DP performance in the sense that the model can learn the rationale behind DP, thereby generalizing to unseen scenarios whose underlying logic resembles the tuned</p>
<p>Table 2: Statistics of reasoning data for instruction tuning. We report the numbers of instances for each task. For ED and DI, the numbers refer to the amount after duplicating the instances having missing values.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Task</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>ED</td>
<td>DI</td>
<td>SM</td>
<td>EM</td>
<td>Total</td>
</tr>
<tr>
<td>reasoning-8k</td>
<td>3056</td>
<td>1364</td>
<td>2000</td>
<td>2000</td>
<td>8420</td>
</tr>
<tr>
<td>reasoning-11k</td>
<td>3056</td>
<td>1364</td>
<td>3500</td>
<td>3500</td>
<td>11420</td>
</tr>
<tr>
<td>reasoning-14k</td>
<td>3056</td>
<td>1364</td>
<td>5000</td>
<td>5000</td>
<td>14420</td>
</tr>
<tr>
<td>reasoning-20k</td>
<td>3056</td>
<td>1364</td>
<td>8600</td>
<td>7000</td>
<td>20020</td>
</tr>
</tbody>
</table>
<p>tasks/datasets. On the other hand, due to the small size of local LLMs, tuning the model with excessive reasoning data may compromise its ability to conduct the tuned DP tasks. Thus, we need to strike a balance between DP performance and generalizability. In general, we observe that native models (Mistral and Llama 3) are more likely to benefit from the use of reasoning data (Section 6.7).</p>
<p>Another key feature in our reasoning data is that we resort to a larger open LLM, Mixtral-8x7B-Instruct-v0.1, to retrieve reasoning answers as ground truths. As such, Jellyfish distills Mixtral's knowledge in reasoning for DP. Since this does not involve external APIs like GPT-4, data security can be ensured, in case users want to include confidential information in the reasoning data.</p>
<p>We use the same set of datasets as DP task data to construct the reasoning data. The prompt in reasoning data only differs from DP task data in the reasoning instructions (Figure 2, system message and output format). To retrieve reasoning answers from Mixtral, we add a hint at the end of the prompt for the correct DP result (e.g., "yes/no" for matching tasks), hence to instruct Mixtral to reason in the right direction (Appendix B.2). Note that such hint does not appear in the prompt given to Jellyfish.</p>
<p>To control the size and quality of reasoning data, we select data as follows: (1) For ED and SM, we keep all positive instances due to their small numbers, and then sample negative instances. (2) For DI, we keep all instances due to the small data size. (3) For EM, we sample instances. From the 115k pool, we tune the numbers in the sample to make four sets of reasoning data with $8 \mathrm{k}, 11 \mathrm{k}, 14 \mathrm{k}$, and 20 k instances, respectively (Table 2). Moreover, from the answers returned by Mixtral, we remove low-quality ones that simply rephrase instance contents, as they barely refers to reasoning.</p>
<h1>4 Inference with Jellyfish</h1>
<p>For inference, the prompt is same as the instruction data shown in Figure 2. Users can craft datasetspecific knowledge into the prompt, such as the domain knowledge (e.g., constraints) outlined in the previous section. Such user-specified knowledge is optional.
Feature Engineering. Users can optionally select a subset of features to improve performance. For instance, for EM in the Beer dataset, name and factory are more relevant features, while style and ABV are less relevant. Hence users may choose to use only name and factory as attributes. Such feature engineering can be also implemented in the prompt as specific knowledge, e.g., you should only consider name and factory and ignore other attributes.
Prompt Engineering. Prompt engineering [101] is the process of structuring text to enhance the model performance. We incorporate few-shot prompting [5], which conditions the Jellyfish models to learn from a small selection of examples drawn from the dataset. The prompts for few-shot examples are reported in Appendix D.
Batch Processing. To enable Jellyfish models to perform inference in batches rather than processing single instances individually, we can employ prefix caching [46], available in the vLLM [96] library, because the instructions for the batch share the same prefix, only differing in the instance content.</p>
<h1>5 Extensions to Unseen Tasks</h1>
<p>For unseen tasks, we consider two case studies: CTA and AVE, as outlined in Section 2.1. Jellyfish models can be easily extended to support them by employing the prompt engineering techniques in existing LLM-based solutions, hence simplifying its use in unseen tasks.
Column Type Annotation. As a task in the realm of table understanding, CTA is an essentially DP step for data search [7], knowledge base completion [82], and data integration a data lake [24]. We follow the two-stage pipeline proposed in [43], which was designed for ChatGPT and based on chain-ofthought [100], a technique that enables complex reasoning capabilities through intermediate reasoning steps.</p>
<p>Given a table to be annotated, in the first stage, the model predicts the domain of the table. In the second stage, given a set of predefined types, the model determines the type of column based on sample values extracted from it. The chain-of-thought prompt instructs the model in a step-by-step manner. For example, to predict the domain of the table, there are four steps: (1) look at the input and make a table out of it, (2) look at the cell values in detail, (3) decide if the table describes domain A, domain B ... and (4) answer with the domain. Then, the model follows this prompt to cope with the task. The column type selection in the second stage works in the same way, except that table is replaced by column and domains are replaced by candidate types.
Attribute Value Extraction. Given a text description, AVE is an information extraction task that discovers missing values of attributes and reconstructs a table. For this task, we follow the prompt in [4] designed for GPT-4. The prompt is simple, beginning with the task description. Then, the instance content follows, with the description of the entity and the attribute to be extracted. Finally, an exception rule is mentioned: if the attribute cannot be extracted, the model should answer "N/A".</p>
<p>We also would like to mention that Jellyfish models enable further fine-tuning. Users may choose to condition the model for specific DP tasks or domains to seek better performance. Moreover, Jellyfish models can be utilized for multiple tasks in a DP pipeline, e.g., data cleaning followed by data integration on the same sets of data. It is likely that the DP tasks within this pipeline belong to the same domain. In this case, Jellyfish models may deliver consistency in handling the data in different tasks due to the built-in domain knowledge acquired through instruction tuning for DP.</p>
<h2>6 Experiments</h2>
<h3>6.1 Experimental Setup</h3>
<p>Datasets. Apart from the seen datasets in Jellyfish (Section 3), we use the following datasets as unseen data, where CTA and AVE are used for case studies on unseen tasks. (1) ED: Flights and Rayyan [65]; (2) DI: Flipkart [21] and Phone [81] from Kaggle; (3) SM: CMS [110]; (4) EM: Abt-Buy and WalmartAmazon from the Magellan data repository [15]; (5) CTA: SOTAB [43]; (6) AVE: AE-110k and OAMine [4]. The statistics of the datasets are reported in Table 3. We generate train/valid/test splits following the protocols for Adult and Hospital [27], Flipkart and Phone [66], and MIMIC-III and CMS [110]. The other datasets have already been provided with splits [70, 43, 4]. A subset of the train/valid splits is used in Jellyfish, as reported in Tables 1 and 2.
Jellyfish Models. We instruction-tune three base models: (1) Mistral-7B (Mistral-7B-Instruct-v0.2 [36]), (2) Llama 3-8B (Llama-3-8B-Instruct [67]), and (3) OOP2-13B (OpenOrca-Platypus2-13B [48]), a Llama 2-13B variant with enhanced reasoning capabilities and logic proficiency. The tuned models are dubbed Jellyfish-7B, Jellyfish-8B, and Jellyfish-13B, respectively. The 7B and 8B models are tuned with both DP task and reasoning data ( 15 k reasoning instances for the 7 B model and 8 k for the 8 B model). The 13B model is tuned with only DP task data. As such, Jellyfish-7B and Jellyfish-8B are interpretation models while Jellyfish-13B is a task solver dedicated to the tuned tasks.</p>
<p>Table 3: Testing dataset statistics. For Walmart-Amazon, the entities belong to a different category of products from the Amazon dataset used for instruction tuning.</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Type</th>
<th>Dataset</th>
<th>#Instances</th>
</tr>
</thead>
<tbody>
<tr>
<td>ED</td>
<td>Seen</td>
<td>Adult</td>
<td>9900</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Hospital</td>
<td>17101</td>
</tr>
<tr>
<td></td>
<td>Unseen</td>
<td>Flights</td>
<td>12832</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Rayyan</td>
<td>8997</td>
</tr>
<tr>
<td>DI</td>
<td>Seen</td>
<td>Buy</td>
<td>65</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Restaurant</td>
<td>86</td>
</tr>
<tr>
<td></td>
<td>Unseen</td>
<td>Flipkart</td>
<td>2675</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Phone</td>
<td>1194</td>
</tr>
<tr>
<td>SM</td>
<td>Seen</td>
<td>MIMIC-III</td>
<td>6408</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Synthea</td>
<td>2964</td>
</tr>
<tr>
<td></td>
<td>Unseen</td>
<td>CMS</td>
<td>2564</td>
</tr>
<tr>
<td>EM</td>
<td>Seen</td>
<td>Amazon-Google</td>
<td>2293</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Beer</td>
<td>91</td>
</tr>
<tr>
<td></td>
<td></td>
<td>DBLP-ACM</td>
<td>2473</td>
</tr>
<tr>
<td></td>
<td></td>
<td>DBLP-GoogleScholar</td>
<td>5742</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Fodors-Zagats</td>
<td>189</td>
</tr>
<tr>
<td></td>
<td></td>
<td>iTunes-Amazon</td>
<td>109</td>
</tr>
<tr>
<td></td>
<td>Unseen</td>
<td>Abt-Buy</td>
<td>1946</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Walmart-Amazon</td>
<td>2049</td>
</tr>
<tr>
<td>CTA</td>
<td>Unseen</td>
<td>SOTAB</td>
<td>250</td>
</tr>
<tr>
<td>AVE</td>
<td>Unseen</td>
<td>AE-110K</td>
<td>1482</td>
</tr>
<tr>
<td></td>
<td></td>
<td>OA-Mine</td>
<td>2451</td>
</tr>
</tbody>
</table>
<p>We report hyperparameter setup in Appendix A and injected knowledge in Appendix C. For inference, the (zero-shot) prompts are the same as DP task data and reasoning data, respectively. We apply general knowledge in the prompts, e.g., missing values in matching tasks and error types in ED. Dataset-specific knowledge is not used. When few-shot prompting is enabled, we equip LLMs with three examples for each dataset, covering both positives and negatives (Appendix D).
Baselines. We categorize existing methods into non-LLM methods and LLM methods. For non-LLM methods, we select the following baselines, in line with [70]: (1) ED: HoloDetect [27] and Raha [65]; (2) DI: IPM [66]; (3) SM: SMAT [110]; (4) EM: Ditto [55] and Unicorn [94]; (5) CTA: RoBERTa [61]. For their performance, we follow the best numbers reported in prior works [70, 43, 94]. Other methods such as Baran [64], HoloClean [80], and DODUO [89], have been shown to be outperformed by the above competitors [66, 70, 43], and hence are not compared here.</p>
<p>LLM methods are GPT-3 (text-davinci-002), GPT-3.5 (gpt-3.5-turbo-0301), Table-GPT [52] (GPT-3.5 fine-tuned for tables), GPT-4 (gpt-4-0314), GPT-4o (gpt-4o-2024-05-13), Stable Beluga 2 70B [63], and SOLAR 70B [95]. We follow the numbers reported in previous works [70, 109, 4]. Few-shots are used in line with Jellyfish models for fair comparison. TableLlama [112], which can handle CTA, is not compared because it is tuned for CTA, whereas our purpose is to evaluate the performance of LLMs on CTA as an unseen task.
Metrics. For DP task solving, we measure accuracy for DI, F1 score for ED, DI, EM, and AVE, and micro-F1 for CTA, all reported on a 100-scale.
Environment. Training and inference of LLMs are conducted on NVIDIA A100 80GB GPUs. We employ LoRA [32] and FlashAttention-2 [14] for tuning and vLLM with PageAttention [46] for inference.</p>
<p>Table 4: DP performance on seen tasks, accuracy for DI and F1 score for the other three tasks, with winners in boldface and runners-up underlined. All datasets are seen for non-LLM methods and Table-GPT. All datasets are unseen for GPT-3/3.5/4/4o. For LLM methods, zero-shot is used on seen datasets and few-shot is used on unseen datasets. "-" indicates numbers not reported in prior works for this dataset.</p>
<p>| Task | Type | Dataset | Model | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |</p>
<p>Table 7: Impact of instruction tuning for DP on the unseen task of CTA. "+ task" denotes the model tuned for the task.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">OOP2-13B</th>
<th style="text-align: center;">+ ED</th>
<th style="text-align: center;">+ DI</th>
<th style="text-align: center;">+ SM</th>
<th style="text-align: center;">+ EM</th>
<th style="text-align: center;">Jellyfish-13B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">56.40</td>
<td style="text-align: center;">74.20</td>
<td style="text-align: center;">79.20</td>
<td style="text-align: center;">76.70</td>
<td style="text-align: center;">71.50</td>
<td style="text-align: center;">$\mathbf{8 2 . 0 0}$</td>
</tr>
</tbody>
</table>
<p>Table 8: Impact of prompt engineering on the unseen task of CTA, varying options in stages and chain-of-thought (CoT) over Jellyfish-13B.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">One-stage, w/o CoT</th>
<th style="text-align: center;">One-stage, w/ CoT</th>
<th style="text-align: center;">Two-stage, w/o CoT</th>
<th style="text-align: center;">Two-stage, w/ CoT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">51.50</td>
<td style="text-align: center;">58.00</td>
<td style="text-align: center;">67.00</td>
<td style="text-align: center;">$\mathbf{8 2 . 0 0}$</td>
</tr>
</tbody>
</table>
<p>Among the four tasks, SM is the hardest task, and all the competitors report relatively low F1 score. Looking into the datasets, we find that even humans have difficulties in telling whether the two attributes match, given only name and description. To compare the methods in more detail, we report precision and recall in Table 5. The non-LLM method, SMAT, reports the highest recall, yet with a very low precision. Among its results, only 1 out of $3-9$ is true positive. This iss because many SM-tailored methods seek high recall, in order to find more candidates for further verification. Jellyfish-13B exhibits relatively high precision ( $41 \%-57 \%$ ), and is close to GPT-4 on the unseen dataset of CMS. This suggests that Jellyfish-13B can be used as a verification method (1 out of 2 is true positive) on top of a filtering approach (e.g., SMAT).
Unseen Tasks. Table 6 reports the performance comparison on the unseen tasks. For CTA, GPT4 performs the best. Jellyfish models also exhibit competitiveness, especially for the 7B and 13B models. For AVE, all the Jellyfish models showcase strong generalizability. In particular, Jellyfish8 B and Jellyfish-13B surpass the two 70B models on both datasets, and outperform GPT-4 on the AE-110k dataset.</p>
<p>To drill down to the impact of tuning on unseen tasks, we investigate the case of CTA with Jellyfish13B as an example. Table 7 helps us find out which task contributes the most to this unseen task. When tuning with only one task, the model reports a micro-F1 in the range of $71 \%-79 \%$, with DI being the highest. We suppose this is because DI is exactly the inverse operation of CTA, i.e., DI fills the value of an attribute, and meanwhile CTA infers the type of an attribute given a set of sample values. Moreover, the four tasks jointly contributes to an overall micro-F1 of $82 \%$ and it surpasses the performance of tuning with only DI, showcasing the usefulness of other tasks as well.</p>
<p>Further, we conduct an ablation study to study the impact of prompting and report the results in Table 8. The two-stage pipeline performs better than the one-stage pipeline, and chain-of-thought, which splits the inference of column types into four steps, is also useful, in line with the observation in a previous work [43]. This demonstrates that the prompt engineering techniques developed for existing LLM-based solutions also work for Jellyfish-13B. In doing so, the design of prompts for Jellyfish-13B on unseen tasks is rendered much easier, as users may directly follow those used in existing works.</p>
<h1>6.3 Improvement of Jellyfish over Base Models</h1>
<p>Table 9 compares Jellyfish models and their base models on DP tasks. Consistent performance improvement is observed on all datasets for the 7B and 13B models, and on all but one dataset for the 8B model. The improvement of the 7 B model is the most significant, with an average score of 35 . For the 8 B and 13 B models, the improvement is also remarkable, with an average of 18 and 21 , respectively. We also observe that the tuning benefits the performance on unseen datasets and unseen tasks. Such impact is the most significant on EM's unseen data, showcasing that the knowledge injected through tuning applies well to unseen scenarios.</p>
<p>Table 9: Improvement of Jellyfish over base models on DP. Zero-shot is used on seen datasets and few-shot is used on unseen datasets. All datasets are unseen for base models.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mistral-7B</td>
<td style="text-align: center;">Jellyfish-7B</td>
<td style="text-align: center;">$\begin{gathered} \text { Llama } \ 3-8 \mathrm{~B} \end{gathered}$</td>
<td style="text-align: center;">Jellyfish-8B</td>
<td style="text-align: center;">OOP2-13B</td>
<td style="text-align: center;">Jellyfish-13B</td>
</tr>
<tr>
<td style="text-align: center;">ED</td>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;">Adult</td>
<td style="text-align: center;">20.66</td>
<td style="text-align: center;">77.40 (+56.74)</td>
<td style="text-align: center;">47.42</td>
<td style="text-align: center;">73.74 (+26.32)</td>
<td style="text-align: center;">61.53</td>
<td style="text-align: center;">99.33 (+37.80)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Hospital</td>
<td style="text-align: center;">37.09</td>
<td style="text-align: center;">94.51 (+57.42)</td>
<td style="text-align: center;">52.51</td>
<td style="text-align: center;">93.40 (+40.89)</td>
<td style="text-align: center;">63.24</td>
<td style="text-align: center;">95.59 (+32.35)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;">Flights</td>
<td style="text-align: center;">28.07</td>
<td style="text-align: center;">69.15 (+41.08)</td>
<td style="text-align: center;">67.71</td>
<td style="text-align: center;">$66.21(-1.50)$</td>
<td style="text-align: center;">73.01</td>
<td style="text-align: center;">82.52 (+9.51)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rayyan</td>
<td style="text-align: center;">22.86</td>
<td style="text-align: center;">75.07 (+52.21)</td>
<td style="text-align: center;">62.46</td>
<td style="text-align: center;">81.06 (+18.64)</td>
<td style="text-align: center;">89.37</td>
<td style="text-align: center;">90.65 (+1.28)</td>
</tr>
<tr>
<td style="text-align: center;">DI</td>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;">Buy</td>
<td style="text-align: center;">76.92</td>
<td style="text-align: center;">98.46 (+21.54)</td>
<td style="text-align: center;">86.15</td>
<td style="text-align: center;">98.46 (+12.31)</td>
<td style="text-align: center;">89.23</td>
<td style="text-align: center;">100 (+10.77)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Restaurant</td>
<td style="text-align: center;">18.75</td>
<td style="text-align: center;">89.53 (+70.78)</td>
<td style="text-align: center;">43.02</td>
<td style="text-align: center;">87.21 (+44.19)</td>
<td style="text-align: center;">81.40</td>
<td style="text-align: center;">89.53 (+8.13)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;">Flipkart</td>
<td style="text-align: center;">79.52</td>
<td style="text-align: center;">87.14 (+7.62)</td>
<td style="text-align: center;">66.50</td>
<td style="text-align: center;">87.48 (+20.98)</td>
<td style="text-align: center;">78.49</td>
<td style="text-align: center;">81.68 (+3.19)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Phone</td>
<td style="text-align: center;">76.72</td>
<td style="text-align: center;">86.52 (+9.80)</td>
<td style="text-align: center;">82.16</td>
<td style="text-align: center;">85.68 (+3.52)</td>
<td style="text-align: center;">84.33</td>
<td style="text-align: center;">87.21 (+2.88)</td>
</tr>
<tr>
<td style="text-align: center;">SM</td>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;">MIMIC-III</td>
<td style="text-align: center;">6.90</td>
<td style="text-align: center;">53.33 (+46.43)</td>
<td style="text-align: center;">14.81</td>
<td style="text-align: center;">45.45 (+30.64)</td>
<td style="text-align: center;">36.36</td>
<td style="text-align: center;">40 (+3.64)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Systhea</td>
<td style="text-align: center;">26.67</td>
<td style="text-align: center;">55.56 (+28.89)</td>
<td style="text-align: center;">23.52</td>
<td style="text-align: center;">47.06 (+23.54)</td>
<td style="text-align: center;">22.22</td>
<td style="text-align: center;">56 (+33.78)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;">CMS</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">42.86 (+42.86)</td>
<td style="text-align: center;">27.02</td>
<td style="text-align: center;">38.10 (+11.08)</td>
<td style="text-align: center;">13.33</td>
<td style="text-align: center;">59.29 (+45.96)</td>
</tr>
<tr>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;">Amazon-Google</td>
<td style="text-align: center;">36.51</td>
<td style="text-align: center;">81.69 (+45.15)</td>
<td style="text-align: center;">60.67</td>
<td style="text-align: center;">81.42 (+20.75)</td>
<td style="text-align: center;">36.70</td>
<td style="text-align: center;">81.34 (+44.64)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Beer</td>
<td style="text-align: center;">69.57</td>
<td style="text-align: center;">100 (+30.43)</td>
<td style="text-align: center;">88</td>
<td style="text-align: center;">100 (+12)</td>
<td style="text-align: center;">85.71</td>
<td style="text-align: center;">96.77 (+11.06)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">DBLP-ACM</td>
<td style="text-align: center;">85.30</td>
<td style="text-align: center;">98.65 (+13.35)</td>
<td style="text-align: center;">82.14</td>
<td style="text-align: center;">98.77 (+16.63)</td>
<td style="text-align: center;">78.86</td>
<td style="text-align: center;">98.98 (+20.12)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">DBLP-GoogleScholar</td>
<td style="text-align: center;">59.54</td>
<td style="text-align: center;">94.88 (+35.34)</td>
<td style="text-align: center;">76.15</td>
<td style="text-align: center;">95.03 (+18.88)</td>
<td style="text-align: center;">59.48</td>
<td style="text-align: center;">98.51 (+39.03)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fodors-Zagats</td>
<td style="text-align: center;">66.67</td>
<td style="text-align: center;">100 (+33.33)</td>
<td style="text-align: center;">95.23</td>
<td style="text-align: center;">100 (+4.77)</td>
<td style="text-align: center;">92.68</td>
<td style="text-align: center;">100 (+7.32)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">iTunes-Amazon</td>
<td style="text-align: center;">70.97</td>
<td style="text-align: center;">96.30 (+25.33)</td>
<td style="text-align: center;">79.36</td>
<td style="text-align: center;">96.30 (+16.94)</td>
<td style="text-align: center;">57.45</td>
<td style="text-align: center;">98.11 (+40.66)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;">Abt-Buy</td>
<td style="text-align: center;">36.99</td>
<td style="text-align: center;">86.06 (+49.07)</td>
<td style="text-align: center;">44.60</td>
<td style="text-align: center;">88.84 (+44.24)</td>
<td style="text-align: center;">31.51</td>
<td style="text-align: center;">89.58 (+58.07)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Walmart-Amazon</td>
<td style="text-align: center;">63.14</td>
<td style="text-align: center;">84.91 (+21.77)</td>
<td style="text-align: center;">59.69</td>
<td style="text-align: center;">85.24 (+25.55)</td>
<td style="text-align: center;">65.21</td>
<td style="text-align: center;">89.42 (+24.21)</td>
</tr>
<tr>
<td style="text-align: center;">CTA</td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;">SOTAB</td>
<td style="text-align: center;">23.49</td>
<td style="text-align: center;">83.00 (+59.1)</td>
<td style="text-align: center;">64.25</td>
<td style="text-align: center;">76.33 (+12.08)</td>
<td style="text-align: center;">56.36</td>
<td style="text-align: center;">82.00 (+25.64)</td>
</tr>
<tr>
<td style="text-align: center;">AVE</td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;">AE-110k</td>
<td style="text-align: center;">32.92</td>
<td style="text-align: center;">56.09 (+23.17)</td>
<td style="text-align: center;">56.33</td>
<td style="text-align: center;">59.55 (+3.22)</td>
<td style="text-align: center;">43.87</td>
<td style="text-align: center;">58.12 (+14.25)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">OA-Mine</td>
<td style="text-align: center;">32.44</td>
<td style="text-align: center;">51.98 (+19.54)</td>
<td style="text-align: center;">55.57</td>
<td style="text-align: center;">59.22 (+3.65)</td>
<td style="text-align: center;">54.81</td>
<td style="text-align: center;">55.96 (+1.15)</td>
</tr>
<tr>
<td style="text-align: center;">Average</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">44.17</td>
<td style="text-align: center;">80.14 (+35.97)</td>
<td style="text-align: center;">60.69</td>
<td style="text-align: center;">79.30 (+18.60)</td>
<td style="text-align: center;">61.60</td>
<td style="text-align: center;">83.21 (+21.61)</td>
</tr>
</tbody>
</table>
<h1>6.4 Impact of Knowledge Injection</h1>
<p>To evaluate the impact of knowledge injection, we report in Table 10 the results for OOP2-13B and its tuned version with knowledge either injected or not. Comparing OOP2-13B and the one without injected knowledge, the performance is significantly raised on seen datasets but drops on a few unseen datasets. When we turn on knowledge injection, the performance further improves the performance on seen datasets and the improvement is consistent on all but two datasets. Furthermore, the improvement is also observed and more significant on unseen datasets, because like seen datasets of Amazon-Google and Beer, they are also product data. This observation suggests that the domain knowledge learned through tuning indeed enhances the model's generalizability to unseen datasets. In addition, the impact is the most remarkable on CMS, the unseen dataset of SM, remedying the model's performance on this dataset and making it highly competitive.</p>
<h3>6.5 NLP Performance</h3>
<p>Table 11 compares Jellyfish models and their original models on various NLP benchmarks [29, 84, 57, $10,13,108]$ of the Open LLM Leaderboard [19]. For the 8B and 13B models, their NLP performance roughly retains after tuning for DP, with very slight change ( 0.56 and 0.36 on average, respectively), and even improves on two benchmarks. This is because we use natural language instructions to tune Jellyfish for DP tasks, keeping the same interaction format of their base models. The 7B model sacrifices more NLP performance ( 1.32 on average) for DP performance. We think this reflects the no free lunch theorem [102], considering its smallest size among the three.</p>
<h3>6.6 Impact of Instruction Data Configuration</h3>
<p>We study the impact of the data configuration in the instruction data. For this set of experiments, we randomly sample data from the datasets in Table 1 and disable the data preparation techniques regarding positives and missing values (Section 3.1) to see the impact of dataset size clearly.</p>
<p>To simplify the evaluation, we tune the 13B model with data for a single DP task and evaluate its</p>
<p>Table 10: Impact of knowledge injection, zero-shot. "w/o KNL" denotes the model tuned without injected knowledge in the DP task data.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">OOP2-13B</td>
<td style="text-align: center;">Jellyfish-13B (w/o <br> KNL)</td>
</tr>
<tr>
<td style="text-align: center;">ED</td>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;">Adult</td>
<td style="text-align: center;">61.53</td>
<td style="text-align: center;">72</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Hospital</td>
<td style="text-align: center;">63.24</td>
<td style="text-align: center;">69.81</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;">Flights</td>
<td style="text-align: center;">73.01</td>
<td style="text-align: center;">65.44</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rayyan</td>
<td style="text-align: center;">89.37</td>
<td style="text-align: center;">76.14</td>
</tr>
<tr>
<td style="text-align: center;">DI</td>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;">Buy</td>
<td style="text-align: center;">89.23</td>
<td style="text-align: center;">93.85</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Restaurant</td>
<td style="text-align: center;">81.40</td>
<td style="text-align: center;">88.37</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;">Flipkart</td>
<td style="text-align: center;">78.49</td>
<td style="text-align: center;">82.80</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Phone</td>
<td style="text-align: center;">84.33</td>
<td style="text-align: center;">83.58</td>
</tr>
<tr>
<td style="text-align: center;">SM</td>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;">MIMIC-III</td>
<td style="text-align: center;">36.36</td>
<td style="text-align: center;">46.15</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Synthea</td>
<td style="text-align: center;">22.22</td>
<td style="text-align: center;">53.33</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;">CMS</td>
<td style="text-align: center;">13.33</td>
<td style="text-align: center;">7.14</td>
</tr>
<tr>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">Seen</td>
<td style="text-align: center;">Amazon-Google</td>
<td style="text-align: center;">36.70</td>
<td style="text-align: center;">77.78</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Beer</td>
<td style="text-align: center;">85.71</td>
<td style="text-align: center;">93.33</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">DBLP-ACM</td>
<td style="text-align: center;">78.86</td>
<td style="text-align: center;">97.36</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">DBLP-GoogleScholar</td>
<td style="text-align: center;">59.48</td>
<td style="text-align: center;">93.10</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fodors-Zagats</td>
<td style="text-align: center;">92.68</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">iTunes-Amazon</td>
<td style="text-align: center;">57.45</td>
<td style="text-align: center;">93.10</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Unseen</td>
<td style="text-align: center;">Abt-Buy</td>
<td style="text-align: center;">31.51</td>
<td style="text-align: center;">86.29</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Walmart-Amazon</td>
<td style="text-align: center;">65.21</td>
<td style="text-align: center;">74.15</td>
</tr>
</tbody>
</table>
<p>Table 11: NLP performance on the Open LLM Leaderboard.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Size</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">$\begin{aligned} &amp; \text { MMLU } \ &amp; \text { (5-shot) } \end{aligned}$</th>
<th style="text-align: center;">WinoGrande (0-shot)</th>
<th style="text-align: center;">$\begin{aligned} &amp; \text { ARC } \ &amp; \text { (25-shot) } \end{aligned}$</th>
<th style="text-align: center;">TruthfulQA (0-shot)</th>
<th style="text-align: center;">GSM8K (8-shot)</th>
<th style="text-align: center;">HellaSwag (10-shot)</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">Mistral-7B</td>
<td style="text-align: center;">62.91</td>
<td style="text-align: center;">73.88</td>
<td style="text-align: center;">63.48</td>
<td style="text-align: center;">66.91</td>
<td style="text-align: center;">41.32</td>
<td style="text-align: center;">84.79</td>
<td style="text-align: center;">65.55</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Jellyfish-7B</td>
<td style="text-align: center;">62.08 (-0.83)</td>
<td style="text-align: center;">72.69 (-1.19)</td>
<td style="text-align: center;">63.48 <br> $(+0.00)$</td>
<td style="text-align: center;">64.76 (-2.15)</td>
<td style="text-align: center;">37.91 (-3.41)</td>
<td style="text-align: center;">84.48 (-0.31)</td>
<td style="text-align: center;">64.23 (-1.32)</td>
</tr>
<tr>
<td style="text-align: center;">8B</td>
<td style="text-align: center;">Llama 3-8B</td>
<td style="text-align: center;">64.51</td>
<td style="text-align: center;">71.74</td>
<td style="text-align: center;">61.01</td>
<td style="text-align: center;">51.63</td>
<td style="text-align: center;">70.36</td>
<td style="text-align: center;">78.61</td>
<td style="text-align: center;">66.31</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Jellyfish-8B</td>
<td style="text-align: center;">64.23 (-0.28)</td>
<td style="text-align: center;">72.06 (+0.32)</td>
<td style="text-align: center;">60.15 (-0.14)</td>
<td style="text-align: center;">51.83 (+0.20)</td>
<td style="text-align: center;">69.29 (-1.07)</td>
<td style="text-align: center;">77.92 (-0.69)</td>
<td style="text-align: center;">65.76 (-0.56)</td>
</tr>
<tr>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">OOP2-13B</td>
<td style="text-align: center;">54.49</td>
<td style="text-align: center;">74.03</td>
<td style="text-align: center;">62.63</td>
<td style="text-align: center;">52.56</td>
<td style="text-align: center;">25.32</td>
<td style="text-align: center;">83.24</td>
<td style="text-align: center;">58.71</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Jellyfish-13B</td>
<td style="text-align: center;">54.04 (-1.45)</td>
<td style="text-align: center;">74.19 (+0.16)</td>
<td style="text-align: center;">62.88 <br> $(+0.25)$</td>
<td style="text-align: center;">52.56 (+0.00)</td>
<td style="text-align: center;">24.26 (-1.06)</td>
<td style="text-align: center;">83.16 (-0.08)</td>
<td style="text-align: center;">58.35 (-0.36)</td>
</tr>
</tbody>
</table>
<p>effect. By varying the amount of data, Figure 3 displays how the tuning data for a specific task affects the DP performance. In general, the four tasks are all useful in improving the overall performance. For intra-task performance (e.g., ED to ED), as expected, the tuning data has a significantly positive impact. For inter-task performance, ED and SM are generally positive to other tasks, while DI and EM report negative effects. Such impact on the overall DP performance is also observed when we increase the amount of tuning data (e.g., doubling EM from 21 k to 43 k ). We also find that DI can benefit from all the other three tasks. We think this is because the other three tasks all contain correct values for the attributes, thereby enhancing the model's ability in filling missing values. In addition, the benefit of increasing tuning data for SM is obvious. Overall, these observations results in the data configuration in constructing Jellyfish (Section 3.1).</p>
<p>We study the impact of tuning the 13B model with multi-task data and plot the results in Figure 4. By feeding the tuning set with data for more tasks, it is obvious that they jointly contribute to better DP performance, and the improvement is consistent. When fully utilized the data, as indicated by $(1,1,1,1)$, it achieves the best performance. Based on the above results, we construct the Jellyfish data by appropriately choosing the size of data for each task. Moreover, with the data preparation techniques (Section 3.1) applied, Jellyfish-13B, even with less amount of tuning data, performs better than $(1,1,1,1)$ in Figure 4.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Impact of tuning with single-task data on DP performance, zero-shot. Above red line is positive.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Impact of tuning with multi-task data on DP performance. Numbers in parenthesis indicate the percentage of data used for each task.</p>
<p>Then, we evaluate how the data for a specific DP task affects the NLP performance and report the results in Figure 5. In general, ED and EM exhibit positive impacts on the overall NLP performance. By increasing the amount of tuning data, all the tasks, except DI, are positive to NLP tasks. Specifically, SM turns from negative to positive when the dataset size is doubled, whereas the trend for DI is reversed, resulting in a significant drop. To drill down to each benchmark, all the four tasks are positive to WinoGrande, while they are generally negative to MMLU, and neutral to the other benchmarks, roughly in line with the results in Table 11. This experiment indicates that we need to choose an appropriate data size for each DP task, specifically, with moderately less data for DI, to prevent the model from losing its NLP capability.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Impact of tuning with single-task data on NLP performance. Above red line is positive.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Impact of tuning with multi-task data on NLP performance. Numbers in parenthesis indicate the percentage of data used for each task.</p>
<p>We also test the impact of tuning the 13B model with multi-task data on its NLP performance over the six benchmarks used in Table 11. The results are reported in Figure 6. The general trend is that with data for more tasks, the NLP performance has a drop, yet this change, as shown in more sporadic points, is less consistent than what we observed in Figure 4. It is noteworthy that the overall decrease in NLP performance is moderate, with an average of 0.36 (from 58.71 to 58.35 ) for Jellyfish-13B.</p>
<h1>6.7 Impact of Reasoning Data</h1>
<p>Figure 7 shows how reasoning data, varying from $8 \mathrm{k}, 11 \mathrm{k}, 14 \mathrm{k}$, to 20 k instances, impacts the DP performance. For the 7B and 8B models, the average scores increase first and then drop when more reasoning data is used for tuning, suggesting that small amount of reasoning data - with the rationale</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Impact of reasoning data on DP performance, zero-shot, plotted in log scale to show trends clearly.
behind DP - can enhance the model's DP performance. Seeing this, we choose 14 k and 8 k for the two models, respectively, as the reasoning data size for tuning, striking a balance for the overall performance. For the 13B model, the scores drastically reduce and then rebound with more reasoning data. This may be attributed to the reasoning and logic capabilities of OOP2-13B, which are intended to enhance those of Llama 2 but ultimately do not align well with the underlying logic of DP. Only when the DP reasoning data reaches 20 k , the model learns to handle DP well with reasoning. Nonetheless, the scores are still below those without reasoning, and thus we choose not to tune the 13B model with reasoning data.</p>
<h1>6.8 Evaluation of Interpretation</h1>
<p>We evaluate the performance of Jellyfish's 7B and 8B models and compare them with GPT-3.5 (gpt-3.5-turbo-0613). Given an answer output by Jellyfish, we generate reasons using both Jellyfish and GPT-3.5, and request GPT-4o to decide which one is better. Note that GPT-4o is unaware of the correct answer to the question in DP. As such, it needs to judge by its own analysis of the question as well.</p>
<p>Table 12 reports the head-to-head comparison of the two models. The results show that Jellyfish7B is almost on a par with GPT-3.5, while Jellyfish-8B is a much better interpreter. In particular, Jellyfish-7B wins over GPT-3.5 on 3 out of 11 datasets, reporting an overall winning rate of $46 \%$ across 220 tests. Jellyfish-8B defeats GPT-3.5 on 10 out of 11 datasets, achieving a winning rate of $73 \%$. To delve into detailed comparison of Jellyfish-8B and GPT-3.5, we show an example on the Amazon-Google dataset in EM. The prompt is as follows.</p>
<p>Table 12: Head-to-head comparison of GPT-3.5 and Jellyfish-7B/8B on interpretation, judged by GPT-4o. The two comparisons share the same sets of questions and the same answers from GPT-3.5.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Comparison 1</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Comparison 2</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">Jellyfish-7B</td>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">Jellyfish-8B</td>
</tr>
<tr>
<td style="text-align: center;">ED</td>
<td style="text-align: center;">Adult</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Hospital</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">DI</td>
<td style="text-align: center;">Buy</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Restaurant</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">11</td>
</tr>
<tr>
<td style="text-align: center;">SM</td>
<td style="text-align: center;">Synthea</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">17</td>
</tr>
<tr>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">Amazon-Google</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">18</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Beer</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">13</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DBLP-ACM</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">18</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DBLP-GoogleScho8lar</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">11</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fodors-Zagats</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">iTunes-Amazon</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">18</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">118</td>
<td style="text-align: center;">102</td>
<td style="text-align: center;">59</td>
<td style="text-align: center;">161</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Winning Rate</td>
<td style="text-align: center;">53.63\%</td>
<td style="text-align: center;">46.36\%</td>
<td style="text-align: center;">26.81\%</td>
<td style="text-align: center;">73.18\%</td>
</tr>
</tbody>
</table>
<h1>Head-to-Head Instance</h1>
<p>You are an AI assistant that follows instruction extremely well. The user will give you a question. Your task is to answer as faithfully as you can. While answering, provide detailed explanation and justify your answer.
Answer your reason first, then finish your response in a separate line with and ONLY with your final answer.
Product A: [name: "adobe dreamweaver cs3 [ mac ]", manufacturer: "adobe", price: "399.0"] Product B: [name: "adobe cs3 web premium", price: "1659.99"]
Are Product $A$ and Product $B$ the same Product?
The reason given by Jellyfish-8B is as follows.
No, Product $A$ and Product $B$ are not the same product. Although both products are manufactured by Adobe, they have different names and prices. Product $A$ is named "adobe dreamweaver cs3 [mac]" and is priced at $\$ 399.00$. On the other hand, Product $B$ is named "adobe cs3 web premium" and is priced at $\$ 1659.99$. The difference in price and name suggests that these are two distinct products with different features and functionalities.
In addition, the name of Product A includes the specific software name "dreamweaver cs3" and the operating system it is designed for, "mac". This suggests that Product $A$ is a specific version of the Dreamweaver software for Mac users. In contrast, Product B is named "cs3 web premium", which suggests that it is a more comprehensive package that includes multiple Adobe software products, rather than just a single software application.
Therefore, based on the available information, it is clear that Product $A$ and Product $B$ are not the same product.
Final Answer: No, Product $A$ and Product $B$ are not the same product.",
The reason given by GPT-3.5 is as follows.
No, Product $A$ and Product $B$ are not the same product. The reason is that although both products are from Adobe and are part of the CS3 suite, they have different names and prices. Product $A$ is specifically named "Adobe Dreamweaver CS3 [Mac]" and is priced at $\$ 399.0$, while Product B is named "Adobe CS3 Web Premium" and is priced at $\$ 1659.99$. The difference in names and prices indicates that they are different products within the CS3 suite, likely with different features and functionalities.
Therefore, Product $A$ and Product $B$ are not the same product.</p>
<p>GPT-40 compares the outputs of both models in the following aspects, where Model 1 refers to Jellyfish-8B and Model 2 refers to GPT-3.5: (1) Faithfulness to Instruction: Model 1: Provides a detailed explanation, comparing the names, prices, and inferred functionalities of both products. It also elaborates on the specific nature of Product A (Dreamweaver for Mac) versus the more comprehensive package suggested by Product B (Web Premium). Model 2: Also states that the products are different, citing differences in names and prices. However, it provides less detail and does not delve into the possible functionalities or the specific nature of the products as much as Model 1 does. (2) Justification: Model 1: Justifies the conclusion by explaining the implications of the product names and prices. It infers that Product A is a specific software for Mac users, while Product B is a more comprehensive suite, which is a logical and well-supported argument. Model 2: Justifies the conclusion by pointing out the differences in names and prices but does not provide as thorough an analysis of what those differences imply about the products. (3) Clarity and Completeness: Model 1: The response is clear, detailed, and logically structured. It ensures that the user understands why the products are different. Model 2: The response is clear but lacks the depth and completeness found in Model 1's explanation. (4) Conclusion: Model 1: Concludes with a clear and well-supported final answer. Model 2: Concludes with a clear final answer but lacks the depth of reasoning provided by Model 1. (5) Winner: Model 1. Based on the analysis, GPT-40 decides that Jellyfish-8B performed better because it adhered more strictly to the original instruction by providing a clear, concise response without unnecessary repetition. In addition, with its built-in knowledge, Jellyfish-8B pointed out the difference in functionalities, whereas GPT-3.5 merely described the difference on the surface.</p>
<p>Furthermore, we find out that when reasoning, GPT-3.5 even fails to respond with a correct answer of matching or not for EM, as shown in its mediocre performance on datasets like Amazon-Google and DBLP-GoogleScholar in Table 4. In contrast to the above example of Jellyfish-8B's landslide win, GPT-3.5 only has a slight edge when it wins. For instance, in an example of the Amazon-Google dataset, GPT-40 points out that GPT-3.5 has more focused justification and additional insights into the implications of the differences between the products, yet it also mentioned that GPT-3.5's repetition of the final answer is a minor deviation from the instruction's format.</p>
<h1>6.9 Evaluation of Efficiency</h1>
<p>With 8 GPUs of A100 80G, instruction tuning spends around 5 hours for Jellyfish-13B, 3 hours for Jellyfish-7B and Jellyfish-8B. For inference on single GPU of A100 80G, Jellyfish-7B, 8B, and 13B spend $0.07,0.08$, and 0.15 seconds, respectively, on average to process an instance. As a reference, GPT-4 spends an average of $1-8$ seconds per instance. Although LLMs require substantial computational resources, thereby increasing the cost of use and compromising the efficiency, some non-LLM methods, such as RoBERTa and those built upon it (e.g., IPM), need fine-tuning when applied to unseen datasets. This fine-tuning time should be counted towards total time expense for fair comparison. Moreover, advanced learning techniques enables Jellyfish models to be quantized [59] or distilled to improve efficiency, which will be considered in the future. To further save processing time, users are also suggested using a simple but faster method to retrieve a set of candidates and then apply Jellyfish models to the candidates. For example, for EM, blocking is often used to group similar records together based on certain attributes and narrow the comparisons to within each block.</p>
<p>For batch processing of multiple instances, the speed can be improved by 1.31 and 1.27 times for 8 B and 13 B models, respectively, when prefix caching is enabled in vLLM. However, this optimization is not available for the 7 B model due to the sliding window attention used in Mistral-7B.</p>
<p>As for memory consumption, Jellyfish-7B, 8B, and 13B spend 18GB, 20GB, and 30GB VRAM (including the model), respectively. To further reduce memory consumption, we can resort to activationaware weight quantization [56]. By doing so, the memory consumption of the 7 B and 8 B models can be reduced to 7.5 GB and 8 GB , respectively, without compromising much of the performance $(-1.25$ and -0.52 average micro-F1/accuracy for the 7 B and 8 B models, respectively).</p>
<h1>7 Related Works</h1>
<p>Since works on LLMs have been introduced in Section 2.2, we briefly review related works on DP here.
Seen Tasks. The tasks targeted in this paper collectively form the most critical part of DP, and they have been extensively studied.</p>
<ul>
<li>ED: Traditional methods mainly depend on hand-crafted rules [11], pattern discovery [12], outlier detection [76], or statistical modeling [33, 98]. Recent works employ more advanced ML techniques such as few-shot learning based on a noisy channel model (HoloDetect) [27], or resort to a series of ML pipelines (Raha) [65], including feature engineering, clustering, and classification.</li>
<li>DI: While rule-based solutions [80, 88] remain one of the prevalent approaches, another stream of works develops ML models for this task, including variational autoencoders [72], generative adversarial networks [107], and attention mechanisms [103, 92]. To seek better imputation performance, recent progress utilizes PLMs to capture semantics [66].</li>
<li>SM: The use of similarity matrices is a traditional way [83]. More advanced methods utilize ML techniques [22], including deep learning models [87]. SMAT [110] is an approach leveraging attention-based deep learning. A recent attempt employs GPT-4 for SM [86].</li>
<li>EM: The procedure is divided into blocking and in-block pairwise matching for the sake of efficiency. Blocking groups pairs of entities that potentially match into the same block, and then pairwise matching is performed within each block to find matching entities. Traditional solutions for blocking mostly rely on attribute equivalence, hashes, or similarities [74]. Recently, the feasibility of using DL methods for blocking has also been examined [91], following the use of DL for pairwise matching [69]. In addition, there are tools that handle both steps such as Megallan [42] and Ditto [55]. A recent evaluation validates the effectiveness of in-context learning in enhancing LLMs' EM performance [75].</li>
</ul>
<p>Unseen Tasks. We review the related studies on CTA and AVE.</p>
<ul>
<li>CTA: As a typical table understanding task, it often appears in the studies on table representation learning [35, 16, 89]. These approaches fine-tune PLMs, typically BERT [17] and its variants [37, 61]. Recently, ChatGPT has been utilized to solve this task [43].</li>
<li>AVE: Early approaches employ LSTM-CRF [45, 117]. With the prevalence of PLMs, like CTA, many solutions to AVE resort to using BERT [106, 99, 118]. A recent work [4] considered finetuning GPT-3.5 and prompting GPT-4, and compared with open-source LLMs like Stable Beluga 2 [63] and SOLAR [95].</li>
</ul>
<p>Generic Solution. Whereas the above solutions are specialized for a task, recent progress developed generic solutions to DP based on GPT-3 [70], or GPT-3.5 and GPT-4 [109], basically employing various prompt engineering techniques on frozen LLMs. Fine-tuning GPT-3.5 and ChatGPT for a variety of table-related tasks has also been investigated [52], and several DP tasks are covered.
Other DP Tasks. Besides the ones covered by this paper, there are many other DP tasks. We name a few examples.</p>
<ul>
<li>Data repairing corrects erroneous values in a dataset. Typical solutions are HoloClean [80] and Baran [64]. HoloClean can detect errors and perform repairing subsequently. Baran only repair errors and resort to Raha to detect errors. Recent advancements [49, 78] utilized Bayesian inference to capture dependencies between attributes.</li>
<li>
<p>Data fusion is the process of integrating multiple data sources that contain information about the same set of entities, with possibly conflicting attribute values. Surveys of early attempts are available [53, 6], with a detailed comparison of various fusion methods on deep web data [53]. More recent endeavors targeted multi-truth data fusion [2] and golden record [28].</p>
</li>
<li>
<p>Data transformation is the process of converting data from one format into another format. Notable approaches are transformation by user-specified examples [26] and learning from large collections of paired table columns [38]. In addition, the aforementioned generic DP solution also covers this task [70].</p>
</li>
</ul>
<p>Data Preparation. DP is also studied in the name of data preparation, which manipulates raw data into a form that can be readily analyzed. A notable Python library is DataPrep [47]. In addition to the DP tasks listed above, data augmentation [9, 68, 60, 116] is another key operation in data preparation. Another line of work studies dataset discovery [3, 44, 20, 71], particularly for integrating data lake tables [40] where joinable [18], unionable [39], and related table search [113] are often used for identifying candidates. Despite search speed being a key concern, LLMs are anticipated to be used on top of their outcomes for automated integration in a data lake [1].</p>
<h1>8 Conclusions</h1>
<p>Contributions. We studied the problem of instruction-tuning LLMs as universal DP task solvers. By devising data preparation and knowledge injection techniques, we proposed Jellyfish, which enables users to craft instructions for DP tasks. Another notable feature of Jellyfish is its interpretation ability, providing explanations of its outputs. We tuned three base models ranging from 7B to 13B, which can operate on a local GPU without compromising data security. The experiments demonstrated the competitiveness of Jellyfish against existing DP solutions, impressive generalizability to new tasks, the ability of retaining performance in NLP tasks, as well as the competence in interpretation.
Limitations. We investigated six DP tasks, whereas there are still many other tasks (e.g., data repairing, data fusion, and data transformation).</p>
<p>We discovered that our reasoning data compromises the 13B model's DP performance, possibly because OpenOrca-Platypus2-13B's reasoning and logic do not align well with the underlying logic of DP. In contrast, the 7B and 8B models, derived from native models Mistral-7B and Llama 3-8B, respectively, can benefit from the use of reasoning data. Due to the lack of (approximately) 13B size for these two base models, we cannot deliver better DP performance than the 13B model while preserving the interpretation ability. Nonetheless, we believe that our instruction data can apply to more advanced base models, with which better DP performance and interpretation ability could be both achieved.</p>
<p>Another limitation is that our prompt is designed as instance-based, rather than the table-based setting which was adopted in many non-LLM approaches. This is partially due to the token limitation of the LLMs we used (e.g., 4096 tokens for a 13B model), and compromises efficiency when we use our models for large-scale datasets. In our future work, we will consider designing prompts that take multiple instance or a table at a time.</p>
<p>Furthermore, when using our models for practical data mining pipelines, we also need to carefully consider the issues of preprocessing and postprocessing. For example, the input data may be scanned copies and contain hierarchical tables, while we focus on relational tables in this work.
Future Work. Our future work aims to address the limitations outlined above, in particular, expanding the instruction data of Jellyfish to encompass more DP tasks, such as data repairing and data transformation. Furthermore, our future research directions include the development of a quantized or distilled model to enhance processing speed, as well as a multi-agent system for adaptable, conversational, code-free DP pipeline.</p>
<h2>Acknowledgements</h2>
<p>This work is mainly supported by NEC Corporation and partially supported by JSPS Kakenhi 23K17456, 23K25157, 23K28096, and JST CREST JPMJCR22M2.</p>
<h1>References</h1>
<p>[1] S. Arora, B. Yang, S. Eyuboglu, A. Narayan, A. Hojel, I. Trummer, and C. RÃ©. Language models enable simple systems for generating structured views of heterogeneous data lakes. arXiv preprint arXiv:2304.09433, 2023.
[2] F. Azzalini, D. Piantella, E. Rabosio, and L. Tanca. Enhancing domain-aware multi-truth data fusion using copy-based source authority and value similarity. The VLDB Journal, 32(3):475-500, 2023 .
[3] A. Bogatu, A. A. Fernandes, N. W. Paton, and N. Konstantinou. Dataset discovery in data lakes. In ICDE, pages 709-720. IEEE, 2020.
[4] A. Brinkmann, R. Shraga, and C. Bizer. Product attribute value extraction using large language models. arXiv preprint arXiv:2310.12537, 2023.
[5] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. NeurIPS, 33:1877-1901, 2020.
[6] G. K. Canalle, A. C. Salgado, and B. F. Loscio. A survey on data fusion: what for? in what form? what is next? Journal of Intelligent Information Systems, 57:25-50, 2021.
[7] A. Chapman, E. Simperl, L. Koesten, G. Konstantinidis, L.-D. IbÃ¡Ã±ez, E. Kacprzak, and P. Groth. Dataset search: a survey. The VLDB Journal, 29(1):251-272, 2020.
[8] L. Cheng, X. Li, and L. Bing. Is GPT-4 a good data analyst? arXiv preprint arXiv:2305.15038, 2023.
[9] N. Chepurko, R. Marcus, E. Zgraggen, R. C. Fernandez, T. Kraska, and D. Karger. ARDA: automatic relational data augmentation for machine learning. arXiv preprint arXiv:2003.09758, 2020 .
[10] F. Chollet. On the measure of intelligence. arXiv preprint arXiv:1911.01547, 2019.
[11] X. Chu, I. F. Ilyas, and P. Papotti. Holistic data cleaning: Putting violations into context. In ICDE, pages 458-469. IEEE, 2013.
[12] X. Chu, J. Morcos, I. F. Ilyas, M. Ouzzani, P. Papotti, N. Tang, and Y. Ye. Katara: A data cleaning system powered by knowledge bases and crowdsourcing. In SIGMOD, pages 1247-1261, 2015 .
[13] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.
[14] T. Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023.
[15] S. Das, A. Doan, P. S. G. C., C. Gokhale, P. Konda, Y. Govind, and D. Paulsen. The magellan data repository. https://sites.google.com/site/anhaidgroup/useful-stuff/ the-magellan-data-repository.
[16] X. Deng, H. Sun, A. Lees, Y. Wu, and C. Yu. TURL: Table understanding through representation learning. ACM SIGMOD Record, 51(1):33-40, 2022.
[17] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Haochen Zhang and Yuyang Dong are co-first authors who contributed equally to this work. Chuan Xiao is the corresponding author.</p>
<p>Our models are available at: https://huggingface.co/NECUUDBFM/Jellyfish . Our instruction dataset is available at: https://huggingface.co/datasets/NECUUDBFM/Jellyfish-Instruct .&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
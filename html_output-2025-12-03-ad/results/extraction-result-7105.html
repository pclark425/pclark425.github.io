<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7105 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7105</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7105</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-132.html">extraction-schema-132</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <p><strong>Paper ID:</strong> paper-073e4f0c3a66b7557abd053301b5104cdc582636</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/073e4f0c3a66b7557abd053301b5104cdc582636" target="_blank">Empowering Molecule Discovery for Molecule-Caption Translation With Large Language Models: A ChatGPT Perspective</a></p>
                <p><strong>Paper Venue:</strong> IEEE Transactions on Knowledge and Data Engineering</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a novel LLM-based framework (MolReGPT) for molecule-caption translation, where an In-Context Few-Shot Molecule Learning paradigm is introduced to empower molecule discovery with LLMs like ChatGPT to perform their in-context learning capability without domain-specific pre-training and fine-tuning.</p>
                <p><strong>Paper Abstract:</strong> Molecule discovery plays a crucial role in various scientific fields, advancing the design of tailored materials and drugs, which contributes to the development of society and human well-being. Specifically, molecule-caption translation is an important task for molecule discovery, aligning human understanding with molecular space. However, most of the existing methods heavily rely on domain experts, require excessive computational cost, or suffer from sub-optimal performance. On the other hand, Large Language Models (LLMs), like ChatGPT, have shown remarkable performance in various cross-modal tasks due to their powerful capabilities in natural language understanding, generalization, and in-context learning (ICL), which provides unprecedented opportunities to advance molecule discovery. Despite several previous works trying to apply LLMs in this task, the lack of domain-specific corpus and difficulties in training specialized LLMs still remain challenges. In this work, we propose a novel LLM-based framework (MolReGPT) for molecule-caption translation, where an In-Context Few-Shot Molecule Learning paradigm is introduced to empower molecule discovery with LLMs like ChatGPT to perform their in-context learning capability without domain-specific pre-training and fine-tuning. MolReGPT leverages the principle of molecular similarity to retrieve similar molecules and their text descriptions from a local database to enable LLMs to learn the task knowledge from context examples. We evaluate the effectiveness of MolReGPT on molecule-caption translation, including molecule understanding and text-based molecule generation. Experimental results show that compared to fine-tuned models, MolReGPT outperforms MolT5-base and is comparable to MolT5-large without additional training. To the best of our knowledge, MolReGPT is the first work to leverage LLMs via in-context learning in molecule-caption translation for advancing molecule discovery. Our work expands the scope of LLM applications, as well as providing a new paradigm for molecule discovery and design.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7105.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7105.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolReGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolReGPT (Retrieval-augmented In-Context Few-Shot Molecule Learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that uses retrieval-augmented in-context few-shot prompting to enable foundation LLMs (e.g., ChatGPT/GPT-4/GPT-3.5 and Llama-2) to perform molecule-caption translation (Mol2Cap and Cap2Mol) without any fine-tuning by retrieving similar molecule-caption pairs from a local database and providing them as context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Framework (uses GPT-4-0314, GPT-3.5-turbo, Llama-2-7B as backends)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>prompt-only; retrieval-augmented in-context learning (no fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>framework (backend model size varies: GPT-4 proprietary, GPT-3.5 proprietary, Llama-2-7B = 7B)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>MolReGPT itself does not train or fine-tune models; it uses a local retrieval corpus built from the ChEBI-20 training split as context examples. Backend LLMs are used as-is (proprietary/web-scale training for GPT models; Llama-2 trained on public corpora).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Retrieval-augmented in-context few-shot prompting: retrieve top-n similar molecule-caption pairs (Morgan fingerprints + Dice similarity for Mol2Cap; BM25 caption retrieval for Cap2Mol), insert into system prompt as examples, then query LLM to directly generate captions (Mol2Cap) or SMILES (Cap2Mol). Includes generation calibration (format checking, regex correction, re-query by dropping longest example when input length exceeded).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES strings for molecules; Morgan fingerprints (binary bit vectors) for retrieval; natural language captions (IUPAC names, functional group descriptions) as text; generated outputs are SMILES (Cap2Mol) or textual captions (Mol2Cap).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Molecule-caption translation as a tool for molecule discovery and design (text-based molecule generation and molecule understanding), intended to support drug discovery/material design workflows by producing candidate SMILES from textual descriptions and descriptive captions from molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>No model fine-tuning; limited by LLM maximum input length (n-shot limited, may remove longest example on re-query), enforced output format (JSON) and validated via generation calibration; only retrieval from local ChEBI-20 training split; no additional property or synthesis filters applied in generation.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Uses local retrieval database and OpenAI API (for GPT-3.5-turbo and GPT-4-0314). No integration with chemistry simulators, docking, quantum calculators, retrosynthesis planners, or external property predictors reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ChEBI-20 (33,010 molecule-caption pairs) used as the local retrieval corpus and evaluation test split (80/10/10 split consistent with MolT5).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>For captions: BLEU-2/4, ROUGE-1/2/L, METEOR, Text2Mol; For molecule generation: BLEU, Exact Match (EM), Levenshtein distance, fingerprint similarities (MACCS, RDK, Morgan), Fréchet ChemNet Distance (FCD), Text2Mol, Validity (fraction of valid SMILES).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>MolReGPT (GPT-4-0314, 10-shot): Mol2Cap Text2Mol=0.585 (BLEU-4=0.525, BLEU-2=0.607), Mol2Cap ROUGE-L=0.562; Cap2Mol Text2Mol=0.593 (BLEU=0.857, Validity=0.899, Morgan FTS=0.739, RDK FTS=0.805, MACCS FTS=0.903, FCD=0.41). MolReGPT (GPT-3.5-turbo, 10-shot): Mol2Cap Text2Mol=0.560 (BLEU-4=0.482), Cap2Mol Text2Mol=0.571 (BLEU=0.790, Validity=0.887, Morgan FTS=0.624). Llama-2-7B (2-shot MolReGPT): Mol2Cap Text2Mol=0.466 (BLEU-4=0.409), Cap2Mol BLEU=0.693 but Text2Mol=0.149 and Validity=0.761. Improvements over zero-shot and random retrieval are reported (e.g., Morgan fingerprint retrieval outperforms random/BM25 in Mol2Cap; BM25 outperforms SentenceBert/random in Cap2Mol).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>No wet-lab synthesis/experimental testing reported; limited by LLM API availability and proprietary models (privacy/security), high compute/cost for training new LLMs (motivation for prompt-only approach), maximum input length limiting number of in-context examples (performance plateaus as n grows), occasional incorrect formats/hallucinations requiring generation calibration, reliance on LLMs' internal factual knowledge which may limit capturing dataset-specific vocabulary/grammar, and lack of integration with property/synthesis filters or downstream validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Molecule Discovery for Molecule-Caption Translation With Large Language Models: A ChatGPT Perspective', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7105.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7105.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4-0314</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (0314 snapshot as used via OpenAI API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary large language model from OpenAI used here as the backend LLM for MolReGPT; accessed via API and used in 10-shot retrieval-augmented in-context prompting to generate captions and SMILES without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-0314</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only large language model; prompt-only (no fine-tuning in this work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper (proprietary, large-scale)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in the paper; referenced generally as web-scale/proprietary pretraining (details not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Retrieval-augmented in-context few-shot prompting (MolReGPT): given system prompt with retrieved molecule-caption examples, directly generate textual captions (Mol2Cap) or SMILES (Cap2Mol).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES output (for Cap2Mol); uses textual captions and retrieved SMILES/Morgan fingerprints as context.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Molecule-caption translation for molecule discovery (Mol2Cap and Cap2Mol), supporting design and retrieval of candidate molecules from textual descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>10-shot in-context examples (limited by input length); enforced JSON output formatting and generation calibration; no external property/synthesis constraints used.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Accessed via OpenAI API; paired with a local retrieval database (ChEBI-20). No chemistry-specific external tools (docking, QM, retrosynthesis) integrated.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Evaluated on ChEBI-20 (test split); context examples retrieved from ChEBI-20 training split.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BLEU, ROUGE, METEOR, Text2Mol for captions; BLEU, EM, Levenshtein, fingerprint similarities (MACCS/RDK/Morgan), FCD, Text2Mol, Validity for molecule generation.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>10-shot MolReGPT with GPT-4-0314: Mol2Cap Text2Mol=0.585 (BLEU-4=0.525, BLEU-2=0.607, ROUGE-L=0.562); Cap2Mol Text2Mol=0.593 (BLEU=0.857, Validity=0.899, Morgan FTS=0.739, RDK FTS=0.805, MACCS FTS=0.903, FCD=0.41). GPT-4-0314 outperformed or matched MolT5-large on several metrics without any fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Proprietary model with closed weights (cannot fine-tune locally), input token length limitation constrains number of in-context examples, occasional format errors or denials requiring calibration, reliance on internal knowledge which may limit adapting to dataset-specific language patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Molecule Discovery for Molecule-Caption Translation With Large Language Models: A ChatGPT Perspective', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7105.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7105.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary OpenAI LLM used in the MolReGPT pipeline (10-shot) to produce molecule captions and SMILES via retrieval-augmented in-context prompting; shows substantial improvement over zero-shot when given retrieved examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only large language model; prompt-only (no fine-tuning in this work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in paper (proprietary)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in the paper; assumed web-scale corpus and instruction-tuning typical for OpenAI models.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Retrieval-augmented in-context few-shot prompting (MolReGPT) to directly generate captions or SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES (Cap2Mol) and textual captions (Mol2Cap); uses Morgan fingerprints for retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Molecule-caption translation for molecule discovery/design.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>10-shot in-context limit (as used in experiments), JSON output enforcement, generation calibration, no chemical property constraints applied.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Used via OpenAI API with a local retrieval database (ChEBI-20). No chemistry-specific validation tools integrated.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ChEBI-20 for retrieval and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same as GPT-4: BLEU, ROUGE, METEOR, Text2Mol for captions; BLEU, EM, Levenshtein, fingerprint similarities, FCD, Text2Mol, Validity for molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>10-shot MolReGPT with GPT-3.5-turbo: Mol2Cap Text2Mol=0.560 (BLEU-4=0.482, BLEU-2=0.565, ROUGE-L=0.543); Cap2Mol Text2Mol=0.571 (BLEU=0.790, Validity=0.887, Morgan FTS=0.624, RDK FTS=0.708, MACCS FTS=0.847, FCD=0.57). Improved substantially over zero-shot (e.g., Text2Mol improved from 0.479 to 0.571 in Cap2Mol).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Same as for GPT-4 but with somewhat lower absolute performance; input length limitations, occasional format issues, dependence on internal pretraining for SMILES grammaticality (zero-shot produces many valid SMILES but quality improves with retrieval examples).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Molecule Discovery for Molecule-Caption Translation With Large Language Models: A ChatGPT Perspective', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7105.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7105.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-2-7B-chat-hf (Llama-2 7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 7-billion-parameter LLM used as a backend for MolReGPT (2-shot, due to hardware/input constraints) demonstrating the method is model-agnostic though with lower absolute performance and sensitivity to example count.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7B-chat-hf</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only open-source LLM; prompt-only (no fine-tuning in this work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Model pretraining not detailed in this paper; referenced as Llama-2 family (publicly released training data by Meta, not enumerated here).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Retrieval-augmented in-context few-shot prompting (MolReGPT) with 2-shot examples (limited by hardware/input length).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES (Cap2Mol) and textual captions (Mol2Cap); retrieval via Morgan fingerprints/BM25 as applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Molecule-caption translation supporting molecule discovery tasks; used to demonstrate MolReGPT's model-agnosticism.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>2-shot limit used in experiments due to input length and hardware constraints; same JSON output enforcement and generation calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Local retrieval database (ChEBI-20); run locally (HuggingFace model variant). No further chemistry tool integration.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ChEBI-20 for retrieval and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BLEU, ROUGE, METEOR, Text2Mol for captions; BLEU, EM, Levenshtein, fingerprint similarities, FCD, Text2Mol, Validity for molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>2-shot MolReGPT with Llama-2-7B: Mol2Cap BLEU-4=0.409, Text2Mol=0.466; Cap2Mol BLEU=0.693 (but Text2Mol=0.149, Validity=0.761, Morgan FTS=0.609, RDK FTS=0.717, MACCS FTS=0.808, FCD=4.90). Performance improves over zero-shot Llama-2-7B but is generally lower than GPT-based backends.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Lower internal knowledge of SMILES/corpus leading to weaker zero-shot performance; constrained to fewer in-context examples (2-shot) due to input/hardware limits; observed mismatch between some metrics (e.g., high BLEU but low Text2Mol) indicating metric sensitivity and possible generation idiosyncrasies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Molecule Discovery for Molecule-Caption Translation With Large Language Models: A ChatGPT Perspective', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7105.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7105.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolT5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolT5 (Translation between molecules and natural language)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously published encoder-decoder model pre-trained on language text and SMILES (C4 and ZINC-15) and fine-tuned for molecule-caption translation; used as a primary baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Translation between molecules and natural language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolT5-base / MolT5-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>encoder-decoder transformer, pre-trained on text and SMILES and fine-tuned on ChEBI-20</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>base and large variants (exact parameter counts not specified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pre-trained on Colossal Clean Crawled Corpus (C4) and ZINC-15 (SMILES) then fine-tuned on ChEBI-20 for molecule-caption translation (per cited prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Model fine-tuned to translate between SMILES and text (seq2seq generation) — requires pre-training and fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES (as sequence tokens) and natural language captions.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Molecule-caption translation: Mol2Cap (captioning) and Cap2Mol (text-based molecule generation) used for molecule discovery and retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Requires pre-training and fine-tuning on domain corpus; separate models or weight reload required to switch between subtasks (paper notes inefficiency).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Not reported in detail in this paper (MolT5 is a cited baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Pretraining: C4 and ZINC-15; Fine-tuning and evaluation referenced on ChEBI-20.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>BLEU, ROUGE, METEOR, Text2Mol, fingerprint similarities, FCD, validity etc. (as used for fair comparison in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported baselines from [10]: MolT5-large example scores on ChEBI-20: Mol2Cap Text2Mol=0.582, BLEU-4=0.508, ROUGE-L=0.594; Cap2Mol Text2Mol=0.554, BLEU=0.854, Validity=0.905, Morgan FTS=0.684, FCD=1.20. MolReGPT (GPT-4-0314) matched or outperformed MolT5-large on some metrics without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Requires domain-specific pretraining and fine-tuning with substantial compute and data; separate fine-tuned weights for different subtasks (inefficient switching); may not generalize to customized textual inputs as well as retrieval-augmented in-context approaches per authors' claims.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Empowering Molecule Discovery for Molecule-Caption Translation With Large Language Models: A ChatGPT Perspective', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Text2mol: Cross-modal molecule retrieval with natural language queries <em>(Rating: 2)</em></li>
                <li>Translation between molecules and natural language <em>(Rating: 2)</em></li>
                <li>Neural scaling of deep chemical models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7105",
    "paper_id": "paper-073e4f0c3a66b7557abd053301b5104cdc582636",
    "extraction_schema_id": "extraction-schema-132",
    "extracted_data": [
        {
            "name_short": "MolReGPT",
            "name_full": "MolReGPT (Retrieval-augmented In-Context Few-Shot Molecule Learning)",
            "brief_description": "A framework that uses retrieval-augmented in-context few-shot prompting to enable foundation LLMs (e.g., ChatGPT/GPT-4/GPT-3.5 and Llama-2) to perform molecule-caption translation (Mol2Cap and Cap2Mol) without any fine-tuning by retrieving similar molecule-caption pairs from a local database and providing them as context.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Framework (uses GPT-4-0314, GPT-3.5-turbo, Llama-2-7B as backends)",
            "model_type": "prompt-only; retrieval-augmented in-context learning (no fine-tuning)",
            "model_size": "framework (backend model size varies: GPT-4 proprietary, GPT-3.5 proprietary, Llama-2-7B = 7B)",
            "training_data_description": "MolReGPT itself does not train or fine-tune models; it uses a local retrieval corpus built from the ChEBI-20 training split as context examples. Backend LLMs are used as-is (proprietary/web-scale training for GPT models; Llama-2 trained on public corpora).",
            "generation_method": "Retrieval-augmented in-context few-shot prompting: retrieve top-n similar molecule-caption pairs (Morgan fingerprints + Dice similarity for Mol2Cap; BM25 caption retrieval for Cap2Mol), insert into system prompt as examples, then query LLM to directly generate captions (Mol2Cap) or SMILES (Cap2Mol). Includes generation calibration (format checking, regex correction, re-query by dropping longest example when input length exceeded).",
            "chemical_representation": "SMILES strings for molecules; Morgan fingerprints (binary bit vectors) for retrieval; natural language captions (IUPAC names, functional group descriptions) as text; generated outputs are SMILES (Cap2Mol) or textual captions (Mol2Cap).",
            "target_application": "Molecule-caption translation as a tool for molecule discovery and design (text-based molecule generation and molecule understanding), intended to support drug discovery/material design workflows by producing candidate SMILES from textual descriptions and descriptive captions from molecules.",
            "constraints_used": "No model fine-tuning; limited by LLM maximum input length (n-shot limited, may remove longest example on re-query), enforced output format (JSON) and validated via generation calibration; only retrieval from local ChEBI-20 training split; no additional property or synthesis filters applied in generation.",
            "integration_with_external_tools": "Uses local retrieval database and OpenAI API (for GPT-3.5-turbo and GPT-4-0314). No integration with chemistry simulators, docking, quantum calculators, retrosynthesis planners, or external property predictors reported.",
            "dataset_used": "ChEBI-20 (33,010 molecule-caption pairs) used as the local retrieval corpus and evaluation test split (80/10/10 split consistent with MolT5).",
            "evaluation_metrics": "For captions: BLEU-2/4, ROUGE-1/2/L, METEOR, Text2Mol; For molecule generation: BLEU, Exact Match (EM), Levenshtein distance, fingerprint similarities (MACCS, RDK, Morgan), Fréchet ChemNet Distance (FCD), Text2Mol, Validity (fraction of valid SMILES).",
            "reported_results": "MolReGPT (GPT-4-0314, 10-shot): Mol2Cap Text2Mol=0.585 (BLEU-4=0.525, BLEU-2=0.607), Mol2Cap ROUGE-L=0.562; Cap2Mol Text2Mol=0.593 (BLEU=0.857, Validity=0.899, Morgan FTS=0.739, RDK FTS=0.805, MACCS FTS=0.903, FCD=0.41). MolReGPT (GPT-3.5-turbo, 10-shot): Mol2Cap Text2Mol=0.560 (BLEU-4=0.482), Cap2Mol Text2Mol=0.571 (BLEU=0.790, Validity=0.887, Morgan FTS=0.624). Llama-2-7B (2-shot MolReGPT): Mol2Cap Text2Mol=0.466 (BLEU-4=0.409), Cap2Mol BLEU=0.693 but Text2Mol=0.149 and Validity=0.761. Improvements over zero-shot and random retrieval are reported (e.g., Morgan fingerprint retrieval outperforms random/BM25 in Mol2Cap; BM25 outperforms SentenceBert/random in Cap2Mol).",
            "experimental_validation": false,
            "challenges_or_limitations": "No wet-lab synthesis/experimental testing reported; limited by LLM API availability and proprietary models (privacy/security), high compute/cost for training new LLMs (motivation for prompt-only approach), maximum input length limiting number of in-context examples (performance plateaus as n grows), occasional incorrect formats/hallucinations requiring generation calibration, reliance on LLMs' internal factual knowledge which may limit capturing dataset-specific vocabulary/grammar, and lack of integration with property/synthesis filters or downstream validation.",
            "uuid": "e7105.0",
            "source_info": {
                "paper_title": "Empowering Molecule Discovery for Molecule-Caption Translation With Large Language Models: A ChatGPT Perspective",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "GPT-4-0314",
            "name_full": "GPT-4 (0314 snapshot as used via OpenAI API)",
            "brief_description": "A proprietary large language model from OpenAI used here as the backend LLM for MolReGPT; accessed via API and used in 10-shot retrieval-augmented in-context prompting to generate captions and SMILES without fine-tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4-0314",
            "model_type": "decoder-only large language model; prompt-only (no fine-tuning in this work)",
            "model_size": "not specified in paper (proprietary, large-scale)",
            "training_data_description": "Not specified in the paper; referenced generally as web-scale/proprietary pretraining (details not provided).",
            "generation_method": "Retrieval-augmented in-context few-shot prompting (MolReGPT): given system prompt with retrieved molecule-caption examples, directly generate textual captions (Mol2Cap) or SMILES (Cap2Mol).",
            "chemical_representation": "SMILES output (for Cap2Mol); uses textual captions and retrieved SMILES/Morgan fingerprints as context.",
            "target_application": "Molecule-caption translation for molecule discovery (Mol2Cap and Cap2Mol), supporting design and retrieval of candidate molecules from textual descriptions.",
            "constraints_used": "10-shot in-context examples (limited by input length); enforced JSON output formatting and generation calibration; no external property/synthesis constraints used.",
            "integration_with_external_tools": "Accessed via OpenAI API; paired with a local retrieval database (ChEBI-20). No chemistry-specific external tools (docking, QM, retrosynthesis) integrated.",
            "dataset_used": "Evaluated on ChEBI-20 (test split); context examples retrieved from ChEBI-20 training split.",
            "evaluation_metrics": "BLEU, ROUGE, METEOR, Text2Mol for captions; BLEU, EM, Levenshtein, fingerprint similarities (MACCS/RDK/Morgan), FCD, Text2Mol, Validity for molecule generation.",
            "reported_results": "10-shot MolReGPT with GPT-4-0314: Mol2Cap Text2Mol=0.585 (BLEU-4=0.525, BLEU-2=0.607, ROUGE-L=0.562); Cap2Mol Text2Mol=0.593 (BLEU=0.857, Validity=0.899, Morgan FTS=0.739, RDK FTS=0.805, MACCS FTS=0.903, FCD=0.41). GPT-4-0314 outperformed or matched MolT5-large on several metrics without any fine-tuning.",
            "experimental_validation": false,
            "challenges_or_limitations": "Proprietary model with closed weights (cannot fine-tune locally), input token length limitation constrains number of in-context examples, occasional format errors or denials requiring calibration, reliance on internal knowledge which may limit adapting to dataset-specific language patterns.",
            "uuid": "e7105.1",
            "source_info": {
                "paper_title": "Empowering Molecule Discovery for Molecule-Caption Translation With Large Language Models: A ChatGPT Perspective",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "GPT-3.5-turbo",
            "name_full": "GPT-3.5-turbo (OpenAI)",
            "brief_description": "A proprietary OpenAI LLM used in the MolReGPT pipeline (10-shot) to produce molecule captions and SMILES via retrieval-augmented in-context prompting; shows substantial improvement over zero-shot when given retrieved examples.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_type": "decoder-only large language model; prompt-only (no fine-tuning in this work)",
            "model_size": "not specified in paper (proprietary)",
            "training_data_description": "Not specified in the paper; assumed web-scale corpus and instruction-tuning typical for OpenAI models.",
            "generation_method": "Retrieval-augmented in-context few-shot prompting (MolReGPT) to directly generate captions or SMILES.",
            "chemical_representation": "SMILES (Cap2Mol) and textual captions (Mol2Cap); uses Morgan fingerprints for retrieval.",
            "target_application": "Molecule-caption translation for molecule discovery/design.",
            "constraints_used": "10-shot in-context limit (as used in experiments), JSON output enforcement, generation calibration, no chemical property constraints applied.",
            "integration_with_external_tools": "Used via OpenAI API with a local retrieval database (ChEBI-20). No chemistry-specific validation tools integrated.",
            "dataset_used": "ChEBI-20 for retrieval and evaluation.",
            "evaluation_metrics": "Same as GPT-4: BLEU, ROUGE, METEOR, Text2Mol for captions; BLEU, EM, Levenshtein, fingerprint similarities, FCD, Text2Mol, Validity for molecules.",
            "reported_results": "10-shot MolReGPT with GPT-3.5-turbo: Mol2Cap Text2Mol=0.560 (BLEU-4=0.482, BLEU-2=0.565, ROUGE-L=0.543); Cap2Mol Text2Mol=0.571 (BLEU=0.790, Validity=0.887, Morgan FTS=0.624, RDK FTS=0.708, MACCS FTS=0.847, FCD=0.57). Improved substantially over zero-shot (e.g., Text2Mol improved from 0.479 to 0.571 in Cap2Mol).",
            "experimental_validation": false,
            "challenges_or_limitations": "Same as for GPT-4 but with somewhat lower absolute performance; input length limitations, occasional format issues, dependence on internal pretraining for SMILES grammaticality (zero-shot produces many valid SMILES but quality improves with retrieval examples).",
            "uuid": "e7105.2",
            "source_info": {
                "paper_title": "Empowering Molecule Discovery for Molecule-Caption Translation With Large Language Models: A ChatGPT Perspective",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Llama-2-7B",
            "name_full": "Llama-2-7B-chat-hf (Llama-2 7B)",
            "brief_description": "An open-source 7-billion-parameter LLM used as a backend for MolReGPT (2-shot, due to hardware/input constraints) demonstrating the method is model-agnostic though with lower absolute performance and sensitivity to example count.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-2-7B-chat-hf",
            "model_type": "decoder-only open-source LLM; prompt-only (no fine-tuning in this work)",
            "model_size": "7B parameters",
            "training_data_description": "Model pretraining not detailed in this paper; referenced as Llama-2 family (publicly released training data by Meta, not enumerated here).",
            "generation_method": "Retrieval-augmented in-context few-shot prompting (MolReGPT) with 2-shot examples (limited by hardware/input length).",
            "chemical_representation": "SMILES (Cap2Mol) and textual captions (Mol2Cap); retrieval via Morgan fingerprints/BM25 as applicable.",
            "target_application": "Molecule-caption translation supporting molecule discovery tasks; used to demonstrate MolReGPT's model-agnosticism.",
            "constraints_used": "2-shot limit used in experiments due to input length and hardware constraints; same JSON output enforcement and generation calibration.",
            "integration_with_external_tools": "Local retrieval database (ChEBI-20); run locally (HuggingFace model variant). No further chemistry tool integration.",
            "dataset_used": "ChEBI-20 for retrieval and evaluation.",
            "evaluation_metrics": "BLEU, ROUGE, METEOR, Text2Mol for captions; BLEU, EM, Levenshtein, fingerprint similarities, FCD, Text2Mol, Validity for molecules.",
            "reported_results": "2-shot MolReGPT with Llama-2-7B: Mol2Cap BLEU-4=0.409, Text2Mol=0.466; Cap2Mol BLEU=0.693 (but Text2Mol=0.149, Validity=0.761, Morgan FTS=0.609, RDK FTS=0.717, MACCS FTS=0.808, FCD=4.90). Performance improves over zero-shot Llama-2-7B but is generally lower than GPT-based backends.",
            "experimental_validation": false,
            "challenges_or_limitations": "Lower internal knowledge of SMILES/corpus leading to weaker zero-shot performance; constrained to fewer in-context examples (2-shot) due to input/hardware limits; observed mismatch between some metrics (e.g., high BLEU but low Text2Mol) indicating metric sensitivity and possible generation idiosyncrasies.",
            "uuid": "e7105.3",
            "source_info": {
                "paper_title": "Empowering Molecule Discovery for Molecule-Caption Translation With Large Language Models: A ChatGPT Perspective",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "MolT5",
            "name_full": "MolT5 (Translation between molecules and natural language)",
            "brief_description": "A previously published encoder-decoder model pre-trained on language text and SMILES (C4 and ZINC-15) and fine-tuned for molecule-caption translation; used as a primary baseline in this paper.",
            "citation_title": "Translation between molecules and natural language",
            "mention_or_use": "mention",
            "model_name": "MolT5-base / MolT5-large",
            "model_type": "encoder-decoder transformer, pre-trained on text and SMILES and fine-tuned on ChEBI-20",
            "model_size": "base and large variants (exact parameter counts not specified in this paper)",
            "training_data_description": "Pre-trained on Colossal Clean Crawled Corpus (C4) and ZINC-15 (SMILES) then fine-tuned on ChEBI-20 for molecule-caption translation (per cited prior work).",
            "generation_method": "Model fine-tuned to translate between SMILES and text (seq2seq generation) — requires pre-training and fine-tuning.",
            "chemical_representation": "SMILES (as sequence tokens) and natural language captions.",
            "target_application": "Molecule-caption translation: Mol2Cap (captioning) and Cap2Mol (text-based molecule generation) used for molecule discovery and retrieval.",
            "constraints_used": "Requires pre-training and fine-tuning on domain corpus; separate models or weight reload required to switch between subtasks (paper notes inefficiency).",
            "integration_with_external_tools": "Not reported in detail in this paper (MolT5 is a cited baseline).",
            "dataset_used": "Pretraining: C4 and ZINC-15; Fine-tuning and evaluation referenced on ChEBI-20.",
            "evaluation_metrics": "BLEU, ROUGE, METEOR, Text2Mol, fingerprint similarities, FCD, validity etc. (as used for fair comparison in this paper).",
            "reported_results": "Reported baselines from [10]: MolT5-large example scores on ChEBI-20: Mol2Cap Text2Mol=0.582, BLEU-4=0.508, ROUGE-L=0.594; Cap2Mol Text2Mol=0.554, BLEU=0.854, Validity=0.905, Morgan FTS=0.684, FCD=1.20. MolReGPT (GPT-4-0314) matched or outperformed MolT5-large on some metrics without fine-tuning.",
            "experimental_validation": false,
            "challenges_or_limitations": "Requires domain-specific pretraining and fine-tuning with substantial compute and data; separate fine-tuned weights for different subtasks (inefficient switching); may not generalize to customized textual inputs as well as retrieval-augmented in-context approaches per authors' claims.",
            "uuid": "e7105.4",
            "source_info": {
                "paper_title": "Empowering Molecule Discovery for Molecule-Caption Translation With Large Language Models: A ChatGPT Perspective",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Text2mol: Cross-modal molecule retrieval with natural language queries",
            "rating": 2
        },
        {
            "paper_title": "Translation between molecules and natural language",
            "rating": 2
        },
        {
            "paper_title": "Neural scaling of deep chemical models",
            "rating": 1
        }
    ],
    "cost": 0.01789575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Empowering Molecule Discovery for Molecule-Caption Translation with Large Language Models: A ChatGPT Perspective</h1>
<p>Jiatong Li, Yunqing Liu, Wenqi Fan, Xiao-Yong Wei, Hui Liu, Jiliang Tang, and Qing Li</p>
<h4>Abstract</h4>
<p>Molecule discovery plays a crucial role in various scientific fields, advancing the design of tailored materials and drugs, which contributes to the development of society and human well-being. Specifically, molecule-caption translation is an important task for molecule discovery, aligning human understanding with molecular space. However, most of the existing methods heavily rely on domain experts, require excessive computational cost, or suffer from sub-optimal performance. On the other hand, Large Language Models (LLMs), like ChatGPT, have shown remarkable performance in various cross-modal tasks due to their powerful capabilities in natural language understanding, generalization, and in-context learning (ICL), which provides unprecedented opportunities to advance molecule discovery. Despite several previous works trying to apply LLMs in this task, the lack of domain-specific corpus and difficulties in training specialized LLMs still remain challenges. In this work, we propose a novel LLM-based framework (MolReGPT) for molecule-caption translation, where an In-Context Few-Shot Molecule Learning paradigm is introduced to empower molecule discovery with LLMs like ChatGPT to perform their in-context learning capability without domain-specific pre-training and fine-tuning. MolReGPT leverages the principle of molecular similarity to retrieve similar molecules and their text descriptions from a local database to enable LLMs to learn the task knowledge from context examples. We evaluate the effectiveness of MolReGPT on molecule-caption translation, including molecule understanding and text-based molecule generation. Experimental results show that compared to fine-tuned models, MolReGPT outperforms MolT5-base and is comparable to MolT5-large without additional training. To the best of our knowledge, MolReGPT is the first work to leverage LLMs via in-context learning in molecule-caption translation for advancing molecule discovery. Our work expands the scope of LLM applications, as well as providing a new paradigm for molecule discovery and design. Notably, our implementation is available at: https://github.com/phenixace/MolReGPT</p>
<p>Index Terms—Drug Discovery, Large Language Models (LLMs), In-context Learning, Retrieval Augmented Generation.</p>
<h2>1 INTRODUCTION</h2>
<p>As the foundation of chemical compounds, molecules are composed of two or more atoms that are chemically bonded together, denoting their unique chemical properties dictated by their specific structures [1]. With a comprehensive understanding of molecules, scientists can effectively design materials, drugs, and products with tailored characteristics and functionalities, impacting a variety of crucial fields such as chemistry [2], pharmacology [3], and material science [4].</p>
<p>Recently, computational technologies such as artificial intelligence (AI) have emerged as powerful tools to expedite the discovery of new molecules [5]. Specifically, molecules can be represented as simplified molecular-input line-entry system (SMILES) strings [6], illustrated in Figure 1 (a), which can be effectively processed by deep sequence models like Recurrent Neural Networks [7] and Transformers [8]. These AI-powered models enable researchers to understand molecular properties and functionalities and thus create promising compounds in a more efficient and cost-effective</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>manner. For example, in order to generate new molecules and better comprehend them, a novel task that translates between molecules and natural language has been proposed by using language models like Text2Mol [9] and MolT5 [10]. It consists of two sub-tasks: molecule captioning (Mol2Cap) and text-based molecule generation (Cap2Mol). As shown in Figure 1 (b-c), the goal of Mol2Cap is to generate a text caption describing molecule features (e.g. structures and characteristics). Specifically, the text caption first demonstrates the molecule structure by describing functional group positions and the according IUPAC name. Then, the characteristics of the molecule will be discussed, including the family that it belongs to and the chemical features that affect its practical use. On the other hand, Cap2Mol aims to generate the corresponding molecule (i.e., SMILES string) based on the given text caption, where the structure and feature information of the molecules could be used to infer the SMILES representation of the molecule. For example, given the molecule "ССССССССССССССССССССССССССССССС", its caption writes "The molecule is a straight-chain alkane comprising of 29 carbon atoms. It has a role as a plant metabolite and a volatile oil component", where "straight-chain alkane" and "29 carbon atoms" directly describe the molecule structure, while the last sentence shows the functions of the molecule.</p>
<p>Despite the impressive progress that has been made in the molecule-caption translation task [9]-[11], the majority of existing advanced approaches still suffer from several limita-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An illustration of molecule-caption translation. (a) Representations of a molecule. (b) Molecule captioning (Mol2Cap) aims to generate a text caption describing the features of the molecule. (c) Text-based molecule generation (Cap2Mol) generates a corresponding molecule to the given caption. (d) Large language models (e.g., ChatGPT) can perform Mol2Cap and Cap2Mol with well-designed prompts.</p>
<p>tions. <em>First</em>, the design of model architectures heavily relies on the labour of domain experts, which can significantly limit the development of AI-powered molecule discovery. <em>Second</em>, most existing methods follow the <em>pre-train&amp;fine-tuning</em> paradigm for molecule-caption translation, putting demanding requirements on computational and domain resources. <em>Third</em>, existing approaches such as Text2Mol and MolT5 fall short in their inability to generalize to unseen examples. Therefore, it is desired to design a novel paradigm for molecule-caption translation.</p>
<p>Recently, Large Language Models (LLMs), scaling up their weights to the billion level, have achieved tremendous success not only in the field of Natural Language Processing (NLP) but also in some cross-modal areas like computer vision [12], recommender systems [13], and molecule discovery [10]. Meanwhile, in addition to the impressive capabilities in natural language understanding and generation, LLMs also demonstrate their powerful generalization and reasoning capabilities [14], [15], which can generalize to other unseen tasks via in-context learning without the necessity of being fine-tuned, largely reducing computational cost. Therefore, LLMs provide unprecedented potential to advance molecule discovery, specifically the task of molecule-caption translation.</p>
<p>Although building specific LLMs in molecule discovery has immense potential for advancing scientific research, we also face significant challenges. <em>First</em>, due to privacy and security concerns, many advanced large language models (e.g., GPT-3.5 and GPT-4) are not publicly available, where LLMs' architectures and parameters are not released publicly for fine-tuning in downstream tasks. <em>Second</em>, owing to their complex architectures and the extensive data required, training advanced LLMs requires significant computing resources and domain corpus, leading to high costs and substantial energy consumption. For instance, it has been reported that the cost of <em>one single training session</em> for GPT-3 exceeds 1 million US dollars. As a result, it is very challenging for us to re-design our own LLMs with pre-training and fine-tuning in specific downstream tasks. <em>At last</em>, it still lacks proper guidelines/paradigms for scientific researchers to make use of powerful LLMs like ChatGPT to enhance their own study in molecule discovery.</p>
<p>To address such challenges, as the early exploration attempt to take advantage of the powerful capabilities of LLMs in the molecule discovery field, in this work, we propose a novel solution to teach LLMs with specific in-context examples for translating between molecules and natural language, as illustrated in Figure 1 (d). More specifically, inspired by the latest ChatGPT, a retrieval-based In-Context Few-Shot Molecule Learning paradigm is developed to conduct two sub-tasks (i.e., Mol2Cap and Cap2Mol) without fine-tuning the LLMs, where n similar molecule-caption pairs are retrieved as context instances under the guidance of molecular similarity, including BM25-based caption retrieval and Morgan Fingerprints-based molecule retrieval. Experiments show that MolReGPT (GPT-4-0314) achieves Text2Mol scores of 0.585 in Mol2Cap and 0.593 in Cap2Mol, showing comparable performance to MolT5-large in Mol2Cap and even outperforming MolT5-large in Cap2Mol without any fine-tuning steps, increasing the Text2Mol metric by 0.5% and 6%, respectively.</p>
<p>Our major contributions are summarized as follows:</p>
<ul>
<li>We introduce a principle strategy based on LLMs, In-Context Few-Shot Molecule Learning, to perform translation between molecules and natural language for molecule discovery. To the best of our knowledge, we are the first to employ the in-context learning ability of LLMs in molecule-caption translation. Our work expands the application scope of LLMs and provides valuable insights into how these LLMs can be adapted for specific scientific tasks.</li>
<li>We develop a novel framework (MolReGPT) to empower LLMs like ChatGPT for scientific purposes without being pre-trained or fine-tuned on domain-specific corpora. By enabling LLMs to understand and generate meaningful molecular descriptions and molecule structures, we pave the way for AI-assisted drug discovery and design. MolReGPT has the potential to accelerate the development of new pharmaceuticals and improve the efficiency of molecular research.</li>
<li>We conduct comprehensive experiments on a real-world dataset with molecule-caption pairs to demonstrate the effectiveness and mechanisms of the proposed method on Mol2Cap and Cap2Mol tasks. The results show that our method could enable GPT4-0314 to achieve comparable performance to MolT5-large.</li>
</ul>
<h1>2 Related Work</h1>
<p><strong>Molecule Discovery</strong>. In recent decades, AI-powered approaches have emerged as mainstream techniques to revolutionize the process of molecule discovery [16], [17]. This study has explored advanced deep representation</p>
<p>methods from other fields, including Convolutional Neural Network (CNN) [18], [19], Recurrent Neural Network (RNN) [20], [21], and Transformer [22], [23]. More recently, as a new task in molecule discovery, Text2Mol [9] is introduced to retrieve molecules using natural language descriptions as search queries, in which a paired dataset of molecules and their corresponding text descriptions are constructed, enabling the learning of a shared semantic embedding space for retrieval. KV-PLM [24] develops a knowledgeable machine reading system pre-trained on a domain corpus, in which SMILES strings are inserted and link molecule structures with biomedical text. What's more, a self-supervised learning framework MolT5 [10] is proposed to pre-train on a substantial volume of unlabeled language text and SMILES strings, enhancing the molecule-caption translation task, such as molecule captioning and text-based molecule generation. MoMu [11] bridges molecular graphs and natural language by pre-training molecular graphs and their semantically related text data through comparative learning.
Large Language Models (LLMs). LLMs have been a trending topic in recent years, with numerous studies exploring their capabilities and potential applications. One of the most wellknown LLMs is the GPT family [25]-[27], which has played a critical role in advancing the field of generative language models. As a representative of the GPT family, ChatGPT is specifically fine-tuned for conversational purposes, which can generate impressively human-like responses [28]. In addition, other LLMs, such as LaMDA [29], PaLM [30], and Vicuna [31], also show a decent performance.</p>
<p>In addition to NLP tasks, LLMs have also shown remarkable potential in various molecule discovery tasks, such as molecule understanding [32], [33]. For instance, ChemBERTa [34] leverages pre-training on an extensive corpus of chemical texts, enabling it to comprehend the structure and properties of chemical compounds. Another notable example is MoleculeSTM [35], which employs incontext learning in conjunction with LLMs. This approach facilitates a deeper understanding of the relationships between chemical structures and their corresponding textual descriptions. Furthermore, ChemGPT [36] represents a variant of the GPT model specifically trained on chemical data. Through well-designed instructions, ChemGPT is capable of generating novel chemical structures and accurately predicting their properties. MolT5 [10] shows that LLMs can perform the cross-modal transition task between molecule and text (i.e. molecule captioning task and text-based molecule generation task), which is one of the most closely related attempts to ours. Besides, it should be noted that MolT5 still needs to be pre-trained on domain corpus and further fine-tuned for translating between molecules and natural language, leading to huge computational costs and harsh data requirements.</p>
<h2>3 MolReGPT</h2>
<p>Due to the huge computation and domain data labelling costs, training or fine-tuning LLMs on the domain-specific corpus in molecule discovery is often infeasible in practice. To address such limitations, we investigate leveraging the great capabilities of LLMs without changing the LLMs,
where we propose a novel framework (MolReGPT) to equip ChatGPT with the ability of molecule-caption translation for molecule discovery. Specifically, in order to improve the quality of prediction, an In-Context Few-Shot Molecule Learning paradigm is introduced to teach ChatGPT to learn the molecule-caption translation task from context examples. The framework of MolReGPT is shown in Figure 2, consisting of four main stages: Molecule-Caption Retrieval, Prompt Management, In-Context Few-Shot Molecule Learning, and Generation Calibration, following the workflow of preprocessing, querying, and post-processing.</p>
<h3>3.1 Molecule-Caption Retrieval</h3>
<p>In order to teach LLMs to handle the molecule-caption translation task without fine-tuning LLMs, we propose in-context few-shot molecule learning to guide LLMs to learn how to translate between molecules and text captions. Normally, in-context learning requires $n$ random examples selected from human-annotated datasets (i.e., moleculecaption pair database), providing a general task instruction to LLMs. However, random examples often provide insufficient knowledge regarding the associations between natural language and molecules, as they fail to provide useful information for the detailed descriptions of functional groups and molecule features. To mitigate this issue, we propose incorporating retrieval methods into the selection of context examples to complement the lack of task-specific knowledge in LLMs, specifically through the stage of Molecule-Caption Retrieval. These retrieval strategies are motivated by the similar property principle, in which molecules similar in structures tend to exhibit similar characteristics [37]. Namely, similar captions containing the descriptions of molecule structures and properties are used to describe similar molecules. Therefore, via these most similar molecules or captions, we could utilize the corresponding moleculecaption pairs as context examples to guide LLMs.</p>
<p>Notably, the SMILES representation of molecules, as a sequence structure, can hardly reveal the actual 2-D graph topology of molecules. Hence, domain-specific methods are required for better molecular similarity calculation during the retrieval stage. Specifically, given a SMILES string representation for Mol2Cap task, we introduce using Morgan Fingerprints (i.e., a molecular structures representation) [38] to calculate molecular similarity using Dice similarity for molecule retrieval. Meanwhile, in Cap2Mol task, we are more focused on the details in the text captions (e.g., IUPAC names and functional group positions). Thus, BM25 caption retrieval, which is widely used in information retrieval [39], is proposed to compute similarity scores between captions of molecules.In both sub-tasks, top-n molecule-caption pair examples are retrieved to serve as context examples in the system prompt. Next, we will detail Morgan Fingerprintsbased molecule retrieval and BM25-based caption retrieval.</p>
<h3>3.1.1 Morgan Fingerprints-based Molecule Retrieval.</h3>
<p>Molecular fingerprints are numerical representations of the chemical structures of molecules, which can be used for various computational objectives [38], such as similarity searching, property prediction, and cluster analysis. One of the most representative molecular fingerprints is the Morgan Fingerprints (Morgan FTS).</p>
<h1>1: Molecule-Caption Retrieval</h1>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The workflow of MolReGPT. MolReGPT consists of four main stages. In stage 1, Molecule-Caption Retrieval is employed to find $n$ best-matched examples from the local database. Then, in stage 2, Prompt Management helps construct the system prompt with the retrieved molecule-caption pairs. Following this, LLMs perform In-Context Few-Shot Molecule Learning based on the provided system prompt and user input prompt. Finally, Generation Calibration ensures the desired output.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Illustrations of Morgan Fingerprints and Dice Similarity. The two molecules will first be transformed into the Morgan Fingerprints. Then, Dice similarity will be calculated. The green colour corresponds to sub-structures that contribute positively to the similarity score between the molecules, while the purple colour represents sub-structures that contribute negatively or have differences between the molecules.</p>
<p>The key idea behind Morgan FTS is to capture the presence or absence of specific sub-structures or chemical fragments in a molecule. Morgan FTS follows a variant of the Morgan algorithm [38], which encodes the structural information of a molecule by representing its connectivity patterns in a circular manner. Morgan FTS is then generated by iteratively expanding a set of atoms from a central atom in the molecule, capturing the neighbouring atoms and their bond types at each expansion step. The process continues until a pre-defined radius is reached. The result is a binary bit vector, where each bit represents the presence or absence
of a particular substructure.
What's more, Morgan FTS has several advantages over other types of fingerprints, including their ability to handle molecules of varying sizes, resistance to small structural changes, and effectiveness in capturing structural similarities between molecules. Subsequently, we apply Dice similarity [40] to measure the similarity between the input molecule and the molecules in the local database. Mathematically, it can be expressed as:</p>
<p>$$
\operatorname{Dice}(A, B)=\frac{(2 *|A \cap B|)}{|A|+|B|}
$$</p>
<p>where $A$ and $B$ are the Morgan Fingerprints of two molecules. $|A|$ and $|B|$ represent the cardinality (i.e., number of substructures) of $A$ and $B \cdot|A \cap B|$ denotes the number of substructures that are common to both $A$ and $B$. Dice similarity ranges from 0 to 1 . The Dice similarity is particularly useful when dealing with imbalanced datasets or focusing on the agreement between positive instances (i.e., sub-structures present in both sets) rather than the overall agreement. As shown in Figure 3, the similarity map explains how Morgan FTS measures the similarities and differences between the two molecules.</p>
<p>Compared to existing molecule embedding methods, Morgan FTS together with Dice similarity provides a distinctive advantage by explicitly indicating the similarities in detailed molecular structures, as these structures are usually directly stated in the molecule captions [41].</p>
<h3>3.1.2 BM25-based Caption Retrieval.</h3>
<p>BM25 is one of the most representative ranking approaches in information retrieval for calculating the relevance of the documents to the given query. The idea is based on the term frequency-inverse document frequency (TF-IDF), which measures how often a term appears in a document (i.e., caption) and how rare it is in the corpus of documents (i.e., the local database) [42]. In addition, BM25 considers the caption's length and the position of the query terms in the caption.</p>
<p>In the Cap2Mol task, we use the input caption as the query sentence, while the captions in the local database (i.e., the training set), are served as the corpus of documents, where each caption represents a document. Mathematically, the formula of BM25 can be defined as follow:</p>
<p>$$
\operatorname{score}(Q, D)=\sum_{i=1}^{N} I D F\left(q_{i}\right) * \frac{f\left(q_{i}, D\right) <em>\left(k_{1}+1\right)}{f\left(q_{i}, D\right)+k_{1} </em>\left(1-b+b * \frac{|D|}{w v g d l}\right)}
$$</p>
<p>where $D$ is the caption corpus and $Q$ is the query caption. $N$ is the number of query terms in the query caption, $q_{i}$ is the $i$-th query term, $I D F\left(q_{i}\right)$ is the inverse document frequency of $q_{i}, f\left(q_{i}, D\right)$ is the term frequency of $q_{i}$ in $D, k_{1}$ and $b$ are tuning parameters, $|D|$ is the length of $D$, and avgdl is the average caption length in the corpus. In Cap2Mol task, as we discussed, structure details usually bring more information gain to the prediction of the molecule SMILES representations, while such details are difficult to capture at the semantic level. In this case, BM25 caption retrieval is applied to calculate the similarity scores between captions at the token level so that the relevant molecule structures described by captions can be learnt via molecule-caption pairs.</p>
<h3>3.2 Prompt Management</h3>
<p>System prompts and user input prompts are two important parts to enable the in-context learning ability of LLMs. User prompts are usually more complex and contain essential instructions for task solving and format formalization, where user prompts are defined to formalize the user inputs. To help LLMs understand the task and generate desired outputs, Prompt Management is proposed to design the system prompt templates, which are further completed with the context examples. As shown in stage 2 of Figure 2, the system prompts consist of the following four parts:</p>
<ul>
<li>Role Identification aims to help LLMs identify the role of experts in the chemistry and molecule discovery domain. By establishing this role, LLMs are encouraged to generate responses aligning with the expected expertise.</li>
<li>Task Description provides a comprehensive explanation of the task's content, ensuring that LLMs have a clear understanding of the specific task they need to address. It also includes critical definitions to clarify terms or concepts that are specialized in the molecule-caption translation task.</li>
<li>Context Examples serves as the evidence for the moleculecaption translation task, allowing LLMs to leverage the information contained within the molecule-caption pairs via in-context learning to generate better responses.</li>
<li>Output Instruction specifies the desired format for the response. Here, we restrict the output to a JSON format for further processing or analysis.</li>
</ul>
<h3>3.3 In-Context Few-Shot Molecule Learning</h3>
<p>Recently, as an alternative to fine-tuning, in-context learning provides great opportunities to teach LLMs to make predictions based on a few context examples. In this work, we introduce In-Context Few-Shot Molecule Learning to perform the Mol2Cap and Cap2Mol tasks without fine-tuning LLMs and analyse the underlying mechanism. This stage is to utilize both the system prompt and user input prompt to query the LLMs. In particular, the combination of the system prompt and user input prompt provides LLMs with a clear guideline via in-context learning; the system prompt establishes domain expertise for molecule-caption translation, while the user prompt narrows the focus and directs the model's attention to the specific user input. As a result, LLMs can learn how to perform the molecule-caption translation from the given task context, without the necessity to modify their parameters.</p>
<p>The formulas below describe the differences between fine-tuning, prompting, and In-context Few-Shot Molecule Learning. Let $M$ be the large language model, $m$ be the molecule, $c$ be the corresponding molecule caption, and $\theta$ be the parameters of the LLM.
Fine-tuning process for the Mol2Cap and Cap2Mol task aims to help LLMs learn the translation probability between the molecule and its text caption:</p>
<p>$$
\begin{aligned}
&amp; c=L\left(m ; \theta_{m}^{<em>}\right) \
&amp; m=L\left(c ; \theta_{c}^{</em>}\right)
\end{aligned}
$$</p>
<p>where $\theta_{m}^{<em>}$ and $\theta_{c}^{</em>}$ are the updated parameters after being fine-tuned on the entire training set ( $\theta_{m}^{<em>}$ for Mol2Cap and $\theta_{c}^{</em>}$ for Cap2Mol).
Prompting uses prompts to wrap the inputs and guide the generation of LLMs without changing the parameters. It can be defined as:</p>
<p>$$
\begin{aligned}
&amp; c=L\left(p_{m}(m) ; \theta\right) \
&amp; m=L\left(p_{c}(c) ; \theta\right)
\end{aligned}
$$</p>
<p>where $p_{m}(\cdot)$ and $p_{c}(\cdot)$ are the Prompt Management templates that transform the original user input (molecules $p_{m}$ or captions $p_{c}$ ) into system prompts with the user input prompts for querying LLMs, and $\theta$ is the original parameters without being fine-tuned.
In-Context Few-Shot Molecule Learning targets the alignment between text and molecule structures. Our method can be formulated as:</p>
<p>$$
\begin{gathered}
c=L\left(p_{m}(m) | T_{m \rightarrow c} ; \theta\right) \
m=L\left(p_{c}(c) | T_{c \rightarrow m} ; \theta\right)
\end{gathered}
$$</p>
<p>where $T_{m \rightarrow c}=\left(m_{1} \rightarrow c_{1}\right) |\left(m_{2} \rightarrow c_{2}\right)| \ldots |\left(m_{n} \rightarrow c_{n}\right)$ and $T_{c \rightarrow m}=\left(c_{1} \rightarrow m_{1}\right) |\left(c_{2} \rightarrow m_{2}\right)| \ldots |\left(c_{n} \rightarrow m_{n}\right)$ are the retrieved context examples for the Mol2Cap and Cap2Mol task. Here, $\left(c_{i} \rightarrow m_{i}\right)$ denotes the $i$-th retrieved example that illustrates how to transfer from caption $c_{i}$ to the molecule $m_{i}$, while $\left(m_{i} \rightarrow c_{i}\right)$ represents the alignment from the molecule $m_{i}$ to its caption $c_{i}$, where $i \in[1, n]$. Notably, $\theta$ is still the original parameter of LLMs without any modification.</p>
<p>Through the way of learning the molecule-caption translation from context examples, LLMs can grasp the</p>
<p>alignment between text description and the molecule structures it implies. Compared to fine-tuning, our method does not require additional model training. Compared to prompting, our method is more explainable and robust due to the in-context learning process.</p>
<h3>3.4 Generation Calibration</h3>
<p>Despite specifying the desired output format, LLMs (e.g., ChatGPT) can occasionally produce unexpected responses, including incorrect output formats and denial of answering. To address these issues, a generation calibration mechanism is introduced to validate the response from LLMs.</p>
<p>In Generation Calibration, we first check the format of original responses. If the format is not correct, several pre-defined format correction strategies, such as Regular Matching, are introduced to correct the format and extract the desired output from the response. If the original response passes the format check or can be calibrated, it is considered valid and accepted as a final response. However, if the original response fails the format check and cannot be corrected within the predefined strategies, we initiate requeries. Notably, there is a special case for re-queries. When the original response reports the "Exceed Maximum Input Length Limitation" error, we will remove the longest example in the re-query phase until the query length meets the length limitation. The re-query process involves making additional queries to the LLMs until a valid response is obtained or until the maximum error allowance is reached to ensure that the system does not get stuck in an endless loop.</p>
<h2>4 EXPERIMENT</h2>
<p>In this section, we aim to evaluate the effectiveness of our proposed MolReGPT by conducting comprehensive experiments on molecule-caption translation task.</p>
<h3>4.1 Experimental Settings</h3>
<p>We first introduce the basic experimental settings. In this work, we use ChatGPT through the OpenAI API ${ }^{1}$ with backend model GPT-3.5-turbo and GPT-4-0314, which can not be fine-tuned in our tasks. To assure the model agnosticism of MolReGPT, we also apply an extra opensource LLM, Llama-2-7b-chat-hf (i.e., Llama-2-7B) [43], for comparison. Notably, due to the input length limitation and hardware limitation, we could only apply 2-shot MolReGPT to this model. Besides, we will provide an overview of the data and metrics employed in this section.
Dataset. The research on molecule-caption translation is still in the early stage, and there is only one public dataset ChEBI-20 [9], which contains 33,010 molecule-caption pairs. To ensure consistency, we adhere to the data split process as used in MolT5 [10], dividing the dataset into $80 / 10 / 10 \%$ train/validation/test splits. For our method evaluation, we focus on the test split while utilizing the training set as the local database to retrieve n -shot examples for in-context learning.
Evaluation Metrics. In terms of evaluation metrics, we align with the metrics adopted in MolT5 [10]. By adopting these</p>
<ol>
<li>
<p>https://openai.com/blog/openai-api
metrics, we ensure consistency and enable a fair assessment of the performance of our method.
Baselines. Specifically, the following baselines are selected for performance evaluation. Most of these baselines are only fine-tuned on ChEBI-20 because of their limited maximum input length and poor reasoning capabilities.</p>
</li>
<li>
<p>Transformer [44]. This method is the most representative language architecture to process natural language. A vanilla Transformer model with six encoder and decoder layers, directly trained on ChEBI-20. Notably, this model is not pre-trained.</p>
</li>
<li>T5 [45]. T5 is pre-trained on the Colossal Clean Crawled Corpus (C4), but no domain knowledge is specifically fed for pre-training. In this work, T5-base and T5-large are directly fine-tuned on ChEBI-20.</li>
<li>MolT5 [10]. MolT5 models are pre-trained on both language texts and SMILES strings. More specifically, MolT5-base and MolT5-large were pre-trained on the Colossal Clean Crawled Corpus (C4) and ZINC-15 datasets and further fine-tuned on ChEBI-20.</li>
</ol>
<h3>4.2 Performance Comparison</h3>
<p>We present the results of the molecule-caption translation task, incorporating both quantitative analysis and detailed examples for comparison.
Molecule Captioning (Mol2Cap). Table 1 illustrates the performance comparison of 10-shot MolReGPT (GPT-3.5turbo) and 10-shot MolReGPT (GPT-4-0314) with the baseline models for Mol2Cap task, offering an overview of the results. Notably, our method can achieve better BLEU scores and comparable ROUGE scores to MolT5-base when using GPT-3.5-turbo as the backend model. Meanwhile, when using GPT-4-0314 as the backend model, MolReGPT can obtain better BLEU and Text2Mol scores than MolT5-large without being fine-tuned on ChEBI-20 dataset. Furthermore, we obtain the following observations:</p>
<ul>
<li>With the instruction of 10-shot MolReGPT, GPT-3.5-turbo achieves significantly improved results that gain an improvement of $59 \%$ to the zero-shot case and $2.4 \%$ to MolT5-base under the Text2Mol metric, which indicates that our proposed method can teach ChatGPT to effectively learn the Mol2Cap task via in-context learning.</li>
<li>Restricted by the number of examples, MolReGPT only gains limited insights from the distribution of molecule captions (e.g., vocabulary and grammar). The LLMs' predictions for captions heavily rely on their internal factual knowledge and the contextual information provided by the system prompt. Thus, such vocabulary and grammar patterns may not be as apparent and can not be captured from the selected $n$ examples. As a result, although our 10-shot MolReGPT (GPT-4-0314) achieves a 0.593 Text2Mol score, which is higher than MolT5-large's 0.554 . MolReGPT, in turn, gets lower ROUGE scores compared to MolT5. However, it is crucial to note that captions generated by 10shot MolReGPT with lower ROUGE scores are not entirely incorrect. In fact, the highest Text2Mol score serves as a reliable indicator of the generation quality and highlights the better relevance between the generated molecules and the molecule captions.</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">BLEU-2 $\uparrow$</th>
<th style="text-align: center;">BLEU-4 $\uparrow$</th>
<th style="text-align: center;">ROUGE-1 $\uparrow$</th>
<th style="text-align: center;">ROUGE-2 $\uparrow$</th>
<th style="text-align: center;">ROUGE-L $\uparrow$</th>
<th style="text-align: center;">METEOR $\uparrow$</th>
<th style="text-align: center;">Text2Mol $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Transformer [10]</td>
<td style="text-align: center;">0.061</td>
<td style="text-align: center;">0.027</td>
<td style="text-align: center;">0.204</td>
<td style="text-align: center;">0.087</td>
<td style="text-align: center;">0.186</td>
<td style="text-align: center;">0.114</td>
<td style="text-align: center;">0.057</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-turbo (zero-shot)</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;">0.050</td>
<td style="text-align: center;">0.261</td>
<td style="text-align: center;">0.088</td>
<td style="text-align: center;">0.204</td>
<td style="text-align: center;">0.161</td>
<td style="text-align: center;">0.352</td>
</tr>
<tr>
<td style="text-align: center;">LLama-2-7B (zero-shot)</td>
<td style="text-align: center;">0.094</td>
<td style="text-align: center;">0.039</td>
<td style="text-align: center;">0.169</td>
<td style="text-align: center;">0.054</td>
<td style="text-align: center;">0.142</td>
<td style="text-align: center;">0.175</td>
<td style="text-align: center;">0.153</td>
</tr>
<tr>
<td style="text-align: center;">LLama-2-7B (2-shot MolReGPT)</td>
<td style="text-align: center;">0.489</td>
<td style="text-align: center;">0.409</td>
<td style="text-align: center;">0.535</td>
<td style="text-align: center;">0.374</td>
<td style="text-align: center;">0.472</td>
<td style="text-align: center;">0.495</td>
<td style="text-align: center;">0.466</td>
</tr>
<tr>
<td style="text-align: center;">T5-base [10]</td>
<td style="text-align: center;">0.511</td>
<td style="text-align: center;">0.423</td>
<td style="text-align: center;">0.607</td>
<td style="text-align: center;">0.451</td>
<td style="text-align: center;">0.550</td>
<td style="text-align: center;">0.539</td>
<td style="text-align: center;">0.523</td>
</tr>
<tr>
<td style="text-align: center;">MolT5-base [10]</td>
<td style="text-align: center;">0.540</td>
<td style="text-align: center;">0.457</td>
<td style="text-align: center;">0.634</td>
<td style="text-align: center;">0.485</td>
<td style="text-align: center;">0.578</td>
<td style="text-align: center;">0.569</td>
<td style="text-align: center;">0.547</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-turbo (10-shot MolReGPT)</td>
<td style="text-align: center;">0.565</td>
<td style="text-align: center;">0.482</td>
<td style="text-align: center;">0.623</td>
<td style="text-align: center;">0.450</td>
<td style="text-align: center;">0.543</td>
<td style="text-align: center;">0.585</td>
<td style="text-align: center;">0.560</td>
</tr>
<tr>
<td style="text-align: center;">T5-large [10]</td>
<td style="text-align: center;">0.558</td>
<td style="text-align: center;">0.467</td>
<td style="text-align: center;">0.630</td>
<td style="text-align: center;">0.478</td>
<td style="text-align: center;">0.569</td>
<td style="text-align: center;">0.586</td>
<td style="text-align: center;">0.563</td>
</tr>
<tr>
<td style="text-align: center;">MolT5-large [10]</td>
<td style="text-align: center;">0.594</td>
<td style="text-align: center;">0.508</td>
<td style="text-align: center;">0.654</td>
<td style="text-align: center;">0.510</td>
<td style="text-align: center;">0.594</td>
<td style="text-align: center;">0.614</td>
<td style="text-align: center;">0.582</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4-0314 (10-shot MolReGPT)</td>
<td style="text-align: center;">0.607</td>
<td style="text-align: center;">0.525</td>
<td style="text-align: center;">0.634</td>
<td style="text-align: center;">0.476</td>
<td style="text-align: center;">0.562</td>
<td style="text-align: center;">0.610</td>
<td style="text-align: center;">0.585</td>
</tr>
</tbody>
</table>
<p>Table 1: The performance of Mol2Cap on ChEBI-20. Experimental results for Transformer, T5-base, MolT5-base, T5-large, and MolT5-large are retrieved from [10]. Due to the input length limitation, we apply 2-shot MolReGPT to llama-2-7B and 10-shot MolReGPT to GPT models. The best scores are in bold, and the second-best scores are underlined.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
<th style="text-align: center;">EM $\uparrow$</th>
<th style="text-align: center;">Levenshtein $\downarrow$</th>
<th style="text-align: center;">MACCS FTS $\uparrow$</th>
<th style="text-align: center;">RDK FTS $\uparrow$</th>
<th style="text-align: center;">Morgan FTS $\uparrow$</th>
<th style="text-align: center;">$\mathrm{FCD} \downarrow$</th>
<th style="text-align: center;">Text2Mol $\uparrow$</th>
<th style="text-align: center;">Validity $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Transformer [10]</td>
<td style="text-align: center;">0.499</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">57.66</td>
<td style="text-align: center;">0.480</td>
<td style="text-align: center;">0.320</td>
<td style="text-align: center;">0.217</td>
<td style="text-align: center;">11.32</td>
<td style="text-align: center;">0.277</td>
<td style="text-align: center;">0.906</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-turbo (zero-shot)</td>
<td style="text-align: center;">0.489</td>
<td style="text-align: center;">0.019</td>
<td style="text-align: center;">52.13</td>
<td style="text-align: center;">0.705</td>
<td style="text-align: center;">0.462</td>
<td style="text-align: center;">0.367</td>
<td style="text-align: center;">2.05</td>
<td style="text-align: center;">0.479</td>
<td style="text-align: center;">0.802</td>
</tr>
<tr>
<td style="text-align: center;">LLama-2-7B (zero-shot)</td>
<td style="text-align: center;">0.104</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">84.18</td>
<td style="text-align: center;">0.243</td>
<td style="text-align: center;">0.119</td>
<td style="text-align: center;">0.089</td>
<td style="text-align: center;">42.01</td>
<td style="text-align: center;">0.148</td>
<td style="text-align: center;">0.631</td>
</tr>
<tr>
<td style="text-align: center;">LLama-2-7B (2-shot MolReGPT)</td>
<td style="text-align: center;">0.693</td>
<td style="text-align: center;">0.022</td>
<td style="text-align: center;">36.77</td>
<td style="text-align: center;">0.808</td>
<td style="text-align: center;">0.717</td>
<td style="text-align: center;">0.609</td>
<td style="text-align: center;">4.90</td>
<td style="text-align: center;">0.149</td>
<td style="text-align: center;">0.761</td>
</tr>
<tr>
<td style="text-align: center;">T5-base [10]</td>
<td style="text-align: center;">0.762</td>
<td style="text-align: center;">0.069</td>
<td style="text-align: center;">24.950</td>
<td style="text-align: center;">0.731</td>
<td style="text-align: center;">0.605</td>
<td style="text-align: center;">0.545</td>
<td style="text-align: center;">2.48</td>
<td style="text-align: center;">0.499</td>
<td style="text-align: center;">0.660</td>
</tr>
<tr>
<td style="text-align: center;">MolT5-base [10]</td>
<td style="text-align: center;">0.769</td>
<td style="text-align: center;">0.081</td>
<td style="text-align: center;">24.458</td>
<td style="text-align: center;">0.721</td>
<td style="text-align: center;">0.588</td>
<td style="text-align: center;">0.529</td>
<td style="text-align: center;">2.18</td>
<td style="text-align: center;">0.496</td>
<td style="text-align: center;">0.772</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-turbo (10-shot MolReGPT)</td>
<td style="text-align: center;">0.790</td>
<td style="text-align: center;">0.139</td>
<td style="text-align: center;">24.91</td>
<td style="text-align: center;">0.847</td>
<td style="text-align: center;">0.708</td>
<td style="text-align: center;">0.624</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.571</td>
<td style="text-align: center;">0.887</td>
</tr>
<tr>
<td style="text-align: center;">T5-large [10]</td>
<td style="text-align: center;">0.854</td>
<td style="text-align: center;">0.279</td>
<td style="text-align: center;">16.721</td>
<td style="text-align: center;">0.823</td>
<td style="text-align: center;">0.731</td>
<td style="text-align: center;">0.670</td>
<td style="text-align: center;">1.22</td>
<td style="text-align: center;">0.552</td>
<td style="text-align: center;">0.902</td>
</tr>
<tr>
<td style="text-align: center;">MolT5-large [10]</td>
<td style="text-align: center;">0.854</td>
<td style="text-align: center;">0.311</td>
<td style="text-align: center;">16.071</td>
<td style="text-align: center;">0.834</td>
<td style="text-align: center;">0.746</td>
<td style="text-align: center;">0.684</td>
<td style="text-align: center;">1.20</td>
<td style="text-align: center;">0.554</td>
<td style="text-align: center;">0.905</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4-0314 (10-shot MolReGPT)</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.280</td>
<td style="text-align: center;">17.14</td>
<td style="text-align: center;">0.903</td>
<td style="text-align: center;">0.805</td>
<td style="text-align: center;">0.739</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.593</td>
<td style="text-align: center;">0.899</td>
</tr>
</tbody>
</table>
<p>Table 2: Cap2Mol results on ChEBI-20. Experimental results for Transformer, T5-base, MolT5-base, T5-large, and MolT5-large are retrieved from [10]. Due to the input length limitation, we apply 2-shot MolReGPT to llama-2-7B and 10-shot MolReGPT to GPT models. The best scores are in bold, and the second-best scores are underlined.</p>
<p>Considering the model agnosticism, MolReGPT increases the caption generation performance consistently across the three different models. This is particularly evident with Llama-2-7B, as it possesses a different model structure. As depicted in Table 1, Llama-2-7B achieves 0.409 BLEU-4 score and 0.466 Text2Mol score without further fine-tuning, which is close to the performance of T5-base. This denotes that MolReGPT is not dependent on specific models but is a general method that could be applicable to a broad spectrum of LLMs.</p>
<p>Figure 4 compares our predicted captions with the ground truth. It is clear that our prediction is quite close to the ground truth, stating accurate details of molecule structures and giving solid predictions of molecule properties. However, it can also be seen that our results have some slight differences in the narrative order, which might influence the translation scores like BLEU and ROUGE. In Contrast, although MolT5large achieves higher ROUGE scores, the generated captions still make many fact and typo errors, contributing to a lower Text2Mol score. Besides, Transformer could hardly generate correct or valid results. For molecule 2 and 3, it even generates the same results, which means that the model is not sensitive to the input.
Text-based Molecule Generation (Cap2Mol). Results of the text-based molecule generation task are presented in Table 2. Comparing all these baselines, 10-shot MolReGPT significantly enhances the capabilities of GPT-3.5-turbo and GPT-4-0314, leading to improved overall performance. In the Text2Mol metric, MolReGPT helps GPT-3.5-turbo and GPT-40314 gain a significant $15 \%$ and $20 \%$ increase, respectively, compared to MolT5-base. What's more, GPT-4-0314 even
achieves a $7 \%$ improvement compared to MolT5-large. Considering the fingerprint scores, our 10-shot MolReGPT (GPT-4-0314) even gets an average of $8.1 \%$ improvement compared to MolT5-large. More importantly, MolReGPT also significantly enhances the molecule generation capabilities of Llama-2-7B, elevating the BELU score from 0.104 to 0.693 . Given that Llama-2-7B was trained on a distinct corpus and possesses a different model architecture, it could further demonstrate the model-agnostic nature of our approach.</p>
<p>Figure 5 compares our predicted molecules with the ground truth. It can be seen that our generated molecules are quite close to the ground truth in the molecular configuration. For molecule 1, both MolT5-large and MolReGPT generate the exact correct molecule, but the 2D graph is slightly different to the ground truth due to the sequence order of the SMILES string. For molecule 2, MolT5-large misses several key structures, while MolReGPT generates the correct representation. For molecule 3, MolT5-large fails to generate the correct configuration, while MolReGPT generates the correct chemical bonds but misses the correct number of carbon atoms.</p>
<p>Besides, in Figure 6, we propose a practical scenario where a scientist aims to obtain molecules with desired structures and properties. In the past, scientist needs to apply domain knowledge to figure out a possible molecule candidate. Then, experiments are required to verify its properties. Now, with the help of MolReGPT, the scientist could formalize the requirements via molecule captions and ask MolReGPT to generate the desired molecules. To be more specific, we list two examples here. Molecule 1 has five benzene rings and several hydrophobic groups as its</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Examples of molecule captions generated by different models, where SMILES strings are converted to molecule graphs for better visualization. Based on the same input molecule graph, our MolReGPT (10-shot GPT-4-0314) can generate accurate and natural captions to describe the structure, properties, and even the functions of the molecule. In contrast, Transformer generates meaningless captions that are far from the ground truth. Captions generated by MolT5-large seem better but still have some typo errors.
unique pattern, while molecule 2 has two benzene rings and several hydrophilic groups. By counting the number of benzene rings, we can easily find that MolT5 generates 3 benzene rings in molecule 1 , which is incorrect. Besides, the functional groups generated by MolT5 in both molecule 1 and molecule 2 also miss-match the requirements given in the captions. Remarkably, these impressive results are achieved without additional fine-tuning steps.</p>
<p>Furthermore, the original weights of T5 are primarily for natural language, which means it has to be finetuned separately to fit the two sub-tasks in this study. Unfortunately, MolT5 does not tackle this issue, as it continues to treat the two sub-tasks of the molecule-caption translation task separately. Switching between the two subtasks in MolT5 requires using a different model class and reloading the weights, making it technically inefficient. In contrast, MolReGPT enables a single foundation LLM to solve both the two sub-tasks simultaneously, providing a comprehensive solution for LLMs to address moleculerelated tasks.</p>
<h3>4.3 Ablation Study</h3>
<p>In addition to the experiments above, we also perform ablation studies to analyze the critical factors that influence the performance of MolReGPT. Note that we select GPT-3.5turbo as the backend model.</p>
<h3>4.3.1 Impact of Retrieval Strategies.</h3>
<p>Retrieval strategies play a key role in guiding LLMs to perform molecule-caption translation tasks for MolReGPT. More similar examples are retrieved, and more valuable information could be contained for In-Context Few-Shot Molecule Learning. For Mol2Cap and Cap2Mol, we choose three different retrieval strategies for comparison. The detailed results are shown in Table 3 and Table 4. We show that in both sub-tasks, compared to random selection, the other retrieval strategies used in this paper can help improve n-shot generation results. Thus, thoughtful selection of retrieval strategies plays a key role in MolReGPT.</p>
<p>In Mol2Cap task, we compare the performance of three retrieval strategies: Random, BM25, and Morgan FTS (adopted in MolReGPT). The Random strategy involves</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Examples of molecules generated by different models, where SMILES strings are converted to molecule graphs for better visualization. Based on the same input caption, our MolReGPT (10-shot GPT-4-0314) can generate accurate molecule graphs described by the caption. In contrast, Transformer generates quite different molecules compared to the ground truth. Compared to Transformer, molecules generated by MolT5-large are closer to the ground truth but still miss so many details.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Illustrations of molecule graphs generated by MolT5 and our MolReGPT (GPT-3.5-turbo), given customized inputs. Notably, the key points in Example 1 highlight the five benzene rings and hydrophobic groups in the structure, which are correctly generated by our MolReGPT. In contrast, the results of MolT5 generate the incorrect number of benzene rings and contain a few hydrophilic groups. In example 2, both generations give the correct number of benzene rings, while MolReGPT generates more hydrophilic groups, which are closer to our input caption.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">BLEU-2 $\uparrow$</th>
<th style="text-align: center;">BLEU-4 $\uparrow$</th>
<th style="text-align: center;">ROUGE-1 $\uparrow$</th>
<th style="text-align: center;">ROUGE-2 $\uparrow$</th>
<th style="text-align: center;">ROUGE-L $\uparrow$</th>
<th style="text-align: center;">METEOR $\uparrow$</th>
<th style="text-align: center;">Text2Mol $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">zero-shot</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;">0.050</td>
<td style="text-align: center;">0.261</td>
<td style="text-align: center;">0.088</td>
<td style="text-align: center;">0.204</td>
<td style="text-align: center;">0.161</td>
<td style="text-align: center;">0.352</td>
</tr>
<tr>
<td style="text-align: center;">1-shot (random)</td>
<td style="text-align: center;">0.236</td>
<td style="text-align: center;">0.131</td>
<td style="text-align: center;">0.335</td>
<td style="text-align: center;">0.135</td>
<td style="text-align: center;">0.257</td>
<td style="text-align: center;">0.253</td>
<td style="text-align: center;">0.372</td>
</tr>
<tr>
<td style="text-align: center;">1-shot (BM25)</td>
<td style="text-align: center;">0.243</td>
<td style="text-align: center;">0.150</td>
<td style="text-align: center;">0.350</td>
<td style="text-align: center;">0.156</td>
<td style="text-align: center;">0.278</td>
<td style="text-align: center;">0.262</td>
<td style="text-align: center;">0.394</td>
</tr>
<tr>
<td style="text-align: center;">1-shot (Morgan FTS)</td>
<td style="text-align: center;">0.506</td>
<td style="text-align: center;">0.416</td>
<td style="text-align: center;">0.547</td>
<td style="text-align: center;">0.372</td>
<td style="text-align: center;">0.473</td>
<td style="text-align: center;">0.499</td>
<td style="text-align: center;">0.529</td>
</tr>
<tr>
<td style="text-align: center;">2-shot (random)</td>
<td style="text-align: center;">0.273</td>
<td style="text-align: center;">0.158</td>
<td style="text-align: center;">0.357</td>
<td style="text-align: center;">0.154</td>
<td style="text-align: center;">0.278</td>
<td style="text-align: center;">0.284</td>
<td style="text-align: center;">0.371</td>
</tr>
<tr>
<td style="text-align: center;">2-shot (BM25)</td>
<td style="text-align: center;">0.287</td>
<td style="text-align: center;">0.188</td>
<td style="text-align: center;">0.380</td>
<td style="text-align: center;">0.185</td>
<td style="text-align: center;">0.307</td>
<td style="text-align: center;">0.297</td>
<td style="text-align: center;">0.397</td>
</tr>
<tr>
<td style="text-align: center;">2-shot (Morgan FTS)</td>
<td style="text-align: center;">0.547</td>
<td style="text-align: center;">0.460</td>
<td style="text-align: center;">0.592</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">0.520</td>
<td style="text-align: center;">0.559</td>
<td style="text-align: center;">0.548</td>
</tr>
<tr>
<td style="text-align: center;">5-shot (random)</td>
<td style="text-align: center;">0.297</td>
<td style="text-align: center;">0.178</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">0.173</td>
<td style="text-align: center;">0.300</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.366</td>
</tr>
<tr>
<td style="text-align: center;">5-shot (BM25)</td>
<td style="text-align: center;">0.311</td>
<td style="text-align: center;">0.213</td>
<td style="text-align: center;">0.398</td>
<td style="text-align: center;">0.205</td>
<td style="text-align: center;">0.327</td>
<td style="text-align: center;">0.317</td>
<td style="text-align: center;">0.405</td>
</tr>
<tr>
<td style="text-align: center;">5-shot (Morgan FTS)</td>
<td style="text-align: center;">0.562</td>
<td style="text-align: center;">0.478</td>
<td style="text-align: center;">0.609</td>
<td style="text-align: center;">0.446</td>
<td style="text-align: center;">0.540</td>
<td style="text-align: center;">0.583</td>
<td style="text-align: center;">0.559(6)</td>
</tr>
<tr>
<td style="text-align: center;">10-shot (random)</td>
<td style="text-align: center;">0.295</td>
<td style="text-align: center;">0.181</td>
<td style="text-align: center;">0.389</td>
<td style="text-align: center;">0.185</td>
<td style="text-align: center;">0.310</td>
<td style="text-align: center;">0.329</td>
<td style="text-align: center;">0.369</td>
</tr>
<tr>
<td style="text-align: center;">10-shot (BM25)</td>
<td style="text-align: center;">0.326</td>
<td style="text-align: center;">0.227</td>
<td style="text-align: center;">0.413</td>
<td style="text-align: center;">0.221</td>
<td style="text-align: center;">0.342</td>
<td style="text-align: center;">0.333</td>
<td style="text-align: center;">0.408</td>
</tr>
<tr>
<td style="text-align: center;">10-shot (Morgan FTS)</td>
<td style="text-align: center;">0.565</td>
<td style="text-align: center;">0.482</td>
<td style="text-align: center;">0.623</td>
<td style="text-align: center;">0.450</td>
<td style="text-align: center;">0.543</td>
<td style="text-align: center;">0.585</td>
<td style="text-align: center;">0.559(8)</td>
</tr>
</tbody>
</table>
<p>Table 3: N-shot Molecule Captioning results on ChEBI-20 dataset with the backend model, GPT-3.5-turbo. The best scores are in bold, and the second-best scores are underlined.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
<th style="text-align: center;">EM $\uparrow$</th>
<th style="text-align: center;">Levenshtein $\downarrow$</th>
<th style="text-align: center;">MACCS FTS $\uparrow$</th>
<th style="text-align: center;">RDK FTS $\uparrow$</th>
<th style="text-align: center;">Morgan FTS $\uparrow$</th>
<th style="text-align: center;">FCD $\downarrow$</th>
<th style="text-align: center;">Text2Mol $\uparrow$</th>
<th style="text-align: center;">Validity $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">zero-shot</td>
<td style="text-align: center;">0.489</td>
<td style="text-align: center;">0.019</td>
<td style="text-align: center;">52.13</td>
<td style="text-align: center;">0.705</td>
<td style="text-align: center;">0.462</td>
<td style="text-align: center;">0.367</td>
<td style="text-align: center;">2.05</td>
<td style="text-align: center;">0.479</td>
<td style="text-align: center;">0.802</td>
</tr>
<tr>
<td style="text-align: center;">1-shot (random)</td>
<td style="text-align: center;">0.525</td>
<td style="text-align: center;">0.027</td>
<td style="text-align: center;">51.86</td>
<td style="text-align: center;">0.716</td>
<td style="text-align: center;">0.475</td>
<td style="text-align: center;">0.373</td>
<td style="text-align: center;">1.67</td>
<td style="text-align: center;">0.482</td>
<td style="text-align: center;">0.821</td>
</tr>
<tr>
<td style="text-align: center;">1-shot (SentenceBert)</td>
<td style="text-align: center;">0.687</td>
<td style="text-align: center;">0.066</td>
<td style="text-align: center;">35.89</td>
<td style="text-align: center;">0.796</td>
<td style="text-align: center;">0.609</td>
<td style="text-align: center;">0.511</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">0.541</td>
<td style="text-align: center;">0.839</td>
</tr>
<tr>
<td style="text-align: center;">1-shot (BM25)</td>
<td style="text-align: center;">0.706</td>
<td style="text-align: center;">0.074</td>
<td style="text-align: center;">33.38</td>
<td style="text-align: center;">0.799</td>
<td style="text-align: center;">0.620</td>
<td style="text-align: center;">0.526</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.540</td>
<td style="text-align: center;">0.842</td>
</tr>
<tr>
<td style="text-align: center;">2-shot (random)</td>
<td style="text-align: center;">0.529</td>
<td style="text-align: center;">0.026</td>
<td style="text-align: center;">49.87</td>
<td style="text-align: center;">0.720</td>
<td style="text-align: center;">0.479</td>
<td style="text-align: center;">0.380</td>
<td style="text-align: center;">1.71</td>
<td style="text-align: center;">0.483</td>
<td style="text-align: center;">0.824</td>
</tr>
<tr>
<td style="text-align: center;">2-shot (SentenceBert)</td>
<td style="text-align: center;">0.642</td>
<td style="text-align: center;">0.048</td>
<td style="text-align: center;">40.98</td>
<td style="text-align: center;">0.770</td>
<td style="text-align: center;">0.560</td>
<td style="text-align: center;">0.463</td>
<td style="text-align: center;">1.01</td>
<td style="text-align: center;">0.557</td>
<td style="text-align: center;">0.841</td>
</tr>
<tr>
<td style="text-align: center;">2-shot (BM25)</td>
<td style="text-align: center;">0.748</td>
<td style="text-align: center;">0.101</td>
<td style="text-align: center;">28.89</td>
<td style="text-align: center;">0.827</td>
<td style="text-align: center;">0.668</td>
<td style="text-align: center;">0.578</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.519</td>
<td style="text-align: center;">0.860</td>
</tr>
<tr>
<td style="text-align: center;">5-shot (random)</td>
<td style="text-align: center;">0.552</td>
<td style="text-align: center;">0.028</td>
<td style="text-align: center;">49.26</td>
<td style="text-align: center;">0.720</td>
<td style="text-align: center;">0.476</td>
<td style="text-align: center;">0.382</td>
<td style="text-align: center;">1.60</td>
<td style="text-align: center;">0.481</td>
<td style="text-align: center;">0.832</td>
</tr>
<tr>
<td style="text-align: center;">5-shot (SentenceBert)</td>
<td style="text-align: center;">0.758</td>
<td style="text-align: center;">0.095</td>
<td style="text-align: center;">28.34</td>
<td style="text-align: center;">0.824</td>
<td style="text-align: center;">0.659</td>
<td style="text-align: center;">0.568</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.558</td>
<td style="text-align: center;">0.871</td>
</tr>
<tr>
<td style="text-align: center;">5-shot (BM25)</td>
<td style="text-align: center;">0.771</td>
<td style="text-align: center;">0.121</td>
<td style="text-align: center;">26.78</td>
<td style="text-align: center;">0.836</td>
<td style="text-align: center;">0.686</td>
<td style="text-align: center;">0.599</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.564</td>
<td style="text-align: center;">0.882</td>
</tr>
<tr>
<td style="text-align: center;">10-shot (random)</td>
<td style="text-align: center;">0.564</td>
<td style="text-align: center;">0.029</td>
<td style="text-align: center;">49.11</td>
<td style="text-align: center;">0.723</td>
<td style="text-align: center;">0.486</td>
<td style="text-align: center;">0.386</td>
<td style="text-align: center;">1.46</td>
<td style="text-align: center;">0.484</td>
<td style="text-align: center;">0.846</td>
</tr>
<tr>
<td style="text-align: center;">10-shot (SentenceBert)</td>
<td style="text-align: center;">0.767</td>
<td style="text-align: center;">0.098</td>
<td style="text-align: center;">27.46</td>
<td style="text-align: center;">0.831</td>
<td style="text-align: center;">0.672</td>
<td style="text-align: center;">0.585</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.562</td>
<td style="text-align: center;">0.890</td>
</tr>
<tr>
<td style="text-align: center;">10-shot (BM25)</td>
<td style="text-align: center;">0.790</td>
<td style="text-align: center;">0.139</td>
<td style="text-align: center;">24.91</td>
<td style="text-align: center;">0.847</td>
<td style="text-align: center;">0.708</td>
<td style="text-align: center;">0.624</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.571</td>
<td style="text-align: center;">0.887</td>
</tr>
</tbody>
</table>
<p>Table 4: N-shot Molecule Generation results on ChEBI-20 dataset with the backend model, GPT-3.5-turbo. The best scores are in bold, and the second-best scores are underlined.
retrieving $n$ random examples, while BM25 applies a character-level BM25 algorithm to the molecule SMILES representations. As shown in Table 3, among the three strategies, Morgan FTS shows the best performance when $n$ is fixed, outperforming BM25 by $37 \%$ in the Text2Mol metric. Besides, the ROUGE-L score achieved by Morgan FTS is almost doubled compared to the Random or BM25 strategies. The use of Morgan FTS with Dice similarity shows a better estimation of the structural similarity between molecules by comparing unique structural features like functional groups. These features are usually revealed in molecule captions with detailed descriptions. In this case, retrieving similar molecules by Morgan FTS could effectively guide the LLM to learn the associations between molecule structures and caption descriptions, resulting in more accurate and desired outputs.</p>
<p>In Cap2Mol task, we also employ these retrieval strategies: Random, SentenceBert, and BM25 (adopted in MolReGPT). The Random strategy still retrieves $n$ random examples, while SentenceBert encodes captions as numerical vectors to compute their semantic similarity. As shown in Table 4, BM25 is the best one in the Cap2Mol task, despite the fact that SentenceBert has outperformed BM25 in many classical NLP text retrieval datasets. When $n$ changes from 1 to 10, BM25 always achieves better BLEU, Exact Match, Levenshtein, and fingerprint scores than SentenceBert. The input molecule captions tend to use phrases with dashes
(-) like "2-methylphenyl" to connect the structure details of the molecule. Understanding such phrases plays a crucial role in generating correct molecule structures. In this case, retrieving similar texts while precisely matching these details significantly contributes to performance improvement. In contrast, SentenceBert, as a neural method, encodes an entire caption into a 1-D embedding vector, focusing more on semantic similarity rather than specific details. Consequently, BM25 is better than SentenceBert in the Cap2Mol task.</p>
<h3>4.3.2 Impact of Example Number for In-Context Learning.</h3>
<p>In this subsection, we study how the number of examples contained in the system prompt through in-context learning affects the performance.</p>
<p>In the zero-shot scenario, where no extra examples are included in the prompt for guiding LLMs for learning the molecule-caption translation task, we utilize two special spans, '[MOLECULE_MASK]' and '[CAPTION_MASK]', to inform the LLMs of the desired output format.</p>
<p>After analyzing the zero-shot results of GPT-3.5-turbo in Tables 3 and 4, we can observe that SMILES strings are included in its pre-training corpus because it can generate basically valid SMILES representations of molecules based on zero-shot prompts, achieving a 0.802 validity score and a 0.479 Text2Mol score in molecule generation. However, it is important to note that the zero-shot results exhibit a performance level similar to a vanilla Transformer model.</p>
<p>This observation provides evidence that GPT-3.5-turbo is not specifically trained on ChEBI-20, thereby alleviating concerns regarding potential information leakage.</p>
<p>For Few-shot Performance, Table 3 and Table 4 list the comprehensive details of the experimental results.Normally, the performance improves as the number of examples, denoted as $n$, increases, as more examples provide additional knowledge for the task. However, due to the input length limitation of LLMs, it is impossible to contain a large number of examples in the system prompt. Therefore, for few-shot scenarios, we choose four different values $1,2,5$, and 10 .</p>
<p>Tables 3 and 4 illustrate that performance generally improves as $n$ increases through in-context learning. Significant performance enhancements are observed as n changes from 0 to 10. Taking Morgan FTS and BM25 as examples, in caption generation, we see remarkable increases from 0.050 to 0.482 , 0.204 to 0.543 , and 0.352 to 0.560 in BLEU-4, ROUGE-L, and Text2Mol scores, respectively. Besides, BM25 improves molecule generation from 0.489 to 0.790 in the BLEU score and 0.479 to 0.571 in the Text2Mol score.</p>
<p>Besides, it is worth noticing that when $n$ increases from 5 to 10 , the Text2Mol metrics almost keep the same, which can be the problem of the maximum input length limitation of LLMs. To fit the input length limitation, we would remove the longest examples to degrade the n-shot to ( $\mathrm{n}-1$ )-shot generation for re-queries. As $n$ increases, there is a higher possibility of exceeding the input length limitation. In this case, unless the maximum input length of the LLM is expanded, the performance will finally converge when $n$ continues to grow.</p>
<h2>5 CONCLUSION</h2>
<p>In this work, we propose MolReGPT, a general retrievalbased in-context learning paradigm that empowers molecule discovery with LLMs like ChatGPT via In-Context Few-Shot Molecule Learning. Our method is focused and evaluated on the task of molecule-caption translation, including molecule captioning (Mol2Cap) and text-based molecule generation (Cap2Mol). MolReGPT leverages the molecular similarity principle to retrieve examples from a local database, guiding LLMs to generate predictions without being fine-tuned. Specifically, BM25 caption retrieval is applied to obtain similar molecule captions, while Morgan Fingerprints and Dice similarity are adopted to retrieve similar molecules. Experimental results show that our proposed MolReGPT can empower ChatGPT to achieve 0.585 and 0.593 Text2Mol scores in Mol2Cap and Cap2Mol, respectively. Compared to MolT5-large, our MolReGPT equips GPT-4-0314 with the ability to achieve comparable performance in Mol2Cap task and even outperform MolT5-large in Cap2Mol task without any fine-tuning steps. To conclude, MolReGPT provides a novel and versatile paradigm to deploy LLMs in molecule discovery through in-context learning, which greatly reduces the cost of domain transfer and explores the potential of LLMs in molecule discovery.</p>
<h2>6 Broader Implication \&amp; Future Directions</h2>
<p>Our work has demonstrated the effectiveness of the MolReGPT framework across various LLMs. Compared to the
existing methods, MolReGPT does not require additional pre-training or fine-tuning, yet it enables powerful LLMs like GPT-4 to achieve comparable and even superior performance. The integration of LLMs and biomolecular science represents a paradigm shift in molecule discovery. As illustrated in Figure 6, fine-tuned methods tend to fail to meet the requirements in customized inputs, while MolReGPT shows a better generalization potential. These unique features make our method more convenient and practical for chemists to adopt and use in their work. We hope that MolReGPT could enable a wider range of scientific researchers to leverage the power of LLMs in their work, ultimately contributing to the advancement of scientific research and discovery.</p>
<p>For future work, there are still several areas that require further exploration and improvement. Firstly, the performance of LLMs is intricately related to the prediction quality of MolReGPT. Thus, with the advancement of more powerful LLMs, our methods could potentially yield even better results in the future. Secondly, we could develop better retrieval algorithms that could help refine the context examples. For example, we could combine BM25 caption retrieval with chemical LLMs and apply Graph Neural Networks for molecular similarity to improve the retrieval quality. Lastly, we anticipate that our work will not only make LLMs more accessible for scientific researchers in the field of chemistry, thereby benefiting drug discovery, but also inspire the AI community to consider the alignment between molecular and text space.</p>
<h2>REFERENCES</h2>
<p>[1] J. Xu, Y. Li, J. Yang, S. Zhou, and W. Situ, "Plasma etching effect on the molecular structure of chitosan-based hydrogels and its biological properties," International Journal of Biological Macromolecules, p. 123257, 2023.
[2] Y. Weng, B. Ding, Y. Liu, C. Song, L.-Y. Chan, and C.-W. Chiang, "Late-stage photoredox c-h amidation of n-unprotected indole derivatives: Access to n-(indol-2-yl) amides," Organic Letters, vol. 23, no. 7, pp. 2710-2714, 2021.
[3] B. Ding, Y. Weng, Y. Liu, C. Song, L. Yin, J. Yuan, Y. Ren, A. Lei, and C.-W. Chiang, "Selective photoredox trifluoromethylation of tryptophan-containing peptides," European Journal of Organic Chemistry, vol. 2019, no. 46, pp. 7596-7605, 2019.
[4] A. Higuchi, T.-C. Sung, T. Wang, Q.-D. Ling, S. S. Kumar, S.-T. Hsu, and A. Umezawa, "Material design for next-generation mrna vaccines using lipid nanoparticles," Polymer Reviews, vol. 63, no. 2, pp. 394-436, 2023.
[5] F. Urbina and S. Ekins, "The commoditization of ai for molecule design," Artificial Intelligence in the Life Sciences, vol. 2, p. 100031, 2022.
[6] D. Weininger, "Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules," Journal of chemical information and computer sciences, vol. 28, no. 1, pp. 31-36, 1988.
[7] J. Artis-Pous, S. V. Johansson, O. Prykhodko, E. J. Bjerrum, C. Tyrchan, J.-L. Reymond, H. Chen, and Ö. Engkvist, "Randomized smiles strings improve the quality of molecular generative models," Journal of cheminformatics, vol. 11, no. 1, pp. 1-13, 2019.
[8] S. Honda, S. Shi, and H. R. Ueda, "Smiles transformer: Pre-trained molecular fingerprint for low data drug discovery," arXiv preprint arXiv:1911.04738, 2019.
[9] C. Edwards, C. Zhai, and H. Ji, "Text2mol: Cross-modal molecule retrieval with natural language queries," in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021, pp. 595-607.
[10] C. Edwards, T. Lai, K. Ros, G. Honke, K. Cho, and H. Ji, "Translation between molecules and natural language," in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Abu Dhabi, United Arab Emirates:</p>
<p>Association for Computational Linguistics, Dec. 2022, pp. 375-413. [Online]. Available: https://aclanthology.org/2022.emnlp-main. 26
[11] B. Su, D. Du, Z. Yang, Y. Zhou, J. Li, A. Rao, H. Sun, Z. Lu, and J.-R. Wen, "A molecular multimodal foundation model associating molecule graphs with natural language," arXiv preprint arXiv:2209.05481, 2022.
[12] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, "Minigpt-4: Enhancing vision-language understanding with advanced large language models," arXiv preprint arXiv:2304.10592, 2023.
[13] K. Bao, J. Zhang, Y. Zhang, W. Wang, F. Feng, and X. He, "Tallrec: An effective and efficient tuning framework to align large language model with recommendation," arXiv preprint arXiv:2305.00447, 2023.
[14] O. Rubin, J. Herzig, and J. Berant, "Learning to retrieve prompts for in-context learning," in Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2022, pp. 2655-2671.
[15] S. Min, M. Lewis, L. Zettlemoyer, and H. Hajishirzi, "Metaicl: Learning to learn in context," in Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2022, pp. 2791-2809.
[16] W. Hu, Y. Liu, X. Chen, W. Chai, H. Chen, H. Wang, and G. Wang, "Deep learning methods for small molecule drug discovery: A survey," IEEE Transactions on Artificial Intelligence, 2023.
[17] W. Fan, C. Liu, Y. Liu, J. Li, H. Li, H. Liu, J. Tang, and Q. Li, "Generative diffusion models on graphs: Methods and applications," arXiv preprint arXiv:2302.02591, 2023.
[18] S.-P. Peng and Y. Zhao, "Convolutional neural networks for the design and analysis of non-fullerene acceptors," Journal of Chemical Information and Modeling, vol. 59, no. 12, pp. 4993-5001, 2019.
[19] N. Q. K. Le, E. K. Y. Yapp, Y.-Y. Ou, and H.-Y. Yeh, "imotor-cnn: Identifying molecular functions of cytoskeleton motor proteins using 2d convolutional neural network via chou's 5-step rule," Analytical biochemistry, vol. 575, pp. 17-26, 2019.
[20] F. Grisoni, M. Moret, R. Lingwood, and G. Schneider, "Bidirectional molecule generation with recurrent neural networks," Journal of chemical information and modeling, vol. 60, no. 3, pp. 1175-1183, 2020.
[21] S. Amabilino, P. Pogány, S. D. Pickett, and D. V. Green, "Guidelines for recurrent neural network transfer learning-based molecular generation of focused libraries," Journal of Chemical Information and Modeling, vol. 60, no. 12, pp. 5699-5713, 2020.
[22] V. Bagal, R. Aggarwal, P. Vinod, and U. D. Priyakumar, "Molgpt: molecular generation using a transformer-decoder model," Journal of Chemical Information and Modeling, vol. 62, no. 9, pp. 2064-2076, 2021.
[23] J. Wang, C.-Y. Hsieh, M. Wang, X. Wang, Z. Wu, D. Jiang, B. Liao, X. Zhang, B. Yang, Q. He et al., "Multi-constraint molecular generation based on conditional transformer, knowledge distillation and reinforcement learning," Nature Machine Intelligence, vol. 3, no. 10, pp. 914-922, 2021.
[24] Z. Zeng, Y. Yao, Z. Liu, and M. Sun, "A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals," Nature communications, vol. 13, no. 1, p. 862, 2022.
[25] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al., "Improving language understanding by generative pre-training," OpenAI, 2018.
[26] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., "Language models are unsupervised multitask learners," OpenAI blog, vol. 1, no. 8, p. 9, 2019.
[27] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al., "Training language models to follow instructions with human feedback," Advances in Neural Information Processing Systems, vol. 35, pp. 27730-27744, 2022.
[28] OpenAI, "Introducing chatgpt," 2022, https://openai.com/blog/ chatgpt.
[29] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du et al., "Lamda: Language models for dialog applications," arXiv preprint arXiv:2201.08239, 2022.
[30] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al., "Palm: Scaling language modeling with pathways," arXiv preprint arXiv:2204.02311, 2022.
[31] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez et al., "Vicuna: An open-
source chatbot impressing gpt-4 with $90 \%$ " chatgpt quality," See https://vicuna. lmsys. org (accessed 14 April 2023), 2023.
[32] A. M. Bran, S. Cox, A. D. White, and P. Schwaller, "Chemcrow: Augmenting large-language models with chemistry tools," arXiv preprint arXiv:2304.05376, 2023.
[33] A. D. White, "The future of chemistry is language," Nature Reviews Chemistry, pp. 1-2, 2023.
[34] S. Chithrananda, G. Grand, and B. Ramsundar, "Chemberta: Largescale self-supervised pretraining for molecular property prediction," arXiv preprint arXiv:2010.09885, 2020.
[35] S. Liu, W. Nie, C. Wang, J. Lu, Z. Qiao, L. Liu, J. Tang, C. Xiao, and A. Anandkumar, "Multi-modal molecule structure-text model for text-based retrieval and editing," arXiv preprint arXiv:2212.10789, 2022.
[36] N. Frey, R. Soklaski, S. Axelrod, S. Samsi, R. Gomez-Bombarelli, C. Coley, and V. Gadepally, "Neural scaling of deep chemical models," chemrxiv, 2022.
[37] Z. Wang, L. Liang, Z. Yin, and J. Lin, "Improving chemical similarity ensemble approach in target prediction," Journal of cheminformatics, vol. 8, pp. 1-10, 2016.
[38] D. Butina, "Unsupervised data base clustering based on daylight's fingerprint and tanimoto similarity: A fast and automated way to cluster small and large data sets," Journal of Chemical Information and Computer Sciences, vol. 39, no. 4, pp. 747-750, 1999.
[39] S. Robertson, H. Zaragoza et al., "The probabilistic relevance framework: Bm25 and beyond," Foundations and Trends® in Information Retrieval, vol. 3, no. 4, pp. 333-389, 2009.
[40] L. R. Dice, "Measures of the amount of ecologic association between species," Ecology, vol. 26, no. 3, pp. 297-302, 1945.
[41] D. E. Coupry and P. Pogány, "Application of deep metric learning to molecular graph similarity," Journal of Cheminformatics, vol. 14, no. 1, pp. 1-12, 2022.
[42] A. Aizawa, "An information-theoretic perspective of tf-idf measures," Information Processing \&amp; Management, vol. 39, no. 1, pp. 45-65, 2003.
[43] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar et al., "Llama: Open and efficient foundation language models," arXiv preprint arXiv:2302.13971, 2023.
[44] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need," Advances in neural information processing systems, vol. 30, 2017.
[45] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, "Exploring the limits of transfer learning with a unified text-to-text transformer," The Journal of Machine Learning Research, vol. 21, no. 1, pp. 5485-5551, 2020.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Jiatong Li is currently a PhD student of the Department of Computing (COMP), The Hong Kong Polytechnic University (funded by HKPFS). Before joining the PolyU, he received my Master's degree of Information Technology (with Distinction) from the University of Melbourne, under the supervision of Dr. Lea Frermann. In 2021, he got his bachelor's degree in Information Security from Shanghai Jiao Tong University. His interest lies in Natural Language Processing, Drug Discovery, and Recommender Systems. He has published innovative works in top-tier conferences such as IJCAI and ACL. For more information, please visit https://phenixace.github.io/.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Yunqing Liu is currently a PhD student of the Department of Computing (COMP), Hong Kong Polytechnic University (PolyU), under the supervision of Dr. Wenqi Fan. Before joining the PolyU, he received his Master's degree in Computer Science from the University of Edinburgh (M.Sc. in Computer Science), under the supervision of Dr. Elizabeth Polgreen. In 2020, he got his bachelor's degrees from Wuhan University (B.Sc. in Chemistry and B.Eng. in Computer Science and Technology). His research interest includes Drug Discovery, Graph Neural Networks, and Natural Language Processing. He has published innovative works in top-tier conferences and journals such as IJCAI, EACL, EurJOC and Organic Letters. For more information, please visit https://liuyunqing.github.io/.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Wenqi Fan is a research assistant professor of the Department of Computing at The Hong Kong Polytechnic University (PolyU). He received his Ph.D. degree from the City University of Hong Kong (CityU) in 2020. From 2018 to 2020, he was a visiting research scholar at Michigan State University (MSU). His research interests are in the broad areas of machine learning and data mining, with a particular focus on Recommender Systems, Graph Neural Networks, and Trustworthy Recommendations. He has published innovative papers in top-tier journals and conferences such as TKDE, TIST, KDD, WWW, ICDE, NeurIPS, SIGIR, IJCAI, AAAI, RecSys, WSDM, etc. He serves as top-tier conference (senior) program committee members and session chairs (e.g., ICML, ICLR, NeurIPS, KDD, WWW, AAAI, IJCAI, WSDM, etc.), and journal reviewers (e.g., TKDE, TIST, TKDD, TOIS, TAI, etc.). More information about him can be found at https://wenqifan03.github.io.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Xiao-yong Wei is a visiting professor of the Department of Computing, The Hong Kong Polytechnic University. He has been a professor and the head of the Department of Computer Science, Sichuan University of China since 2010. He received his Ph.D. in Computer Science from the City University of Hong Kong and has worked as a postdoctoral fellow in the University of California, Berkeley. His research interests include Multimedia Computing, Health Computing, Machine Learning, and Large-Scale Data Mining. He is a senior member of IEEE, and has served as an associate editor of Interdisciplinary Sciences: Computational Life Sciences since 2020, the program chair of ICMR 2019, ICIMCS 2012, and the technical committee member of over 20 conferences such as ICCV, CVPR, ACM MM, ICME, and ICIP.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Hui Liu received her PhD degree in Electrical Engineering from Southern Methodist University in 2015. Her research interests include trustworthy AI, designing data mining algorithms for wireless communication of smart devices, and applying machine learning and data mining in wireless communications. She has more than 7 -year research experience in data science on mobile data.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Jiliang Tang is a University Foundation Professor in the computer science and engineering department at Michigan State University since 2022. He was an associate professor (20212022) and an assistant professor (2016-2021) in the same department. Before that, he was a research scientist in Yahoo Research and got his PhD from Arizona State University in 2015 under Dr. Huan Liu. His research interests include graph machine learning, trustworthy AI and their applications in education and biology. He was the recipient of various awards including 2022 AI's 10 to Watch, 2022 IAPR J. K. AGGARWAL Award, 2022 SIAM/IBM Early Career Research Award, 2021 IEEE ICDM Tao Li Award, 2021 IEEE Big Data Security Junior Research Award, 2020 ACM SIGKDD Rising Star Award, 2020 Distinguished Withrow Research Award, 2019 NSF Career Award, and 8 best paper awards (or runner-ups). His dissertation won the 2015 KDD Best Dissertation runner up and Dean's Dissertation Award. He serves as conference organizers (e.g., KDD, SIGIR, WSDM and SDM) and journal editors (e.g., TKDD, TOIS and TKDE). He has published his research in highly ranked journals and top conference proceedings, which have received tens of thousands of citations with h-index 82 (Google Scholar) and extensive media coverage. More details about him can be found at https://www.cse.msu.edu/ tanggili/.
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Qing Li received the B.Eng. degree from Hunan University, Changsha, China, and the M.Sc. and Ph.D. degrees from the University of Southern California, Los Angeles, all in computer science. He is currently a Chair Professor (Data Science) and the Head of the Department of Computing, the Hong Kong Polytechnic University. He is a Fellow of IEEE and IET, a member of ACM SIGMOD and IEEE Technical Committee on Data Engineering. His research interests include object modeling, multimedia databases, social media, and recommender systems. He has been actively involved in the research community by serving as an associate editor and reviewer for technical journals, and as an organizer/co-organizer of numerous international conferences. He is the chairperson of the Hong Kong Web Society, and also served/is serving as an executive committee (EXCO) member of IEEE-Hong Kong Computer Chapter and ACM Hong Kong Chapter. In addition, he serves as a councilor of the Database Society of Chinese Computer Federation (CCF), a member of the Big Data Expert Committee of CCF, and is a Steering Committee member of DASFAA, ER, ICWL, UMEDIA, and WISE Society.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>J. Li, Y. Liu, W. Fan, X. Wei, and Q. Li are with the Department of Computing, The Hong Kong Polytechnic University. E-mail: {jiatong.li, yunqing617.liu}@connect.polyu.hk, wenqifan03@gmail.com, x1wei@polyu.edu.hk, csqli@comp.polyu.edu.hk.</li>
<li>J. Tang and H. Liu are with Michigan State University. E-mail: tangjili@msu.edu and liuliui7@msu.edu.
(Corresponding authors: Yunqing Liu and Qing Li.)</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
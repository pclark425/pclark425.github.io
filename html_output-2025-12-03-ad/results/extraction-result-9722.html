<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9722 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9722</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9722</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-278715216</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.10586v1.pdf" target="_blank">Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports</a></p>
                <p><strong>Paper Abstract:</strong> Timely and accurate situation awareness is vital for decision-making in humanitarian response, conflict monitoring, and early warning and early action. However, the manual analysis of vast and heterogeneous data sources often results in delays, limiting the effectiveness of interventions. This paper introduces a dynamic Retrieval-Augmented Generation (RAG) system that autonomously generates situation awareness reports by integrating real-time data from diverse sources, including news articles, conflict event databases, and economic indicators. Our system constructs query-specific knowledge bases on demand, ensuring timely, relevant, and accurate insights. To ensure the quality of generated reports, we propose a three-level evaluation framework that combines semantic similarity metrics, factual consistency checks, and expert feedback. The first level employs automated NLP metrics to assess coherence and factual accuracy. The second level involves human expert evaluation to verify the relevance and completeness of the reports. The third level utilizes LLM-as-a-Judge, where large language models provide an additional layer of assessment to ensure robustness. The system is tested across multiple real-world scenarios, demonstrating its effectiveness in producing coherent, insightful, and actionable reports. By automating report generation, our approach reduces the burden on human analysts and accelerates decision-making processes. To promote reproducibility and further research, we openly share our code and evaluation tools with the community via GitHub.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9722.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9722.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-as-a-judge (GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o used as an automated judge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4o was prompted to evaluate RAG-generated situation awareness reports using the same structured questionnaire given to human experts; it tended to assign systematically higher scores and showed clear self-preference when judging its own outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Situation awareness report generation / summarization (RAG-based peacebuilding reports)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>GPT-4o was prompted with the same two-part questionnaire used by human experts (Part A: binary relevance/completeness questions Q1–Q7; Part B: pairwise preference comparisons Q8–Q10) and applied to all generated reports to produce evaluation scores.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Two senior UNDP human experts independently evaluated each report (60 reports total) answering the same two-part questionnaire (binary relevancy/completeness and pairwise preference); inter-annotator agreement measured with Cohen's Kappa.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Cohen's Kappa (human inter-annotator agreement: 0.54 for GPT-generated reports and 0.57 for LLaMA-generated reports). LLM-as-judge numeric outcomes: GPT-as-a-judge gave its own reports a perfect 1.0 score; Claude scored GPT reports at 0.95. (Reported in 'Results and Discussion' and 'Human vs. LLM Evaluation' sections.)</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Using GPT-as-a-judge loses human-like critical variability and skepticism: it exhibits overconfidence and self-preference (inflated scores), different prioritization of evaluation criteria (e.g., focusing on coverage rather than redundancy), and therefore may fail to surface the same weaknesses humans detect (notably redundancy and irrelevant content).</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Concrete example: GPT-as-a-judge assigned a perfect 100% score to reports it generated, whereas human experts gave GPT-generated reports an average of 62% on binary questions; humans repeatedly highlighted redundancy (Q4) and irrelevant content proportions (Q5) as key weaknesses, which GPT-as-a-judge did not emphasize.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>GPT-as-a-judge did align with human overall preference in one respect: humans preferred GPT-generated reports in 76% of cases and GPT-as-a-judge also favored its own reports, so in preference ranking there can be agreement; nevertheless this alignment may reflect shared bias rather than independent validation. The paper also notes that LLM judging can scale evaluation but must be used cautiously.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Sections: 'LLM-as-a-Judge Evaluation', 'Results and Discussion', 'Human vs. LLM Evaluation' in Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9722.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9722.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-as-a-judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anthropic Claude used as an automated judge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Claude was used as a third, independent LLM judge to reduce single-model self-bias; its evaluations diverged from human evaluators in some respects, and it preferred LLaMA-generated reports slightly more than GPT-generated reports.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Situation awareness report generation / summarization (RAG-based peacebuilding reports)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Claude (Anthropic)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Claude was prompted with the same questionnaire used by humans (Parts A and B) and applied to all reports; used specifically to assess and avoid self-bias of other LLM judges.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Same human setup as above: two senior UNDP evaluators independently scored reports using the two-part questionnaire; inter-annotator agreement reported via Cohen's Kappa.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Claude scored GPT reports at 0.95 (near-perfect), and Claude-as-a-judge preferred LLaMA-generated reports (0.98) slightly more than GPT reports (0.95). Humans preferred GPT-generated reports in 76% of cases, indicating divergence between Claude and human preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Using Claude (or other LLM judges) can produce evaluations that diverge from human judgments — Claude favored LLaMA outputs more than humans did — indicating that LLM judges may prioritize different quality aspects and thus miss or mis-rank human-valued weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Example of divergence: Claude's preference scores (LLaMA 0.98 vs GPT 0.95) differ from human preferences (humans preferred GPT reports 76% of the time), showing that Claude's ranking of outputs can contradict human evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Claude still provided high-quality, near-consistent judgements (useful for scaling) and served as a cross-check against model self-bias (e.g., checking GPT's self-ratings). However, its preference divergence demonstrates the need to combine LLM judges with human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Sections: 'LLM-as-a-Judge Evaluation', 'Results and Discussion', 'Human vs. LLM Evaluation' in Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9722.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9722.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-as-a-judge (LLaMA 3)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 3 used as an automated judge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLaMA 3 was used both as a generator of reports and as an automated judge; as a judge it was generally more critical than GPT and in some respects mirrored human evaluators by giving lower/self-critical scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Situation awareness report generation / summarization (RAG-based peacebuilding reports)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>LLaMA 3</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>LLaMA 3 was prompted using the same human questionnaire (binary Part A and preference Part B) to evaluate all generated reports, producing internal judgment scores for both its own and GPT outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Two senior UNDP human experts independently scored each report; the same questionnaire and Cohen's Kappa inter-annotator agreement were used as in other evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>LLaMA-as-a-judge was more critical: it scored itself at 0.78 and scored GPT at 0.82 in relative evaluations; human inter-annotator agreement values were ~0.54–0.57 for different sets. Humans preferred GPT-generated reports 76% of the time, while LLaMA-as-judge's metrics indicate it was less favorable overall.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Although LLaMA-as-a-judge is more critical (reducing the overconfidence problem seen with GPT), automated LLaMA evaluation can still differ from human judgments in which specific failure modes are prioritized, and automated judges in general may still miss human-subjective assessments (e.g., usability, nuance, redundancy perceived by practitioners).</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Example divergence: humans consistently flagged Q4 (duplicate/redundant information) and Q5 (proportion of irrelevant information) as top problems; LLM judges (in aggregate) focused more on Q3 (coverage >90%), indicating differing emphasis and potential blind spots in automated judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>LLaMA-as-a-judge's greater criticality suggests some LLM judges can better approximate human skepticism than others, illustrating that choice of judge-model affects alignment; still, differences in criterion interpretation persist and human review remained necessary according to authors.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Sections: 'LLM-as-a-Judge Evaluation', 'Results and Discussion', 'Human vs. LLM Evaluation' in Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Veriscore: Evaluating the factuality of verifiable claims in long-form text generation <em>(Rating: 2)</em></li>
                <li>Summac: Revisiting nli-based models for inconsistency detection in summarization <em>(Rating: 2)</em></li>
                <li>Evaluating retrieval-augmented generation models for financial report question and answering <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9722",
    "paper_id": "paper-278715216",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "GPT-as-a-judge (GPT-4o)",
            "name_full": "GPT-4o used as an automated judge",
            "brief_description": "GPT-4o was prompted to evaluate RAG-generated situation awareness reports using the same structured questionnaire given to human experts; it tended to assign systematically higher scores and showed clear self-preference when judging its own outputs.",
            "citation_title": "Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports",
            "mention_or_use": "use",
            "task_domain": "Situation awareness report generation / summarization (RAG-based peacebuilding reports)",
            "llm_judge_model": "GPT-4o",
            "llm_judge_setup": "GPT-4o was prompted with the same two-part questionnaire used by human experts (Part A: binary relevance/completeness questions Q1–Q7; Part B: pairwise preference comparisons Q8–Q10) and applied to all generated reports to produce evaluation scores.",
            "human_evaluation_setup": "Two senior UNDP human experts independently evaluated each report (60 reports total) answering the same two-part questionnaire (binary relevancy/completeness and pairwise preference); inter-annotator agreement measured with Cohen's Kappa.",
            "agreement_metric": "Cohen's Kappa (human inter-annotator agreement: 0.54 for GPT-generated reports and 0.57 for LLaMA-generated reports). LLM-as-judge numeric outcomes: GPT-as-a-judge gave its own reports a perfect 1.0 score; Claude scored GPT reports at 0.95. (Reported in 'Results and Discussion' and 'Human vs. LLM Evaluation' sections.)",
            "losses_identified": "Using GPT-as-a-judge loses human-like critical variability and skepticism: it exhibits overconfidence and self-preference (inflated scores), different prioritization of evaluation criteria (e.g., focusing on coverage rather than redundancy), and therefore may fail to surface the same weaknesses humans detect (notably redundancy and irrelevant content).",
            "examples_of_loss": "Concrete example: GPT-as-a-judge assigned a perfect 100% score to reports it generated, whereas human experts gave GPT-generated reports an average of 62% on binary questions; humans repeatedly highlighted redundancy (Q4) and irrelevant content proportions (Q5) as key weaknesses, which GPT-as-a-judge did not emphasize.",
            "counterexamples_or_caveats": "GPT-as-a-judge did align with human overall preference in one respect: humans preferred GPT-generated reports in 76% of cases and GPT-as-a-judge also favored its own reports, so in preference ranking there can be agreement; nevertheless this alignment may reflect shared bias rather than independent validation. The paper also notes that LLM judging can scale evaluation but must be used cautiously.",
            "paper_reference": "Sections: 'LLM-as-a-Judge Evaluation', 'Results and Discussion', 'Human vs. LLM Evaluation' in Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports",
            "uuid": "e9722.0",
            "source_info": {
                "paper_title": "Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Claude-as-a-judge",
            "name_full": "Anthropic Claude used as an automated judge",
            "brief_description": "Claude was used as a third, independent LLM judge to reduce single-model self-bias; its evaluations diverged from human evaluators in some respects, and it preferred LLaMA-generated reports slightly more than GPT-generated reports.",
            "citation_title": "Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports",
            "mention_or_use": "use",
            "task_domain": "Situation awareness report generation / summarization (RAG-based peacebuilding reports)",
            "llm_judge_model": "Claude (Anthropic)",
            "llm_judge_setup": "Claude was prompted with the same questionnaire used by humans (Parts A and B) and applied to all reports; used specifically to assess and avoid self-bias of other LLM judges.",
            "human_evaluation_setup": "Same human setup as above: two senior UNDP evaluators independently scored reports using the two-part questionnaire; inter-annotator agreement reported via Cohen's Kappa.",
            "agreement_metric": "Claude scored GPT reports at 0.95 (near-perfect), and Claude-as-a-judge preferred LLaMA-generated reports (0.98) slightly more than GPT reports (0.95). Humans preferred GPT-generated reports in 76% of cases, indicating divergence between Claude and human preferences.",
            "losses_identified": "Using Claude (or other LLM judges) can produce evaluations that diverge from human judgments — Claude favored LLaMA outputs more than humans did — indicating that LLM judges may prioritize different quality aspects and thus miss or mis-rank human-valued weaknesses.",
            "examples_of_loss": "Example of divergence: Claude's preference scores (LLaMA 0.98 vs GPT 0.95) differ from human preferences (humans preferred GPT reports 76% of the time), showing that Claude's ranking of outputs can contradict human evaluators.",
            "counterexamples_or_caveats": "Claude still provided high-quality, near-consistent judgements (useful for scaling) and served as a cross-check against model self-bias (e.g., checking GPT's self-ratings). However, its preference divergence demonstrates the need to combine LLM judges with human oversight.",
            "paper_reference": "Sections: 'LLM-as-a-Judge Evaluation', 'Results and Discussion', 'Human vs. LLM Evaluation' in Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports",
            "uuid": "e9722.1",
            "source_info": {
                "paper_title": "Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "LLaMA-as-a-judge (LLaMA 3)",
            "name_full": "LLaMA 3 used as an automated judge",
            "brief_description": "LLaMA 3 was used both as a generator of reports and as an automated judge; as a judge it was generally more critical than GPT and in some respects mirrored human evaluators by giving lower/self-critical scores.",
            "citation_title": "Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports",
            "mention_or_use": "use",
            "task_domain": "Situation awareness report generation / summarization (RAG-based peacebuilding reports)",
            "llm_judge_model": "LLaMA 3",
            "llm_judge_setup": "LLaMA 3 was prompted using the same human questionnaire (binary Part A and preference Part B) to evaluate all generated reports, producing internal judgment scores for both its own and GPT outputs.",
            "human_evaluation_setup": "Two senior UNDP human experts independently scored each report; the same questionnaire and Cohen's Kappa inter-annotator agreement were used as in other evaluations.",
            "agreement_metric": "LLaMA-as-a-judge was more critical: it scored itself at 0.78 and scored GPT at 0.82 in relative evaluations; human inter-annotator agreement values were ~0.54–0.57 for different sets. Humans preferred GPT-generated reports 76% of the time, while LLaMA-as-judge's metrics indicate it was less favorable overall.",
            "losses_identified": "Although LLaMA-as-a-judge is more critical (reducing the overconfidence problem seen with GPT), automated LLaMA evaluation can still differ from human judgments in which specific failure modes are prioritized, and automated judges in general may still miss human-subjective assessments (e.g., usability, nuance, redundancy perceived by practitioners).",
            "examples_of_loss": "Example divergence: humans consistently flagged Q4 (duplicate/redundant information) and Q5 (proportion of irrelevant information) as top problems; LLM judges (in aggregate) focused more on Q3 (coverage &gt;90%), indicating differing emphasis and potential blind spots in automated judgments.",
            "counterexamples_or_caveats": "LLaMA-as-a-judge's greater criticality suggests some LLM judges can better approximate human skepticism than others, illustrating that choice of judge-model affects alignment; still, differences in criterion interpretation persist and human review remained necessary according to authors.",
            "paper_reference": "Sections: 'LLM-as-a-Judge Evaluation', 'Results and Discussion', 'Human vs. LLM Evaluation' in Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports",
            "uuid": "e9722.2",
            "source_info": {
                "paper_title": "Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Veriscore: Evaluating the factuality of verifiable claims in long-form text generation",
            "rating": 2,
            "sanitized_title": "veriscore_evaluating_the_factuality_of_verifiable_claims_in_longform_text_generation"
        },
        {
            "paper_title": "Summac: Revisiting nli-based models for inconsistency detection in summarization",
            "rating": 2,
            "sanitized_title": "summac_revisiting_nlibased_models_for_inconsistency_detection_in_summarization"
        },
        {
            "paper_title": "Evaluating retrieval-augmented generation models for financial report question and answering",
            "rating": 1,
            "sanitized_title": "evaluating_retrievalaugmented_generation_models_for_financial_report_question_and_answering"
        }
    ],
    "cost": 0.00973125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports</p>
<p>Poli A Nemkova poli.nemkova@unt.edu 
University of North Texas</p>
<p>United Nations Development Programme</p>
<p>Suleyman O Polat suleymanolcaypolat@my.unt.edu 
University of North Texas</p>
<p>Rafid I Jahan rafidishrakjahan@my.unt.edu 
University of North Texas</p>
<p>Sagnik Ray Choudhury sagnik.raychoudhury@unt.edu 
University of North Texas</p>
<p>Sun-Joo Lee sun-joo.lee@undp.org 
United Nations Development Programme</p>
<p>Shouryadipta Sarkar shouryadipta.sarkar@undp.org 
United Nations Development Programme</p>
<p>Mark V Albert mark.albert@unt.edu 
University of North Texas</p>
<p>Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports
4A1B7206D464486553DC31818F913983
Timely and accurate situation awareness is vital for decision-making in humanitarian response, conflict monitoring, and early warning and early action.However, the manual analysis of vast and heterogeneous data sources often results in delays, limiting the effectiveness of interventions.This paper introduces a dynamic Retrieval-Augmented Generation (RAG) system that autonomously generates situation awareness reports by integrating real-time data from diverse sources, including news articles, conflict event databases, and economic indicators.Our system constructs query-specific knowledge bases on demand, ensuring timely, relevant, and accurate insights.To ensure the quality of generated reports, we propose a three-level evaluation framework that combines semantic similarity metrics, factual consistency checks, and expert feedback.The first level employs automated NLP metrics to assess coherence and factual accuracy.The second level involves human expert evaluation to verify the relevance and completeness of the reports.The third level utilizes LLM-as-a-Judge, where large language models provide an additional layer of assessment to ensure robustness.The system is tested across multiple real-world scenarios, demonstrating its effectiveness in producing coherent, insightful, and actionable reports.By automating report generation, our approach reduces the burden on human analysts and accelerates decision-making processes.To promote reproducibility and further research, we openly share our code and evaluation tools with the community via GitHub.</p>
<p>Introduction</p>
<p>Timely situation awareness is essential for effective decisionmaking in peacekeeping operations, humanitarian response, and governmental interventions.Organizations such as the United Nations (UN), non-governmental organizations (NGOs), and policy-makers rely on accurate and up-to-date reports to allocate resources, anticipate crises, and mitigate conflict escalation.However, generating these reports manually is time-intensive, requiring extensive data collection, synthesis, and expert analysis.The complexity of integrating heterogeneous, real-time data from multiple sources further exacerbates this challenge, leading to delays that can hinder timely interventions.</p>
<p>Advancements in large language models (LLMs) have demonstrated strong capabilities in text aggregation and summarization.However, standard LLM-based approaches face limitations such as hallucination, inconsistencies, and limited fact verification, making them unreliable for high-stakes decision-making.Retrieval-Augmented Generation (RAG) offers a promising alternative by integrating retrieval-based fact grounding with generative models.By dynamically retrieving and incorporating relevant, real-world data into the response generation process, RAG significantly improves factual consistency while reducing hallucinations.</p>
<p>In this paper, we propose a multi-modal dynamic RAG framework designed for automated situation awareness reporting.Our system ingests real-time data from diverse sources, including news articles, conflict event databases, and economic indicators, constructing query-specific knowledge bases to ensure contextually relevant and temporally prompt insights.Reports generated through this system provide stakeholders with structured, evidence-backed summaries, reducing the time and effort required for manual analysis.</p>
<p>To assess the reliability and effectiveness of our system, we introduce a three-level evaluation framework that includes:</p>
<p>Automated NLP-Based Metrics -Assessing factual accuracy, completeness, and coherence using semantic similarity models, factual consistency checks, and bias detection tools.Human Expert Review -Evaluating relevance, completeness, and usability through domain experts from the UN.LLM-as-a-Judge Assessment -Benchmarking the model's performance using LLM-based evaluation, comparing outputs with human judgments.We evaluate our approach across multiple real-world scenarios and demonstrate that our RAG system produces coherent, insightful, and actionable reports while reducing the burden on human analysts.By automating the early-stage report generation process, we accelerate decision-making while maintaining expert oversight.To encourage further research and reproducibility, we openly share our code, evaluation framework, and sample reports with the arXiv:2505.10586v1[cs.CY] 14 May 2025 community.</p>
<p>To the best of our knowledge, this study presents the first application of a Retrieval-Augmented Generation (RAG) framework integrated with large language models (LLMs) in the peacebuilding domain.Furthermore, it constitutes the first documented instance of AI-driven automation leveraging publicly available data to support decision-making processes for high-impact non-governmental organizations (NGOs), including the United Nations, a key partner in this project.</p>
<p>As a technical contribution, we introduce a dynamic RAG implementation coupled with a three-level reference-free evaluation framework.Additionally, we developed a modified version of VERISCORE [Song et al., 2024], packaged as ragve, which enables verification that the RAG-generated outputs are grounded solely in the retrieved data-a valuable tool for researchers working on similar models.The ragve package, along with our full implementation, is publicly available on GitHub.</p>
<p>Literature Review</p>
<p>Retrieval-Augmented Generation (RAG) represents an advanced methodology that integrates the generative power of large language models (LLMs) with the precision of information retrieval systems.By leveraging external knowledge sources, RAG frameworks enhance the contextual relevance and factual accuracy of generated content.This hybrid approach has demonstrated substantial benefits across diverse fields such as safety, finance, healthcare, and scientific research, significantly improving the quality and efficiency of automated reporting [Gao et al., 2023;Fan et al., 2024;Arslan et al., 2024].</p>
<p>In safety-critical domains, RAG frameworks have been effectively customized to generate comprehensive reports based on session logs and incident descriptions.For instance, in the aviation sector, models like LLaMA combined with embedding techniques have automated safety report generation, achieving significant improvements in accuracy and compliance with documentation standards.Metrics such as Recall@5, GLEU, METEOR, and BERTScore highlight the superiority of RAG-based methods over traditional reporting systems [Bernardi et al., 2024;Suresh et al., 2024].</p>
<p>In the financial sector, RAG has enhanced the analysis and interpretation of complex financial reports, particularly in question-answering tasks for private investors.Utilizing models like OpenAI's ADA and GPT-4, these systems process half-yearly and quarterly reports with high contextual relevance and accuracy.Research underscores that wellstructured financial documents further optimize RAG performance, especially in addressing qualitative queries [Iaroshev et al., 2024].</p>
<p>Similarly, in healthcare, RAG has been employed to automate the generation of radiology reports through multimodal embeddings, facilitating the retrieval of relevant clinical information.Generative models, such as OpenAI's GPT series, integrate user-specific clinical requirements to improve report quality while mitigating hallucination risks.These systems have achieved enhanced performance on clinical metrics like BERTScore and Semb score, showcasing the framework's utility in high-stakes environments [Ranjit et al., 2023;Markey et al., 2024;Wang et al., 2024;Assistant et al., 2024;Alam et al., 2024].</p>
<p>While RAG has proven successful across these domains, its application in peacebuilding remains an emerging area of exploration.Recent advances in Natural Language Processing (NLP) have demonstrated the potential of LLMs and RAG frameworks for conflict prediction and monitoring human rights violations.Studies have successfully applied NLP techniques to detect armed conflicts and human rights abuses using diverse data sources, including social media and news reports [Trivedi et al., 2020;Alhelbawy et al., 2020;Mueller et al., 2024;Nemkova et al., 2023].However, comprehensive, automated situation awareness reporting for conflict prevention, resolution, and post-conflict recovery is still underdeveloped.Our work seeks to bridge this gap by extending the proven capabilities of RAG frameworks to peacebuilding, offering a robust solution for humanitarian and governmental organizations.</p>
<p>The evolution of multimodal RAG systems has further expanded report generation capabilities by incorporating diverse data types, including text, tables, and images.These systems enhance retrieval and content generation by leveraging interrelationships between different modalities, thereby improving the contextual richness and accuracy of generated reports [Joshi et al., 2024;?].</p>
<p>RAG frameworks have demonstrated considerable potential across multiple fields, improving the accuracy, contextual relevance, and efficiency of automated reporting.Our work builds upon these successes by adapting RAG for peacebuilding applications, addressing the unique challenges of conflict monitoring and humanitarian reporting.By doing so, we aim to contribute a novel, practical tool that enhances decisionmaking in complex geopolitical contexts.</p>
<p>Method and System Design</p>
<p>To generate comprehensive situation awareness reports, we implemented a dynamic Retrieval-Augmented Generation (RAG) framework.System design can be seen on Figure 1.</p>
<p>Data Fetching.The system operates based on user input specifying the country of interest and the start and end dates of the desired reporting period.Upon receiving these parameters, the system issues four distinct API queries to retrieve relevant data from GDELT, ACLED, ReliefWeb, and the World Bank.</p>
<p>• GDELT (Global Database of Events, Language, and Tone)1 : GDELT is a publicly available database that captures global political and international affairs events extracted from online news articles.For each relevant event, GDELT provides a link to the original article.After retrieving GDELT event data, we scrape the full text of the referenced news articles to enrich the dataset using library Python newspaper2 .</p>
<p>• ACLED (Armed Conflict Location &amp; Event Data • World Bank5 : The World Bank API is used to collect relevant economic indicators, including GDP (current US$), GDP growth (annual %), inflation rate, unemployment rate, and military expenditure (% of GDP).</p>
<p>Data Processing.</p>
<p>To ensure compatibility with large language model (LLM) processing, all numerical data from ACLED and the World Bank were converted into textual descriptions by wrapping numbers into a pre-defined textual template.We removed missing values and duplicates for textual data (GDELT and ReliefWeb).</p>
<p>Vectorization Following data collection, we employed MiniLM6 to encode all extracted textual data into vector representations.MiniLM is efficient for encoding as it offers a compact, lightweight architecture that delivers good performance in generating contextualized embeddings while maintaining fast processing speeds and low resource consumption.At this step we have dynamic knowledge base ready.</p>
<p>Querying the Knowledge Base.Once the encoded dataset is prepared, we queried the KB with "Conflict and social unrest issues in {country}".</p>
<p>Evidence Retrieval.A similarity search, using FAISS (Facebook AI Similarity Search) [Douze et al., 2024], was then conducted to retrieve the most relevant data points for each query.Once the top 10 most relevant vector entries were identified, the corresponding full-text content was retrieved.This refined dataset serves as evidence set, which is subsequently used in the report generation process.LLM Report Generation.After retrieving the most relevant evidence, we used a prompt-based approach to generate a situation awareness report using a Large Language Model (LLM).The prompt is designed to structure the output in a clear and actionable format.We utilized two prompting strategies: (1) instruction and (2) personification.</p>
<p>The instruction prompt (1) included the following:</p>
<p>Provide a structured summary including the following sections:</p>
<p>• Important ongoing situation (if any, optional)</p>
<p>• Key recent insights</p>
<p>• Trends</p>
<p>• Recommendations (label this section as: Recommendation [experimental]) Whenever you reference a numerical value or factual information, cite the exact source from which it was obtained in parentheses.Below is the relevant evidence: {extracted earlier evidence}.</p>
<p>This structured prompt ensures that the generated report maintains coherence while providing evidence-based insights.By explicitly instructing the model to cite sources, we enhance the transparency and reliability of the generated content.</p>
<p>The personification prompt (2) included the same strict sectioning, but started with "You are a conflict analyst preparing a situation awareness report for humanitarian decision-makers.Use the evidence below to craft a clear, concise, and professional report."</p>
<p>To evaluate the effectiveness of our approach, we tested our prompt strategies using two different LLMs: GPT-4o 7 and LLaMA 3 8 .Additionally, we conducted preliminary experiments with DeepSeek [Guo et al., 2025], but the generated outputs were of insufficient quality for our use case.Consequently, we focused our analysis on GPT-4o and LLaMA 3.</p>
<p>The generated reports were saved in both .txtand .pdfformats to facilitate further analysis and sharing.The .txt format was used for structured text processing and comparison, while the .pdfformat ensured preservation of formatting and readability across different platforms.</p>
<p>All processing for this version of the model was conducted using Google Colab 9 with A100 GPU instances (with Pro+ membership).</p>
<p>Evaluation</p>
<p>Test Sample Selection.To evaluate the performance of our system, we generated 15 distinct input sets, each comprising a combination of a country, a start date, and an end date.As previously outlined, we employed two different LLM models and applied two distinct prompts.This experimental design resulted in a total of 60 reports for subsequent evaluation 10 .</p>
<p>To ensure a comprehensive evaluation, we generated reports for diverse geographical regions, covering a wide range of geopolitical and conflict dynamics.The selected countries included:</p>
<p>• Middle East (ME): Iran, Israel, Syria, Lebanon, Yemen • Eastern Europe (EE): Ukraine, Russia • Horns of Africa (HOA): Sudan, Ethiopia, Somalia, South Sudan</p>
<p>• Asia: Myanmar, China</p>
<p>Reports were generated for different timeframes, specifically covering periods of 1 month, 3 months, and 1 year.</p>
<p>To assess the performance and quality of the generated situation awareness reports, we implemented a three-layer evaluation framework as presented on Figure 2.</p>
<p>Level 1: NLP-Based Automated Metrics</p>
<p>At the first evaluation level, we leveraged existing NLP models to measure key text quality attributes.The following metrics were used:</p>
<p>• Accuracy: To assess factual correctness and detect potential hallucinations, we employed VERISCORE tool [Song et al., 2024].We used the tool to check the accuracy against most recent Google search.And also, we modified the tool to check the accuracy against the dynamic knowledge base -we provide a new library for this adjusted tool on GitHub. 117 https://openai.com/index/hello-gpt-4o/ 8https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct 9 https://colab.research.google.com/ 10 Sample of the reports can be found on author's Github 11 https://github.com/withheld-for-anonymity• Consistency: To evaluate consistency, we assessed whether the generated reports accurately retained the key points from the original, larger text.This task was conceptualized as analogous to summary evaluation, given the focus on preserving essential information.Accordingly, we utilized the SummaC library [Laban et al., 2022], a tool specifically designed for evaluating the quality and fidelity of summaries.</p>
<p>• Objectivity/Bias: To identify potential bias in the generated reports, we utilized politicalBiasBERT [Baly et al., 2020], a political classifier, as implemented on Hugging-Face 12 .</p>
<p>• Coherence/Clarity: To evaluate the structural and linguistic coherence of the reports, we computed a coherence score based on BERT-based text similarity measures [Devlin, 2018].</p>
<p>Only reports that met acceptable threshold values in Level 1 evaluation were forwarded to the next stage, Level 2: Human Expert Evaluation.</p>
<p>Level 2: Human Expert Evaluation At this stage, the reports were assessed by two Human Expert Evaluators, who are senior employees at United Nations Development Programme (UNDP) 13 and represent the target users of the generated reports.The evaluation panel consisted of both male and female experts to ensure diverse perspectives in the assessment process.Each report was independently evaluated by two experts, ensuring that each report received two separate evaluation scores.</p>
<p>Evaluation Criteria.The human expert evaluators assessed each report based on a structured set of evaluation questions categorized into two main parts: Relevancy and Completeness (binary evaluation) and Preference-Based Comparisons.Part A: Relevancy and Completeness (True/False).Each evaluator answered the following binary questions to assess the report's quality:</p>
<p>• Q1.Is the report relevant?</p>
<p>• Q2.Does more than 50% of the report contain relevant information?</p>
<p>• Q3.Does more than 90% of the report contain relevant information?</p>
<p>• Q4.Does the report avoid duplicate information?</p>
<p>• Q5.Does the report contain no more than 10% of the irrelevant information?</p>
<p>• Q6.Does the report seem to be complete?</p>
<p>• Q7.Does the report cover economic, political, social, or humanitarian aspects?</p>
<p>Part B: Preference-Based Comparison.In addition to the binary evaluation, the experts were asked to compare pairs of reports and indicate their preferences:</p>
<p>• Q8.Which report is more complete?(Report 1 vs. Report 2, Report 3 vs.Report 4)</p>
<p>• Q9.Which report is more accurate?(Report 1 vs. Report 2, Report 3 vs.Report 4)</p>
<p>• Q10.Which report do you prefer overall?(Report 1 vs. Report 2, Report 3 vs.Report 4)</p>
<p>This evaluation framework ensures a rigorous assessment of the reports' factual accuracy, completeness, and overall usability from the perspective of expert users.</p>
<p>Level 3: LLM-as-a-Judge Evaluation.</p>
<p>To ensure the potential scalability of our evaluation framework, we implemented an LLM-as-a-Judge approach.Specifically, we prompted GPT-4o and LLaMA 3 to evaluate all reports using the same questionnaire as the Human Expert Evaluators.To assess and avoid model's self bias, we also used third LLM -Claude 214 to evaluate the reports.</p>
<p>Results and Discussion</p>
<p>Human evaluation.Results of human evaluation are presented in Table 3.The inter-annotator agreement between the two human experts, measured using Cohen's Kappa [Cohen, 1960], indicated a moderate level of agreement, with values of 0.54 for GPT-generated reports and 0.57 for LLaMagenerated reports.</p>
<p>In the binary evaluation of Part A (Relevance and Completeness), GPT-generated reports achieved an average of 62% of the total possible points, while LLaMa-generated reports scored slightly higher at 64%.</p>
<p>In the preference-based evaluation of Part B, human experts selected GPT-generated reports in 76% of cases, whereas LLaMA-generated reports were preferred in only 24% of cases.</p>
<p>Notably, the aspects that consistently received the lowest scores were largely similar for both GPT-generated and LLaMa-generated reports.These included questions 4, 5, and 7, which focus on issues of redundant information and the omission of specific aspect of coverage.Additionally, LLaMa-generated reports demonstrated particular difficulties with question 6, which pertains to the completeness of the reports.</p>
<p>LLM-as-a-Judge.Notably, GPT assigned an average perfect score of 100% to reports generated by itself, compared to 93% for those produced by LLaMA.The primary weakness identified by GPT across both sets of reports was the presence of redundant information, as reflected in Question 4 (Q4) of the evaluation criteria.This issue was more pronounced in the LLaMA-generated reports, which received the lowest scores in this category.</p>
<p>Human vs. LLM Evaluation</p>
<p>The highest Cohen's Kappa values from humans were around 0.57 (GPT-generated, prompt 2) and 0.54 (LLaMAgenerated, prompt2).These values indicate moderate agreement among human evaluators.For binary evaluations, humans showed varying agreement, with LLaMA-generated reports (prompt 2) achieving the highest human agreement (0.61).GPT-as-a-judge gave itself a perfect 1.0 score, while Claude scored GPT reports at 0.95-both suggesting nearperfect evaluations.LLaMA, however, was more critical of both itself (0.78) and GPT (0.82).</p>
<p>Human evaluations show more variability and critical judgment, reflected in moderate Cohen's Kappa scores, while LLM judges-especially GPT and Claude-tend to rate reports significantly higher, hinting at possible overconfidence or evaluation bias in LLMs.LLaMA-as-a-judge mirrors human evaluators to some extent by being more critical, particularly towards its own reports.Interestingly, Claude-asa-judge preferred LLaMA-generated reports (0.98) slightly more than GPT reports (0.95), suggesting Claude's evaluation diverges from human preferences.GPT-as-a-judge rated its own reports higher than LLaMA's, aligning more closely with human preferences.While GPT-as-a-judge aligns with human preferences by favoring its own reports, Claude shows a bias toward LLaMA-generated reports, diverging from human evaluators.This indicates that LLM judges may not always reflect human judgment, especially when cross-model evaluations are involved.Human evaluators and LLM judges identify different weaknesses in the reports.Humans consistently highlight Q4 and Q5, while LLMs focus on Q3.This discrepancy suggests that LLMs and humans prioritize different aspects of report quality or interpret the evaluation criteria differently.</p>
<p>Strengths and Limitations of the Approach</p>
<p>Strengths:</p>
<p>• The system dynamically retrieves relevant information, ensuring reports are evidence-based.All the data used in the study is free and publicly available.• The multi-layered evaluation framework enhances reliability and robustness.Human Evaluation is only used for aligning automated evaluation that will be needed for scaling.• The use of multiple LLMs (GPT-4o, LLaMA 3) allows for comparative analysis and improved output quality.• The generated reports significantly reduce the time required for human analysts to draft reports from scratch.</p>
<p>Currently a human analyst, on average, can take up to 2 weeks to create similar report.With our system, this time can drop to 1 week (generated report is used as a base for review and refinement).Limitations:</p>
<p>• Potential biases in retrieved evidence may affect the objectivity of the reports.</p>
<p>• LLMs may struggle with complex geopolitical nuances and context-dependent interpretations.</p>
<p>• Human evaluation introduces subjectivity.</p>
<p>• Despite automation, human review is still mandatory before reports can be delivered to stakeholders (human-inthe-loop requirement).</p>
<p>Regional Variations in Model Performance</p>
<p>In our study, we observed distinct variations in model performance across different regions.While GPT models generally outperformed LLaMA in overall reporting quality, regionspecific differences emerged.GPT demonstrated superior performance in generating reports for Asia and Eastern Europe, whereas LLaMA produced more accurate and contextually relevant outputs for the Middle East and Africa.</p>
<p>A notable factor influencing these results is the disparity in media coverage across regions.Events in Europe and the Middle East tend to receive significantly more international attention compared to regions like the Horn of Africa.This uneven distribution of data likely contributes to variations in model performance, as regions with limited coverage may present greater challenges for accurate information retrieval and synthesis.</p>
<p>Benefits for Real-World Applications</p>
<p>The proposed system offers several distinct advantages for practical implementation:</p>
<ol>
<li>
<p>Enhanced Time Efficiency: The manual production of comparable analytical reports typically requires up to two weeks of continuous effort by a human analyst.Our system reduces this time by approximately 50%, generating a preliminary report that serves as a foundation for further refinement.This significantly accelerates the reporting pipeline while maintaining analytical rigor.</p>
</li>
<li>
<p>Scalability and Expanded Coverage: Human resource constraints often limit the geographical or thematic scope that analysts can feasibly cover.By automating the initial stages of report generation, our system enables broader coverage across multiple regions or topics on a more frequent or regular basis, thereby enhancing the scalability of monitoring and analysis efforts without proportional increases in staffing.</p>
</li>
<li>
<p>Resource Optimization and Cost Efficiency: The system exclusively utilizes publicly available, open-access data sources, eliminating the need for costly proprietary datasets.This approach not only reduces operational expenditures but also makes the system particularly suitable for resourceconstrained organizations, such as NGOs and humanitarian agencies.</p>
</li>
<li>
<p>analytical processes.This facilitates reproducibility and fosters trust among stakeholders, including policymakers, researchers, and civil society actors, who can validate and build upon the generated reports. 5. Rapid Situational Awareness: In the event of a sudden conflict outbreak, the system can generate automated reports that offer stakeholders an immediate preliminary assessment of the situation.This rapid access to critical information enables timely decision-making and response, bridging the gap before official reports are published.</p>
</li>
</ol>
<p>Future Work</p>
<p>This version of the system serves as an initial implementation, with planned improvements in the following areas:</p>
<p>• Visualizations to improve data representation and readability.</p>
<p>• Forecasting sections to provide predictive insights for stakeholders.</p>
<p>• Analytical sections that integrates or synthesizes information to detect new trends.</p>
<p>• Refining bias detection and mitigation strategies.</p>
<p>• Expanding LLM-as-a-judge to other evaluation tasks.</p>
<p>• Providing length and style options on the input stage.</p>
<p>Conclusion</p>
<p>In this work, we introduced a multimodal dynamic Retrieval-Augmented Generation (RAG) system for automated situation awareness reporting, designed to support peacekeeping operations, humanitarian organizations, and government agencies in making timely and informed decisions.By integrating real-time data from diverse sources, including news articles, conflict databases, and economic indicators, our system provides comprehensive and actionable reports while reducing the burden on human analysts.</p>
<p>While our automated reporting system offers a significant step forward in streamlining intelligence-gathering processes, it is not intended to replace human expertise but rather to augment it by providing preliminary insights quickly and efficiently.Future work will focus on refining the system's adaptability to new data sources, improving its interpretability, and expanding the evaluation framework to further validate its effectiveness across diverse operational contexts.</p>
<p>To foster further research and collaboration, we openly share our code and evaluation framework, encouraging the development of enhanced AI-driven solutions for dynamic situation awareness reporting.</p>
<p>Ethical Statement</p>
<p>All data utilized in this project is sourced exclusively from publicly accessible APIs, ensuring transparency and verifiability.No proprietary or confidential data is used, maintaining compliance with ethical and legal standards.</p>
<p>Figure 1 :
1
Figure 1: Overall system design presented as a flowchart.Created with Miro.</p>
<p>Figure 2 :
2
Figure 2: Report evaluation three-level framework.Created with Miro.</p>
<p>Table 2 :
2
Comparison of GPT and LLaMA judgment of the reports (Level 3 Evaluation)</p>
<p>Table 3 :
3
Transparency and Reproducibility: By leveraging open data, the system ensures transparency in data sourcing and Comparison of GPT and LLaMA Reports Based on Human Expert Evaluation (Level 2 Evaluation)
Evaluation MetricGPT-GPT-LLaMA-LLaMA-generated,generated,generated,generated,prompt 1prompt 2prompt 1prompt 2Cohen's Kappa Overall0.540.570.420.54Binary EvaluationCohen's Kappa on Binary Evaluation0.530.510.500.61Preference-based EvaluationCohen's Kappa on Preference Evaluation0.520.520.260.31Cohen's Kappa for Q8: Which report is more complete?0.580.580.240.12Cohen's Kappa for Q9: Which report is more accurate?0.540.540.170.12Cohen's Kappa for Q10: Which report do you prefer overall?0.440.540.360.48Avg. Max Score (Binary Questions)0.620.640.600.63Preferred Report (%)0.760.240.650.35Poorly Performed QuestionsQ4, Q5, Q7Q4, Q5, Q6,Q4, Q5Q4, Q5Q7Regional Best PerformanceAsia, EEHOA, MEAsia, MEAsia, EERegional Worst PerformanceHOA, MEAsia, EEEE, HOAME
https://www.gdeltproject.org/
https://pypi.org/project/newspaper3k/
https://acleddata.com/
https://reliefweb.int/
https://pypi.org/project/wbgapi/
https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2
https://www.anthropic.com/claude
AcknowledgmentsWe sincerely appreciate the support of the United Nations Development Programme (UNDP) Crisis Bureau for introducing the research problem and providing valuable insights throughout the study.Their comprehensive feedback and expert evaluation were instrumental in refining our approach and assessing the effectiveness of the generated reports.
A real-time medical report analysis and ai-powered diagnosis: A cloud-based solution for improved patient care. Alam, arXiv:2412.16086Towards interpretable radiology report generation via concept bottlenecks using a multi-agentic rag. 2024. 2024. 2020. 2020. 2024. 2024. 2024. 2024. 2020. 2020153arXiv preprintEMNLP '20</p>
<p>Automatic job safety report generation using rag-based llms. Bernardi, 2024 International Joint Conference on Neural Networks (IJCNN). 2024. 2024</p>
<p>A coefficient of agreement for nominal scales. Educational and psychological measurement. Jacob Cohen, Cohen, 1960. 196020</p>
<p>The faiss library. Jacob Devlin Devlin, Bert ; Douze, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. 2018. 2018. 2024. 2024arXiv preprint</p>
<p>A survey on rag meeting llms: Towards retrievalaugmented large language models. Fan , arXiv:2312.10997Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024. 2024. 2023. 2023arXiv preprintaugmented generation for large language models: A survey</p>
<p>Evaluating retrievalaugmented generation models for financial report question and answering. Guo, arXiv:2501.12948Applied Sciences. Ivan Iaroshev, Ramalingam Pillai, Leandro Vaglietti, and T. Hanne2025. 2025. 2024arXiv preprintDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</p>
<p>Summac: Revisiting nli-based models for inconsistency detection in summarization. Joshi, arXiv:2402.16406arXiv:2406.19276Yixiao Song, Yekyung Kim, and Mohit Iyyer. Veriscore: Evaluating the factuality of verifiable claims in long-form text generation. 2024. 2024. 2024. 2022. 2022. 2024. 2024. 2024. 2024. 2023. 2023. 2010. 2023. 2024. 2024. 202410arXiv preprintJournal of Instrumentation</p>
<p>How to handle armed conflict data in a real-world scenario?. Trivedi, Philosophy &amp; Technology. 342020. 2020</p>
<p>Optimizing data extraction: Harnessing rag and llms for german medical documents. Wang , Studies in health technology and informatics. 3162024. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
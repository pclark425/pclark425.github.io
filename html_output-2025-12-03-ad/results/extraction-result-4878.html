<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4878 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4878</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4878</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-104.html">extraction-schema-104</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <p><strong>Paper ID:</strong> paper-04a343ffba0120ac02e3138ffd6d7b85f753846b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/04a343ffba0120ac02e3138ffd6d7b85f753846b" target="_blank">Playing the Werewolf game with artificial intelligence for language understanding</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The results suggest that current language models have the capability to suspect what others are saying, tell a lie, or detect lies in conversations, as well as to interact with humans and choose the best voting target on the basis of its probability from the value network.</p>
                <p><strong>Paper Abstract:</strong> The Werewolf game is a social deduction game based on free natural language communication, in which players try to deceive others in order to survive. An important feature of this game is that a large portion of the conversations are false information, and the behavior of artificial intelligence (AI) in such a situation has not been widely investigated. The purpose of this study is to develop an AI agent that can play Werewolf through natural language conversations. First, we collected game logs from 15 human players. Next, we fine-tuned a Transformer-based pretrained language model to construct a value network that can predict a posterior probability of winning a game at any given phase of the game and given a candidate for the next action. We then developed an AI agent that can interact with humans and choose the best voting target on the basis of its probability from the value network. Lastly, we evaluated the performance of the agent by having it actually play the game with human players. We found that our AI agent, Deep Wolf, could play Werewolf as competitively as average human players in a villager or a betrayer role, whereas Deep Wolf was inferior to human players in a werewolf or a seer role. These results suggest that current language models have the capability to suspect what others are saying, tell a lie, or detect lies in conversations.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4878.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4878.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deep Wolf</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Wolf (Werewolf-playing LLM agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based agent that plays five-player text-chat Werewolf by scoring candidate actions with a fine-tuned Longformer value network and selecting the action with the highest predicted posterior win probability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Deep Wolf</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An AI agent that selects its next utterance, vote, or divination by evaluating ~100 candidate actions using a fine-tuned Longformer value network (regression model predicting posterior probability of winning) and picking the action with maximum score; the system uses 20 role-and-player-number-specific value networks to avoid role/number misclassification.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Longformer (AllenAI longformer-base-4096; RoBERTa-like bidirectional encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Werewolf (five-player text-chat Werewolf)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Interactive social-deduction gameplay: given the intermediate text-chat game log from the agent's viewpoint, choose the next utterance or vote to maximize the probability of winning the game.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>No external memory beyond the model context window; the agent relies on Longformer's attention over the provided intermediate game log (within model context length) and uses multiple specialized value networks (role/player-number-specific) as model-selection rather than an external memory module.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw text: concatenated intermediate game log (the chat history visible to the agent's viewpoint) plus a candidate action fed as model input; there is no separate structured memory store or persistent key-value memory described.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Implicit: the current chat log (intermediate game transcript) is appended/updated between decisions and provided as input to the value network for each candidate action; no separate update procedure for an external memory module is described. Additionally, the agent selects one of 20 pre-trained networks based on current role and player number.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td>Implicit retrieval via Longformer's self-attention over the entire input sequence (intermediate log + candidate action) when computing the value (posterior win probability); the agent does not perform explicit retrieval from an external memory store.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>No explicit ablations comparing memory mechanisms. The only architecture-related analysis tied to internal representation was that the value network sometimes misidentified role/player-number, which motivated training 20 independent value networks specialized by role and player number; there were no experiments varying context-window size, external memory, or summarization strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Reported limitations relevant to memory/use of context: (1) limited training data (32 games) which constrains learning of long-term conversation dynamics; (2) finite candidate-action set (~100 utterances) prevents free-form generation and can produce awkward behavior; (3) occasional misidentification of current role/player number by the network (addressed via multiple specialized networks); (4) no external memory/store beyond the model context, so long-term summarization or prediction of multi-turn plans was not implemented; (5) the authors note context-length and model-capacity limits and suggest larger models or more data could help.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Authors recommend several directions (not explicit memory modules): (1) use attention-visualization tools (e.g., BertViz) to inspect which parts of the input the model attends to and to build explainability; (2) consider larger language models to improve performance; (3) consider a differentiable tokenizer/generator for more efficient search over utterances; (4) improve the agent by predicting future actions of other players (deep rollouts) and by training on more game logs; they do not provide a concrete prescription for adding external memory, but suggest enhancing prediction of others' future actions and using interpretability to justify choices.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Playing the Werewolf game with artificial intelligence for language understanding', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4878.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4878.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based agents playing text games, with a focus on how memory is used, the type of memory mechanisms, comparative performance with and without memory, and any recommendations or challenges regarding memory usage.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>wolfbbsRoBERTa-large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>wolfbbsRoBERTa-large (RoBERTa model trained on Werewolf logs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A RoBERTa model trained on a large set of Werewolf chat logs (public release), cited as prior work that can infer player roles from text but not used as an agent that plays the game in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>wolfbbsroberta-large</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>wolfbbsRoBERTa-large</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A RoBERTa-based model trained on Werewolf/BBS logs for role inference; referenced as existing work that processes chat logs to infer roles but that did not implement a value network or an actual playing agent in the cited resource.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>RoBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Werewolf (chat logs / role-inference tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Role inference from Werewolf chat logs (classification of players' roles based on conversation), not an interactive action-selection agent.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>No memory-related analyses reported in this paper's citation; the current paper cites it only as a pretrained model resource.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Mentioned only as prior work; this paper notes that prior RoBERTa-based models inferred roles but did not build a value network or an interactive agent.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Playing the Werewolf game with artificial intelligence for language understanding', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>wolfbbsroberta-large <em>(Rating: 2)</em></li>
                <li>Automatically detecting player roles in Among Us <em>(Rating: 2)</em></li>
                <li>Human-like artificial intelligence using deep reinforcement learning for the werewolf game <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4878",
    "paper_id": "paper-04a343ffba0120ac02e3138ffd6d7b85f753846b",
    "extraction_schema_id": "extraction-schema-104",
    "extracted_data": [
        {
            "name_short": "Deep Wolf",
            "name_full": "Deep Wolf (Werewolf-playing LLM agent)",
            "brief_description": "An LLM-based agent that plays five-player text-chat Werewolf by scoring candidate actions with a fine-tuned Longformer value network and selecting the action with the highest predicted posterior win probability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Deep Wolf",
            "agent_description": "An AI agent that selects its next utterance, vote, or divination by evaluating ~100 candidate actions using a fine-tuned Longformer value network (regression model predicting posterior probability of winning) and picking the action with maximum score; the system uses 20 role-and-player-number-specific value networks to avoid role/number misclassification.",
            "llm_model_name": "Longformer (AllenAI longformer-base-4096; RoBERTa-like bidirectional encoder)",
            "game_or_benchmark_name": "Werewolf (five-player text-chat Werewolf)",
            "task_description": "Interactive social-deduction gameplay: given the intermediate text-chat game log from the agent's viewpoint, choose the next utterance or vote to maximize the probability of winning the game.",
            "memory_used": false,
            "memory_type": "No external memory beyond the model context window; the agent relies on Longformer's attention over the provided intermediate game log (within model context length) and uses multiple specialized value networks (role/player-number-specific) as model-selection rather than an external memory module.",
            "memory_representation": "Raw text: concatenated intermediate game log (the chat history visible to the agent's viewpoint) plus a candidate action fed as model input; there is no separate structured memory store or persistent key-value memory described.",
            "memory_update_mechanism": "Implicit: the current chat log (intermediate game transcript) is appended/updated between decisions and provided as input to the value network for each candidate action; no separate update procedure for an external memory module is described. Additionally, the agent selects one of 20 pre-trained networks based on current role and player number.",
            "memory_retrieval_mechanism": "Implicit retrieval via Longformer's self-attention over the entire input sequence (intermediate log + candidate action) when computing the value (posterior win probability); the agent does not perform explicit retrieval from an external memory store.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "ablation_or_analysis": "No explicit ablations comparing memory mechanisms. The only architecture-related analysis tied to internal representation was that the value network sometimes misidentified role/player-number, which motivated training 20 independent value networks specialized by role and player number; there were no experiments varying context-window size, external memory, or summarization strategies.",
            "challenges_or_limitations": "Reported limitations relevant to memory/use of context: (1) limited training data (32 games) which constrains learning of long-term conversation dynamics; (2) finite candidate-action set (~100 utterances) prevents free-form generation and can produce awkward behavior; (3) occasional misidentification of current role/player number by the network (addressed via multiple specialized networks); (4) no external memory/store beyond the model context, so long-term summarization or prediction of multi-turn plans was not implemented; (5) the authors note context-length and model-capacity limits and suggest larger models or more data could help.",
            "best_practices_or_recommendations": "Authors recommend several directions (not explicit memory modules): (1) use attention-visualization tools (e.g., BertViz) to inspect which parts of the input the model attends to and to build explainability; (2) consider larger language models to improve performance; (3) consider a differentiable tokenizer/generator for more efficient search over utterances; (4) improve the agent by predicting future actions of other players (deep rollouts) and by training on more game logs; they do not provide a concrete prescription for adding external memory, but suggest enhancing prediction of others' future actions and using interpretability to justify choices.",
            "uuid": "e4878.0",
            "source_info": {
                "paper_title": "Playing the Werewolf game with artificial intelligence for language understanding",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "wolfbbsRoBERTa-large",
            "name_full": "wolfbbsRoBERTa-large (RoBERTa model trained on Werewolf logs)",
            "brief_description": "A RoBERTa model trained on a large set of Werewolf chat logs (public release), cited as prior work that can infer player roles from text but not used as an agent that plays the game in this study.",
            "citation_title": "wolfbbsroberta-large",
            "mention_or_use": "mention",
            "agent_name": "wolfbbsRoBERTa-large",
            "agent_description": "A RoBERTa-based model trained on Werewolf/BBS logs for role inference; referenced as existing work that processes chat logs to infer roles but that did not implement a value network or an actual playing agent in the cited resource.",
            "llm_model_name": "RoBERTa",
            "game_or_benchmark_name": "Werewolf (chat logs / role-inference tasks)",
            "task_description": "Role inference from Werewolf chat logs (classification of players' roles based on conversation), not an interactive action-selection agent.",
            "memory_used": null,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_mechanism": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "ablation_or_analysis": "No memory-related analyses reported in this paper's citation; the current paper cites it only as a pretrained model resource.",
            "challenges_or_limitations": "Mentioned only as prior work; this paper notes that prior RoBERTa-based models inferred roles but did not build a value network or an interactive agent.",
            "best_practices_or_recommendations": null,
            "uuid": "e4878.1",
            "source_info": {
                "paper_title": "Playing the Werewolf game with artificial intelligence for language understanding",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "wolfbbsroberta-large",
            "rating": 2
        },
        {
            "paper_title": "Automatically detecting player roles in Among Us",
            "rating": 2
        },
        {
            "paper_title": "Human-like artificial intelligence using deep reinforcement learning for the werewolf game",
            "rating": 1
        }
    ],
    "cost": 0.00985025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Playing the Werewolf game with artificial intelligence for language understanding</h1>
<p>Hisaichi SHIBATA ${ }^{\mathrm{a}}$, Soichiro MIKI ${ }^{\mathrm{a}}$, Yuta NAKAMURA ${ }^{\mathrm{b}}$<br>${ }^{a}$ Department of Radiology, The University of Tokyo Hospital, 7-3-1 Hongo, Bunkyo, Tokyo 113-8655, Japan<br>${ }^{b}$ Division of Radiology and Biomedical Engineering, Graduate School of Medicine, The University of Tokyo, 7-3-1 Hongo, Bunkyo, Tokyo 113-8655, Japan</p>
<h4>Abstract</h4>
<p>The Werewolf game is a social deduction game based on free natural language communication, in which players try to deceive others in order to survive. An important feature of this game is that a large portion of the conversations are false information, and the behavior of artificial intelligence (AI) in such a situation has not been widely investigated. The purpose of this study is to develop an AI agent that can play Werewolf through natural language conversations. First, we collected game logs from 15 human players. Next, we fine-tuned a Transformer-based pretrained language model to construct a value network that can predict a posterior probability of winning a game at any given phase of the game and given a candidate for the next action. We then developed an AI agent that can interact with humans and choose the best voting target on the basis of its probability from the value network. Lastly, we evaluated the performance of the agent by having it actually play the game with human players. We found that our AI agent, Deep Wolf, could play Werewolf as competitively as average human players in a villager or a betrayer role, whereas Deep Wolf was inferior to human players in a werewolf or a seer role. These results suggest that current language models have the capability to suspect what others are saying, tell a lie, or detect lies in conversations.</p>
<p>Keywords: natural language processing, deep generative models, werewolf game</p>
<h1>1. Introduction</h1>
<p>Most current artificial intelligence (AI) systems learn and output data under the assumption that most of the data they process is correct. However, since real society is plagued by false information, it is helpful if AI can detect contradictions and false information. The Werewolf game, also known as Among Us, is a social deduction game based on free natural language communication (see Appendix A for details). In Werewolf, all the players (usually 5 to 10 ) are randomly assigned a role (e.g., villager, werewolf, betrayer, and seer), and they try to infer the roles of other players. Villagers and seers are on the villager side and try to expel werewolves by voting during the day. On the werewolf side, werewolves try to kill villagers at night, and betrayers try to disturb the villager side. Players must deceive other players in order to survive, and conversations in Werewolf inevitably contain a large amount of false information. To win in Werewolf, players need to remember the details of conversations, make hypotheses about other players' roles, and detect contradictions in them. In other words, all players deduce the necessary conditions that the claims of other players are true. Werewolf played on a text chat basis rather than in verbal conversations is also popular and is played online. Recent studies aim to apply AI mainly to text-chat-based Werewolf.</p>
<p>While it is difficult to develop AI that can detect all kinds of fakes in the real world, the relatively limited vocabulary and types of information handled in Werewolf have attracted attention as a research subject for AI that can detect false information. Here, we focused on Werewolf based on text chat.</p>
<p>The purpose of this study is to develop an AI agent that can understand natural language and play Werewolf. Such an AI agent should reasonably compute the statements that can effectively lead to the successful expulsion or killing of players of the opponent side. To establish such AI agent, we must develop an approximate algorithm to extract the optimum output $\hat{\boldsymbol{y}}$ constrained with an input context $\boldsymbol{x}$, i.e., $\hat{\boldsymbol{y}}=\operatorname{argmax}_{\boldsymbol{y}} p(\boldsymbol{y} \mid \boldsymbol{x})$, where $p$ is the posterior probability of winning the game.</p>
<p>Transformer-based language models [1, 2] can efficiently learn the conditional probability $p(\boldsymbol{y} \mid \boldsymbol{x})$ of a text. We fine-tune one of such models using chat log data of Werewolf, and we construct a value network that can predict the posterior probability of winning a game at any given game phase. We use this value network to realize an AI agent that can play Werewolf</p>
<p>with human players. Lastly, we evaluate the performance of the AI agent by actually having it play Werewolf with human players and gathering its win rate.</p>
<h1>2. Related Works</h1>
<p>There are many AI agents that can play Werewolf using a predefined protocol instead of natural languages. For example, Fukuda and Anada [3] realized an AI agent in which deep reinforcement learning is applied. Kimura and Ito [4] proposed a method for inferring player roles with a support vector machine. An exhaustive list of such protocol-based AI agents can be found on the website of International AI Werewolf Competition [5]. Several algorithms that can process a natural language have been proposed, most of which can infer player roles but cannot play Werewolf itself. For instance, Tuin and Rooijackers [6] applied a support vector machine to game logs of Werewolf to infer player roles. A RoBERTa [1] model trained with a large set of Werewolf logs has been published [7], but the authors did not establish a value network nor an AI agent that can actually play the game. To the best of our knowledge, no study has been published on AI agents that can directly understand a natural language and play Werewolf in it. In particular, to date, there has been no published study on statistical information for the win rates of AI agents.</p>
<h2>3. Methods</h2>
<p>In this study, we focused on Werewolf played by five players: two villagers, one seer, one betrayer, and one werewolf (see Appendix A for the details). Briefly, we first collected a dataset of game logs from the actual play history of human participants. Second, we constructed a value network that can predict the posterior probability of winning a game. Third, we developed an AI agent named Deep Wolf, which can play Werewolf in English on the basis of the value network. The details of each step are described below.</p>
<h3>3.1. Dataset Building</h3>
<p>Fifteen participants played text chat-based, five-player Werewolf without AI players. We used our newly developed web-based Werewolf platform. The participants used a natural language (Japanese), but their names were</p>
<p>masked and replaced with randomly assigned number-based pronouns (#1$# 5$ ), and they were instructed to refer to one another using the numbers. Logs from a total of 48 games were collected, and we calculated win rates for each player and each role. Due to memory constraints, we randomly picked 32 games for machine learning, 16 of which were won by the villager side. Next, we translated the game logs into English using a commercial machine translation platform, and then performed manual grammar correction and data cleansing (e.g., the normalization of symbol usage). Since Werewolf is an imperfect information game, we converted each "full" game log into five logs representing the viewpoint of each player by masking information that must be invisible, such as other players' roles. We further augmented each game log by swapping number-based pronouns of coplayers in all possible $4!=24$ patterns so that our AI agent could learn an immutable structure irrelevant to the replacement of player numbers. As a result, we obtained a dataset consisting of $32 \times 5 \times 4!=3,840$ game logs.</p>
<h1>3.2. Value Network</h1>
<p>We built a regression model that can calculate the posterior probability of winning a game given an intermediate log at any given game phase from the viewpoint of any player role. We adopted an open, Transformer-based model developed by Allen Institute for AI ${ }^{1}$. This is a RoBERTa-like bidirectional encoder using Longformer [2] pretrained with English web corpora. We fine-tuned the model with the aforementioned game logs. The loss function to be minimized was a cross-entropy function that contained only the posterior probability of winning the game. We provided the results of the games as labels in addition to the intermediate game logs. The network was trained to calculate the probability of winning given any intermediate log and a candidate for the next action (e.g., an utterance, a vote to expel or kill another player). Although previous studies have often built a model to explicitly infer the roles of other players, we did not build such a model, because we assumed such a capability of inferring roles would be implicitly integrated in our large language model.</p>
<p>During development, we noticed that our network sometimes made mistakes about its current role and player number. To work around this, we decided to build 20 independent value networks dedicated to each combina-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Hyperparameters to train the value network.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Content</th>
<th style="text-align: right;">Parameter</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Epochs</td>
<td style="text-align: right;">1</td>
</tr>
<tr>
<td style="text-align: left;">Batch size</td>
<td style="text-align: right;">1</td>
</tr>
<tr>
<td style="text-align: left;">max_input_length</td>
<td style="text-align: right;">2,048</td>
</tr>
<tr>
<td style="text-align: left;">Learning rate</td>
<td style="text-align: right;">$1 \times 10^{-5}$</td>
</tr>
</tbody>
</table>
<p>tion of four different roles and five different player numbers. In the following steps, one of the 20 networks was selected according to the AI agent's current role and player number.</p>
<p>In the training, we set hyperparameters of the value network (Longformer) as in Table. 1. To accelerate the inference of the value network, we converted it into the Open Neural Network Exchange (ONNX) format and then optimized (simplified) the model using onnxsim. Although it was possible to employ batch processing during the training, we decided to set the batch size to 1. Training and inference were performed on a single graphics processing unit (NVIDIA RTX-A6000). The training of the value network finished in 1.5 hours per epoch and required about 30 hours in total. The value network inference typically finished within 10 seconds, which was considered fast enough for the chat-based Werewolf.</p>
<h1>3.3. Deep Wolf</h1>
<p>The next step was to construct an AI agent that could infer the next optimal action to win the game. The possible actions (including utterances, divinations, and votes) were collected from the logs of humans playing the same role as the current AI role. The number of utterances in the collected logs was about 300 per role, but we refined these by deleting similar or duplicate sentences, and the number of possible utterances became about 100. The agent input these action candidates into the value network to evaluate the posterior probability of winning the game for each action, and the agent determined the next action that could maximize the chance of winning.</p>
<p>We set the actions of Deep Wolf as follows. Note that this Werewolf game with five players always finishes within two days. See Appendix B for a sample game log.</p>
<p>Table 2: Win rates of five human players in Werewolf game. N/A means that we cannot define the win rate since no competition was executed.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Werewolf</th>
<th style="text-align: left;">Seer</th>
<th style="text-align: left;">Betrayer</th>
<th style="text-align: left;">Villager</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Player 1</td>
<td style="text-align: left;">0.50</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">0</td>
</tr>
<tr>
<td style="text-align: left;">Player 2</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.33</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.33</td>
</tr>
<tr>
<td style="text-align: left;">Player 3</td>
<td style="text-align: left;">0.63</td>
<td style="text-align: left;">0.75</td>
<td style="text-align: left;">0.33</td>
<td style="text-align: left;">0.80</td>
</tr>
<tr>
<td style="text-align: left;">Player 4</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.75</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.60</td>
</tr>
<tr>
<td style="text-align: left;">Player 5</td>
<td style="text-align: left;">0.50</td>
<td style="text-align: left;">0.75</td>
<td style="text-align: left;">0.50</td>
<td style="text-align: left;">0.57</td>
</tr>
<tr>
<td style="text-align: left;">Player 6</td>
<td style="text-align: left;">0.60</td>
<td style="text-align: left;">0.60</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.60</td>
</tr>
<tr>
<td style="text-align: left;">Player 7</td>
<td style="text-align: left;">0.67</td>
<td style="text-align: left;">0.60</td>
<td style="text-align: left;">0.50</td>
<td style="text-align: left;">0.70</td>
</tr>
<tr>
<td style="text-align: left;">Player 8</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">0.63</td>
</tr>
<tr>
<td style="text-align: left;">Player 9</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.60</td>
<td style="text-align: left;">0.50</td>
<td style="text-align: left;">0.71</td>
</tr>
<tr>
<td style="text-align: left;">Player 10</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.25</td>
<td style="text-align: left;">0.40</td>
<td style="text-align: left;">0</td>
</tr>
<tr>
<td style="text-align: left;">Player 11</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.50</td>
</tr>
<tr>
<td style="text-align: left;">Player 12</td>
<td style="text-align: left;">0.40</td>
<td style="text-align: left;">0.67</td>
<td style="text-align: left;">0.60</td>
<td style="text-align: left;">0.83</td>
</tr>
<tr>
<td style="text-align: left;">Player 13</td>
<td style="text-align: left;">0.67</td>
<td style="text-align: left;">0.67</td>
<td style="text-align: left;">0.50</td>
<td style="text-align: left;">0.70</td>
</tr>
<tr>
<td style="text-align: left;">Player 14</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">0.50</td>
</tr>
<tr>
<td style="text-align: left;">Player 15</td>
<td style="text-align: left;">0.25</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.20</td>
</tr>
<tr>
<td style="text-align: left;">Average</td>
<td style="text-align: left;">0.41</td>
<td style="text-align: left;">0.62</td>
<td style="text-align: left;">0.40</td>
<td style="text-align: left;">0.60</td>
</tr>
</tbody>
</table>
<ol>
<li>If all the agents except Deep Wolf emitted Over and finished their conversation, Deep Wolf also emits Over and finishes its conversation.</li>
<li>If different $k$ players except Deep Wolf uttered, and Deep Wolf did not utter, Deep Wolf begins to speak. In this study, we set $k=3$ for day 1 , and $k=1$ for day 2 .</li>
<li>Deep Wolf does not utter the same sentence in the same game.</li>
</ol>
<h1>4. Results</h1>
<h3>4.1. Competition among human players and training of the value network</h3>
<p>We show in Table 2 win rates in the Werewolf game of five human players.</p>
<h3>4.2. Competition among human players and Deep Wolf</h3>
<p>We show in Table 3 the win rates of four human players and one AI agent (Deep Wolf) in the Werewolf game.</p>
<p>Table 3: Win rates of four human players and one AI agent in Werewolf game. N/A means that we cannot define the win rate since no competition was executed. Player numbers in Table 2 and Table 3 do not necessarily represent the same player.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Werewolf</th>
<th style="text-align: left;">Seer</th>
<th style="text-align: left;">Betrayer</th>
<th style="text-align: left;">Villager</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Player 1</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">1.0</td>
</tr>
<tr>
<td style="text-align: left;">Player 2</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.67</td>
<td style="text-align: left;">0.33</td>
<td style="text-align: left;">0.50</td>
</tr>
<tr>
<td style="text-align: left;">Player 3</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">1.0</td>
</tr>
<tr>
<td style="text-align: left;">Player 4</td>
<td style="text-align: left;">0.50</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.55</td>
</tr>
<tr>
<td style="text-align: left;">Player 5</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">0.50</td>
<td style="text-align: left;">0.67</td>
<td style="text-align: left;">1.0</td>
</tr>
<tr>
<td style="text-align: left;">Player 6</td>
<td style="text-align: left;">0.75</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.75</td>
<td style="text-align: left;">0.75</td>
</tr>
<tr>
<td style="text-align: left;">Player 7</td>
<td style="text-align: left;">0.50</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.40</td>
</tr>
<tr>
<td style="text-align: left;">Player 8</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">0.40</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">0.50</td>
</tr>
<tr>
<td style="text-align: left;">Player 9</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">0.25</td>
<td style="text-align: left;">0.20</td>
</tr>
<tr>
<td style="text-align: left;">Player 10</td>
<td style="text-align: left;">0.33</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">N/A</td>
</tr>
<tr>
<td style="text-align: left;">Player 11</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">0</td>
</tr>
<tr>
<td style="text-align: left;">Average (Human)</td>
<td style="text-align: left;">0.52</td>
<td style="text-align: left;">0.60</td>
<td style="text-align: left;">0.45</td>
<td style="text-align: left;">0.55</td>
</tr>
<tr>
<td style="text-align: left;">Deep Wolf</td>
<td style="text-align: left;">0.29</td>
<td style="text-align: left;">0.25</td>
<td style="text-align: left;">0.44</td>
<td style="text-align: left;">0.56</td>
</tr>
</tbody>
</table>
<h1>5. Discussion</h1>
<h3>5.1. Behavior of the value network</h3>
<p>When the role of the value network was a werewolf, the posterior probability was almost always low in the early stages of the game. On the other hand, when the role of the value network was a seer, the posterior probability was almost always high in the early stages of the game. This suggests that the network judged the villager side had an advantage in this five-player game setting, which was consistent with the tendency of the actual game results. This demonstrates an ability of the value network to correctly estimate the probability of winning, even though it was trained with a dataset where the overall win rates were 0.5 for both the werewolf and villager sides.</p>
<h3>5.2. Performance of Deep Wolf</h3>
<p>By comparing the win rates in Table 3, we found that the average win rates of the human betrayer and villager are not significantly different from the win rates of the Deep Wolf betrayer and villager. This suggests that Deep Wolf can play the game almost equally to human players if its role</p>
<p>is the betrayer or villager. On the other hand, for the seer and werewolf, Deep Wolf was significantly inferior to average human players. However, it is interesting that there are cases when Deep Wolf can win over human players even if its role is the werewolf.</p>
<h1>5.3. Novelty</h1>
<p>We, for the first time, established an AI agent that can play Werewolf by understanding a natural language with human players. We observed that the behavior of the AI agent was sometimes awkward, but we confirmed that the AI agent can win the game against human players with a certain probability.</p>
<h3>5.4. Limitation</h3>
<p>The AI agent we have constructed chooses its next utterance from a large but finite set of actions gathered from the log of games played by humans. Additionally, it would be necessary to improve the model in order for humans to be satisfied with the content of the game that includes AI participants.</p>
<h3>5.5. Future Works</h3>
<p>By using a system that can visualize the weight of attention, e.g., BertViz [8], we may be able to construct an AI agent that can explain the reason for choosing a particular action or utterance. Although we were unable to obtain an appropriate pretrained Japanese language model for Longformer, the construction of a value network that can handle the Japanese language will be included in our future works. Although we employed a model with about 140 million parameters, there is a possibility that the win rates of the AI agent can be improved by employing a language model with a larger number of parameters. In this study, we used a brute force approach to choose the next utterance, but the introduction of a differentiable tokenizer and a generator may allow us to efficiently search for the optimal solution. In this study, only the player's own actions were evaluated using the value network, but it may be possible to improve the win rate of the AI agent by deeply predicting the future actions of other players. In a game involving AI agents, in addition to the task of inferring the other player's role, there would be an additional task of inferring whether each player is an AI agent or a human. It would be an interesting topic to evaluate the changes in the strategy used by human players in such situations. Although only one AI agent at a time was used in this study, we believe that eventually, it will be possible to play Werewolf with only AI agents and entertain the game's</p>
<p>audience. We trained and evaluated Deep Wolf on the basis of only 32 game logs, but we have obtained about 100 game logs so far, and we will consider retraining the AI agent because we believe that increasing the amount of training data will improve its performance.</p>
<p>In this experiment on evaluating the performance of the AI agents, we did not measure the degree of fatigue of the human participants. Since the concept of fatigue does not exist for AI agents, and AI agents are guaranteed to always be able to play the game at a certain strength unless the program is changed, this implies that a human player's winning rate can determine the degree of fatigue of the human player. Evaluating the degree of fatigue through games is included in future works.</p>
<h1>6. Conclusion</h1>
<p>By fine-tuning a Transformer-based model, Longformer, for natural language processing, we developed an AI agent (Deep Wolf) that can play Werewolf through natural language conversations. We found that Deep Wolf could play Werewolf with a performance comparable to average human players when its role was a villager or a betrayer, but Deep Wolf was inferior to human players when its role was a werewolf or a seer. These results suggest that current language models have the capability to understand statements, tell a lie, or detect lies in conversations.</p>
<h2>Acknowledgements</h2>
<p>This work was supported by JST, CREST Grant Number JPMJCR21M2, including the AIP challenge program (Necessary conditions that personal claims are objective facts), Japan. We thank many participants who played Werewolf again and again to generate game logs and evaluate Deep Wolf.</p>
<h2>Appendix A. Preliminary for Werewolf game</h2>
<p>Werewolf game adopted in this project is a five-player game and will largely conform to the regulations of the Natural Language Division of the 4th International Werewolf Intelligence Convention. One of the five players is a werewolf, two are villagers, one is a seer (diviner), and the remaining one is a betrayer. Players can only know their own role. The other players' roles are not known. However, the seer can divine the role of any participant by divination. However, divination can be done only once a day. The</p>
<p>results of divination during the daytime are shown only to the seer the next morning, so there is a possibility of being targeted by werewolves or suspected by villagers if the results are not well communicated to others. The betrayer poses as a villager (and is determined to be a villager when divined by the seer), but the betrayer is on the werewolf side and manipulates information to the advantage of the werewolves. The game is played through text conversations and voting to determine who is eliminated. In addition, the surviving werewolves at night decide who to attack and who to eliminate. The game begins at noon on Day 0 . Depending on the rules, all participants may introduce themselves and greet each other at noon on Day 0 , but in this case, the diviner only decides who to divine. The diviner may also announce himself/herself (e.g., "I am a diviner. I divined #1 and he was a werewolf". This is called the diviner coming out.) In some cases, a betrayer or werewolf may come forward as a diviner, lying to confuse the players. The villager side wins when all werewolves are eliminated, and the werewolf side wins when the number of werewolves equals that of villagers; these conditions are automatically applied in the game.</p>
<h1>Appendix B. Example of Competition with Deep Wolf</h1>
<p>We show a log for the Werewolf game when the role of Deep Wolf was the werewolf (werewolf side) and Deep Wolf won the game. We recognized a description, which is not suitable for publication, in the log; hence, we replaced it with other words.</p>
<ul>
<li>#1 divined #2 and #2 is not a werewolf.</li>
<li>#4) Good morning. I am a villager.</li>
<li>#1) I am the diviner. #2 is clean.</li>
<li>#4) Hello! I'm a villager.</li>
<li>#2) Hi, I am a villager!</li>
<li>#3) I have a feeling that's the case, shall we hang suspicious #5? I'm sorry if he's one of them!</li>
<li>#2) &gt; #1 Thanks.</li>
<li>
<p>#5) I must be innocent and not suspect.</p>
</li>
<li>
<p>#5) #1 seems to be the real fortune teller.</p>
</li>
<li>#1) Yes. And #2 is not a werewolf.</li>
<li>#2) But there may be two diviners.</li>
<li>#5) I agree</li>
<li>#3) Hello, I am a villager.</li>
<li>#3) I don't think I would attack a diviner with a large number of people.</li>
<li>#2) So #1 must be a diviner.</li>
<li>#2) But the werewolf will kill #1 tonight.</li>
<li>#1) That is not a problem.</li>
<li>#4) Then we should choose #3, #4 or #5 to expel?</li>
<li>#1) Yes!</li>
<li>#2) The werewolf pretends to be a villager.</li>
<li>#3) Well, it is one strategy to leave the betrayal-like #2 as betrayal and hang the other grays at random.</li>
<li>#3) I will vote for #1.</li>
<li>#1) #3 may be a betrayer.</li>
<li>#3) I am a villager. Pleased to meet you.</li>
<li>#1) Oh, I made a mistake. Sorry.</li>
<li>#1) I will vote #3, #4 or #5.</li>
<li>#4) I think #3 is suspicious.</li>
<li>#2) $&gt;# 4$, but I may be a betrayer.Why do not you think I am not a betrayer?</li>
<li>
<p>#3) Well, it is one strategy to leave the betrayal-like #5 as betrayal and hang the other grays at random.</p>
</li>
<li>
<p>#4) #4, you're right. Sorry I couldn't think that much.</p>
</li>
<li>#2) I will vote for #4. #4 seems to rush to conclusion.</li>
<li>#1) I feel #2 is a betrayer. #4 is not suspicious.</li>
<li>#2) I am a villager.</li>
<li>#3) I am also a villager, but I wonder if the later #1 are suspicious...?</li>
<li>#2) If I were a betrayer, I would not say such a thing.</li>
<li>#2) Over.</li>
<li>#4) Over.</li>
<li>#5) Over.</li>
<li>#3) Over.</li>
<li>#1) Over.</li>
<li>#1 voted for #2.</li>
<li>#4 voted for #3.</li>
<li>#3 voted for #5.</li>
<li>#5 voted for #4.</li>
<li>#2 voted for #4.</li>
<li>#4 has been erased.</li>
<li>The werewolf erased #2.</li>
<li>#1 divined #3 and #3 is the werewolf.</li>
<li>#3) Over.</li>
<li>#1) #3 is a werewolf.</li>
<li>#5) I am a traitor. Let's do a power play, #3.</li>
<li>
<p>#5) Over.</p>
</li>
<li>
<p>#1) #5, are you a villager?</p>
</li>
<li>#1) ok</li>
<li>#1) Over.</li>
<li>#1 voted for #3.</li>
<li>#3 voted for #1.</li>
<li>#5 voted for #1.</li>
<li>#1 has been erased.</li>
<li>The werewolf erased #5.
#1 was a seer, #2 and #4 are villagers, #3 was a werewolf (Deep Wolf), and #5 was a betrayer.</li>
</ul>
<h1>References</h1>
<p>[1] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, V. Stoyanov, RoBERTa: A robustly optimized BERT pretraining approach, arXiv preprint arXiv:1907.11692 (2019).
[2] I. Beltagy, M. E. Peters, A. Cohan, Longformer: The long-document transformer, arXiv preprint arXiv:2004.05150 (2020).
[3] M. Fukuda, H. Anada, Human-like artificial intelligence using deep reinforcement learning for the werewolf game, Special Interest Group on Society and Artificial Intelligence (SIG-SAI) (2015).
[4] Y. Kimura, T. Ito, Improvement of role estimation using machine learning in Werewolf AI, GAT2018 (2018).
[5] Artificial-intelligence-based Werewolf, http://aiwolf.org/ (2015).
[6] H. Tuin, M. Rooijackers, Automatically detecting player roles in Among Us, in: 2021 IEEE Conference on Games (CoG), IEEE, 2021, pp. 1-5.
[7] I. Tsunoda, wolfbbsroberta-large, https://huggingface.co/itsunoda/wolfbbsRoBERTa-large (2021).</p>
<p>[8] J. Vig, BertViz: A tool for visualizing multihead self-attention in the BERT model, in: ICLR Workshop: Debugging Machine Learning Models, 2019 .</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://huggingface.co/allenai/longformer-base-4096&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>